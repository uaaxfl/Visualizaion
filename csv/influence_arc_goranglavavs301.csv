2020.acl-main.151,Q19-1038,0,0.362553,"ality labels) (Specia et al., 2010) or language-specific preprocessing like semantic parsing (Lo et al., 2014; Lo, 2019), both hindering the wide applicability of the proposed metrics. Moreover, they have also typically exhibited performance levels well below those of standard reference-based metrics (Ma et al., 2019). In this work, we comparatively evaluate a number of reference-free MT evaluation metrics that build on the most recent developments in multilingual representation learning, namely cross-lingual contextualized embeddings (Devlin et al., 2019) and cross-lingual sentence encoders (Artetxe and Schwenk, 2019). We investigate two types of crosslingual reference-free metrics: (1) Soft token-level alignment metrics find the optimal soft alignment between source sentence and system translation using Word Mover’s Distance (WMD) (Kusner et al., 2015). Zhao et al. (2019) recently demonstrated that WMD operating on BERT representations (Devlin et al., 2019) substantially outperforms baseline MT evaluation metrics in the reference-based setting. In this work, we investigate whether WMD can yield comparable success in the reference-free (i.e., cross-lingual) setup; (2) Sentence-level similarity metrics meas"
2020.acl-main.151,W17-4755,0,0.423254,"erview of both reference-based MT evaluation metrics and recent research efforts towards reference-free MT evaluation, which leverage cross-lingual semantic representations and unsupervised MT techniques. Reference-based MT evaluation. Most of the commonly used evaluation metrics in MT compare system and reference translations. They are often based on surface forms such as n-gram overlaps like BLEU (Papineni et al., 2002), SentBLEU, NIST (Doddington, 2002), chrF++ (Popovi´c, 2017) or METEOR++(Guo and Hu, 2019). They have been extensively tested and compared in recent WMT metrics shared tasks (Bojar et al., 2017a; Ma et al., 2018a, 2019). These metrics, however, operate at the surface level, and by design fail to recognize semantic equivalence lacking lexical overlap. To overcome these limitations, some research efforts exploited static word embeddings (Mikolov et al., 2013b) and trained embedding-based supervised metrics on sufficiently large datasets with available human judgments of translation quality (Shimanaka 1657 et al., 2018). With the development of contextual word embeddings (Peters et al., 2018; Devlin et al., 2019), we have witnessed proposals of semantic metrics that account for word or"
2020.acl-main.151,S17-2001,0,0.0423846,"at are best-aligned mutual translations. YiSi2-SRL optionally combines an additional similarity score based on the alignment over the semantic structures (e.g., semantic roles and frames). Both metrics are reference-free, but YiSi-2-SRL is not resource-lean as it requires a semantic parser for both languages. Moreover, in contrast to our proposed metrics, they do not mitigate the misalignment of cross-lingual embedding spaces and do not integrate a target-side language model, which we identify to be crucial components. Recent progress in cross-lingual semantic similarity (Agirre et al., 2016; Cer et al., 2017) and unsupervised MT (Artetxe and Schwenk, 2019) has also led to novel reference-free metrics. For instance, Yankovskaya et al. (2019) propose to train a metric combining multilingual embeddings extracted from M-BERT and LASER (Artetxe and Schwenk, 2019) together with the log-probability scores from neural machine translation. Our work differs from that of Yankovskaya et al. (2019) in one crucial aspect: the cross-lingual reference-free metrics that we investigate and benchmark do not require any human supervision. Cross-lingual Representations. Cross-lingual text representations offer a prosp"
2020.acl-main.151,P19-1264,0,0.0652817,"sk has clearly defined and unambiguous labels and, in most cases, that an instance can be assigned few labels. These assumptions, however, do not hold for natural language generation (NLG) tasks like machine trans1 https://github.com/AIPHES/ ACL20-Reference-Free-MT-Evaluation The first remedy is to replace the hard symbolic comparison of natural language “labels” with a soft comparison of texts’ meaning, using semantic vector space representations. Recently, a number of MT evaluation methods appeared focusing on semantic comparison of reference and system translations (Shimanaka et al., 2018; Clark et al., 2019; Zhao et al., 2019). While these correlate better than n-gram overlap metrics with human assessments, they do not address inherent limitations stemming from the need for reference translations, namely: (1) references are expensive to obtain; (2) they assume a single correct solution and bias the evaluation, both automatic and human (Dreyer and Marcu, 2012; Fomicheva and Specia, 2016), and (3) limitation of MT evaluation to language pairs with available parallel data. Reliable reference-free evaluation metrics, directly measuring the (semantic) correspondence between the source language text a"
2020.acl-main.151,2020.acl-main.747,0,0.146174,"Missing"
2020.acl-main.151,N19-1423,0,0.445273,"her non-negligible supervision (i.e., human translation quality labels) (Specia et al., 2010) or language-specific preprocessing like semantic parsing (Lo et al., 2014; Lo, 2019), both hindering the wide applicability of the proposed metrics. Moreover, they have also typically exhibited performance levels well below those of standard reference-based metrics (Ma et al., 2019). In this work, we comparatively evaluate a number of reference-free MT evaluation metrics that build on the most recent developments in multilingual representation learning, namely cross-lingual contextualized embeddings (Devlin et al., 2019) and cross-lingual sentence encoders (Artetxe and Schwenk, 2019). We investigate two types of crosslingual reference-free metrics: (1) Soft token-level alignment metrics find the optimal soft alignment between source sentence and system translation using Word Mover’s Distance (WMD) (Kusner et al., 2015). Zhao et al. (2019) recently demonstrated that WMD operating on BERT representations (Devlin et al., 2019) substantially outperforms baseline MT evaluation metrics in the reference-based setting. In this work, we investigate whether WMD can yield comparable success in the reference-free (i.e.,"
2020.acl-main.151,N12-1017,0,0.0336737,"ge “labels” with a soft comparison of texts’ meaning, using semantic vector space representations. Recently, a number of MT evaluation methods appeared focusing on semantic comparison of reference and system translations (Shimanaka et al., 2018; Clark et al., 2019; Zhao et al., 2019). While these correlate better than n-gram overlap metrics with human assessments, they do not address inherent limitations stemming from the need for reference translations, namely: (1) references are expensive to obtain; (2) they assume a single correct solution and bias the evaluation, both automatic and human (Dreyer and Marcu, 2012; Fomicheva and Specia, 2016), and (3) limitation of MT evaluation to language pairs with available parallel data. Reliable reference-free evaluation metrics, directly measuring the (semantic) correspondence between the source language text and system translation, would remove the need for human references and allow for unlimited MT evaluations: any 1656 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1656–1671 c July 5 - 10, 2020. 2020 Association for Computational Linguistics monolingual corpus could be used for evaluating MT systems. However, t"
2020.acl-main.151,N13-1073,0,0.0603668,"e resulting shared vector space. We investigate two resource-lean choices for the re-mapping function f . Linear Cross-lingual Projection (CLP). Following related work (Schuster et al., 2019), we re-map contextualized embedding spaces using linear projection. Given ` and k, we stack all vectors of the source language words and target language words for pairs D, respectively, to form matrices X` and Xk ∈ Rn×d , with d as the embedding dimension and n as the number of word or sentence alignments. The word pairs we use to calibrate MBERT are extracted from EuroParl (Koehn, 2005) using FastAlign (Dyer et al., 2013), and the sentence pairs to calibrate LASER are sampled directly from EuroParl.4 Mikolov et al. (2013a) propose to learn a projection matrix W ∈ Rd×d by minimizing the Euclidean distance beetween the projected source language vectors and their corresponding target language vectors: min kW X` − Xk k2 . W Xing et al. (2015) achieve further improvement on the task of bilingual lexicon induction (BLI) by constraining W to an orthogonal matrix, i.e., such that W |W = I. This turns the optimization into the well-known Procrustes problem (Sch¨onemann, 1966) with the following closed-form solution: ˆ"
2020.acl-main.151,P16-2013,0,0.0239696,"comparison of texts’ meaning, using semantic vector space representations. Recently, a number of MT evaluation methods appeared focusing on semantic comparison of reference and system translations (Shimanaka et al., 2018; Clark et al., 2019; Zhao et al., 2019). While these correlate better than n-gram overlap metrics with human assessments, they do not address inherent limitations stemming from the need for reference translations, namely: (1) references are expensive to obtain; (2) they assume a single correct solution and bias the evaluation, both automatic and human (Dreyer and Marcu, 2012; Fomicheva and Specia, 2016), and (3) limitation of MT evaluation to language pairs with available parallel data. Reliable reference-free evaluation metrics, directly measuring the (semantic) correspondence between the source language text and system translation, would remove the need for human references and allow for unlimited MT evaluations: any 1656 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1656–1671 c July 5 - 10, 2020. 2020 Association for Computational Linguistics monolingual corpus could be used for evaluating MT systems. However, the proposals of referencefree"
2020.acl-main.151,P19-1070,1,0.907836,"Missing"
2020.acl-main.151,2020.acl-main.675,1,0.843387,"Missing"
2020.acl-main.151,W19-5357,0,0.0277596,"dedicated to the study of automatic evaluation metrics for machine translation. Here, we provide an overview of both reference-based MT evaluation metrics and recent research efforts towards reference-free MT evaluation, which leverage cross-lingual semantic representations and unsupervised MT techniques. Reference-based MT evaluation. Most of the commonly used evaluation metrics in MT compare system and reference translations. They are often based on surface forms such as n-gram overlaps like BLEU (Papineni et al., 2002), SentBLEU, NIST (Doddington, 2002), chrF++ (Popovi´c, 2017) or METEOR++(Guo and Hu, 2019). They have been extensively tested and compared in recent WMT metrics shared tasks (Bojar et al., 2017a; Ma et al., 2018a, 2019). These metrics, however, operate at the surface level, and by design fail to recognize semantic equivalence lacking lexical overlap. To overcome these limitations, some research efforts exploited static word embeddings (Mikolov et al., 2013b) and trained embedding-based supervised metrics on sufficiently large datasets with available human judgments of translation quality (Shimanaka 1657 et al., 2018). With the development of contextual word embeddings (Peters et al"
2020.acl-main.151,D18-1330,0,0.028184,"Missing"
2020.acl-main.151,C12-1089,0,0.0565539,"r instance, Yankovskaya et al. (2019) propose to train a metric combining multilingual embeddings extracted from M-BERT and LASER (Artetxe and Schwenk, 2019) together with the log-probability scores from neural machine translation. Our work differs from that of Yankovskaya et al. (2019) in one crucial aspect: the cross-lingual reference-free metrics that we investigate and benchmark do not require any human supervision. Cross-lingual Representations. Cross-lingual text representations offer a prospect of modeling meaning across languages and support crosslingual transfer for downstream tasks (Klementiev et al., 2012; R¨uckl´e et al., 2018; Glavaˇs et al., 2019; Josifoski et al., 2019; Conneau et al., 2020). Most recently, the (massively) multilingual encoders, such as multilingual M-BERT (Devlin et al., 2019), XLM-on-RoBERTa (Conneau et al., 2020), and (sentence-based) LASER, have profiled themselves as state-of-the-art solutions for (massively) multilingual semantic encoding of text. While LASER has been jointly trained on parallel data of 93 languages, M-BERT has been trained on the concatenation of monolingual data in more than 100 languages, without any cross-lingual mapping signal. There has been a"
2020.acl-main.151,2005.mtsummit-papers.11,0,0.0425747,"(wk ) are better aligned in the resulting shared vector space. We investigate two resource-lean choices for the re-mapping function f . Linear Cross-lingual Projection (CLP). Following related work (Schuster et al., 2019), we re-map contextualized embedding spaces using linear projection. Given ` and k, we stack all vectors of the source language words and target language words for pairs D, respectively, to form matrices X` and Xk ∈ Rn×d , with d as the embedding dimension and n as the number of word or sentence alignments. The word pairs we use to calibrate MBERT are extracted from EuroParl (Koehn, 2005) using FastAlign (Dyer et al., 2013), and the sentence pairs to calibrate LASER are sampled directly from EuroParl.4 Mikolov et al. (2013a) propose to learn a projection matrix W ∈ Rd×d by minimizing the Euclidean distance beetween the projected source language vectors and their corresponding target language vectors: min kW X` − Xk k2 . W Xing et al. (2015) achieve further improvement on the task of bilingual lexicon induction (BLI) by constraining W to an orthogonal matrix, i.e., such that W |W = I. This turns the optimization into the well-known Procrustes problem (Sch¨onemann, 1966) with th"
2020.acl-main.151,P07-2045,0,0.00620842,"cs), Latvian (lv), Finnish (fi), Russian (ru), and Turkish (tr), Gujarati (gu), Kazakh (kk), Lithuanian (lt) and Estonian (et). Each language pair in WMT17-19 has approximately 3,000 source sentences, each associated to one reference translation and to the automatic translations generated by participating systems. 4.2 Baselines We compare with a range of reference-free metrics: ibm1-morpheme and ibm1-pos4gram (Popovi´c, 2012), LASIM (Yankovskaya et al., 2019), LP (Yankovskaya et al., 2019), YiSi-2 and YiSi-2-srl (Lo, 2019), and reference-based baselines BLEU (Papineni et al., 2002), SentBLEU (Koehn et al., 2007) and ChrF++ (Popovi´c, 2017) for MT evaluation (see §2).6 The main results are reported on WMT17. We report the results obtained on WMT18 and WMT19 in the Appendix. 6 The code of these unsupervised metrics is not released, thus we compare to their official results on WMT19 only. 1660 Setting Metrics cs-en de-en fi-en lv-en ru-en tr-en zh-en Average m(y∗ , y) SENT BLEU CHR F++ 43.5 52.3 43.2 53.4 57.1 39.3 67.8 52.0 48.4 53.8 58.8 61.4 51.2 59.3 48.1 57.9 22.7 32.6 37.1 40.2 34.8 26.0 41.4 48.3 26.7 42.5 36.3 42.3 48.2 46.7 34.0 41.1 40.5 28.1 42.0 48.6 45.5 48.5 36.0 44.7 31.3 46.2 42.2 49.4 4"
2020.acl-main.151,W07-0734,0,0.151943,"i-mannheim.de, yang.gao@rhul.ac.uk {maxime.peyrard,robert.west}@epfl.ch Abstract lation (MT) (Bahdanau et al., 2015; Johnson et al., 2017) and text summarization (Rush et al., 2015; Tan et al., 2017), where we do not predict a single discrete label but generate natural language text. Thus, the set of labels for NLG is neither clearly defined nor finite. Yet, the standard evaluation protocols for NLG still predominantly follow the described default paradigm: (1) evaluation datasets come with human-created reference texts and (2) evaluation metrics, e.g., BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007) for MT and ROUGE (Lin and Hovy, 2003) for summarization, count the exact “label” (i.e., n-gram) matches between reference and system-generated text. In other words, established NLG evaluation compares semantically ambiguous labels from an unbounded set (i.e., natural language texts) via hard symbolic matching (i.e., string overlap). Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation ("
2020.acl-main.151,N03-1020,0,0.632893,"Missing"
2020.acl-main.151,W19-5358,0,0.464572,"remove the need for human references and allow for unlimited MT evaluations: any 1656 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1656–1671 c July 5 - 10, 2020. 2020 Association for Computational Linguistics monolingual corpus could be used for evaluating MT systems. However, the proposals of referencefree MT evaluation metrics have been few and far apart and have required either non-negligible supervision (i.e., human translation quality labels) (Specia et al., 2010) or language-specific preprocessing like semantic parsing (Lo et al., 2014; Lo, 2019), both hindering the wide applicability of the proposed metrics. Moreover, they have also typically exhibited performance levels well below those of standard reference-based metrics (Ma et al., 2019). In this work, we comparatively evaluate a number of reference-free MT evaluation metrics that build on the most recent developments in multilingual representation learning, namely cross-lingual contextualized embeddings (Devlin et al., 2019) and cross-lingual sentence encoders (Artetxe and Schwenk, 2019). We investigate two types of crosslingual reference-free metrics: (1) Soft token-level alignm"
2020.acl-main.151,P14-2124,0,0.184908,"ranslation, would remove the need for human references and allow for unlimited MT evaluations: any 1656 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1656–1671 c July 5 - 10, 2020. 2020 Association for Computational Linguistics monolingual corpus could be used for evaluating MT systems. However, the proposals of referencefree MT evaluation metrics have been few and far apart and have required either non-negligible supervision (i.e., human translation quality labels) (Specia et al., 2010) or language-specific preprocessing like semantic parsing (Lo et al., 2014; Lo, 2019), both hindering the wide applicability of the proposed metrics. Moreover, they have also typically exhibited performance levels well below those of standard reference-based metrics (Ma et al., 2019). In this work, we comparatively evaluate a number of reference-free MT evaluation metrics that build on the most recent developments in multilingual representation learning, namely cross-lingual contextualized embeddings (Devlin et al., 2019) and cross-lingual sentence encoders (Artetxe and Schwenk, 2019). We investigate two types of crosslingual reference-free metrics: (1) Soft token-l"
2020.acl-main.151,W18-6450,0,0.165948,"nce-based MT evaluation metrics and recent research efforts towards reference-free MT evaluation, which leverage cross-lingual semantic representations and unsupervised MT techniques. Reference-based MT evaluation. Most of the commonly used evaluation metrics in MT compare system and reference translations. They are often based on surface forms such as n-gram overlaps like BLEU (Papineni et al., 2002), SentBLEU, NIST (Doddington, 2002), chrF++ (Popovi´c, 2017) or METEOR++(Guo and Hu, 2019). They have been extensively tested and compared in recent WMT metrics shared tasks (Bojar et al., 2017a; Ma et al., 2018a, 2019). These metrics, however, operate at the surface level, and by design fail to recognize semantic equivalence lacking lexical overlap. To overcome these limitations, some research efforts exploited static word embeddings (Mikolov et al., 2013b) and trained embedding-based supervised metrics on sufficiently large datasets with available human judgments of translation quality (Shimanaka 1657 et al., 2018). With the development of contextual word embeddings (Peters et al., 2018; Devlin et al., 2019), we have witnessed proposals of semantic metrics that account for word order. For example,"
2020.acl-main.151,W19-5302,0,0.312607,"c July 5 - 10, 2020. 2020 Association for Computational Linguistics monolingual corpus could be used for evaluating MT systems. However, the proposals of referencefree MT evaluation metrics have been few and far apart and have required either non-negligible supervision (i.e., human translation quality labels) (Specia et al., 2010) or language-specific preprocessing like semantic parsing (Lo et al., 2014; Lo, 2019), both hindering the wide applicability of the proposed metrics. Moreover, they have also typically exhibited performance levels well below those of standard reference-based metrics (Ma et al., 2019). In this work, we comparatively evaluate a number of reference-free MT evaluation metrics that build on the most recent developments in multilingual representation learning, namely cross-lingual contextualized embeddings (Devlin et al., 2019) and cross-lingual sentence encoders (Artetxe and Schwenk, 2019). We investigate two types of crosslingual reference-free metrics: (1) Soft token-level alignment metrics find the optimal soft alignment between source sentence and system translation using Word Mover’s Distance (WMD) (Kusner et al., 2015). Zhao et al. (2019) recently demonstrated that WMD o"
2020.acl-main.151,N18-1202,0,0.0262973,"nd Hu, 2019). They have been extensively tested and compared in recent WMT metrics shared tasks (Bojar et al., 2017a; Ma et al., 2018a, 2019). These metrics, however, operate at the surface level, and by design fail to recognize semantic equivalence lacking lexical overlap. To overcome these limitations, some research efforts exploited static word embeddings (Mikolov et al., 2013b) and trained embedding-based supervised metrics on sufficiently large datasets with available human judgments of translation quality (Shimanaka 1657 et al., 2018). With the development of contextual word embeddings (Peters et al., 2018; Devlin et al., 2019), we have witnessed proposals of semantic metrics that account for word order. For example, Clark et al. (2019) introduce a semantic metric relying on sentence mover’s similarity and the contextualized ELMo embeddings (Peters et al., 2018). Similarly, Zhang et al. (2019) describe a reference-based semantic similarity metric based on contextualized BERT representations (Devlin et al., 2019). Zhao et al. (2019) generalize this line of work with their MoverScore metric, which computes the mover’s distance, i.e., the optimal soft alignment between tokens of the two sentences,"
2020.acl-main.151,N19-1162,0,0.031059,"ntence translations.3 To this end, we apply two simple, weakly-supervised linear projection methods for post-hoc improvement of the cross-lingual alignments in these multilingual representation spaces. Notation. Let D = {(w`1 , wk1 ), . . . , (w`n , wkn )} be a set of matched word or sentence pairs from two different languages ` and k. We define a remapping function f such that any f (E(w` )) and E(wk ) are better aligned in the resulting shared vector space. We investigate two resource-lean choices for the re-mapping function f . Linear Cross-lingual Projection (CLP). Following related work (Schuster et al., 2019), we re-map contextualized embedding spaces using linear projection. Given ` and k, we stack all vectors of the source language words and target language words for pairs D, respectively, to form matrices X` and Xk ∈ Rn×d , with d as the embedding dimension and n as the number of word or sentence alignments. The word pairs we use to calibrate MBERT are extracted from EuroParl (Koehn, 2005) using FastAlign (Dyer et al., 2013), and the sentence pairs to calibrate LASER are sampled directly from EuroParl.4 Mikolov et al. (2013a) propose to learn a projection matrix W ∈ Rd×d by minimizing the Eucli"
2020.acl-main.151,P19-1493,0,0.0396949,"019; Conneau et al., 2020). Most recently, the (massively) multilingual encoders, such as multilingual M-BERT (Devlin et al., 2019), XLM-on-RoBERTa (Conneau et al., 2020), and (sentence-based) LASER, have profiled themselves as state-of-the-art solutions for (massively) multilingual semantic encoding of text. While LASER has been jointly trained on parallel data of 93 languages, M-BERT has been trained on the concatenation of monolingual data in more than 100 languages, without any cross-lingual mapping signal. There has been a recent vivid discussion on the cross-lingual abilities of M-BERT (Pires et al., 2019; K et al., 2020; Cao et al., 2020). In particular, Cao et al. (2020) show that M-BERT often yields disparate vector space representations for mutual translations and propose a multilingual remapping based on parallel corpora, to remedy for this issue. In this work, we introduce re-mapping solutions that are resource-leaner and require easyto-obtain limited-size word translation dictionaries rather than large parallel corpora. 3 Reference-Free MT Evaluation Metrics In the following, we use x to denote a source sentence (i.e., a sequence of tokens in the source language), y to denote a system t"
2020.acl-main.151,W18-6456,0,0.0184043,"etup assumes that the task has clearly defined and unambiguous labels and, in most cases, that an instance can be assigned few labels. These assumptions, however, do not hold for natural language generation (NLG) tasks like machine trans1 https://github.com/AIPHES/ ACL20-Reference-Free-MT-Evaluation The first remedy is to replace the hard symbolic comparison of natural language “labels” with a soft comparison of texts’ meaning, using semantic vector space representations. Recently, a number of MT evaluation methods appeared focusing on semantic comparison of reference and system translations (Shimanaka et al., 2018; Clark et al., 2019; Zhao et al., 2019). While these correlate better than n-gram overlap metrics with human assessments, they do not address inherent limitations stemming from the need for reference translations, namely: (1) references are expensive to obtain; (2) they assume a single correct solution and bias the evaluation, both automatic and human (Dreyer and Marcu, 2012; Fomicheva and Specia, 2016), and (3) limitation of MT evaluation to language pairs with available parallel data. Reliable reference-free evaluation metrics, directly measuring the (semantic) correspondence between the so"
2020.acl-main.151,W12-3116,0,0.0409835,"Missing"
2020.acl-main.151,P18-1072,0,0.0464211,"Missing"
2020.acl-main.151,W17-4770,0,0.0274625,"Missing"
2020.acl-main.151,P13-4014,0,0.0327234,"Missing"
2020.acl-main.151,P17-1049,0,0.0342672,"temlevel human judgments that we use for evaluating the quality of our reference-free metrics. The segment-level judgments assign one direct assessment (DA) score to each pair of system and human translation, while system-level judgments associate each system with a single DA score averaged across all pairs in the dataset. We initially suspected the DA scores to be biased for our setup—which compares x with y—as they are based on comparing y? and y. Indeed, it is known that (especially) human professional translators “improve” y? , e.g., by making it more readable, relative to the original x (Rabinovich et al., 2017). We investigated the validity of DA scores by collecting human assessments in the cross-lingual settings (CLDA), where annotators directly compare source and translation pairs (x, y) from the WMT17 dataset. This small-scale manual analysis hints that DA scores are a valid proxy for CLDA. Therefore, we decided to treat them as reliable scores for our setup and evaluate our proposed metrics by comparing their correlation with DA scores. 6 Conclusion Existing semantically-motivated metrics for reference-free evaluation of MT systems have so far displayed rather poor correlation with human estima"
2020.acl-main.151,P02-1040,0,\N,Missing
2020.acl-main.151,W11-2109,0,\N,Missing
2020.acl-main.151,N15-1104,0,\N,Missing
2020.acl-main.151,S16-1081,0,\N,Missing
2020.acl-main.151,P17-1108,0,\N,Missing
2020.acl-main.151,P19-1269,0,\N,Missing
2020.acl-main.151,D19-1449,1,\N,Missing
2020.acl-main.151,D19-1053,1,\N,Missing
2020.acl-main.151,W19-5410,0,\N,Missing
2020.acl-main.151,2020.emnlp-main.215,0,\N,Missing
2020.acl-main.618,P15-1165,0,0.0732183,"Missing"
2020.acl-main.618,P18-1072,1,0.867314,"Missing"
2020.acl-main.618,D19-1449,1,0.869192,"Missing"
2020.acl-main.675,P15-1165,0,0.0243921,"Missing"
2020.acl-main.675,P18-1072,1,0.9198,"Missing"
2020.acl-main.675,P11-2084,1,0.842277,"Missing"
2020.acl-main.675,D19-1449,1,0.902946,"Missing"
2020.acl-main.675,P16-1024,1,0.782186,"Missing"
2020.acl-main.675,2020.acl-main.536,0,0.0902746,"e., languages with nonisomorphic monolingual spaces). 1 Introduction and Motivation Induction of cross-lingual word embeddings (CLWEs) (Vuli´c et al., 2011; Mikolov et al., 2013; Xing et al., 2015; Smith et al., 2017; Artetxe et al., 2018) has been one of the key mechanisms for enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks. Even though CLWEs are recently being contested in cross-lingual downstream transfer by pretrained multilingual language models (Pires et al., 2019; Conneau et al., 2020; Artetxe et al., 2019; Wu and Dredze, 2019; Wu et al., 2020), they are still paramount in word-level translation, that is, bilingual lexicon induction (BLI). While earlier work focused on joint induction of multilingual embeddings from multilingual corpora, relying on word- (Klementiev et al., 2012; Koˇcisk`y et al., 2014; Gouws and Søgaard, 2015), sentence- (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015; Coulmance et al., 2015; Levy et al., 2017), or document-level (Søgaard et al., 2015; Mogadala and Rettinger, 2016; Vuli´c and Moens, 2016) alignments, most recent efforts focus on post-hoc alignment of independently trained monolingu"
2020.acl-main.675,D19-1077,0,0.0182767,"nt language pairs (i.e., languages with nonisomorphic monolingual spaces). 1 Introduction and Motivation Induction of cross-lingual word embeddings (CLWEs) (Vuli´c et al., 2011; Mikolov et al., 2013; Xing et al., 2015; Smith et al., 2017; Artetxe et al., 2018) has been one of the key mechanisms for enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks. Even though CLWEs are recently being contested in cross-lingual downstream transfer by pretrained multilingual language models (Pires et al., 2019; Conneau et al., 2020; Artetxe et al., 2019; Wu and Dredze, 2019; Wu et al., 2020), they are still paramount in word-level translation, that is, bilingual lexicon induction (BLI). While earlier work focused on joint induction of multilingual embeddings from multilingual corpora, relying on word- (Klementiev et al., 2012; Koˇcisk`y et al., 2014; Gouws and Søgaard, 2015), sentence- (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015; Coulmance et al., 2015; Levy et al., 2017), or document-level (Søgaard et al., 2015; Mogadala and Rettinger, 2016; Vuli´c and Moens, 2016) alignments, most recent efforts focus on post-hoc alignment of independently"
2020.acl-main.675,N15-1104,0,0.15939,"Missing"
2020.acl-main.675,C00-2137,0,0.110738,"t groups of language pairs identifies I NSTA M AP as particularly beneficial for pairs of distant languages (setups No-EN and HARD) and languages with least reliable monolingual vectors (TR, HR). For example, 3 Competing models – VecMap, RCSLS, and BLISS – all come with much larger sets of hyperparameters. 4 For some pairs other configurations yield slightly better results: for simplicity, we report the results with base configuration K = 70; T = 4 for all pairs. 5 We provide detailed results for each of 28 language language pairs in the supplemental material. 6 Non-parametric shuffling test (Yeh, 2000) with the Bonferroni correction: α &lt; 0.05 in comparison with VecMap and α &lt; 0.01 in comparison with other models. while I NSTA M AP alone and IM ◦ VM yield gains of 0.9 and 2.6 points, respectively, w.r.t. VecMap across ALL language pairs, these gaps widen to 1.5 and 3.5 points on most challenging language pairs (HARD). In contrast, BLISS, a model specifically tailored to improve the mappings between non-isomorphic spaces, appears to be robust only on pairs of close languages (e.g., HR-RU) and pairs involving EN (setup EN-*). It exhibits barely any improvement over the baseline orthogonal proj"
2020.coling-main.118,2020.acl-srw.36,0,0.0330834,"Missing"
2020.coling-main.118,Q17-1010,0,0.34049,"r).1 We transform the constraints from C into a BERT-compatible input format and feed them as additional training examples for the model. The encoding of a constraint is then forwarded to the relation classifier, which predicts whether the input word pair represents a valid lexical relation. From Linguistic Constraints to Training Instances. We start from a set of linguistic constraints C = {(w1 , w2 )i }Ni=1 and an auxiliary static word embedding space Xaux ∈ Rd . The space Xaux can be obtained via any standard static word embedding model such as Skip-Gram (Mikolov et al., 2013) or fastText (Bojanowski et al., 2017) (used in this work). Each constraint c = (w1 , w2 ) corresponds to a true/positive relation of semantic similarity, and thus represents a positive training example for the model (label 1). For each positive example c, we create corresponding negative examples following prior work on specialization of static embeddings (Wieting et al., 2015; Glavaˇs and Vuli´c, 2018; Ponti et al., 2019). We first group positive constraints from C into mini-batches B p of size k. For each positive example c = (w1 , w2 ), we create two negatives cˆ1 = (wˆ 1 , w2 ) and cˆ2 = (w1 , wˆ 2 ) such that wˆ 1 is the wor"
2020.coling-main.118,S17-2001,0,0.0785084,"Missing"
2020.coling-main.118,2020.acl-main.747,0,0.0992258,"Missing"
2020.coling-main.118,N19-1423,0,0.606051,"arity, yields better performance than the lexically blind “vanilla” BERT on several language understanding tasks. Concretely, LIBERT outperforms BERT in 9 out of 10 tasks of the GLUE benchmark and is on a par with BERT in the remaining one. Moreover, we show consistent gains on 3 benchmarks for lexical simplification, a task where knowledge about word-level semantic similarity is paramount, as well as large gains on lexical reasoning probes. 1 Introduction Unsupervised pretraining models, such as GPT and GPT-2 (Radford et al., 2018; Radford et al., 2019), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019) yield state-of-the-art performance on a wide range of natural language processing tasks. All these models rely on language modeling (LM) objectives that exploit the knowledge encoded in large text corpora. BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP). Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences. While several concurrent research threads are focused on makin"
2020.coling-main.118,I05-5002,0,0.0570753,"Missing"
2020.coling-main.118,N15-1184,0,0.133257,"Missing"
2020.coling-main.118,P15-2011,1,0.904732,"Missing"
2020.coling-main.118,P18-1004,1,0.919583,"Missing"
2020.coling-main.118,P19-1476,1,0.878044,"Missing"
2020.coling-main.118,J15-4004,1,0.777108,"tances from a batch B p of k positive training instances. Next, we transform each instance (i.e., a pair of words) into a “BERT-compatible” format, i.e., into a sequence of WordPiece (Wu et al., 2016) tokens.2 We split both w1 and w2 into WordPiece tokens, insert the special separator token (with a randomly initialized embedding) before and after the tokens of w2 and prepend the whole sequence with BERT’s sequence start token, as shown in this example for the constraint (mended, regenerated):3 1 As the goal is to inform the BERT model on the relation of true semantic similarity between words (Hill et al., 2015), according to prior work on static word embeddings, the sets of both synonym pairs and direct hyponym-hypernym pairs are useful to boost the model’s ability to capture true semantic similarity, which in turn has a positive effect on downstream language understanding applications. See the work of Hill et al. (2015) and Vuli´c (2018) for further details regarding the relationship between direct hyponym-hypernym pairs and true semantic similarity. 2 We use the same 30K WordPiece vocabulary as Devlin et al. (2019). Sharing WordPieces helps our word-level task as lexico-semantic relationships are"
2020.coling-main.118,P14-2075,0,0.0658816,"emantic similarity between the fastText vectors (Bojanowski et al., 2017) of the original word w and the candidate ci , and (4) word frequency of ci in the top 12 million texts of Wikipedia and in the Children’s Book Test corpus.8 Based on the individual features, we next rank the candidates in C and consequently, obtain a set of ranks for each ci . The best candidate is chosen according to its average rank across all features. In our experiments, we fix the number of candidates k to 6. Evaluation Data. We run the evaluation on three standard datasets for lexical simplification: (1) LexMTurk (Horn et al., 2014). The dataset consists of 500 English instances, which are collected from Wikipedia. The complex word and the simpler substitutions were annotated by 50 crowd workers on Amazon Mechanical Turk. (2) BenchLS (Paetzold and Specia, 2016) is a merge of LexMTurk and LSeval (De Belder and Moens, 2010) containing 929 sentences. The latter dataset focuses on text simplification for children. The authors of BenchLS applied additional corrections over the instances of the two datasets. (3) NNSeval (Paetzold and Specia, 2017) is an English dataset focused on text simplification for non-native speakers and"
2020.coling-main.118,W18-3003,0,0.0138404,"with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external constraints, but rather the entire embedding space. In explicit retrofitting models (Glavaˇs and Vuli´c, 2018; Glavaˇs and Vuli´c, 2019), a (deep, non-linear) specialization function is directly learned from external constraints. Post-specialization models (Vuli´c et al., 2018; Ponti et al., 2018; Kamath et al., 2019; Biesialska et al., 2020), instead, propagate lexico-semantic information to unseen words by imitating the transformation undergone by seen words dur"
2020.coling-main.118,W19-4310,1,0.883945,"Missing"
2020.coling-main.118,D15-1242,0,0.0229554,"ely researched problem. For instance, clearly discerning between true/pure semantic similarity and broader conceptual relatedness in static embeddings benefits a range of natural language understanding tasks such as dialog state tracking (Mrkˇsi´c et al., 2017), text simplification (Glavaˇs and Vuli´c, 2018), and spoken language understanding (Kim et al., 2016). The most widespread solution relies on the use of specialization algorithms to enrich word embeddings with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external co"
2020.coling-main.118,2020.deelio-1.5,1,0.878646,"Missing"
2020.coling-main.118,C18-1205,0,0.0281817,"al knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external constraints, but rather the entire embedding space. In explicit retrofitting models (Glavaˇs and Vuli´c, 2018; Glavaˇs and Vuli´c, 2019), a (deep, non-linear) specialization function is directly learned from external constraints. Post-specialization models (Vuli´c et al., 2018; Ponti et al., 2018; Kamath et al., 2019; Biesialska et al., 2020), instead, propagate lexico-semantic information to unseen words by imitating the transformation undergone by seen words during the initial specializ"
2020.coling-main.118,P15-1145,0,0.0248839,"em. For instance, clearly discerning between true/pure semantic similarity and broader conceptual relatedness in static embeddings benefits a range of natural language understanding tasks such as dialog state tracking (Mrkˇsi´c et al., 2017), text simplification (Glavaˇs and Vuli´c, 2018), and spoken language understanding (Kim et al., 2016). The most widespread solution relies on the use of specialization algorithms to enrich word embeddings with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external constraints, but rat"
2020.coling-main.118,N16-1018,0,0.0427133,"Missing"
2020.coling-main.118,Q17-1022,1,0.931397,"Missing"
2020.coling-main.118,Q16-1030,0,0.0174702,"clearly discerning between true/pure semantic similarity and broader conceptual relatedness in static embeddings benefits a range of natural language understanding tasks such as dialog state tracking (Mrkˇsi´c et al., 2017), text simplification (Glavaˇs and Vuli´c, 2018), and spoken language understanding (Kim et al., 2016). The most widespread solution relies on the use of specialization algorithms to enrich word embeddings with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external constraints, but rather the entire embeddi"
2020.coling-main.118,L16-1491,0,0.0237812,"pus.8 Based on the individual features, we next rank the candidates in C and consequently, obtain a set of ranks for each ci . The best candidate is chosen according to its average rank across all features. In our experiments, we fix the number of candidates k to 6. Evaluation Data. We run the evaluation on three standard datasets for lexical simplification: (1) LexMTurk (Horn et al., 2014). The dataset consists of 500 English instances, which are collected from Wikipedia. The complex word and the simpler substitutions were annotated by 50 crowd workers on Amazon Mechanical Turk. (2) BenchLS (Paetzold and Specia, 2016) is a merge of LexMTurk and LSeval (De Belder and Moens, 2010) containing 929 sentences. The latter dataset focuses on text simplification for children. The authors of BenchLS applied additional corrections over the instances of the two datasets. (3) NNSeval (Paetzold and Specia, 2017) is an English dataset focused on text simplification for non-native speakers and consists in total of 239 instances. Similar to BenchLS, the dataset is based on LexMTurk, but filtered for a) instances that contain a complex target word for non-native speakers, and b) simplification candidates that were found to"
2020.coling-main.118,N18-1202,0,0.0455618,"or the word-level semantic similarity, yields better performance than the lexically blind “vanilla” BERT on several language understanding tasks. Concretely, LIBERT outperforms BERT in 9 out of 10 tasks of the GLUE benchmark and is on a par with BERT in the remaining one. Moreover, we show consistent gains on 3 benchmarks for lexical simplification, a task where knowledge about word-level semantic similarity is paramount, as well as large gains on lexical reasoning probes. 1 Introduction Unsupervised pretraining models, such as GPT and GPT-2 (Radford et al., 2018; Radford et al., 2019), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019) yield state-of-the-art performance on a wide range of natural language processing tasks. All these models rely on language modeling (LM) objectives that exploit the knowledge encoded in large text corpora. BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP). Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences. While several concurrent resea"
2020.coling-main.118,D19-1005,0,0.0962985,"e text corpora. BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP). Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences. While several concurrent research threads are focused on making BERT optimization more robust (Liu et al., 2019) or on imprinting external world knowledge on its representations (Sun et al., 2019; Zhang et al., 2019; Sun et al., 2020; Liu et al., 2020; Peters et al., 2019; Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broader topical relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015; Wietin"
2020.coling-main.118,D19-1250,0,0.0534456,"Missing"
2020.coling-main.118,2020.emnlp-main.617,1,0.883586,"Missing"
2020.coling-main.118,D18-1026,1,0.91771,"Missing"
2020.coling-main.118,D19-1226,1,0.894989,"Missing"
2020.coling-main.118,D16-1264,0,0.10254,"Missing"
2020.coling-main.118,D18-1299,0,0.0170595,"n the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018, inter alia) in order to emphasize a particular lexical relation in a specialized embedding space. For instance, lexically informed word vectors specialized for pure semantic similarity result in substantial gains in a number of downstream tasks where such similarity plays an important role, e.g., in dialog state tracking (Mrkˇsi´c et al., 2017; Ren et al., 2018) or for lexical This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1371 Proceedings of the 28th International Conference on Computational Linguistics, pages 1371–1383 Barcelona, Spain (Online), December 8-13, 2020 simplification (Glavaˇs and Vuli´c, 2018; Ponti et al., 2019). Existing specialization methods are, however, not directly applicable to unsupervised pretraining models because they are either (1) tied to a particular training objective of a static word embedding model, or (2) predicated"
2020.coling-main.118,K15-1026,0,0.0583297,"rch threads are focused on making BERT optimization more robust (Liu et al., 2019) or on imprinting external world knowledge on its representations (Sun et al., 2019; Zhang et al., 2019; Sun et al., 2020; Liu et al., 2020; Peters et al., 2019; Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broader topical relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018, inter alia) in order to emphasize a particular lexical relation in a specialized embedding space. For instance, lexically informed word vectors specialized for pure semantic similarity result in substantial gains in a number of downstream tasks where such similarity plays an important role, e.g., in dialog state tracki"
2020.coling-main.118,D13-1170,0,0.0139938,"Missing"
2020.coling-main.118,N18-1103,1,0.886385,"Missing"
2020.coling-main.118,N18-1048,1,0.919448,"Missing"
2020.coling-main.118,W18-3018,1,0.88785,"Missing"
2020.coling-main.118,W18-5446,0,0.34382,"ances in the training batch: LLRC = − ∑ ln yˆ k · yk . (2) k where y ∈ {[0, 1], [1, 0]} is the true relation label for a word-pair training instance. 4 Language Understanding Evaluation To isolate the effects of injecting linguistic knowledge into BERT, we train base BERT and LIBERT in the same setting: the only difference is that we additionally update the parameters of LIBERT’s Transformer encoder based on the gradients of the LRC loss LLRC from Eq. (2). In the first set of experiments, we probe the usefulness of injecting semantic similarity knowledge on the well-known suite of GLUE tasks (Wang et al., 2018), while we also present the results on lexical simplification, another task that has been shown to benefit from lexico-semantic similarity specialization (Glavaˇs and Vuli´c, 2018), later in §5. 4.1 Experimental Setup Pretraining Data. We minimize BERT’s original objective LMLM + LNSP on training examples coming from English Wikipedia.4 We obtain the set of constraints C for the LLRC term from the body of previous work on semantic specialization of static word embeddings (Zhang et al., 2014; Vuli´c et al., 2018; Ponti et al., 2018). In particular, we collect 1,023,082 synonymy pairs from WordN"
2020.coling-main.118,Q19-1040,0,0.0209636,"lization of static word embeddings (Zhang et al., 2014; Vuli´c et al., 2018; Ponti et al., 2018). In particular, we collect 1,023,082 synonymy pairs from WordNet (Miller, 1995) and Roget’s Thesaurus (Kipfer, 2009) and 326,187 direct hyponym-hypernym pairs (Vuli´c and Mrkˇsi´c, 2018) from WordNet, and use them as positive instances for the binary classifier (LRC).5 Fine-Tuning (Downstream) Tasks. We evaluate BERT and LIBERT on the the following tasks from the GLUE benchmark (Wang et al., 2018), where sizes of training, development, and test datasets for each task are provided in Table 1: CoLA (Warstadt et al., 2019): Binary sentence classification, predicting if sentences from linguistic publications are grammatically acceptable; 4 We acknowledge that training the models on larger corpora would likely lead to better absolute downstream scores; however, the main goal of this work is not to achieve state-of-the-art downstream performance, but to compare the base model against its lexically informed counterpart. 5 Note again that similar to work of Vuli´c (2018), both WordNet synonyms and direct hyponym-hypernym pairs are treated exactly the same: as positive examples for the relation of true semantic simil"
2020.coling-main.118,Q15-1025,0,0.524219,", 2019; Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broader topical relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018, inter alia) in order to emphasize a particular lexical relation in a specialized embedding space. For instance, lexically informed word vectors specialized for pure semantic similarity result in substantial gains in a number of downstream tasks where such similarity plays an important role, e.g., in dialog state tracking (Mrkˇsi´c et al., 2017; Ren et al., 2018) or for lexical This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1371 Proceedings of the 28"
2020.coling-main.118,N18-1101,0,0.139902,"Missing"
2020.coling-main.118,P14-2089,0,0.0618712,"ations is an extensively researched problem. For instance, clearly discerning between true/pure semantic similarity and broader conceptual relatedness in static embeddings benefits a range of natural language understanding tasks such as dialog state tracking (Mrkˇsi´c et al., 2017), text simplification (Glavaˇs and Vuli´c, 2018), and spoken language understanding (Kim et al., 2016). The most widespread solution relies on the use of specialization algorithms to enrich word embeddings with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words foun"
2020.coling-main.118,D14-1161,0,0.0234709,"e probe the usefulness of injecting semantic similarity knowledge on the well-known suite of GLUE tasks (Wang et al., 2018), while we also present the results on lexical simplification, another task that has been shown to benefit from lexico-semantic similarity specialization (Glavaˇs and Vuli´c, 2018), later in §5. 4.1 Experimental Setup Pretraining Data. We minimize BERT’s original objective LMLM + LNSP on training examples coming from English Wikipedia.4 We obtain the set of constraints C for the LLRC term from the body of previous work on semantic specialization of static word embeddings (Zhang et al., 2014; Vuli´c et al., 2018; Ponti et al., 2018). In particular, we collect 1,023,082 synonymy pairs from WordNet (Miller, 1995) and Roget’s Thesaurus (Kipfer, 2009) and 326,187 direct hyponym-hypernym pairs (Vuli´c and Mrkˇsi´c, 2018) from WordNet, and use them as positive instances for the binary classifier (LRC).5 Fine-Tuning (Downstream) Tasks. We evaluate BERT and LIBERT on the the following tasks from the GLUE benchmark (Wang et al., 2018), where sizes of training, development, and test datasets for each task are provided in Table 1: CoLA (Warstadt et al., 2019): Binary sentence classification"
2020.coling-main.118,P19-1139,0,0.144719,"M) objectives that exploit the knowledge encoded in large text corpora. BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP). Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences. While several concurrent research threads are focused on making BERT optimization more robust (Liu et al., 2019) or on imprinting external world knowledge on its representations (Sun et al., 2019; Zhang et al., 2019; Sun et al., 2020; Liu et al., 2020; Peters et al., 2019; Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broader topical relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resource"
2020.coling-main.118,2020.acl-main.201,0,0.015309,"ther the entire embedding space. In explicit retrofitting models (Glavaˇs and Vuli´c, 2018; Glavaˇs and Vuli´c, 2019), a (deep, non-linear) specialization function is directly learned from external constraints. Post-specialization models (Vuli´c et al., 2018; Ponti et al., 2018; Kamath et al., 2019; Biesialska et al., 2020), instead, propagate lexico-semantic information to unseen words by imitating the transformation undergone by seen words during the initial specialization. This family of models can also transfer specialization across languages (Glavaˇs and Vuli´c, 2018; Ponti et al., 2019; Zhang et al., 2020). The goal of this work is to move beyond similarity-based specialization of static word embeddings only. We present a novel methodology for enriching unsupervised pretraining models such as BERT (Devlin et al., 2019) with readily available discrete lexico-semantic knowledge, and measure the benefits of such semantic specialization on similarity-oriented downstream applications. 1372 2.2 Injecting Knowledge into Unsupervised Pretraining Models Unsupervised pretraining models do retain some of the limitations of static word embeddings. First, they still conflate separate lexico-semantic relatio"
2020.coling-main.345,P14-1126,0,0.0570066,"Missing"
2020.coling-main.345,D11-1006,0,0.0370746,"merges parses by all 42 parsers, but uses oracle performance as parser weights; E NS -A LL – ensembles all 42 parsers, with equal weights. An exception is the MSP model which is not an ensemble model, but rather trains a single parser on the concatenation of all training treebanks. Ma & Mi: average performance across 20 languages, macro- and micro-averaged scores, respectively. The best result in each column, not considering oracle scores, is in bold. 6 Related Work Parsing languages with no training data has been a very active topic of research for nearly a decade since the pivotal works by McDonald et al. (2011) and Petrov et al. (2012). Many diverse approaches are explored along the lines of model transfer, annotation projection, machine translation (T¨ackstr¨om et al., 2013; Guo et al., 2015; Zhang and Barzilay, 2015; Tiedemann and Agi´c, 2016; Rasooli and Collins, 2017), and selective sharing based on language typology (Naseem et al., 2012) and structural similarity (Ponti et al., 2018; Meng et al., 2019). However, vast majority of prior work involves bulk evaluation, whereby transfer parsers are validated by mean accuracy on test data. Such evaluation protocols stand in contrast with the fact tha"
2020.coling-main.345,D19-1103,0,0.0113354,"lumn, not considering oracle scores, is in bold. 6 Related Work Parsing languages with no training data has been a very active topic of research for nearly a decade since the pivotal works by McDonald et al. (2011) and Petrov et al. (2012). Many diverse approaches are explored along the lines of model transfer, annotation projection, machine translation (T¨ackstr¨om et al., 2013; Guo et al., 2015; Zhang and Barzilay, 2015; Tiedemann and Agi´c, 2016; Rasooli and Collins, 2017), and selective sharing based on language typology (Naseem et al., 2012) and structural similarity (Ponti et al., 2018; Meng et al., 2019). However, vast majority of prior work involves bulk evaluation, whereby transfer parsers are validated by mean accuracy on test data. Such evaluation protocols stand in contrast with the fact that languages exhibit high variance in syntactic structure, which calls for a sensitive treatment of every sentence. While an oracle single-source parser may be appropriate for the majority of sentences in a given dataset, instance-based treatment closes the gap to the best achievable result given an array of pretrained parsers, as we also show in §2. Early efforts in this line of research include data"
2020.coling-main.345,K19-1029,0,0.0286747,"Missing"
2020.coling-main.345,P12-1066,0,0.0306445,"and micro-averaged scores, respectively. The best result in each column, not considering oracle scores, is in bold. 6 Related Work Parsing languages with no training data has been a very active topic of research for nearly a decade since the pivotal works by McDonald et al. (2011) and Petrov et al. (2012). Many diverse approaches are explored along the lines of model transfer, annotation projection, machine translation (T¨ackstr¨om et al., 2013; Guo et al., 2015; Zhang and Barzilay, 2015; Tiedemann and Agi´c, 2016; Rasooli and Collins, 2017), and selective sharing based on language typology (Naseem et al., 2012) and structural similarity (Ponti et al., 2018; Meng et al., 2019). However, vast majority of prior work involves bulk evaluation, whereby transfer parsers are validated by mean accuracy on test data. Such evaluation protocols stand in contrast with the fact that languages exhibit high variance in syntactic structure, which calls for a sensitive treatment of every sentence. While an oracle single-source parser may be appropriate for the majority of sentences in a given dataset, instance-based treatment closes the gap to the best achievable result given an array of pretrained parsers, as we als"
2020.coling-main.345,petrov-etal-2012-universal,0,0.639447,"2020; Lauscher et al., 2020). Therefore, cross-lingual transfer of dependency parsers has profiled as the most viable strategy to use parsing technology in resource-low languages (McDonald et al., 2011; Søgaard, 2011; Kondratyuk ¨ un et al., 2020). Delexicalized transfer is conceptually the least demanding option and Straka, 2019; Ust¨ in terms of language-specific resource requirements. The only provision, in order to transfer the parser trained on a delexicalized treebank of a resource-rich language, is a POS tagger in a low-resource target language based on the Universal POS (UPOS) tagset (Petrov et al., 2012). Delexicalized transfer is nowadays used primarily as a simple yet competitive baseline for more sophisticated transfer models when porting parsing technology in a new language. However, in realistic truly low-resource setups, one cannot guarantee additional resources such as parallel sentences (Ma and Xia, 2014; Rasooli and Collins, 2015; Rasooli and Collins, 2017; Wang et al., 2019; Zhang et al., 2019), word alignments (Lacroix et al., 2016), sufficiently large monolingual corpus in the target language (Mulcaire et al., 2019), and language coverage This work is licensed under a Creative Com"
2020.coling-main.345,P11-1157,0,0.0929328,"Missing"
2020.coling-main.345,P18-1142,1,0.928731,"Missing"
2020.coling-main.345,J19-3005,1,0.844979,"Missing"
2020.coling-main.345,D15-1039,0,0.0451007,"Missing"
2020.coling-main.345,Q17-1020,0,0.0720177,"ific resource requirements. The only provision, in order to transfer the parser trained on a delexicalized treebank of a resource-rich language, is a POS tagger in a low-resource target language based on the Universal POS (UPOS) tagset (Petrov et al., 2012). Delexicalized transfer is nowadays used primarily as a simple yet competitive baseline for more sophisticated transfer models when porting parsing technology in a new language. However, in realistic truly low-resource setups, one cannot guarantee additional resources such as parallel sentences (Ma and Xia, 2014; Rasooli and Collins, 2015; Rasooli and Collins, 2017; Wang et al., 2019; Zhang et al., 2019), word alignments (Lacroix et al., 2016), sufficiently large monolingual corpus in the target language (Mulcaire et al., 2019), and language coverage This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 3886 Proceedings of the 28th International Conference on Computational Linguistics, pages 3886–3898 Barcelona, Spain (Online), December 8-13, 2020 in massively multilingual language models which are used as the basis for modern parsers (Kondratyuk ¨ un et al.,"
2020.coling-main.345,P15-2040,0,0.277129,"Missing"
2020.coling-main.345,N06-2033,0,0.09635,"determine a threshold τ ∈ [0, 1] that defines the set of “good enough” parsers, in relative terms w.r.t. the performance of the best parser. The sets of parsers whose trees are to be merged are obtained as follows: {iILPS }τ (j) = {i|∀i : yˆi,j ≥ max(ˆ yi,j ) · τ }, (8) {iSBPSILPS }τ = {i |∀i : y¯i ≥ max(¯ yi ) · τ }. (9) where Eq (8) refers to the pure ILPS setting, and Eq. (9) refers to the SBPSILPS setting. 3.4 Reparsing After selecting multiple parsers in the ensemble settings, we need to merge their produced parse trees into a final tree. Such a step is commonly referred to as reparsing (Sagae and Lavie, 2006). Here we resort to a standard reparsing procedure in which we: (1) merge the trees produced by individual parsers into a weighted graph G – the parser i contributes to an edge with the weight wi = yˆi,j (for pure ILPS; for SBPSILPS , wi = y¯i ) if the parser i predicted that edge, and with wi = 0 otherwise; (2) induce the Maximum Spanning Tree (MST) of G (Edmonds, 1967) as the final parse of the input UPOS-sentence (see again Figure 2). 4 Experimental Setup Data. We perform all experiments on the UD v2.3 dataset,7 as it contains a wide array of both resourcerich languages with large treebanks"
2020.coling-main.345,D18-1545,0,0.0710174,"Missing"
2020.coling-main.345,C12-2115,0,0.20997,"ance-based treatment closes the gap to the best achievable result given an array of pretrained parsers, as we also show in §2. Early efforts in this line of research include data point selection where language models are used to capture the prevalent syntactic structure of a language and score potential training instances such that a multi-source parser is trained on the mixture of training instances that are most similar to the test language instances (Søgaard, 2011). Instead of instance selection one can also apply instance reweighting in accordance to their similarity to the test language (Søgaard and Wulff, 2012). Regardless of whether we attempt to align languages on an instance-level or on a treebank-level there is a need for a similarity measure between languages. Prior work relied on existing manually curated resources such as the URIEL ˇ database (Littell et al., 2017), using the KL-Divergence on POS-trigrams (Rosa and Zabokrtsk´ y, 2015), or handcrafted features derived from the datasets at hand. Our work is most similar to the recent work of Lin et al. (2019): they learn to score and rank languages in order to predict the top transfer languages. However, contrary to their work, our approach doe"
2020.coling-main.345,P11-2120,0,0.0388067,"ve treatment of every sentence. While an oracle single-source parser may be appropriate for the majority of sentences in a given dataset, instance-based treatment closes the gap to the best achievable result given an array of pretrained parsers, as we also show in §2. Early efforts in this line of research include data point selection where language models are used to capture the prevalent syntactic structure of a language and score potential training instances such that a multi-source parser is trained on the mixture of training instances that are most similar to the test language instances (Søgaard, 2011). Instead of instance selection one can also apply instance reweighting in accordance to their similarity to the test language (Søgaard and Wulff, 2012). Regardless of whether we attempt to align languages on an instance-level or on a treebank-level there is a need for a similarity measure between languages. Prior work relied on existing manually curated resources such as the URIEL ˇ database (Littell et al., 2017), using the KL-Divergence on POS-trigrams (Rosa and Zabokrtsk´ y, 2015), or handcrafted features derived from the datasets at hand. Our work is most similar to the recent work of Lin"
2020.coling-main.345,N13-1126,0,0.0427885,"Missing"
2020.coling-main.345,2020.emnlp-main.180,0,0.0293142,"Missing"
2020.coling-main.345,D19-1102,0,0.0129897,"¨ un et al., 2020). Thus, delexicalized transfer still remains a widely useful baseline and Straka, 2019; Ust¨ and plausible option (Johannsen et al., 2016; Agi´c, 2017). Cross-lingual transfer comes in two main flavors. We either (1) choose the best parser from a set of available parsers, trained on treebanks of various resource-rich languages (single-best parser selection, SBPS) or (2) use the parser trained on a mixture of treebanks of (ideally related) resource-rich languages (multi-source parser transfer, MSP). Other transfer paradigms, like data augmentation (S¸ahin and Steedman, 2018; Vania et al., 2019), assume the existence of at least a small treebank for a target language, violating the assumption of a (treebank-wise) fully low-resource target language. Both SBPS and MSP rely on some measure of structural alignment between languages in order to select either the single best source language parser (SBPS) or a set of (syntactically related) source languages (MSP). Existing solutions rely on measures like the Kullback–Leibler (KL) divergence between sourceˇ and target-language distributions of POS trigrams (Rosa and Zabokrtsk´ y, 2015), which can be unreliable for small target language corpo"
2020.coling-main.345,Q16-1035,0,0.0200005,"er languages. However, contrary to their work, our approach does not employ a model to learn the ranking, but transforms the labels to directly reflect the ranking when we train the scoring model. In addition, we stress the importance of instance-based learning for cross-lingual parser transfer in particular. Another core difference is that our approach is an end-to-end system without external resources or handcrafted static features. Instead, our framework relies on trainable parser embeddings that encode the necessary features in a single representation. From another viewpoint, the work of (Wang and Eisner, 2016; Wang and Eisner, 2018a; Wang and Eisner, 2018b) explores the potential of synthesizing and reordering delexicalized POS sequences to come up with better parser transfer without unrealistic assumptions on target-language resources. Their work in synthetic delexicalization is compatible with ours as it lends itself entirely to instance-based parsing. Finally, the line of work by Ammar et al. (2016) in learning monolithic models over multiple languages, 3894 and its continuation for zero-shot learning by Kondratyuk and Straka (2019) also promises to abstract away from language boundaries, but s"
2020.coling-main.345,Q18-1046,0,0.0679321,"w-resource languages with small test treebanks. For our experiments, we select 42 languages with the largest treebanks as our resource-rich source languages for training, and a set of 20 typologically diverse low-resource 7 https://universaldependencies.org/ 3891 Figure 3: Performance (UAS) for single-parser se- Figure 4: Performance (UAS) for ensemble (i.e., lection models, micro- and macro- averaged, respec- few-parser selection) models, micro- and macrotively, across 20 test languages. averaged, respectively, across 20 test languages. languages for testing.8 Following established practice (Wang and Eisner, 2018b), at inference we use gold UPOS-tags of test treebanks for all models in comparison.9 ILPS Hyperparameters are optimized via fixed-split cross-validation on our training set (see §3.1). We set the embedding size for both parser embeddings and UPOS-tag embeddings, as well as the hidden size of the feed-forward Transformer layers to 256. The transformer encoder has NT = 3 layers with 8 attention heads in each layer. We update the model in mini-batches of 16 examples, using Adam (Kingma and Ba, 2015) with the default parameters: β1 = 0.9, β2 = 0.999, and  = 10−8 , with an initial learning rate"
2020.coling-main.345,D18-1163,0,0.0518162,"w-resource languages with small test treebanks. For our experiments, we select 42 languages with the largest treebanks as our resource-rich source languages for training, and a set of 20 typologically diverse low-resource 7 https://universaldependencies.org/ 3891 Figure 3: Performance (UAS) for single-parser se- Figure 4: Performance (UAS) for ensemble (i.e., lection models, micro- and macro- averaged, respec- few-parser selection) models, micro- and macrotively, across 20 test languages. averaged, respectively, across 20 test languages. languages for testing.8 Following established practice (Wang and Eisner, 2018b), at inference we use gold UPOS-tags of test treebanks for all models in comparison.9 ILPS Hyperparameters are optimized via fixed-split cross-validation on our training set (see §3.1). We set the embedding size for both parser embeddings and UPOS-tag embeddings, as well as the hidden size of the feed-forward Transformer layers to 256. The transformer encoder has NT = 3 layers with 8 attention heads in each layer. We update the model in mini-batches of 16 examples, using Adam (Kingma and Ba, 2015) with the default parameters: β1 = 0.9, β2 = 0.999, and  = 10−8 , with an initial learning rate"
2020.coling-main.345,D19-1575,0,0.0388791,"Missing"
2020.coling-main.345,D15-1213,0,0.017293,"her trains a single parser on the concatenation of all training treebanks. Ma & Mi: average performance across 20 languages, macro- and micro-averaged scores, respectively. The best result in each column, not considering oracle scores, is in bold. 6 Related Work Parsing languages with no training data has been a very active topic of research for nearly a decade since the pivotal works by McDonald et al. (2011) and Petrov et al. (2012). Many diverse approaches are explored along the lines of model transfer, annotation projection, machine translation (T¨ackstr¨om et al., 2013; Guo et al., 2015; Zhang and Barzilay, 2015; Tiedemann and Agi´c, 2016; Rasooli and Collins, 2017), and selective sharing based on language typology (Naseem et al., 2012) and structural similarity (Ponti et al., 2018; Meng et al., 2019). However, vast majority of prior work involves bulk evaluation, whereby transfer parsers are validated by mean accuracy on test data. Such evaluation protocols stand in contrast with the fact that languages exhibit high variance in syntactic structure, which calls for a sensitive treatment of every sentence. While an oracle single-source parser may be appropriate for the majority of sentences in a given"
2020.coling-main.345,D19-1092,0,0.0217857,"Missing"
2020.coling-main.345,P15-1119,0,\N,Missing
2020.coling-main.345,N16-1121,0,\N,Missing
2020.coling-main.345,P16-2091,1,\N,Missing
2020.coling-main.345,W17-0401,1,\N,Missing
2020.coling-main.345,K17-3002,0,\N,Missing
2020.coling-main.345,E17-2002,0,\N,Missing
2020.coling-main.345,N19-1423,0,\N,Missing
2020.coling-main.559,S19-2007,0,0.127781,"Missing"
2020.coling-main.559,R19-1132,0,0.0175161,"(Zampieri et al., 2020) introduced a multilingual data set for 5 languages (English, Arabic, Danish, Hebrew, Turkish), which was expanded to German and Italian by Casula (2020). The 1 In the actual experiments, we do not assess if the raw text is considered abusive - the criterion for sentence inclusion is that it simply contains at least one cue word that is considered abusive - e.g., stupid, racist, hate, fool, kill, ridiculous. 6351 HatEval shared task (Basile et al., 2019) spans only English and German, and other works (Steinberger et al., 2017; Sohn and Lee, 2019; Ousidhoum et al., 2019; Steimel et al., 2019; Stappen et al., 2020; Corazza et al., 2020, inter alia) similarly target only major European languages such as French, German, Italian, Czech, and Spanish.2 As indicated by Stappen et al. (2020), annotated evaluation data for more diverse and resource-poor languages is a prerequisite to develop portable and widely reachable abusive language detection methodology. With XH ATE -999, we make a step towards reaching out also to such languages. In the cross-lingual settings, Steinberger et al. (2017) train separate detection models for several languages, but link results via named entities and di"
2020.coling-main.559,steinberger-etal-2017-cross,0,0.0242634,"es, realized mostly through shared tasks. The recent OffenseEval task (Zampieri et al., 2020) introduced a multilingual data set for 5 languages (English, Arabic, Danish, Hebrew, Turkish), which was expanded to German and Italian by Casula (2020). The 1 In the actual experiments, we do not assess if the raw text is considered abusive - the criterion for sentence inclusion is that it simply contains at least one cue word that is considered abusive - e.g., stupid, racist, hate, fool, kill, ridiculous. 6351 HatEval shared task (Basile et al., 2019) spans only English and German, and other works (Steinberger et al., 2017; Sohn and Lee, 2019; Ousidhoum et al., 2019; Steimel et al., 2019; Stappen et al., 2020; Corazza et al., 2020, inter alia) similarly target only major European languages such as French, German, Italian, Czech, and Spanish.2 As indicated by Stappen et al. (2020), annotated evaluation data for more diverse and resource-poor languages is a prerequisite to develop portable and widely reachable abusive language detection methodology. With XH ATE -999, we make a step towards reaching out also to such languages. In the cross-lingual settings, Steinberger et al. (2017) train separate detection models"
2020.coling-main.559,K19-1088,0,0.0133012,"who experiment with multi-task learning for domain transfer on three data sets. In a similar vein, Karan and Šnajder (2018) employ frustratingly easy domain adaptation (Daumé III, 2007) to experiment with domain transfer on a wide range of abusive language data sets. Some cross-domain approaches rely on term analysis, e.g., Wiegand et al. (2018a) start from a manually constructed sample of abusive terms and augment it automatically to aid domain adaptation, while Rizoiu et al. (2019) aim to construct task-agnostic representations of abusive language. This stands in contrast with insights from Swamy et al. (2019), which suggest that the high variation in abusive language typically precludes wide generalisations and domain adaptation. The work of Pamungkas and Patti (2019) is closest to ours, as they provide some preliminary experiments on domain transfer across languages, mostly indicating its complexity, key challenges, and usefulness of available abusive language lexicons. However, they focus on readily available and unaligned data sets in major European languages (English, German, Italian, Spanish), do not provide direct comparisons across languages, now enabled by XH ATE -999, and do not investiga"
2020.coling-main.559,R15-1086,0,0.0716638,"Missing"
2020.coling-main.559,D19-1449,1,0.879871,"Missing"
2020.coling-main.559,N16-2013,0,0.0108282,"modeling (MLM) (i.e., the so-called intermediate MLM-ing) on automatically extracted “hateful” raw text in the target languages.1 We show that this additional language and domain adaptation of the base massively multilingual model can yield further performance gains: we obtain higher scores than MLM-ing on randomly sampled raw text of the same size, confirming that both language adaptation and adaptation to abusive language are required to boost transfer performance. 2 Related Work and Motivation Variants of Abusive Language. Abusive language appears in many flavors, including sexism, racism (Waseem and Hovy, 2016; Waseem, 2016), toxicity (Kolhatkar et al., 2019), hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), attack (Wulczyn et al., 2017), cyberbullying (Van Hee et al., 2015; Sprugnoli et al., 2018), misogyny (Fersini et al., 2018), obscenity, threats, and insults. Waseem et al. (2017) proposed a systematic typology of toxic language. Another typology focusing more on the nature of targets of abusive texts was proposed by Zampieri et al. (2019). A similar scheme, expanded to include the personal sentiments of annotators, was introduced by Ousidhoum et al. (2019). A very fine-grain"
2020.coling-main.559,W17-3012,0,0.0548048,"-ing on randomly sampled raw text of the same size, confirming that both language adaptation and adaptation to abusive language are required to boost transfer performance. 2 Related Work and Motivation Variants of Abusive Language. Abusive language appears in many flavors, including sexism, racism (Waseem and Hovy, 2016; Waseem, 2016), toxicity (Kolhatkar et al., 2019), hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), attack (Wulczyn et al., 2017), cyberbullying (Van Hee et al., 2015; Sprugnoli et al., 2018), misogyny (Fersini et al., 2018), obscenity, threats, and insults. Waseem et al. (2017) proposed a systematic typology of toxic language. Another typology focusing more on the nature of targets of abusive texts was proposed by Zampieri et al. (2019). A similar scheme, expanded to include the personal sentiments of annotators, was introduced by Ousidhoum et al. (2019). A very fine-grained hierarchical annotation scheme including 81 different types of annotations was used to label the data set of Fortuna et al. (2019). Furthermore, Founta et al. (2018) propose an iterative crowdsourcing-based approach to derive a set of high-quality abusive language labels. Recently, it has been p"
2020.coling-main.559,W16-5618,0,0.0275167,"he so-called intermediate MLM-ing) on automatically extracted “hateful” raw text in the target languages.1 We show that this additional language and domain adaptation of the base massively multilingual model can yield further performance gains: we obtain higher scores than MLM-ing on randomly sampled raw text of the same size, confirming that both language adaptation and adaptation to abusive language are required to boost transfer performance. 2 Related Work and Motivation Variants of Abusive Language. Abusive language appears in many flavors, including sexism, racism (Waseem and Hovy, 2016; Waseem, 2016), toxicity (Kolhatkar et al., 2019), hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), attack (Wulczyn et al., 2017), cyberbullying (Van Hee et al., 2015; Sprugnoli et al., 2018), misogyny (Fersini et al., 2018), obscenity, threats, and insults. Waseem et al. (2017) proposed a systematic typology of toxic language. Another typology focusing more on the nature of targets of abusive texts was proposed by Zampieri et al. (2019). A similar scheme, expanded to include the personal sentiments of annotators, was introduced by Ousidhoum et al. (2019). A very fine-grained hierarchical"
2020.coling-main.559,2020.semeval-1.213,0,0.0309202,"ant. While such methods cannot completely replace human moderators, they are very helpful as assistance tools, offering moderation suggestions, thus partially automating and expediting human moderation work. The focus of abusive language detection is still predominantly on a single language – English, and single-domain setups (e.g., Twitter). However, some recent initiatives have aspired to broaden the scope of abusive detection methodology to other languages, showcasing the usefulness of cross-lingual transfer for the task (Sohn and Lee, 2019; Stappen et al., 2020; Pamungkas and Patti, 2019; Wiedemann et al., 2020, inter alia). Another line of research (Wiegand et al., 2018a; Karan and Šnajder, 2018; Waseem et al., 2018, inter alia) focuses on benefits of cross-domain transfer in monolingual settings. An interesting aspect, currently lacking in prior work, is the interaction of cross-lingual and cross-domain settings. Furthermore, except for some notable exceptions discussed in §2, previous work in cross-lingual setups is still tied to resource-rich and typologically similar languages (e.g., English, German, Spanish, Italian) (Stappen et al., 2020). We aim to fill both these gaps by introducing XH ATE"
2020.coling-main.559,N18-1095,0,0.232338,"ors, they are very helpful as assistance tools, offering moderation suggestions, thus partially automating and expediting human moderation work. The focus of abusive language detection is still predominantly on a single language – English, and single-domain setups (e.g., Twitter). However, some recent initiatives have aspired to broaden the scope of abusive detection methodology to other languages, showcasing the usefulness of cross-lingual transfer for the task (Sohn and Lee, 2019; Stappen et al., 2020; Pamungkas and Patti, 2019; Wiedemann et al., 2020, inter alia). Another line of research (Wiegand et al., 2018a; Karan and Šnajder, 2018; Waseem et al., 2018, inter alia) focuses on benefits of cross-domain transfer in monolingual settings. An interesting aspect, currently lacking in prior work, is the interaction of cross-lingual and cross-domain settings. Furthermore, except for some notable exceptions discussed in §2, previous work in cross-lingual setups is still tied to resource-rich and typologically similar languages (e.g., English, German, Spanish, Italian) (Stappen et al., 2020). We aim to fill both these gaps by introducing XH ATE -999, a multilingual data set annotated for abusive language"
2020.coling-main.559,N19-1060,0,0.0283851,"similar scheme, expanded to include the personal sentiments of annotators, was introduced by Ousidhoum et al. (2019). A very fine-grained hierarchical annotation scheme including 81 different types of annotations was used to label the data set of Fortuna et al. (2019). Furthermore, Founta et al. (2018) propose an iterative crowdsourcing-based approach to derive a set of high-quality abusive language labels. Recently, it has been pointed out that existing abusive language data sets are biased towards certain types of abuse (Jurgens et al., 2019; Vidgen and Derczynski, 2020) and domains/topics (Wiegand et al., 2019). In this work, we combine three different abusive language variants – hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), and attack (Wulczyn et al., 2017) – spanning three distinct data sources (comments under Fox News stories, Twitter/Facebook posts, and Wikipedia edit messages, respectively) into an integrated and cross-language aligned multilingual evaluation resource. Multilingual and Cross-Lingual Abusive Language Detection. There is a growing body of work on abusive language detection for other languages, realized mostly through shared tasks. The recent OffenseEval task"
2020.coling-main.559,N19-1144,0,0.0358884,"erformance. 2 Related Work and Motivation Variants of Abusive Language. Abusive language appears in many flavors, including sexism, racism (Waseem and Hovy, 2016; Waseem, 2016), toxicity (Kolhatkar et al., 2019), hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), attack (Wulczyn et al., 2017), cyberbullying (Van Hee et al., 2015; Sprugnoli et al., 2018), misogyny (Fersini et al., 2018), obscenity, threats, and insults. Waseem et al. (2017) proposed a systematic typology of toxic language. Another typology focusing more on the nature of targets of abusive texts was proposed by Zampieri et al. (2019). A similar scheme, expanded to include the personal sentiments of annotators, was introduced by Ousidhoum et al. (2019). A very fine-grained hierarchical annotation scheme including 81 different types of annotations was used to label the data set of Fortuna et al. (2019). Furthermore, Founta et al. (2018) propose an iterative crowdsourcing-based approach to derive a set of high-quality abusive language labels. Recently, it has been pointed out that existing abusive language data sets are biased towards certain types of abuse (Jurgens et al., 2019; Vidgen and Derczynski, 2020) and domains/topi"
2020.deelio-1.5,S17-2001,0,0.0777077,"Missing"
2020.deelio-1.5,2021.ccl-1.108,0,0.158187,"Missing"
2020.deelio-1.5,N19-1423,0,0.14102,"ConceptNet and its corresponding Open Mind Common Sense (OMCS) corpus, respectively, using adapter training. While overall results on the GLUE benchmark paint an inconclusive picture, a deeper analysis reveals that our adapter-based models substantially outperform BERT (up to 15-20 performance points) on inference tasks that require the type of conceptual knowledge explicitly present in ConceptNet and OMCS. We also open source all our experiments and relevant code under: https://github.com/ wluper/retrograph. 1 Introduction Self-supervised neural models like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019; Liu et al., 2019b), GPT (Radford et al., 2018, 2019), or XLNet (Yang et al., 2019) have rendered language modeling a very suitable pretraining task for learning language representations that are useful for a wide range of language understanding tasks (Wang et al., 2018, 2019). Although shown versatile w.r.t. the types of knowledge (Rogers et al., 2020) they encode, much like their predecessors – static word embedding models (Mikolov et al., 2013; Pennington et al., 2014) – neural LMs still only “consume” 43 Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Ext"
2020.deelio-1.5,P10-1023,0,0.0240196,"s Anne Lauscher♣ Olga Majewska♠ Leonardo F. R. Ribeiro♦ Iryna Gurevych♦ Nikolai Rozanov♠ Goran Glavaˇs♣ ♣ Data and Web Science Group, University of Mannheim, Germany ♠ Wluper, London, United Kingdom ♦ Ubiquitous Knowledge Processing (UKP) Lab, TU Darmstadt, Germany {anne,goran}@informatik.uni-mannheim.de {olga,nikolai}@wluper.com www.ukp.tu-darmstadt.de Abstract the distributional information from large corpora. Yet, a number of structured knowledge sources exist – knowledge bases (KBs) (Suchanek et al., 2007; Auer et al., 2007) and lexico-semantic networks (Miller, 1995; Liu and Singh, 2004; Navigli and Ponzetto, 2010) – encoding many types of knowledge that are underrepresented in text corpora. Starting from this observation, most recent efforts focused on injecting factual (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019) and linguistic knowledge (Lauscher et al., 2019; Peters et al., 2019) into pretrained LMs and demonstrated the usefulness of such knowledge in language understanding tasks (Wang et al., 2018, 2019). Joint pretraining models, on the one hand, augment distributional LM objectives with additional objectives based on external resources (Yu and Dredze, 2014; Nguyen et al., 2016; La"
2020.deelio-1.5,I05-5002,0,0.0284653,"Missing"
2020.deelio-1.5,P16-2074,0,0.029756,"gli and Ponzetto, 2010) – encoding many types of knowledge that are underrepresented in text corpora. Starting from this observation, most recent efforts focused on injecting factual (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019) and linguistic knowledge (Lauscher et al., 2019; Peters et al., 2019) into pretrained LMs and demonstrated the usefulness of such knowledge in language understanding tasks (Wang et al., 2018, 2019). Joint pretraining models, on the one hand, augment distributional LM objectives with additional objectives based on external resources (Yu and Dredze, 2014; Nguyen et al., 2016; Lauscher et al., 2019) and train the extended model from scratch. For models like BERT, this implies computationally expensive retraining from scratch of the encoding transformer network. Post-hoc fine-tuning models (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019), on the other hand, use the objectives based on external resources to fine-tune the encoder’s parameters, pretrained via distributional LM objectives. If the amount of fine-tuning data is substantial, however, this approach may lead to catastrophic forgetting of distributional knowledge obtained in pretraining (Goodfell"
2020.deelio-1.5,D14-1162,0,0.0985503,"der: https://github.com/ wluper/retrograph. 1 Introduction Self-supervised neural models like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019; Liu et al., 2019b), GPT (Radford et al., 2018, 2019), or XLNet (Yang et al., 2019) have rendered language modeling a very suitable pretraining task for learning language representations that are useful for a wide range of language understanding tasks (Wang et al., 2018, 2019). Although shown versatile w.r.t. the types of knowledge (Rogers et al., 2020) they encode, much like their predecessors – static word embedding models (Mikolov et al., 2013; Pennington et al., 2014) – neural LMs still only “consume” 43 Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 43–49 c Online, November 19, 2020. 2020 Association for Computational Linguistics cause of this, adapter training preserves the distributional information obtained in LM pretraining, without the need for any distributional (re-)training. While (Wang et al., 2020) inject factual knowledge from Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) into BERT, in this work, we investigate two resources that are commonly assum"
2020.deelio-1.5,N18-1202,0,0.0352532,"th conceptual knowledge from ConceptNet and its corresponding Open Mind Common Sense (OMCS) corpus, respectively, using adapter training. While overall results on the GLUE benchmark paint an inconclusive picture, a deeper analysis reveals that our adapter-based models substantially outperform BERT (up to 15-20 performance points) on inference tasks that require the type of conceptual knowledge explicitly present in ConceptNet and OMCS. We also open source all our experiments and relevant code under: https://github.com/ wluper/retrograph. 1 Introduction Self-supervised neural models like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019; Liu et al., 2019b), GPT (Radford et al., 2018, 2019), or XLNet (Yang et al., 2019) have rendered language modeling a very suitable pretraining task for learning language representations that are useful for a wide range of language understanding tasks (Wang et al., 2018, 2019). Although shown versatile w.r.t. the types of knowledge (Rogers et al., 2020) they encode, much like their predecessors – static word embedding models (Mikolov et al., 2013; Pennington et al., 2014) – neural LMs still only “consume” 43 Proceedings of Deep Learning Inside Out (DeeLIO): The Firs"
2020.deelio-1.5,D19-1005,0,0.0618754,"Missing"
2020.deelio-1.5,W18-5446,0,0.312602,"a number of structured knowledge sources exist – knowledge bases (KBs) (Suchanek et al., 2007; Auer et al., 2007) and lexico-semantic networks (Miller, 1995; Liu and Singh, 2004; Navigli and Ponzetto, 2010) – encoding many types of knowledge that are underrepresented in text corpora. Starting from this observation, most recent efforts focused on injecting factual (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019) and linguistic knowledge (Lauscher et al., 2019; Peters et al., 2019) into pretrained LMs and demonstrated the usefulness of such knowledge in language understanding tasks (Wang et al., 2018, 2019). Joint pretraining models, on the one hand, augment distributional LM objectives with additional objectives based on external resources (Yu and Dredze, 2014; Nguyen et al., 2016; Lauscher et al., 2019) and train the extended model from scratch. For models like BERT, this implies computationally expensive retraining from scratch of the encoding transformer network. Post-hoc fine-tuning models (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019), on the other hand, use the objectives based on external resources to fine-tune the encoder’s parameters, pretrained via distributional"
2020.deelio-1.5,D16-1264,0,0.0895152,"Missing"
2020.deelio-1.5,N18-1101,0,0.0640444,"Missing"
2020.deelio-1.5,2020.tacl-1.54,0,0.0251937,"ual knowledge explicitly present in ConceptNet and OMCS. We also open source all our experiments and relevant code under: https://github.com/ wluper/retrograph. 1 Introduction Self-supervised neural models like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019; Liu et al., 2019b), GPT (Radford et al., 2018, 2019), or XLNet (Yang et al., 2019) have rendered language modeling a very suitable pretraining task for learning language representations that are useful for a wide range of language understanding tasks (Wang et al., 2018, 2019). Although shown versatile w.r.t. the types of knowledge (Rogers et al., 2020) they encode, much like their predecessors – static word embedding models (Mikolov et al., 2013; Pennington et al., 2014) – neural LMs still only “consume” 43 Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 43–49 c Online, November 19, 2020. 2020 Association for Computational Linguistics cause of this, adapter training preserves the distributional information obtained in LM pretraining, without the need for any distributional (re-)training. While (Wang et al., 2020) inject factual knowledge from"
2020.deelio-1.5,P14-2089,0,0.0711671,"and Singh, 2004; Navigli and Ponzetto, 2010) – encoding many types of knowledge that are underrepresented in text corpora. Starting from this observation, most recent efforts focused on injecting factual (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019) and linguistic knowledge (Lauscher et al., 2019; Peters et al., 2019) into pretrained LMs and demonstrated the usefulness of such knowledge in language understanding tasks (Wang et al., 2018, 2019). Joint pretraining models, on the one hand, augment distributional LM objectives with additional objectives based on external resources (Yu and Dredze, 2014; Nguyen et al., 2016; Lauscher et al., 2019) and train the extended model from scratch. For models like BERT, this implies computationally expensive retraining from scratch of the encoding transformer network. Post-hoc fine-tuning models (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019), on the other hand, use the objectives based on external resources to fine-tune the encoder’s parameters, pretrained via distributional LM objectives. If the amount of fine-tuning data is substantial, however, this approach may lead to catastrophic forgetting of distributional knowledge obtained in"
2020.deelio-1.5,P19-1139,0,0.0442039,"ed Kingdom ♦ Ubiquitous Knowledge Processing (UKP) Lab, TU Darmstadt, Germany {anne,goran}@informatik.uni-mannheim.de {olga,nikolai}@wluper.com www.ukp.tu-darmstadt.de Abstract the distributional information from large corpora. Yet, a number of structured knowledge sources exist – knowledge bases (KBs) (Suchanek et al., 2007; Auer et al., 2007) and lexico-semantic networks (Miller, 1995; Liu and Singh, 2004; Navigli and Ponzetto, 2010) – encoding many types of knowledge that are underrepresented in text corpora. Starting from this observation, most recent efforts focused on injecting factual (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019) and linguistic knowledge (Lauscher et al., 2019; Peters et al., 2019) into pretrained LMs and demonstrated the usefulness of such knowledge in language understanding tasks (Wang et al., 2018, 2019). Joint pretraining models, on the one hand, augment distributional LM objectives with additional objectives based on external resources (Yu and Dredze, 2014; Nguyen et al., 2016; Lauscher et al., 2019) and train the extended model from scratch. For models like BERT, this implies computationally expensive retraining from scratch of the encoding transformer ne"
2020.deelio-1.5,D13-1170,0,0.00938693,"Missing"
2020.emnlp-main.185,2020.acl-main.747,0,0.168246,"Missing"
2020.emnlp-main.185,D18-1269,0,0.379567,"hkin et al., 2018; Sap et al., 2019, inter alia). Unfortunately, the extensive efforts related to this thread of research have so far been limited only to the English language.2 Such a narrow scope not only curbs the development of natural language understanding tools in other languages (Bender, 2011; Ponti et al., 2019a), but also exacerbates the Anglocentric bias in modeling commonsense reasoning. In fact, the expectations about typical situations do vary across cultures (Thomas, 1983). Datasets that cover multiple languages for other natural understanding tasks, such as language inference (Conneau et al., 2018), question answering (Lewis et al., 2020; Artetxe et al., 2020a; Clark et al., 2020), and paraphrase identification (Yang et al., 2019b) have received increasing attention. In fact, the requirement to generalise to new languages encourages the development of more versatile language understanding models, which can be ported across different grammars and lexica. These efforts have recently culminated in the integration of several multilingual tasks into the XTREME evaluation suite (Hu et al., 2020). However, a compre1 Moreover, there are often multiple legitimate chains of sentences that can be"
2020.emnlp-main.185,W17-1504,0,0.0993792,"rted across different grammars and lexica. These efforts have recently culminated in the integration of several multilingual tasks into the XTREME evaluation suite (Hu et al., 2020). However, a compre1 Moreover, there are often multiple legitimate chains of sentences that can be invoked in between premises and hypotheses. In short, commonsense reasoning does not just involve understanding what is possible, but also ranking what is most plausible. 2 The only exception is direct translation of the 272 paired English Winograd Schema Challenge instances to Japanese (Shibata et al., 2015), French (Amsili and Seminck, 2017), and Portuguese (Melo et al., 2020). 2362 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2362–2376, c November 16–20, 2020. 2020 Association for Computational Linguistics PREMISE qu en th en Sipasqa cereal mikhunanpi kuruta tarirqan. The girl found a bug in her cereal. ตาของฉันแดงและบวม My eyes became red and puffy. R C CHOICE 1 Payqa pukunman n˜ uq˜nuta churakurqan. She poured milk in the bowl. ฉันรองไห I was sobbing. CHOICE 2 Payqa manam mikhuyta munarqanchu. She lost her appetite. ฉันหัวเราะ I was laughing. Table 1: Examples of forward (Resu"
2020.emnlp-main.185,2020.emnlp-main.618,0,0.023636,"Missing"
2020.emnlp-main.185,N19-1423,0,0.197445,"ce in digital texts. Since resource-rich languages tend to belong to a few families and areas, samples inspired by this criterion are highly biased and not indicative of true models’ performance (Gerz et al., 2018; Ponti et al., 2019a; Joshi et al., 2020; Lauscher et al., 2020). Following this guiding principle, we select 11 languages from 11 distinct families, and 5 geographical macro-areas (Africa, Eurasia, Papunesia, North America, and South America). We leverage XCOPA to benchmark a series of state-of-the-art pretrained multilingual models, including XLM - R (Conneau et al., 2020), MBERT (Devlin et al., 2019), and multilingual USE (Yang et al., 2019a). Two XCOPA languages (i.e., Southern Quechua and Haitian Creole) are out-of-sample for the pretrained models: this naturally raises the question of how to adapt the pretrained models to such unseen languages. In particular, we investigate the resource-lean scenarios where either some monolingual data or a bilingual dictionary with English (or both) are available for the target language. In summary, we offer the following contributions. 1) We create the first large-scale multilingual evaluation set for commonsense reasoning, spanning 11 languages, and"
2020.emnlp-main.185,2020.acl-main.421,0,0.58447,", the extensive efforts related to this thread of research have so far been limited only to the English language.2 Such a narrow scope not only curbs the development of natural language understanding tools in other languages (Bender, 2011; Ponti et al., 2019a), but also exacerbates the Anglocentric bias in modeling commonsense reasoning. In fact, the expectations about typical situations do vary across cultures (Thomas, 1983). Datasets that cover multiple languages for other natural understanding tasks, such as language inference (Conneau et al., 2018), question answering (Lewis et al., 2020; Artetxe et al., 2020a; Clark et al., 2020), and paraphrase identification (Yang et al., 2019b) have received increasing attention. In fact, the requirement to generalise to new languages encourages the development of more versatile language understanding models, which can be ported across different grammars and lexica. These efforts have recently culminated in the integration of several multilingual tasks into the XTREME evaluation suite (Hu et al., 2020). However, a compre1 Moreover, there are often multiple legitimate chains of sentences that can be invoked in between premises and hypotheses. In short, commonse"
2020.emnlp-main.185,P18-1231,0,0.0284989,"ltiple-choice problems where the most sensible option is chosen. Another line of evaluation involves commonsense-enabled reading comprehension and question answering (Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). Multilingual Evaluation of Natural Language Understanding. While the above commonsense reasoning datasets are limited to English, several multilingual datasets for other natural language understanding tasks are available, e.g., lexical semantic similarity (Multi-SimLex; Vuli´c et al., 2020), document classification (MLDoc; Schwenk and Li, 2018), sentiment analysis (Barnes et al., 2018), and natural language inference (XNLI; Conneau et al., 2018). Other recent multilingual sets target the QA task based on reading comprehenseen scripts (e.g., both HT and QU are written in Latin script). 13 Abductive reasoning is inference to the most plausible explanation of incomplete observations (Peirce, 1960). sion: MLQA (Lewis et al., 2020) in 7 languages, XQuAD (Artetxe et al., 2020b) in 10; and TyDiQA (Clark et al., 2020) in 11 typologically diverse languages. Further, PAWS-X (Yang et al., 2019b) evaluates paraphrase identification in 6 languages. A standard and pragmatic approach to m"
2020.emnlp-main.185,2020.acl-main.653,0,0.124033,"Missing"
2020.emnlp-main.185,D19-1243,0,0.0381807,"Missing"
2020.emnlp-main.185,E17-2002,0,0.162717,"Missing"
2020.emnlp-main.185,2020.acl-main.560,0,0.0659782,"lar is still missing. In order to address this gap, we develop a novel dataset, XCOPA (see examples in Table 1), by carefully translating and re-annotating the validation and test sets of English COPA into 11 target languages. A key design choice is the selection of a typologically diverse sample of languages. In particular, we privilege variety over the abundance in digital texts. Since resource-rich languages tend to belong to a few families and areas, samples inspired by this criterion are highly biased and not indicative of true models’ performance (Gerz et al., 2018; Ponti et al., 2019a; Joshi et al., 2020; Lauscher et al., 2020). Following this guiding principle, we select 11 languages from 11 distinct families, and 5 geographical macro-areas (Africa, Eurasia, Papunesia, North America, and South America). We leverage XCOPA to benchmark a series of state-of-the-art pretrained multilingual models, including XLM - R (Conneau et al., 2020), MBERT (Devlin et al., 2019), and multilingual USE (Yang et al., 2019a). Two XCOPA languages (i.e., Southern Quechua and Haitian Creole) are out-of-sample for the pretrained models: this naturally raises the question of how to adapt the pretrained models to such"
2020.emnlp-main.185,kamholz-etal-2014-panlex,0,0.029351,"HT and QU by concatenating their respective Wikipedia dumps with their respective text from the JW300 corpus (Agi´c and Vuli´c, 2019). In total, the training size is 5,710,426 tokens for HT, and 2,263,134 tokens for QU. 2) S. Sentences in English (EN). This could prevent (catastrophic) forgetting of the source language while fine-tuning, which presumably may occur with T only. We create the English corpus of comparable size to HT and QU corpora by randomly sampling 200K sentences from EN Wikipedia. 3) D. A bilingual EN – HT and EN – QU dictionary. The dictionaries were extracted from PanLex (Kamholz et al., 2014): we retain the 5k most reliable word translation pairs according to the available PanLex confidence scores. We create a synthetic corpus from the dictionary (termed D-corpus henceforth) by concatenating each translation pair from the dictionary into a quasi-sentence. 4) T-REP. T data with all occurrences of target language terms from the 5K dictionary replaced with their English translations. HT QU CO-ZS Even massively multilingual encoders like MBERT and XLM-R, pretrained on corpora of over 100 languages, cover only a fraction of the world’s 7,000+ languages. In fact, the majority of the wor"
2020.emnlp-main.185,W15-2137,0,0.0276317,"are hidden (Niven and Kao, 2019). Finally, in §4.4 we explore several strategies to adapt massively multilingual models to new languages not observed during pretraining, such as Quechua and Haitian Creole. 4.1 Baselines We evaluate baselines in several combinations of experimental setups based on: 1) different methods for cross-lingual transfer, either based on model transfer or machine translation; 2) different multilingual pretrained encoders; 3) different sources of training and validation data. Cross-lingual Transfer Methods. We consider two high-level methods for cross-lingual transfer (Tiedemann, 2015; Ponti et al., 2019a): 1) multilingual model transfer (MuMoTr), whereby a Transformer-based encoder is pretrained on multiple languages in an unsupervised fashion, and subsequently trained on English annotated data for multiple-choice classification, therefore enabling zero-shot generalisation to the other languages. 2) translate test (TrTe), whereby target test data7 are translated into English via Google Translate. This includes all languages except for QU, for which the service is not available. Multilingual Encoders. For model transfer, we evaluate the following state-of-the-art pretraine"
2020.emnlp-main.185,2020.cl-4.5,1,0.86898,"Missing"
2020.emnlp-main.185,N18-1101,0,0.0568428,"written in Latin script). 13 Abductive reasoning is inference to the most plausible explanation of incomplete observations (Peirce, 1960). sion: MLQA (Lewis et al., 2020) in 7 languages, XQuAD (Artetxe et al., 2020b) in 10; and TyDiQA (Clark et al., 2020) in 11 typologically diverse languages. Further, PAWS-X (Yang et al., 2019b) evaluates paraphrase identification in 6 languages. A standard and pragmatic approach to multilingual dataset creation is translation from an existing (English) dataset, e.g., Multi-SimLex from the extended English SimLex-999 (Hill et al., 2015), XNLI from MultiNLI (Williams et al., 2018), XQuAD from SQuAD (Rajpurkar et al., 2016), and PAWS-X from PAWS (Zhang et al., 2019). TyDiQA, however, was built independently in each language. Finally, a large number of tasks has been recently integrated into unified multilingual evaluation suites, XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020). 6 Conclusion and Future Work We presented the Cross-lingual Choice of Plausible Alternatives (XCOPA), a multilingual evaluation benchmark for causal commonsense reasoning. All XCOPA instances are aligned across 11 languages, which enables cross-lingual comparisons. The language selection"
2020.emnlp-main.185,2020.acl-demos.12,0,0.139316,"Missing"
2020.emnlp-main.185,D19-1382,0,0.392467,"limited only to the English language.2 Such a narrow scope not only curbs the development of natural language understanding tools in other languages (Bender, 2011; Ponti et al., 2019a), but also exacerbates the Anglocentric bias in modeling commonsense reasoning. In fact, the expectations about typical situations do vary across cultures (Thomas, 1983). Datasets that cover multiple languages for other natural understanding tasks, such as language inference (Conneau et al., 2018), question answering (Lewis et al., 2020; Artetxe et al., 2020a; Clark et al., 2020), and paraphrase identification (Yang et al., 2019b) have received increasing attention. In fact, the requirement to generalise to new languages encourages the development of more versatile language understanding models, which can be ported across different grammars and lexica. These efforts have recently culminated in the integration of several multilingual tasks into the XTREME evaluation suite (Hu et al., 2020). However, a compre1 Moreover, there are often multiple legitimate chains of sentences that can be invoked in between premises and hypotheses. In short, commonsense reasoning does not just involve understanding what is possible, but"
2020.emnlp-main.185,D19-1454,0,0.0645931,"Missing"
2020.emnlp-main.185,L18-1560,0,0.0311419,"ers et al., 2019b) are cast as openended multiple-choice problems where the most sensible option is chosen. Another line of evaluation involves commonsense-enabled reading comprehension and question answering (Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). Multilingual Evaluation of Natural Language Understanding. While the above commonsense reasoning datasets are limited to English, several multilingual datasets for other natural language understanding tasks are available, e.g., lexical semantic similarity (Multi-SimLex; Vuli´c et al., 2020), document classification (MLDoc; Schwenk and Li, 2018), sentiment analysis (Barnes et al., 2018), and natural language inference (XNLI; Conneau et al., 2018). Other recent multilingual sets target the QA task based on reading comprehenseen scripts (e.g., both HT and QU are written in Latin script). 13 Abductive reasoning is inference to the most plausible explanation of incomplete observations (Peirce, 1960). sion: MLQA (Lewis et al., 2020) in 7 languages, XQuAD (Artetxe et al., 2020b) in 10; and TyDiQA (Clark et al., 2020) in 11 typologically diverse languages. Further, PAWS-X (Yang et al., 2019b) evaluates paraphrase identification in 6 languag"
2020.emnlp-main.185,D18-1009,0,0.027514,"Levesque et al., 2012; Morgenstern and Ortiz, 2015). WSC consists in a pronoun coreference resolution task with paired instances, and has been recently expanded into the WinoGrande dataset (Sakaguchi et al., 2020) through crowd-sourcing. Several recent evaluation sets target particular aspects of commonsense, e.g., abductive reasoning (Bhagavatula et al., 2020),13 intents and reactions to events (Rashkin et al., 2018), social (Sap et al., 2019) and physical (Bisk et al., 2020) interactions, or visual commonsense (Zellers et al., 2019a). Others, e.g., CommonsenseQA (Talmor et al., 2019), SWAG (Zellers et al., 2018), and HellaSWAG (Zellers et al., 2019b) are cast as openended multiple-choice problems where the most sensible option is chosen. Another line of evaluation involves commonsense-enabled reading comprehension and question answering (Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). Multilingual Evaluation of Natural Language Understanding. While the above commonsense reasoning datasets are limited to English, several multilingual datasets for other natural language understanding tasks are available, e.g., lexical semantic similarity (Multi-SimLex; Vuli´c et al., 2020), document cl"
2020.emnlp-main.185,P19-1472,0,0.0271269,"ational modeling of commonsense reasoning is the Winograd Schema Challenge (WSC; Levesque et al., 2012; Morgenstern and Ortiz, 2015). WSC consists in a pronoun coreference resolution task with paired instances, and has been recently expanded into the WinoGrande dataset (Sakaguchi et al., 2020) through crowd-sourcing. Several recent evaluation sets target particular aspects of commonsense, e.g., abductive reasoning (Bhagavatula et al., 2020),13 intents and reactions to events (Rashkin et al., 2018), social (Sap et al., 2019) and physical (Bisk et al., 2020) interactions, or visual commonsense (Zellers et al., 2019a). Others, e.g., CommonsenseQA (Talmor et al., 2019), SWAG (Zellers et al., 2018), and HellaSWAG (Zellers et al., 2019b) are cast as openended multiple-choice problems where the most sensible option is chosen. Another line of evaluation involves commonsense-enabled reading comprehension and question answering (Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). Multilingual Evaluation of Natural Language Understanding. While the above commonsense reasoning datasets are limited to English, several multilingual datasets for other natural language understanding tasks are available,"
2020.emnlp-main.185,J15-4004,1,\N,Missing
2020.emnlp-main.185,P11-1132,0,\N,Missing
2020.emnlp-main.185,J19-3005,1,\N,Missing
2020.emnlp-main.185,D18-1029,1,\N,Missing
2020.emnlp-main.185,N19-1131,0,\N,Missing
2020.emnlp-main.185,P19-1310,1,\N,Missing
2020.emnlp-main.185,N19-5004,0,\N,Missing
2020.emnlp-main.185,N19-1421,0,\N,Missing
2020.emnlp-main.185,D19-1288,1,\N,Missing
2020.emnlp-main.363,2020.emnlp-main.618,0,0.0336263,"Missing"
2020.emnlp-main.363,2020.acl-main.421,0,0.517326,"1 Data and Web Science Group, University of Mannheim, Germany 2 Language Technology Group, University of Oslo, Norway 3 Language Technology Lab, University of Cambridge, UK 1 {anne,goran}@informatik.uni-mannheim.de, 2 vinitr@ifi.uio.no, 3 iv250@cam.ac.uk Abstract predictions in resource-lean languages? In the most extreme scenario, termed zero-shot cross-lingual transfer, not a single labeled instance exists for a target language. Recent work has placed much emphasis on this scenario exactly; in theory, it offers the widest portability across the world’s 7,000+ languages (Pires et al., 2019; Artetxe et al., 2020b; Lin et al., 2019; Cao et al., 2020; Hu et al., 2020). The current mainstay of cross-lingual transfer in NLP are approaches based on continuous crosslingual representation spaces such as cross-lingual word embeddings (CLWEs) (Ruder et al., 2019) and, most recently, massively multilingual transformer networks (MMTs), pretrained on multilingual corpora with language modeling (LM) objectives (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). The latter have de facto become the default language transfer paradigm, with multiple studies reporting their unparalleled transfer per"
2020.emnlp-main.363,2020.acl-main.658,0,0.103664,"1 Data and Web Science Group, University of Mannheim, Germany 2 Language Technology Group, University of Oslo, Norway 3 Language Technology Lab, University of Cambridge, UK 1 {anne,goran}@informatik.uni-mannheim.de, 2 vinitr@ifi.uio.no, 3 iv250@cam.ac.uk Abstract predictions in resource-lean languages? In the most extreme scenario, termed zero-shot cross-lingual transfer, not a single labeled instance exists for a target language. Recent work has placed much emphasis on this scenario exactly; in theory, it offers the widest portability across the world’s 7,000+ languages (Pires et al., 2019; Artetxe et al., 2020b; Lin et al., 2019; Cao et al., 2020; Hu et al., 2020). The current mainstay of cross-lingual transfer in NLP are approaches based on continuous crosslingual representation spaces such as cross-lingual word embeddings (CLWEs) (Ruder et al., 2019) and, most recently, massively multilingual transformer networks (MMTs), pretrained on multilingual corpora with language modeling (LM) objectives (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). The latter have de facto become the default language transfer paradigm, with multiple studies reporting their unparalleled transfer per"
2020.emnlp-main.363,Q19-1038,0,0.0330511,"e reliable MT hinges on availability of large parallel corpora, transfer via multilingual KBs (Camacho-Collados et al., 2016; Mrkˇsi´c et al., 2017) is impaired by the limited KB coverage and inaccurate entity linking (Moro et al., 2014; Raiman and Raiman, 2018). Therefore, recent years have seen a surge of language transfer methods based on continuous representation spaces. The previous state-of-the-art, cross-lingual word embeddings (CLWEs) (Mikolov et al., 2013; Ammar et al., 2016; Artetxe et al., 2017; Smith et al., 2017; Glavaˇs et al., 2019; Vuli´c et al., 2019) and sentence embeddings (Artetxe and Schwenk, 2019), have most recently been replaced by massively multilingual transformers (MMTs) pretrained with LM objectives (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). 2.2 Massively Multilingual Transformers Multilingual BERT (mBERT). At BERT’s (Devlin et al., 2019) core is a multi-layer transformer network (Vaswani et al., 2017), parameters of which are pretrained using masked language modeling (MLM) and next sentence prediction (NSP). In MLM, some tokens are masked out and they need to be recovered from the context; NSP predicts adjacency of sentences in text, informing the tra"
2020.emnlp-main.363,D19-1279,0,0.036118,"languages (ES, DE, EL, RU, TR, AR, VI, TH, ZH, and HI ). In order to allow for a comparison between zero-shot and few-shot transfer (see §4), we reserve 10 documents as the development set for our experiments and evaluate on the remaining 38 articles.4 Fine-tuning. For higher-level tasks, we perform standard downstream fine-tuning of LM-pretrained mBERT and XLM-R. For lower-level tasks, we instead freeze the transformer and train only taskspecific classifiers.5,6 We add the following task-specific architectures on top of MMTs: for DEP we add the biaffine parsing head (Dozat and Manning, 2017; Kondratyuk and Straka, 2019); for POS, we attach a simple 4 As a general note, while the effects of “translationese” might have some impact on the absolute numbers (Artetxe et al., 2020a), they are not prominent enough to have any impact on the relative trends in the reported results (e.g., zeroshot vs. few-shot performance). For both XNLI and XQuAD, the translations were done completely manually and not via post-editing of MT (which would pose a higher “translationese” risk). Moreover, having an independently created test set in each language would impede comparability across languages. 5 This gave slightly better perfo"
2020.emnlp-main.363,2020.findings-emnlp.150,0,0.106535,"Missing"
2020.emnlp-main.363,E17-2002,0,0.35783,"a particular task in consideration for transfer performance? We conduct all analyses across five different tasks, which we roughly divide into two groups: (1) “lowlevel” tasks (POS-tagging, dependency parsing, and NER); and (2) “high-level” language understanding (LU) tasks (NLI and QA). We show that transfer performance in both zero-shot and fewshot scenarios largely depends on the “task level”. (Q3) Can we (even) predict transfer performance? Running a simple regression on available transfer results, we show that we can (roughly) predict the transfer performance from (1) language proximity (Littell et al., 2017) for low-level tasks; (2) combination of language proximity and size of targetlanguage pretraining corpora for high-level tasks. (Q4) Should we focus more on few-shot transfer scenarios and quick annotation cycles? Complementing the efforts on improving zeroshot transfer (Cao et al., 2020), we point to fewshot transfer as a very effective mechanism for improving target-language performance. Similar to the seminal “pre-neural” work of Garrette and Baldridge (2013), our results suggest that only several hours (or even minutes) of annotation work can “buy” substantial performance gains for lowres"
2020.emnlp-main.363,J19-3005,1,0.781718,"Missing"
2020.emnlp-main.363,2020.acl-main.467,0,0.0550715,"Missing"
2020.emnlp-main.363,P19-1015,0,0.0356261,"E covers all 40 languages, but much smaller language subsets. 3 We leave an even more general analysis that combines transfer both across tasks (Pruksachatkun et al., 2020; Glavaˇs and Vuli´c, 2020) and across languages for future work. 4485 darin) Chinese (ZH), Finnish (FI), Hebrew (HE), Hindi (HI), Italian (IT), Japanese (JA), Korean (KO), Russian (RU), Swedish (SV), and Turkish (TR). Part-of-speech Tagging (POS). Again, we use UD and obtain the Universal POS-tag (UPOS) annotations from the same treebanks as with DEP. Named Entity Recognition (NER). We resort to the NER WikiANN dataset from Rahimi et al. (2019). We experiment with the same set of 12 target languages as in DEP and POS. Cross-lingual Natural Language Inference (XNLI). We evaluate on the XNLI corpus (Conneau et al., 2018) created by translating dev and test portions of the English Multi-NLI dataset (Williams et al., 2018) into 14 languages by professional translators (French (FR), Spanish (ES), German (DE), Greek (EL), Bulgarian (BG), Russian (RU), Turkish (TR), Arabic (AR), Vietnamese (VI), Thai (TH), Chinese (ZH), Hindi (HI), Swahili (SW), and Urdu (UR)). Cross-lingual Question Answering (XQuAD). We rely on the XQuAD dataset (Artetxe"
2020.emnlp-main.363,D16-1264,0,0.219834,"luate on the XNLI corpus (Conneau et al., 2018) created by translating dev and test portions of the English Multi-NLI dataset (Williams et al., 2018) into 14 languages by professional translators (French (FR), Spanish (ES), German (DE), Greek (EL), Bulgarian (BG), Russian (RU), Turkish (TR), Arabic (AR), Vietnamese (VI), Thai (TH), Chinese (ZH), Hindi (HI), Swahili (SW), and Urdu (UR)). Cross-lingual Question Answering (XQuAD). We rely on the XQuAD dataset (Artetxe et al., 2020b), created by translating the 240 dev paragraphs (from 48 documents) and corresponding 1,190 QA pairs of SQuAD v1.1 (Rajpurkar et al., 2016) to 11 languages (ES, DE, EL, RU, TR, AR, VI, TH, ZH, and HI ). In order to allow for a comparison between zero-shot and few-shot transfer (see §4), we reserve 10 documents as the development set for our experiments and evaluate on the remaining 38 articles.4 Fine-tuning. For higher-level tasks, we perform standard downstream fine-tuning of LM-pretrained mBERT and XLM-R. For lower-level tasks, we instead freeze the transformer and train only taskspecific classifiers.5,6 We add the following task-specific architectures on top of MMTs: for DEP we add the biaffine parsing head (Dozat and Manning,"
2020.emnlp-main.363,W19-6204,0,0.0672448,"Missing"
2020.emnlp-main.363,P19-1355,0,0.0488697,"Missing"
2020.emnlp-main.363,D19-5901,0,0.0231081,"ances. 4.2 Cost of Language Transfer Gains As shown in §4.1, moving to few-shot transfer can massively improve performance and reduce the gaps observed with zero-shot transfer, especially for low-resource languages. While additional finetuning on few target-language examples is computationally cheap, data annotation may be expensive, especially for minor languages. What are the annotation costs, and how do they translate into performance gains? Table 5 provides ballpark estimates for our five evaluation tasks; the estimates are based on annotation costs from the literature (Hovy et al., 2014; Tratz, 2019; Bontcheva et al., 2017; Marelli et al., 2014; Rajpurkar et al., 2016). We explain these cost-to-gain conversion estimates in more detail in Appendix C). A provocative high-level question that calls for further discussion in future work can be framed as: are GPU hours effectively more costly13 than data annotations are in the long run? While MMTs are extremely useful as general-purpose models of language, their potential for some (target) languages can be quickly unlocked by pairing them with a small number of annotated target-language exam13 Conclusion Research on zero-shot language transfer"
2020.emnlp-main.363,D19-1449,1,0.855998,"Missing"
2020.emnlp-main.363,2020.emnlp-main.257,1,0.891424,"Missing"
2020.emnlp-main.363,N18-1101,0,0.0405413,"ebrew (HE), Hindi (HI), Italian (IT), Japanese (JA), Korean (KO), Russian (RU), Swedish (SV), and Turkish (TR). Part-of-speech Tagging (POS). Again, we use UD and obtain the Universal POS-tag (UPOS) annotations from the same treebanks as with DEP. Named Entity Recognition (NER). We resort to the NER WikiANN dataset from Rahimi et al. (2019). We experiment with the same set of 12 target languages as in DEP and POS. Cross-lingual Natural Language Inference (XNLI). We evaluate on the XNLI corpus (Conneau et al., 2018) created by translating dev and test portions of the English Multi-NLI dataset (Williams et al., 2018) into 14 languages by professional translators (French (FR), Spanish (ES), German (DE), Greek (EL), Bulgarian (BG), Russian (RU), Turkish (TR), Arabic (AR), Vietnamese (VI), Thai (TH), Chinese (ZH), Hindi (HI), Swahili (SW), and Urdu (UR)). Cross-lingual Question Answering (XQuAD). We rely on the XQuAD dataset (Artetxe et al., 2020b), created by translating the 240 dev paragraphs (from 48 documents) and corresponding 1,190 QA pairs of SQuAD v1.1 (Rajpurkar et al., 2016) to 11 languages (ES, DE, EL, RU, TR, AR, VI, TH, ZH, and HI ). In order to allow for a comparison between zero-shot and few-s"
2020.emnlp-main.363,W09-3002,0,\N,Missing
2020.emnlp-main.363,Q14-1019,0,\N,Missing
2020.emnlp-main.363,P14-2062,0,\N,Missing
2020.emnlp-main.363,N13-1014,0,\N,Missing
2020.emnlp-main.363,marelli-etal-2014-sick,0,\N,Missing
2020.emnlp-main.363,Q17-1022,1,\N,Missing
2020.emnlp-main.363,P17-1042,0,\N,Missing
2020.emnlp-main.363,D17-1269,0,\N,Missing
2020.emnlp-main.363,C18-1071,0,\N,Missing
2020.emnlp-main.363,N19-1423,0,\N,Missing
2020.emnlp-main.363,2020.acl-main.560,0,\N,Missing
2020.emnlp-main.586,2020.acl-main.421,0,0.0383607,", are reported for EN and DE. 4 Results and Discussion A summary of the results is shown in Figure 2 for LSIM, in Figure 3a for BLI, in Figure 3b for CLIR, in Figure 4a and Figure 4b for RELP, and in Figure 4c for WA. These results offer multiple axes of comparison, and the ensuing discussion focuses on the central questions Q1-Q3 posed in §1.6 Monolingual versus Multilingual LMs. Results across all tasks validate the intuition that languagespecific monolingual LMs contain much more lexical information for a particular target language than massively multilingual models such as mBERT or XLM-R (Artetxe et al., 2020). We see large drops between MONO.* and MULTI.* configurations even for very high-resource languages (EN and DE), and they are even more prominent for FI and TR. Encompassing 100+ training languages with limited model capacity, multilingual models suffer from the “curse of multilinguality” (Conneau et al., 2020): they must trade off monolingual lexical information coverage (and consequently monolingual performance) for a wider language coverage.7 How Important is Context? Another observation that holds across all configurations concerns the usefulness of providing contexts drawn from external"
2020.emnlp-main.586,Q19-1004,0,0.020417,"et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across dif"
2020.emnlp-main.586,Q17-1010,0,0.807302,"r layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers. 1 Introduction and Motivation Language models (LMs) based on deep Transformer networks (Vaswani et al., 2017), pretrained on unprecedentedly large amounts of text, offer unmatched performance in virtually every NLP task (Qiu et al., 2020). Models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edm"
2020.emnlp-main.586,C18-1140,0,0.0236558,"fastText (Bojanowski et al., 2017). This study has only scratched the surface of this research avenue. In future work, we plan to investigate how domains of external corpora affect AOC configurations, and how to sample representative contexts from the corpora. We will also extend the study to more languages, more lexical semantic probes, and other larger underlying LMs. The difference in performance across layers also calls for more sophisticated lexical representation extraction methods (e.g., through layer weighting or attention) similar to meta-embedding approaches (Yin and Sch¨utze, 2016; Bollegala and Bao, 2018; Kiela et al., 2018). Given the current large gaps between monolingual and multilingual LMs, we will also focus on lightweight methods to enrich lexical content in multilingual LMs (Wang et al., 2020; Pfeiffer et al., 2020). Acknowledgments This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909) awarded to Anna Korhonen. The work of Goran Glavaˇs and Robert Litschko is supported by the Baden-W¨urttemberg Stiftung (AGREE grant of the Eliteprogramm). References Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. A robust self-learning meth"
2020.emnlp-main.586,2020.nlp4convai-1.5,1,0.835928,"Missing"
2020.emnlp-main.586,D19-1627,0,0.0202607,"19; Mickus et al., 2020). As a consequence, we hypothesise that abstract, type-level information could be codified in lower layers instead. However, given the absence of a direct equivalent to a static word type embedding, we still need to establish how to extract such type-level information. In prior work, contextualised representations (and attention weights) have been interpreted in the light of linguistic knowledge mostly through probes. These consist in learned classifier predicting annotations like POS tags (Pimentel et al., 2020) and word senses (Peters et al., 2018; Reif et al., 2019; Chang and Chen, 2019), or linear transformations to a space where distances mirror dependency tree structures (Hewitt and Manning, 2019).1 In this work, we explore several unsupervised word-level representation extraction strategies and configurations for lexico-semantic tasks (i.e., probes), stemming from different combinations of the components detailed in Table 1 and illustrated in Figure 1. In particular, we assess the impact of: 1) encoding tokens with monolingual LM-pretrained Transformers vs. with their mas1 The interplay between the complexity of a probe and its accuracy, as well as its effect on the overa"
2020.emnlp-main.586,2020.acl-main.493,0,0.0265766,"Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across different languages is currently lacking. We present such a study, spanning six typologically diverse languages for which comparable pretrained BERT models and evaluation data are"
2020.emnlp-main.586,2020.acl-main.747,0,0.0810209,"Missing"
2020.emnlp-main.586,D19-1572,0,0.019846,"match configurations that average subword embeddings from multiple contexts (AOC-10 and AOC100). However, it is worth noting that 1) perfor5 Note that RELP is structurally different from the other four tasks: instead of direct computations with word embeddings, called metric learning or similarity-based evaluation (Ruder et al., 2019), it uses them as features in a neural architecture. 6 Full results are available in the appendix. 7 For a particular target language, monolingual performance can be partially recovered by additional in-language monolingual training via masked language modeling (Eisenschlos et al., 2019; Pfeiffer et al., 2020). In a side experiment, we have also verified that the same holds for lexical information coverage. 7225 Spearman ρ correlation Spearman ρ correlation 0.55 0.45 0.35 0.25 0.15 L≤2 L≤4 L≤6 L ≤ 8 L ≤ 10 Average over layers 0.55 0.45 0.35 0.25 0.15 L ≤ 12 L≤2 (a) English L≤6 L ≤ 8 L ≤ 10 Average over layers L ≤ 12 (b) Finnish 0.4 Spearman ρ correlation 0.65 Spearman ρ correlation L≤4 0.55 0.45 0.35 L≤2 L≤4 L≤6 L ≤ 8 L ≤ 10 Average over layers 0.3 0.2 0.1 0.0 L≤2 L ≤ 12 (c) Mandarin Chinese L≤4 L≤6 L ≤ 8 L ≤ 10 Average over layers L ≤ 12 (d) Russian Figure 2: Spearman’s ρ c"
2020.emnlp-main.586,D19-1006,0,0.406652,"gers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across different languages is currently lacking. We present such a study, spanning six typologically diverse languages for which comparable pretrained BERT models and evaluation data are readily available. We dissect the pipeline for extracting lexical representations, and divide it into crucial components, including: the underlying source LM, the selection of subword tokens, external corpora, and which Transformer layers to average o"
2020.emnlp-main.586,D18-1029,1,0.828082,"Missing"
2020.emnlp-main.586,N18-2029,1,0.893221,"Missing"
2020.emnlp-main.586,P19-1070,1,0.894028,"Missing"
2020.emnlp-main.586,P19-1356,0,0.0244048,"word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across different languages is currently lacking. We present such a study, spanning six typologically diverse languages for which comparable pretr"
2020.emnlp-main.586,D18-1176,0,0.0193142,"l., 2017). This study has only scratched the surface of this research avenue. In future work, we plan to investigate how domains of external corpora affect AOC configurations, and how to sample representative contexts from the corpora. We will also extend the study to more languages, more lexical semantic probes, and other larger underlying LMs. The difference in performance across layers also calls for more sophisticated lexical representation extraction methods (e.g., through layer weighting or attention) similar to meta-embedding approaches (Yin and Sch¨utze, 2016; Bollegala and Bao, 2018; Kiela et al., 2018). Given the current large gaps between monolingual and multilingual LMs, we will also focus on lightweight methods to enrich lexical content in multilingual LMs (Wang et al., 2020; Pfeiffer et al., 2020). Acknowledgments This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909) awarded to Anna Korhonen. The work of Goran Glavaˇs and Robert Litschko is supported by the Baden-W¨urttemberg Stiftung (AGREE grant of the Eliteprogramm). References Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. A robust self-learning method for fully unsuperv"
2020.emnlp-main.586,2005.mtsummit-papers.11,0,0.115125,"esented in the respective fastText (FT) vectors, which were trained on lowercased monolingual Wikipedias by Bojanowski et al. (2017). The equivalent vocabulary coverage allows a direct comparison to fastText vectors, which we use as a baseline static WE method in all evaluation tasks. To retain the same vocabulary across all configurations, in AOC variants we back off to the related ISO variant for words that have zero occurrences in external corpora. For all AOC vector variants, we leverage 1M sentences of maximum sequence length 512, which we randomly sample from external corpora: Europarl (Koehn, 2005) for EN, DE, FI, available via OPUS (Tiedemann, 2009); the United Nations Parallel Corpus for RU and ZH (Ziemski et al., 2016), and monolingual TR WMT17 data (Bojar et al., 2017). Evaluation Tasks. We carry out the evaluation on five standard and diverse lexical semantic tasks: Task 1: Lexical semantic similarity (LSIM) is the most widespread intrinsic task for evaluation of traditional word embeddings (Hill et al., 2015). The evaluation metric is the Spearman’s rank correlation between the average of human-elicited semantic similarity scores for word pairs and the cosine similarity between th"
2020.emnlp-main.586,2020.acl-main.375,0,0.0252215,"Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across different languages is currently lacking. We present such a study, spanning six typologically diverse languages for which comparable pretrained BERT models and e"
2020.emnlp-main.586,D14-1162,0,0.0919991,"aim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers. 1 Introduction and Motivation Language models (LMs) based on deep Transformer networks (Vaswani et al., 2017), pretrained on unprecedentedly large amounts of text, offer unmatched performance in virtually every NLP task (Qiu et al., 2020). Models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter"
2020.emnlp-main.586,N18-1202,0,0.25049,"e that translation pairs indeed obtain similar representations (Q4), but the similarity depends on the extraction configuration, as well as on the typological distance between the two languages. 2 Lexical Representations from Pretrained Language Models Classical static word embeddings (Bengio et al., 2003; Mikolov et al., 2013b; Pennington et al., 2014) are grounded in distributional semantics, as they infer the meaning of each word type from its co-occurrence patterns. However, LM-pretrained Transformer encoders have introduced at least two levels of misalignment with the classical approach (Peters et al., 2018; Devlin et al., 2019). First, representations are assigned to word tokens and are affected by the current context and position within a sentence (Mickus et al., 2020). Second, tokens may correspond to subword strings rather than complete word forms. This begs the question: do pretrained encoders still retain a notion of lexical concepts, abstracted from their instances in texts? Analyses of lexical semantic information in large pretrained LMs have been limited so far, focusing only on the English language and on the task of word sense disambiguation. Reif et al. (2019) showed that senses are"
2020.emnlp-main.586,2020.emnlp-main.617,1,0.88287,"Missing"
2020.emnlp-main.586,2020.acl-main.420,0,0.0210954,"ame token tends not to be self-similar across different contexts (Ethayarajh, 2019; Mickus et al., 2020). As a consequence, we hypothesise that abstract, type-level information could be codified in lower layers instead. However, given the absence of a direct equivalent to a static word type embedding, we still need to establish how to extract such type-level information. In prior work, contextualised representations (and attention weights) have been interpreted in the light of linguistic knowledge mostly through probes. These consist in learned classifier predicting annotations like POS tags (Pimentel et al., 2020) and word senses (Peters et al., 2018; Reif et al., 2019; Chang and Chen, 2019), or linear transformations to a space where distances mirror dependency tree structures (Hewitt and Manning, 2019).1 In this work, we explore several unsupervised word-level representation extraction strategies and configurations for lexico-semantic tasks (i.e., probes), stemming from different combinations of the components detailed in Table 1 and illustrated in Figure 1. In particular, we assess the impact of: 1) encoding tokens with monolingual LM-pretrained Transformers vs. with their mas1 The interplay between"
2020.emnlp-main.586,2020.emnlp-main.185,1,0.865778,"Missing"
2020.emnlp-main.586,N19-1112,0,0.283984,"ults indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers. 1 Introduction and Motivation Language models (LMs) based on deep Transformer networks (Vaswani et al., 2017), pretrained on unprecedentedly large amounts of text, offer unmatched performance in virtually every NLP task (Qiu et al., 2020). Models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al.,"
2020.emnlp-main.586,K19-1004,1,0.869925,"Missing"
2020.emnlp-main.586,2021.ccl-1.108,0,0.114032,"Missing"
2020.emnlp-main.586,2020.tacl-1.54,0,0.0262276,"., 2017), pretrained on unprecedentedly large amounts of text, offer unmatched performance in virtually every NLP task (Qiu et al., 2020). Models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019)"
2020.emnlp-main.586,D19-1374,0,0.0238678,"BLI, the respective scores are 0.486 and 0.315 with ISO, and 0.503 and 0.334 with AOC-10. Similar observations hold for FI and ZH LSIM, and also in the RELP task. In RELP, it is notable that ‘BERT-based’ embeddings can recover more lexical relation knowledge than standard FT vectors. These findings reveal that pretrained LMs indeed implicitly capture plenty of lexical type-level knowledge (which needs to be ‘recovered’ from the models); this also suggests why pretrained LMs have been successful in tasks where this knowledge is directly useful, such as NER and POS tagging (Tenney et al., 2019; Tsai et al., 2019). Finally, we also note that gains with AOC over ISO are much more pronounced for the under-performing MULTI.* configurations: this indicates that MONO models store more lexical information even in absence of context. How Important are Special Tokens? The results reveal that the inclusion of special tokens [CLS] and [SEP] into type-level embedding extraction deteriorates the final lexical information contained in the embeddings. This finding holds for different languages, underlying LMs, and averaging across various layers. The NOSPEC configurations consistently outperform their ALL and WITHCL"
2020.emnlp-main.586,2020.emnlp-main.14,0,0.0225443,"or dependency tree structures (Hewitt and Manning, 2019).1 In this work, we explore several unsupervised word-level representation extraction strategies and configurations for lexico-semantic tasks (i.e., probes), stemming from different combinations of the components detailed in Table 1 and illustrated in Figure 1. In particular, we assess the impact of: 1) encoding tokens with monolingual LM-pretrained Transformers vs. with their mas1 The interplay between the complexity of a probe and its accuracy, as well as its effect on the overall procedure, remain controversial (Pimentel et al., 2020; Voita and Titov, 2020). 7223 Component Source LM Label Short Description MONO Language-specific (i.e., monolingually pretrained) BERT Multilingual BERT, pretrained on 104 languages (with shared subword vocabulary) MULTI ISO Context AOC -M NOSPEC Subword Tokens ALL WITHCLS Layerwise Avg Each vocabulary word w is encoded in isolation, without any external context Average-over-context: average over word’s encodings from M different contexts/sentences Special tokens [CLS] and [SEP] are excluded from subword embedding averaging Both special tokens [CLS] and [SEP] are included into subword embedding averaging [CLS] is in"
2020.emnlp-main.586,2020.cl-4.5,1,0.881404,"Missing"
2020.emnlp-main.586,D19-1575,0,0.0186957,"Averaging across layers bottom-to-top (i.e., from L0 to L12 ) is beneficial across the board, but we notice that scores typically saturate or even decrease in some tasks and languages when we include 8 For this reason, we report the results of tions only in the NOSPEC setting. AOC configurahigher layers into averaging: see the scores with *.AVG(L≤10) and *.AVG(L≤12) configurations, e.g., for FI LSIM; EN/DE RELP, and summary BLI and CLIR scores. This hints to the fact that two strategies typically used in prior work, either to take the vectors only from the embedding layer L0 (Wu et al., 2020; Wang et al., 2019) or to average across all layers (Liu et al., 2019b), extract suboptimal word representations for a wide range of setups and languages. The sweet spot for n in *.AVG(L≤n) configurations seems largely task- and language-dependent, as peak scores are obtained with different n-s. Whereas averaging across all layers generally hurts performance, the results strongly suggest that averaging across layer subsets (rather than selecting a single layer) is widely useful, especially across bottom-most layers: e.g., L ≤ 6 with MONO.ISO.NOSPEC yields an average score of 0.561 in LSIM, 0.076 in CLIR, and 0.4"
2020.repl4nlp-1.7,P19-1070,1,0.874475,"Missing"
2020.repl4nlp-1.7,P19-4007,1,0.895056,"Missing"
2020.repl4nlp-1.7,D18-1330,0,0.113978,"uli´c♦ Anna Korhonen♦ Goran Glavaš♣ ♦ Language Technology Lab, TAL, University of Cambridge ♣ Data and Web Science Group, University of Mannheim {iv250,alk23}@cam.ac.uk goran@informatik.uni-mannheim.de Abstract sources is the main reason for popularity of the socalled projection-based CLWE methods (Mikolov et al., 2013a; Artetxe et al., 2016, 2018a). These models align two independently trained monolingual word vector spaces post-hoc, using limited bilingual supervision in the form of several hundred to several thousand word translation pairs (Mikolov et al., 2013a; Vuli´c and Korhonen, 2016; Joulin et al., 2018; Ruder et al., 2018). Some models even align the monolingual spaces using only identical strings (Smith et al., 2017; Søgaard et al., 2018) or numerals (Artetxe et al., 2017). The most recent work focused on fully unsupervised CLWE induction: they extract seed translation lexicons relying on topological similarities between monolingual spaces (Conneau et al., 2018; Artetxe et al., 2018a; Hoshen and Wolf, 2018; Alaux et al., 2019). In this work, we do not focus on projection itself: rather, we investigate a transformation of input monolingual word vector spaces that facilitates the projection"
2020.repl4nlp-1.7,kamholz-etal-2014-panlex,0,0.0195343,"Missing"
2020.repl4nlp-1.7,N19-1162,0,0.0281812,"rs as well as in different BLI setups and with different CLWE methods. In future work, we will test other unsupervised post-processors, and also probe similar methods that inject external lexical knowledge into monolingual word vectors towards improved BLI. We also plan to probe if similar gains still hold with recently proposed more sophisticated self-learning methods (Karan et al., 2020), non-linear mappingbased CLWE methods (Glavaš and Vuli´c, 2020; Mohiuddin and Joty, 2020). Another idea is to also apply a similar principle to contextualised word representations in cross-lingual settings (Schuster et al., 2019; Liu et al., 2019). Mikel Artetxe, Gorka Labaka, Iñigo Lopez-Gazpio, and Eneko Agirre. 2018c. Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation. In Proceedings of CoNLL, pages 282–291. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the ACL, 5:135–146. Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018. Word translation without parallel data. In Proceedings of ICLR. Yerai Doval, Jose Camacho-Colla"
2020.repl4nlp-1.7,2020.acl-main.618,1,0.838234,"Missing"
2020.repl4nlp-1.7,C12-1089,0,0.0444421,"I setups), and in combination with two different projection methods. 1 Introduction Cross-lingual word embeddings (CLWEs) are a mainstay of modern cross-lingual NLP (Ruder et al., 2019b). CLWE models induce a shared cross-lingual vector space in which words with similar meanings obtain similar vectors regardless of their language. Their usefulness has been attested in tasks such as bilingual lexicon induction (BLI) (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Litschko et al., 2018), machine translation (Artetxe et al., 2018b; Lample et al., 2018), document classification (Klementiev et al., 2012), and many others (Ruder et al., 2019b). Importantly, CLWEs are one of the central mechanisms for facilitating transfer of language technologies for low-resource languages, which often lack sufficient bilingual signal for obvious transfer via machine translation. Lack of language re45 Proceedings of the 5th Workshop on Representation Learning for NLP (RepL4NLP-2020), pages 45–54 c July 9, 2020. 2020 Association for Computational Linguistics formation on both monolingual spaces before any standard projection-based CLWE framework yields consistent BLI gains for a wide array of languages. We run"
2020.repl4nlp-1.7,J98-1004,0,0.762655,"each self-learning iteration k, a dictionary D(k) is first used to learn the joint space (k) (k) Y (k) = XW x ∪ ZW z . The mutual crosslingual nearest neighbours in Y (k) are then used to extract the new dictionary D(k+1) . Relying on mutual nearest neighbours partially removes the noise, leading to better performance. For more technical Unsupervised Monolingual Post-processing. We now outline the simple post-processing method of Artetxe et al. (2018c) used in this work, and then extend it to the bilingual setup. The core idea is to generalise the notion of first-and second-order similarity (Schütze, 1998)2 to nth-order similarity. Let us define the (standard, first-order) similarity matrix of the source language space X as M1 (X) = XX T (similar for Z). The second-order similarity can then be defined as M2 (X) = XX T XX T , where it holds M2 (X) = M1 (M1 (X)); the nth-order similarity is then Mn (X) = (XX T )n . The embeddings of words wi and wj are given by the rows i and j of each Mn matrix. We are then looking for a general linear transformation that adjusts the similarity order of input 1 Recent empirical studies (Glavaš et al., 2019; Vuli´c et al., 2019) show that, under fair evaluation,"
2020.repl4nlp-1.7,K19-1004,1,0.879568,"Missing"
2020.repl4nlp-1.7,D19-1449,1,0.839786,"Missing"
2020.repl4nlp-1.7,P16-1024,1,0.882173,"Missing"
2020.semeval-1.2,P18-1073,0,0.0246037,"traints in target languages by translating EN constraints to target languages via Google Translate. A similar approach of automatic constraint translation has already been proven very effective in the context of symmetric similarity-based specialization of embedding spaces for low-resource languages (Ponti et al., 2019). This way, Wang et al. (2020) obtain an LE-specialized embedding space for each language. Following that, in the second step, they learn a linear projection mapping between the LE-specialized monolingual spaces with the VecMap tool for inducing bilingual word embedding spaces (Artetxe et al., 2018). Recent comparative evaluations (Glavaˇs et al., 2019; Vuli´c et al., 2019a) rendered VecMap as one of the most robust algorithms for inducing cross-lingual embedding spaces. The word translations obtained with Google Translate when translating EN constraints are also forwarded to VecMap as supervision for inducing bilingual embedding spaces. 5 Official Evaluation We now report the official results of our evaluation. We first describe the baselines (Section 5.1) and then show the performances for all submitted runs (Section 5.2).5 5.1 Baselines For the Dist track we use simple cosine similari"
2020.semeval-1.2,I13-1095,0,0.0123381,"-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relation holds between concepts (“To which degree is"
2020.semeval-1.2,Q17-1010,0,0.00755369,"bust algorithms for inducing cross-lingual embedding spaces. The word translations obtained with Google Translate when translating EN constraints are also forwarded to VecMap as supervision for inducing bilingual embedding spaces. 5 Official Evaluation We now report the official results of our evaluation. We first describe the baselines (Section 5.1) and then show the performances for all submitted runs (Section 5.2).5 5.1 Baselines For the Dist track we use simple cosine similarity between distributional word vectors as a baseline. To this end, we use the 300-dimensional FastText embeddings (Bojanowski et al., 2017) trained on Wikipedias of respective languages.6 For the cross-lingual (sub)tasks we induce the bilingual embedding spaces via the simple Procrustes alignment (Smith et al., 2017), using 5K word translation dictionaries, as described in Glavaˇs et al. (2019). Since LE is an asymmetric relation and cosine similarity is a symmetric measure, we did not expect this baseline to be particularly competitive and expected most participants to outperform it. For the Any track, we used GLEN, our recent neural explicit specialization model for LE (Glavaˇs and Vuli´c, 2019) as a competitive baseline. GLEN"
2020.semeval-1.2,S16-1168,0,0.059972,"lly validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014"
2020.semeval-1.2,D15-1075,0,0.0445674,"ded LE prediction. 1 Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degr"
2020.semeval-1.2,P15-2001,0,0.0651471,"Missing"
2020.semeval-1.2,S17-2002,0,0.336887,"we previously created and published (Vuli´c et al., 2017; Vuli´c et al., 2019b), covering four languages (EN, DE, IT, HR) and extended those datasets to two new languages (TR, SQ). For completeness, we describe the details of the annotation process and the creation of final multilingual and cross-lingual datasets for the shared task. Starting Point: Graded LE in English. HyperLex (Vuli´c et al., 2017) comprises 2,616 English (EN) word pairs (2,163 noun pairs and 453 verb pairs) annotated for the graded LE relation. Unlike in symmetric similarity datasets (Hill et al., 2015; Gerz et al., 2016; Camacho-Collados et al., 2017), word order in each pair (X, Y ) is important: this means that pairs (X, Y ) and (Y, X) can obtain drastically different graded LE ratings. The word pairs were first sampled from WordNet to represent a spectrum of different word relations (e.g., hyponymy-hypernymy, meronymy, co-hyponymy, synonymy, antonymy, no relation). The ratings in the [0, 6] interval were then collected through crowdsourcing by posing the graded LE “To what degree is X a type of Y?” question to human subjects, with each pair rated by at least 10 raters: the score of 6 indicates a perfect LE relation between the concepts"
2020.semeval-1.2,D18-1269,0,0.0318256,"n annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014;"
2020.semeval-1.2,N13-1073,0,0.0750154,"resume something similar to be the case with LE and the proposed 4lang graphs – it is inherently difficult to create a reliable LE score based on paths and distances in a symbolic representation that is a (directed) graph. Team UAlberta (Hauer et al., 2020). The approach of UAlberta for cross-lingual binary LE detection combines sentence-level translations (i.e., parallel corpora), distributional word vectors (i.e., word embeddings) and multilingual lexical resources. Their base method, dubbed BITEXT, mines candidates for cross-lingual LE from parallel corpora – they simply run the FastAlign (Dyer et al., 2013) word alignment algorithm and assume that the LE relation holds between all aligned pairs of words. As clarified by the authors, this will, in most cases, extract cross-lingual synonyms, which, strictly speaking, do satisfy the LE relation; also, in some cases, the alignments will be established between close (e.g., first order) hyponymy-hypernymy pairs – in this case, however, the bitext alignment of words alone does not suggest the direction of the LE relation. The authors simply declare any pair of words from our cross-lingual datasets to stand in the LE relation if they find this pair in t"
2020.semeval-1.2,ehrmann-etal-2014-representing,0,0.0286213,"re of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online),"
2020.semeval-1.2,E17-1056,1,0.777636,"all languages and language pairs, for both binary LE detection and graded LE prediction. 1 Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) sugg"
2020.semeval-1.2,N15-1184,0,0.0325169,"Missing"
2020.semeval-1.2,P14-1113,0,0.0244977,"has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 202"
2020.semeval-1.2,P05-1014,0,0.0147749,"Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models an"
2020.semeval-1.2,D16-1235,1,0.900545,"Missing"
2020.semeval-1.2,P18-1004,1,0.899108,"Missing"
2020.semeval-1.2,P19-1476,1,0.837489,"Missing"
2020.semeval-1.2,P19-1070,1,0.894937,"Missing"
2020.semeval-1.2,D17-1185,1,0.900423,"Missing"
2020.semeval-1.2,P16-1193,0,0.0238705,"SQ). We offered two different evaluation tracks. In the distributional (Dist) track we allowed only for fully distributional systems, capturing LE only on the basis of unannotated corpora. In contrast, the Any track invited systems that exploit any kind of additional external resources, including lexico-semantic networks. Overall, we did not observe any empirically confirmed strong systems in the Dist track, further corroborating the findings from prior work that building LE-oriented vectors distributionally is more difficult than for some other relations such as broader semantic relatedness (Henderson and Popa, 2016). However, several runs submitted to the Any track pushed the state of the art both in binary LE detection and graded LE prediction, for most of the languages and language pairs in our evaluation. 2 Data We started from the LE datasets we previously created and published (Vuli´c et al., 2017; Vuli´c et al., 2019b), covering four languages (EN, DE, IT, HR) and extended those datasets to two new languages (TR, SQ). For completeness, we describe the details of the annotation process and the creation of final multilingual and cross-lingual datasets for the shared task. Starting Point: Graded LE in"
2020.semeval-1.2,J15-4004,1,0.896991,"Data We started from the LE datasets we previously created and published (Vuli´c et al., 2017; Vuli´c et al., 2019b), covering four languages (EN, DE, IT, HR) and extended those datasets to two new languages (TR, SQ). For completeness, we describe the details of the annotation process and the creation of final multilingual and cross-lingual datasets for the shared task. Starting Point: Graded LE in English. HyperLex (Vuli´c et al., 2017) comprises 2,616 English (EN) word pairs (2,163 noun pairs and 453 verb pairs) annotated for the graded LE relation. Unlike in symmetric similarity datasets (Hill et al., 2015; Gerz et al., 2016; Camacho-Collados et al., 2017), word order in each pair (X, Y ) is important: this means that pairs (X, Y ) and (Y, X) can obtain drastically different graded LE ratings. The word pairs were first sampled from WordNet to represent a spectrum of different word relations (e.g., hyponymy-hypernymy, meronymy, co-hyponymy, synonymy, antonymy, no relation). The ratings in the [0, 6] interval were then collected through crowdsourcing by posing the graded LE “To what degree is X a type of Y?” question to human subjects, with each pair rated by at least 10 raters: the score of 6 in"
2020.semeval-1.2,W19-4310,1,0.900241,"Missing"
2020.semeval-1.2,P15-2020,1,0.902688,"Missing"
2020.semeval-1.2,S15-1019,0,0.138539,"in more detail the approaches adopted by the three teams who submitted their system description papers.3 Team BMEAUT (Kov´acs et al., 2020). The BMEAUT method for LE detection and prediction is a rule-based approach that exploits Wiktionary definitions (Meyer and Gurevych, 2012) and relies on dependency parsing and semantic graphs. In the first step, the authors apply the dict to 4lang tool (Recski et al., 2016) on Wiktionary definitions of concepts (which can be both unigrams and multi-word expressions, i.e., phrases) in order to induce the directed graphs conforming to the 4lang formalism (Kornai et al., 2015). 4lang graphs are directed graphs with concepts as nodes and three types of edges: 0 0 edges of type 0 denote attribution (cat → − four-legged), lexical entailment (cat → − mammal), or 0 unary predication (cat → − meow); edges of type 1 and 2 denote relations between the predicate and its 1 2 subject and object, respectively (e.g., cat ← − catch → − mouse).4 Kov´acs et al. (2020) first extract definitions from Wiktionary using language-specific templates. Each definition is then transformed into a 4lang graph with the help of a language-specific Universal Dependencies (Nivre et al., 2016) par"
2020.semeval-1.2,P19-1313,0,0.0113187,"a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relation holds between concepts (“To which degree is X a type of Y?”).1 The graded nature of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License."
2020.semeval-1.2,Q15-1016,0,0.0406784,"rediction for SQ) and each language-pair in (3) and (4) (e.g., binary LE detection for HR-TR) instantiates one concrete subtask. We allowed participants to submit their predictions for an arbitrary set of subtasks. Moreover, the participants were allowed to tackle only graded LE prediction or only binary LE detection. Evaluation Metrics. For each graded LE prediction subtask, we measured the alignment of predictions and gold LE scores using the Spearman’s Rank Correlation Coefficient (Spearman ρ), which is in line with previous work on similar concept pair scoring datasets (Hill et al., 2015; Levy et al., 2015; Vuli´c et 27 al., 2017, inter alia). For the binary LE detection subtasks we resorted to the standard F1 measure. 4 Participating Systems We now describe in more detail the approaches adopted by the three teams who submitted their system description papers.3 Team BMEAUT (Kov´acs et al., 2020). The BMEAUT method for LE detection and prediction is a rule-based approach that exploits Wiktionary definitions (Meyer and Gurevych, 2012) and relies on dependency parsing and semantic graphs. In the first step, the authors apply the dict to 4lang tool (Recski et al., 2016) on Wiktionary definitions of"
2020.semeval-1.2,S10-1002,0,0.040472,"egree is X a type of Y?”).1 The graded nature of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluati"
2020.semeval-1.2,W13-0904,0,0.0173071,"c lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relation holds between concepts (“To which degree is X a type of Y?”).1 The graded nature of the"
2020.semeval-1.2,Q17-1022,1,0.903619,"Missing"
2020.semeval-1.2,S13-2005,0,0.0319147,"”).1 The graded nature of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barc"
2020.semeval-1.2,D18-1026,1,0.886103,"Missing"
2020.semeval-1.2,D19-1226,1,0.869316,"Missing"
2020.semeval-1.2,W16-1622,0,0.0342201,"Missing"
2020.semeval-1.2,P18-2057,0,0.0129808,"died in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical entailment from multiple perspectives. Namely, the su"
2020.semeval-1.2,E14-4008,0,0.0287191,"Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared ta"
2020.semeval-1.2,P16-1226,0,0.0129442,"ection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reaso"
2020.semeval-1.2,E17-1007,0,0.0156047,"graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical enta"
2020.semeval-1.2,P06-1101,0,0.114198,"em runs that push state-of-the-art across all languages and language pairs, for both binary LE detection and graded LE prediction. 1 Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vaguen"
2020.semeval-1.2,N18-1056,0,0.334657,".org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical entailment from multiple perspectives. Namely, the subtasks covered both monolingual and cross-lingual setups as well as both binary LE detection and graded LE prediction (i.e., prediction of a degree to which LE holds between concepts"
2020.semeval-1.2,N18-1103,1,0.907335,"Missing"
2020.semeval-1.2,J17-4004,1,0.90587,"Missing"
2020.semeval-1.2,N18-1048,1,0.882981,"Missing"
2020.semeval-1.2,D19-1449,1,0.894355,"Missing"
2020.semeval-1.2,P19-1490,1,0.629745,"Missing"
2020.semeval-1.2,N16-1142,0,0.0991043,"http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical entailment from multiple perspectives. Namely, the subtasks covered both monolingual and cross-lingual setups as well as both binary LE detection and graded LE prediction (i.e., prediction of a degree to which LE"
2020.semeval-1.2,2020.semeval-1.31,0,0.482762,"quire a bilingual word embedding space, merely two monolingual word embedding spaces: sets X and Y are obtained by thresholding monolingual word embedding similarities (the threshold value is tuned on the development portions of our LE sets). Finally, all possible pairs (xi , yj ) ∈ X × Y are considered to stand in the LE relation. Finally, in the third run, the authors couple the bitext-based FastText aligner with the BABEL A LIGN algorithm, which aligns concepts across languages based on BabelNet (Navigli and Ponzetto, 2012), a massively multilingual lexico-semantic network. Team SHIKEBLCU (Wang et al., 2020). The approach of Wang et al. (2020) extends the wellestablished line of work based on specializing (i.e., fine-tuning) distributional word vectors for lexical relations, be it symmetric semantic similarity (Faruqui et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c et al., 2018; Glavaˇs and Vuli´c, 2018; Ponti et al., 2018) or the asymmetric LE relation (Vuli´c and Mrkˇsi´c, 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019; Vuli´c et al., 2019b), using constraints from external lexico-semantic resources like WordNet for supervision. At the core of the approach is the Lexical-Entailment Attract Re"
2020.semeval-1.2,C14-1212,0,0.0213805,"ordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this"
2020.semeval-1.2,N18-1101,0,0.0246649,"Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relat"
2020.wanlp-1.17,2020.acl-main.485,0,0.230454,"Missing"
2020.wanlp-1.17,Q17-1010,0,0.775331,"er programmer as woman is to homemaker”, which is algebraically encoded in the embedding ~ ~ ~ ~ space with the analogical relation man− computer programmer ≈ woman− homemaker (Bolukbasi et al., 2016). The existence of such biases in word embeddings stems from the combination of (1) human biases manifesting themselves in terms of word co-occurrences (e.g., the word woman appearing in a training corpus much more often in the context of homemaker than together with computer programmer) and (2) the distributional nature of the word embedding models (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017), which induce word vectors precisely by exploiting word co-occurrences, i.e., thus also encoding the human biases as a (negative) side-effect, which represents, expressed according to the taxnomy of harms proposed by Blodgett et al. (2020), a representational harm, more specifically, stereotyping. In order to quantify the amount of bias in word embeddings, Caliskan et al. (2017) proposed the Word Embedding Association Test (WEAT), which is based on the associative difference in terms of semantic similarity between two sets of target terms, e.g., male and female terms, towards two sets of attr"
2020.wanlp-1.17,S17-2001,0,0.164749,"Online), December 12, 2020 focus on the multi-dimensional analysis of biases in Arabic word embeddings. The motivation for this work is twofold: (1) Arabic is one of the most widely spoken languages in the world:1 this means that the biases encoded in language technology for Arabic have the potential for affecting more people than for most other languages; (2) language resources for Arabic – large corpora (Goldhahn et al., 2012), pretrained word embeddings (Mohammad et al., 2017; Bojanowski et al., 2017), and datasets for measuring semantic quality of Arabic embeddings (Elrazzaz et al., 2017; Cer et al., 2017) – are publicly available, allowing for the analyses of biases that these resources potentially hide. As a first step in the analysis of language technology biases for Arabic, we present A RAWEAT, an Arabic extension to the multilingual XWEAT framework (Lauscher and Glavaˇs, 2019). Because the WEAT test (Caliskan et al., 2017), though it has the notable advantage of drawing inspiration from psychology literature, has recently been shown to systematically overestimate the bias present in an embedding space (Ethayarajh et al., 2019), in this work, we couple it with several other bias tests, desi"
2020.wanlp-1.17,P17-2072,0,0.0168841,"–199 Barcelona, Spain (Online), December 12, 2020 focus on the multi-dimensional analysis of biases in Arabic word embeddings. The motivation for this work is twofold: (1) Arabic is one of the most widely spoken languages in the world:1 this means that the biases encoded in language technology for Arabic have the potential for affecting more people than for most other languages; (2) language resources for Arabic – large corpora (Goldhahn et al., 2012), pretrained word embeddings (Mohammad et al., 2017; Bojanowski et al., 2017), and datasets for measuring semantic quality of Arabic embeddings (Elrazzaz et al., 2017; Cer et al., 2017) – are publicly available, allowing for the analyses of biases that these resources potentially hide. As a first step in the analysis of language technology biases for Arabic, we present A RAWEAT, an Arabic extension to the multilingual XWEAT framework (Lauscher and Glavaˇs, 2019). Because the WEAT test (Caliskan et al., 2017), though it has the notable advantage of drawing inspiration from psychology literature, has recently been shown to systematically overestimate the bias present in an embedding space (Ethayarajh et al., 2019), in this work, we couple it with several oth"
2020.wanlp-1.17,P19-1166,0,0.0156101,"easuring semantic quality of Arabic embeddings (Elrazzaz et al., 2017; Cer et al., 2017) – are publicly available, allowing for the analyses of biases that these resources potentially hide. As a first step in the analysis of language technology biases for Arabic, we present A RAWEAT, an Arabic extension to the multilingual XWEAT framework (Lauscher and Glavaˇs, 2019). Because the WEAT test (Caliskan et al., 2017), though it has the notable advantage of drawing inspiration from psychology literature, has recently been shown to systematically overestimate the bias present in an embedding space (Ethayarajh et al., 2019), in this work, we couple it with several other bias tests, designed to capture and quantify other aspects of human biases: Embedding Coherence Test (Dev and Phillips, 2019), Bias Analogy Test (Lauscher et al., 2020) and Implicit Bias Tests (Gonen and Goldberg, 2019). Our work, which is to the best of our knowledge the first study on quantifying biases in Arabic distributional word vector spaces, yields some interesting findings: biases seem more prominent in vectors trained on texts written in Egyptian Arabic than those written in Modern Standard Arabic (MSA). Also, the implicit gender bias i"
2020.wanlp-1.17,goldhahn-etal-2012-building,0,0.16885,"ternational License. creativecommons.org/licenses/by/4.0/. License details: http:// 192 Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 192–199 Barcelona, Spain (Online), December 12, 2020 focus on the multi-dimensional analysis of biases in Arabic word embeddings. The motivation for this work is twofold: (1) Arabic is one of the most widely spoken languages in the world:1 this means that the biases encoded in language technology for Arabic have the potential for affecting more people than for most other languages; (2) language resources for Arabic – large corpora (Goldhahn et al., 2012), pretrained word embeddings (Mohammad et al., 2017; Bojanowski et al., 2017), and datasets for measuring semantic quality of Arabic embeddings (Elrazzaz et al., 2017; Cer et al., 2017) – are publicly available, allowing for the analyses of biases that these resources potentially hide. As a first step in the analysis of language technology biases for Arabic, we present A RAWEAT, an Arabic extension to the multilingual XWEAT framework (Lauscher and Glavaˇs, 2019). Because the WEAT test (Caliskan et al., 2017), though it has the notable advantage of drawing inspiration from psychology literature"
2020.wanlp-1.17,W19-3621,0,0.422201,"tional harm, more specifically, stereotyping. In order to quantify the amount of bias in word embeddings, Caliskan et al. (2017) proposed the Word Embedding Association Test (WEAT), which is based on the associative difference in terms of semantic similarity between two sets of target terms, e.g., male and female terms, towards two sets of attribute terms, e.g., career and family terms. Most recently, the WEAT test, measuring the degree of explicit bias in the distributional space, has been coupled with other tests, aiming to measure other aspects of bias, such as the amount of implicit bias (Gonen and Goldberg, 2019) or the presence of the analogical bias (Lauscher et al., 2020). While there is evidence that distributional vectors often encode human biases, the amount of biases does not seem to be universal across different languages and corpora, as recently shown by Lauscher and Glavaˇs (2019) in the analysis of distributional biases across seven different languages. In this work, we This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 192 Proceedings of the Fifth Arabic Natural Language Processing Workshop, p"
2020.wanlp-1.17,W19-3822,0,0.0237785,"from a set of attribute terms. They also propose several debiasing methods. Gonen and Goldberg (2019) show that many debiasing methods only mask but do not fully remove biases present in the embedding spaces. They propose to additionally test for 197 implicit biases, by trying to classify or cluster the sets of target terms. Lauscher et al. (2020) unify the different notions of biases into explicit and implicit bias specifications, based on which they propose methods for quantifying and removing biases. While their is some effort to account for gender-awareness in Arabic machine translation (Habash et al., 2019), we are, to the best of our knowledge, the first to measure bias in Arabic Language Technology. 5 Conclusion Language technologies aim to avoid reflecting negative human biases such as racism and sexism. Yet, the ubiquitous word embeddings, used as input for many NLP models, seem to encode many such biases. In this work, we extensively quantify and analyze the biases in different vector spaces built from text in Arabic, a major world language with close to 300M native speakers. To this effect, we translate existing bias specifications from English to Arabic and investigate biases in embedding"
2020.wanlp-1.17,D19-1530,0,0.0194305,"cts (e.g., ant, flea) Weapons (e.g., gun) Arts (e.g., poetry) Arts Mental (e.g., sad) Pleasant (e.g., health) Pleasant Male (e.g., brother, son) Male Long-term (e.g., always) Unpleasant (e.g., abuse) Unpleasant Female (e.g., woman, sister) Female Short-term (e.g., occasional) Table 2: WEAT bias tests. cases; and even in those cases the MSA translation is also in usage in other Arabic dialects.2 We further omitted WEAT tests 3–6 and 10 as they are based on proper names. While it has been shown that names are a good proxy for identifying and removing bias towards specific groups of people (Hall Maudslay et al., 2019), it is difficult to “translate” them.3 As an example of the resulting AraWEAT test, Table 1 list the Arabic translation of WEAT test T 7. An overview on the remaining tests with their respective target and attribute term sets is provided in Table 2. T1 ECT BAT KM T2 W ECT BAT KM T7 ECT BAT KM T8 ECT BAT KM Model Lang FT A RABIC FT E GYPT MSA 0.85 0.69 0.47 0.71 0.51* 0.62 0.43 0.63 -0.15* 0.17* 0.5 0.56 0.05* 0.02* 0.44 0.56 Egyptian 1.17 0.45 0.49 0.95 0.97 0.56 0.51 0.65 0.65* 0.54 0.51 0.6 0.09* 0.63 0.47 0.6 W W W AV SG W IKI MSA AV SG T WITTER Mixed 0.27* 0.82 0.49 0.62 0.98 0.61 0.43 0."
2020.wanlp-1.17,S19-1010,1,0.634103,"Missing"
2020.wanlp-1.17,D14-1162,0,0.0845284,"example “Man is to computer programmer as woman is to homemaker”, which is algebraically encoded in the embedding ~ ~ ~ ~ space with the analogical relation man− computer programmer ≈ woman− homemaker (Bolukbasi et al., 2016). The existence of such biases in word embeddings stems from the combination of (1) human biases manifesting themselves in terms of word co-occurrences (e.g., the word woman appearing in a training corpus much more often in the context of homemaker than together with computer programmer) and (2) the distributional nature of the word embedding models (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017), which induce word vectors precisely by exploiting word co-occurrences, i.e., thus also encoding the human biases as a (negative) side-effect, which represents, expressed according to the taxnomy of harms proposed by Blodgett et al. (2020), a representational harm, more specifically, stereotyping. In order to quantify the amount of bias in word embeddings, Caliskan et al. (2017) proposed the Word Embedding Association Test (WEAT), which is based on the associative difference in terms of semantic similarity between two sets of target terms, e.g., male and female terms"
2020.wanlp-1.17,D19-1531,0,0.0263612,"Missing"
2021.acl-long.151,W19-3805,0,0.0517219,"Missing"
2021.acl-long.151,2020.acl-tutorials.2,0,0.0608011,"Missing"
2021.acl-long.151,2020.acl-main.485,0,0.0620893,"Missing"
2021.acl-long.151,N19-3002,0,0.0282129,"presentations of t1 and t2 from the output LM layer (i.e., output embeddings of t1 and t2),12 and cos denotes the cosine similarity. ADD forces the output representations of target terms from the dominant group (e.g., christian) to be equally distant to the representation of a stereotypical attribute for the minoritized group (e.g., dangerous) as the representations of corresponding target terms denoting the minoritized group (e.g., muslim). Similar to LMD, for all occurrences of a ∈ A1 , the final loss is the weighted sum of LLM and LADD , see Eq. (2). 4.3 Hard Debiasing Loss (HD) Similar to Bordia and Bowman (2019), we next devise a loss based on the idea of hard debiasing from Bolukbasi et al. (2016). We compute this loss in two steps: (1) identification of the bias subspace, and (2) neutralization of the attribute words w.r.t. to the previously identified bias subspace. (1) Bias Subspace Identification. We start from the same set of manually curated target term pairs P as in LMD and ADD. Let t be the output vector of some term t from the LM head. We then obtain partial bias vectors bi for pairs (t1i , t2i ) ∈ P by computing the differences between t1i and t2i : bi = (t1i − t2i )/2. We then stack the p"
2021.acl-long.151,W19-3655,0,0.0712183,"Missing"
2021.acl-long.151,2020.acl-main.488,0,0.0834776,"Missing"
2021.acl-long.151,2020.coling-main.446,0,0.0863191,"Missing"
2021.acl-long.151,N19-1063,0,0.0270244,"ammer as woman is to homemaker”, Bolukbasi et al. (2016) first drew attention 15 Two exceptions, which requires further investigation are DST performance drops of LMD when debiasing for Race and of ADD when debiasing for Gender. to the issue. Caliskan et al. (2017) presented the Word Embedding Association Test (WEAT), quantifying the bias between two sets of target terms towards two sets of attribute terms. Subsequent work proposed extensions to further embedding models (Liang et al., 2020a,b) and languages (e.g., McCurdy and Serbetci, 2020; Lauscher and Glavaˇs, 2019; Lauscher et al., 2020b; May et al., 2019), analyses of the proposed measures (e.g., Gonen and Goldberg, 2019; Ethayarajh et al., 2019), more comprehensive evaluation frameworks (Lauscher et al., 2020a), new debiasing approaches (Dev and Phillips, 2019; Karve et al., 2019) and task-specific bias measures and resources for tasks like coreference resolution (Zhao et al., 2018), machine translation (Stanovsky et al., 2019) and natural language inference (Dev et al., 2020). In our work, we similarly acknowledge the importance of understanding bias w.r.t. downstream tasks, but focus on dialog systems, for which the landscape of research ef"
2021.acl-long.151,P17-1163,0,0.0746699,"Missing"
2021.acl-long.151,P19-1164,0,0.022803,"owards two sets of attribute terms. Subsequent work proposed extensions to further embedding models (Liang et al., 2020a,b) and languages (e.g., McCurdy and Serbetci, 2020; Lauscher and Glavaˇs, 2019; Lauscher et al., 2020b; May et al., 2019), analyses of the proposed measures (e.g., Gonen and Goldberg, 2019; Ethayarajh et al., 2019), more comprehensive evaluation frameworks (Lauscher et al., 2020a), new debiasing approaches (Dev and Phillips, 2019; Karve et al., 2019) and task-specific bias measures and resources for tasks like coreference resolution (Zhao et al., 2018), machine translation (Stanovsky et al., 2019) and natural language inference (Dev et al., 2020). In our work, we similarly acknowledge the importance of understanding bias w.r.t. downstream tasks, but focus on dialog systems, for which the landscape of research efforts is surprisingly scarce. (2) Bias in Language Generation. Dialog systems crucially depend on natural language generation (NLG) models. Yeo and Chen (2020) experimented with gender bias in word embeddings for NLG. Sheng et al. (2019) introduce the notion of a regard for a demographic, and compile a data set and devise a bias classification model based on that notion. Webster"
2021.acl-long.151,P19-1159,0,0.0152656,"(see Tables 4 and 5).15 Interestingly, while LMD drastically increases the perplexity on Reddit utterances (Figure 1b; see LMP in §3) this does not have negative consequences on DST and CRG. To summarize, from the benchmarked debiasing methods, HD and CDA are able to significantly reduce the bias and preserve conversational capabilities; Our results suggest that the dialog performance would remain unaffected even if HD and CDA are to be applied more than once, in order to mitigate multiple bias types. 6 Related Work For a comprehensive overview of work on bias in NLP, we refer the reader to (Sun et al., 2019; Blodgett et al., 2020; Shah et al., 2020). Here, we provide (1) a brief overview of bias measures and mitigation methods and their usage in (2) language generation and, specifically, in (3) dialog. (1) Bias in NLP. Resources, measures, and mitigation methods largely target static word embedding models: with their famous analogy “man is to computer programmer as woman is to homemaker”, Bolukbasi et al. (2016) first drew attention 15 Two exceptions, which requires further investigation are DST performance drops of LMD when debiasing for Race and of ADD when debiasing for Gender. to the issue."
2021.acl-long.410,Q17-1010,0,0.0713222,"nt mapping-based approach (Mikolov et al., 2013a; Smith et al., 2017) with the V EC M AP framework (Artetxe et al., 2018). We run main BLI evaluations for 10 language pairs spanning EN, DE, RU, FI, TR.7 Word Vocabularies and Baselines. We extract decontextualized type-level WEs in each language both from the original BERTs (termed BERT- REG)4 and the L EX F IT-ed BERT models for exactly the same vocabulary. Following Vuli´c et al. (2020), the vocabularies cover the top 100K most frequent words represented in the respective fastText (FT) vectors, trained on lowercased monolingual Wikipedias by Bojanowski et al. (2017).5 The equivalent vocabulary coverage allows for a direct comparison of all WEs regardless of the induction/extraction method; this also includes the FT Task 3: Lexical Relation Prediction (RELP). We assess the usefulness of lexical knowledge in WEs to learn relation classifiers for standard lexical relations (i.e., synonymy, antonymy, hypernymy, meronymy, plus no relation) via a state-ofthe-art neural model for RELP which learns solely based on input type-level WEs (Glavaš and Vuli´c, 2018). We use the WordNet-based evaluation data of Glavaš and Vuli´c (2018) for EN, DE, ES; they contain 10K"
2021.acl-long.410,2020.acl-main.431,0,0.547689,"ll pipeline for obtaining decontextualized word representations, based on lexically fine-tuning pretrained LMs via dual-encoder networks (Step 1, §2.1), and then extracting the representations from their (fine-tuned) layers (Step 2, §2.2). Introduction Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017). This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkši´c et al., 2017; Lauscher et al., 2020). Our work takes inspiration from the methods to correc"
2021.acl-long.410,P13-1133,0,0.0173955,"LI fall under similarity-based evaluation tasks (Ruder et al., 2019). sults demonstrate that the inexpensive lexical finetuning procedure can indeed turn large pretrained LMs into effective decontextualized word encoders, and this can be achieved for a reasonably wide spectrum of languages for which such pretrained LMs exist. What is more, L EX F IT for all nonEN languages has been run with noisy automatically translated lexical constraints, which holds promise to support even stronger static L EX F ITbased WEs with human-curated data in the future, e.g., extracted from multilingual WordNets (Bond and Foster, 2013), PanLex (Kamholz et al., 2014), or BabelNet (Ehrmann et al., 2014). The results give rise to additional general implications. First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vuli´c et al., 2020); the role of L EX F IT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision. To further investigate the ‘rewiring’ hypothesis, in §4.1, we also run L EX F IT with a drastically reduced amount of external knowledge. BERT- REG vectors display larg"
2021.acl-long.410,D18-2029,0,0.136669,"Missing"
2021.acl-long.410,2020.conll-1.17,0,0.026315,"g decontextualized word representations, based on lexically fine-tuning pretrained LMs via dual-encoder networks (Step 1, §2.1), and then extracting the representations from their (fine-tuned) layers (Step 2, §2.2). Introduction Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017). This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkši´c et al., 2017; Lauscher et al., 2020). Our work takes inspiration from the methods to correct these distortions and"
2021.acl-long.410,2020.emnlp-main.367,0,0.0318772,"0.24 to 0.60 for FI ; BLI scores for EN - FI rise from 0.21 to 0.37), it cannot match the absolute performance peaks of L EX F IT-ed monolingual BERTs. Storing the knowledge of 100+ languages in its limited parameter budget, mBERT still cannot capture monolingual knowledge as accurately as language-specific BERTs (Conneau et al., 2020). However, we believe that its performance with L EX F IT may be further improved by leveraging recently proposed multilingual LM adaptation strategies that mitigate a mismatch between shared multilingual and language-specific vocabularies (Artetxe et al., 2020; Chung et al., 2020; Pfeiffer et al., 2020); we leave this for future work. Layerwise Averaging. A consensus in prior work (Tenney et al., 2019; Ethayarajh, 2019; Vuli´c et al., 2020) points that out-of-context lexical knowledge in pretrained LMs is typically stored in bottom Transformer layers (see Table 5). However, Table 5 also reveals that this does not hold after L EX F ITing: the tuned model requires knowledge from all layers to extract effective decontextualized WEs and reach peak task scores. Effectively, this means 11 When sampling all reduced sets, we again deliberately excluded all words occurring in"
2021.acl-long.410,2020.acl-main.747,0,0.116681,"Missing"
2021.acl-long.410,N19-1423,0,0.259071,"cal tasks, also directly questioning the usefulness of traditional WE models in the era of large neural models. 1 Word embedding extraction BERT LexFit loss u 1. SOFTMAX 2. MNEG 3. MSIM w v Pooling Pooling BERT BERT (w, v) = (dormant, asleep) Step 1: Lexical ﬁne-tuning Figure 1: Illustration of the full pipeline for obtaining decontextualized word representations, based on lexically fine-tuning pretrained LMs via dual-encoder networks (Step 1, §2.1), and then extracting the representations from their (fine-tuned) layers (Step 2, §2.2). Introduction Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than aut"
2021.acl-long.410,ehrmann-etal-2014-representing,0,0.112194,". sults demonstrate that the inexpensive lexical finetuning procedure can indeed turn large pretrained LMs into effective decontextualized word encoders, and this can be achieved for a reasonably wide spectrum of languages for which such pretrained LMs exist. What is more, L EX F IT for all nonEN languages has been run with noisy automatically translated lexical constraints, which holds promise to support even stronger static L EX F ITbased WEs with human-curated data in the future, e.g., extracted from multilingual WordNets (Bond and Foster, 2013), PanLex (Kamholz et al., 2014), or BabelNet (Ehrmann et al., 2014). The results give rise to additional general implications. First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vuli´c et al., 2020); the role of L EX F IT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision. To further investigate the ‘rewiring’ hypothesis, in §4.1, we also run L EX F IT with a drastically reduced amount of external knowledge. BERT- REG vectors display large gains over FT vectors in tasks such as RELP and LexSIMP, again hi"
2021.acl-long.410,D19-1006,0,0.449664,"the era of large neural models. 1 Word embedding extraction BERT LexFit loss u 1. SOFTMAX 2. MNEG 3. MSIM w v Pooling Pooling BERT BERT (w, v) = (dormant, asleep) Step 1: Lexical ﬁne-tuning Figure 1: Illustration of the full pipeline for obtaining decontextualized word representations, based on lexically fine-tuning pretrained LMs via dual-encoder networks (Step 1, §2.1), and then extracting the representations from their (fine-tuned) layers (Step 2, §2.2). Introduction Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vuli´"
2021.acl-long.410,N15-1184,0,0.0368031,"or static WEs. In particular, the process known as semantic specialization (or retrofitting) injects information about lexical relations from databases like WordNet (Beckwith et al., 1991) or the Paraphrase Database (Ganitkevitch et al., 2013) into WEs. Thus, it accentuates relationships of pure semantic similarity in the re5269 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5269–5283 August 1–6, 2021. ©2021 Association for Computational Linguistics fined representations (Faruqui et al., 2015; Mrkši´c et al., 2017; Ponti et al., 2019, inter alia). Our goal is to create representations that take advantage of both 1) the expressivity and lexical knowledge already stored in pretrained language models (LMs) and 2) the precision of lexical finetuning. To this effect, we develop L EX F IT, a versatile lexical fine-tuning framework, illustrated in Figure 1, drawing a parallel with universal sentence encoders like SentenceBERT (Reimers and Gurevych, 2019).1 Our working hypothesis, extensively evaluated in this paper, is as follows: pretrained encoders store a wealth of lexical knowledge,"
2021.acl-long.410,N13-1092,0,0.115153,"Missing"
2021.acl-long.410,D16-1235,1,0.882037,"Missing"
2021.acl-long.410,2020.acl-main.247,0,0.0241888,"FTMAX SOFTMAX Table 4: LexSIMP results (Accuracy ×100). BERTs. However, there are differences across their task performance: the ranking-based MNEG and MSIM variants display stronger performance on similarity-based ranking lexical tasks such as LSIM and BLI. The classification-based SOFTMAX objective is, as expected, better aligned with the RELP task, and we note slight gains with its ternary variant which leverages extra antonymy knowledge. This finding is well aligned with the recent findings demonstrating that task-specific pretraining results in stronger (sentence-level) task performance (Glass et al., 2020; Henderson et al., 2020; Lewis et al., 2020). In our case, we show that task-specific lexical fine-tuning can reshape the underlying LM’s parameters to not only act as a universal word encoder, but also towards a particular lexical task. The per-epoch time measurements from Table 1 validate the efficiency of L EX F IT as a post-training fine-tuning procedure. Previous approaches that attempted to inject lexical information (i.e., word senses and relations) into large LMs (Lauscher et al., 2020; Levine et al., 2020) relied on joint LM (re)training from scratch: it is effectively costlier than"
2021.acl-long.410,P15-2011,1,0.825395,"age.8 Task 4: Lexical Simplification (LexSIMP) aims to automatically replace complex words (i.e., specialized terms, less-frequent words) with their simpler in-context synonyms, while retaining grammaticality and conveying the same meaning as the more complex input text (Paetzold and Specia, 2017). Therefore, discerning between semantic similarity (e.g., synonymy injected via L EX F IT) and broader relatedness is critical for LexSIMP (Glavaš and Vuli´c, 2018). We adopt the standard LexSIMP evaluation protocol used in prior research on static WEs (Ponti et al., 2018, 2019). 1) We use Light-LS (Glavaš and Štajner, 2015), a languageagnostic LexSIMP tool that makes simplifications in an unsupervised way based solely on word similarity in an input (static) WE space; 2) we rely on standard LexSIMP benchmarks, available for EN (Horn et al., 2014), IT (Tonelli et al., 2016), and ES (Saggion, 2017); and 3) we report the standard Accuracy scores (Horn et al., 2014).9 Important Disclaimer. We note that the main purpose of the chosen evaluation tasks and experimental protocols is not necessarily achieving state-ofthe-art performance, but rather probing the vectors in different lexical tasks requiring different types o"
2021.acl-long.410,N18-2029,1,0.905965,"Missing"
2021.acl-long.410,P18-1004,1,0.888961,"Missing"
2021.acl-long.410,P19-1070,1,0.911229,"Missing"
2021.acl-long.410,2020.findings-emnlp.196,1,0.890486,"Missing"
2021.acl-long.410,J15-4004,1,0.91951,"a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017). This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkši´c et al., 2017; Lauscher et al., 2020). Our work takes inspiration from the methods to correct these distortions and complement the distributional signal with structured information, which were originally devised for static WEs. In particular, the process known as semantic specialization (or retrofitting) injects information about lexical relations from databases like WordNet (Beckwith et al., 1991) or the Paraphrase Database (Gan"
2021.acl-long.410,P14-2075,0,0.0258555,"eaning as the more complex input text (Paetzold and Specia, 2017). Therefore, discerning between semantic similarity (e.g., synonymy injected via L EX F IT) and broader relatedness is critical for LexSIMP (Glavaš and Vuli´c, 2018). We adopt the standard LexSIMP evaluation protocol used in prior research on static WEs (Ponti et al., 2018, 2019). 1) We use Light-LS (Glavaš and Štajner, 2015), a languageagnostic LexSIMP tool that makes simplifications in an unsupervised way based solely on word similarity in an input (static) WE space; 2) we rely on standard LexSIMP benchmarks, available for EN (Horn et al., 2014), IT (Tonelli et al., 2016), and ES (Saggion, 2017); and 3) we report the standard Accuracy scores (Horn et al., 2014).9 Important Disclaimer. We note that the main purpose of the chosen evaluation tasks and experimental protocols is not necessarily achieving state-ofthe-art performance, but rather probing the vectors in different lexical tasks requiring different types of lexical knowledge,10 and offering fair and insightful comparisons between different L EX F IT variants, as well as against standard static WEs (fastText) and non-tuned BERT-based static WEs. 4 Results and Discussion The main"
2021.acl-long.410,2021.eacl-main.10,0,0.0341408,"ss diverse languages in controlled evaluations, thus directly questioning the practical usefulness of the traditional WE models in modern NLP. Besides inducing better static WEs for lexical tasks, following the line of lexical probing work (Ethayarajh, 2019; Vuli´c et al., 2020), our goal in this work was to understand how (and how much) lexical semantic knowledge is coded in pretrained LMs, and how to ‘unlock’ the knowledge from the LMs. We hope that our work will be beneficial for all lexical tasks where static WEs from traditional WE models are still largely used (Schlechtweg et al., 2020; Kaiser et al., 2021). Despite the extensive experiments, we only scratched the surface, and can indicate a spectrum of future enhancements to the proof-of-concept L EX F IT framework beyond the scope of this work. We will test other dual-encoder loss functions, including finer-grained relation classification tasks (e.g., in the SOFTMAX variant), and hard (instead of random) negative examples (Wieting et al., 2015; Mrkši´c et al., 2017; Lauscher et al., 2020; Kalantidis et al., 2020). While in this work, for simplicity and efficiency, we focused on fully decontextualized ISO setup (see §2.2), we will also probe al"
2021.acl-long.410,kamholz-etal-2014-panlex,0,0.011838,"valuation tasks (Ruder et al., 2019). sults demonstrate that the inexpensive lexical finetuning procedure can indeed turn large pretrained LMs into effective decontextualized word encoders, and this can be achieved for a reasonably wide spectrum of languages for which such pretrained LMs exist. What is more, L EX F IT for all nonEN languages has been run with noisy automatically translated lexical constraints, which holds promise to support even stronger static L EX F ITbased WEs with human-curated data in the future, e.g., extracted from multilingual WordNets (Bond and Foster, 2013), PanLex (Kamholz et al., 2014), or BabelNet (Ehrmann et al., 2014). The results give rise to additional general implications. First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vuli´c et al., 2020); the role of L EX F IT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision. To further investigate the ‘rewiring’ hypothesis, in §4.1, we also run L EX F IT with a drastically reduced amount of external knowledge. BERT- REG vectors display large gains over FT vectors in task"
2021.acl-long.410,2020.coling-main.118,1,0.90184,"Missing"
2021.acl-long.410,2021.emnlp-main.109,1,0.821601,"Missing"
2021.acl-long.410,K19-1004,1,0.90017,"Missing"
2021.acl-long.410,P17-1163,0,0.0702013,"Missing"
2021.acl-long.410,Q17-1022,1,0.929258,"Missing"
2021.acl-long.410,N15-1100,0,0.0289612,"diversity of the selection. The final test languages are English (EN), German (DE), Spanish (ES), Finnish (FI), Italian (IT), Polish (PL), Russian (RU), and Turkish (TR). For comparability across languages, we use monolingual uncased BERT Base models for all languages (N = 12 Transformer layers, 12 attention heads, hidden layer dimensionality is 768), available (see the appendix) via the HuggingFace repository (Wolf et al., 2020). External Lexical Knowledge. We use the standard collection of EN lexical constraints from previous work on (static) word vector specialization (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018; Ponti et al., 2018, 2019). It covers the lexical relations from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009); it comprises 1,023,082 synonymy (Psyn ) word pairs and 380,873 antonymy pairs (Pant ). For all other languages, we rely on non-curated noisy lexical constraints, obtained via an automatic word translation method by Ponti et al. (2019); see the original work for the details of the translation procedure. L EX F IT: Technical Details. The implementation is based on the SBERT framework (Reimers and Gurevych, 2019), using the suggested settings: AdamW"
2021.acl-long.410,D18-1026,1,0.887916,"Missing"
2021.acl-long.410,D19-1226,1,0.877527,"Missing"
2021.acl-long.410,D19-1410,0,0.221705,"rence on Natural Language Processing, pages 5269–5283 August 1–6, 2021. ©2021 Association for Computational Linguistics fined representations (Faruqui et al., 2015; Mrkši´c et al., 2017; Ponti et al., 2019, inter alia). Our goal is to create representations that take advantage of both 1) the expressivity and lexical knowledge already stored in pretrained language models (LMs) and 2) the precision of lexical finetuning. To this effect, we develop L EX F IT, a versatile lexical fine-tuning framework, illustrated in Figure 1, drawing a parallel with universal sentence encoders like SentenceBERT (Reimers and Gurevych, 2019).1 Our working hypothesis, extensively evaluated in this paper, is as follows: pretrained encoders store a wealth of lexical knowledge, but it is not straightforward to extract that knowledge. We can expose this knowledge by rewiring their parameters through lexical fine-tuning, and turn the LMs into universal (decontextualized) word encoders. Compared to prior attempts at injecting lexical knowledge into large LMs (Lauscher et al., 2020), our L EX F IT method is innovative as it is deployed post-hoc on top of already pretrained LMs, rather than requiring joint multi-task training. Moreover, L"
2021.acl-long.410,2020.tacl-1.54,0,0.031043,"demonstrate the usefulness of L EX F IT: we report large gains over WEs extracted from vanilla LMs and over traditional WE models across 8 languages and 4 lexical tasks, even with very limited and noisy external lexical knowledge, validating the rewiring hypothesis. The code is available at: https://github.com/cambridgeltl/lexfit. 2 From Language Models to (Decontextualized) Word Encoders The motivation for this work largely stems from the recent work on probing and analyzing pretrained language models for various types of knowledge they might implicitly store (e.g., syntax, world knowledge) (Rogers et al., 2020). Here, we focus on their lexical semantic knowledge (Vuli´c et al., 2020; Liu et al., 2021), with an aim of extracting high-quality static word embeddings from the parameters of the input LMs. In what follows, we describe lexical fine-tuning via dual-encoder networks (§2.1), followed by the WE extraction pro1 These approaches are connected as they are both trained via contrastive learning on dual-encoder architectures, but they provide representations for a different granularity of meaning. cess from the fine-tuned layers of pretrained LMs (§2.2), see Figure 1. 2.1 L EX F IT: Methodology Our"
2021.acl-long.410,2021.acl-long.243,1,0.826374,"Missing"
2021.acl-long.410,2020.semeval-1.1,0,0.0402676,"trum of lexical tasks across diverse languages in controlled evaluations, thus directly questioning the practical usefulness of the traditional WE models in modern NLP. Besides inducing better static WEs for lexical tasks, following the line of lexical probing work (Ethayarajh, 2019; Vuli´c et al., 2020), our goal in this work was to understand how (and how much) lexical semantic knowledge is coded in pretrained LMs, and how to ‘unlock’ the knowledge from the LMs. We hope that our work will be beneficial for all lexical tasks where static WEs from traditional WE models are still largely used (Schlechtweg et al., 2020; Kaiser et al., 2021). Despite the extensive experiments, we only scratched the surface, and can indicate a spectrum of future enhancements to the proof-of-concept L EX F IT framework beyond the scope of this work. We will test other dual-encoder loss functions, including finer-grained relation classification tasks (e.g., in the SOFTMAX variant), and hard (instead of random) negative examples (Wieting et al., 2015; Mrkši´c et al., 2017; Lauscher et al., 2020; Kalantidis et al., 2020). While in this work, for simplicity and efficiency, we focused on fully decontextualized ISO setup (see §2.2),"
2021.acl-long.410,K15-1026,0,0.0675206,"Missing"
2021.acl-long.410,P18-1072,1,0.903042,"Missing"
2021.acl-long.410,2020.cl-4.5,1,0.903622,"Missing"
2021.acl-long.410,N18-1048,1,0.933808,"Missing"
2021.acl-long.541,P98-1013,0,0.268885,"semanticsyntactic properties, provide a mapping between the verbs’ senses and the morpho-syntactic realisation of their arguments (Jackendoff, 1992; Levin, 1993). The potential of verb classifications lies in their predictive power: for any given verb, a set of rich semantic-syntactic properties can be inferred based on its class membership. In this work, we explicitly harness this rich linguistic knowledge to aid pretrained LMs in capturing regularities in the properties of verbs and their arguments. We select two major English lexical databases – VerbNet (Kipper Schuler, 2005) and FrameNet (Baker et al., 1998) – as sources of verb knowledge at the semantic-syntactic interface, each representing a different lexical framework. 6953 VerbNet (VN) (Kipper Schuler, 2005; Kipper et al., 2006), the largest available verb-focused lexicon, organises verbs into classes based on the overlap in their semantic properties and syntactic behaviour; it builds on the premise that a verb’s predicateargument structure informs its meaning (Levin, 1993). Each entry provides a set of thematic roles and selectional preferences for the verbs’ arguments; it also lists the syntactic contexts characteristic for the class membe"
2021.acl-long.541,2020.acl-main.463,0,0.0134269,"ased encoders, pretrained with self-supervised language modeling (LM) objectives, form the backbone of state-of-the-art models for most NLP tasks (Devlin et al., 2019; Yang et al., 2019b; Liu et al., 2019). Recent probes showed that they implicitly extract a non-negligible amount of linguistic knowledge from text corpora in an unsupervised fashion (Hewitt and Manning, 2019; Vuli´c et al., 2020; Rogers et al., 2020, inter alia). In downstream tasks, however, they often rely on spurious correlations and superficial cues (Niven and Kao, 2019) rather than a deep understanding of language meaning (Bender and Koller, 2020), which is detrimental to both generalisation and interpretability (McCoy et al., 2019). In this work, we focus on a specific facet of linguistic knowledge: reasoning about events.1 Identifying tokens in the text that mention events and classifying the temporal and causal relations among them is crucial to understand the structure of a story or dialogue (Carlson et al., 2002; Miltsakaki et al., 2004) and to ground a text in real-world facts. Verbs (with their arguments) are prominently used for expressing events (with their participants). Thus, fine-grained knowledge about verbs, e.g., the syn"
2021.acl-long.541,W11-4606,0,0.0169673,", It freed him of guilt), there exists a subset of verbs participating in a syntactic frame NP V NP S_ING (‘free-80-1’), within which there exists an even more constrained subset of verbs appearing with prepositional phrases headed specifically by the preposition from (e.g., The scientist purified the water from bacteria). 3 For instance, descriptions of transactions will include the same frame elements Buyer, Seller, Goods, Money in most languages. Indeed, English FN has inspired similar projects in other languages: e.g., Spanish (Subirats and Sato, 2004), Japanese (Ohara, 2012), and Danish (Bick, 2011). Adapter Architecture. Instead of directly finetuning all parameters of the pretrained Transformer, we opt for storing verb knowledge in a separate set of adapter parameters, keeping the verb knowledge 4 We also experimented with sentence-level tasks: we fed (a) pairs of sentence examples from VN/FN in a binary classification setup (e.g., Jackie leads Rose to the store. – Jackie escorts Rose.); and (b) individual sentences in a multi-class classification setup (predicting the correct VN class/FN frame). These variants, however, led to weaker performance. 6954 separate from the general languag"
2021.acl-long.541,Q17-1010,0,0.0124165,"data scarcity. MAVEN also demonstrates that even the state-of-the-art Transformer models fail to yield satisfying event detection performance in the general domain. The fact that it is unlikely to expect datasets of similar size for other event extraction tasks and especially for other languages only emphasises the need for external event-related knowledge and transfer learning approaches, such as the ones introduced in this work. Semantic Specialisation. Representation spaces induced through self-supervised objectives from large corpora, be it the word embedding spaces (Mikolov et al., 2013; Bojanowski et al., 2017) or those spanned by LM-pretrained Transformers (Devlin et al., 2019; Liu et al., 2019), encode only distributional knowledge. A large body of work focused on semantic specialisation of such distributional spaces by injecting lexico-semantic knowledge from external resources (e.g., WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010) or ConceptNet (Liu and Singh, 2004)) in the form of lexical constraints (Faruqui et al., 2015; Mrkši´c et al., 2017; Glavaš and Vuli´c, 2018b; Kamath et al., 2019; Vuli´c et al., 2021). Joint specialisation models (Yu and Dredze, 2014; Lauscher et al.,"
2021.acl-long.541,P17-1038,0,0.0238949,"raph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and bootstrapping for training data generation (Ferguson et al., 2018; Wang et al., 2019). The recent release of the large English event detection dataset MAVEN (Wang et al., 2020c), with annotations of event triggers only, partially remedies for English data scarcity. MAVEN also demonstrates that even the state-of-the-art Transformer models fail to yield satisfying event detection performance in the general domain. The fact that it is unlikely to expect datasets of similar size for other event extraction tasks and especially for other languages only em"
2021.acl-long.541,P15-1017,0,0.0313724,"ns capable of making fine-grained predictions in the face of data scarcity. Traditional event extraction methods relied on hand-crafted, language-specific features (Ahn, 2006; Gupta and Ji, 2009; Llorens et al., 2010; Hong et al., 2011; Li et al., 2013; Glavaš and Šnajder, 2015) (e.g., POS tags, entity knowledge), which limited their generalisation ability and effectively prevented language transfer. More recent approaches commonly resorted to word embedding input and neural text encoders such as recurrent nets (Nguyen et al., 2016; Duan et al., 2017; Sha et al., 2018) and convolutional nets (Chen et al., 2015; Nguyen and Grishman, 2015), 6959 as well as graph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through"
2021.acl-long.541,2020.acl-main.747,0,0.041835,"do not exist for a vast majority of languages. Given the inherent cross-lingual nature of verb classes and semantic frames (see English (EN) Spanish (ES) Chinese (ZH) Arabic (AR) VerbNet FrameNet 181,882 96,300 60,365 70,278 57,335 36,623 21,815 24,551 Table 1: Number of positive verb pairs in English, and in each target language obtained via VTRANS (§2.4). §2.1), we investigate the potential for verb knowledge transfer from English to target languages, without any manual target-language adjustments. Massively multilingual LMs, such as multilingual BERT (mBERT) (Devlin et al., 2019) or XLMR (Conneau et al., 2020) have become the de facto standard mechanisms for zero-shot (ZS) crosslingual transfer. In our first transfer approach: we fine-tune mBERT first on the English verb knowledge, then on English task data, and then simply make task predictions for the target language input. The second approach, dubbed VTRANS, is inspired by the work on cross-lingual transfer of semantic specialisation for static word embeddings (Glava´s et al., 2019; Ponti et al., 2019; Wang et al., 2020b). In brief (with full details in Appendix C), starting from a set of positive pairs from English VN/FN, VTRANS involves three"
2021.acl-long.541,D19-2007,1,0.874739,"Missing"
2021.acl-long.541,P18-1048,0,0.0213021,"atures (Ahn, 2006; Gupta and Ji, 2009; Llorens et al., 2010; Hong et al., 2011; Li et al., 2013; Glavaš and Šnajder, 2015) (e.g., POS tags, entity knowledge), which limited their generalisation ability and effectively prevented language transfer. More recent approaches commonly resorted to word embedding input and neural text encoders such as recurrent nets (Nguyen et al., 2016; Duan et al., 2017; Sha et al., 2018) and convolutional nets (Chen et al., 2015; Nguyen and Grishman, 2015), 6959 as well as graph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and bootstrapping for training data generation (Ferg"
2021.acl-long.541,C16-1114,0,0.0203051,"verbs) may trigger the same type of event, and conversely, the same word (verb) can evoke differ5 We provide more details about the frameworks and their corresponding annotation schemes in Appendix A. 6 E.g., in the sentence: “The rules can also affect small businesses, which sometimes pay premiums tied to employees’ health status and claims history.”, affect and pay are event triggers of type STATE and OCCURRENCE, respectively. 7 The ACE annotations distinguish 34 trigger types (e.g., Business:Merge-Org, Justice:Trial-Hearing, Conflict:Attack) and 35 argument roles. Following previous work (Hsi et al., 2016), we conflate eight time-related argument roles - e.g., ‘Time-At-End’, ‘Time-Before’, ‘Time-At-Beginning’ - into a single ‘Time’ role in order to alleviate training data sparsity. ent types of event schemata depending on the context. Adopting these tasks for evaluation thus tests whether leveraging fine-grained curated knowledge of verbs’ semantic-syntactic behaviour can improve pretrained LMs’ reasoning about event-triggering predicates and their arguments. Model Configurations. For each task, we compare the performance of the underlying “vanilla” BERT-based model (see §2.3) against its varia"
2021.acl-long.541,D18-1330,0,0.0375193,"Missing"
2021.acl-long.541,D17-1206,0,0.0158938,"edge 4 We also experimented with sentence-level tasks: we fed (a) pairs of sentence examples from VN/FN in a binary classification setup (e.g., Jackie leads Rose to the store. – Jackie escorts Rose.); and (b) individual sentences in a multi-class classification setup (predicting the correct VN class/FN frame). These variants, however, led to weaker performance. 6954 separate from the general language knowledge acquired in pretraining. This (1) allows downstream training to flexibly combine the two sources of knowledge, and (2) bypasses the issues with catastrophic forgetting and interference (Hashimoto et al., 2017; de Masson d&apos;Autume et al., 2019). We adopt the standard efficient adapter architecture of Pfeiffer et al. (2020a,c). In each Transformer layer l, we insert a single adapter (Adapterl ) after the feed-forward sub-layer. The adapter itself is a two-layer feed-forward neural network with a residual connection, consisting of a down-projection D ∈ Rh×m , a GeLU activation (Hendrycks and Gimpel, 2016), and an upprojection U ∈ Rm×h , where h is the hidden size of the Transformer model and m is the dimensionality of the adapter: Adapterl (hl , rl ) = Ul (GeLU(Dl (hl ))) + rl ; where rl is the residu"
2021.acl-long.541,W19-4310,1,0.899186,"Missing"
2021.acl-long.541,2020.deelio-1.5,1,0.869674,"Missing"
2021.acl-long.541,K19-1061,0,0.0389188,"Missing"
2021.acl-long.541,miltsakaki-etal-2004-penn,0,0.027807,"s et al., 2020, inter alia). In downstream tasks, however, they often rely on spurious correlations and superficial cues (Niven and Kao, 2019) rather than a deep understanding of language meaning (Bender and Koller, 2020), which is detrimental to both generalisation and interpretability (McCoy et al., 2019). In this work, we focus on a specific facet of linguistic knowledge: reasoning about events.1 Identifying tokens in the text that mention events and classifying the temporal and causal relations among them is crucial to understand the structure of a story or dialogue (Carlson et al., 2002; Miltsakaki et al., 2004) and to ground a text in real-world facts. Verbs (with their arguments) are prominently used for expressing events (with their participants). Thus, fine-grained knowledge about verbs, e.g., the syntactic patterns in which they partake and the semantic frames, may help pretrained encoders to achieve a deeper understanding of text and improve their performance in event-oriented downstream tasks. There already exist some expert-curated computational resources that organise verbs into classes based on their syntactic-semantic properties (Jackendoff, 1992; Levin, 1993). In particular, here we consi"
2021.acl-long.541,2020.tacl-1.54,0,0.0262688,"knowledge. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when relying on noisily translated lexical knowledge. 1 Introduction Large Transformer-based encoders, pretrained with self-supervised language modeling (LM) objectives, form the backbone of state-of-the-art models for most NLP tasks (Devlin et al., 2019; Yang et al., 2019b; Liu et al., 2019). Recent probes showed that they implicitly extract a non-negligible amount of linguistic knowledge from text corpora in an unsupervised fashion (Hewitt and Manning, 2019; Vuli´c et al., 2020; Rogers et al., 2020, inter alia). In downstream tasks, however, they often rely on spurious correlations and superficial cues (Niven and Kao, 2019) rather than a deep understanding of language meaning (Bender and Koller, 2020), which is detrimental to both generalisation and interpretability (McCoy et al., 2019). In this work, we focus on a specific facet of linguistic knowledge: reasoning about events.1 Identifying tokens in the text that mention events and classifying the temporal and causal relations among them is crucial to understand the structure of a story or dialogue (Carlson et al., 2002; Miltsakaki et"
2021.acl-long.541,S13-2001,0,0.0809782,"of BEARING with an additional participant, a BOWL. 6952 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6952–6969 August 1–6, 2021. ©2021 Association for Computational Linguistics We hypothesise that complementing pretrained LMs with verb knowledge should benefit model performance in downstream tasks that involve event extraction and processing. We first put this hypothesis to the test in English monolingual event identification and classification tasks from the TempEval (UzZaman et al., 2013) and ACE (Doddington et al., 2004) datasets. We report modest but consistent improvements in the former, and significant performance boosts in the latter, thus verifying that verb knowledge is indeed paramount for a deeper understanding of events and their structure. Moreover, expert-curated resources are not available for most of the languages spoken worldwide. Therefore, we also investigate the effectiveness of transferring verb knowledge across languages; in particular, from English to Spanish, Arabic and Chinese. The results demonstrate the success of the transfer techniques, and also shed"
2021.acl-long.541,C08-3012,0,0.14324,"Missing"
2021.acl-long.541,N18-1048,1,0.897103,"Missing"
2021.acl-long.541,D17-1270,1,0.878618,"Missing"
2021.acl-long.541,2021.acl-long.410,1,0.836059,"Missing"
2021.acl-long.541,2020.emnlp-main.586,1,0.842521,"Missing"
2021.acl-long.541,D19-1585,0,0.0136948,"r. More recent approaches commonly resorted to word embedding input and neural text encoders such as recurrent nets (Nguyen et al., 2016; Duan et al., 2017; Sha et al., 2018) and convolutional nets (Chen et al., 2015; Nguyen and Grishman, 2015), 6959 as well as graph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and bootstrapping for training data generation (Ferguson et al., 2018; Wang et al., 2019). The recent release of the large English event detection dataset MAVEN (Wang et al., 2020c), with annotations of event triggers only, partially remedies for English data scarcity. MAVEN also demonstrates that"
2021.acl-long.541,2021.findings-acl.121,0,0.0656224,"Missing"
2021.acl-long.541,2020.semeval-1.31,0,0.455707,"l target-language adjustments. Massively multilingual LMs, such as multilingual BERT (mBERT) (Devlin et al., 2019) or XLMR (Conneau et al., 2020) have become the de facto standard mechanisms for zero-shot (ZS) crosslingual transfer. In our first transfer approach: we fine-tune mBERT first on the English verb knowledge, then on English task data, and then simply make task predictions for the target language input. The second approach, dubbed VTRANS, is inspired by the work on cross-lingual transfer of semantic specialisation for static word embeddings (Glava´s et al., 2019; Ponti et al., 2019; Wang et al., 2020b). In brief (with full details in Appendix C), starting from a set of positive pairs from English VN/FN, VTRANS involves three steps: (1) automatic translation of verbs in each pair into the target language, (2) filtering of the noisy target language pairs by means of a transferred relation prediction model trained on the English examples, and (3) training the verb adapters injected into the pretrained model, now with the translated and filtered target-language verb pairs. For the monolingual target-language FN-/VN-Adapter training, we follow the protocol used for English, see §2.2. 3 Experim"
2021.acl-long.541,N19-1105,0,0.0867627,"dden state, output of the subsequent layer normalisation. 2.3 Downstream Fine-Tuning for Event Tasks The next step is downstream fine-tuning for event processing tasks. We experiment with (1) tokenlevel event trigger identification and classification and (2) span extraction for event triggers and arguments (a sequence labeling task); see §3. For the former, we mount a classification head – a simple single-layer feed-forward softmax regression classifier – on top of the Transformer augmented with VN-/FN-Adapters. For the latter, we follow the architecture from prior work (M’hamdi et al., 2019; Wang et al., 2019) and add a CRF layer (Lafferty et al., 2001) on top of the sequence of Transformer’s outputs (for subword tokens). For all tasks, we propose and evaluate two different fine-tuning regimes: (1) full fine-tuning, where we update both the original Transformer’s parameters and VN-/FN-Adapters (see 2a in Figure 1); and (2) task-adapter (TA) fine-tuning, where we keep both Transformer’s original parameters and VN/FN-Adapters frozen, while stacking a new trainable task adapter on top of the VN-/FN-Adapter in each Transformer layer (see 2b in Figure 1). 2.4 Cross-Lingual Transfer Creation of curated r"
2021.acl-long.541,2020.emnlp-main.129,0,0.280266,"l target-language adjustments. Massively multilingual LMs, such as multilingual BERT (mBERT) (Devlin et al., 2019) or XLMR (Conneau et al., 2020) have become the de facto standard mechanisms for zero-shot (ZS) crosslingual transfer. In our first transfer approach: we fine-tune mBERT first on the English verb knowledge, then on English task data, and then simply make task predictions for the target language input. The second approach, dubbed VTRANS, is inspired by the work on cross-lingual transfer of semantic specialisation for static word embeddings (Glava´s et al., 2019; Ponti et al., 2019; Wang et al., 2020b). In brief (with full details in Appendix C), starting from a set of positive pairs from English VN/FN, VTRANS involves three steps: (1) automatic translation of verbs in each pair into the target language, (2) filtering of the noisy target language pairs by means of a transferred relation prediction model trained on the English examples, and (3) training the verb adapters injected into the pretrained model, now with the translated and filtered target-language verb pairs. For the monolingual target-language FN-/VN-Adapter training, we follow the protocol used for English, see §2.2. 3 Experim"
2021.acl-long.541,Q15-1025,0,0.0310524,"two verbs belong to the same VN class or FN frame. We extract training instances from FN and VN independently. This allows for a separate analysis of the impact of verb knowledge from each resource. We generate positive training instances by extracting all unique verb pairings from the set of members of each main VN class/FN frame (e.g., walk–march), resulting in 181,882 instances created from VN and 57,335 from FN. We then generate k = 3 negative examples per positive example by combining controlled and random sampling. In controlled sampling, we follow prior work on semantic specialisation (Wieting et al., 2015; Glavaš and Vuli´c, 2018b; Lauscher et al., 2020b). For each positive example p = (w1 , w2 ) in the training batch B, we create two negatives pˆ1 = (w ˆ1 , w2 ) and pˆ2 = (w1 , w ˆ2 ); w ˆ1 is the verb from batch B other than w1 that is closest to w2 in terms of their cosine similarity in an auxiliary static word embedding space Xaux ∈ Rd ; conversely, w ˆ2 is the verb from B other than w2 closest to w1 . We additionally create one negative instance pˆ3 = (w ˆ1 ,w ˆ2 ) by randomly sampling w ˆ1 and w ˆ2 from batch B, not considering w1 and w2 . We ensure that the negatives are not present in"
2021.acl-long.541,D19-1582,0,0.0124374,"relied on hand-crafted, language-specific features (Ahn, 2006; Gupta and Ji, 2009; Llorens et al., 2010; Hong et al., 2011; Li et al., 2013; Glavaš and Šnajder, 2015) (e.g., POS tags, entity knowledge), which limited their generalisation ability and effectively prevented language transfer. More recent approaches commonly resorted to word embedding input and neural text encoders such as recurrent nets (Nguyen et al., 2016; Duan et al., 2017; Sha et al., 2018) and convolutional nets (Chen et al., 2015; Nguyen and Grishman, 2015), 6959 as well as graph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and boot"
2021.acl-long.541,P19-1522,0,0.103672,"ore the utility of verb adapters for event extraction in other languages: we investigate 1) zero-shot language transfer with multilingual Transformers and 2) transfer via (noisy automatic) translation of English verb-based lexical knowledge. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when relying on noisily translated lexical knowledge. 1 Introduction Large Transformer-based encoders, pretrained with self-supervised language modeling (LM) objectives, form the backbone of state-of-the-art models for most NLP tasks (Devlin et al., 2019; Yang et al., 2019b; Liu et al., 2019). Recent probes showed that they implicitly extract a non-negligible amount of linguistic knowledge from text corpora in an unsupervised fashion (Hewitt and Manning, 2019; Vuli´c et al., 2020; Rogers et al., 2020, inter alia). In downstream tasks, however, they often rely on spurious correlations and superficial cues (Niven and Kao, 2019) rather than a deep understanding of language meaning (Bender and Koller, 2020), which is detrimental to both generalisation and interpretability (McCoy et al., 2019). In this work, we focus on a specific facet of linguistic knowledge: reas"
2021.acl-long.541,P14-2089,0,0.036386,"et al., 2013; Bojanowski et al., 2017) or those spanned by LM-pretrained Transformers (Devlin et al., 2019; Liu et al., 2019), encode only distributional knowledge. A large body of work focused on semantic specialisation of such distributional spaces by injecting lexico-semantic knowledge from external resources (e.g., WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010) or ConceptNet (Liu and Singh, 2004)) in the form of lexical constraints (Faruqui et al., 2015; Mrkši´c et al., 2017; Glavaš and Vuli´c, 2018b; Kamath et al., 2019; Vuli´c et al., 2021). Joint specialisation models (Yu and Dredze, 2014; Lauscher et al., 2020b; Levine et al., 2020, inter alia) train the representation space from scratch on the large corpus, but augment the selfsupervised training objective with an additional objective based on external lexical constraints. Lauscher et al. (2020b) add to the Masked LM (MLM) and next sentence prediction (NSP) pretraining objectives of BERT (Devlin et al., 2019) an objective that predicts pairs of (near-)synonyms, aiming to improve word-level semantic similarity in BERT’s representation space. In a similar vein, Levine et al. (2020) add the objective that predicts WordNet super"
2021.acl-long.541,Y18-1097,0,0.00983903,"s (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and bootstrapping for training data generation (Ferguson et al., 2018; Wang et al., 2019). The recent release of the large English event detection dataset MAVEN (Wang et al., 2020c), with annotations of event triggers only, partially remedies for English data scarcity. MAVEN also demonstrates that even the state-of-the-art Transformer models fail to yield satisfying event detection performance in the general domain. The fact that it is unlikely to expect datasets of similar size for other event extraction tasks and especially for other languages only emphasises the"
2021.bea-1.11,L18-1241,0,0.0122227,"in as it enables large-scale passage extraction. Educators, for example, need to extract coherent passage segments from books to create reading materials for students. Similarly, test developers must create reading assessments at scale by extracting coherent segments from a variety of sources. Most segmentation models allow for (i.e., sequential) segmentation (Hearst, 1994; Choi, 2000; Riedl and Biemann, 2012; Glavaˇs et al., 2016; Koshorek et al., 2018; Glavaˇs and Somasundaran, 2020), though methods for hierarchical segmentation have been proposed as well (Eisenstein, 2009; Du et al., 2013; Bayomi and Lawless, 2018). Owing to the absence of large annotated datasets, (linear) text segmentation has long been limited to unsupervised models, relying on various measures of lexical and semantic sentence overlap (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Fragkou et al., 2004; Glavaˇs et al., 2016) and topic modeling (Brants et al., 2002; Misra et al., 2009; Riedl and Biemann, 2012). More recently, Koshorek et al. (2018) automatically created a large segment-annotated dataset W IKI 727 by leveraging the headings structure in Wikipedia articles, effectively enabling supervised text segmentation; they t"
2021.bea-1.11,N09-1042,0,0.0409472,"genre between W IKI 727 and K12S EG, this stark difference between their distributions of segment numbers and sizes poses an additional challenge for the domain transfer. We split the total of 18,906 K12S EG documents into train (15,570 documents), development (3,000), and test portions (336). An example 2-segment document from from K12S EG is shown in Table 1. Wikipedia-Based Test Sets. For the in-domain (Wikipedia) evaluation, we use three small-sized test sets. W IKI 50 is an additional test set consisting of 50 documents, created by Koshorek et al. (2018) in the same manner as W IKI 727. Chen et al. (2009) similarly created the C ITIES (64 articles) and E LEMENTS (117) from Wikipedia pages of world cities and chemical elements, respectively. 3.2 Experimental Setup Experiments. We carry out two sets of experiments. We first benchmark the performance of our HITS model “in domain”, i.e., by training it on W IKI 727 and evaluating it on W IKI 50, E L EMENTS , and C ITIES . Here we directly compare HITS (with full and adapter-based fine-tuning) with state-of-the-art segmentation models: the hierarchical Bi-LSTM (HBi-LSTM) model of Koshorek et al. (2018), and two hierarchical transformer variants of"
2021.bea-1.11,A00-2004,0,0.291794,"et al., 2016), passage retrieval (Huang et al., 2003; Prince and Labadi´e, 2007; Shtekh et al., 2018), and sentiment analysis (Xia et al., 2010; Li et al., 2020). Text segmentation is very important in the educational domain as it enables large-scale passage extraction. Educators, for example, need to extract coherent passage segments from books to create reading materials for students. Similarly, test developers must create reading assessments at scale by extracting coherent segments from a variety of sources. Most segmentation models allow for (i.e., sequential) segmentation (Hearst, 1994; Choi, 2000; Riedl and Biemann, 2012; Glavaˇs et al., 2016; Koshorek et al., 2018; Glavaˇs and Somasundaran, 2020), though methods for hierarchical segmentation have been proposed as well (Eisenstein, 2009; Du et al., 2013; Bayomi and Lawless, 2018). Owing to the absence of large annotated datasets, (linear) text segmentation has long been limited to unsupervised models, relying on various measures of lexical and semantic sentence overlap (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Fragkou et al., 2004; Glavaˇs et al., 2016) and topic modeling (Brants et al., 2002; Misra et al., 2009; Riedl and"
2021.bea-1.11,N13-1019,0,0.0254117,"educational domain as it enables large-scale passage extraction. Educators, for example, need to extract coherent passage segments from books to create reading materials for students. Similarly, test developers must create reading assessments at scale by extracting coherent segments from a variety of sources. Most segmentation models allow for (i.e., sequential) segmentation (Hearst, 1994; Choi, 2000; Riedl and Biemann, 2012; Glavaˇs et al., 2016; Koshorek et al., 2018; Glavaˇs and Somasundaran, 2020), though methods for hierarchical segmentation have been proposed as well (Eisenstein, 2009; Du et al., 2013; Bayomi and Lawless, 2018). Owing to the absence of large annotated datasets, (linear) text segmentation has long been limited to unsupervised models, relying on various measures of lexical and semantic sentence overlap (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Fragkou et al., 2004; Glavaˇs et al., 2016) and topic modeling (Brants et al., 2002; Misra et al., 2009; Riedl and Biemann, 2012). More recently, Koshorek et al. (2018) automatically created a large segment-annotated dataset W IKI 727 by leveraging the headings structure in Wikipedia articles, effectively enabling supervise"
2021.bea-1.11,N09-1040,0,0.0374516,"y important in the educational domain as it enables large-scale passage extraction. Educators, for example, need to extract coherent passage segments from books to create reading materials for students. Similarly, test developers must create reading assessments at scale by extracting coherent segments from a variety of sources. Most segmentation models allow for (i.e., sequential) segmentation (Hearst, 1994; Choi, 2000; Riedl and Biemann, 2012; Glavaˇs et al., 2016; Koshorek et al., 2018; Glavaˇs and Somasundaran, 2020), though methods for hierarchical segmentation have been proposed as well (Eisenstein, 2009; Du et al., 2013; Bayomi and Lawless, 2018). Owing to the absence of large annotated datasets, (linear) text segmentation has long been limited to unsupervised models, relying on various measures of lexical and semantic sentence overlap (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Fragkou et al., 2004; Glavaˇs et al., 2016) and topic modeling (Brants et al., 2002; Misra et al., 2009; Riedl and Biemann, 2012). More recently, Koshorek et al. (2018) automatically created a large segment-annotated dataset W IKI 727 by leveraging the headings structure in Wikipedia articles, effectively e"
2021.bea-1.11,S16-2016,1,0.853464,"Missing"
2021.bea-1.11,P94-1002,0,0.84004,", 2002; Bokaei et al., 2016), passage retrieval (Huang et al., 2003; Prince and Labadi´e, 2007; Shtekh et al., 2018), and sentiment analysis (Xia et al., 2010; Li et al., 2020). Text segmentation is very important in the educational domain as it enables large-scale passage extraction. Educators, for example, need to extract coherent passage segments from books to create reading materials for students. Similarly, test developers must create reading assessments at scale by extracting coherent segments from a variety of sources. Most segmentation models allow for (i.e., sequential) segmentation (Hearst, 1994; Choi, 2000; Riedl and Biemann, 2012; Glavaˇs et al., 2016; Koshorek et al., 2018; Glavaˇs and Somasundaran, 2020), though methods for hierarchical segmentation have been proposed as well (Eisenstein, 2009; Du et al., 2013; Bayomi and Lawless, 2018). Owing to the absence of large annotated datasets, (linear) text segmentation has long been limited to unsupervised models, relying on various measures of lexical and semantic sentence overlap (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Fragkou et al., 2004; Glavaˇs et al., 2016) and topic modeling (Brants et al., 2002; Misra et al., 200"
2021.bea-1.11,N18-2075,0,0.131119,"ce and Labadi´e, 2007; Shtekh et al., 2018), and sentiment analysis (Xia et al., 2010; Li et al., 2020). Text segmentation is very important in the educational domain as it enables large-scale passage extraction. Educators, for example, need to extract coherent passage segments from books to create reading materials for students. Similarly, test developers must create reading assessments at scale by extracting coherent segments from a variety of sources. Most segmentation models allow for (i.e., sequential) segmentation (Hearst, 1994; Choi, 2000; Riedl and Biemann, 2012; Glavaˇs et al., 2016; Koshorek et al., 2018; Glavaˇs and Somasundaran, 2020), though methods for hierarchical segmentation have been proposed as well (Eisenstein, 2009; Du et al., 2013; Bayomi and Lawless, 2018). Owing to the absence of large annotated datasets, (linear) text segmentation has long been limited to unsupervised models, relying on various measures of lexical and semantic sentence overlap (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Fragkou et al., 2004; Glavaˇs et al., 2016) and topic modeling (Brants et al., 2002; Misra et al., 2009; Riedl and Biemann, 2012). More recently, Koshorek et al. (2018) automatically c"
2021.bea-1.11,2021.ccl-1.108,0,0.0918418,"Missing"
2021.bea-1.11,2020.emnlp-main.617,0,0.12246,"Missing"
2021.bea-1.11,W12-3307,0,0.0245293,"6), passage retrieval (Huang et al., 2003; Prince and Labadi´e, 2007; Shtekh et al., 2018), and sentiment analysis (Xia et al., 2010; Li et al., 2020). Text segmentation is very important in the educational domain as it enables large-scale passage extraction. Educators, for example, need to extract coherent passage segments from books to create reading materials for students. Similarly, test developers must create reading assessments at scale by extracting coherent segments from a variety of sources. Most segmentation models allow for (i.e., sequential) segmentation (Hearst, 1994; Choi, 2000; Riedl and Biemann, 2012; Glavaˇs et al., 2016; Koshorek et al., 2018; Glavaˇs and Somasundaran, 2020), though methods for hierarchical segmentation have been proposed as well (Eisenstein, 2009; Du et al., 2013; Bayomi and Lawless, 2018). Owing to the absence of large annotated datasets, (linear) text segmentation has long been limited to unsupervised models, relying on various measures of lexical and semantic sentence overlap (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Fragkou et al., 2004; Glavaˇs et al., 2016) and topic modeling (Brants et al., 2002; Misra et al., 2009; Riedl and Biemann, 2012). More rec"
2021.bea-1.11,2020.emnlp-main.194,0,0.0992292,"Missing"
2021.bea-1.11,P01-1064,0,0.265651,"ing coherent segments from a variety of sources. Most segmentation models allow for (i.e., sequential) segmentation (Hearst, 1994; Choi, 2000; Riedl and Biemann, 2012; Glavaˇs et al., 2016; Koshorek et al., 2018; Glavaˇs and Somasundaran, 2020), though methods for hierarchical segmentation have been proposed as well (Eisenstein, 2009; Du et al., 2013; Bayomi and Lawless, 2018). Owing to the absence of large annotated datasets, (linear) text segmentation has long been limited to unsupervised models, relying on various measures of lexical and semantic sentence overlap (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Fragkou et al., 2004; Glavaˇs et al., 2016) and topic modeling (Brants et al., 2002; Misra et al., 2009; Riedl and Biemann, 2012). More recently, Koshorek et al. (2018) automatically created a large segment-annotated dataset W IKI 727 by leveraging the headings structure in Wikipedia articles, effectively enabling supervised text segmentation; they then trained a hierarchical recurrent neural segmentation model on W IKI 727. In subsequent work, Glavaˇs and Somasundaran (2020) improved on their segmentation performance by (i) replacing recurrent components of the hierarchical model with trans"
2021.eacl-demos.11,2020.acl-main.485,0,0.0799835,"Missing"
2021.eacl-demos.11,Q17-1010,0,0.10173,"Missing"
2021.eacl-demos.11,N19-3002,0,0.0202981,"onal nature of word representation models (Harris, 1954) it is – depending on the sociotechnical context – an undesired artefact of distributional representation learning (Blodgett et al., 2020) which can, in turn, lead to unfair decisions in downstream applications. A number of different measures for quantifying biases in representation spaces have been proposed in recent years (Caliskan et al., 2017; Gonen and Goldberg, 2019; Dev and Phillips, 2019; Garg et al., 2018; Lauscher et al., 2020a) and even more models for removing or attenuating such biases have been developed (Zhao et al., 2019; Bordia and Bowman, 2019; Dinan et al., 2020; Webster et al., 2020; Qian et al., 2019, inter alia). What is still missing, however, is the ability to seamlessly apply different bias measures and debiasing models on arbitrary embedding spaces and for custom (i.e., user-specified) bias specifications. In this work, we address this gap by introducing D EB IE, the first integrated platform offering bias measurement and mitigation for arbitrary static embedding spaces and bias specifications. The D E B IE platform is grounded in the general framework for implicit and explicit debiasing of word embedding spaces (Lauscher e"
2021.eacl-demos.11,2020.emnlp-main.656,0,0.0405917,"Missing"
2021.eacl-demos.11,D14-1162,0,0.0865587,"BAM (GBDD). 3.4 Integrated Data D EB IE is designed as a general tool, which allows user to upload their own embedding spaces and define their own bias specifications for testing and/or debiasing. Nonetheless, we include into the platform a set of commonly used bias specifications and word embedding spaces. Concretely, D EB IE includes the whole WEAT test collection (Caliskan et al., 2017), containing the explicit bias specifications summarized in Table 1. D EB IE also comes with three word embedding spaces, pretrained with different models: (1) fastText (Bojanowski et al., 2017),6 (2) GloVe (Pennington et al., 2014),7 and (3) CBOW (Mikolov et al., 2013).8 All three spaces are 300-dimensional and their vocabularies are limited to 200K most frequent words. Debiasing Methods D EB IE encompasses implementations of two debiasing models from (Lauscher et al., 2020a), for which an implicit bias specification suffices:5 General Bias Direction Debiasing (GBDD). As an extension of the linear projection model of Dev and Phillips (2019), GBDD relies on identifying the bias direction in the distributional space. Let (ti1 , tj2 ) be word pairs with ti1 ∈ T1 , tj2 ∈ T2 , respectively. First, we obtain partial bias dire"
2021.eacl-demos.11,P19-2031,0,0.0145707,"pending on the sociotechnical context – an undesired artefact of distributional representation learning (Blodgett et al., 2020) which can, in turn, lead to unfair decisions in downstream applications. A number of different measures for quantifying biases in representation spaces have been proposed in recent years (Caliskan et al., 2017; Gonen and Goldberg, 2019; Dev and Phillips, 2019; Garg et al., 2018; Lauscher et al., 2020a) and even more models for removing or attenuating such biases have been developed (Zhao et al., 2019; Bordia and Bowman, 2019; Dinan et al., 2020; Webster et al., 2020; Qian et al., 2019, inter alia). What is still missing, however, is the ability to seamlessly apply different bias measures and debiasing models on arbitrary embedding spaces and for custom (i.e., user-specified) bias specifications. In this work, we address this gap by introducing D EB IE, the first integrated platform offering bias measurement and mitigation for arbitrary static embedding spaces and bias specifications. The D E B IE platform is grounded in the general framework for implicit and explicit debiasing of word embedding spaces (Lauscher et al., 2020a). Within this framework, an implicit bias consis"
2021.eacl-demos.11,P19-1070,1,0.892893,"Missing"
2021.eacl-demos.11,W19-3621,0,0.206795,"logies, such as the fa−→ − − −−−−−−−→ ≈ mous example of sexism: − man programmer −−−−−−−→ − − − − → woman − homemaker (Bolukbasi et al., 2016). While this is not surprising, given the distributional nature of word representation models (Harris, 1954) it is – depending on the sociotechnical context – an undesired artefact of distributional representation learning (Blodgett et al., 2020) which can, in turn, lead to unfair decisions in downstream applications. A number of different measures for quantifying biases in representation spaces have been proposed in recent years (Caliskan et al., 2017; Gonen and Goldberg, 2019; Dev and Phillips, 2019; Garg et al., 2018; Lauscher et al., 2020a) and even more models for removing or attenuating such biases have been developed (Zhao et al., 2019; Bordia and Bowman, 2019; Dinan et al., 2020; Webster et al., 2020; Qian et al., 2019, inter alia). What is still missing, however, is the ability to seamlessly apply different bias measures and debiasing models on arbitrary embedding spaces and for custom (i.e., user-specified) bias specifications. In this work, we address this gap by introducing D EB IE, the first integrated platform offering bias measurement and mitigation f"
2021.eacl-demos.11,J15-4004,0,0.0347694,"h U ΣV > as the singular value decomposition of XT2 XT>1 . In the last step, the original space and its “translation” X = XWX (which is equally biased), are averaged to obtain the debiased embedding space: Semantic Quality Tests (SQ). The debiasing models (3.3) modify the embedding space. While they reduce the bias, they may reduce the general semantic quality of the embedding space, which could be detrimental for model performance in downstream applications. This is why we couple the bias tests with measures of semantic word similarity on two established word-similarity datasets: SimLex-999 (Hill et al., 2015) or WordSim-353 (Finkelstein et al., 2001). We compute the Spearman correlation between the human similarity scores assigned to word pairs and corresponding cosines computed from the embedding space. 3.3 1 BAM(X) = (X + XWX ) . 2 Note that D EB IE can trivially compose the two debiasing models – the resulting space after applying GBDD (BAM) can be the input for BAM (GBDD). 3.4 Integrated Data D EB IE is designed as a general tool, which allows user to upload their own embedding spaces and define their own bias specifications for testing and/or debiasing. Nonetheless, we include into the platfo"
2021.eacl-demos.11,N19-1064,0,0.0362523,"Missing"
2021.eacl-demos.11,S19-1010,1,0.838466,"Missing"
2021.eacl-demos.11,2020.wanlp-1.17,1,0.794946,"Missing"
2021.eacl-main.270,2020.acl-main.493,0,0.221541,"er to make an empirically driven step towards a deeper understanding of the relationship between LU and formalised syntactic knowledge, and the extent of its impact to modern semantic LU and applications. (RQ) Is explicit structural language information, provided in the form of a widely adopted syntactic formalism (Universal Dependencies, UD) (Nivre et al., 2016) and injected in a supervised manner into LM-pretrained transformers beneficial for transformers’ downstream LU performance? While existing body of work (Lin et al., 2019; Tenney et al., 2019; Liu et al., 2019a; Kulmizev et al., 2020; Chi et al., 2020) probes transformers for structural phenomena, our work is more pragmatically motivated. We directly evaluate the effect of infusing structural language information from UD treebanks, via intermediate dependency parsing (DP) training, on transformers’ performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Conneau et al., 2020) produces DP results which are comparable to state of the art. We t"
2021.eacl-main.270,2020.acl-main.747,0,0.10477,"Missing"
2021.eacl-main.270,D18-1269,0,0.163913,"l language information from UD treebanks, via intermediate dependency parsing (DP) training, on transformers’ performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Conneau et al., 2020) produces DP results which are comparable to state of the art. We then fine-tune the syntactically-informed transformers for three downstream LU tasks: natural language inference (NLI) (Williams et al., 2018; Conneau et al., 2018), paraphrase identification (Zhang et al., 2019b; Yang et al., 2019), and causal commonsense reasoning (Sap et al., 2019; Ponti et al., 2020). We quantify the contribution of explicit syntax by comparing LU performance of the transformer exposed to intermediate parsing training (IPT) and its counterpart directly fine-tuned for the downstream task. We investigate the effects of IPT (1) monolingually, by fine-tuning English transformers, BERT and RoBERTa, on an English UD treebank and for (2) downstream zero-shot language transfer, by fine-tuning massively multilingual transformers (MMTs) – mBER"
2021.eacl-main.270,de-marneffe-etal-2006-generating,0,0.0180012,"Missing"
2021.eacl-main.270,N19-1423,0,0.369971,"n, 2007; Nivre et al., 2016, 2020, inter alia), has been the beating heart of NLP pipelines for decades (Klein and Manning, 2003; Chen and Manning, 2014; Dozat and Manning, 2017; Kondratyuk and Straka, 2019), establishing Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin et al., 2019; Liu et al., 2019b; Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU ta"
2021.eacl-main.270,2020.tacl-1.3,0,0.0177432,"languages. They also provide evidence that clusters of head–dependency pairs roughly correspond to UD relations. Similarly, Kulmizev et al. (2020) show that BERT’s latent syntax corresponds more to UD trees than to shallower SUD (Gerdes et al., 2018) structures. Despite the evident similarity between BERT’s latent syntax and formalisms such as UD, there is ample evidence that BERT insufficiently leverages syntax in downstream tasks: it often produces similar predictions for syntactically valid as well as for structurally corrupt sentences (e.g., with random word order) (Wallace et al., 2019; Ettinger, 2020; Zhao et al., 2020). Intermediate Training. Sometimes called Supplementary Training on Intermediate Labeled-data Tasks (STILT) (Phang et al., 2018), intermediate Relation classifier Arc classifier X X’ concat concat [root] brown + jumps + ... Word-level average pooling ... Transformer (BERT / RoBERTa) [CLS] The quick br ##own fox jump ##s ... [SEP] Figure 1: Architecture of our transformer-based biaffine dependency parser. training is a transfer learning setup in which one trains an LM-pretrained transformer on one or more supervised tasks (ideally with large training sets) before final fine-"
2021.eacl-main.270,W18-6008,0,0.0150566,"applied on BERT’s contextualized word vectors, reflect distances in dependency trees. This suggests that BERT encodes sufficient structural information to reconstruct dependency trees (though without arc directionality and relations). Chi et al. (2020) extend the analysis to multilingual BERT, finding that its representation subspaces may recover trees also for other languages. They also provide evidence that clusters of head–dependency pairs roughly correspond to UD relations. Similarly, Kulmizev et al. (2020) show that BERT’s latent syntax corresponds more to UD trees than to shallower SUD (Gerdes et al., 2018) structures. Despite the evident similarity between BERT’s latent syntax and formalisms such as UD, there is ample evidence that BERT insufficiently leverages syntax in downstream tasks: it often produces similar predictions for syntactically valid as well as for structurally corrupt sentences (e.g., with random word order) (Wallace et al., 2019; Ettinger, 2020; Zhao et al., 2020). Intermediate Training. Sometimes called Supplementary Training on Intermediate Labeled-data Tasks (STILT) (Phang et al., 2018), intermediate Relation classifier Arc classifier X X’ concat concat [root] brown + jumps"
2021.eacl-main.270,N19-1419,0,0.038581,"directly on representations from transformer’s output layer, eliminating the head- and dependendantbased feed-forward mapping. Despite this simplification, our biaffine parser produces DP results comparable to current state-of-the-art parsers. Syntactic BERTology. The substantial body of syntactic probing work shows that BERT (Devlin et al., 2019) (a) encodes text in a hierarchical manner (i.e., it encodes some implicit underlying syntax) (Lin et al., 2019); and (b) captures specific shallow syntactic information (parts-of-speech and syntactic chunks) (Tenney et al., 2019; Liu et al., 2019a). Hewitt and Manning (2019) find that linear transformations, when applied on BERT’s contextualized word vectors, reflect distances in dependency trees. This suggests that BERT encodes sufficient structural information to reconstruct dependency trees (though without arc directionality and relations). Chi et al. (2020) extend the analysis to multilingual BERT, finding that its representation subspaces may recover trees also for other languages. They also provide evidence that clusters of head–dependency pairs roughly correspond to UD relations. Similarly, Kulmizev et al. (2020) show that BERT’s latent syntax corresponds"
2021.eacl-main.270,J07-3004,0,0.0177526,"ed syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers’ representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models? 1 Introduction Structural analysis of sentences, based on a variety of syntactic formalisms (Charniak, 1996; Taylor et al., 2003; De Marneffe et al., 2006; Hockenmaier and Steedman, 2007; Nivre et al., 2016, 2020, inter alia), has been the beating heart of NLP pipelines for decades (Klein and Manning, 2003; Chen and Manning, 2014; Dozat and Manning, 2017; Kondratyuk and Straka, 2019), establishing Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin"
2021.eacl-main.270,N19-1075,0,0.0253781,"s (Wang et al., 2018; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Goldberg, 2014; Cheng and Kartsaklis, 2015; Bastings et al., 2017; Kasai et al., 2019; Zhang et al., 2019a, inter alia). However, we believe that the massive improvements brought about by the LM-pretrained transformers – unexposed to any explicit syntactic signal – warrant a renewed scrutiny of the utility of supervised parsing for high-level language understanding.1,2 The research question we address in this work can be summarized as follows: 1 Disclaimer 1: In this work, we make a clear distinction between Computational Linguistics (CL), i.e., the area of linguistics leveraging computational methods for analyses of human languages and NLP, the area of artificial intelligence"
2021.eacl-main.270,Q16-1023,0,0.0343826,"nce and phrase structure tree) into an LSTMbased student pretrained on a much larger corpus. They show that distillation helps the student in structured prediction tasks, but their downstream evaluation does not involve LU tasks. Their subsequent work (Kuncoro et al., 2020) replaces the RNN student with BERT (Devlin et al., 2019): syntactic distillation again helps structured prediction, but hurts (slightly) the performance on LU tasks from the GLUE benchmark (Wang et al., 2018). Transformer-Based Dependency Parsing. Building on the success of preceding neural parsers (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016), Dozat and Manning (2017) proposed a biaffine parsing head on top of a Bi-LSTM encoder: contextualized word vectors are fed to two feedforward networks, producing dependent- and headspecific token representations, respectively. Arc and relation scores are produced via biaffine products between these dependent- and head-specific representation matrices. Finally, the Edmonds algorithm induces the optimal tree from pairwise arc predictions. Most recent DP work (Kondratyuk ¨ un et al., 2020) replaces the and Straka, 2019; Ust¨ Bi-LSTM encoder with multilingual BERT’s transformer, reporting state-"
2021.eacl-main.270,P03-1054,0,0.161861,"sults, coupled with our analysis of transformers’ representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models? 1 Introduction Structural analysis of sentences, based on a variety of syntactic formalisms (Charniak, 1996; Taylor et al., 2003; De Marneffe et al., 2006; Hockenmaier and Steedman, 2007; Nivre et al., 2016, 2020, inter alia), has been the beating heart of NLP pipelines for decades (Klein and Manning, 2003; Chen and Manning, 2014; Dozat and Manning, 2017; Kondratyuk and Straka, 2019), establishing Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin et al., 2019; Liu et al., 2019b; Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018; Hu et"
2021.eacl-main.270,D19-1279,0,0.349309,"fore and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models? 1 Introduction Structural analysis of sentences, based on a variety of syntactic formalisms (Charniak, 1996; Taylor et al., 2003; De Marneffe et al., 2006; Hockenmaier and Steedman, 2007; Nivre et al., 2016, 2020, inter alia), has been the beating heart of NLP pipelines for decades (Klein and Manning, 2003; Chen and Manning, 2014; Dozat and Manning, 2017; Kondratyuk and Straka, 2019), establishing Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin et al., 2019; Liu et al., 2019b; Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018; Hu et al., 2020), however, questions this widely accepted assumption. The question of"
2021.eacl-main.270,2020.acl-main.375,0,0.068379,"and modeling, but rather to make an empirically driven step towards a deeper understanding of the relationship between LU and formalised syntactic knowledge, and the extent of its impact to modern semantic LU and applications. (RQ) Is explicit structural language information, provided in the form of a widely adopted syntactic formalism (Universal Dependencies, UD) (Nivre et al., 2016) and injected in a supervised manner into LM-pretrained transformers beneficial for transformers’ downstream LU performance? While existing body of work (Lin et al., 2019; Tenney et al., 2019; Liu et al., 2019a; Kulmizev et al., 2020; Chi et al., 2020) probes transformers for structural phenomena, our work is more pragmatically motivated. We directly evaluate the effect of infusing structural language information from UD treebanks, via intermediate dependency parsing (DP) training, on transformers’ performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Conneau et al., 2020) produces DP results which are comparable to sta"
2021.eacl-main.270,P19-1337,0,0.0676021,"training (IPT) and its counterpart directly fine-tuned for the downstream task. We investigate the effects of IPT (1) monolingually, by fine-tuning English transformers, BERT and RoBERTa, on an English UD treebank and for (2) downstream zero-shot language transfer, by fine-tuning massively multilingual transformers (MMTs) – mBERT and XLM-R (Conneau et al., 2020) – on treebanks of downstream target languages, before the downstream fine-tuning on source language (English) data. While intermediate parsing training is obviously not the only way of bringing syntactic knowledge to downstream tasks (Kuncoro et al., 2019; Swayamdipta et al., 2019; Kuncoro et al., 2020), it is arguably the most straightforward way of injecting syntactic signal in the context of the predominant pretraining-fine-tuning paradigm that has, nonetheless, not been investigated up to this point. Other methods of bringing syntactic signal to downstream tasks such as knowledge distillation (Kuncoro et al., 2020) and pre-training on shallow trees instead of sequences (Swayamdipta et al., 2019) have failed to demonstrate significant gains on higher-level LU tasks. Our results also render supervised UD parsing largely inconsequential to LU"
2021.eacl-main.270,2020.tacl-1.50,0,0.17146,"-tuned for the downstream task. We investigate the effects of IPT (1) monolingually, by fine-tuning English transformers, BERT and RoBERTa, on an English UD treebank and for (2) downstream zero-shot language transfer, by fine-tuning massively multilingual transformers (MMTs) – mBERT and XLM-R (Conneau et al., 2020) – on treebanks of downstream target languages, before the downstream fine-tuning on source language (English) data. While intermediate parsing training is obviously not the only way of bringing syntactic knowledge to downstream tasks (Kuncoro et al., 2019; Swayamdipta et al., 2019; Kuncoro et al., 2020), it is arguably the most straightforward way of injecting syntactic signal in the context of the predominant pretraining-fine-tuning paradigm that has, nonetheless, not been investigated up to this point. Other methods of bringing syntactic signal to downstream tasks such as knowledge distillation (Kuncoro et al., 2020) and pre-training on shallow trees instead of sequences (Swayamdipta et al., 2019) have failed to demonstrate significant gains on higher-level LU tasks. Our results also render supervised UD parsing largely inconsequential to LU. We observe limited and inconsistent gains only"
2021.eacl-main.270,2020.emnlp-main.185,1,0.890601,"Missing"
2021.eacl-main.270,P14-2050,0,0.010901,"et al., 2019b; Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Goldberg, 2014; Cheng and Kartsaklis, 2015; Bastings et al., 2017; Kasai et al., 2019; Zhang et al., 2019a, inter alia). However, we believe that the massive improvements brought about by the LM-pretrained transformers – unexposed to any explicit syntactic signal – warrant a renewed scrutiny of the utility of supervised parsing for high-level language understanding.1,2 The research question we address in this work can be summarized as follows: 1 Disclaimer 1: In this work, we make a clear distinction between Computational Linguistics (CL), i.e., the area of linguistics leveraging computational methods for a"
2021.eacl-main.270,2020.acl-main.467,0,0.0274302,"brown + jumps + ... Word-level average pooling ... Transformer (BERT / RoBERTa) [CLS] The quick br ##own fox jump ##s ... [SEP] Figure 1: Architecture of our transformer-based biaffine dependency parser. training is a transfer learning setup in which one trains an LM-pretrained transformer on one or more supervised tasks (ideally with large training sets) before final fine-tuning for the target task. Phang et al. (2018) show that intermediate NLI training of BERT on the Multi-NLI dataset (Williams et al., 2018) benefits several language understanding tasks. Subsequent work (Wang et al., 2019; Pruksachatkun et al., 2020) investigated many combinations of intermediate and target LU tasks, failing to identify any universally beneficial intermediate task. In this work we use DP as an intermediate training task (IPT) for LM-pretrained transformers. 3 Methodology Biaffine Parser. Our parsing model, illustrated in Figure 1, consists of a biaffine attention layer applied directly on the transformer’s output (BERT, RoBERTa, mBERT, or XLM-R). We first obtain word-level vectors by averaging transformed representations of their constituent subwords, produced by the transformer. Let X ∈ RN ×H denote the encoding of a sen"
2021.eacl-main.270,W19-4825,0,0.103726,"o invalidate the admirable efforts on syntactic annotation and modeling, but rather to make an empirically driven step towards a deeper understanding of the relationship between LU and formalised syntactic knowledge, and the extent of its impact to modern semantic LU and applications. (RQ) Is explicit structural language information, provided in the form of a widely adopted syntactic formalism (Universal Dependencies, UD) (Nivre et al., 2016) and injected in a supervised manner into LM-pretrained transformers beneficial for transformers’ downstream LU performance? While existing body of work (Lin et al., 2019; Tenney et al., 2019; Liu et al., 2019a; Kulmizev et al., 2020; Chi et al., 2020) probes transformers for structural phenomena, our work is more pragmatically motivated. We directly evaluate the effect of infusing structural language information from UD treebanks, via intermediate dependency parsing (DP) training, on transformers’ performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Connea"
2021.eacl-main.270,N19-1112,0,0.353879,", 2016, 2020, inter alia), has been the beating heart of NLP pipelines for decades (Klein and Manning, 2003; Chen and Manning, 2014; Dozat and Manning, 2017; Kondratyuk and Straka, 2019), establishing Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin et al., 2019; Liu et al., 2019b; Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Gold"
2021.eacl-main.270,2021.ccl-1.108,0,0.052308,"Missing"
2021.eacl-main.270,P19-1334,0,0.050602,"Missing"
2021.eacl-main.270,2020.lrec-1.497,0,0.0489337,"Missing"
2021.eacl-main.270,N18-1202,0,0.0418824,"pe that these empirical findings will shed new light on the relationship between supervised parsing (and manually labeled treebanks) and LU with transformer networks, and guide further similar investigations in future work, in order to fully understand the impact of formal syntactic knowledge on LU performance with modern neural architectures. 2 Related Work Bringing Explicit Syntax to LMs. Previous work has attempted to enrich language models with explicit syntactic knowledge in ways other than intermediate parsing training. Swayamdipta et al. (2019) modify the pretraining objective of ELMo (Peters et al., 2018) to learn from shallowly parsed (i.e., chunked) corpora. They, however, report no notable improvements on downstream tasks. Kuncoro et al. (2019) propose to distil the knowledge from a Recurrent NN Grammar (RNNG) teacher trained on a small syntactically annotated corpus (by modeling the joint probability of surface sequence and phrase structure tree) into an LSTMbased student pretrained on a much larger corpus. They show that distillation helps the student in structured prediction tasks, but their downstream evaluation does not involve LU tasks. Their subsequent work (Kuncoro et al., 2020) rep"
2021.eacl-main.270,2020.emnlp-demos.7,1,0.914258,"Missing"
2021.eacl-main.270,D19-1454,0,0.0442123,"Missing"
2021.eacl-main.270,2020.emnlp-main.180,0,0.182024,"Missing"
2021.eacl-main.270,D19-1221,0,0.0184478,"r trees also for other languages. They also provide evidence that clusters of head–dependency pairs roughly correspond to UD relations. Similarly, Kulmizev et al. (2020) show that BERT’s latent syntax corresponds more to UD trees than to shallower SUD (Gerdes et al., 2018) structures. Despite the evident similarity between BERT’s latent syntax and formalisms such as UD, there is ample evidence that BERT insufficiently leverages syntax in downstream tasks: it often produces similar predictions for syntactically valid as well as for structurally corrupt sentences (e.g., with random word order) (Wallace et al., 2019; Ettinger, 2020; Zhao et al., 2020). Intermediate Training. Sometimes called Supplementary Training on Intermediate Labeled-data Tasks (STILT) (Phang et al., 2018), intermediate Relation classifier Arc classifier X X’ concat concat [root] brown + jumps + ... Word-level average pooling ... Transformer (BERT / RoBERTa) [CLS] The quick br ##own fox jump ##s ... [SEP] Figure 1: Architecture of our transformer-based biaffine dependency parser. training is a transfer learning setup in which one trains an LM-pretrained transformer on one or more supervised tasks (ideally with large training sets) be"
2021.eacl-main.270,P19-1439,0,0.0198921,"ncat concat [root] brown + jumps + ... Word-level average pooling ... Transformer (BERT / RoBERTa) [CLS] The quick br ##own fox jump ##s ... [SEP] Figure 1: Architecture of our transformer-based biaffine dependency parser. training is a transfer learning setup in which one trains an LM-pretrained transformer on one or more supervised tasks (ideally with large training sets) before final fine-tuning for the target task. Phang et al. (2018) show that intermediate NLI training of BERT on the Multi-NLI dataset (Williams et al., 2018) benefits several language understanding tasks. Subsequent work (Wang et al., 2019; Pruksachatkun et al., 2020) investigated many combinations of intermediate and target LU tasks, failing to identify any universally beneficial intermediate task. In this work we use DP as an intermediate training task (IPT) for LM-pretrained transformers. 3 Methodology Biaffine Parser. Our parsing model, illustrated in Figure 1, consists of a biaffine attention layer applied directly on the transformer’s output (BERT, RoBERTa, mBERT, or XLM-R). We first obtain word-level vectors by averaging transformed representations of their constituent subwords, produced by the transformer. Let X ∈ RN ×H"
2021.eacl-main.270,W18-5446,0,0.184779,"(Klein and Manning, 2003; Chen and Manning, 2014; Dozat and Manning, 2017; Kondratyuk and Straka, 2019), establishing Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin et al., 2019; Liu et al., 2019b; Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Goldberg, 2014; Cheng and Kartsaklis, 2015; Bastings et al., 2017; Kasai et al., 2019;"
2021.eacl-main.270,N18-1101,0,0.327694,"t of infusing structural language information from UD treebanks, via intermediate dependency parsing (DP) training, on transformers’ performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Conneau et al., 2020) produces DP results which are comparable to state of the art. We then fine-tune the syntactically-informed transformers for three downstream LU tasks: natural language inference (NLI) (Williams et al., 2018; Conneau et al., 2018), paraphrase identification (Zhang et al., 2019b; Yang et al., 2019), and causal commonsense reasoning (Sap et al., 2019; Ponti et al., 2020). We quantify the contribution of explicit syntax by comparing LU performance of the transformer exposed to intermediate parsing training (IPT) and its counterpart directly fine-tuned for the downstream task. We investigate the effects of IPT (1) monolingually, by fine-tuning English transformers, BERT and RoBERTa, on an English UD treebank and for (2) downstream zero-shot language transfer, by fine-tuning massively multilingual tra"
2021.eacl-main.270,D19-1382,0,0.10091,"arsing (DP) training, on transformers’ performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Conneau et al., 2020) produces DP results which are comparable to state of the art. We then fine-tune the syntactically-informed transformers for three downstream LU tasks: natural language inference (NLI) (Williams et al., 2018; Conneau et al., 2018), paraphrase identification (Zhang et al., 2019b; Yang et al., 2019), and causal commonsense reasoning (Sap et al., 2019; Ponti et al., 2020). We quantify the contribution of explicit syntax by comparing LU performance of the transformer exposed to intermediate parsing training (IPT) and its counterpart directly fine-tuned for the downstream task. We investigate the effects of IPT (1) monolingually, by fine-tuning English transformers, BERT and RoBERTa, on an English UD treebank and for (2) downstream zero-shot language transfer, by fine-tuning massively multilingual transformers (MMTs) – mBERT and XLM-R (Conneau et al., 2020) – on treebanks of downstream targ"
2021.eacl-main.270,N19-1118,0,0.338711,"; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Goldberg, 2014; Cheng and Kartsaklis, 2015; Bastings et al., 2017; Kasai et al., 2019; Zhang et al., 2019a, inter alia). However, we believe that the massive improvements brought about by the LM-pretrained transformers – unexposed to any explicit syntactic signal – warrant a renewed scrutiny of the utility of supervised parsing for high-level language understanding.1,2 The research question we address in this work can be summarized as follows: 1 Disclaimer 1: In this work, we make a clear distinction between Computational Linguistics (CL), i.e., the area of linguistics leveraging computational methods for analyses of human languages and NLP, the area of artificial intelligence tackling human lang"
2021.eacl-main.270,E17-1063,0,0.0191432,"he representation of syntactic dependants and X0 as the representation of dependency heads. We then directly compute the arc and relation scores as biaffine products of X and X0 : &gt; &gt; Yarc = XWarc X0 + Barc ; Yrel = XWrel X0 + Brel RH×H RH×H×R where Warc ∈ and Wrel ∈ denote, respectively, the arc classification matrix and relation classification tensor (with R as the number of relations); Barc and Brel denote the corresponding bias parameters. We greedily select the dependency head for each word by finding the maximal score in each row of Yarc : while this is not guaranteed to produce a tree, Zhang et al. (2017) show that in most cases it does.3 Our arc prediction loss is the cross-entropy loss with sentence words (plus the root node) as categorical labels: this implies a different number of labels for different sentences. We compute the relation prediction loss as a cross-entropy loss over gold arcs. Our final loss is the sum of the arc loss and relation loss. Note that, in comparison with the original biaffine parser (Dozat and Manning, 2017) and its other transformer-based variants (Kondratyuk and ¨ un et al., 2020), we feed wordStraka, 2019; Ust¨ level representations derived from the transformer"
2021.eacl-main.270,N19-1131,0,0.354988,"; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Goldberg, 2014; Cheng and Kartsaklis, 2015; Bastings et al., 2017; Kasai et al., 2019; Zhang et al., 2019a, inter alia). However, we believe that the massive improvements brought about by the LM-pretrained transformers – unexposed to any explicit syntactic signal – warrant a renewed scrutiny of the utility of supervised parsing for high-level language understanding.1,2 The research question we address in this work can be summarized as follows: 1 Disclaimer 1: In this work, we make a clear distinction between Computational Linguistics (CL), i.e., the area of linguistics leveraging computational methods for analyses of human languages and NLP, the area of artificial intelligence tackling human lang"
2021.eacl-main.270,2020.acl-main.151,1,0.861869,"Missing"
2021.findings-acl.431,2020.emnlp-main.40,0,0.0242914,"lopment data in the target the training portions of Russian GSD, PUD, and SynTagRus treebanks. 5 Note that the number of climbs sl needed to reach some hierarchy level depends on the language l: e.g., the hierarchy level joining Tagalog (tl) with Scottish, Irish, and Welsh ({gd, ga, cy}) is reached in sl = 1 climbs from Tagalog, sl = 2 climbs from Scottish and sl = 3 climbs from Irish and Welsh. language. Model selection based on the development set of the source language, on the other hand, overfits the model to the source language, which may hurt effectiveness of the cross-lingual transfer (Keung et al., 2020; Chen and Ritter, 2020). For test treebanks with a respective development portion, T OWER uses that development set for model selection. For low-resource languages l without development treebanks, we compile a proxy development set Dl = ∪{Dk }K k=1 by collecting all development treebanks Dk from the hierarchy level closest to l that encompasses at least one treebank with a development set.6 Intuitively, the more syntactically similar Dl is to l, the more beneficial the model selection based on Dl will be for performance on l, the optimal model checkpoint w.r.t. l should be closer to the model"
2021.findings-acl.431,D19-1279,0,0.0524103,"e leaf node of the target language. We stop ‘climbing’ (i.e., select the set of source treebanks subsumed by the current hierarchy level), when the relative decrease in linguistic similarity of the training sample w.r.t the target language outweighs the increase in size of the training sample. We additionally exploit the linguistic similarity between the target language and its closest sources with existing development treebanks to inform a model selection (that is, early-stopping) heuristic. T OWER substantially outperforms stateof-the-art multilingual parsers – UDPipe (Straka, 2018), UDify (Kondratyuk and Straka, 2019), and ¨ un et al., 2020) on low-resource lanUDapter (Ust¨ guages, while offering comparable performance for high-resource languages. 2 Climbing the T OWER of Treebanks Constructing the T OWER. We start by hierarchically clustering the set of 89 languages from Universal Dependencies 3 based on their syntactic collection of N source treebanks. 2 For the vast majority of world languages there does not exist a single manually annotated syntactic tree. 3 We worked with the UD version 2.5. Figure 1: Part of the syntax-based hierarchical clustering of UD languages (ISO 639-1 codes). similarity. To th"
2021.findings-acl.431,D19-1277,0,0.0232923,"Missing"
2021.findings-acl.431,2020.tacl-1.50,0,0.0256652,"pretrained language models (PLMs) (Devlin Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk et al., 2019; Liu et al., 2019; Brown et al., 2020) and their end-to-end fine-tuning for downstream tasks has reduced the downstream relevance of supervised syntactic parsing. What is more, there is more and more evidence that PLMs implicitly acquire rich syntactic knowledge through large-scale pretraining (Hewitt and Manning, 2019; Chi et al., 2020) and that exposing them to explicit syntax from human-coded treebanks does not offer significant language understanding benefits (Kuncoro et al., 2020; Glavaˇs and Vuli´c, 2021). In order to implicitly acquire syntactic competencies, however, PLMs need language-specific corpora at the scale at which it can only be obtained for a tiny portion of world’s 7,000+ languages. For the remaining vast majority of languages – with limited-size monolingual corpora – explicit syntax still provides valuable linguistic bias for more sample-efficient learning in downstream NLP tasks. Reliable syntactic parsing requires annotated treebanks of reasonable size: this prerequisite is, unfortunately, satisfied for even fewer languages. Despite the multi-year, w"
2021.findings-acl.431,2020.coling-main.345,1,0.895755,"Missing"
2021.findings-acl.431,E17-2002,0,0.0923696,"ata augmentation (S¸ahin and Steedman, 2018; Vania et al., 2019), violate the zero-shot transfer by assuming a small target-language treebank – a requirement unfulfilled for most world languages.2 In this work, we propose a simple and effective heuristic for selecting a good set of source treebanks for any given low-resource target language. In our approach, named T OWER, we first hierarchically cluster all Universal Dependencies (UD) languagues. To this end, we compute syntactic similarity of languages by comparing manually coded vectors of their syntactic properties from the URIEL database (Littell et al., 2017). We then iteratively ‘climb’ that language hierarchy level by level, starting from the leaf node of the target language. We stop ‘climbing’ (i.e., select the set of source treebanks subsumed by the current hierarchy level), when the relative decrease in linguistic similarity of the training sample w.r.t the target language outweighs the increase in size of the training sample. We additionally exploit the linguistic similarity between the target language and its closest sources with existing development treebanks to inform a model selection (that is, early-stopping) heuristic. T OWER substanti"
2021.findings-acl.431,2021.ccl-1.108,0,0.0316735,"Missing"
2021.findings-acl.431,2020.lrec-1.497,0,0.035975,"Missing"
2021.findings-acl.431,2020.emnlp-demos.7,1,0.826128,"Missing"
2021.findings-acl.431,D18-1039,0,0.0281471,"8.0 bm br tr (IMST) 74.3 59.3 44.5 39.8 38.4 sv (TB) Tower 69.2 68.4 58.5 0 UDApter ja (GSD) 69.6 70.0 fo 36.7 35.0 16.4 19.2 19.4 17.3 18.6 40.1 22.2 8.0 kpv myv pcm sa tl 12.1 16.2 wbp 22.2 29.3 33.8 AVG Figure 2: LAS performance of UDify, UDapter and T OWER on 12 high-resource treebanks (top figure), and 11 low-resource languages (bottom figure). 2019) with the deep biaffine parser (Dozat and Manning, 2017) and trains on all UD treebanks; (2) ¨ un et al., 2020) extends mBERT with UDapter (Ust¨ adapter parameters (Houlsby et al., 2019; Pfeiffer et al., 2020) that are contextually generated (Platanios et al., 2018) from URIEL vectors – the parameters of the adapter generator are trained on treebanks of 13 diverse resource-rich languages selected by Kulmizev et al. (2019). We additionally quantify the contributions of T OWER’s heuristic components (TBS and MS, see §2) by evaluating variants in which we (1) remove TBS and train on the closest language with training data (-TBS), (2) remove MS and just select the model checkpoint that performs best on the proxy dev set Dl (-MS), and (3) remove both TBS and MS (-TBS-MS). Training and Optimization Details. We limit input sequences to 128 subword tokens. We us"
2021.findings-acl.431,P15-2040,0,0.0290423,"ustive search over all possible subsets of source treebanks is not only computationally intractable1 but also 1 One can create 2N − 1 different training sets from a 4878 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4878–4888 August 1–6, 2021. ©2021 Association for Computational Linguistics uninformative in true zero-shot scenarios in which there is no development treebank (i.e., any syntactically annotated data) for the target language. Most existing transfer methods therefore either (1) choose one (or a few) best source languages for each target language (Rosa and Zabokrtsky, 2015; Agi´c, 2017; Lin et al., 2019; Litschko et al., 2020) or (2) train a single multilingual parser on all available treebanks; such parsers, based on pretrained multilingual encoders, currently produce best results in low-resource parsing (Kondratyuk ¨ un et al., 2020). Other transand Straka, 2019; Ust¨ fer approaches, e.g., based on data augmentation (S¸ahin and Steedman, 2018; Vania et al., 2019), violate the zero-shot transfer by assuming a small target-language treebank – a requirement unfulfilled for most world languages.2 In this work, we propose a simple and effective heuristic for selec"
2021.findings-acl.431,D18-1545,0,0.0350568,"Missing"
2021.findings-emnlp.410,P18-1073,0,0.0529912,"Missing"
2021.findings-emnlp.410,2020.acl-main.421,1,0.808072,"ponent inserted into a MMT such as mBERT the parameters of the adapter generator as well as (Devlin et al., 2019) or XLM-R (Conneau et al., (ii) at the typological level by conditioning on fea- 2020) with the purpose of specializing the MMT tures from the URIEL database (Littell et al., 2017). for a particular language, in order to either (a) supThe latter additionally enables zero-shot transfer port a new language not covered by the MMT’s to unseen languages. Further, we propose a vari- original multilingual pretraining (Pfeiffer et al., ant of MAD-G in which we generate adapters also 2020b; Artetxe et al., 2020) or (b) recover/improve conditioned on their Transformer layer position the performance for a particular (resource-rich) lan(see Section 3.2), allowing MAD-G to be much guage (Bapna and Firat, 2019; Rust et al., 2021). In more parameter-efficient than adapter-based trans- this work, we adopt the competitive and lightweight fer methods of prior work. (so-called bottleneck) adapter variant of Pfeiffer In experiments on zero-shot cross-lingual trans- et al. (2021a). There, only one adapter module, 4763 consisting of a successive down-projection and up-projection, is injected per Transformer layer"
2021.findings-emnlp.410,N19-1191,0,0.133549,"ross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed as a parameter-efficient means to extend multilingual models to underrepresented languages (Bapna and Firat, 2019; Üstün et al., 2020). The general practice is to train a language adapter on the unlabeled data for each language (Pfeiffer et al., 2020b) via masked language modeling (MLM). However, this generally requires substantial amounts of monolingual data, which prevents adapters from serving under-resourced languages where such additional language-specific capacity would be most useful. To address this deficiency, we propose multilingual adapter generation (MAD-G), a novel paradigm that enables the generation of adapters for low-resource languages by sharing information across languages. Instead of"
2021.findings-emnlp.410,N16-1101,0,0.0319479,"he nl languages of interest. 2 According to Pfeiffer et al. (2020a, 2021a) and Rücklé et al. (2021), such an architecture with a single adapter per Transformer layer is more parameter-efficient while performing on par with the architecture of Houlsby et al. (2019) with two adapters per Transformer layer (one after the multi-head attention sublayer and one after the feed-forward sublayer). 3 Other examples include the training of language-specific pretrained language models (Rust et al., 2021) as well as language pair-specific encoder–decoder models for machine translation (Luong et al., 2016; Firat et al., 2016). In CPG, the only language-specific parameters that we learn are the low-dimensional language embeddings λ(l) ∈ Rdl . These are used by the generator g, a hyper-network (Ha et al., 2017) component4 with its own parameterization φ, to produce the language-specific parameterization of the main model: θ (l) = gφ (λ(l) ). While g can in principle be any differentiable function (i.e., arbitrarily deep neural model), in practice it is typically set to a simple linear projection (i.e., φ = W ): gW (λ(l) ) , W λ(l) , (2) where W ∈ Rnp ×dl is a learnable weight matrix, np being the number of parameter"
2021.findings-emnlp.410,P19-1070,1,0.898419,"Missing"
2021.findings-emnlp.410,2021.eacl-main.270,1,0.690338,"Missing"
2021.findings-emnlp.410,2020.acl-main.560,0,0.0116496,"e generation of task-agnostic LAs that can support downstream cross-lingual transfer for arbitrary NLP tasks. 4 A hyper-network is a neural model that generates the parameters of another (main) neural model. 5 Training MAD-G on 95 languages with dl = 32 (this work) achieves roughly a threefold saving in parameter size. 4764 3 MAD-G: Methodology MAD-G aims to enable resource-efficient adaptation of MMTs to a wide range of previously unseen, radically resource-poor languages,6 and contribute in this manner to more sustainable (Strubell et al., 2019; Moosavi et al., 2020) and more inclusive NLP (Joshi et al., 2020). We couple (i) the computational efficiency of the light-weight adapters (cf. Section 2.1) and (ii) knowledge sharing and zero-shot language transfer capabilities of CPG (cf. Section 2.2), with (iii) external linguistic (i.e., typological) knowledge (Ponti et al., 2019a) towards supporting arbitrary NLP tasks for (even radically) resource-poor languages. MAD-G mitigates important limitations of prior work. Unlike Üstün et al. (2020), we generate taskagnostic LAs, (re)usable across NLP tasks. Unlike the MAD-X framework (Pfeiffer et al., 2020b), which trains LAs independently for each language"
2021.findings-emnlp.410,2020.emnlp-main.363,1,0.824906,"Missing"
2021.findings-emnlp.410,E17-2002,0,0.408844,"rs can be leveraged in typically achieved through the use of adapter layers arbitrary downstream tasks (Pfeiffer et al., 2020b). (Houlsby et al., 2019; Pfeiffer et al., 2020b). MAD-G shares information across languages (i) In particular, a language adapter is a light-weight at the level of hidden representations by sharing component inserted into a MMT such as mBERT the parameters of the adapter generator as well as (Devlin et al., 2019) or XLM-R (Conneau et al., (ii) at the typological level by conditioning on fea- 2020) with the purpose of specializing the MMT tures from the URIEL database (Littell et al., 2017). for a particular language, in order to either (a) supThe latter additionally enables zero-shot transfer port a new language not covered by the MMT’s to unseen languages. Further, we propose a vari- original multilingual pretraining (Pfeiffer et al., ant of MAD-G in which we generate adapters also 2020b; Artetxe et al., 2020) or (b) recover/improve conditioned on their Transformer layer position the performance for a particular (resource-rich) lan(see Section 3.2), allowing MAD-G to be much guage (Bapna and Firat, 2019; Rust et al., 2021). In more parameter-efficient than adapter-based trans-"
2021.findings-emnlp.410,2021.eacl-main.39,1,0.794583,"amed entity recognition (NER) on the MasakhaNER dataset for African languages (Adelani et al., 2021). For POS and DP, we evaluate on a substantial subset of all UD languages with available treebanks.8 We discern between three language groups in evaluation, with some examples in Table 1: (i) mBERT-seen languages are those included in mBERT’s pretraining; (ii) MAD-G-seen languages were not part of mBERT’s pretraining but are included in MAD8 For POS and DP, we omit only (i) languages with scripts unseen in mBERT’s pretraining, where mBERT’s tokenizer predominantly produces unknown (UNK) tokens (Pfeiffer et al., 2021b), (ii) languages lacking any information in URIEL, and (iii) languages whose treebanks have missing fields. For MasakhaNER, we evaluate on all dataset languages except Amharic, as Amharic also uses a script unseen by mBERT. G training; and (iii) unseen languages are those not included in mBERT pretraining nor in MAD-G training. 4.1 Baselines and MAD-G Variants mBERT is an MMT pretrained on the Wikipedias of 104 languages. We use mBERT as the base MMT for MAD-G. XLM-R is a state-of-the-art MMT pretrained on the CommonCrawl data of 100 languages (Conneau et al., 2020).9 We evaluate them in the"
2021.findings-emnlp.410,2020.emnlp-demos.7,1,0.70459,"Missing"
2021.findings-emnlp.410,2020.emnlp-main.617,1,0.622952,"Missing"
2021.findings-emnlp.410,2021.emnlp-main.800,1,0.87244,"Missing"
2021.findings-emnlp.410,P19-1493,0,0.0388325,"te that MAD-G’s transfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available taskspecific training data; and (ii) by further finetuning generated MAD-G adapters for languages with monolingual data. 1 Introduction et al., 2019) and, more recently, massively multilingual Transformers (MMTs) like mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and mT5 (Xue et al., 2021) as main vehicles of cross-lingual transfer. Although MMTs display impressive (zero-shot) cross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed as a parameter-efficient means to extend multilingual models to underrepresented languages (Bapna and Firat, 2019; Üstün et al., 2020). The gen"
2021.findings-emnlp.410,D18-1039,0,0.123067,"each language (Pfeiffer et al., 2020b) via masked language modeling (MLM). However, this generally requires substantial amounts of monolingual data, which prevents adapters from serving under-resourced languages where such additional language-specific capacity would be most useful. To address this deficiency, we propose multilingual adapter generation (MAD-G), a novel paradigm that enables the generation of adapters for low-resource languages by sharing information across languages. Instead of learning separate adapters for each language, MAD-G leverages contextual parameter generation (CPG; Platanios et al., 2018a; Ponti et al., 2019b), that is, it learns a single model that can generate a language adapter for an arbitrary target language. At the core of MAD-G is a contextual parameter generator which 1 mBERT and XLM-R have been trained on corpora from Multilingual NLP has witnessed large ad104 and 100 languages, respectively. According to Glottolog vances, with cross-lingual word embedding spaces (Hammarström et al., 2017), however, there are over 7,000 (Mikolov et al., 2013; Artetxe et al., 2018; Glavaš languages spoken around the world. 4762 Findings of the Association for Computational Linguistics"
2021.findings-emnlp.410,2020.emnlp-main.185,1,0.891273,"Missing"
2021.findings-emnlp.410,J19-3005,1,0.858703,"Missing"
2021.findings-emnlp.410,D19-1288,1,0.883413,"Missing"
2021.findings-emnlp.410,2021.emnlp-main.626,1,0.768969,"genealogical ties to high(er)-resource languages. CPG is a technique introduced by Platanios et al. (2018a) to address these drawbacks. While originally conceived for neural machine translation (NMT), CPG can be applied to any neural model f parameterized by θ, for which we aim to learn parameterizations for a number of different contexts; in multilingual NLP, these “contexts” are languages. In the instance-per-language approach, an independent parameterization θ (l) , l ∈ {1, . . . , nl }, is learned for each of the nl languages of interest. 2 According to Pfeiffer et al. (2020a, 2021a) and Rücklé et al. (2021), such an architecture with a single adapter per Transformer layer is more parameter-efficient while performing on par with the architecture of Houlsby et al. (2019) with two adapters per Transformer layer (one after the multi-head attention sublayer and one after the feed-forward sublayer). 3 Other examples include the training of language-specific pretrained language models (Rust et al., 2021) as well as language pair-specific encoder–decoder models for machine translation (Luong et al., 2016; Firat et al., 2016). In CPG, the only language-specific parameters that we learn are the low-dimens"
2021.findings-emnlp.410,2021.acl-long.243,1,0.823183,"Missing"
2021.findings-emnlp.410,2021.findings-acl.106,1,0.880987,"2) where W ∈ Rnp ×dl is a learnable weight matrix, np being the number of parameters of f . The total number of parameters learned when training nl independent models is nl np , whereas the number of parameters in the W matrix is dl np . Therefore, neglecting the small number of parameters dedicated to language embeddings, the CPG approach uses fewer parameters when dl < nl .5 More importantly, in multilingual training the generator matrix W is shared across all languages, which enables knowledge sharing across languages and leads to improved transfer performance. Platanios et al. (2018b) and Ponti et al. (2021a) opt for randomly initializing language embeddings λ(l) and learning them end-to-end. Specified like this, however, CPG cannot generalize to languages unseen in training, as it would lack embeddings for those languages at inference. To support generalization to arbitrary new languages, one must ground language embeddings in some external language representation, available for many languages. To this end, Ponti et al. (2019b) exploit typological language vectors from the URIEL database (Littell et al., 2017) directly as language embeddings to generate a full set of model parameters. In a simi"
2021.findings-emnlp.410,2020.emnlp-main.180,0,0.119503,"Missing"
2021.findings-emnlp.410,D19-1077,0,0.021338,"sfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available taskspecific training data; and (ii) by further finetuning generated MAD-G adapters for languages with monolingual data. 1 Introduction et al., 2019) and, more recently, massively multilingual Transformers (MMTs) like mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and mT5 (Xue et al., 2021) as main vehicles of cross-lingual transfer. Although MMTs display impressive (zero-shot) cross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed as a parameter-efficient means to extend multilingual models to underrepresented languages (Bapna and Firat, 2019; Üstün et al., 2020). The general practice is to tr"
2021.findings-emnlp.410,2021.naacl-main.41,0,0.0352696,"offers substantial benefits for low-resource languages, particularly on the NER task in low-resource African languages. Finally, we demonstrate that MAD-G’s transfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available taskspecific training data; and (ii) by further finetuning generated MAD-G adapters for languages with monolingual data. 1 Introduction et al., 2019) and, more recently, massively multilingual Transformers (MMTs) like mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and mT5 (Xue et al., 2021) as main vehicles of cross-lingual transfer. Although MMTs display impressive (zero-shot) cross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed a"
2021.findings-emnlp.411,N19-3002,0,0.307635,"er) and the PLMs models, being trained with language modeling objectives, consequently encode these biased associations in their parameters. While this effect can lend itself to diachronic analysis of societal biases (e.g., Garg et al., 2018; Walter et al., 2021), it represents stereotyping, one of the main types of representational harm (Blodgett et al., 2020) and, if unmitigated, may cause severe ethical issues in various sociotechnical deployment scenarios. To alleviate this problem and ensure fair language technology, previous work introduced a wide range of bias mitigation methods (e.g., Bordia and Bowman, 2019; Dev et al., 2020; Lauscher et al., 2020a, inter alia). All existing debiasing approaches, however, modify all parameters of the PLMs which has two prominent shortcomings: (1) it comes with a high computational cost1 and (2) can lead to (catastrophic) forgetting (McCloskey and Cohen, 1989; Kirkpatrick et al., 2017) of the useful distributional knowledge obtained during pretraining. For example, Webster et al. (2020) incorporate counterfactual debiasing already into BERT’s pretraining: this implies a debiasing framework in which a separate “debiased BERT” instance needs to be trained from scra"
2021.findings-emnlp.411,2021.bea-1.11,1,0.739086,"(2019), who demonstrated their ef4789 fectiveness and efficiency for general language understanding (NLU). Since then, they have been employed for various purposes: apart from NLU, task adapters have been explored for natural language generation (Lin et al., 2020) and machine translation quality estimation (Yang et al., 2020). Other works use language adapters encoding language-specific knowledge, e.g., for machine translation (Philip et al., 2020; Kim et al., 2019) or multilingual parsing (Üstün et al., 2020). Further, adapters have been shown useful in domain adaptation (Pham et al., 2020; Glavaš et al., 2021) and for injection of external knowlege (Wang et al., 2020; Lauscher et al., 2020b). Pfeiffer et al. (2020b) use adapters to learn both language and task representations. Building on top of this, Vidoni et al. (2020) prevent adapters from learning redundant information by introducing orthogonality constraints. Debiasing Methods. A recent survey covering research on stereotypical biases in NLP is provided by Blodgett et al. (2020). In the following, we focus on approaches for mitigating biases from PLMs, which are largely inspired by debiasing for static word embeddings (e.g., Bolukbasi et al.,"
2021.findings-emnlp.411,D19-1530,0,0.0191962,"embeddings (e.g., Bolukbasi et al., 2016; Dev and Phillips, 2019; Lauscher et al., 2020a; Karve et al., 2019, inter alia). While several works propose projection-based debiasing for PLMs (e.g., Dev et al., 2020; Liang et al., 2020; Kaneko and Bollegala, 2021), most of the debiasing approaches require training. Here, some methods rely on debiasing objectives (e.g., Qian et al., 2019; Bordia and Bowman, 2019). In contrast, the debiasing approach we employ in this work, CDA (Zhao et al., 2018), relies on adapting the input data and is more generally applicable. Variants of CDA exist, e.g., Hall Maudslay et al. (2019) use names as bias proxies and substitute instances instead of augmenting the data, whereas Zhao et al. (2019) use CDA at test time to neutralize the models’ biased predictions. Webster et al. (2020) investigate one-sided vs. twosided CDA for debiasing BERT in pretraining and show dropout to be effective for bias mitigation. evaluated A DELE on gender debiasing of BERT, demonstrating its effectiveness on three intrinsic and two extrinsic debiasing benchmarks. Further, applying A DELE on top of mBERT, we successfully transfered its debiasing effects to six target languages. Finally, we showed t"
2021.findings-emnlp.411,2021.eacl-main.107,0,0.0224008,"idoni et al. (2020) prevent adapters from learning redundant information by introducing orthogonality constraints. Debiasing Methods. A recent survey covering research on stereotypical biases in NLP is provided by Blodgett et al. (2020). In the following, we focus on approaches for mitigating biases from PLMs, which are largely inspired by debiasing for static word embeddings (e.g., Bolukbasi et al., 2016; Dev and Phillips, 2019; Lauscher et al., 2020a; Karve et al., 2019, inter alia). While several works propose projection-based debiasing for PLMs (e.g., Dev et al., 2020; Liang et al., 2020; Kaneko and Bollegala, 2021), most of the debiasing approaches require training. Here, some methods rely on debiasing objectives (e.g., Qian et al., 2019; Bordia and Bowman, 2019). In contrast, the debiasing approach we employ in this work, CDA (Zhao et al., 2018), relies on adapting the input data and is more generally applicable. Variants of CDA exist, e.g., Hall Maudslay et al. (2019) use names as bias proxies and substitute instances instead of augmenting the data, whereas Zhao et al. (2019) use CDA at test time to neutralize the models’ biased predictions. Webster et al. (2020) investigate one-sided vs. twosided CDA"
2021.findings-emnlp.411,W19-3806,0,0.0201456,"020; Lauscher et al., 2020b). Pfeiffer et al. (2020b) use adapters to learn both language and task representations. Building on top of this, Vidoni et al. (2020) prevent adapters from learning redundant information by introducing orthogonality constraints. Debiasing Methods. A recent survey covering research on stereotypical biases in NLP is provided by Blodgett et al. (2020). In the following, we focus on approaches for mitigating biases from PLMs, which are largely inspired by debiasing for static word embeddings (e.g., Bolukbasi et al., 2016; Dev and Phillips, 2019; Lauscher et al., 2020a; Karve et al., 2019, inter alia). While several works propose projection-based debiasing for PLMs (e.g., Dev et al., 2020; Liang et al., 2020; Kaneko and Bollegala, 2021), most of the debiasing approaches require training. Here, some methods rely on debiasing objectives (e.g., Qian et al., 2019; Bordia and Bowman, 2019). In contrast, the debiasing approach we employ in this work, CDA (Zhao et al., 2018), relies on adapting the input data and is more generally applicable. Variants of CDA exist, e.g., Hall Maudslay et al. (2019) use names as bias proxies and substitute instances instead of augmenting the data, whe"
2021.findings-emnlp.411,D19-1080,0,0.0283841,"itional task-specific adapter (TA) et al., 2018) have been introduced to NLP by on top of A DELE’s debiasing adapter and (2) update Houlsby et al. (2019), who demonstrated their ef4789 fectiveness and efficiency for general language understanding (NLU). Since then, they have been employed for various purposes: apart from NLU, task adapters have been explored for natural language generation (Lin et al., 2020) and machine translation quality estimation (Yang et al., 2020). Other works use language adapters encoding language-specific knowledge, e.g., for machine translation (Philip et al., 2020; Kim et al., 2019) or multilingual parsing (Üstün et al., 2020). Further, adapters have been shown useful in domain adaptation (Pham et al., 2020; Glavaš et al., 2021) and for injection of external knowlege (Wang et al., 2020; Lauscher et al., 2020b). Pfeiffer et al. (2020b) use adapters to learn both language and task representations. Building on top of this, Vidoni et al. (2020) prevent adapters from learning redundant information by introducing orthogonality constraints. Debiasing Methods. A recent survey covering research on stereotypical biases in NLP is provided by Blodgett et al. (2020). In the following"
2021.findings-emnlp.411,W19-3823,0,0.069481,"three intrinsic and two extrinsic bias measures, renders A DELE very effective in bias mitigation. We further show that – due to its modular nature – A DELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer A DELE to six target languages. 1 Introduction Recent work has shown that pretrained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), or GPT-2 (Radford et al., 2019) tend to exhibit a range of stereotypical societal biases, such as racism and sexism (e.g., Kurita et al., 2019; Dev et al., 2020; Webster et al., 2020; Nangia et al., 2020; Barikeri et al., 2021, ∗ Equal contribution. Most of the work was conducted while Anne Lauscher was employed at the University of Mannheim. † inter alia). The reason for this lies in the distributional nature of these models: human-produced corpora on which these models are trained are abundant with stereotypically biased concept cooccurrences (for instance, male terms like man or son appear more often together with certain career terms like doctor or programmer than female terms like women or daughter) and the PLMs models, being t"
2021.findings-emnlp.411,S19-1010,1,0.890247,"Missing"
2021.findings-emnlp.411,2020.deelio-1.5,1,0.834518,"Missing"
2021.findings-emnlp.411,2020.findings-emnlp.41,0,0.0457855,"Missing"
2021.findings-emnlp.411,2020.emnlp-main.154,0,0.0160569,"LE very effective in bias mitigation. We further show that – due to its modular nature – A DELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer A DELE to six target languages. 1 Introduction Recent work has shown that pretrained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), or GPT-2 (Radford et al., 2019) tend to exhibit a range of stereotypical societal biases, such as racism and sexism (e.g., Kurita et al., 2019; Dev et al., 2020; Webster et al., 2020; Nangia et al., 2020; Barikeri et al., 2021, ∗ Equal contribution. Most of the work was conducted while Anne Lauscher was employed at the University of Mannheim. † inter alia). The reason for this lies in the distributional nature of these models: human-produced corpora on which these models are trained are abundant with stereotypically biased concept cooccurrences (for instance, male terms like man or son appear more often together with certain career terms like doctor or programmer than female terms like women or daughter) and the PLMs models, being trained with language modeling objectives, consequently encode"
2021.findings-emnlp.411,N18-1202,0,0.0364062,"meters frozen) via language modeling training on a counterfactually augmented corpus. We showcase A DELE in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders A DELE very effective in bias mitigation. We further show that – due to its modular nature – A DELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer A DELE to six target languages. 1 Introduction Recent work has shown that pretrained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), or GPT-2 (Radford et al., 2019) tend to exhibit a range of stereotypical societal biases, such as racism and sexism (e.g., Kurita et al., 2019; Dev et al., 2020; Webster et al., 2020; Nangia et al., 2020; Barikeri et al., 2021, ∗ Equal contribution. Most of the work was conducted while Anne Lauscher was employed at the University of Mannheim. † inter alia). The reason for this lies in the distributional nature of these models: human-produced corpora on which these models are trained are abundant with stereotypically biased concept cooccurrences (for instance, male"
2021.findings-emnlp.411,2021.eacl-main.39,0,0.025076,"the literature, e.g., via additional debiasing loss objectives Qian et al. (2019); Bordia and Bowman (2019); Lauscher et al. (2020a, inter alia) or data-driven approaches such as Counterfactual Data Augmentation (Zhao et al., 2018). For simplicity, we opt for the data-driven CDA approach: it has been shown to offer reliable debiasing performance (Zhao et al., 2018; Webster et al., 2020) and, unlike other approaches, it does not require any modifications of the model architecture nor training procedure. 2.1 Debiasing Adapters In this work, we employ the simple adapter architecture proposed by Pfeiffer et al. (2021), in which only one adapter module is added to each layer of the pretrained Transformer, after the feed-forward sub-layer. The more widely used architecture of Houlsby et al. (2019) inserts two adapter modules per Transformer layer, with the other adapter injected after the multi-head attention sublayer. We opt for the “Pfeiffer architecture” because in comparison with the “Houlsby architecture” it is more parameter-efficient and has been shown to yield slightly better performance on a wide range of downstream NLP tasks (Pfeiffer et al., 2020a, 2021). The output of the adapter, a two-layer fee"
2021.findings-emnlp.411,P19-1355,0,0.0230629,"1) it comes with a high computational cost1 and (2) can lead to (catastrophic) forgetting (McCloskey and Cohen, 1989; Kirkpatrick et al., 2017) of the useful distributional knowledge obtained during pretraining. For example, Webster et al. (2020) incorporate counterfactual debiasing already into BERT’s pretraining: this implies a debiasing framework in which a separate “debiased BERT” instance needs to be trained from scratch for each individual bias type and specification. In sum, current debiasing procedures designed for pretraining or full fine-tuning of PLMs have a large carbon footprint (Strubell et al., 2019) and consequently 1 While a full fine-tuning approach to PLM debiasing may still be feasible for moderate-sized PLMs like BERT (Devlin et al., 2019), it is prohibitively computationally expensive for giant language models like GPT-3 (Brown et al., 2020) or GShard (Lepikhin et al., 2020). 4782 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4782–4797 November 7–11, 2021. ©2021 Association for Computational Linguistics jeopardize the sustainability (Moosavi et al., 2020) of fair representation learning in NLP. In this work, we move towards more sustainable removal of"
2021.findings-emnlp.411,2020.emnlp-demos.7,0,0.0626097,"Missing"
2021.findings-emnlp.411,2020.emnlp-main.180,0,0.0545722,"Missing"
2021.findings-emnlp.411,2020.emnlp-main.617,0,0.0391189,"Missing"
2021.findings-emnlp.411,2020.wmt-1.72,0,0.0404145,"date Houlsby et al. (2019), who demonstrated their ef4789 fectiveness and efficiency for general language understanding (NLU). Since then, they have been employed for various purposes: apart from NLU, task adapters have been explored for natural language generation (Lin et al., 2020) and machine translation quality estimation (Yang et al., 2020). Other works use language adapters encoding language-specific knowledge, e.g., for machine translation (Philip et al., 2020; Kim et al., 2019) or multilingual parsing (Üstün et al., 2020). Further, adapters have been shown useful in domain adaptation (Pham et al., 2020; Glavaš et al., 2021) and for injection of external knowlege (Wang et al., 2020; Lauscher et al., 2020b). Pfeiffer et al. (2020b) use adapters to learn both language and task representations. Building on top of this, Vidoni et al. (2020) prevent adapters from learning redundant information by introducing orthogonality constraints. Debiasing Methods. A recent survey covering research on stereotypical biases in NLP is provided by Blodgett et al. (2020). In the following, we focus on approaches for mitigating biases from PLMs, which are largely inspired by debiasing for static word embeddings (e"
2021.findings-emnlp.411,2020.emnlp-main.361,0,0.0322207,"ffi (1) inject an additional task-specific adapter (TA) et al., 2018) have been introduced to NLP by on top of A DELE’s debiasing adapter and (2) update Houlsby et al. (2019), who demonstrated their ef4789 fectiveness and efficiency for general language understanding (NLU). Since then, they have been employed for various purposes: apart from NLU, task adapters have been explored for natural language generation (Lin et al., 2020) and machine translation quality estimation (Yang et al., 2020). Other works use language adapters encoding language-specific knowledge, e.g., for machine translation (Philip et al., 2020; Kim et al., 2019) or multilingual parsing (Üstün et al., 2020). Further, adapters have been shown useful in domain adaptation (Pham et al., 2020; Glavaš et al., 2021) and for injection of external knowlege (Wang et al., 2020; Lauscher et al., 2020b). Pfeiffer et al. (2020b) use adapters to learn both language and task representations. Building on top of this, Vidoni et al. (2020) prevent adapters from learning redundant information by introducing orthogonality constraints. Debiasing Methods. A recent survey covering research on stereotypical biases in NLP is provided by Blodgett et al. (2020"
2021.findings-emnlp.411,P19-2031,0,0.127983,"for parameter-efficient fine-tuning of PLMs, injected into the PLM layers. In downstream fine-tuning, all original PLM parameters are kept frozen and only the adapters are trained. Because adapters have fewer parameters than the original PLM, adapter-based fine-tuning is more computationally efficient. And since finetuning does not update the PLM’s original parameters, all distributional knowledge is preserved. The debiasing adapters could, in principle, be trained using any of the debiasing strategies and training objectives from the literature, e.g., via additional debiasing loss objectives Qian et al. (2019); Bordia and Bowman (2019); Lauscher et al. (2020a, inter alia) or data-driven approaches such as Counterfactual Data Augmentation (Zhao et al., 2018). For simplicity, we opt for the data-driven CDA approach: it has been shown to offer reliable debiasing performance (Zhao et al., 2018; Webster et al., 2020) and, unlike other approaches, it does not require any modifications of the model architecture nor training procedure. 2.1 Debiasing Adapters In this work, we employ the simple adapter architecture proposed by Pfeiffer et al. (2021), in which only one adapter module is added to each layer of"
2021.findings-emnlp.411,N18-2002,0,0.0587133,"Missing"
2021.findings-emnlp.411,2020.cl-4.5,0,0.0471158,"Missing"
2021.findings-emnlp.411,N18-1101,0,0.0208107,"score with the actual STS task performance score (Pearson correlation with human similarity scores), measured on the STS-B development set. stances. Following the original work, we compute two bias scores: (1) the fraction neutral (FN) score is the percentage of instances for which the model predicts the NEUTRAL class; (2) net neutral (NN) score is the average probability that the model assigns to the NEUTRAL class across all instances. In both cases, the higher score corresponds to a lower bias. We couple FN and NN on Bias-NLI with the actual NLI accuracy on the MNLI matched development set (Williams et al., 2018). 3.2 Experimental Setup Data. Aligned with BERT’s pretraining, we carry out the debiasing MLM training on the concatenation of the English Wikipedia and the BookCorpus (Zhu et al., 2015). Since we are only training the parameters of the debiasing adapters, we uniformly subsample the corpus to one third of its original size. We adopt the set of gender term pairs T for CDA from Zhao et al. (2018) (e.g., actor-actress, bride-groom)4 and augment it with three additional pairs: his-her, himself -herself, and male-female, resulting with the total of 193 term pairs. Our final debiasing CDA corpus co"
2021.findings-emnlp.411,2020.eamt-1.4,0,0.0402334,"rness forgetting and which is aligned with the modular debiasing nature of A DELE: we Adapter Layers in NLP. Adapters (Rebuffi (1) inject an additional task-specific adapter (TA) et al., 2018) have been introduced to NLP by on top of A DELE’s debiasing adapter and (2) update Houlsby et al. (2019), who demonstrated their ef4789 fectiveness and efficiency for general language understanding (NLU). Since then, they have been employed for various purposes: apart from NLU, task adapters have been explored for natural language generation (Lin et al., 2020) and machine translation quality estimation (Yang et al., 2020). Other works use language adapters encoding language-specific knowledge, e.g., for machine translation (Philip et al., 2020; Kim et al., 2019) or multilingual parsing (Üstün et al., 2020). Further, adapters have been shown useful in domain adaptation (Pham et al., 2020; Glavaš et al., 2021) and for injection of external knowlege (Wang et al., 2020; Lauscher et al., 2020b). Pfeiffer et al. (2020b) use adapters to learn both language and task representations. Building on top of this, Vidoni et al. (2020) prevent adapters from learning redundant information by introducing orthogonality constrain"
2021.findings-emnlp.411,N19-1064,0,0.0401071,"Missing"
2021.findings-emnlp.411,N18-2003,0,0.390345,"nd incorporate the “debiasing” knowledge only in those parameters, without changing the pretrained knowledge in the PLM. We show that, while being substantially more efficient (i.e., sustainable) than existing state-of-the-art debiasing approaches, A DELE is just as effective in bias attenuation. Contributions. The contributions of this work are three-fold: (i) we first present A DELE, our novel adapter-based framework for parameterefficient and knowledge-preserving debiasing of PLMs. We combine A DELE with one of the most effective debiasing strategies, Counterfactual Data Augmentation (CDA; Zhao et al., 2018), and demonstrate its effectiveness in gender-debiasing of BERT (Devlin et al., 2019), the most widely used PLM. (ii) We benchmark A DELE in what is arguably the most comprehensive set of bias measures and data sets for both intrinsic and extrinsic evaluation of biases in representation spaces spanned by PLMs. Additionally, we study a previously neglected effect of fairness forgetting present when debiased PLMs are subjected to large-scale downstream training for specific tasks (e.g., natural language inference, NLI); we show that A DELE’s modular nature allows to counter this undesirable effe"
D17-1185,E12-1004,0,0.598618,"ymy and meronymy. On the other hand, models for embedding KBs (Bordes et al., 2013; Socher et al., 2013; Yang et al., 2015) uniformly model both symmetric and asymmetric relations. They learn a single vector representation (i.e., embedding) for each KB concept, assuming implicitly that the same concept representation is equally useful for predicting symmetric and asymmetric relations alike. Relation-specific learning-based models have, to the largest extent, targeted hypernymy. Distributional models predict the hypernymy relations by combining raw distributional vectors of concepts in a pair (Baroni et al., 2012; Roller et al., 2014; Santus et al., 2014), whereas path-based models base predictions on lexico-syntactic paths from co-occurrence contexts obtained from a large corpus (Snow et al., 2004; Nakashole et al., 2012; Shwartz et al., 2016). Shwartz et al. (2016) combine the path-based and distributional models to 1757 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1757–1767 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics reach state-of-the-art performance in hypernymy detection. Both distributional and path"
D17-1185,W11-2501,0,0.186963,"prediction performance, we evaluate two reduced models variants. 4.1 Datasets We evaluate the Dual Tensor model on the following hypernymy and meronymy detection datasets: HypeNet dataset. Arguing that existing datasets were too small for training their recurrent network, Shwartz et al. (2016) compiled this dataset for hypernymy detection from several external KBs, taking only pairs of concepts in direct relation (i.e., no transitive closure). Other hypernymy detection datasets. We additionally evaluate the Dual Tensor model on four smaller datasets for hypernymy detection: (1) BLESS dataset (Baroni and Lenci, 2011) and EVALuation dataset (Santus et al., 2015) contain instances of hypernymy and four other relations. BLESS additionally contains random word pairs; (2) Weeds dataset (Weeds et al., 2014) contains hypernymy and co-hyponymy pairs; (3) Benotto dataset (Benotto, 2015) couples hypernymy pairs with synonymy and antonymy pairs. Because these datasets contain at most several thousand pairs, we only use them to evaluate the performance of models trained on larger datasets; WN-Hy and WN-Me datasets. We create these datasets by taking concept pairs from WordNet. We take all instances from the transitiv"
D17-1185,P99-1008,0,0.0561512,"edicting all relations, symmetric and asymmetric alike. By directly updating concept embeddings in training, they cannot make relation predictions for concepts outside of the training set. Hypernymy and Meronymy Detection. Hypernymy and meronymy are arguably the two most prominent asymmetric lexico-semantic relations. Methods for their detection can roughly be classified as either distributional or path-based. Pathbased methods consider lexico-syntactic paths con1758 necting pairs of words in their co-occurrence contexts in large corpus. Early approaches, e.g., Hearst (1992) for hypernymy and Berland and Charniak (1999) for meronymy, exploited a small set of manually created lexico-syntactic patterns that imply a relation of interest (e.g., a such as b). Subsequent approaches looked at ways to eliminate the need for manual compilation of extraction patterns. Pantel and Pennacchiotti (2006) and Girju et al. (2006) proposed bootstrapping approaches to meronymy detection, starting from a seed set of part-whole pairs. Snow et al. (2004) provided all dependency paths connecting the concepts in corpus to a logistic regression classifier for hypernymy detection. Distributional methods detect asymmetric relations us"
D17-1185,E17-1056,1,0.792705,"Missing"
D17-1185,N15-1184,0,0.197212,"Missing"
D17-1185,N13-1092,0,0.0797104,"Missing"
D17-1185,J06-1005,0,0.0287255,"c relations. Methods for their detection can roughly be classified as either distributional or path-based. Pathbased methods consider lexico-syntactic paths con1758 necting pairs of words in their co-occurrence contexts in large corpus. Early approaches, e.g., Hearst (1992) for hypernymy and Berland and Charniak (1999) for meronymy, exploited a small set of manually created lexico-syntactic patterns that imply a relation of interest (e.g., a such as b). Subsequent approaches looked at ways to eliminate the need for manual compilation of extraction patterns. Pantel and Pennacchiotti (2006) and Girju et al. (2006) proposed bootstrapping approaches to meronymy detection, starting from a seed set of part-whole pairs. Snow et al. (2004) provided all dependency paths connecting the concepts in corpus to a logistic regression classifier for hypernymy detection. Distributional methods detect asymmetric relations using only distributional vectors of words as input. Distributional models come in both unsupervised and supervised flavors. Unsupervised metrics for hypernymy detection assume either that the hyponym’s contexts are included in the hypernym’s contexts (Weeds and Weir, 2003; Kotlerman et al., 2010) or"
D17-1185,C92-2082,0,0.374207,"e same concept embeddings for predicting all relations, symmetric and asymmetric alike. By directly updating concept embeddings in training, they cannot make relation predictions for concepts outside of the training set. Hypernymy and Meronymy Detection. Hypernymy and meronymy are arguably the two most prominent asymmetric lexico-semantic relations. Methods for their detection can roughly be classified as either distributional or path-based. Pathbased methods consider lexico-syntactic paths con1758 necting pairs of words in their co-occurrence contexts in large corpus. Early approaches, e.g., Hearst (1992) for hypernymy and Berland and Charniak (1999) for meronymy, exploited a small set of manually created lexico-syntactic patterns that imply a relation of interest (e.g., a such as b). Subsequent approaches looked at ways to eliminate the need for manual compilation of extraction patterns. Pantel and Pennacchiotti (2006) and Girju et al. (2006) proposed bootstrapping approaches to meronymy detection, starting from a seed set of part-whole pairs. Snow et al. (2004) provided all dependency paths connecting the concepts in corpus to a logistic regression classifier for hypernymy detection. Distrib"
D17-1185,D15-1242,0,0.156284,"Missing"
D17-1185,N15-1098,0,0.260129,"Missing"
D17-1185,N16-1018,0,0.00553929,"Missing"
D17-1185,D12-1104,0,0.00462594,"presentation (i.e., embedding) for each KB concept, assuming implicitly that the same concept representation is equally useful for predicting symmetric and asymmetric relations alike. Relation-specific learning-based models have, to the largest extent, targeted hypernymy. Distributional models predict the hypernymy relations by combining raw distributional vectors of concepts in a pair (Baroni et al., 2012; Roller et al., 2014; Santus et al., 2014), whereas path-based models base predictions on lexico-syntactic paths from co-occurrence contexts obtained from a large corpus (Snow et al., 2004; Nakashole et al., 2012; Shwartz et al., 2016). Shwartz et al. (2016) combine the path-based and distributional models to 1757 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1757–1767 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics reach state-of-the-art performance in hypernymy detection. Both distributional and path-based methods, however, model asymmetry only implicitly (e.g., via the order of embeddings in the concatenation). Besides, path-based models are languagedependent since they require syntactically preprocessed dat"
D17-1185,P06-1015,0,0.0215926,"prominent asymmetric lexico-semantic relations. Methods for their detection can roughly be classified as either distributional or path-based. Pathbased methods consider lexico-syntactic paths con1758 necting pairs of words in their co-occurrence contexts in large corpus. Early approaches, e.g., Hearst (1992) for hypernymy and Berland and Charniak (1999) for meronymy, exploited a small set of manually created lexico-syntactic patterns that imply a relation of interest (e.g., a such as b). Subsequent approaches looked at ways to eliminate the need for manual compilation of extraction patterns. Pantel and Pennacchiotti (2006) and Girju et al. (2006) proposed bootstrapping approaches to meronymy detection, starting from a seed set of part-whole pairs. Snow et al. (2004) provided all dependency paths connecting the concepts in corpus to a logistic regression classifier for hypernymy detection. Distributional methods detect asymmetric relations using only distributional vectors of words as input. Distributional models come in both unsupervised and supervised flavors. Unsupervised metrics for hypernymy detection assume either that the hyponym’s contexts are included in the hypernym’s contexts (Weeds and Weir, 2003; Ko"
D17-1185,E17-1007,0,0.387102,"try and relation-specific embedding specialization. Meronymy classification. We next evaluate the meronymy classification performance of the models on the WN-Me dataset. The results are shown in Table 3. Same as in the case of hypernymy classification, D UAL -T significantly outperforms all three baselines, with S INGLE -T outperforming B ILIN -P ROD. All distributional models we evaluate achieve poorer performance on meronymy than hypernymy detection, especially considering that WN-Me is a balanced dataset, whereas HypeNet is heavily skewed towards negative instances. 4.4 Ranking Experiments Shwartz et al. (2017) propose ranking as an alternative evaluation setting for hypernymy detection. The goal is to rank positive relation pairs higher than negative ones. Our D UAL -T model (and associated baselines) rank the concept pairs in decreasing order of assigned relations scores s(c1 , c2 ). Following Shwartz et al. (2017), we report performance in terms of overall average precision (AP) and average precision at rank 100 (AP@100). Hypernymy ranking. We evaluate the ranking performance on four small hypernymy test sets: BLESS, EVALuation, Benotto, and Weeds (cf. Table 1). As these datasets are not big enou"
D17-1185,D14-1162,0,0.107298,"can be considered distributional as it requires only distributional vectors of words as input. Consequently, in contrast to path-based methods, it is language-independent and more widely applicable. Experimental results on hypernymy and meronymy detection show that the Dual Tensor model outperforms both distributional and path-based models. We additionally demonstrate that our approach exhibits stable performance across languages and can, to some extent, diminish the negative effects of polysemy. 2 Related Work Specializing Word Embeddings. Unspecialized word embeddings (Mikolov et al., 2013; Pennington et al., 2014) capture general semantic properties of words, but are unable to differentiate between different types of semantic relations (e.g., vectors of car and driver might be as similar as vectors of car and vehicle). However, we often need embeddings to be similar only if an exact lexico-semantic relation holds between the words. Numerous methods for specializing word embeddings for particular relations have been proposed (Yu and Dredze, 2014; Faruqui et al., 2015; Kiela et al., 2015; Mrkˇsi´c et al., 2016, inter alia), primarily aiming to differentiate synonymic similarity from other types of semant"
D17-1185,E14-1054,0,0.0603517,"s. Snow et al. (2004) provided all dependency paths connecting the concepts in corpus to a logistic regression classifier for hypernymy detection. Distributional methods detect asymmetric relations using only distributional vectors of words as input. Distributional models come in both unsupervised and supervised flavors. Unsupervised metrics for hypernymy detection assume either that the hyponym’s contexts are included in the hypernym’s contexts (Weeds and Weir, 2003; Kotlerman et al., 2010) or that the linguistics contexts of a hyponym are more informative than the contexts of its hypernyms (Rimell, 2014; Santus et al., 2014). Supervised hypernymy classifiers represent the pair of words by combining their distributional vectors in different ways – concatenating them (Baroni et al., 2012) or subtracting them (Roller et al., 2014) – and feeding the resulting vector to a supervised classifier like logistic regression. Most recently, Shwartz et al. (2016) coupled path-based and distributional information with a recurrent neural network (RNN), yielding state-of-the-art hypernymy detection performance. Although our Dual Tensor model is purely distributional, we show that it may outperform such a hy"
D17-1185,C14-1097,0,0.106143,"Missing"
D17-1185,C14-1212,0,0.255308,"uing that existing datasets were too small for training their recurrent network, Shwartz et al. (2016) compiled this dataset for hypernymy detection from several external KBs, taking only pairs of concepts in direct relation (i.e., no transitive closure). Other hypernymy detection datasets. We additionally evaluate the Dual Tensor model on four smaller datasets for hypernymy detection: (1) BLESS dataset (Baroni and Lenci, 2011) and EVALuation dataset (Santus et al., 2015) contain instances of hypernymy and four other relations. BLESS additionally contains random word pairs; (2) Weeds dataset (Weeds et al., 2014) contains hypernymy and co-hyponymy pairs; (3) Benotto dataset (Benotto, 2015) couples hypernymy pairs with synonymy and antonymy pairs. Because these datasets contain at most several thousand pairs, we only use them to evaluate the performance of models trained on larger datasets; WN-Hy and WN-Me datasets. We create these datasets by taking concept pairs from WordNet. We take all instances from the transitive closure of hypernymy (all parts of speech) and meronymy (nouns) relations and couple them with all synonym and antonym relations (all parts of speech), as well as lexical entailment rela"
D17-1185,W03-1011,0,0.0865055,"and Pennacchiotti (2006) and Girju et al. (2006) proposed bootstrapping approaches to meronymy detection, starting from a seed set of part-whole pairs. Snow et al. (2004) provided all dependency paths connecting the concepts in corpus to a logistic regression classifier for hypernymy detection. Distributional methods detect asymmetric relations using only distributional vectors of words as input. Distributional models come in both unsupervised and supervised flavors. Unsupervised metrics for hypernymy detection assume either that the hyponym’s contexts are included in the hypernym’s contexts (Weeds and Weir, 2003; Kotlerman et al., 2010) or that the linguistics contexts of a hyponym are more informative than the contexts of its hypernyms (Rimell, 2014; Santus et al., 2014). Supervised hypernymy classifiers represent the pair of words by combining their distributional vectors in different ways – concatenating them (Baroni et al., 2012) or subtracting them (Roller et al., 2014) – and feeding the resulting vector to a supervised classifier like logistic regression. Most recently, Shwartz et al. (2016) coupled path-based and distributional information with a recurrent neural network (RNN), yielding state-"
D17-1185,W16-5309,0,0.162772,"et al., 2014) – and feeding the resulting vector to a supervised classifier like logistic regression. Most recently, Shwartz et al. (2016) coupled path-based and distributional information with a recurrent neural network (RNN), yielding state-of-the-art hypernymy detection performance. Although our Dual Tensor model is purely distributional, we show that it may outperform such a hybrid model which additionally exploits syntactic information. Distributional and path-based models have been used to discriminate between multiple lexicosemantic relations, including hypernymy and meronymy, at once (Santus et al., 2016; Shwartz and Dagan, 2016). However, as pointed out by (Chersoni et al., 2016), distributional vectors and scores based on their comparison fail to discriminate between multiple relation types at once. In this work, we focus on binary classification for a single relation (hypernymy and meronymy) at a time. 3 Dual Tensor Model The following assumptions and desirable properties guided the design of the Dual Tensor model for detection of asymmetric lexico-semantic relations: (1) Unspecialized distributional vectors are not good signals for detecting specific lexico-semantic relations. We thus nee"
D17-1185,E14-4008,0,0.592214,"s for embedding KBs (Bordes et al., 2013; Socher et al., 2013; Yang et al., 2015) uniformly model both symmetric and asymmetric relations. They learn a single vector representation (i.e., embedding) for each KB concept, assuming implicitly that the same concept representation is equally useful for predicting symmetric and asymmetric relations alike. Relation-specific learning-based models have, to the largest extent, targeted hypernymy. Distributional models predict the hypernymy relations by combining raw distributional vectors of concepts in a pair (Baroni et al., 2012; Roller et al., 2014; Santus et al., 2014), whereas path-based models base predictions on lexico-syntactic paths from co-occurrence contexts obtained from a large corpus (Snow et al., 2004; Nakashole et al., 2012; Shwartz et al., 2016). Shwartz et al. (2016) combine the path-based and distributional models to 1757 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1757–1767 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics reach state-of-the-art performance in hypernymy detection. Both distributional and path-based methods, however, model asymmetry on"
D17-1185,W15-4208,0,0.153154,"models variants. 4.1 Datasets We evaluate the Dual Tensor model on the following hypernymy and meronymy detection datasets: HypeNet dataset. Arguing that existing datasets were too small for training their recurrent network, Shwartz et al. (2016) compiled this dataset for hypernymy detection from several external KBs, taking only pairs of concepts in direct relation (i.e., no transitive closure). Other hypernymy detection datasets. We additionally evaluate the Dual Tensor model on four smaller datasets for hypernymy detection: (1) BLESS dataset (Baroni and Lenci, 2011) and EVALuation dataset (Santus et al., 2015) contain instances of hypernymy and four other relations. BLESS additionally contains random word pairs; (2) Weeds dataset (Weeds et al., 2014) contains hypernymy and co-hyponymy pairs; (3) Benotto dataset (Benotto, 2015) couples hypernymy pairs with synonymy and antonymy pairs. Because these datasets contain at most several thousand pairs, we only use them to evaluate the performance of models trained on larger datasets; WN-Hy and WN-Me datasets. We create these datasets by taking concept pairs from WordNet. We take all instances from the transitive closure of hypernymy (all parts of speech)"
D17-1185,W16-5310,0,0.194742,"eeding the resulting vector to a supervised classifier like logistic regression. Most recently, Shwartz et al. (2016) coupled path-based and distributional information with a recurrent neural network (RNN), yielding state-of-the-art hypernymy detection performance. Although our Dual Tensor model is purely distributional, we show that it may outperform such a hybrid model which additionally exploits syntactic information. Distributional and path-based models have been used to discriminate between multiple lexicosemantic relations, including hypernymy and meronymy, at once (Santus et al., 2016; Shwartz and Dagan, 2016). However, as pointed out by (Chersoni et al., 2016), distributional vectors and scores based on their comparison fail to discriminate between multiple relation types at once. In this work, we focus on binary classification for a single relation (hypernymy and meronymy) at a time. 3 Dual Tensor Model The following assumptions and desirable properties guided the design of the Dual Tensor model for detection of asymmetric lexico-semantic relations: (1) Unspecialized distributional vectors are not good signals for detecting specific lexico-semantic relations. We thus need to derive specialized re"
D17-1185,Q15-1025,0,0.134167,"alized and specialized word embeddings via a pair of tensors. Although our Dual Tensor model needs only unspecialized embeddings as input, our experiments on hypernymy and meronymy detection suggest that it can outperform more complex and resource-intensive models. We further demonstrate that the model can account for polysemy and that it exhibits stable performance across languages. 1 Introduction Detection of semantic relations that hold between words is the central task of lexical semantics, tightly coupled with obtaining representations that capture meaning of words (Mikolov et al., 2013; Wieting et al., 2015; Mrkˇsi´c et al., 2016, inter alia). As such, robust detection of lexico-semantic relations may benefit virtually any natural language processing application. Because lexico-semantic knowledge bases (KBs) like WordNet (Fellbaum, 1998) are general and of limited coverage, numerous methods for detecting lexico-semantic relations rely on distributional word representations obtained from large corpora. Although distributional models have evolved over time, from count-based (Landauer et al., 1998) and generative (Blei et al., 2003) to prediction-based (Mikolov et al., 2013), the similarity between"
D17-1185,C00-2137,0,0.236339,"on the HypeNet dataset (Shwartz et al., 2016). We show the performance of the D UAL -T model in Table 2, together with the path-based and hybrid (combination of path-based and distributional signal) variants of the the state-of-the-art RNN model of Shwartz et al. (2016). On the more challenging, lexically-split dataset D UAL -T model significantly3 outperforms the more complex hybrid HypeNet model (Shwartz et al., 2016), an RNN model coupling representations of syntactic paths from a large corpus with 3 All performance differences were tested using the nonparametric stratified shuffling test (Yeh, 2000) with α = 0.05. 1762 Lex. split Model respect to supervised baselines even more clearly than hypernymy classification results. All supervised models outperform the best unsupervised model in terms of AP, but only D UAL -T is consistently better when considering only 100 top-ranked pairs (AP@100). This adds to the conclusion that explicit modeling of asymmetry using dual tensors yields crucial performance boost. Rand. split P R F1 P R F1 C ONCAT-SVM B ILIN -P ROD S INGLE -T 78.6 73.3 77.7 44.6 50.0 55.5 56.9 59.4 64.8 79.9 81.0 85.7 75.9 79.8 82.6 77.9 80.5 84.1 D UAL -T 76.5 61.1 67.9 87.7 85."
D17-1185,P14-2089,0,0.446633,"from count-based (Landauer et al., 1998) and generative (Blei et al., 2003) to prediction-based (Mikolov et al., 2013), the similarity between distributional vectors still indicates only the abstract semantic association and not a precise semantic relation (e.g., vectors of antonyms may be as similar as vectors of synonyms). Consequently, a number of approaches have been proposed for specializing distributional spaces for specific lexico-semantic relations, either by (1) modifying the learning objective or regularization of the original embedding model by incorporating linguistic constraints (Yu and Dredze, 2014; Kiela et al., 2015) or (2) retroactively fitting the pre-trained unspecialized embeddings to linguistic constraints (Faruqui et al., 2015; Mrkˇsi´c et al., 2016). However, these methods specialize distributional vector spaces primarily for detecting the symmetric relation of semantic similarity (i.e., graded synonymy) and not for asymmetric lexico-semantic relations such as hypernymy and meronymy. On the other hand, models for embedding KBs (Bordes et al., 2013; Socher et al., 2013; Yang et al., 2015) uniformly model both symmetric and asymmetric relations. They learn a single vector represe"
D17-1185,P16-1226,0,0.650181,"Missing"
D18-1026,W13-3520,0,0.0562518,"− Xs ||F = UV&gt; W UΣV&gt; = SVD(Xt X&gt; s ) (7) where ||· ||F is the Frobenius norm. After mapping the original target embeddings into the shared space with this method, we post-specialize them with the function outlined in §2.2, learnt on the source language. This yields the specialized target vectors ˆ t = G(W ˆ Xt ; θG ). Y 3 Experimental Setup Distributional Vectors. We estimate the robustness of adversarial post-specialization by experimenting with three widely used collections of distributional English vectors. 1) SGNS - W 2 vectors are trained on the cleaned and tokenized Polyglot Wikipedia (Al-Rfou et al., 2013) using Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) by Levy and Goldberg (2014) with bag-ofwords contexts (window size is 2). 2) GLOVE - CC are GloVe vectors trained on the Common Crawl (Pennington et al., 2014). 3) FASTTEXT are vectors trained on Wikipedia with a SGNS variant that builds word vectors by summing the vectors of their constituent character n-grams (Bojanowski et al., 2017). All vectors are 300-dimensional.4 Constraints and Initial Specialization. We experiment with the sets of linguistic constraints used in prior work (Zhang et al., 2014; Ono et al., 2015; Vuli"
D18-1026,P14-2131,0,0.06586,"Missing"
D18-1026,Q17-1010,0,0.552254,"xical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the semantic properties of pre"
D18-1026,D14-1082,0,0.00851156,"plification. We report consistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move bey"
D18-1026,P16-1156,0,0.0192147,"straints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017). By design, these methods fine-tune only vectors of words seen in external resources. Vuli´c et al. (2018) suggest that specializing the full vocabulary is beneficial for downstream applications. Comparing to their work, we show that a more sophisticated adversarial post-specia"
D18-1026,N15-1184,0,0.2529,"Missing"
D18-1026,N13-1092,0,0.0607536,"Missing"
D18-1026,D16-1235,1,0.92545,"Missing"
D18-1026,P15-2011,1,0.950221,"nstream NLP applications (Faruqui, 2016). Such models are versatile as they can be applied to arbitrary distributional spaces, but they have a major drawback: they locally update only vectors of words present in linguistic constraints (i.e., seen words), whereas vectors of all other (i.e., unseen) words remain intact (see Figure 1). Both authors equally contributed to this work. 1 For instance, it is difficult to discern synonyms from antonyms in distributional vector spaces: this has a negative impact on language understanding tasks such as statistical dialog modeling or text simplification (Glavaš and Štajner, 2015; Faruqui et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016) 282 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 282–293 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Source Target posed model in two downstream tasks: lexical text simplification and dialog state tracking. Finally, we demonstrate that, by coupling our adversarial specialization model with any unsupervised model for inducing bilingual vector spaces, such as the algorithm proposed by Conneau et al. (2018), we can successfully per"
D18-1026,W14-4337,0,0.114879,"Missing"
D18-1026,J15-4004,1,0.949832,"P), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the semantic properties of pre-trained vectors and specialize the distributional spaces for a particular relation, e.g., synonymy (i.e., true similarity) (Faruqui et al., 2015; Mrkši´c et al.,"
D18-1026,P14-2075,0,0.121359,"Missing"
D18-1026,N15-1070,0,0.0340618,"lexical resources. An overview of the proposed methodology from this section is provided in Figure 1. 2.1 Initial Specialization Linguistic Constraints. Adopting the nomenclature from Mrkši´c et al. (2017), post-processing models are generally guided by two broad sets of constraints: 1) ATTRACT constraints specify which words should be close to each other in the finetuned vector space (e.g. synonyms like graceful and amiable); 2) REPEL constraints describe which words should be pulled away from each other (e.g. antonyms like innocent and sinful). Earlier postprocessors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015) operate only with ATTRACT constraints, and are thus not suited to model both aspects contributing to the specialization process. We first outline the state-of-the-art ATTRACTREPEL specialization model (Mrkši´c et al., 2017) 283 which leverages both sets of constraints. Here, we again stress two important aspects relevant to our post-specialization model: a) all initial specialization models fine-tune only representations for the subspace of words seen in the external constraints, while all other words remain unaffected by specialization; b) post-specialization is not ti"
D18-1026,D15-1242,0,0.222314,"Missing"
D18-1026,P17-1163,1,0.900293,"Missing"
D18-1026,P14-2050,0,0.556242,"onal space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the"
D18-1026,P15-1145,0,0.126883,"ss, in the long run, these transfer results hold promise to support the specialization of vector spaces even for resource-lean languages, and their applications. 5 Related Work Vector Space Specialization. Specialization methods embed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrkši´c et"
D18-1026,Q17-1022,1,0.887228,"Missing"
D18-1026,P16-2074,0,0.299861,"twork (i.e., the generator). We show that the proposed adversarial model yields state-of-the-art performance on standard word similarity benchmarks, outperforming the post-specialization model of Vuli´c et al. (2018). We further demonstrate the effectiveness of the proMethodology The post-specialization procedure (Vuli´c et al., 2018) is a two-step process. First, a subspace of vectors for words observed in external resources is fine-tuned using any off-the-shelf specialization model, such as the original retrofitting model (Faruqui et al., 2015), counter-fitting (Mrkši´c et al., 2016), dLCE (Nguyen et al., 2016), or state-of-theart ATTRACT- REPEL ( AR ) specialization (Mrkši´c et al., 2017; Vuli´c et al., 2017). We outline the initial specialization algorithms in §2.1. In the second step, the initial specialization is propagated to the entire vocabulary, including words not observed in the resources, relying on an adversarial architecture augmented with a distance loss. This adversarial post-specialization model, compatible with any specialization model, is described in §2.2. Finally, in §2.3, we introduce a cross-lingual zero-shot specialization model which transfers the specialization to a target l"
D18-1026,K16-1006,0,0.0212333,"onsistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervise"
D18-1026,N15-1100,0,0.536844,"(Al-Rfou et al., 2013) using Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) by Levy and Goldberg (2014) with bag-ofwords contexts (window size is 2). 2) GLOVE - CC are GloVe vectors trained on the Common Crawl (Pennington et al., 2014). 3) FASTTEXT are vectors trained on Wikipedia with a SGNS variant that builds word vectors by summing the vectors of their constituent character n-grams (Bojanowski et al., 2017). All vectors are 300-dimensional.4 Constraints and Initial Specialization. We experiment with the sets of linguistic constraints used in prior work (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018). These constraints, extracted from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009), comprise a total of 1,023,082 synonymy/ATTRACT word pairs and 380,873 antonymy/REPEL pairs. Note that the sets of constraints cover only a fraction of the full distributional vocabulary, providing direct motivation for post-specialization methods 4 3 See the recent survey papers on cross-lingual word embeddings and their typology (Upadhyay et al., 2016; Vuli´c and Korhonen, 2016; Ruder et al., 2017) 286 Experiments with other standard word vectors, such as (Melamud et al., 2"
D18-1026,N16-1118,0,0.0700653,"onsistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervise"
D18-1026,Q16-1030,0,0.390812,"results hold promise to support the specialization of vector spaces even for resource-lean languages, and their applications. 5 Related Work Vector Space Specialization. Specialization methods embed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al"
D18-1026,D14-1162,0,0.0930116,"a full target distributional space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et"
D18-1026,P15-1173,0,0.0417519,"ed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017). By design, these methods fine-tune only vectors of words seen in external resources. Vuli´c et al. (2018) suggest that specializing the full vocabulary is beneficial for downstream applica"
D18-1026,K15-1026,0,0.222497,"ess has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the semantic properties of pre-trained vectors and specialize the distributional spaces for a particular relation, e.g., synonymy (i.e., true similarity) (Faruqui et al., 2015; Mrkši´c et al., 2017) or hypernymy (Nic"
D18-1026,P16-1157,0,0.102759,"Missing"
D18-1026,N18-1048,1,0.24712,"Missing"
D18-1026,P16-1024,1,0.923484,"Missing"
D18-1026,N18-1103,1,0.824775,"Missing"
D18-1026,P17-1006,1,0.860576,"Missing"
D18-1026,K17-1013,1,0.898667,"Missing"
D18-1026,E17-1042,1,0.840593,"Missing"
D18-1026,Q15-1025,0,0.637092,"n overview of the proposed methodology from this section is provided in Figure 1. 2.1 Initial Specialization Linguistic Constraints. Adopting the nomenclature from Mrkši´c et al. (2017), post-processing models are generally guided by two broad sets of constraints: 1) ATTRACT constraints specify which words should be close to each other in the finetuned vector space (e.g. synonyms like graceful and amiable); 2) REPEL constraints describe which words should be pulled away from each other (e.g. antonyms like innocent and sinful). Earlier postprocessors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015) operate only with ATTRACT constraints, and are thus not suited to model both aspects contributing to the specialization process. We first outline the state-of-the-art ATTRACTREPEL specialization model (Mrkši´c et al., 2017) 283 which leverages both sets of constraints. Here, we again stress two important aspects relevant to our post-specialization model: a) all initial specialization models fine-tune only representations for the subspace of words seen in the external constraints, while all other words remain unaffected by specialization; b) post-specialization is not tied to ATTRACT- REPEL in"
D18-1026,P14-2089,0,0.199538,"he different ways concepts are lexicalized across languages, as studied by semantic typology (Ponti et al., 2018). Nonetheless, in the long run, these transfer results hold promise to support the specialization of vector spaces even for resource-lean languages, and their applications. 5 Related Work Vector Space Specialization. Specialization methods embed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, ou"
D18-1026,D14-1161,0,0.205358,"Polyglot Wikipedia (Al-Rfou et al., 2013) using Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) by Levy and Goldberg (2014) with bag-ofwords contexts (window size is 2). 2) GLOVE - CC are GloVe vectors trained on the Common Crawl (Pennington et al., 2014). 3) FASTTEXT are vectors trained on Wikipedia with a SGNS variant that builds word vectors by summing the vectors of their constituent character n-grams (Bojanowski et al., 2017). All vectors are 300-dimensional.4 Constraints and Initial Specialization. We experiment with the sets of linguistic constraints used in prior work (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018). These constraints, extracted from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009), comprise a total of 1,023,082 synonymy/ATTRACT word pairs and 380,873 antonymy/REPEL pairs. Note that the sets of constraints cover only a fraction of the full distributional vocabulary, providing direct motivation for post-specialization methods 4 3 See the recent survey papers on cross-lingual word embeddings and their typology (Upadhyay et al., 2016; Vuli´c and Korhonen, 2016; Ruder et al., 2017) 286 Experiments with other standard word vectors, such as"
D18-1026,P17-1179,0,0.031446,"distance can be formulated as the mean squared error between the input and the target (Pathak et al., 2016), their feature maps (Li and Wand, 2016), both (Zhu et al., 2016), or a loss calculated on feature maps of a deep convolutional network (Ledig et al., 2017). In the textual domain, adversarial models have been proven to support domain adaptation (Ganin et al., 2016) and language transfer (Chen et al., 2016) by learning domain/language-invariant latent features. Adversarial training also powers unsupervised mapping between monolingual vector spaces to learn cross-lingual word embeddings (Zhang et al., 2017; Conneau et al., 2018). In this work, we show how to apply adversarial techniques to the problem of vector specialization, which has a substantial impact on language understanding tasks. 6 Conclusion and Future Work We have presented adversarial post-specialization, a novel model supported by adversarial training which specializes word vectors for the full vocabulary of the input distributional vector space, including words unseen in external lexical resources. We have also introduced a method for zero-shot specialization of word vectors in languages without any external resources. The benefi"
D19-1226,Q17-1010,0,0.0982944,"revious state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog"
D19-1226,S17-2001,0,0.0533888,"Missing"
D19-1226,P19-1070,1,0.721523,"Missing"
D19-1226,P15-2011,1,0.869561,"Missing"
D19-1226,W16-2501,1,0.82432,", word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrkˇsi´c et al., 2016; Kim et al., 2016b). Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018) or lexical entailment (Nguyen et al., 2017; Vuli´c and Mrkˇsi´c, 2018) over other types of semantic association in the word vector space. The best-performing sp"
D19-1226,J15-4004,1,0.915895,"Missing"
D19-1226,P14-2075,0,0.0774785,"pecialization on LS, we employ ˇ Light-LS (Glavaˇs and Stajner, 2015), a languageagnostic LS tool that makes simplifications based on word similarities in a given vector space. The quality of similarity-based information encoded in the vector space encode is thus expected to directly correlate with the performance of Light-LS. We use LS datasets for Italian (IT) (Tonelli et al., 2016), Spanish (ES) (Saggion et al., 2015; Saggion, 2017), and Portuguese (PT) (Hartmann et al., 2018) to evaluate the specialized spaces in those languages. We rely on the standard LS evaluation metric of Accuˇ racy (Horn et al., 2014; Glavaˇs and Stajner, 2015): it quantifies both the quality and frequency of replacements as a number of correct simplifications divided by the total number of complex words. Results and Analysis. The results are reported in Table 3. As shown in previous work (Vuli´c et al., 2018; Ponti et al., 2018), retrofitting (CLSRI-AR) and the cross-lingual post-specialization transfer (X-PS) are substantially better in the LS task than the original distributional space. However, our full CLSRI-PS model results in substantial boosts in the 2213 LS any additional input for the lexical prediction step (i."
D19-1226,N15-1184,0,0.302339,"Missing"
D19-1226,D18-1330,0,0.0952862,"Missing"
D19-1226,W19-4310,1,0.811783,"Missing"
D19-1226,D15-1242,0,0.242713,"Missing"
D19-1226,W16-1607,0,0.262024,"Missing"
D19-1226,N18-2029,1,0.870858,"Missing"
D19-1226,P18-1004,1,0.752226,"Missing"
D19-1226,N16-1018,0,0.139801,"Missing"
D19-1226,C18-1205,0,0.107783,"and translation of incorrect senses of Ls words. We thus subsequently refine the noisy set of target constraints by having a state-of-the-art neural model for lexico-semantic relation prediction (Glavaˇs and Vuli´c, 2018a), trained on the Ls constraints, discern valid from invalid Lt constraints. Following that, we perform monolingual retrofitting and post-specialization in the target language Lt , as outlined in § 3.2. The Lt distributional vectors can be specialized with the cleaned Lt constraints using any off-the-shelf retrofitting model (Faruqui et al., 2015; Mrkˇsi´c et al., 2016; 2208 Lengerich et al., 2018, inter alia). In this work we opt for the best-performing retrofitting model ATTRACT- REPEL ( AR ) (Mrkˇsi´c et al., 2017; Vuli´c et al., 2017b). AR specializes only the words seen in the cleaned Lt constraints. As the final step, we generalize AR’s specialization to the entire target vocabulary with a post-specialization model (Ponti et al., 2018) that learns the global specialization function from pairs of distributional and ARspecialized vectors of words from Lt constraints. A visual summary of our transfer model is presented in Figure 1. Our proposed CLSRI specialization conceptually diff"
D19-1226,P14-2050,0,0.0346927,"og state tracking, and semantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., betwe"
D19-1226,C18-1172,0,0.0157486,"ext simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the latter yield better downstream performance (Mrkˇsi´c et al., 2016). Moreover, while the joint models are tightly coupled to a concrete word embedding objective, retrofitting models can be applied on top"
D19-1226,P15-1145,0,0.0602024,"ntic relation (e.g., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical"
D19-1226,N16-1118,0,0.020871,". The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can br"
D19-1226,P18-2018,1,0.880048,"Missing"
D19-1226,Q17-1022,1,0.892103,"Missing"
D19-1226,L18-1381,0,0.208203,"Missing"
D19-1226,C16-1123,1,0.872206,"Missing"
D19-1226,N15-1100,0,0.0535031,"., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the l"
D19-1226,Q16-1030,0,0.0603068,"rity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the latter yield better dow"
D19-1226,D14-1162,0,0.0845732,"mantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensi"
D19-1226,N18-1202,0,0.0147967,"ialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation s"
D19-1226,J19-3005,1,0.873216,"Missing"
D19-1226,P17-1163,0,0.0403913,"Missing"
D19-1226,D18-1026,1,0.750703,"Missing"
D19-1226,D18-1299,0,0.171899,"Missing"
D19-1226,Q15-1025,0,0.0752505,"(Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrkˇsi´c et al., 2016; Kim et al., 2016b). Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018) or lexical entailment (Nguyen et al., 2017; Vuli´c and Mrkˇsi´c, 2018) over other types of semantic association in the word vector space. The best-performing specialization models (cf. Mrkˇsi´c et al. 2017; Ponti et al. 2018) are executed as vector space post-processors. In short, these techniques force the distributional vectors to conform to external linguistic constraints (e.g., synonymy, meronymy, lexical entailment) extracted from structured external resources (e.g., WordNet, BabelNet) to emphasize the particular relation. As post-processors th"
D19-1226,K15-1026,1,0.801861,"ally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrkˇsi´c et al., 2016; Kim et al., 2016b). Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015;"
D19-1226,S17-2016,0,0.0528375,"Missing"
D19-1226,P18-1072,1,0.903026,"Missing"
D19-1226,P14-2089,0,0.035718,"d vectors; semantic specialization of such spaces for a particular lexicosemantic relation (e.g., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of"
D19-1226,D14-1161,0,0.211456,"Missing"
D19-1226,P18-1135,0,0.0126936,"ble 2. Several findings emerge from the results. First, as already confirmed in prior work (Vuli´c et al., 2018; Ponti et al., 2018), vectors specialized for semantic similarity are indeed important for DST: we observe improvements with all specialized vectors. The highest gains are observed with the full CSLRIPS model. This confirms two main intuitions: 1) our proposed specialization transfer via lexical induction in the target language is more robust than 15 Note that the original NBT framework in the English DST task has been recently surpassed by more intricate taskspecific architectures (Zhong et al., 2018; Ren et al., 2018), but its lightweight design coupled with its strong dependence on input word vectors still makes it a convenient means to evaluate the effects of different specialization methods. Lexical Simplification Lexical simplification (LS) aims to automatically replace complex words (i.e., specialized terms, words used less frequently and known to fewer speakers) with their simpler in-context synonyms: the simplified text must be grammatical and retain the meaning of the original text. Lexical simplification critically depends on discerning semantic similarity from other types of se"
D19-1226,W17-0228,0,0.123249,"Missing"
D19-1226,N18-1048,1,0.885485,"Missing"
D19-1226,N18-1103,1,0.913929,"Missing"
D19-1226,D17-1270,1,0.902499,"Missing"
D19-1226,P17-1006,1,0.89768,"Missing"
D19-1226,E17-1042,0,0.0520418,"Missing"
D19-1449,E17-1088,0,0.0317067,"ble 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces which scored above a PanLex-predefined threshold. As in prior work (Conneau et al., 2018a; Glavaš et al., 2019), we then reserve the 5K pairs created from the more frequent L1 words for training, while the next 2K pairs are used for test. Smaller training dictionaries (1K and 500 pairs) are created by again"
D19-1449,W13-3520,0,0.0499261,"LWEs for similar languages in the first place: we can harvest cheap supervision here, e.g., cognates. The main motivation behind unsupervised approaches is to support dissimilar and resourcepoor language pairs for which supervision cannot be guaranteed. Domain Differences. Finally, we also verify that UNSUPERVISED CLWEs still cannot account for domain differences when training monolingual vectors. We rely on the probing test of Søgaard et al. (2018): 300-dim fastText vectors are trained on 1.1M sentences on three corpora: 1) EuroParl.v7 (Koehn, 2005) (parliamentary proceedings); 2) Wikipedia (Al-Rfou et al., 2013), and 3) EMEA (Tiedemann, 2009) (medical), and BLI evaluation for three language pairs is conducted on standard MUSE BLI test sets (Conneau et al., 2018a). The results, summarized in Figure 4, reveal that UN SUPERVISED methods are able to yield a good solution only when there is no domain mismatch and for the pair with two most similar languages (English-Spanish), again questioning their robustness and portability to truly low-resource and more challenging setups. Weakly supervised methods (|D0 |= 500 or D0 seeded with identical strings), in contrast, yield good solutions for all setups. 5 Fur"
D19-1449,D18-1062,0,0.0195462,"seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their m"
D19-1449,2020.lrec-1.495,0,0.468497,"Missing"
D19-1449,P18-1073,0,0.0976736,"e induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dict"
D19-1449,J82-2005,0,0.673463,"Missing"
D19-1449,D16-1136,0,0.0311352,"tive agglutinative agglutinative agglutinative introflexive agglutinative isolating agglutinative agglutinative fusional fusional isolating agglutinative BG CA EO ET EU FI HE HU ID KA KO LT NO TH TR Table 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces which scored above a PanLex-predefined threshold. As in prior work (Conneau et al., 2018a; Glavaš et al.,"
D19-1449,E14-1049,0,0.0557935,"l., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019"
D19-1449,C10-3010,0,0.0486842,"c Uralic Austronesian Kartvelian Koreanic IE: Baltic IE: Germanic Kra-Dai Turkic fusional fusional agglutinative agglutinative agglutinative agglutinative introflexive agglutinative isolating agglutinative agglutinative fusional fusional isolating agglutinative BG CA EO ET EU FI HE HU ID KA KO LT NO TH TR Table 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces"
D19-1449,D18-1029,1,0.891637,"Missing"
D19-1449,W16-1614,0,0.118504,"et al. (2018) and further verified by Glavaš et al. (2019) and Doval et al. (2019), the language pair at hand can have a huge impact on CLWE induction: the adversarial method of Conneau et al. (2018a) often gets stuck in poor local optima and yields degenerate solutions for distant language pairs such as English-Finnish. More recent CLWE methods (Artetxe et al., 2018b; Mohiuddin and Joty, 2019) focus on mitigating this robustness issue. However, they still rely on one critical assumption which leads them to degraded performance for distant language pairs: they assume approximate isomorphism (Barone, 2016; Søgaard et al., 2018) between monolingual embedding spaces to learn the initial seed dictionary. In other words, they assume very similar geometric constellations between two monolingual spaces: due to the Zipfian phenomena in language (Zipf, 1949) such near-isomorphism can be satisfied only for similar languages and for similar domains used for training monolingual vectors. This property is reflected in the results reported in Table 3, the number of unsuccessful setups in Table 4, as well as later in Figure 4. For instance, the largest number of unsuccessful BLI setups with the UNSUPERVISED"
D19-1449,P19-1070,1,0.828323,"Missing"
D19-1449,Q17-1010,0,0.186535,"rature. These two properties will facilitate analyses between (dis)similar language pairs and offer a comprehensive set of evaluation setups that test the robustness and portability of fully unsupervised CLWEs. The final list of 15 diverse test languages is provided in Table 2, and includes samples from different languages types and families. We run BLI evaluations for all language pairs in both directions, for a total of 15×14=210 BLI setups. Monolingual Embeddings. We use the 300-dim vectors of Grave et al. (2018) for all 15 languages, pretrained on Common Crawl and Wikipedia with fastText (Bojanowski et al., 2017).7 We trim all 5 While BLI is an intrinsic task, as discussed by Glavaš et al. (2019) it is a strong indicator of CLWE quality also for downstream tasks: relative performance in the BLI task correlates well with performance in cross-lingual information retrieval (Litschko et al., 2018) or natural language inference (Conneau et al., 2018b). More importantly, it also provides a means to analyze whether a CLWE method manages to learn anything meaningful at all, and can indicate “unsuccessful” CLWE induction (e.g., when BLI performance is similar to a random baseline): detecting such CLWEs is espe"
D19-1449,D14-1082,0,0.0593674,"uage pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods. 1 Introduction and Motivation The wide use and success of monolingual word embeddings in NLP tasks (Turian et al., 2010; Chen and Manning, 2014) has inspired further research focus on the induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Lit"
D19-1449,N15-1157,0,0.0457001,"lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries t"
D19-1449,D18-1024,0,0.0746406,"typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can rough"
D19-1449,L18-1550,0,0.0283189,"nguage pairs and offer new evaluation data which extends and surpasses other work in the CLWE literature. These two properties will facilitate analyses between (dis)similar language pairs and offer a comprehensive set of evaluation setups that test the robustness and portability of fully unsupervised CLWEs. The final list of 15 diverse test languages is provided in Table 2, and includes samples from different languages types and families. We run BLI evaluations for all language pairs in both directions, for a total of 15×14=210 BLI setups. Monolingual Embeddings. We use the 300-dim vectors of Grave et al. (2018) for all 15 languages, pretrained on Common Crawl and Wikipedia with fastText (Bojanowski et al., 2017).7 We trim all 5 While BLI is an intrinsic task, as discussed by Glavaš et al. (2019) it is a strong indicator of CLWE quality also for downstream tasks: relative performance in the BLI task correlates well with performance in cross-lingual information retrieval (Litschko et al., 2018) or natural language inference (Conneau et al., 2018b). More importantly, it also provides a means to analyze whether a CLWE method manages to learn anything meaningful at all, and can indicate “unsuccessful” CL"
D19-1449,P15-1119,0,0.0322898,"transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui"
D19-1449,D18-1269,0,0.0727556,"eak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-tra"
D19-1449,N19-1188,1,0.675442,"Missing"
D19-1449,N18-2085,0,0.0180662,"nslations for a test set of source language words. Its lightweight nature allows us to conduct a comprehensive evaluation across a large number of language pairs.5 Since BLI is cast as a ranking task, following Glavaš et al. (2019) we use mean average precision (MAP) as the main evaluation metric: in our BLI setup with only one correct translation for each “query” word, MAP is equal to mean reciprocal rank (MRR).6 (Selection of) Language Pairs. Our selection of test languages is guided by the following goals: a) following recent initiatives in other NLP research (e.g., for language modeling) (Cotterell et al., 2018; Gerz et al., 2018), we aim to ensure the coverage of different genealogical and typological language properties, and b) we aim to analyze a large set of language pairs and offer new evaluation data which extends and surpasses other work in the CLWE literature. These two properties will facilitate analyses between (dis)similar language pairs and offer a comprehensive set of evaluation setups that test the robustness and portability of fully unsupervised CLWEs. The final list of 15 diverse test languages is provided in Table 2, and includes samples from different languages types and families."
D19-1449,E17-1102,1,0.919319,"Missing"
D19-1449,D18-1043,0,0.222539,"languages and language pairs. However, the first attempts at fully unsupervised CLWE induction failed exactly for these use cases, as shown by Søgaard et al. (2018). Therefore, the follow-up work aimed to improve the robustness of unsupervised CLWE induction by introducing more robust self-learning procedures (Artetxe et al., 2018b; Kementchedjhieva et al., 2018). Besides increased robustness, recent work claims that fully unsupervised projection-based CLWEs can even match or surpass their supervised counterparts (Conneau et al., 2018a; Artetxe et al., 2018b; Alvarez-Melis and Jaakkola, 2018; Hoshen and Wolf, 2018; Heyman et al., 2019). In this paper, we critically examine these claims on robustness and improved performance of unsupervised CLWEs by running a large-scale evaluation in the bilingual lexicon induction (BLI) task on 15 languages (i.e., 210 languages pairs, see Table 2 in §3). The languages were selected to represent different language families and morphological types, as we argue that fully unsupervised CLWEs have been designed to support exactly these setups. However, we show that even the most robust unsupervised CLWE method (Artetxe et al., 2018b) still fails for a large number of langu"
D19-1449,N19-1386,0,0.228386,"Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can roughly be described by three main components: C1) unsupervised extraction of a seed dictionary; C2) a"
D19-1449,kamholz-etal-2014-panlex,0,0.516415,"Kartvelian Koreanic IE: Baltic IE: Germanic Kra-Dai Turkic fusional fusional agglutinative agglutinative agglutinative agglutinative introflexive agglutinative isolating agglutinative agglutinative fusional fusional isolating agglutinative BG CA EO ET EU FI HE HU ID KA KO LT NO TH TR Table 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces which scored above a Pa"
D19-1449,D18-1047,0,0.0484142,"solute BLI scores for distant pairs (see Table 4 and results in the supplemental material). Unsupervised approaches even exploit the assumption twice as their seed extraction is fully based on the topological similarity. Future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (Zhang et al., 2019) by distinguishing between language-specific and language-pairinvariant subspaces; 2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of Nakashole (2018); 3) similar to Vuli´c and Korhonen (2016) and Lubin et al. (2019) “denoisify” seed lexicons during the self-learning procedure. For instance, keeping only mutual/symmetric nearest neighbour as in FULL + SL + SYM can be seen as a form of rudimentary denoisifying: it is indicative to see that the best overall performance in this work is reported with that model configuration. Further, the most important contributions of unsupervised CLWE models are, in fact, the improved and more robust self-learning procedures (component C2) and technical enhancements (component C3). In this work we have demon"
D19-1449,K18-1021,0,0.0602112,"an inherently interesting research topic per se. Nonetheless, the main practical motivation for developing such approaches in the first place is to facilitate the construction of multilingual NLP tools and widen the access to language technology for resource-poor languages and language pairs. However, the first attempts at fully unsupervised CLWE induction failed exactly for these use cases, as shown by Søgaard et al. (2018). Therefore, the follow-up work aimed to improve the robustness of unsupervised CLWE induction by introducing more robust self-learning procedures (Artetxe et al., 2018b; Kementchedjhieva et al., 2018). Besides increased robustness, recent work claims that fully unsupervised projection-based CLWEs can even match or surpass their supervised counterparts (Conneau et al., 2018a; Artetxe et al., 2018b; Alvarez-Melis and Jaakkola, 2018; Hoshen and Wolf, 2018; Heyman et al., 2019). In this paper, we critically examine these claims on robustness and improved performance of unsupervised CLWEs by running a large-scale evaluation in the bilingual lexicon induction (BLI) task on 15 languages (i.e., 210 languages pairs, see Table 2 in §3). The languages were selected to represent different language fam"
D19-1449,P19-1492,0,0.0933756,"Missing"
D19-1449,D18-1101,0,0.0167767,"t al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can roughly be described by three main components: C1) unsupe"
D19-1449,C12-1089,0,0.180369,"n a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and com"
D19-1449,2005.mtsummit-papers.11,0,0.0217419,"more, one could argue that we do not need unsupervised CLWEs for similar languages in the first place: we can harvest cheap supervision here, e.g., cognates. The main motivation behind unsupervised approaches is to support dissimilar and resourcepoor language pairs for which supervision cannot be guaranteed. Domain Differences. Finally, we also verify that UNSUPERVISED CLWEs still cannot account for domain differences when training monolingual vectors. We rely on the probing test of Søgaard et al. (2018): 300-dim fastText vectors are trained on 1.1M sentences on three corpora: 1) EuroParl.v7 (Koehn, 2005) (parliamentary proceedings); 2) Wikipedia (Al-Rfou et al., 2013), and 3) EMEA (Tiedemann, 2009) (medical), and BLI evaluation for three language pairs is conducted on standard MUSE BLI test sets (Conneau et al., 2018a). The results, summarized in Figure 4, reveal that UN SUPERVISED methods are able to yield a good solution only when there is no domain mismatch and for the pair with two most similar languages (English-Spanish), again questioning their robustness and portability to truly low-resource and more challenging setups. Weakly supervised methods (|D0 |= 500 or D0 seeded with identical"
D19-1449,P15-1165,0,0.159533,"Missing"
D19-1449,P18-1072,1,0.810814,"Missing"
D19-1449,D18-1549,0,0.0740195,"Missing"
D19-1449,P10-1040,0,0.0777116,"e languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods. 1 Introduction and Motivation The wide use and success of monolingual word embeddings in NLP tasks (Turian et al., 2010; Chen and Manning, 2014) has inspired further research focus on the induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vu"
D19-1449,P16-1024,1,0.918618,"Missing"
D19-1449,N19-1045,0,0.0138207,"the supplemental material). Unsupervised approaches even exploit the assumption twice as their seed extraction is fully based on the topological similarity. Future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (Zhang et al., 2019) by distinguishing between language-specific and language-pairinvariant subspaces; 2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of Nakashole (2018); 3) similar to Vuli´c and Korhonen (2016) and Lubin et al. (2019) “denoisify” seed lexicons during the self-learning procedure. For instance, keeping only mutual/symmetric nearest neighbour as in FULL + SL + SYM can be seen as a form of rudimentary denoisifying: it is indicative to see that the best overall performance in this work is reported with that model configuration. Further, the most important contributions of unsupervised CLWE models are, in fact, the improved and more robust self-learning procedures (component C2) and technical enhancements (component C3). In this work we have demonstrated that these components can be equally applied to weakly sup"
D19-1449,D17-1270,1,0.904042,"Missing"
D19-1449,N15-1104,0,0.525331,"Missing"
D19-1449,P19-1307,0,0.0605721,"tance, the underlying assumption of all projection-based methods (both supervised and unsupervised) is the topological similarity between monolingual spaces, which is why standard simple linear projections result in lower absolute BLI scores for distant pairs (see Table 4 and results in the supplemental material). Unsupervised approaches even exploit the assumption twice as their seed extraction is fully based on the topological similarity. Future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (Zhang et al., 2019) by distinguishing between language-specific and language-pairinvariant subspaces; 2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of Nakashole (2018); 3) similar to Vuli´c and Korhonen (2016) and Lubin et al. (2019) “denoisify” seed lexicons during the self-learning procedure. For instance, keeping only mutual/symmetric nearest neighbour as in FULL + SL + SYM can be seen as a form of rudimentary denoisifying: it is indicative to see that the best overall performance in this work is reported with that model configuration. F"
D19-1449,N16-1156,0,0.0208315,"tilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned seve"
D19-1449,D18-1022,1,0.928371,"ardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-ling"
D19-3034,P19-1070,1,0.888836,"Missing"
D19-3034,Q17-1010,0,0.274436,"ments and queries differently lexicalize concepts. Semantic search seeks to overcome such lexical mismatches between document and user queries (Li and Xu, 2014). Early approaches to semantic search relied on external resources like WordNet (Moldovan and Mihalcea, 2000) and Wikipedia (Strube and Ponzetto, 2006), suffering from the resource’s limited coverage. More recent semantic search systems (Vuli´c and Moens, 2015; Litschko et al., 2018; Nogueira and Cho, 2019) remedy for those coverage issues by encoding text using distributional word vectors (i.e., word embeddings) (Mikolov et al., 2013; Bojanowski et al., 2017) and neural text encoders (Devlin et al., 2018). While there is a plethora of semantic text encoding models, there have been few attempts to empirically compare them on IR tasks. In this 2 We first describe S EAGLE’s semantic encoders, belonging to two categories: word embedding aggregators and pre-trained text encoders. 2.1 Word Embedding Aggregators Word embedding aggregators encode the text by aggregating pretrained d-dimensional embeddings of its words. Formally, a document matrix Vd ∈ RN ×d sequentially stacks embeddings ti ∈ Rd corresponding to tokens ti (i ∈ {1, · · · , N }) of document"
D19-3034,D19-1059,0,0.148074,"rs for Information Retrieval Fabian David Schmidt∗, Markus Dietsche∗, Simone Paolo Ponzetto and Goran Glavaˇs Data and Web Science Group University of Mannheim fabian.david.schmidt@hotmail.de dietsche.markus@googlemail.com {simone, goran}@informatik.uni-mannheim.de Abstract work, we aim to allow for such comparative evaluations on arbitrary IR test collections. We introduce S EAGLE, a platform for concurrent execution and comparative evaluation of semantic search models. S EAGLE implements most recently proposed (1) word embedding aggregation models (Arora et al., 2017; R¨uckl´e et al., 2018; Yang et al., 2019; Zhelezniak et al., 2019) as well as (2) two pretraining-based text encoders (Gysel et al., 2017; Devlin et al., 2018) and allows users to evaluate them on arbitrary IR collections. Coupled with pretrained cross-lingual embedding spaces (Glavaˇs et al., 2019), S EAGLE also supports cross-lingual search. The platform’s modular architecture based on micro-services makes it easy to extend it with additional semantic encoding models. S EAGLE is accessible via an easy-to-use web interface. We introduce S EAGLE,1 a platform for comparative evaluation of semantic text encoding models on information"
E17-2083,N15-1151,0,0.0215897,"observe what textual patterns hold between them. They then associate the recognized patterns to particular KB relations and finally search the corpus for other entity pairs mentioned using the same patterns (Snow et al., 2004; Snow et al., ; Mintz et al., 2009; Aprosio et al., 2013). A slight modification is the approach by (West et al., 2014) where lexicalized KB relations are posed as queries to a search engine and results are parsed to find pairs of entities between which the initially queried relation holds. Complementary to this, open information extraction methods (Etzioni et al., 2011; Faruqui and Kumar, 2015) extract large amounts of facts from text that can then be used for extending KBs (Dutta et al., 2014). Text-centered approaches, however, simply cannot capture knowledge that is rarely made explicit in texts. For example, much of the common-sense knowledge that is obvious to people such as, for instance, that bananas are yellow or that humans breath are rarely (or never) made explicit in textual corpora. A partial solution to this problem is provided by internal approaches that primarily rely on existing information in the KB itself (Bordes et al., 2011; Jenatton et al., 2012; Socher et al.,"
E17-2083,N15-1184,0,0.0558918,"Missing"
E17-2083,D15-1038,0,0.0305965,"and multilingual embedding space. Our results indicate that using cross-lingual links between entity lexicalizations in different languages yields better NTNKBC model. That is, our experiments imply that the cross-lingual signal enabled through the multilingual KB and shared multilingual embedding space provides improved regularization for the neural KBC model. We intend to investigate whether such cross-lingual regularization can yield similar improvements for other neural KBC models and whether it can be combined with other types of regularization, such as that based on augmenting KB paths (Guu et al., 2015). We will also evaluate the cross-lingually extended KB-embedding models on other high-level tasks such as error detection and KB consistency checking. 520 References Alessio Palmero Aprosio, Claudio Giuliano, and Alberto Lavelli. 2013. Extending the coverage of DBpedia properties using distant supervision over Wikipedia. In Proceedings of the 2013 Workshop on Natural Language Processing and DBpedia, pages 20–31, Trento, Italy. Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed webcrawled corpora"
E17-2083,P09-1113,0,0.0187637,"ethods typically employ a form of a distant supervi516 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 516–522, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics sion. They first recognize mentions of pairs of KB entities in text and observe what textual patterns hold between them. They then associate the recognized patterns to particular KB relations and finally search the corpus for other entity pairs mentioned using the same patterns (Snow et al., 2004; Snow et al., ; Mintz et al., 2009; Aprosio et al., 2013). A slight modification is the approach by (West et al., 2014) where lexicalized KB relations are posed as queries to a search engine and results are parsed to find pairs of entities between which the initially queried relation holds. Complementary to this, open information extraction methods (Etzioni et al., 2011; Faruqui and Kumar, 2015) extract large amounts of facts from text that can then be used for extending KBs (Dutta et al., 2014). Text-centered approaches, however, simply cannot capture knowledge that is rarely made explicit in texts. For example, much of the c"
E17-2083,D12-1128,1,0.816098,"Missing"
E17-2083,P16-1157,0,0.0261236,"KB completion, however, make no use of cross-lingual links between entities, which are readily available in existing multilingual resources like BabelNet (Navigli and Ponzetto, 2012b). Here, we extend the model of Socher et al. (2013) with cross-lingual links from BabelNet and demonstrate how introducing additional (cross-lingual) knowledge through these links improves the reasoning over the KB in terms of better performance on the link prediction task. Our findings are, in turn, different yet complementary to those found by building crosslingual embeddings using parallel or comparable data (Upadhyay et al., 2016) or KB-centric multilingual joint approaches to word understanding like, for instance, that of Navigli and Ponzetto (2012b). Assuming that each monolingual embedding space captures a slightly different aspect of a relation between same concepts, by introducing cross-lingual links over a shared embedding space we believe we provide an additional external regularization mechanism for the NTNKBC model. 3 Cross-Lingual Information for Knowledge Base Completion In Figure 1 we highlight the main steps of our cross-lingual extension of the NTNKBC model. We first use BabelNet to translate KB triples u"
E17-2083,C00-2137,0,0.0118443,".5K triples vs. 33.4K entities and 92K triples for both German and Italian). The Italian monolingual model (Mono-IT) outperforms the German monolingual model (Mono-DE) despite comparable training set sizes, which we credit to the lower quality of the DE→EN translation matrix in comparison with the IT→EN translation matrix (see Table 2). The multilingual model outperforms only one of the three monolingual models. This is not so surprising (although it might seem so at first glance) if 2 All performance differences were tested for significance using the non-parametric stratified shuffling test (Yeh, 2000). we consider that ML-NTN merely combines three disjoint KBs which share semantic information only through shared embedding space and relation tensors. Without the direct, cross-lingual links between entities of different monolingual KBs, these signals seem to be insufficient to compensate for a much larger number of parameters (three times larger number of entities) that the ML-NTN model has to learn compared to monolingual models. The cross-lingual model (CL-NTN), on the other hand, significantly outperforms all monolingual models. We believe that this is because by adding cross-lingual trip"
E17-2109,W16-2102,1,0.397961,"Missing"
E17-2109,D14-1181,0,0.00505458,"ed models (Kellstedt, 2000; Young and Soroka, 2012), supervised classification models (Purpura and Hillard, 2006; Stewart and Zhukov, 2009; Verberne et al., 2014; Karan et al., 2016), and unsupervised scaling models (Slapin and Proksch, 2008; Proksch and Slapin, 2010). All of these models use the discrete, word-based representations of text. Recently, however, continuous semantic text representations (Mikolov et al., 2013b; Le and Mikolov, 2014; Kiros et al., 2015; Mrkˇsi´c et al., 2016) outperformed word-based text representations on a battery of mainstream natural language processing tasks (Kim, 2014; Bordes et al., 2014; Tang et al., 2016). Although the idea of automated estimation of ideological beliefs is old (Abelson and Carroll, 1965), models estimating these beliefs from texts have only appeared in the last fifteen years (Laver and Garry, 2000; Laver et al., 2003; Slapin and Proksch, 2008; Proksch and Slapin, 2010). In the pioneering work on political text scaling, Laver and Garry (2000) used predefined dictionaries of words labeled with position scores. They then scored documents by aggregating the scores of dictionary words they contain. Extending this work, they proposed the mode"
E17-2109,N16-1018,0,0.0230683,"Missing"
E17-2109,D14-1162,0,0.0810828,"Missing"
E17-2109,C00-2137,0,0.225784,"observations from (Proksch and Slapin, 2010). The same holds for our alignment model (A LIGN HFLP). In contrast, the scaling based on the aggregation similarity measure (AGG -HFLP) seems to better correspond to the left-to-right ideological positioning. We hypothesize that this is because the comparison between semantically more imprecise aggregated text embeddings assigns more weight to the most salient dimension of speeches, which we speculate is the ideological position. In contrast, by comparing semantically more precise word em7 According to the non-parametric stratified shuffling test (Yeh, 2000) 692 Conclusion In this work, we presented what is, to the best of our knowledge, the first approach for cross-lingual scaling of political texts. We induce a multilingual embedding space and compute semantic similarities for all pairs of texts using unsupervised measures for semantic textual similarity. We then use a graph-based score propagation algorithm to transform pairwise similarities into position scores. Experimental results from the straightforward quantitative evaluation we propose show that our semantically-informed scaling predicts party positions for two relevant political dimens"
glavas-etal-2014-hieve,S10-1062,0,\N,Missing
glavas-etal-2014-hieve,W04-1017,0,\N,Missing
glavas-etal-2014-hieve,E12-1034,0,\N,Missing
glavas-etal-2014-hieve,chambers-jurafsky-2010-database,0,\N,Missing
glavas-etal-2014-hieve,S07-1014,0,\N,Missing
glavas-etal-2014-hieve,W03-0502,0,\N,Missing
glavas-etal-2014-hieve,J05-2005,0,\N,Missing
glavas-etal-2014-hieve,S10-1010,0,\N,Missing
glavas-etal-2014-hieve,P08-1090,0,\N,Missing
glavas-etal-2014-hieve,S13-2001,0,\N,Missing
glavas-etal-2014-hieve,P09-1068,0,\N,Missing
glavas-etal-2014-hieve,W13-0119,0,\N,Missing
glavas-etal-2014-hieve,S13-2002,0,\N,Missing
glavas-etal-2014-hieve,mani-etal-2008-spatialml,0,\N,Missing
glavas-etal-2014-hieve,P11-2061,0,\N,Missing
glavas-etal-2014-hieve,kordjamshidi-etal-2010-spatial,1,\N,Missing
N18-1048,W13-3520,0,0.199805,"Missing"
N18-1048,D16-1250,0,0.180495,"places word vectors from X0s with their approximations, i.e., f -mapped vectors.2 Objective Functions As mentioned, the N seen words xi ∈ Vs in fact serve as our “pseudotranslation” pairs supporting the learning of a crossspace mapping function. In practice, in its highlevel formulation, our mapping problem is equivalent to those encountered in the literature on crosslingual word embeddings where the goal is to learn a shared cross-lingual space given monolingual vector spaces in two languages and N1 translation pairs (Mikolov et al., 2013a; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Artetxe et al., 2016, 2017; Conneau et al., 2017; Ruder et al., 2017). In our setup, the standard objective based on L2 -penalised 2 We have empirically confirmed the intuition that the first variant is superior to this alternative. We do not report the actual quantitative comparison for brevity. 518 swish swish swish ... swish Xd = Xs ∪ Xu attract-repel X0s ∪ Xu (distributional) (specialised: seen) xi (d=300) mapping c0 Xf = X0s ∪ X u (specialised final: all) (a) High-level illustration Xu x&apos;i,h2 (d2=512) Hidden 1 Hidden 2 ... ... ... ... ... ...... ... ... ... ... ... ... x&apos;i,h1 (d1=512) Input ... ... ... ... T"
N18-1048,P17-1042,0,0.0514536,"Missing"
N18-1048,P14-2131,0,0.0446942,"ector and the correct target vector to have a maximum cosine similarity. We do not report the results with this variant as, although it outscores the MSE-style objective, it was consistently outperformed by the MM objective. 4 For further details regarding the architectures and training setup of the used vector collections, we refer the reader to the original papers. Additional experiments with other word vectors, e.g., with CONTEXT 2 VEC (Melamud et al., 2016a) (which uses bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for context modeling), and with dependency-word based embeddings (Bansal et al., 2014; Melamud et al., 2016b) lead to similar results and same conclusions. 5 We have experimented with another set of constraints used in prior work (Zhang et al., 2014; Ono et al., 2015), reaching similar conclusions: these were extracted from WordNet (Fellbaum, 1998) and Roget (Kipfer, 2009), and comprise 1,023,082 synonymy pairs and 380,873 antonymy pairs. PN  omitted only before the final output layer to enable full-range predictions (see Fig. 1b again). The choices of non-linear activation and initialisation are guided by recent recommendations from the literature. First, we use swish (Ramac"
N18-1048,Q17-1010,0,0.59926,"cialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often done as a post-processing (sometimes referred"
N18-1048,D14-1082,0,0.0136967,". This approach, applicable to any postprocessing model, yields considerable gains over the initial specialisation models both in intrinsic word similarity tasks, and in two downstream tasks: dialogue state tracking and lexical text simplification. The positive effects persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of inte"
N18-1048,N15-1184,0,0.37829,"Missing"
N18-1048,N13-1092,0,0.108384,"Missing"
N18-1048,D16-1235,1,0.913856,"Missing"
N18-1048,D17-1185,1,0.901185,"Missing"
N18-1048,P15-2011,1,0.915838,"Missing"
N18-1048,P16-1156,0,0.0292948,"these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). Such post-processing models are popular because they offer a portable, flexible, and light-weight approach to incorporating external knowledge into arbitrary vector spaces, yielding state-of-the-art results on language understanding tasks (Faruqui et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017b). Existing post-processing models, however, suffer from a major limitation. Their modus operandi is to enrich the distributional information with external knowledge only if such knowledge is present in a lexical resource. This means that they update an"
N18-1048,W14-4337,0,0.149535,"Missing"
N18-1048,J15-4004,1,0.950289,"demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often done as a post-processing (sometimes referred to as retrofitting) step: input word vectors are fine-tuned to satisfy linguistic constraints extracted from lexical resources such as WordNet or BabelNet (Faruqui et al., 2015; Mrkši´c et al."
N18-1048,P14-2075,0,0.105851,".10 For a complex word, L IGHT-LS considers the most similar words from the vector space as simplification candidates. Candidates are ranked according to several features, indicating simplicity and fitness for the context (semantic relatedness to the context of the complex word). The substitution is made if the best candidate is simpler than the original word. By providing vector spaces post-specialised for semantic similarity to L IGHT-LS, we expect to more often replace complex words with their true synonyms. We evaluate L IGHT-LS performance in the all setup on the LS benchmark compiled by Horn et al. (2014), who crowdsourced 50 manual simplifications for each complex word. As in prior work, we evaluate performance with the following metrics: 1) Accurracy (Acc.) is the number of correct simplifications made (i.e., the system made the simplification and its substitution is found in the list of crowdsourced substitutions), divided by the total number of indicated complex words; 2) Changed (Ch.) is the percentage of indicated complex words 10 https://github.com/codogogo/lightls Conclusion and Future Work We have presented a novel post-processing model, termed post-specialisation, that specialises wo"
N18-1048,D15-1242,0,0.251526,"Missing"
N18-1048,D15-1032,0,0.0200176,"can thus replace the linear map with a nonlinear function f : Rdim → Rdim . The non-linear mapping, illustrated by Fig. 1b, is implemented as a deep feed-forward fully-connected neural network (DFFN) with H hidden layers and non-linear activations. This variant is called NONLINEAR - MSE. Another variant objective is the contrastive margin-based ranking loss with negative sampling (MM) similar to the original ATTRACT- REPEL objective, used in other applications in prior work (e.g., for cross-modal mapping) (Weston et al., 2011; Frome et al., 2013; Lazaridou et al., 2015; c0 i = f (xi ) denote Kummerfeld et al., 2015). Let x the predicted vector for the word xi ∈ Vs , and let x0 i refer to the “true” vector of xi in the specialised space X0s after the AR specialisation procedure. The MM loss is then defined as follows: JMM = N X k   X  0  0 , x0 0 c c τ δmm − cos x i i + cos x i , x j i=1 j6=i where cos is the cosine similarity measure, δmm is the margin, and k is the number of negative samples. The objective tries to learn the mapping f so c0 i is by the specified that each predicted vector x margin δmm closer to the correct target vector x0 i than to any other of k target vectors x0 j serving as nega"
N18-1048,P15-1027,0,0.481288,"utput of the initial specialisation procedure and replaces word vectors from X0s with their approximations, i.e., f -mapped vectors.2 Objective Functions As mentioned, the N seen words xi ∈ Vs in fact serve as our “pseudotranslation” pairs supporting the learning of a crossspace mapping function. In practice, in its highlevel formulation, our mapping problem is equivalent to those encountered in the literature on crosslingual word embeddings where the goal is to learn a shared cross-lingual space given monolingual vector spaces in two languages and N1 translation pairs (Mikolov et al., 2013a; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Artetxe et al., 2016, 2017; Conneau et al., 2017; Ruder et al., 2017). In our setup, the standard objective based on L2 -penalised 2 We have empirically confirmed the intuition that the first variant is superior to this alternative. We do not report the actual quantitative comparison for brevity. 518 swish swish swish ... swish Xd = Xs ∪ Xu attract-repel X0s ∪ Xu (distributional) (specialised: seen) xi (d=300) mapping c0 Xf = X0s ∪ X u (specialised final: all) (a) High-level illustration Xu x&apos;i,h2 (d2=512) Hidden 1 Hidden 2 ... ... ... ... ... ...... ... ... ... ."
N18-1048,P14-2050,0,0.108038,"nguages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often"
N18-1048,K16-1006,0,0.283738,"ble to any postprocessing model, yields considerable gains over the initial specialisation models both in intrinsic word similarity tasks, and in two downstream tasks: dialogue state tracking and lexical text simplification. The positive effects persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation"
N18-1048,N16-1118,0,0.193378,"ble to any postprocessing model, yields considerable gains over the initial specialisation models both in intrinsic word similarity tasks, and in two downstream tasks: dialogue state tracking and lexical text simplification. The positive effects persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation"
N18-1048,P17-1163,1,0.902962,"Missing"
N18-1048,P15-2130,1,0.814001,"Missing"
N18-1048,Q15-1016,0,0.0670043,"e importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often done as a post-proc"
N18-1048,Q17-1022,1,0.895295,"Missing"
N18-1048,P15-1145,0,0.186188,"ated Work and Motivation Vector Space Specialisation A standard approach to incorporating external and background knowledge into word vector spaces is to pull the representations of similar words closer together and to push words in undesirable relations (e.g., antonyms) away from each other. Some models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et"
N18-1048,P16-2074,0,0.109936,"17). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). Such post-processing models are popular because they offer a portable, flexible, and light-weight approach to incorporating external knowledge into arbitrary vector spaces, yielding state-of-the-art results on language understanding tasks (Faruqui et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017b). Existing post-processing models, however, suffer from a major limitation. Their modus operandi is to enrich the distributional information with external knowledge only if such knowledge is present in a le"
N18-1048,N15-1100,0,0.578375,"vation Vector Space Specialisation A standard approach to incorporating external and background knowledge into word vector spaces is to pull the representations of similar words closer together and to push words in undesirable relations (e.g., antonyms) away from each other. Some models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe"
N18-1048,Q16-1030,0,0.371651,"e Specialisation A standard approach to incorporating external and background knowledge into word vector spaces is to pull the representations of similar words closer together and to push words in undesirable relations (e.g., antonyms) away from each other. Some models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wie"
N18-1048,P15-2070,0,0.0619664,"Missing"
N18-1048,D14-1162,0,0.106855,"s persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic speci"
N18-1048,P15-1173,0,0.0539443,", 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). Such post-processing models are popular because they offer a portable, flexible, and light-weight approach to incorporating external knowledge into arbitrary vector spaces, yielding state-of-the-art results on language understanding tasks (Faruqui et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017b). Existing post-processing models, however, suffer from a major limitation. Their modus operandi is to enrich the distributional information with external knowledg"
N18-1048,K15-1026,0,0.190252,"s a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often done as a post-processing (sometimes referred to as retrofitting) step: input word vectors are fine-tuned to satisfy linguistic constraints extracted from lexical resources such as WordNet or BabelNet (Faruqui et al., 2015; Mrkši´c et al., 2017). The use of exte"
N18-1048,P16-2084,1,0.896635,"Missing"
N18-1048,P16-1024,1,0.911463,"Missing"
N18-1048,N18-1103,1,0.857227,"Missing"
N18-1048,D17-1270,1,0.894634,"Missing"
N18-1048,P17-1006,1,0.878454,"Missing"
N18-1048,E17-1042,1,0.811334,"Missing"
N18-1048,Q15-1025,0,0.28346,"e models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). Such post-processing models are popular because they offer a portable, flexible, and light-weight approach to incorporating external knowledge into arbit"
N18-1048,P14-2089,0,0.217393,"spaces for English and for three test languages (English, German, Italian), verifying the robustness of our approach. 2 Related Work and Motivation Vector Space Specialisation A standard approach to incorporating external and background knowledge into word vector spaces is to pull the representations of similar words closer together and to push words in undesirable relations (e.g., antonyms) away from each other. Some models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge f"
N18-1048,D14-1161,0,0.248362,"tive, it was consistently outperformed by the MM objective. 4 For further details regarding the architectures and training setup of the used vector collections, we refer the reader to the original papers. Additional experiments with other word vectors, e.g., with CONTEXT 2 VEC (Melamud et al., 2016a) (which uses bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for context modeling), and with dependency-word based embeddings (Bansal et al., 2014; Melamud et al., 2016b) lead to similar results and same conclusions. 5 We have experimented with another set of constraints used in prior work (Zhang et al., 2014; Ono et al., 2015), reaching similar conclusions: these were extracted from WordNet (Fellbaum, 1998) and Roget (Kipfer, 2009), and comprise 1,023,082 synonymy pairs and 380,873 antonymy pairs. PN  omitted only before the final output layer to enable full-range predictions (see Fig. 1b again). The choices of non-linear activation and initialisation are guided by recent recommendations from the literature. First, we use swish (Ramachandran et al., 2017; Elfwing et al., 2017) as nonlinearity, defined as swish(x) = x · sigmoid(βx). We fix β = 1 as suggested by Ramachandran et al. (2017).6 Second"
N18-2029,P17-1042,0,0.0157153,"r discriminating between (arguably) most prominent lexico-semantic relations – synonymy, antonymy, hypernymy, and meronymy. The STM architecture is based on the hypothesis that different specializations of input distributional vectors are needed for predicting different lexico-semantic relations. Our results show that, despite its simplicity, STM outperforms more complex models on the benchmarking CogALex-V dataset (Santus et al., 2016). Further, it exhibits stable performance across languages. Finally, we show that, when coupled with a method for inducing a multilingual distributional space (Artetxe et al., 2017; Smith et al., 2017, inter alia), STM can predict lexico-semantic relations also for languages with no training data available from external linguistic resources. While in this work we use STM to discriminate between four prominent lexico-semantic relations, it can, at least conceptually, be trained to predict over an arbitrary set of lexico-semantic relations, provided the availability of respective training data. 2 Related Work Specializing distributional vectors. Given a pair of words, we cannot reliably determine the nature of the lexico-semantic association between them (if any), purely"
N18-2029,W16-5311,0,0.498255,"ties of a particular relation, e.g., synonymy (Wieting et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Vendrov et al., 2016; Vuli´c and Mrkˇsi´c, 2018), they are not able to discriminate between multiple lexico-semantic relations at the same time, i.e., the embedding space gets post-specialized for one particular relation. Classifying lexico-semantic relations. Supervised relation classifiers learn to either identify one particular relation of interest (Baroni et al., 2012; Roller et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017) or to discriminate between multiple relations (Attia et al., 2016; Shwartz and Dagan, 2016), using labeled word pairs from external resources like WordNet. The LexNet model (Shwartz and Dagan, 2016) combines distributional vectors with recurrent encodings of syntactic paths taken from word co-occurrences in text corpora. While adding the syntactic information boosts performance, it limits the model’s portability to other languages. Attia et al. (2016) train a convolutional model in a multi-task setting, coupling multi-class relation classification with binary classification of word relatedness. Unlike LexNet, this model requires only distributional vectors"
N18-2029,E12-1004,0,0.0566818,", 2015; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018). While these methods specialize the distributional space to better reflect properties of a particular relation, e.g., synonymy (Wieting et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Vendrov et al., 2016; Vuli´c and Mrkˇsi´c, 2018), they are not able to discriminate between multiple lexico-semantic relations at the same time, i.e., the embedding space gets post-specialized for one particular relation. Classifying lexico-semantic relations. Supervised relation classifiers learn to either identify one particular relation of interest (Baroni et al., 2012; Roller et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017) or to discriminate between multiple relations (Attia et al., 2016; Shwartz and Dagan, 2016), using labeled word pairs from external resources like WordNet. The LexNet model (Shwartz and Dagan, 2016) combines distributional vectors with recurrent encodings of syntactic paths taken from word co-occurrences in text corpora. While adding the syntactic information boosts performance, it limits the model’s portability to other languages. Attia et al. (2016) train a convolutional model in a multi-task setting, coupling multi-cla"
N18-2029,Q17-1010,0,0.600957,"ces multiple different specializations of input distributional word vectors, tailored for predicting lexico-semantic relations for word pairs. STM outperforms more complex state-of-the-art architectures on two benchmark datasets and exhibits stable performance across languages. We also show that, if coupled with a lingual distributional space, the proposed model can transfer the prediction of lexico-semantic relations to a resource-lean target language without any training data. 1 Introduction Distributional vector spaces (i.e., word embeddings) (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) are ubiquitous in modern natural language processing (NLP). While such vector spaces capture general semantic relatedness, their well-known limitation is the inability to indicate the exact nature of the semantic relation that holds between words. Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (Fu et al., 2014; Ristoski et al., 2017), natural language inference (Tatu and Moldovan, 2005; Chen et al., 2017), text simplification (Glavaˇs and ˇ Stajner, 2015), and paraphrase generation (Madnani and Dorr, 2010), to n"
N18-2029,N15-1184,0,0.038855,"icate the exact nature of the semantic relation that holds between words. Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (Fu et al., 2014; Ristoski et al., 2017), natural language inference (Tatu and Moldovan, 2005; Chen et al., 2017), text simplification (Glavaˇs and ˇ Stajner, 2015), and paraphrase generation (Madnani and Dorr, 2010), to name a few. This is why numerous methods have been proposed that either (1) specialize distributional vectors to better reflect a particular relation (most commonly synonymy) (Faruqui et al., 2015; Kiela et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017) or (2) train supervised relation classifiers using lexico-semantic relations (i.e., labeled word pairs) Contributions. We present the Specialization Tensor Model (STM), a simple and effective feedforward neural model for discriminating between (arguably) most prominent lexico-semantic relations – synonymy, antonymy, hypernymy, and meronymy. The STM architecture is based on the hypothesis that different specializations of input distributional vectors are needed for predicting different lexico-semantic relations. Our results show"
N18-2029,P14-1113,0,0.0318849,"semantic relations to a resource-lean target language without any training data. 1 Introduction Distributional vector spaces (i.e., word embeddings) (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) are ubiquitous in modern natural language processing (NLP). While such vector spaces capture general semantic relatedness, their well-known limitation is the inability to indicate the exact nature of the semantic relation that holds between words. Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (Fu et al., 2014; Ristoski et al., 2017), natural language inference (Tatu and Moldovan, 2005; Chen et al., 2017), text simplification (Glavaˇs and ˇ Stajner, 2015), and paraphrase generation (Madnani and Dorr, 2010), to name a few. This is why numerous methods have been proposed that either (1) specialize distributional vectors to better reflect a particular relation (most commonly synonymy) (Faruqui et al., 2015; Kiela et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017) or (2) train supervised relation classifiers using lexico-semantic relations (i.e., labeled word pairs) Contributions. We present the"
N18-2029,P15-2011,1,0.885342,"Missing"
N18-2029,D17-1185,1,0.897839,"Missing"
N18-2029,D15-1242,0,0.13515,"Missing"
N18-2029,J10-3003,0,0.010054,"014; Bojanowski et al., 2017) are ubiquitous in modern natural language processing (NLP). While such vector spaces capture general semantic relatedness, their well-known limitation is the inability to indicate the exact nature of the semantic relation that holds between words. Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (Fu et al., 2014; Ristoski et al., 2017), natural language inference (Tatu and Moldovan, 2005; Chen et al., 2017), text simplification (Glavaˇs and ˇ Stajner, 2015), and paraphrase generation (Madnani and Dorr, 2010), to name a few. This is why numerous methods have been proposed that either (1) specialize distributional vectors to better reflect a particular relation (most commonly synonymy) (Faruqui et al., 2015; Kiela et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017) or (2) train supervised relation classifiers using lexico-semantic relations (i.e., labeled word pairs) Contributions. We present the Specialization Tensor Model (STM), a simple and effective feedforward neural model for discriminating between (arguably) most prominent lexico-semantic relations – synonymy, antonymy, hypernymy, and"
N18-2029,Q17-1022,1,0.863522,"Missing"
N18-2029,D14-1162,0,0.0895772,"STM) simultaneously produces multiple different specializations of input distributional word vectors, tailored for predicting lexico-semantic relations for word pairs. STM outperforms more complex state-of-the-art architectures on two benchmark datasets and exhibits stable performance across languages. We also show that, if coupled with a lingual distributional space, the proposed model can transfer the prediction of lexico-semantic relations to a resource-lean target language without any training data. 1 Introduction Distributional vector spaces (i.e., word embeddings) (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) are ubiquitous in modern natural language processing (NLP). While such vector spaces capture general semantic relatedness, their well-known limitation is the inability to indicate the exact nature of the semantic relation that holds between words. Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (Fu et al., 2014; Ristoski et al., 2017), natural language inference (Tatu and Moldovan, 2005; Chen et al., 2017), text simplification (Glavaˇs and ˇ Stajner, 2015), and paraphrase generation (Mad"
N18-2029,C14-1097,0,0.15897,"Missing"
N18-2029,W16-5309,0,0.234642,"fiers using lexico-semantic relations (i.e., labeled word pairs) Contributions. We present the Specialization Tensor Model (STM), a simple and effective feedforward neural model for discriminating between (arguably) most prominent lexico-semantic relations – synonymy, antonymy, hypernymy, and meronymy. The STM architecture is based on the hypothesis that different specializations of input distributional vectors are needed for predicting different lexico-semantic relations. Our results show that, despite its simplicity, STM outperforms more complex models on the benchmarking CogALex-V dataset (Santus et al., 2016). Further, it exhibits stable performance across languages. Finally, we show that, when coupled with a method for inducing a multilingual distributional space (Artetxe et al., 2017; Smith et al., 2017, inter alia), STM can predict lexico-semantic relations also for languages with no training data available from external linguistic resources. While in this work we use STM to discriminate between four prominent lexico-semantic relations, it can, at least conceptually, be trained to predict over an arbitrary set of lexico-semantic relations, provided the availability of respective training data."
N18-2029,W16-5310,0,0.32063,"relation, e.g., synonymy (Wieting et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Vendrov et al., 2016; Vuli´c and Mrkˇsi´c, 2018), they are not able to discriminate between multiple lexico-semantic relations at the same time, i.e., the embedding space gets post-specialized for one particular relation. Classifying lexico-semantic relations. Supervised relation classifiers learn to either identify one particular relation of interest (Baroni et al., 2012; Roller et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017) or to discriminate between multiple relations (Attia et al., 2016; Shwartz and Dagan, 2016), using labeled word pairs from external resources like WordNet. The LexNet model (Shwartz and Dagan, 2016) combines distributional vectors with recurrent encodings of syntactic paths taken from word co-occurrences in text corpora. While adding the syntactic information boosts performance, it limits the model’s portability to other languages. Attia et al. (2016) train a convolutional model in a multi-task setting, coupling multi-class relation classification with binary classification of word relatedness. Unlike LexNet, this model requires only distributional vectors as input. Our specializati"
N18-2029,P16-1226,0,0.084535,"Missing"
N18-2029,H05-1047,0,0.0614312,"ining data. 1 Introduction Distributional vector spaces (i.e., word embeddings) (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) are ubiquitous in modern natural language processing (NLP). While such vector spaces capture general semantic relatedness, their well-known limitation is the inability to indicate the exact nature of the semantic relation that holds between words. Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (Fu et al., 2014; Ristoski et al., 2017), natural language inference (Tatu and Moldovan, 2005; Chen et al., 2017), text simplification (Glavaˇs and ˇ Stajner, 2015), and paraphrase generation (Madnani and Dorr, 2010), to name a few. This is why numerous methods have been proposed that either (1) specialize distributional vectors to better reflect a particular relation (most commonly synonymy) (Faruqui et al., 2015; Kiela et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017) or (2) train supervised relation classifiers using lexico-semantic relations (i.e., labeled word pairs) Contributions. We present the Specialization Tensor Model (STM), a simple and effective feedforward neural"
N18-2029,N18-1103,1,0.896903,"Missing"
N18-2029,P17-1006,1,0.867947,"-occurrences in text corpora. While adding the syntactic information boosts performance, it limits the model’s portability to other languages. Attia et al. (2016) train a convolutional model in a multi-task setting, coupling multi-class relation classification with binary classification of word relatedness. Unlike LexNet, this model requires only distributional vectors as input. Our specialization tensor model also requires only distributional vectors as input, but compared to the model of Attia et al. (2016), it has a simpler and more intuitive feed-forward architecture. Glavaˇs and Ponzetto (2017) recently showed that asymmetric specialization of distributional vectors helps to detect asymmetric relations (hypernymy, meronymy). Following these findings, we hypothesize that detection of different relations requires different specializations of distributional vectors, so we design STM accordingly. 3 Specialization Tensor Model The high-level architecture of the Specialization Tensor Model is depicted in Figure 1. The input to the model is a pair of unspecialized distributional word vectors (x1 , x2 ). Both input vectors are first transformed in K different ways with functions (1) (K) fS"
N18-2029,Q15-1025,0,0.0390057,"na, June 1 - 6, 2018. 2018 Association for Computational Linguistics ... fS(2) fP(2) ... fP(1) fclass ... ... ... ... fS(1) fS(K) fP(K) Figure 1: Architecture of the Specialization Tensor Model (STM). ticular relations use external linguistic constraints (e.g., from WordNet) to either (1) modify the original objective of general embedding algorithms and directly train relation-specific embeddings from corpora (Yu and Dredze, 2014; Kiela et al., 2015) or (2) post-process the pre-trained distributional space by moving closer together (or further apart) words that stand in a particular relation (Wieting et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018). While these methods specialize the distributional space to better reflect properties of a particular relation, e.g., synonymy (Wieting et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Vendrov et al., 2016; Vuli´c and Mrkˇsi´c, 2018), they are not able to discriminate between multiple lexico-semantic relations at the same time, i.e., the embedding space gets post-specialized for one particular relation. Classifying lexico-semantic relations. Supervised relation classifiers learn to either identify one particular relation of interest (Baroni"
N18-2029,P14-2089,0,0.0550538,"ifferent types of semantic associations between words. This is why methods for specializing word embeddings for par181 Proceedings of NAACL-HLT 2018, pages 181–187 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics ... fS(2) fP(2) ... fP(1) fclass ... ... ... ... fS(1) fS(K) fP(K) Figure 1: Architecture of the Specialization Tensor Model (STM). ticular relations use external linguistic constraints (e.g., from WordNet) to either (1) modify the original objective of general embedding algorithms and directly train relation-specific embeddings from corpora (Yu and Dredze, 2014; Kiela et al., 2015) or (2) post-process the pre-trained distributional space by moving closer together (or further apart) words that stand in a particular relation (Wieting et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018). While these methods specialize the distributional space to better reflect properties of a particular relation, e.g., synonymy (Wieting et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Vendrov et al., 2016; Vuli´c and Mrkˇsi´c, 2018), they are not able to discriminate between multiple lexico-semantic relations at the same time, i.e., the embedding space get"
P13-2139,H05-1016,0,0.0358848,"ches rely on traditional vector space models (Salton et al., 1975), as more sophisticated natural language processing techniques have not yet proven to be useful for this task. On the other hand, significant advances in sentence-level event extraction have been made over the last decade, in particular as the result of 2 Related Work The traditional vector space model (VSM) (Salton et al., 1975) computes the cosine between bag-ofwords representations of documents. The VSM is at the core of most approaches that identify sametopic news stories (Hatzivassiloglou et al., 2000; Brants et al., 2003; Kumaran and Allan, 2005; Atkinson and Van der Goot, 2009). However, it has been observed that some word classes (e.g., named entities, noun phrases, collocations) have more significance than the others. Among them, named entities have been considered as particularly important, as they often identify the participants of an event. In view of this, Hatzivassiloglou et al. (2000) restrict the set of words to be used for document representation to words constituting noun phrases and named entities. Makkonen et 797 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 797–803, c So"
P13-2139,bejan-harabagiu-2008-linguistic,0,0.22822,"Missing"
P13-2139,D12-1045,0,0.0727162,"Missing"
P13-2139,P10-1143,0,0.106609,"Missing"
P13-2139,S07-1014,0,0.0985379,"Missing"
P13-2139,wayne-2000-multilingual,0,0.052955,"oreferent event mentions: (yanked, stolen), (recovered, recovered), and (arrests, arrested). Experiments We conducted two preliminary experiments to investigate whether kernels on event graphs can be used to recognize identical events. 4.1 R Table 1: Results for recognition of identical events (v,v )∈VP 4 P Task 1: Recognizing identical events Dataset. In the first experiment, we classify pairs of news stories as either describing identical real-world events or not. For this we need a collection of stories in which pairs of stories on identical events have been annotated as such. TDT corpora (Wayne, 2000) is not directly usable because it has no such annotations. We therefore decided to build a small annotated dataset.1 To this end, we use the news clusters of the EMM NewsBrief service (Steinberger et al., 2009). EMM clusters news stories from different sources using a document similarity score. We acquired 10 randomly chosen news clusters, manually inspected each of them, and retained in each cluster only the documents that describe the same real-world events. Additionally, we ensured that no documents from Results. For each graph kernel and the VSM baseline, we determine the optimal threshol"
P13-2139,C00-2137,0,0.0712328,"c. Avg. prec. Tensor PGK Conormal PGK WDK 86.7 93.3 86.7 96.8 97.5 95.7 VSM baseline 80.0 77.1 Event-shifting paraphrase “Taliban militants have been arrested in north-west Pakistan. At least 380 militants have been arrested. . . ” Table 3: Results for event-based similarity ranking Table 2: Event paraphrasing example two different rank evaluation metrics: R-precision (precision at rank 30, as there are 30 positive pairs) and average precision. The performance of graph kernel models and the VSM baseline is given in Table 3. We tested the significance of differences using stratified shuffling (Yeh, 2000). When considering average precision, all kernel models significantly (at p &lt; 0.01) outperform the baseline. However, when considering R-precision, only the conormal PGK model significantly (at p &lt; 0.05) outperforms the baseline. There is no statistical significance in performance differences between the considered kernel methods. Inspection of the rankings reveals that graph kernels assign very low scores to negative pairs, i.e., they distinguish well between textual representations of topically similar, but different real-world events. individual kernel scores to an SVM model (with RBF kerne"
P13-2139,S10-1010,0,\N,Missing
P15-2011,P11-1027,0,0.0145039,"omplexity reduction (or gain) that would be introduced should the simplification candidate replace the original word. Language model features. The rationale for having language model features is obvious – a simplification candidate is more likely to be a compatible substitute if it fits into the sequence of words preceding and following the original word. Let w−2 w−1 ww1 w2 be the context of the original word w. We consider a simplification candidate c to be a good substitute for w if w−2 w−1 cw1 w2 is a likely sequence according to the language model. We employed the Berkeley language model (Pauls and Klein, 2011) to compute the likelihoods. Since Berkeley LM contains only bigrams and trigrams, we retrieve the likelihoods for ngrams w−1 c, cw1 , w−2 w−1 c, cw1 w2 , and w−1 cw1 , for each simplification candidate c. 3.3 has no dedicated component for deciding whether simplifying a word is necessary, it accounts for this implicitly by performing the simplification only if the best candidate has lower information content than the original word (lines 13–15). Since simplification candidates need not have the same POS tag as the original word, to preserve grammaticality, we transform the chosen candidate in"
P15-2011,P11-2087,0,0.0299036,"Missing"
P15-2011,D14-1162,0,0.125697,"rpora (Biran et al., 2011) to supervised methods learning substitutions from the sentence-aligned corpora (Horn et al., 2014). Using simplified corpora improves the simplification performance, but reduces method applicability to the few languages for which such corpora exist. The research question motivating this work relates to achieving comparable simplification performance without resorting to simplified corpora or lexicons like WordNet. Observing that “simple” words appear in regular (i.e., “complex”, not simplified) text as well, we exploit recent advances in word vector representations (Pennington et al., 2014) to find suitable simplifications for complex words. We evaluate the performance of our resource-light approach (1) automatically, on two existing lexical simplification datasets and (2) manually, via human judgements of grammaticality, simplicity, and meaning preservation. The obtained results support the claim that effective lexical simplification can be achieved without using simplified corpora. Simplification of lexically complex texts, by replacing complex words with their simpler synonyms, helps non-native speakers, children, and language-impaired people understand text better. Recent le"
P15-2011,shardlow-2014-open,0,0.205273,"fying multi-word expressions. In this work, we associate word complexity with the commonness of the word in the corpus, and not with the length of the word. Net, for a predefined set of complex words (Carroll et al., 1998; Bautista et al., 2009), and then choosing the “simplest” of these synonyms, typically using some frequency-based (Devlin and Tait, 1998; De Belder and Moens, 2010) or lengthbased heuristics (Bautista et al., 2009). The main shortcomings of the rule-based systems include low recall (De Belder and Moens, 2010) and misclassification of simple words as complex (and vice versa) (Shardlow, 2014). The paradigm shift from knowledge-based to data-driven simplification came with the creation of Simple Wikipedia, which, aligned with the “original” Wikipedia, constitutes a large comparable corpus to learn from. Yatskar et al. (2010) used the edit history of Simple Wikipedia to recognize lexical simplifications. They employed a probabilistic model to discern simplification edits from other types of content changes. Biran et al. (2011) presented an unsupervised method for learning substitution pairs from a corpus of comparable texts from Wikipedia and Simple Wikipedia, although they exploite"
P15-2011,S12-1046,0,0.030363,"r grammaticality (0.71), followed by meaning preservation (0.62) and simplicity (0.57), which we consider to be a fair agreement, especially for inherently subjective notions of simplicity and meaning preservation. The results of human evaluation are shown in Table 3. In addition to grammaticality (Gr), simplicity (Smp), and meaning preservation (MP), we measured the percentage of sentences with at least one change made by the system (Ch). The results imply that the sentences produced by L IGHTRanking Task We next evaluated L IGHT-LS on the SemEval2012 lexical simplification task for English (Specia et al., 2012), which focused on ranking a target word (in a context) and three candidate replacements, from the simplest to the most complex. To account for the peculiarity of the task where the target word is also one of the simplification candidates, we modified the features as follows (otherwise, an unfair advantage would be given to the target word): (1) we excluded the semantic similarity feature, and (2) we used the information content of the candidate instead of the difference of information contents. We used the official SemEval task evaluation script to compute the Cohen’s kappa index for the agre"
P15-2011,D11-1038,0,0.0181616,"ex words, which results in very low accuracy. Our method numerically outperforms the supervised method of Horn et al. (2014), but the difference is not statistically significant. 4.2 κ baseline-random baseline-frequency 0.013 0.471 Jauhar and Specia (2012) L IGHT-LS 0.496 0.540 our information content-based feature (the higher the frequency, the lower the information content). 4.3 Human Evaluation Although automated task-specific evaluations provide useful indications of a method’s performance, they are not as reliable as human assessment of simplification quality. In line with previous work (Woodsend and Lapata, 2011; Wubben et al., 2012), we let human evaluators judge the grammaticality, simplicity, and meaning preservation of the simplified text. We compiled a dataset of 80 sentence-aligned pairs from Wikipedia and Simple Wikipedia and simplified the original sentences with L IGHT-LS and the publicly available system of Biran et al. (2011). We then let two annotators (with prior experience in simplification annotations) grade grammaticality and simplicity for the manual simplification from Simple Wikipedia and simplifications produced by each of the two systems (total of 320 annotations per annotator)."
P15-2011,R13-2011,1,0.886639,"Missing"
P15-2011,P12-1107,0,0.407788,"Missing"
P15-2011,P14-2075,0,0.35549,"p in Computational Linguistics SanjaStajner@wlv.ac.uk Goran Glavaˇs University of Zagreb Faculty of Electrical Engineering and Computing goran.glavas@fer.hr Abstract ber of languages diminishes the impact of these simplification methods. The emergence of the Simple Wikipedia1 shifted the focus towards the data-driven approaches to lexical simplification, ranging from unsupervised methods leveraging either the metadata (Yatskar et al., 2010) or co-occurrence statistics of the simplified corpora (Biran et al., 2011) to supervised methods learning substitutions from the sentence-aligned corpora (Horn et al., 2014). Using simplified corpora improves the simplification performance, but reduces method applicability to the few languages for which such corpora exist. The research question motivating this work relates to achieving comparable simplification performance without resorting to simplified corpora or lexicons like WordNet. Observing that “simple” words appear in regular (i.e., “complex”, not simplified) text as well, we exploit recent advances in word vector representations (Pennington et al., 2014) to find suitable simplifications for complex words. We evaluate the performance of our resource-ligh"
P15-2011,N10-1056,0,0.0151031,"a et al., 2009), and then choosing the “simplest” of these synonyms, typically using some frequency-based (Devlin and Tait, 1998; De Belder and Moens, 2010) or lengthbased heuristics (Bautista et al., 2009). The main shortcomings of the rule-based systems include low recall (De Belder and Moens, 2010) and misclassification of simple words as complex (and vice versa) (Shardlow, 2014). The paradigm shift from knowledge-based to data-driven simplification came with the creation of Simple Wikipedia, which, aligned with the “original” Wikipedia, constitutes a large comparable corpus to learn from. Yatskar et al. (2010) used the edit history of Simple Wikipedia to recognize lexical simplifications. They employed a probabilistic model to discern simplification edits from other types of content changes. Biran et al. (2011) presented an unsupervised method for learning substitution pairs from a corpus of comparable texts from Wikipedia and Simple Wikipedia, although they exploited the (co-)occurrence statistics of the simplified corpora rather than its metadata. Horn et al. (2014) proposed a supervised framework for learning simplification rules. Using a sentence-aligned simplified corpus, they generated the ca"
P15-2011,S12-1066,0,0.130251,"ccount for the peculiarity of the task where the target word is also one of the simplification candidates, we modified the features as follows (otherwise, an unfair advantage would be given to the target word): (1) we excluded the semantic similarity feature, and (2) we used the information content of the candidate instead of the difference of information contents. We used the official SemEval task evaluation script to compute the Cohen’s kappa index for the agreement on the ordering for each pair of candidates. The performance of L IGHT-LS together with results of the best-performing system (Jauhar and Specia, 2012) from the SemEval-2012 task and two baselines (random and frequency-based) is given in Table 2. L IGHT-LS significantly outperforms the supervised model by Jauhar and Specia (2012) with p < 0.05, according to the nonparametric stratified shuffling test (Yeh, 2000). An interesting observation is that the competitive frequency-based baseline highly correlates with 66 Table 4: Example simplifications Source Sentence Original sentence The contrast between a high level of education and a low level of political rights was particularly great in Aarau, and the city refused to send troops to defend the"
P15-2011,C00-2137,0,0.019585,"e information content of the candidate instead of the difference of information contents. We used the official SemEval task evaluation script to compute the Cohen’s kappa index for the agreement on the ordering for each pair of candidates. The performance of L IGHT-LS together with results of the best-performing system (Jauhar and Specia, 2012) from the SemEval-2012 task and two baselines (random and frequency-based) is given in Table 2. L IGHT-LS significantly outperforms the supervised model by Jauhar and Specia (2012) with p < 0.05, according to the nonparametric stratified shuffling test (Yeh, 2000). An interesting observation is that the competitive frequency-based baseline highly correlates with 66 Table 4: Example simplifications Source Sentence Original sentence The contrast between a high level of education and a low level of political rights was particularly great in Aarau, and the city refused to send troops to defend the Bernese border. The separate between a high level of education and a low level of political rights was particularly great in Aarau , and the city refused to send troops to defend the Bernese border. The contrast between a high level of education and a low level o"
P15-2011,C10-1152,0,0.342819,"Missing"
P18-1004,W13-3520,0,0.0820068,"xi2 , g i )}i=1 be one micro-batch created from one synonymy constraint and let Ma be the analogous micro-batch created from one antonymy constraint. Let us then assume that the first triple (i.e., for i = 1) in every microbatch corresponds to the constraint pair and the remaining 2K triples (i.e., for i ∈ {2, . . . , 2K + 1}) to respective non-constraint word pairs. We then define the contrastive objective as follows: JCNT = X 2K+1 X  i 1 2 1 i 2 (g i − gmin ) − (g 0 − g 0 ) distributional vectors for English: (1) SGNS-W2 – vectors trained on the Wikipedia dump from the Polyglot project (Al-Rfou et al., 2013) using the Skip-Gram algorithm with Negative Sampling (SGNS) (Mikolov et al., 2013b) by Levy and Goldberg (2014b), using the context windows of size 2; (2) G LOV E -CC – vectors trained with the GloVe (Pennington et al., 2014) model on the Common Crawl; and (3) FAST T EXT – vectors trained on Wikipedia with a variant of SGNS that builds word vectors by summing the vectors of their constituent character n-grams (Bojanowski et al., 2017). Linguistic Constraints. We experiment with the sets of linguistic constraints used in prior work (Zhang et al., 2014; Ono et al., 2015). These constraints, ext"
P18-1004,N13-1092,0,0.222432,"Missing"
P18-1004,P98-1013,0,0.152635,"ontrast modeling (Nguyen et al., 2016), and cross-lingual transfer of lexical resources (Vuli´c et al., 2017a). A common goal pertaining to all retrofitting models is to pull the vectors of similar words (e.g., synonyms) closer together, while some models also push the vectors of dissimilar words (e.g., antonyms) further apart. The specialization methods fall into two categories: (1) joint specialization methods, and (2) post-processing (i.e., retrofitting) methods. Methods from both categories make use of similar lexical resources – they typically leverage WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998), the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015), morphological lexicons (Cotterell et al., 2016), or simple handcrafted linguistic rules (Vuli´c et al., 2017b). In what follows, we discuss the two model categories. Joint Specialization Models. These models integrate external constraints into the distributional training procedure of general word embedding algorithms such as CBOW, Skip-Gram (Mikolov et al., 2013b), or Canonical Correlation Analysis (Dhillon et al., 2015). They modify the prior or the regularization of the original objective (Yu and Dredze, 2014"
P18-1004,D16-1235,1,0.899315,"Missing"
P18-1004,Q17-1010,0,0.16366,"dels trained with different constraints. Italian Croatian .407 .360 .249 .415 .533 .406 .448 .287 .315 Table 3: Spearman’s ρ correlation scores for German, Italian, and Croatian embeddings in the transfer setup: the vectors are specialized using the models trained on English constraints and evaluated on respective language-specific SimLex-999 variants. tor space6 containing word vectors of three other languages – German, Italian, and Croatian – along with the English vectors.7 Concretely, we map the Italian CBOW vectors (Dinu et al., 2015), German FastText vectors trained on German Wikipedia (Bojanowski et al., 2017), and Croatian Skip-Gram vectors trained on HrWaC corpus (Ljubeˇsi´c and Erjavec, 2011) to the G LOVE -CC English space. We create the translation pairs needed to learn the projections by automatically translating 4,000 most frequent English words to all three other languages with Google Translate. We then employ the ER model trained to specialize the G LOVE -CC space using the full set of English constraints, to specialize the distributional spaces of other languages. We evaluate the quality of the specialized spaces on the respective SimLex-999 dataset for each language (Leviant and Reichart"
P18-1004,D17-1185,1,0.874908,"Missing"
P18-1004,P15-2011,1,0.905471,"Missing"
P18-1004,W14-4337,0,0.109219,"Missing"
P18-1004,D14-1082,0,0.0149326,"We report large gains over original distributional vector spaces in (1) intrinsic word similarity evaluation and on (2) two downstream tasks – lexical simplification and dialog state tracking. Finally, we also successfully specialize vector spaces of new languages (i.e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces. 1 Introduction Algebraic modeling of word vector spaces is one of the core research areas in modern Natural Language Processing (NLP) and its usefulness has been shown across a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Commonly employed distributional models for word vector induction are based on the distributional hypothesis (Harris, 1954), i.e., they rely on word co-occurrences obtained from large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014a; Levy 34 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 34–45 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Mrkˇsi´c et al., 2017, inter alia). The latter, in general, outperform the former (Mrkˇsi"
P18-1004,J15-4004,0,0.343088,"re expensive re-training on large text corpora, but is directly applied on top of any pre-trained vector space. The key idea of ER is to directly learn a specialization function in a supervised setting, using lexical constraints as training instances. In other words, our model, implemented as a deep feedforward neural architecture, learns a (non-linear) function which “translates” word vectors from the distributional space into the specialized space. We show that the proposed ER approach yields considerable gains over distributional spaces in word similarity evaluation on standard benchmarks (Hill et al., 2015; Gerz et al., 2016), as well as in two downstream tasks – lexical simplification and dialog state tracking. Furthermore, we show that, by coupling the ER model with shared multilingual embedding spaces (Mikolov et al., 2013a; Smith et al., 2017), we can also specialize distributional spaces for languages unseen in the training data in a zero-shot language transfer setup. In other words, we show that an explicit retrofitting model trained with external constraints from one language can be successfully used to specialize the distributional space of another language. 2 Mrkˇsi´c, 2017), lexical c"
P18-1004,P14-2075,0,0.134957,"ctor space.8 For each word in the input text L IGHT-LS retrieves most similar replacement candidates from the vector space. The candidates are then ranked according to several measures of simplicity and fitness for the context. Finally, the replacement is made if the top-ranked candidate is estimated to be simpler than the original word. By plugging-in vector spaces specialized by the ER model into L IGHT-LS, we hope to generate true synonymous candidates more frequently than with the unspecialized distributional space. Evaluation Setup. We evaluate L IGHT-LS on the LS dataset crowdsourced by Horn et al. (2014). For each indicated complex word Horn et al. (2014) collected 50 manual simplifications. We use two evaluation metrics from prior work (Horn et al., ˇ 2014; Glavaˇs and Stajner, 2015) to quantify the quality and frequency of word replacements: (1) 5.3.2 Dialog State Tracking Finally, we also evaluate the importance of explicit retrofitting in a downstream language understand8 The Light-LS implementation is available at: https://bitbucket.org/gg42554/embesimp 41 Text G LOV E -CC ATTRACT-R EPEL ER-CNT Wrestlers portrayed a villain or a hero as they followed a series of events that built tension"
P18-1004,P16-1156,0,0.0310283,"Missing"
P18-1004,N15-1070,0,0.0625061,"et al., 2015; Bollegala et al., 2016; Osborne et al., 2016; Nguyen et al., 2016, 2017). Besides generally displaying lower performance compared to retrofitting methods (Mrkˇsi´c et al., 2016), these models are also tied to the distributional objective and any change of the underlying distributional model induces a change of the entire joint model. This makes them less versatile than the retrofitting methods. Post-Processing Models. Models from the popularly termed retrofitting family inject lexical knowledge from external resources into arbitrary pretrained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2016). These models fine-tune the vectors of words present in the linguistic constraints to reflect the ground-truth lexical knowledge. While the large majority of specialization models from both classes operate only with similarity constraints, a line of recent work (Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017b) demonstrates that knowledge about both similar and dissimilar words leads to Related Work The importance of vector space specialization for downstream tasks has been observed,"
P18-1004,D15-1242,0,0.266231,"Missing"
P18-1004,W16-1607,0,0.228634,"Missing"
P18-1004,N15-1184,0,0.417018,"Missing"
P18-1004,N16-1077,0,0.0280739,"t only vectors of words from external constraints. 3 Our goal is to discern semantic similarity from semantic relatedness by comparing, in the specialized space, the distances between word pairs (wi , wj , r) ∈ C with distances that words wi and wj from those pairs have with other vocabulary words wm . It is intuitive to enforce that the synonyms are as close as possible and antonyms as far as possible. However, we do not know what the distances between non-synonymous and nonantonymous words g(x0 i , xm ) in the specialized space should look like. This is why, for all other words, similar to (Faruqui et al., 2016; Mrkˇsi´c et al., 2017), we assume that the distances in the specialized space for all word pairs not found in C should stay the same as in the distributional space: g(x0 i , x0 m ) = g(xi , xm ). This way we preserve the useful semantic content available in the original distributional space. In downstream tasks most errors stem from vectors of semantically related words (e.g., car – driver) being as similar as vectors of semantically similar words (e.g., car – automobile). To anticipate this, we compare the distances of pairs (wi , wj , r) ∈ C with the distances for pairs (wi , wm ) and (wj"
P18-1004,Q17-1022,1,0.891051,"Missing"
P18-1004,P14-2050,0,0.654274,"ng data) by coupling ER with shared multilingual distributional vector spaces. 1 Introduction Algebraic modeling of word vector spaces is one of the core research areas in modern Natural Language Processing (NLP) and its usefulness has been shown across a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Commonly employed distributional models for word vector induction are based on the distributional hypothesis (Harris, 1954), i.e., they rely on word co-occurrences obtained from large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014a; Levy 34 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 34–45 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Mrkˇsi´c et al., 2017, inter alia). The latter, in general, outperform the former (Mrkˇsi´c et al., 2016). Retrofitting models can be applied to arbitrary distributional spaces but they suffer from a major limitation – they locally update only vectors of words present in the external constraints, whereas vectors of all other (unseen) words remain intact. In contrast, joint special"
P18-1004,Q15-1016,0,0.119183,"Missing"
P18-1004,P16-2074,0,0.477053,"al., 2013), or BabelNet (Navigli and Ponzetto, 2012), to specialize distributional spaces for a particular lexical relation, e.g., synonymy (Faruqui et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Glavaˇs and Ponzetto, 2017). External constraints are commonly pairs of words between which a particular relation holds. Existing specialization methods exploit the external linguistic constraints in two prominent ways: (1) joint specialization models modify the learning objective of the original distributional model by integrating the constraints into it (Yu and Dredze, 2014; Kiela et al., 2015; Nguyen et al., 2016, inter alia); (2) post-processing models fine-tune distributional vectors retroactively after training to satisfy the external constraints (Faruqui et al., 2015; Semantic specialization of distributional word vectors, referred to as retrofitting, is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation. Existing retrofitting models integrate linguistic constraints directly into learning objectives and, consequently, specialize only the vectors of words from the constraints. In this work, in contrast, we transform external lexico"
P18-1004,P15-1145,0,0.173321,"dcrafted linguistic rules (Vuli´c et al., 2017b). In what follows, we discuss the two model categories. Joint Specialization Models. These models integrate external constraints into the distributional training procedure of general word embedding algorithms such as CBOW, Skip-Gram (Mikolov et al., 2013b), or Canonical Correlation Analysis (Dhillon et al., 2015). They modify the prior or the regularization of the original objective (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015) or integrate the constraints directly into the, e.g., an SGNS- or CBOW-style objective (Liu et al., 2015; Ono et al., 2015; Bollegala et al., 2016; Osborne et al., 2016; Nguyen et al., 2016, 2017). Besides generally displaying lower performance compared to retrofitting methods (Mrkˇsi´c et al., 2016), these models are also tied to the distributional objective and any change of the underlying distributional model induces a change of the entire joint model. This makes them less versatile than the retrofitting methods. Post-Processing Models. Models from the popularly termed retrofitting family inject lexical knowledge from external resources into arbitrary pretrained word vectors (Faruqui et al.,"
P18-1004,N15-1100,0,0.354979,"c rules (Vuli´c et al., 2017b). In what follows, we discuss the two model categories. Joint Specialization Models. These models integrate external constraints into the distributional training procedure of general word embedding algorithms such as CBOW, Skip-Gram (Mikolov et al., 2013b), or Canonical Correlation Analysis (Dhillon et al., 2015). They modify the prior or the regularization of the original objective (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015) or integrate the constraints directly into the, e.g., an SGNS- or CBOW-style objective (Liu et al., 2015; Ono et al., 2015; Bollegala et al., 2016; Osborne et al., 2016; Nguyen et al., 2016, 2017). Besides generally displaying lower performance compared to retrofitting methods (Mrkˇsi´c et al., 2016), these models are also tied to the distributional objective and any change of the underlying distributional model induces a change of the entire joint model. This makes them less versatile than the retrofitting methods. Post-Processing Models. Models from the popularly termed retrofitting family inject lexical knowledge from external resources into arbitrary pretrained word vectors (Faruqui et al., 2015; Jauhar et al"
P18-1004,Q16-1030,0,0.407434,"llows, we discuss the two model categories. Joint Specialization Models. These models integrate external constraints into the distributional training procedure of general word embedding algorithms such as CBOW, Skip-Gram (Mikolov et al., 2013b), or Canonical Correlation Analysis (Dhillon et al., 2015). They modify the prior or the regularization of the original objective (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015) or integrate the constraints directly into the, e.g., an SGNS- or CBOW-style objective (Liu et al., 2015; Ono et al., 2015; Bollegala et al., 2016; Osborne et al., 2016; Nguyen et al., 2016, 2017). Besides generally displaying lower performance compared to retrofitting methods (Mrkˇsi´c et al., 2016), these models are also tied to the distributional objective and any change of the underlying distributional model induces a change of the entire joint model. This makes them less versatile than the retrofitting methods. Post-Processing Models. Models from the popularly termed retrofitting family inject lexical knowledge from external resources into arbitrary pretrained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Wieting et"
P18-1004,N16-1118,0,0.0607403,"ver original distributional vector spaces in (1) intrinsic word similarity evaluation and on (2) two downstream tasks – lexical simplification and dialog state tracking. Finally, we also successfully specialize vector spaces of new languages (i.e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces. 1 Introduction Algebraic modeling of word vector spaces is one of the core research areas in modern Natural Language Processing (NLP) and its usefulness has been shown across a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Commonly employed distributional models for word vector induction are based on the distributional hypothesis (Harris, 1954), i.e., they rely on word co-occurrences obtained from large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014a; Levy 34 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 34–45 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Mrkˇsi´c et al., 2017, inter alia). The latter, in general, outperform the former (Mrkˇsi´c et al., 2016). Retro"
P18-1004,P15-2070,0,0.0613007,"Missing"
P18-1004,D14-1162,0,0.0919155,".e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces. 1 Introduction Algebraic modeling of word vector spaces is one of the core research areas in modern Natural Language Processing (NLP) and its usefulness has been shown across a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Commonly employed distributional models for word vector induction are based on the distributional hypothesis (Harris, 1954), i.e., they rely on word co-occurrences obtained from large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014a; Levy 34 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 34–45 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Mrkˇsi´c et al., 2017, inter alia). The latter, in general, outperform the former (Mrkˇsi´c et al., 2016). Retrofitting models can be applied to arbitrary distributional spaces but they suffer from a major limitation – they locally update only vectors of words present in the external constraints, whereas vectors of all other (unseen) words remain intact. I"
P18-1004,P15-1173,0,0.117133,"Missing"
P18-1004,J13-3004,0,0.0258719,"e Group University of Cambridge University of Mannheim 9 West Road, Cambridge CB3 9DA B6, 29, DE-68161 Mannheim iv250@cam.ac.uk goran@informatik.uni-mannheim.de Abstract et al., 2015; Bojanowski et al., 2017). The dependence on purely distributional knowledge results in a well-known tendency of fusing semantic similarity with other types of semantic relatedness (Hill et al., 2015; Schwartz et al., 2015) in the induced vector spaces. Consequently, the similarity between distributional vectors indicates just an abstract semantic association and not a precise semantic relation (Yih et al., 2012; Mohammad et al., 2013). For example, it is difficult to discern synonyms from antonyms in distributional spaces. This property has a particularly negative effect on NLP applications like text simplification and statistical dialog modeling, in which discerning semantic similarity from other types of semantic relatedness is pivotal to the system performance (Glavaˇs and ˇ Stajner, 2015; Faruqui et al., 2015; Mrkˇsi´c et al., 2016; Kim et al., 2016b). A standard solution is to move beyond purely unsupervised learning of word representations, in a process referred to as word vector space specialization or retrofitting."
P18-1004,P17-1163,0,0.0861994,"Missing"
P18-1004,K15-1026,0,0.424347,"Missing"
P18-1004,N18-1103,1,0.847418,"Missing"
P18-1004,D17-1270,1,0.911525,"Missing"
P18-1004,P17-1006,1,0.846512,"Missing"
P18-1004,E17-1042,0,0.0874422,"Missing"
P18-1004,Q15-1025,0,0.184681,"t al., 2016; Nguyen et al., 2016, 2017). Besides generally displaying lower performance compared to retrofitting methods (Mrkˇsi´c et al., 2016), these models are also tied to the distributional objective and any change of the underlying distributional model induces a change of the entire joint model. This makes them less versatile than the retrofitting methods. Post-Processing Models. Models from the popularly termed retrofitting family inject lexical knowledge from external resources into arbitrary pretrained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2016). These models fine-tune the vectors of words present in the linguistic constraints to reflect the ground-truth lexical knowledge. While the large majority of specialization models from both classes operate only with similarity constraints, a line of recent work (Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017b) demonstrates that knowledge about both similar and dissimilar words leads to Related Work The importance of vector space specialization for downstream tasks has been observed, inter alia, for dialog state tracking (Mrkˇsi´c"
P18-1004,D12-1111,0,0.0212671,"ata and Web Science Group University of Cambridge University of Mannheim 9 West Road, Cambridge CB3 9DA B6, 29, DE-68161 Mannheim iv250@cam.ac.uk goran@informatik.uni-mannheim.de Abstract et al., 2015; Bojanowski et al., 2017). The dependence on purely distributional knowledge results in a well-known tendency of fusing semantic similarity with other types of semantic relatedness (Hill et al., 2015; Schwartz et al., 2015) in the induced vector spaces. Consequently, the similarity between distributional vectors indicates just an abstract semantic association and not a precise semantic relation (Yih et al., 2012; Mohammad et al., 2013). For example, it is difficult to discern synonyms from antonyms in distributional spaces. This property has a particularly negative effect on NLP applications like text simplification and statistical dialog modeling, in which discerning semantic similarity from other types of semantic relatedness is pivotal to the system performance (Glavaˇs and ˇ Stajner, 2015; Faruqui et al., 2015; Mrkˇsi´c et al., 2016; Kim et al., 2016b). A standard solution is to move beyond purely unsupervised learning of word representations, in a process referred to as word vector space special"
P18-1004,P14-2089,0,0.528449,"the Paraphrase Database (Ganitkevitch et al., 2013), or BabelNet (Navigli and Ponzetto, 2012), to specialize distributional spaces for a particular lexical relation, e.g., synonymy (Faruqui et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Glavaˇs and Ponzetto, 2017). External constraints are commonly pairs of words between which a particular relation holds. Existing specialization methods exploit the external linguistic constraints in two prominent ways: (1) joint specialization models modify the learning objective of the original distributional model by integrating the constraints into it (Yu and Dredze, 2014; Kiela et al., 2015; Nguyen et al., 2016, inter alia); (2) post-processing models fine-tune distributional vectors retroactively after training to satisfy the external constraints (Faruqui et al., 2015; Semantic specialization of distributional word vectors, referred to as retrofitting, is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation. Existing retrofitting models integrate linguistic constraints directly into learning objectives and, consequently, specialize only the vectors of words from the constraints. In this work,"
P18-1004,D14-1161,0,0.47788,"Missing"
P19-1070,D18-1214,0,0.335056,"Missing"
P19-1070,P17-1042,0,0.649125,"olingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections can be bootstrapped from small dictionaries of 50–100 pairs (Vuli´c and Korhonen, 2016; Zhang et al., 2016), identical strings and cognates (Smith et al., 2017; Søgaard et al., 2018), and shared numerals (Artetxe et al., 2017). Moreover, recent work has leveraged topological similarities between monolingual vector spaces to introduce fully unsupervised projection-based CLE models, not demanding any bilingual supervision (Conneau et al., 2018a; Artetxe et al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of"
P19-1070,P18-1073,0,0.0782391,"irrespective of their language. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision ("
P19-1070,J82-2005,0,0.676033,"Missing"
P19-1070,D18-1024,0,0.240154,"r alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom"
P19-1070,D18-1269,0,0.132185,"m seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections can be bootstrapped from small dictionaries of 50–100 pairs (Vuli´c and Korhonen, 2016; Zhang et al., 2016), identical strings and cognates (Smith et al., 2017; Søgaard et al., 2018), and shared numerals (Artetxe et al., 2017). Moreover, recent work has leveraged topological similarities between monolingual vector spaces to introduce fully unsupervised projection-based CLE models, not demanding any bilingual supervision (Conneau et al., 2018a; Artetxe et al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et"
P19-1070,D18-1062,0,0.0575484,"Missing"
P19-1070,D18-1027,0,0.560132,"ding any bilingual supervision (Conneau et al., 2018a; Artetxe et al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well o"
P19-1070,P18-1128,0,0.0627467,"Missing"
P19-1070,E14-1049,0,0.141575,"S and XT . In the general case, we learn two projection matrices WL1 and WL2 : XCL = XL1 WL1 ∪ XL2 WL2 . Many models, however, learn to directly project XL1 to XL2 , i.e., WL2 = I and XCL = XL1 WL1 ∪ XL2 . 2.2 Projection-Based CLE Models While supervised models employ external dictionaries, unsupervised models automatically induce seed translations using diverse strategies: adversarial learning (Conneau et al., 2018a), similaritybased heuristics (Artetxe et al., 2018b), PCA (Hoshen and Wolf, 2018), and optimal transport (Alvarez-Melis and Jaakkola, 2018). Canonical Correlation Analysis (CCA). Faruqui and Dyer (2014) use CCA to project XL1 and XL2 obtain monolingual vectors) and b) they do not require any multilingual corpora, they lend themselves to a wider spectrum of languages than the alternatives (Ruder et al., 2018b). XL1 , XL2 ← monolingual embeddings of L1 and L2 D ← initial word translation dictionary for each of n iterations do XS , XT ← lookups for D in XL1 , XL2 WL1 ← arg minW kXS W − XT k2 WL2 ← arg minW kXT W − XS k2 X0 L1 ← XL1 WL1 ; X0 L2 ← XL2 WL2 D1,2 ← nn(X0 L1 , XL2 ); D2,1 ← nn(X0 L2 , XL1 ) D ← D ∪ (D1,2 ∩ D2,1 ) return: WL1 (and/or WL2 ) into a shared space XCL . Projection matrices"
P19-1070,S18-2010,0,0.0429158,"of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom, 2014) were evaluated on downstream tasks like text classification, a large body of recent work is judged exclusively on the task of bilingual lexicon induction (BLI). This limits our understanding of CLE methodology as: 1) BLI is an intrinsic task, and agreement between BLI and downstream performance has been challenged (Ammar et al., 2016; Bakarov et al., 2018); 2) BLI is not the main motivation for inducing cross-lingual embedding spaces— rather, we seek to exploit CLEs to tackle multilinguality and downstream language transfer (Ruder et al., 2018b). In other words, previous research does not evaluate the true capacity of projectionbased CLE models to support cross-lingual NLP. It is unclear whether and to which extent BLI performance of (projection-based) CLE models correlates with various downstream tasks of different types. At the moment, it is virtually impossible to directly compare all recent projection-based CLE models on BLI due to the lack"
P19-1070,P15-1119,0,0.0573364,"em in a shared cross-lingual word vector space where words with similar meanings obtain similar vectors, irrespective of their language. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from com"
P19-1070,Q17-1010,0,0.431645,"e.g., (in)appropriate evaluation metrics and lack of significance testing. Language Pairs. Our evaluation comprises eight languages: Croatian (HR), English (EN), Finnish (FI), French (FR), German (DE), Italian (IT), Russian (RU), and Turkish (TR). For diversity, we selected two languages from three different IndoEuropean branches: Germanic (EN, DE), Romance (FR, IT), and Slavic (HR, RU); as well as two nonIndo-European languages (FI, TR). From these, we create a total of 28 language pairs for evaluation. Monolingual Embeddings. Following prior work, we use 300-dimensional fastText embeddings (Bojanowski et al., 2017)6 , pretrained on complete Wikipedias of each language. We trim all vocabularies to the 200K most frequent words. Translation Dictionaries. We automatically created translation dictionaries using Google Translate, similar to prior work (Conneau et al., 2018a). We selected the 20K most frequent English words and automatically translated them to the other seven languages. We retained only tuples for which all translations were unigrams found in vocabularies of respective monolingual embedding spaces, leaving us with ≈7K tuples. We reserved 5K tuples created from the more frequent English words f"
P19-1070,P14-1006,0,0.272128,"ks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that r"
P19-1070,D15-1075,0,0.0199549,"txe and Schwenk, 2018; Lample and Conneau, 2019), but rather to provide means to analyze properties and relative performance of diverse CLE models in a downstream language understanding task. Avg 0.561 0.607 0.613 0.615 0.614 0.376 0.390 0.504 0.534 0.543 0.532 0.556 0.357 0.363 0.534 0.568 0.568 0.573 0.536 0.387 0.387 0.544 0.585 0.593 0.599 0.579 0.378 0.399 0.536 0.574 0.579 0.580 0.571 0.374 0.385 0.604 0.611 0.580 0.427* 0.613 0.536 0.510 0.383* 0.534 0.359* 0.400* 0.359* 0.574 0.363* 0.572 0.376* 0.581 0.467 0.516 0.386 Unsupervised Large training corpora for NLI exist only in English (Bowman et al., 2015; Williams et al., 2018). Recently, Conneau et al. (2018b) released a multilingual XNLI corpus created by translating the development and test portions of the MultiNLI corpus (Williams et al., 2018) to 15 other languages. Evaluation Setup. XNLI covers 5 out of 8 languages from our BLI evaluation: EN, DE, FR, RU, and TR. Our setup is straightforward: we train a well-known robust neural NLI model, Enhanced Sequential Inference Model (ESIM; Chen et al., 2017)12 on the large English MultiNLI corpus, using EN word embeddings from a shared EN–L2 (L2 ∈ {DE, FR, RU, TR }) embedding space. We then eval"
P19-1070,P17-1152,0,0.0156855,"0.359* 0.400* 0.359* 0.574 0.363* 0.572 0.376* 0.581 0.467 0.516 0.386 Unsupervised Large training corpora for NLI exist only in English (Bowman et al., 2015; Williams et al., 2018). Recently, Conneau et al. (2018b) released a multilingual XNLI corpus created by translating the development and test portions of the MultiNLI corpus (Williams et al., 2018) to 15 other languages. Evaluation Setup. XNLI covers 5 out of 8 languages from our BLI evaluation: EN, DE, FR, RU, and TR. Our setup is straightforward: we train a well-known robust neural NLI model, Enhanced Sequential Inference Model (ESIM; Chen et al., 2017)12 on the large English MultiNLI corpus, using EN word embeddings from a shared EN–L2 (L2 ∈ {DE, FR, RU, TR }) embedding space. We then evaluate the model on the L2 portion of the XNLI by feeding L2 vectors from the shared space. 13 1K 5K 1K 3K 5K 1K 5K EN – DE EN – FR EN – TR EN – RU V EC M AP M USE ICP GWA Table 3: XNLI performance (test set accuracy). Bold: highest scores, with mutually insignificant differences according to the non-parametric shuffling test (Yeh, 2000). Asterisks denote language pairs for which CLE models could not yield successful runs in the BLI task. are significant dif"
P19-1070,D18-1043,0,0.168294,"upervision (Conneau et al., 2018a; Artetxe et al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While"
P19-1070,D15-1127,0,0.190112,"Missing"
P19-1070,D18-1063,0,0.0729896,"ally attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom, 2014) were evaluated o"
P19-1070,D18-1330,0,0.452523,"I due to the lack of a common evaluation protocol: different papers consider different language pairs and employ different training and evaluation dictionaries. Furthermore, there is a surprising lack of testing of BLI results for statistical significance. The mismatches in evaluation yield partial conclusions and inconsistencies: on the one hand, some unsupervised models (Artetxe et al., 2018b; Hoshen and Wolf, 2018) reportedly outperform competitive supervised CLE models (Artetxe et al., 2017; Smith et al., 2017). On the other hand, the most recent supervised approaches (Doval et al., 2018; Joulin et al., 2018) report performances surpassing the best unsupervised models. is easily obtainable for most language pairs.2 Therefore, despite the attractive zero-supervision setup, we see unsupervised CLE models practically justified only if such models can, unintuitively, indeed outperform their supervised competition. Contributions. We provide a comprehensive comparative evaluation of a wide range of stateof-the-art—both supervised and unsupervised— projection-based CLE models. Our benchmark encompasses BLI and three cross-lingual (CL) downstream tasks of different nature: document classification (CLDC),"
P19-1070,D18-1047,0,0.174622,"akly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom, 2014) were evaluated on downstream task"
P19-1070,D18-1101,0,0.0224836,"t al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 20"
P19-1070,C12-1089,0,0.765619,"embeddings (CLEs). CLE models learn vectors of words in two or more languages and represent them in a shared cross-lingual word vector space where words with similar meanings obtain similar vectors, irrespective of their language. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 ,"
P19-1070,D18-1549,0,0.0266683,"anguage. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): t"
P19-1070,P15-1027,0,0.0610819,"d bipartite weighted graph G = (E, VL1 ∪ VL2 ) with edges E = VL1 × VL2 . By drawing embeddings from a Gaussian distribution and normalizing them, the weight of each edge (i, j) ∈ E is shown to correspond to the cosine similarity between vectors. In the E-step, a maximal bipartite matching is found on the sparsified graph using the Jonker-Volgenant algorithm (Jonker and Volgenant, 1987). In the M-step, a better projection WL1 is learned by solving the Procrustes problem. Ranking-Based Optimization (RCSLS). Instead of minimizing the Euclidean distance, Joulin et al. (2018) follow earlier work (Lazaridou et al., 2015) and maximize a ranking-based objective, specifically cross-domain similarity local scaling (CSLS; Conneau et al., 2018a), between the XS WL1 and XT . CSLS is an extension of cosine similarity commonly used for BLI inference. Let r(xkL1 W, XL2 ) be the average cosine similarity of the projected source vector with its N nearest neighbors from XL2 . Inversely, let r(xkL2 , XL1 W) be the average cosine similarity of a target vector with its N nearest neighbors from the projected source space XL1 W. By relaxing the orthogonality constraint on WL1 , maximization of relaxed CSLS (dubbed RCSLS) becom"
P19-1070,E17-1072,0,0.0207304,"ual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections can be bootstrapped from small dictionaries of 50–100 pairs (Vuli´c and Korhonen, 2016; Zhan"
P19-1070,W15-1521,0,0.103779,"nguages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections"
P19-1070,D17-1269,0,0.0403092,"Missing"
P19-1070,D18-1042,1,0.877682,"c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections can be bootstrapped from small dictionaries of 50–100 pairs (Vuli´c and Korhonen, 2016; Zhang et al., 2016), identical strings and cognates (Smith et al., 2017; Søgaard et al., 2018), and shared numerals (Artetxe et al., 2017). Moreover, recent work has leveraged topological similarities between monolingual vector spaces to introduce fully unsupervised projection-based CLE models,"
P19-1070,P15-1165,0,0.135665,"Missing"
P19-1070,P18-1072,1,0.840679,"Missing"
P19-1070,P16-1157,0,0.0339789,"Procrustes model (P ROC -B, see §2.2) and show it is competitive across the board. We find that overfitting to BLI may severely hurt downstream performance, warranting the coupling of BLI experiments with downstream evaluations in order to paint a more informative picture of CLE models’ properties. 2 Projection-Based CLEs: Methodology In contrast to more recent unsupervised models, CLE models typically require bilingual signal: aligned words, sentences, or documents. CLE models based on sentence and document alignments have been extensively studied in previous work (Vuli´c and Korhonen, 2016; Upadhyay et al., 2016; Ruder et al., 2018b). Current CLE research is almost exclusively focused on projection-based CLE models; they are thus also the focus of our study.3 Supervised projection-based CLEs require merely small-sized translation dictionaries (up to a few thousand word pairs) and such bilingual signal 711 2 We argue that, if acquiring a few thousand word translation pairs is a challenge, one probably deals with a truly underresourced language for which it would be difficult to obtain reliable monolingual embeddings in the first place. Furthermore, there are initiatives in typological linguistics rese"
P19-1070,P16-1024,1,0.936437,"Missing"
P19-1070,N18-1101,0,0.0184084,"; Lample and Conneau, 2019), but rather to provide means to analyze properties and relative performance of diverse CLE models in a downstream language understanding task. Avg 0.561 0.607 0.613 0.615 0.614 0.376 0.390 0.504 0.534 0.543 0.532 0.556 0.357 0.363 0.534 0.568 0.568 0.573 0.536 0.387 0.387 0.544 0.585 0.593 0.599 0.579 0.378 0.399 0.536 0.574 0.579 0.580 0.571 0.374 0.385 0.604 0.611 0.580 0.427* 0.613 0.536 0.510 0.383* 0.534 0.359* 0.400* 0.359* 0.574 0.363* 0.572 0.376* 0.581 0.467 0.516 0.386 Unsupervised Large training corpora for NLI exist only in English (Bowman et al., 2015; Williams et al., 2018). Recently, Conneau et al. (2018b) released a multilingual XNLI corpus created by translating the development and test portions of the MultiNLI corpus (Williams et al., 2018) to 15 other languages. Evaluation Setup. XNLI covers 5 out of 8 languages from our BLI evaluation: EN, DE, FR, RU, and TR. Our setup is straightforward: we train a well-known robust neural NLI model, Enhanced Sequential Inference Model (ESIM; Chen et al., 2017)12 on the large English MultiNLI corpus, using EN word embeddings from a shared EN–L2 (L2 ∈ {DE, FR, RU, TR }) embedding space. We then evaluate the model on the L2"
P19-1070,N15-1104,0,0.527117,"Missing"
P19-1070,D18-1268,0,0.0814185,"nd unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom, 2014) were evaluated on downstream tasks like text class"
P19-1070,C00-2137,0,0.327675,"etup is straightforward: we train a well-known robust neural NLI model, Enhanced Sequential Inference Model (ESIM; Chen et al., 2017)12 on the large English MultiNLI corpus, using EN word embeddings from a shared EN–L2 (L2 ∈ {DE, FR, RU, TR }) embedding space. We then evaluate the model on the L2 portion of the XNLI by feeding L2 vectors from the shared space. 13 1K 5K 1K 3K 5K 1K 5K EN – DE EN – FR EN – TR EN – RU V EC M AP M USE ICP GWA Table 3: XNLI performance (test set accuracy). Bold: highest scores, with mutually insignificant differences according to the non-parametric shuffling test (Yeh, 2000). Asterisks denote language pairs for which CLE models could not yield successful runs in the BLI task. are significant differences between BLI and XNLI performance across language pairs—while we observe much better BLI performance for EN–DE and EN– FR compared to EN – RU and especially EN – TR , XNLI performance of most models for EN–RU and EN – TR surpasses that for EN – FR and is close to that for EN–DE. While this can be an artifact of the XNLI dataset creation, we support these observations for invidivual language pairs by measuring an overall Spearman correlation of only 0.13 between BLI"
P19-1070,N16-1156,0,0.132294,"or space where words with similar meanings obtain similar vectors, irrespective of their language. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a con"
P19-1070,D13-1141,0,0.0623751,"downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it"
P19-1070,P17-1179,0,\N,Missing
P19-1476,P18-1073,0,0.206711,"the entire distributional space. The difference between LE-retrofitting and GLEN is illustrated in Figure 1. Moreover, with GLEN’s ability to LE-specialize unseen words we can seamlessly LE-specialize word vectors of another language (L2), assuming we previously project them to the distributional space of L1 for which we had learned the specialization function. To this end, we can leverage any from the plethora of resource-lean methods for learning the cross-lingual projection (function g in Figure 1) between monolingual distributional vector spaces (Smith et al., 2017; Conneau et al., 2018; Artetxe et al., 2018, inter alia).1 Conceptually, GLEN is similar to the explicit retrofitting model of Glavaˇs and Vuli´c (2018), who focus on the symmetric semantic similarity relation. In contrast, GLEN has to account for the asymmetric nature of the LE relation. Besides joint (Nguyen et al., 2017) and retrofitting (Vuli´c and Mrkˇsi´c, 2018) models for LE, there is a number of supervised LE detection models that employ distributional vectors as input features (Tuan et al., 2016; Shwartz et al., 2016; Glavaˇs and Ponzetto, 1 See (Ruder et al., 2018b; Glavaˇs et al., 2019) for a comprehensive overview of models"
P19-1476,I13-1095,0,0.0853898,"Lexical entailment (LE; hyponymy-hypernymy or is-a relation), is a fundamental asymmetric lexicosemantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a key building block of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding s"
P19-1476,Q17-1010,0,0.264838,"ck of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding specialization methods remedy for the semantic vagueness of distributional spaces, forcing the vectors to conform to external linguistic constraints (e.g., synonymy or LE word pairs) in ord"
P19-1476,D15-1075,0,0.106085,"Missing"
P19-1476,N15-1184,0,0.143539,"Missing"
P19-1476,P05-1014,0,0.45601,"Missing"
P19-1476,P18-1004,1,0.899409,"Missing"
P19-1476,P19-1070,1,0.901302,"Missing"
P19-1476,D17-1185,1,0.912081,"Missing"
P19-1476,D18-1330,0,0.0607001,"Missing"
P19-1476,D15-1242,0,0.112892,"Missing"
P19-1476,P14-2050,0,0.0283665,"ollins and Quillian, 1972; Beckwith et al., 1991) and a key building block of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding specialization methods remedy for the semantic vagueness of distributional spaces, forcing the vectors to conform to"
P19-1476,N16-1118,0,0.0424006,"and a key building block of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding specialization methods remedy for the semantic vagueness of distributional spaces, forcing the vectors to conform to external linguistic constraints (e.g., synonymy"
P19-1476,W13-0904,0,0.0278264,"l LE detection. 1 Background and Motivation Lexical entailment (LE; hyponymy-hypernymy or is-a relation), is a fundamental asymmetric lexicosemantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a key building block of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be d"
P19-1476,Q17-1022,1,0.929649,"Missing"
P19-1476,P16-2074,0,0.0450297,"ng model, e.g., Skip-Gram (Mikolov et al., 2013), by modifying the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or by augmenting the objective with additional factors reflecting linguistic constraints (Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Joint models are tightly coupled to a concrete embedding model – any modification to the underlying embedding models warrants a modification of the whole joint model, along with the expensive retraining. Conversely, retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018, inter alia) change the distributional spaces post-hoc, by fine-tuning word vectors so that they conform to external linguistic constraints. Advantageously, this makes retrofitting models more flexible, as they can be applied to any pre-trained distributional space. On the downside, retrofitting models specialize only the vectors of words seen in constraints, leaving vectors of unseen words unchanged. In this work, we propose an LE-specialization framework that combines the strengths of both 4824 Proceedings of the 57th Annual Meeting of the A"
P19-1476,N15-1100,0,0.462736,") and diminish the contributions of other types of semantic association. Lexical specialization models generally belong to one of the two families: (1) joint optimization models and (2) retrofitting (also known as fine-tuning or post-processing) models. Joint models incorporate linguistic constraints directly into the objective of an embedding model, e.g., Skip-Gram (Mikolov et al., 2013), by modifying the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or by augmenting the objective with additional factors reflecting linguistic constraints (Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Joint models are tightly coupled to a concrete embedding model – any modification to the underlying embedding models warrants a modification of the whole joint model, along with the expensive retraining. Conversely, retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018, inter alia) change the distributional spaces post-hoc, by fine-tuning word vectors so that they conform to external linguistic constraints. Advantageously, this makes retrofitting models more flexible,"
P19-1476,Q16-1030,0,0.0372214,"contributions of other types of semantic association. Lexical specialization models generally belong to one of the two families: (1) joint optimization models and (2) retrofitting (also known as fine-tuning or post-processing) models. Joint models incorporate linguistic constraints directly into the objective of an embedding model, e.g., Skip-Gram (Mikolov et al., 2013), by modifying the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or by augmenting the objective with additional factors reflecting linguistic constraints (Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Joint models are tightly coupled to a concrete embedding model – any modification to the underlying embedding models warrants a modification of the whole joint model, along with the expensive retraining. Conversely, retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018, inter alia) change the distributional spaces post-hoc, by fine-tuning word vectors so that they conform to external linguistic constraints. Advantageously, this makes retrofitting models more flexible, as they can be applied"
P19-1476,D14-1162,0,0.0874129,"; Beckwith et al., 1991) and a key building block of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding specialization methods remedy for the semantic vagueness of distributional spaces, forcing the vectors to conform to external linguistic const"
P19-1476,N18-1202,0,0.0363363,"works and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding specialization methods remedy for the semantic vagueness of distributional spaces, forcing the vectors to conform to external linguistic constraints (e.g., synonymy or LE word pairs) in order to emphasize the l"
P19-1476,D18-1026,1,0.895667,"Missing"
P19-1476,P18-2101,1,0.873555,"Missing"
P19-1476,D18-1042,0,0.111928,"nal vector spaces (Smith et al., 2017; Conneau et al., 2018; Artetxe et al., 2018, inter alia).1 Conceptually, GLEN is similar to the explicit retrofitting model of Glavaˇs and Vuli´c (2018), who focus on the symmetric semantic similarity relation. In contrast, GLEN has to account for the asymmetric nature of the LE relation. Besides joint (Nguyen et al., 2017) and retrofitting (Vuli´c and Mrkˇsi´c, 2018) models for LE, there is a number of supervised LE detection models that employ distributional vectors as input features (Tuan et al., 2016; Shwartz et al., 2016; Glavaˇs and Ponzetto, 1 See (Ruder et al., 2018b; Glavaˇs et al., 2019) for a comprehensive overview of models for inducing cross-lingual word embedding spaces. Generalized Lexical Entailment Following LEAR (Vuli´c and Mrkˇsi´c, 2018), the state-of-the-art LE-retrofitting model, we use three types of linguistic constraints to learn the general specialization f : synonyms, antonyms, and LE (i.e., hyponym-hypernym) pairs. Similarityfocused specialization models tune only the direction of distributional vectors (Mrkˇsi´c et al., 2017; Glavaˇs and Vuli´c, 2018; Ponti et al., 2018). In LEspecialization we need to emphasize similarities but also"
P19-1476,K15-1026,0,0.0480629,"ference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding specialization methods remedy for the semantic vagueness of distributional spaces, forcing the vectors to conform to external linguistic constraints (e.g., synonymy or LE word pairs) in order to emphasize the lexico-semantic relation of interest (e.g., semantic similarity of LE) and diminish the contributions of other types of semantic association. Lexical specialization models generally belong to one of the two families: (1) joint optimization models and ("
P19-1476,P16-1226,0,0.182048,"Missing"
P19-1476,P06-1101,0,0.230473,"Missing"
P19-1476,D16-1039,0,0.0194116,"al projection (function g in Figure 1) between monolingual distributional vector spaces (Smith et al., 2017; Conneau et al., 2018; Artetxe et al., 2018, inter alia).1 Conceptually, GLEN is similar to the explicit retrofitting model of Glavaˇs and Vuli´c (2018), who focus on the symmetric semantic similarity relation. In contrast, GLEN has to account for the asymmetric nature of the LE relation. Besides joint (Nguyen et al., 2017) and retrofitting (Vuli´c and Mrkˇsi´c, 2018) models for LE, there is a number of supervised LE detection models that employ distributional vectors as input features (Tuan et al., 2016; Shwartz et al., 2016; Glavaˇs and Ponzetto, 1 See (Ruder et al., 2018b; Glavaˇs et al., 2019) for a comprehensive overview of models for inducing cross-lingual word embedding spaces. Generalized Lexical Entailment Following LEAR (Vuli´c and Mrkˇsi´c, 2018), the state-of-the-art LE-retrofitting model, we use three types of linguistic constraints to learn the general specialization f : synonyms, antonyms, and LE (i.e., hyponym-hypernym) pairs. Similarityfocused specialization models tune only the direction of distributional vectors (Mrkˇsi´c et al., 2017; Glavaˇs and Vuli´c, 2018; Ponti et al."
P19-1476,N18-1056,0,0.226603,"Missing"
P19-1476,E17-2065,1,0.902352,"Missing"
P19-1476,J17-4004,1,0.871316,"Missing"
P19-1476,N18-1103,1,0.837789,"Missing"
P19-1476,P19-1490,1,0.770987,"Missing"
P19-1476,N16-1142,0,0.253326,"word pairs) and test portions (900-1000 word pairs): we use the train portions to tune the threshold t that binarizes GLEN’s predictions ILE . We induce the CL embeddings (i.e., learn the projections Wg , see Section §2) by projecting AR, FR, and RU embeddings to the EN space in a supervised fashion, by finding the optimal solution to the Procrustes problem for given 5K word translation pairs (for each language pair). 6 We compare GLEN with more complex models from (Upadhyay et al., 2018): they couple two methods for inducing syntactic CL embeddings – CL-D EP (Vuli´c, 2017) and B I -S PARSE (Vyas and Carpuat, 2016) – with 6 We automatically translated 5K most frequent EN words to AR, FR, and RU with Google Translate. Model EN-FR EN-RU EN-AR Avg HYPO CL-D EP B I -S PARSE GLEN .538 .566 .792 .602 .590 .811 .567 .526 .816 .569 .561 .806 COHYP CL-D EP B I -S PARSE GLEN .610 .667 .779 .562 .636 .849 .631 .668 .821 .601 .657 .816 Table 2: CL LE detection results (accuracy) on CL datasets (HYPO, COHYP) (Upadhyay et al., 2018). an LE scorer based on the distributional inclusion hypothesis (Geffet and Dagan, 2005). Results. GLEN’s cross-lingual LE detection performance is shown in Table 2. GLEN dramatically outp"
P19-1476,Q15-1025,0,0.0429115,"bjective of an embedding model, e.g., Skip-Gram (Mikolov et al., 2013), by modifying the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or by augmenting the objective with additional factors reflecting linguistic constraints (Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Joint models are tightly coupled to a concrete embedding model – any modification to the underlying embedding models warrants a modification of the whole joint model, along with the expensive retraining. Conversely, retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018, inter alia) change the distributional spaces post-hoc, by fine-tuning word vectors so that they conform to external linguistic constraints. Advantageously, this makes retrofitting models more flexible, as they can be applied to any pre-trained distributional space. On the downside, retrofitting models specialize only the vectors of words seen in constraints, leaving vectors of unseen words unchanged. In this work, we propose an LE-specialization framework that combines the strengths of both 4824 Proceedings of the 57th An"
P19-1476,N18-1101,0,0.0281051,"curacy) over state-ofthe-art in cross-lingual LE detection. 1 Background and Motivation Lexical entailment (LE; hyponymy-hypernymy or is-a relation), is a fundamental asymmetric lexicosemantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a key building block of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributio"
P19-1476,C00-2137,0,0.118832,"Missing"
P19-1476,P14-2089,0,0.038428,"uistic constraints (e.g., synonymy or LE word pairs) in order to emphasize the lexico-semantic relation of interest (e.g., semantic similarity of LE) and diminish the contributions of other types of semantic association. Lexical specialization models generally belong to one of the two families: (1) joint optimization models and (2) retrofitting (also known as fine-tuning or post-processing) models. Joint models incorporate linguistic constraints directly into the objective of an embedding model, e.g., Skip-Gram (Mikolov et al., 2013), by modifying the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or by augmenting the objective with additional factors reflecting linguistic constraints (Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Joint models are tightly coupled to a concrete embedding model – any modification to the underlying embedding models warrants a modification of the whole joint model, along with the expensive retraining. Conversely, retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018, inter alia) change the distributional spaces post-hoc, by"
P19-1476,D14-1161,0,0.257923,"(Smith et al., 2017) based on (closed-form) solution of the Procrustes problem (Sch¨onemann, 1966). Let XS ⊂ XL2 and XT ⊂ XL1 be the subsets of the two monolingual embedding spaces, containing (row-aligned) vectors of word translations. We then obtain the projection matrix as Wg = UV&gt; , where UΣV&gt; is the singular value decomposition of the product matrix XT XS &gt; . 3 Evaluation Experimental Setup. We work with Wikipediatrained FAST T EXT embeddings (Bojanowski et al., 2017). We take English constraints from previous work – synonyms and antonyms were created from WordNet and Roget’s Thesaurus (Zhang et al., 2014; Ono et al., 2015); LE constraints were collected from WordNet by Vuli´c and Mrkˇsi´c (2018) and contain both direct and transitively obtained LE pairs. We retain the constraints for which both words exist in the trimmed (200K) FAST T EXT vocabulary, resulting in a total of 1,493,686 LE, 521,037 synonym, and 141,311 antonym pairs. We reserve 4,000 constraints (E: 2k, S: 1k, A: 1k) for validation and use the rest for training. We identify the following best hyperparameter configuration via grid search: H = 5, dh = 300, ψ = tanh, δa = 1, δs = δd = 0.5, λa = 2, and λr = 1. 3 For a comprehensive"
P19-1490,E17-1088,0,0.0390417,"t al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers support also to low-resource transfer settings.9 Training Setup. CLEAR hyperparameters are adopted from the original Attract-Repel work (Mrkši´c et al., 2017): δatt = 0.6, δrep = 0.0, λreg = λD = 10−9 . All batches are of size 128 (see Eq. (4)), and the model is trained for 5 epochs with Adagrad (Duchi et al., 2011). Baseline Models. In monolingual evaluation, we compare CLEAR to original non-specialised distributional vectors in each language. Another instructive baseline is the"
P19-1490,W13-3520,0,0.0279406,"alisation performed by CLEAR, and analyse its performance in comparison with distributional word vectors and non-specialised cross-lingual word embeddings. 4.1 Experimental Setup Distributional Vectors. Graded LE is evaluated on EN, DE, IT, and HR (see §2); we also evaluate CLEAR on ungraded cross-lingual LE (Upadhyay et al., 2018) for the following language pairs: ENFR (French); EN - RU (Russian); EN - AR (Arabic). All results are reported with English Skip-Gram with Negative Sampling (SGNS - BOW 2) vectors (Mikolov et al., 2013) trained by Levy and Goldberg (2014) on the Polyglot Wikipedia (Al-Rfou et al., 2013) with bag-of-words context (window size of 2).7 Input vectors for other languages come from various sources: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dictionaries. We use the same set of monolingual const"
P19-1490,P18-1073,0,0.226842,"pecialisation of the English distributional space, and then 2) translates all test examples in the target language to English relying on the bilingual dictionary D.10 All LE reasoning is then conducted monolingually in English. The TRANS baseline is also used in cross-lingual graded LE evaluation. For cross-lingual datasets without English (e.g., DE-IT), we again translate all words to English and use the English specialised space for graded LE assertions. In addition, for each language pair we also report results of two stateof-the-art cross-lingual word embedding models (Smith et al., 2017; Artetxe et al., 2018), showing the better scoring one in each run (XEMB). For ungraded LE evaluation, in addition to TRANS, we compare CLEAR to two bestperforming baselines from (Upadhyay et al., 2018): they couple two methods for inducing syntactic cross-lingual vectors: 1) B I -S PARSE (Vyas and Carpuat, 2016) and 2) CL-D EP (Vuli´c, 2017) with an LE scorer based on the distributional inclusion hypothesis (Geffet and Dagan, 2005). For more details we refer the reader to (Upadhyay et al., 2018). 9 The translations in PanLex were derived from various sources (e.g., glossaries, dictionaries, automatic inference). T"
P19-1490,Q17-1010,0,0.0292145,"my constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior"
P19-1490,P13-1133,0,0.119829,"Missing"
P19-1490,P05-1014,0,0.443497,"and category vagueness from cognitive science (Rosch, 1973, 1975; Kamp and Partee, 1995). Instead of answering the simpler (discrete) question “Is X a type of Y?”, as in standard LE detection tasks (Kotlerman et al., 2010; Turney and Mohammad, 2015), GR - LE aims at answering the following question: “To what degree is X a type of Y?” The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Co"
P19-1490,S17-2002,0,0.301956,"peakers.5 Cross-Lingual Datasets. The cross-lingual CL HYPERLEX datasets were then constructed automatically, leveraging word pair translations and scores in three target languages. To this end, we follow the methodology of Camacho-Collados et al. (2015, 2017), used previously for creating cross-lingual semantic similarity datasets. In short, we first intersect aligned concept pairs (obtained through translation) in two languages: e.g., father-ancestor in English and padre-antenato in Italian are used 5 As opposed to (Hill et al., 2015; Gerz et al., 2016; Vuli´c et al., 2017), but similar to (Camacho-Collados et al., 2017; Pilehvar et al., 2018) we did not divide the dataset into smaller tranches; each annotator scored the entire target-language dataset instead. The target languages were selected based on the availability of native speakers; the total number of annotations was restricted by the annotation budget. 4965 Monolingual Datasets 50 picture Person Fahrrad cibo rekreacija 5.90 4.0 0.25 3.25 5.75 Cross-Lingual Datasets (CL - HYPERLEX) EN - DE EN - IT EN - HR DE - IT DE - HR IT- HR dinosaur eye religija Medikation Form aritmetica Kreatur viso belief trattamento prizma matematika EN DE IT HR 40 Percentage"
P19-1490,D16-1235,1,0.922237,"Missing"
P19-1490,P15-2001,0,0.450197,"al lexical substitution (Mihalcea et al., 2010). In this work, we introduce the first set of benchmarks and methods that target cross-lingual and multilingual graded lexical entailment. We make several important contributions related to GR - LE in multilingual settings. First, we extend the research on GR - LE beyond English (Vuli´c et al., 2017; Rei et al., 2018) and provide new human-annotated GR - LE datasets in three other languages: German, Italian, and Croatian. Second, following an established methodology for constructing evaluation datasets for cross-lingual lexico-semantic relations (Camacho-Collados et al., 2015, 2017), we automatically derive a collection of six cross-lingual GR - LE datasets: CL - HYPERLEX. We analyse in detail the cross-lingual datasets (e.g., by comparing the scores to human-elicited ratings), demonstrating their robustness and reliability. In order to provide a competitive baseline on new monolingual and cross-lingual datasets, we next introduce a cross-lingual specialisation/retrofitting method termed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel): starting from any two monolingual distributional spaces, CLEAR induces a bilingual cross-lingual space that reflects the as"
P19-1490,D18-1269,0,0.159875,"05; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In contrast, the pair food → pie receives the average rating of only 0.92/6, which confirms the inherent asymmetry of the GR - LE relation. 2 For instance, translating the Italian word calcio to calcium"
P19-1490,D16-1136,0,0.0207865,"embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers support also to low-resource transfer settings.9 Training Setup. CLEAR hyperparameters are adopted from the original Attract-Repel work (Mrkši´c et al., 2017): δatt = 0.6, δrep = 0.0, λreg = λD = 10−9 . All batches are of size 128 (see Eq. (4)), and the model is trained for 5 epochs with Adagrad (Duchi et al., 2011). Baseline Models. In monolingual evaluation, we compare CLEAR to original non-specialised distributional vectors in each language. Another instruc"
P19-1490,ehrmann-etal-2014-representing,0,0.625,"5; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In contrast, the pair food → pie receives the average rating of only 0.92/6, which confirms the inherent asymmetry of the GR - LE relation. 2 For instance, translating the Italian word calcio to calcium prevents identifying sport as a hypernym of calcio. 4963 Proc"
P19-1490,P14-1113,0,0.103701,"6, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In contrast, the pair food → pie receives the average rating of only 0.92/6, which confirms the inherent asymmetry of the GR - LE relation. 2 For instance, translating the Italian word calcio to calcium prevents identifying sport as a hypernym of calcio. 4963 Proceedings of the 57t"
P19-1490,P18-1004,1,0.894858,"Missing"
P19-1490,P19-1070,1,0.857159,"Missing"
P19-1490,D17-1185,1,0.786418,"Missing"
P19-1490,P19-1476,1,0.771144,"Missing"
P19-1490,L18-1550,0,0.0176646,"ors. Graded LE is evaluated on EN, DE, IT, and HR (see §2); we also evaluate CLEAR on ungraded cross-lingual LE (Upadhyay et al., 2018) for the following language pairs: ENFR (French); EN - RU (Russian); EN - AR (Arabic). All results are reported with English Skip-Gram with Negative Sampling (SGNS - BOW 2) vectors (Mikolov et al., 2013) trained by Levy and Goldberg (2014) on the Polyglot Wikipedia (Al-Rfou et al., 2013) with bag-of-words context (window size of 2).7 Input vectors for other languages come from various sources: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dictionaries. We use the same set of monolingual constraints as LEAR (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in othe"
P19-1490,J15-4004,0,0.693533,"ation of concepts (i.e., it is not tied to a particular language), a graded LE repository has so far been created only for English: it is the HyperLex dataset of Vuli´c et al. (2017). Starting from the established data creation protocol for HyperLex, in this work we compile similar HyperLex datasets in three other languages and introduce novel multilingual and cross-lingual GR - LE tasks. Graded LE in English. HyperLex (Vuli´c et al., 2017) comprises 2,616 English (EN) word pairs (2,163 noun pairs and 453 verb pairs) annotated for the GR - LE relation. Unlike in symmetric similarity datasets (Hill et al., 2015; Gerz et al., 2016), word order in each pair (X, Y ) is important: this means that pairs (X, Y ) and (Y, X) can obtain drastically different graded LE ratings. The word pairs were first sampled from WordNet to represent a spectrum of different word relations (e.g., hyponymyhypernymy, meronymy, co-hyponymy, synonymy, antonymy, no relation). The ratings in the [0, 6] interval were then collected through crowdsourcing by posing the GR - LE “To what degree...” question to human subjects, with each pair rated by at least 10 raters: the score of 6 indicates strong LE relation between the concepts X"
P19-1490,W19-4310,1,0.844555,"Missing"
P19-1490,kamholz-etal-2014-panlex,0,0.0402875,"glish such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers support also to low-resource transfer settings.9 Training Setup. CLEAR hyperparameters are adopted from the original Attract-Repel work (Mrkši´c et al., 2017): δatt = 0.6, δrep = 0.0, λreg = λD = 10−9 . All batches are of size 128 (see Eq. (4)), and the model is trained for 5 epochs with Adagrad (Duchi et al., 2011). Baseline Models. In monolingual evaluation, we compare CLEAR t"
P19-1490,P15-2020,1,0.932684,"Missing"
P19-1490,P14-2050,0,0.0537507,"lyse the usefulness of cross-lingual graded LE specialisation performed by CLEAR, and analyse its performance in comparison with distributional word vectors and non-specialised cross-lingual word embeddings. 4.1 Experimental Setup Distributional Vectors. Graded LE is evaluated on EN, DE, IT, and HR (see §2); we also evaluate CLEAR on ungraded cross-lingual LE (Upadhyay et al., 2018) for the following language pairs: ENFR (French); EN - RU (Russian); EN - AR (Arabic). All results are reported with English Skip-Gram with Negative Sampling (SGNS - BOW 2) vectors (Mikolov et al., 2013) trained by Levy and Goldberg (2014) on the Polyglot Wikipedia (Al-Rfou et al., 2013) with bag-of-words context (window size of 2).7 Input vectors for other languages come from various sources: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dicti"
P19-1490,W14-0405,0,0.0262033,"Missing"
P19-1490,S13-2005,0,0.197578,"Missing"
P19-1490,N15-1100,0,0.0526911,"rces: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dictionaries. We use the same set of monolingual constraints as LEAR (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based E"
P19-1490,P09-1034,0,0.0344902,"actual language (the example shows English and Spanish words with the respective prefixes en_ and es_) is reflected by their small co−−−−−−→ sine distances (e.g., the small angle between en_beagle − − − − − − − → −−−−−→ and en_animal), while simultaneously and − es_perro higher-level concepts are assigned larger norms to enforce the LE arrangement in the vector space. An asymmetric distance that takes into account the vector direction as well as the vector magnitude can be used to grade the LE relation strength between any two concepts in the shared cross-lingual vector space. terpretability (Padó et al., 2009), and cross-lingual lexical substitution (Mihalcea et al., 2010). In this work, we introduce the first set of benchmarks and methods that target cross-lingual and multilingual graded lexical entailment. We make several important contributions related to GR - LE in multilingual settings. First, we extend the research on GR - LE beyond English (Vuli´c et al., 2017; Rei et al., 2018) and provide new human-annotated GR - LE datasets in three other languages: German, Italian, and Croatian. Second, following an established methodology for constructing evaluation datasets for cross-lingual lexico-sem"
P19-1490,D14-1162,0,0.0828019,"n LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex currently spans around 1,300 langua"
P19-1490,N16-1118,0,0.0352914,"Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex"
P19-1490,N18-1202,0,0.0212284,"rom WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al."
P19-1490,S10-1002,0,0.0931379,"with the respective prefixes en_ and es_) is reflected by their small co−−−−−−→ sine distances (e.g., the small angle between en_beagle − − − − − − − → −−−−−→ and en_animal), while simultaneously and − es_perro higher-level concepts are assigned larger norms to enforce the LE arrangement in the vector space. An asymmetric distance that takes into account the vector direction as well as the vector magnitude can be used to grade the LE relation strength between any two concepts in the shared cross-lingual vector space. terpretability (Padó et al., 2009), and cross-lingual lexical substitution (Mihalcea et al., 2010). In this work, we introduce the first set of benchmarks and methods that target cross-lingual and multilingual graded lexical entailment. We make several important contributions related to GR - LE in multilingual settings. First, we extend the research on GR - LE beyond English (Vuli´c et al., 2017; Rei et al., 2018) and provide new human-annotated GR - LE datasets in three other languages: German, Italian, and Croatian. Second, following an established methodology for constructing evaluation datasets for cross-lingual lexico-semantic relations (Camacho-Collados et al., 2015, 2017), we automa"
P19-1490,D18-1169,0,0.0500479,"E), 0.909 (EN - IT), and 0.905 (EN - HR). LE 3 Methodology In order to provide benchmarking graded LE scores on new monolingual and cross-lingual evaluation sets, we now introduce a novel method that can capture GR - LE cross-lingually. CLEAR ( CrossLingual Lexical Entailment Attract-Repel) is a cross-lingual extension of the monolingual LEAR specialisation method (Vuli´c and Mrkši´c, 2018), a state-of-the-art vector space fine-tuning method which specialises any input distributional vector 6 Similarity benchmarks report much lower Pairwise-IAA scores: 0.61 on SimVerb-3500 (Gerz et al., 2016; Pilehvar et al., 2018), and 0.67 on SimLex-999 (Hill et al., 2015) and on WordSim-353 (Finkelstein et al., 2002) CLEAR Specialisation. A high-level overview of the CLEAR specialisation method is provided in Figure 3. The input to the method is as follows: 1) two independently trained monolingual word vector spaces in two languages L1 and L2 ; 2) sets of external lexical constraints in the resource-rich language L1 (e.g., English) extracted from an external lexical resource such as WordNet (Fellbaum, 1998) or BabelNet (Ehrmann et al., 2014); and 3) a bilingual L1 -L2 dictionary D. The goal is to fine-tune input word"
P19-1490,P17-1163,0,0.0216099,"Missing"
P19-1490,Q17-1022,1,0.924323,"Missing"
P19-1490,S12-1053,0,0.0762876,"Missing"
P19-1490,D18-1026,1,0.864529,"Missing"
P19-1490,P18-2101,1,0.869814,"Missing"
P19-1490,P18-2057,0,0.13905,"ion “Is X a type of Y?”, as in standard LE detection tasks (Kotlerman et al., 2010; Turney and Mohammad, 2015), GR - LE aims at answering the following question: “To what degree is X a type of Y?” The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song"
P19-1490,E14-4008,0,0.0904314,"ce (Rosch, 1973, 1975; Kamp and Partee, 1995). Instead of answering the simpler (discrete) question “Is X a type of Y?”, as in standard LE detection tasks (Kotlerman et al., 2010; Turney and Mohammad, 2015), GR - LE aims at answering the following question: “To what degree is X a type of Y?” The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multil"
P19-1490,N16-1142,0,0.265208,"uman judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rat"
P19-1490,P16-1226,0,0.17476,"Missing"
P19-1490,C14-1212,0,0.0844206,"from cognitive science (Rosch, 1973, 1975; Kamp and Partee, 1995). Instead of answering the simpler (discrete) question “Is X a type of Y?”, as in standard LE detection tasks (Kotlerman et al., 2010; Turney and Mohammad, 2015), GR - LE aims at answering the following question: “To what degree is X a type of Y?” The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b)"
P19-1490,E17-1007,0,0.171718,"Missing"
P19-1490,D14-1161,0,0.062278,"ome from various sources: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dictionaries. We use the same set of monolingual constraints as LEAR (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al.,"
P19-1490,P14-1068,0,0.0674999,"Missing"
P19-1490,L18-1558,0,0.0166563,"2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In contrast, the pair food → pie receives the average rating of only 0.92/6, which confirms the inherent asymmetry of the GR - LE relation. 2 For instance, translating the Italian word calcio to calcium prevents identifying sport as a hypernym of calcio. 4963 Proceedings of the 57th Annual Meeting of the Association for Computational"
P19-1490,N18-1056,0,0.255581,"for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In cont"
P19-1490,E17-2065,1,0.908149,"Missing"
P19-1490,J17-4004,1,0.784862,"Missing"
P19-1490,N18-1103,1,0.896402,"Missing"
P19-1490,D17-1270,1,0.902928,"Missing"
P19-4004,D13-1191,0,0.0252605,"litical text scaling – one of the central tasks in quantitative political science, where the goal is to quantify positions of politicians and/or parties on a scale based on the textual content they produce – has not received any attention by the NLP community until last year, whereas it has been at the core of political science research for almost two decades. At the same time, NLP researchers have addressed closely related tasks such as election prediction (O’Connor et al., 2010), ideology classification (Hirst et al., 2010), stance detection (Thomas et al., 2006), and agreement measurement (Gottipati et al., 2013), all rarely considered in the same format by the textas-data political science community. In summary, these two communities have been largely agnostic of one another, resulting in NLP researchers not contributing to relevant research questions in political science and political scientists not employing cutting-edge NLP methodology for their tasks. The development and adoption of natural language processing (NLP) methods by the political science community dates back to over twenty years ago. In the last decade the usage of computational methods for text analysis has drastically expanded in sco"
P19-4004,W16-2102,1,0.888838,"Missing"
P19-4004,N18-1178,0,0.0198472,"ial Overview 3. Topical Analysis of Political Texts. Next, we focus on a large body of work of topical analysis of political texts, covering unsupervised topic induction, including dictionarybased, topic-modelling and text segmentation approaches (Quinn et al., 2006, 2010; Grimmer, 2010; Albaugh et al., 2013; Glavaˇs et al., 2016; Menini et al., 2017), as well as supervised topic classification studies (Hillard et al., 2008; Collingwood and Wilkerson, 2012; Karan et al., 2016). We will also cover more recent work on cross-lingual topic classification in political texts (Glavaˇs et al., 2017a; Subramanian et al., 2018). We will further emphasize topic classification models that exploit large manually anotated corpora from CMP (Zirn et al., 2016; Subramanian et al., 2017) and CAP (Karan et al., 2016; Albaugh et al., 2013) projects, which we cover in the previous part. This introductory tutorial aims to systematically organise and analyse the overall body of research in computational analysis of political texts. This body of work has been split between two largely disjoint research communities – researchers in natural language processing and researchers in political science – and the tutorial is designed bear"
P19-4004,U17-1003,0,0.018116,"opic induction, including dictionarybased, topic-modelling and text segmentation approaches (Quinn et al., 2006, 2010; Grimmer, 2010; Albaugh et al., 2013; Glavaˇs et al., 2016; Menini et al., 2017), as well as supervised topic classification studies (Hillard et al., 2008; Collingwood and Wilkerson, 2012; Karan et al., 2016). We will also cover more recent work on cross-lingual topic classification in political texts (Glavaˇs et al., 2017a; Subramanian et al., 2018). We will further emphasize topic classification models that exploit large manually anotated corpora from CMP (Zirn et al., 2016; Subramanian et al., 2017) and CAP (Karan et al., 2016; Albaugh et al., 2013) projects, which we cover in the previous part. This introductory tutorial aims to systematically organise and analyse the overall body of research in computational analysis of political texts. This body of work has been split between two largely disjoint research communities – researchers in natural language processing and researchers in political science – and the tutorial is designed bearing this in mind. We first explain the role that textual data plays in political analyses and then proceed to examine the concrete resources and tasks addr"
P19-4004,W06-1639,0,0.0203801,"heim.de 1 Introduction community. For example, political text scaling – one of the central tasks in quantitative political science, where the goal is to quantify positions of politicians and/or parties on a scale based on the textual content they produce – has not received any attention by the NLP community until last year, whereas it has been at the core of political science research for almost two decades. At the same time, NLP researchers have addressed closely related tasks such as election prediction (O’Connor et al., 2010), ideology classification (Hirst et al., 2010), stance detection (Thomas et al., 2006), and agreement measurement (Gottipati et al., 2013), all rarely considered in the same format by the textas-data political science community. In summary, these two communities have been largely agnostic of one another, resulting in NLP researchers not contributing to relevant research questions in political science and political scientists not employing cutting-edge NLP methodology for their tasks. The development and adoption of natural language processing (NLP) methods by the political science community dates back to over twenty years ago. In the last decade the usage of computational metho"
R13-2011,W06-0901,0,0.039649,"rely on lexical and syntactic simplification, performing little content reduction, the exception being deletion of parenthetical expressions (Drndarevic et al., 2013). On the one hand, lack of content reduction has been recognized as one of the main shortcomings of automated systems (Drndarevic et al., 2013) which produce much worse simplification results compared to human. On the other hand, information extraction techniques help identify relevant content (e.g., named entities, events), but have not yet proven useful for text simplification. However, significant advances in event extraction (Ahn, 2006; Bethard, 2008; Llorens et al., 2010; Grover et al., 2010), achieved as the result of standardization efforts (Pustejovsky et al., 2003a; Pustejovsky et al., 2003b) and dedicated tasks (ACE, 2005; Verhagen et al., 2010), encourage event-oriented simplification attempts. To the best of our knowledge, the only reported work exploiting events for text simplification is that of Barlacchi and Tonelli (2013). They extract factual events from a set of Italian children’s stories and eliminate non-mandatory event arguments. They evaluate simplified texts using only the automated score which can hardly"
R13-2011,P05-1045,0,0.0161841,"Missing"
R13-2011,N07-4002,0,0.011865,"Carroll et al., 1998; Devlin and Unthank, 2006), cognitive disabilities (Saggion et al., 2011), autism (Orasan et al., 2013), congenital deafness (Inui et al., 2003), and low literacy (Alu´ısio et al., 2008). Most of these approaches rely on rule-based lexical and syntactic simplification. Syntactic simplification is usually carried out by recursively applying a set of hand-crafted rules at a sentence level, not considering interactions across sentence boundaries. Lexical simplification usually substitutes difficult words with their simpler synonyms (Carroll et al., 1998; Lal and Ruger, 2002; Burstein et al., 2007). we offer two different simplification schemes. To the best of our knowledge, this is the first work on event-based text simplification for English. 3 Event-Centered Simplification The simplification schemes we propose exploit the structure of extracted event mentions. We employ robust event extraction that involves supervised extraction of factual event anchors (i.e., words that convey the core meaning of the event) and the rulebased extraction of event arguments of coarse semantic types. Although a thorough description of the event extraction system is outside the scope of this paper, we de"
R13-2011,S10-1074,0,0.0183608,"rforming little content reduction, the exception being deletion of parenthetical expressions (Drndarevic et al., 2013). On the one hand, lack of content reduction has been recognized as one of the main shortcomings of automated systems (Drndarevic et al., 2013) which produce much worse simplification results compared to human. On the other hand, information extraction techniques help identify relevant content (e.g., named entities, events), but have not yet proven useful for text simplification. However, significant advances in event extraction (Ahn, 2006; Bethard, 2008; Llorens et al., 2010; Grover et al., 2010), achieved as the result of standardization efforts (Pustejovsky et al., 2003a; Pustejovsky et al., 2003b) and dedicated tasks (ACE, 2005; Verhagen et al., 2010), encourage event-oriented simplification attempts. To the best of our knowledge, the only reported work exploiting events for text simplification is that of Barlacchi and Tonelli (2013). They extract factual events from a set of Italian children’s stories and eliminate non-mandatory event arguments. They evaluate simplified texts using only the automated score which can hardly account for grammaticality and information relevance of th"
R13-2011,E99-1042,0,0.194837,"Missing"
R13-2011,W03-1602,0,0.0759543,"s confrontation in years”. However, studies indicate that people with reading disabilities, especially people with intellectual disabilities, have difficulties discriminating relevant 2 Related Work Several projects dealt with automated text simplification for people with different reading difficulties: 71 Proceedings of the Student Research Workshop associated with RANLP 2013, pages 71–78, Hissar, Bulgaria, 9-11 September 2013. people with alexia (Carroll et al., 1998; Devlin and Unthank, 2006), cognitive disabilities (Saggion et al., 2011), autism (Orasan et al., 2013), congenital deafness (Inui et al., 2003), and low literacy (Alu´ısio et al., 2008). Most of these approaches rely on rule-based lexical and syntactic simplification. Syntactic simplification is usually carried out by recursively applying a set of hand-crafted rules at a sentence level, not considering interactions across sentence boundaries. Lexical simplification usually substitutes difficult words with their simpler synonyms (Carroll et al., 1998; Lal and Ruger, 2002; Burstein et al., 2007). we offer two different simplification schemes. To the best of our knowledge, this is the first work on event-based text simplification for En"
R13-2011,P03-1054,0,0.00587134,"t is well-simplified if its readability is increased, while its grammaticality (syntactic correctness), meaning, and information relevance (semantic correctness) are preserved. We measure the readability of the simplified text automatically with two commonly used formulae. However, we rely on human assessment of grammaticality and relevance, given that these aspects are difficult to capture automatically (Wubben et al., 2012). We employ a syntactically motivated baseline that retains only the main clause of a sentence and discards all subordinate clauses. We used Stanford constituency parser (Klein and Manning, 2003) to identify the main and subordinate clauses. transform events with nominal anchors into separate sentences, as such events tend to have very few arguments and are often arguments of verbal events. For example, in “China and Philippines resolved a naval standoff” mention “standoff” is a target of the mention “resolved”. Thirdly, we convert gerundive events that govern the clausal complement of the main sentence event into past simple for preserving grammaticality of the output. E.g., “Philippines disputed China’s territorial claims, triggering the naval confrontation” is transformed into “Phi"
R13-2011,W11-1902,0,0.0255955,"l-Megrahi has died at his home. Baset al-Megrahi was released from a Scottish prison.” Algorithm 2. Event-wise simplification input: sentence s input: set of event mentions E // set of event-output sentence pairs It has been shown that anaphoric mentions cause difficulties for people with cognitive disabilities (Ehrlich et al., 1999; Shapiro and Milkes, 2004). To investigate this phenomenon, we additionally employ pronominal anaphora resolution on top of event-wise simplification scheme. To resolve reference of anaphoric pronouns, we use the coreference resolution tool from Stanford Core NLP (Lee et al., 2011). An example of the original text snippet accompanied by its (1) sentence-wise simplification, (2) event-wise simplification, and (3) event-wise simplification with anaphoric pronoun resolution is given in Table 2. S = {} // initialize output token set for each event foreach e in E do S = S ∪ (e, {}) // list of original sentence tokens T = tokenize(s) foreach token t in T do foreach event mention e in E do // set of event tokens a = anchor (e) A = anchorAndArgumentTokens(e) // part of verbal, non-reporting event if t in A & PoS (a) 6= N & type(t) 6= Rep do // token is gerundive anchor if t = a"
R13-2011,S10-1063,0,0.0332637,"tic simplification, performing little content reduction, the exception being deletion of parenthetical expressions (Drndarevic et al., 2013). On the one hand, lack of content reduction has been recognized as one of the main shortcomings of automated systems (Drndarevic et al., 2013) which produce much worse simplification results compared to human. On the other hand, information extraction techniques help identify relevant content (e.g., named entities, events), but have not yet proven useful for text simplification. However, significant advances in event extraction (Ahn, 2006; Bethard, 2008; Llorens et al., 2010; Grover et al., 2010), achieved as the result of standardization efforts (Pustejovsky et al., 2003a; Pustejovsky et al., 2003b) and dedicated tasks (ACE, 2005; Verhagen et al., 2010), encourage event-oriented simplification attempts. To the best of our knowledge, the only reported work exploiting events for text simplification is that of Barlacchi and Tonelli (2013). They extract factual events from a set of Italian children’s stories and eliminate non-mandatory event arguments. They evaluate simplified texts using only the automated score which can hardly account for grammaticality and infor"
R13-2011,D11-1038,0,0.272641,"Missing"
R13-2011,P12-1107,0,0.258972,"Missing"
R13-2011,N10-2011,0,0.0699518,"Missing"
R13-2011,P94-1019,0,\N,Missing
R13-2011,S10-1010,0,\N,Missing
R13-2011,chang-manning-2012-sutime,0,\N,Missing
R19-1138,abdul-mageed-diab-2014-sana,0,0.0188021,"bic word by word. Transliterating whole Arabizi texts to Arabic produces orthographic errors. Google Translate for example, detects Arabizi, converts it to Arabic, and translates it to the target language. It translated the tweet da5l jamelik w hadamtik / Oh-my (expression), your-beauty and your-humour (feminine) to inside your camel and demolished due to the dialect (Lebanese), choice of letters, and word ambiguity. 3 Literature Review Recent efforts in creating lexical resources for Arabic focus mainly on MSA (Badaro et al., 2014; Eskander and Rambow, 2015; Al-Twairesh et al., 2016) and DA (Abdul-Mageed and Diab, 2014). Several works that analysed sentiment from Arabic social data filtered out Arabizi completely from their datasets (Al-Kabi et al., 2013, 2014; Duwairi and Qarqaz, 2014), missing the sentiment from a considerable portion of the population in general and the youth in specific. To the best of our knowledge, (Duwairi et al., 2016; Mataoui et al., 2016; GUELLIL et al., 2018) are the only works that looked into sentiment analysis for Arabizi. All three papers proposed to transliterate Arabizi to Arabic. (Duwairi et al., 2016) transliterated Jordanian dialect Arabizi to Arabic using their own trans"
R19-1138,W14-1604,0,0.0264202,"dialect Arabizi as part of their sentiment analysis for Algerian Arabic pipeline. They used Google Translate without evaluating the transliterations as well. (GUELLIL et al., 2018) created a rule-based transliterator that generates several transliterations per word, then used a language model based on a large corpus to select the best transliteration. Thus, minimis2 3 http://www.yamli.com http://www.google.com/inputtools/try ing the error of transliteration but maximising the complexity of the task. The following papers propose sophisticated works for transliterating Egyptian (Darwish, 2014; Al-Badrashiny et al., 2014; Eskander et al., 2014) and Algerian dialect Arabizi (Guellil et al., 2017). The limitations of these works include transliterating datasets manually, hand-crafting rules to preprocess and normalise Arabizi, and mapping Arabizi with Arabic heuristically. In this work we aim to advance the state of the art in Arabic sentiment analysis by analysing Arabizi directly, without the need to filter, preprocess, or transliterate it. 4 Data Collection and Annotation This section describes the collection and annotation of Twitter datasets to evaluate SenZi and train an Arabizi identifier. It then descri"
R19-1138,P16-1066,0,0.018431,"text by typing in Latin script Arabic word by word. Transliterating whole Arabizi texts to Arabic produces orthographic errors. Google Translate for example, detects Arabizi, converts it to Arabic, and translates it to the target language. It translated the tweet da5l jamelik w hadamtik / Oh-my (expression), your-beauty and your-humour (feminine) to inside your camel and demolished due to the dialect (Lebanese), choice of letters, and word ambiguity. 3 Literature Review Recent efforts in creating lexical resources for Arabic focus mainly on MSA (Badaro et al., 2014; Eskander and Rambow, 2015; Al-Twairesh et al., 2016) and DA (Abdul-Mageed and Diab, 2014). Several works that analysed sentiment from Arabic social data filtered out Arabizi completely from their datasets (Al-Kabi et al., 2013, 2014; Duwairi and Qarqaz, 2014), missing the sentiment from a considerable portion of the population in general and the youth in specific. To the best of our knowledge, (Duwairi et al., 2016; Mataoui et al., 2016; GUELLIL et al., 2018) are the only works that looked into sentiment analysis for Arabizi. All three papers proposed to transliterate Arabizi to Arabic. (Duwairi et al., 2016) transliterated Jordanian dialect Ar"
R19-1138,W14-3623,1,0.741252,"ls are designed to help Arabic users output MSA text by typing in Latin script Arabic word by word. Transliterating whole Arabizi texts to Arabic produces orthographic errors. Google Translate for example, detects Arabizi, converts it to Arabic, and translates it to the target language. It translated the tweet da5l jamelik w hadamtik / Oh-my (expression), your-beauty and your-humour (feminine) to inside your camel and demolished due to the dialect (Lebanese), choice of letters, and word ambiguity. 3 Literature Review Recent efforts in creating lexical resources for Arabic focus mainly on MSA (Badaro et al., 2014; Eskander and Rambow, 2015; Al-Twairesh et al., 2016) and DA (Abdul-Mageed and Diab, 2014). Several works that analysed sentiment from Arabic social data filtered out Arabizi completely from their datasets (Al-Kabi et al., 2013, 2014; Duwairi and Qarqaz, 2014), missing the sentiment from a considerable portion of the population in general and the youth in specific. To the best of our knowledge, (Duwairi et al., 2016; Mataoui et al., 2016; GUELLIL et al., 2018) are the only works that looked into sentiment analysis for Arabizi. All three papers proposed to transliterate Arabizi to Arabic. (Duw"
R19-1138,D15-1304,0,0.0183268,"lp Arabic users output MSA text by typing in Latin script Arabic word by word. Transliterating whole Arabizi texts to Arabic produces orthographic errors. Google Translate for example, detects Arabizi, converts it to Arabic, and translates it to the target language. It translated the tweet da5l jamelik w hadamtik / Oh-my (expression), your-beauty and your-humour (feminine) to inside your camel and demolished due to the dialect (Lebanese), choice of letters, and word ambiguity. 3 Literature Review Recent efforts in creating lexical resources for Arabic focus mainly on MSA (Badaro et al., 2014; Eskander and Rambow, 2015; Al-Twairesh et al., 2016) and DA (Abdul-Mageed and Diab, 2014). Several works that analysed sentiment from Arabic social data filtered out Arabizi completely from their datasets (Al-Kabi et al., 2013, 2014; Duwairi and Qarqaz, 2014), missing the sentiment from a considerable portion of the population in general and the youth in specific. To the best of our knowledge, (Duwairi et al., 2016; Mataoui et al., 2016; GUELLIL et al., 2018) are the only works that looked into sentiment analysis for Arabizi. All three papers proposed to transliterate Arabizi to Arabic. (Duwairi et al., 2016) translit"
R19-1138,W18-6249,0,0.167971,"choice of letters, and word ambiguity. 3 Literature Review Recent efforts in creating lexical resources for Arabic focus mainly on MSA (Badaro et al., 2014; Eskander and Rambow, 2015; Al-Twairesh et al., 2016) and DA (Abdul-Mageed and Diab, 2014). Several works that analysed sentiment from Arabic social data filtered out Arabizi completely from their datasets (Al-Kabi et al., 2013, 2014; Duwairi and Qarqaz, 2014), missing the sentiment from a considerable portion of the population in general and the youth in specific. To the best of our knowledge, (Duwairi et al., 2016; Mataoui et al., 2016; GUELLIL et al., 2018) are the only works that looked into sentiment analysis for Arabizi. All three papers proposed to transliterate Arabizi to Arabic. (Duwairi et al., 2016) transliterated Jordanian dialect Arabizi to Arabic using their own transliterator without evaluating the transliterations. (Mataoui et al., 2016) transliterated Algerian dialect Arabizi as part of their sentiment analysis for Algerian Arabic pipeline. They used Google Translate without evaluating the transliterations as well. (GUELLIL et al., 2018) created a rule-based transliterator that generates several transliterations per word, then used"
R19-1138,W14-3612,0,0.325388,"Missing"
R19-1138,W14-3629,0,0.0320303,"erated Algerian dialect Arabizi as part of their sentiment analysis for Algerian Arabic pipeline. They used Google Translate without evaluating the transliterations as well. (GUELLIL et al., 2018) created a rule-based transliterator that generates several transliterations per word, then used a language model based on a large corpus to select the best transliteration. Thus, minimis2 3 http://www.yamli.com http://www.google.com/inputtools/try ing the error of transliteration but maximising the complexity of the task. The following papers propose sophisticated works for transliterating Egyptian (Darwish, 2014; Al-Badrashiny et al., 2014; Eskander et al., 2014) and Algerian dialect Arabizi (Guellil et al., 2017). The limitations of these works include transliterating datasets manually, hand-crafting rules to preprocess and normalise Arabizi, and mapping Arabizi with Arabic heuristically. In this work we aim to advance the state of the art in Arabic sentiment analysis by analysing Arabizi directly, without the need to filter, preprocess, or transliterate it. 4 Data Collection and Annotation This section describes the collection and annotation of Twitter datasets to evaluate SenZi and train an Arabiz"
R19-1138,W14-3901,0,0.279885,"their sentiment analysis for Algerian Arabic pipeline. They used Google Translate without evaluating the transliterations as well. (GUELLIL et al., 2018) created a rule-based transliterator that generates several transliterations per word, then used a language model based on a large corpus to select the best transliteration. Thus, minimis2 3 http://www.yamli.com http://www.google.com/inputtools/try ing the error of transliteration but maximising the complexity of the task. The following papers propose sophisticated works for transliterating Egyptian (Darwish, 2014; Al-Badrashiny et al., 2014; Eskander et al., 2014) and Algerian dialect Arabizi (Guellil et al., 2017). The limitations of these works include transliterating datasets manually, hand-crafting rules to preprocess and normalise Arabizi, and mapping Arabizi with Arabic heuristically. In this work we aim to advance the state of the art in Arabic sentiment analysis by analysing Arabizi directly, without the need to filter, preprocess, or transliterate it. 4 Data Collection and Annotation This section describes the collection and annotation of Twitter datasets to evaluate SenZi and train an Arabizi identifier. It then describes the creation of a Fa"
R19-1138,P16-3008,1,0.658892,"Missing"
R19-1138,H05-1044,0,0.133978,"on presents the pipeline of building SenZi. It was built in two phases: 1. Lexicon Generation: Using existing resources to generate an initial list of Arabizi sentiment words. 2. Lexicon Expansion: Expanding this sentiment word list using the created Facebook corpus (from Section 4.3). 5.1 Lexicon Generation Resources: We started with two English sentiment lexicons and one Lebanese dialect word list as seeds to build SenZi. We chose two of the most common English sentiment lexicons in the literature: Hiu and Liu5 (2K positive and 4.8K negative) and the MPQA6 (2.7K positive and 4.9K negative) (Wilson et al., 2005). LivingArabic is a list of 7.1K Lebanese dialect words compiled by the Living Arabic project7 . We generated the Lebanese Arabizi sentiment words in five steps: 1. Combine the existing English lexicons. 2. Translate to Arabic. 3. Select the dialectal sentiment words. 4. Combine the resulting Arabic lexicons. 5. Transliterate to Arabizi. 5 https://www.cs.uic.edu/~liub/FBS/ sentiment-analysis.html 6 https://mpqa.cs.pitt.edu/lexicons/ subj_lexicon 7 http://www.livingarabic.com Combination: English Lexicons We took the union of Hiu Liu and MPQA to have a list of 2.7K positive and 5.1K negative wo"
S12-1060,N09-1003,0,0.01562,"Missing"
S12-1060,S12-1051,0,0.584667,"l., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses. Our two systems, simple and syntax, use supervised machine learning, more specifically the support vector regression (SVR), to combine a large amount of features computed from pairs of sentences. The two systems differ in the set of features they employ. Our systems placed in the top 5 (out of 89 submitted systems) for all three aggregate correlation measures: 2nd (syntax) and 3rd (simple) for overall Pearson, 1st (simple) and"
S12-1060,P06-4018,0,0.115304,"d Similarity ws353 PathLen Lin Dist (NYT) Dist (Wikipedia) Knowledge-based Word Similarity Knowledge-based word similarity approaches rely on a semantic network of words, such as WordNet. Given two words, their similarity can be estimated by considering their relative positions within the knowledge base hierarchy. All of our knowledge-based word similarity measures are based on WordNet. Some measures use the concept of a lowest common subsumer (LCS) of concepts c1 and c2 , which represents the lowest node in the WordNet hierarchy that is a hypernym of both c1 and c2 . We use the NLTK library (Bird, 2006) to compute the PathLen similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998) measures. A single word often denotes several concepts, depending on its context. In order to compute the similarity score for a pair of words, we take the maximum similarity score over all possible pairs of concepts (i.e., WordNet synsets). 2.2 Table 1: Evaluation of word similarity measures Preprocessing We list all of the preprocessing steps our systems perform. If a preprocessing step is executed by only one of our systems, the system’s name is indicated in parentheses. 1. All hyphens and slashes"
S12-1060,J06-1003,0,0.117718,"in the top 5, for the three overall evaluation metrics used (overall Pearson – 2nd and 3rd, normalized Pearson – 1st and 3rd, weighted mean – 2nd and 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity"
S12-1060,W08-1301,0,0.00811189,"Missing"
S12-1060,de-marneffe-etal-2006-generating,0,0.00613622,"Missing"
S12-1060,N03-1020,0,0.112565,"ndensed into short text snippets such as social media posts, image captions, and scientific abstracts. We predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity. Out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall Pearson – 2nd and 3rd, normalized Pearson – 1st and 3rd, weighted mean – 2nd and 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et"
S12-1060,P08-1028,0,0.0519795,"c(S1 , S2 ) = Pw∈S1 ∩S2 0 0 w ∈S2 ic(w ) The weighted word overlap between two sentences is calculated as the harmonic mean of the wwc(S1 , S2 ) and wwc(S2 , S1 ). This measure proved to be very useful, but it could be improved even further. Misspelled frequent words are more frequent than some correctly spelled but rarely used words. Hence dealing with misspelled words would remove the inappropriate heavy penalty for a mismatch between correctly and incorrectly spelled words. Vector Space Sentence Similarity This measure is motivated by the idea of compositionality of distributional vectors (Mitchell and Lapata, 2008). We represent each sentence as a single distributional vector u(·) by summing the distributional (i.e., LSA) vector of each word w in the P sentence S: u(S) = w∈S xw , where xw is the vector representation of the word w. Another similar representation uW (·) uses the information content ic(w) to weigh the LSA vector P of each word before summation: uW (S) = w∈S ic(w)xw . The simple system uses |cos(u(S1 ), u(S2 )) |and |cos(uW (S1 ), uW (S2 )) |for the vector space sentence similarity features. 3.4 Greedy Lemma Aligning Overlap This measure computes the similarity between sentences using the"
S12-1060,J98-1004,0,0.0575789,"Missing"
S12-1060,U06-1019,0,0.291952,"retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses. Our two systems, simple and syntax, use supervised machine learning, more specifically the support vector regression (SVR), to combine a large amount of features computed from pairs of"
S15-2012,S12-1051,0,0.0391829,"r a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 70–74, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011)"
S15-2012,S14-2010,0,0.030874,"Missing"
S15-2012,I13-1041,0,0.0128648,"dditionally provide an analysis of the dataset and point to some peculiarities of the evaluation setup. 1 Introduction Recognizing tweets that convey the same meaning (paraphrases) or similar meaning is useful in applications such as event detection (Petrovi´c et al., 2012), tweet summarization (Yang et al., 2011), and tweet retrieval (Naveed et al., 2011). Paraphrase detection in tweets is a more challenging task than paraphrase detection in other domains such as news (Xu et al., 2013). Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in st"
S15-2012,P09-1053,0,0.172779,"ammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluatio"
S15-2012,P12-1091,0,0.0512082,"g the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose M UL TI P, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇ c et al. (2012), and (2) word alignment features, Sari´ based on (a) the output of the word alignment model by Sultan et al. (2014) and (b) a re-implementation of the M ULTI P model by Xu et al. (2014). In the dataset prov"
S15-2012,J10-3003,0,0.109662,"es brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 201"
S15-2012,N12-1019,0,0.0843001,"ncy, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages"
S15-2012,D14-1162,0,0.0807917,"tokens in tweets, and the other taking into account only content words. 2 https://github.com/ma-sultan/ monolingual-word-aligner 72 Anchor count (ANC). We re-implemented the M ULTI P model of Xu et al. (2014).3 As anchor candidates we consider all pairs of content words from the two tweets. We use a minimalistic set of features including (1) Levenshtein distance between candidate words, (2) several binary features indicating relatedness of words (e.g., lowercased tokens match, POStags match), and (3) semantic similarity obtained as the cosine of word embeddings, obtained with the GloVe model (Pennington et al., 2014) trained on Twitter data.4 To account for feature interactions, following (Xu et al., 2014), we also use conjunction features. We use the number of anchors identified by this method for a pair of tweets as a feature for our SVM model. 4 Evaluation Each team was allowed to submit two runs on the test set provided by the task organizers (Xu et al., 2015). Participants were provided with a training set (13,063 pairs) and a development set (4,727 pairs). We used the train and development set to optimize the hyperparameters C and γ of our SVM model with the RBF kernel. For the final evaluation, the"
S15-2012,N12-1034,0,0.314377,"Missing"
S15-2012,S12-1060,1,0.880991,"Missing"
S15-2012,S14-2039,0,0.0567314,"nces. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇ c et al. (2012), and (2) word alignment features, Sari´ based on (a) the output of the word alignment model by Sultan et al. (2014) and (b) a re-implementation of the M ULTI P model by Xu et al. (2014). In the dataset provided by the organizers, each tweet is associated with a topic, with 10 to 100 tweet pairs per topic. An important preprocessing step is to remove tokens that can be found in the name of a topic. For example, for the topic “Roberto Mancini”, we trim the tweets “Roberto Mancini gets the boot from the Man City” and “City sacked Mancini” to “gets the boot from the Man City” and “City sacked”, respectively, and then compute the features on the trimmed tweets. The rationale is that, given a topic, there is an"
S15-2012,W13-2515,0,0.245099,"Missing"
S15-2012,Q14-1034,0,0.531452,"Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose M UL TI P, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇ c et al. (2"
S15-2012,D11-1061,0,0.158719,"2 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 70–74, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose M UL TI P, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number"
S15-2012,S15-2001,0,\N,Missing
S15-2067,S13-2002,0,0.0726721,"Missing"
S15-2067,P06-1141,0,0.0762052,"Missing"
S15-2067,W03-0430,0,0.077863,"Missing"
S15-2067,S12-1060,1,0.882972,"Missing"
S16-2016,A00-2004,0,0.383052,"as paragraphs), many texts, especially on the web, lack any explicit segmentation. Linear text segmentation aims to represent texts as sequences of semantically coherent segments. Besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for NLP and IR tasks such as text summarization (Angheluta et al., 2002; Dias et al., 2007) and passage retrieval (Huang et al., 2003; Dias et al., 2007). Whereas early approaches to unsupervised text segmentation measured the coherence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000), more recent methods (Misra et al., 2009; Riedl and Biemann, 2012) addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics. A topical representation of text is, however, merely a vague approximation of its meaning. Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity. We employ word embeddings (Mikolov et al., 2013) and a measure ˇ c et al., of semantic relatedn"
S16-2016,N09-1040,0,0.382839,"different segments in a text) , or (2) in order to obtain hierarchical segmentations (i.e. defining a structure of subtopics between the detected segments). Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009; Riedl 125 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 125–130, Berlin, Germany, August 11-12, 2016. and Biemann, 2012), in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009). Hearst (1994) introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation. She exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term-vectors. Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments. Galley et al. (2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al. (2004) minimized different s"
S16-2016,P07-1061,0,0.0277584,"me state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009) and that it is also faster than other LDA-based methods (Misra et al., 2009). In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments. Similarly, Ferret (2007) builds the similarity graph, only between words instead of between sentences, using sparse co-occurrence vectors as semantic representations for words. He then identifies topics by clustering the word similarity graph via the Shared Nearest Neighbor algorithm (Ert¨oz et al., 2004). Unlike these works, we use the dense semantic representations of words and sentences (i.e., embeddings), which have been shown to outperform sparse semantic vectors on a range of NLP tasks. Also, instead of looking for minimal cuts in the relatedness graph, we exploit the maximal cliques of the relatedness graph be"
S16-2016,P03-1071,0,0.253557,"this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009). Hearst (1994) introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation. She exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term-vectors. Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments. Galley et al. (2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al. (2004) minimized different segmentation cost functions with dynamic programming. The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments’ latent vectors. More recent models (Misra et al., 2009; Riedl and Biemann, 2012) employed the latent Dirichlet allocation (LDA"
S16-2016,P94-1002,0,0.663193,"the best-performing topic modeling-based TS method on a real-world dataset of political manifestos. 2 Related Work Automated text segmentation received a lot of attention in NLP and IR communities due to its usefulness for text summarization and text indexing. Text segmentation can be performed in two different ways, namely (1) with the goal of obtaining linear segmentations (i.e. detecting the sequence of different segments in a text) , or (2) in order to obtain hierarchical segmentations (i.e. defining a structure of subtopics between the detected segments). Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009; Riedl 125 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 125–130, Berlin, Germany, August 11-12, 2016. and Biemann, 2012), in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009). Hearst (1994) introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation. She exploits the fact that words tend to be repeated in coherent segments an"
S16-2016,J97-1003,0,0.322462,"gments (e.g., as paragraphs), many texts, especially on the web, lack any explicit segmentation. Linear text segmentation aims to represent texts as sequences of semantically coherent segments. Besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for NLP and IR tasks such as text summarization (Angheluta et al., 2002; Dias et al., 2007) and passage retrieval (Huang et al., 2003; Dias et al., 2007). Whereas early approaches to unsupervised text segmentation measured the coherence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000), more recent methods (Misra et al., 2009; Riedl and Biemann, 2012) addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics. A topical representation of text is, however, merely a vague approximation of its meaning. Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity. We employ word embeddings (Mikolov et al., 2013) and a measure ˇ c et al., of sema"
S16-2016,P06-1004,0,0.0793795,", whereas Riedl and Biemann (2012) introduced TopicTiling, an LDA-driven extension of Hearst’s TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). Riedl and Biemann (2012) show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009) and that it is also faster than other LDA-based methods (Misra et al., 2009). In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments. Similarly, Ferret (2007) builds the similarity graph, only between words instead of between sentences, using sparse co-occurrence vectors as semantic representations for words. He then identifies topics by clustering the word similarity graph via the Shared Nearest Neighbor algorithm (Ert¨oz et al., 2004). Unlike these works,"
S16-2016,J02-1002,0,0.74952,"Missing"
S16-2016,W12-3307,0,0.638491,"k any explicit segmentation. Linear text segmentation aims to represent texts as sequences of semantically coherent segments. Besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for NLP and IR tasks such as text summarization (Angheluta et al., 2002; Dias et al., 2007) and passage retrieval (Huang et al., 2003; Dias et al., 2007). Whereas early approaches to unsupervised text segmentation measured the coherence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000), more recent methods (Misra et al., 2009; Riedl and Biemann, 2012) addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics. A topical representation of text is, however, merely a vague approximation of its meaning. Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity. We employ word embeddings (Mikolov et al., 2013) and a measure ˇ c et al., of semantic relatedness of short texts (Sari´ 2012) to construct a relatedness graph of"
S16-2016,S12-1060,1,0.717444,"Missing"
S16-2016,P01-1064,0,0.127417,"ch toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009). Hearst (1994) introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation. She exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term-vectors. Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments. Galley et al. (2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al. (2004) minimized different segmentation cost functions with dynamic programming. The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments’ latent vectors. More recent models (Misra et al., 2009; Riedl and Biemann, 2012) employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthet"
W12-0501,W06-1669,0,0.0732088,"Missing"
W12-0501,E06-1027,0,0.0272846,"d polarity analysis. Subjectivity analysis answers whether the text unit is subjective or neutral, while polarity analysis determines whether a subjective text unit is positive or negative. The majority of research approaches (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Wilson et al., 2009) see subjectivity and polarity as categorical terms (i.e., classification problems). Intuitively, not all words express the sentiment with the same intensity. Accordingly, there has been some research effort in assessing subjectivity and polarity as graded values (Baccianella et al., 2010; Andreevskaia and Bergler, 2006). Most of the work on sentence or document level sentiment makes usage of sentiment annotated lexicon providing subjectivity and polarity information for individual words (Wilson et al., 2009; Taboada et al., 2011). In this paper we present a hybrid approach for automated acquisition of sentiment lexicon. The method is language independent and corpusbased and therefore suitable for languages lacking general lexical resources such as WordNet (Fellbaum, 2010). The two-step hybrid process combines semi-supervised graph-based algorithms and supervised learning models. We consider three different t"
W12-0501,baccianella-etal-2010-sentiwordnet,0,0.627575,"s subjectivity analysis and polarity analysis. Subjectivity analysis answers whether the text unit is subjective or neutral, while polarity analysis determines whether a subjective text unit is positive or negative. The majority of research approaches (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Wilson et al., 2009) see subjectivity and polarity as categorical terms (i.e., classification problems). Intuitively, not all words express the sentiment with the same intensity. Accordingly, there has been some research effort in assessing subjectivity and polarity as graded values (Baccianella et al., 2010; Andreevskaia and Bergler, 2006). Most of the work on sentence or document level sentiment makes usage of sentiment annotated lexicon providing subjectivity and polarity information for individual words (Wilson et al., 2009; Taboada et al., 2011). In this paper we present a hybrid approach for automated acquisition of sentiment lexicon. The method is language independent and corpusbased and therefore suitable for languages lacking general lexical resources such as WordNet (Fellbaum, 2010). The two-step hybrid process combines semi-supervised graph-based algorithms and supervised learning mode"
W12-0501,J90-1003,0,0.416762,"ompiled lexical resource like WordNet does not exist. In such a case, semantic relations between words may be extracted from corpus. In their pioneering work, Hatzivassiloglou and McKeown (1997) attempt to determine the polarity of adjectives based on their co-occurrences in conjunctions. They start with a small manually labeled seed set and build on the observation that adjectives of the same polarity are often conjoined with the conjunction and, while adjectives of the opposite polarity are conjoined with the conjunction but. Turney and Littman (2003) use pointwise mutual information (PMI) (Church and Hanks, 1990) and latent semantic analysis (LSA) (Dumais, 2004) to determine the similarity of the word of unknown polarity with the words in both positive and negative seed sets. The aforementioned work presumes that there is a correlation between lexical semantics and sentiment. We base our work on the same assumption, but instead of directly comparing the words with the seed sets, we use distributional semantics to build a word similarity graph. In contrast to the approaches above, this allows us to potentially account for similarities between all pairs of words from corpus. To the best of our knowledge"
W12-0501,P07-1054,0,0.420568,"rest of the paper is structured as follows. In Section 2 we present the related work on sentiment lexicon acquisition. Section 3 discusses the semi-supervised step of the hybrid approach. In Section 4 we explain the supervised step in more detail. In Section 5 the experimental setup, the evaluation procedure, and the results of the approach are discussed. Section 6 concludes the paper and outlines future work. 2 Related Work Several approaches have been proposed for determining the prior polarity of words. Most of the approaches can be classified as either dictionarybased (Kamps et al., 2004; Esuli and Sebastiani, 2007; Baccianella et al., 2010) or corpus-based (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Regardless of the resource used, most of the approaches focus on bootstrapping, starting from a small seed set of manually labeled words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Esuli and Sebastiani, 2007). In this paper we also follow this idea of the semi-supervised bootstrapping as the first step of the sentiment lexicon acquisition. Dictionary-based approaches grow the seed sets according to the explicit paradigmatic semantic relations (synonymy, antonymy, hypon"
W12-0501,W06-3808,0,0.0414275,"n the same assumption, but instead of directly comparing the words with the seed sets, we use distributional semantics to build a word similarity graph. In contrast to the approaches above, this allows us to potentially account for similarities between all pairs of words from corpus. To the best of our knowledge, such an approach that combines corpus-based lexical semantics with graph-based propagation has not yet been applied to the task of building sentiment lexicon. However, similar approaches have been proven rather efficient on other tasks such as document level sentiment classification (Goldberg and Zhu, 2006) and word sense disambiguation (Agirre et al., 2006). word into one of the three sentiment classes (positive, negative, or neutral). Accordingly, we evaluate our method using three different measures – one to evaluate the quality of the ordering by positivity and negativity, other to evaluate the absolute sentiment scores assigned to each corpus word, and another to evaluate the classification performance. The rest of the paper is structured as follows. In Section 2 we present the related work on sentiment lexicon acquisition. Section 3 discusses the semi-supervised step of the hybrid approach"
W12-0501,P97-1023,0,0.914895,"e Processing of Textual Data (Hybrid2012), EACL 2012, pages 1–9, c Avignon, France, April 23 2012. 2012 Association for Computational Linguistics for the synsets from the negative seed set. Word’s polarity is then decided based on the difference between its PageRank values of the two runs. We also believe that graph is the appropriate structure for the propagation of sentiment properties of words. Unfortunately, for many languages a precompiled lexical resource like WordNet does not exist. In such a case, semantic relations between words may be extracted from corpus. In their pioneering work, Hatzivassiloglou and McKeown (1997) attempt to determine the polarity of adjectives based on their co-occurrences in conjunctions. They start with a small manually labeled seed set and build on the observation that adjectives of the same polarity are often conjoined with the conjunction and, while adjectives of the opposite polarity are conjoined with the conjunction but. Turney and Littman (2003) use pointwise mutual information (PMI) (Church and Hanks, 1990) and latent semantic analysis (LSA) (Dumais, 2004) to determine the similarity of the word of unknown polarity with the words in both positive and negative seed sets. The"
W12-0501,kamps-etal-2004-using,0,0.0333739,"Missing"
W12-0501,W06-1652,0,0.0181436,"also comparable to SentiWordNet when restricted to monosentimous (all senses carry the same sentiment) words. This is satisfactory, given the absence of explicit semantic relations between words in the corpus. 1 Introduction Knowing someone’s attitude towards events, entities, and phenomena can be very important in various areas of human activity. Sentiment analysis is an area of computational linguistics that aims to recognize the subjectivity and attitude expressed in natural language texts. Applications of sentiment analysis are numerous, including sentiment-based document classification (Riloff et al., 2006), opinion-oriented information extraction (Hu and Liu, 2004), and question answering (Somasundaran et al., 2007). 1. Polarity ranking task – determine the relative rankings of words, i.e., order lexicon items descendingly by positivity and negativity; 2. Polarity regression task – assign each word absolute scores (between 0 and 1) for positivity and negativity; 3. Sentiment classification task – classify each 1 Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 1–9, c Avignon, France, April 23 2012. 2012 Association for"
W12-0501,J11-2001,0,0.0995927,"s (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Wilson et al., 2009) see subjectivity and polarity as categorical terms (i.e., classification problems). Intuitively, not all words express the sentiment with the same intensity. Accordingly, there has been some research effort in assessing subjectivity and polarity as graded values (Baccianella et al., 2010; Andreevskaia and Bergler, 2006). Most of the work on sentence or document level sentiment makes usage of sentiment annotated lexicon providing subjectivity and polarity information for individual words (Wilson et al., 2009; Taboada et al., 2011). In this paper we present a hybrid approach for automated acquisition of sentiment lexicon. The method is language independent and corpusbased and therefore suitable for languages lacking general lexical resources such as WordNet (Fellbaum, 2010). The two-step hybrid process combines semi-supervised graph-based algorithms and supervised learning models. We consider three different tasks, each capturing different aspect of a sentiment lexicon: Numerous sentiment analysis applications make usage of a sentiment lexicon. In this paper we present experiments on hybrid sentiment lexicon acquisition"
W12-0501,J09-3003,0,0.017454,"ents on Hybrid Corpus-Based Sentiment Lexicon Acquisition ˇ Goran Glavaˇs, Jan Snajder and Bojana Dalbelo Baˇsi´c Faculty of Electrical Engineering and Computing University of Zagreb Zagreb, Croatia {goran.glavas, jan.snajder, bojana.dalbelo}@fer.hr Abstract Sentiment analysis combines subjectivity analysis and polarity analysis. Subjectivity analysis answers whether the text unit is subjective or neutral, while polarity analysis determines whether a subjective text unit is positive or negative. The majority of research approaches (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Wilson et al., 2009) see subjectivity and polarity as categorical terms (i.e., classification problems). Intuitively, not all words express the sentiment with the same intensity. Accordingly, there has been some research effort in assessing subjectivity and polarity as graded values (Baccianella et al., 2010; Andreevskaia and Bergler, 2006). Most of the work on sentence or document level sentiment makes usage of sentiment annotated lexicon providing subjectivity and polarity information for individual words (Wilson et al., 2009; Taboada et al., 2011). In this paper we present a hybrid approach for automated acqui"
W13-2404,W06-3808,0,0.03436,"lemmas that property pairs. Opinion classification of reviews frequently co-occur with opinion clues. We then has been approached using supervised text cate- manually filter out the false positives from the lists gorization techniques (Pang et al., 2002; Funk et of candidate clues and aspects. al., 2008) and semi-supervised methods based on Unlike some approaches (Popescu and Etzioni, the similarity between unlabeled documents and a 2007; Kobayashi et al., 2007), we do not require small set of manually labeled documents or clues that clues or aspects belong to certain word cate(Turney, 2002; Goldberg and Zhu, 2006). gories or to a predefined taxonomy. Our approach Sentiment analysis and opinion mining ap- is pragmatic – clues are words that express opinproaches have been proposed for several Slavic ions about aspects, while aspects are words that languages (Chetviorkin et al., 2012; Buczynski and opinion clues target. For example, we treat words Wawer, 2008; Smrž, 2006; Smailovi´c et al., 2012). like sti´ci (to arrive) and sve (everything) as aspects, Methods that rely on translation, using resources because they can be targets of opinion clues, as in developed for major languages, have also been pro- “"
W13-2404,agic-etal-2010-towards,0,0.0435545,"Missing"
W13-2404,P10-1060,0,0.0281604,"Missing"
W13-2404,C12-2001,0,0.0489295,"Missing"
W13-2404,W06-1642,0,0.0799172,"Missing"
W13-2404,W12-1702,0,0.0303502,"occurrence or syntactic criteria in a domainspecific corpus (Kanayama and Nasukawa, 2006; Popescu and Etzioni, 2007; Fahrni and Klenner, 2008; Mukherjee and Liu, 2012). Kobayashi et 18 Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 18–23, c Sofia, Bulgaria, 8-9 August 2013. 2010 Association for Computational Linguistics al. (2007) extract aspect-clue pairs from weblog rating). Analogously, we consider as negative posts using a supervised model with parts of de- clue candidates lemmas that occur much more frependency trees as features. Kelly et al. (2012) quently in negative than in positive reviews. Asuse a semi-supervised SVM model with syntactic suming that opinion clues target product aspects, features to classify the relations between entity- we extract as aspect candidates all lemmas that property pairs. Opinion classification of reviews frequently co-occur with opinion clues. We then has been approached using supervised text cate- manually filter out the false positives from the lists gorization techniques (Pang et al., 2002; Funk et of candidate clues and aspects. al., 2008) and semi-supervised methods based on Unlike some approaches ("
W13-2404,D07-1114,0,0.0227591,"odel with syntactic suming that opinion clues target product aspects, features to classify the relations between entity- we extract as aspect candidates all lemmas that property pairs. Opinion classification of reviews frequently co-occur with opinion clues. We then has been approached using supervised text cate- manually filter out the false positives from the lists gorization techniques (Pang et al., 2002; Funk et of candidate clues and aspects. al., 2008) and semi-supervised methods based on Unlike some approaches (Popescu and Etzioni, the similarity between unlabeled documents and a 2007; Kobayashi et al., 2007), we do not require small set of manually labeled documents or clues that clues or aspects belong to certain word cate(Turney, 2002; Goldberg and Zhu, 2006). gories or to a predefined taxonomy. Our approach Sentiment analysis and opinion mining ap- is pragmatic – clues are words that express opinproaches have been proposed for several Slavic ions about aspects, while aspects are words that languages (Chetviorkin et al., 2012; Buczynski and opinion clues target. For example, we treat words Wawer, 2008; Smrž, 2006; Smailovi´c et al., 2012). like sti´ci (to arrive) and sve (everything) as aspects"
W13-2404,P12-1036,0,0.0243172,"ave been proposed, ranging from domainspecific (Fahrni and Klenner, 2008; Qiu et al., 2009; Choi et al., 2009) to cross-domain approaches (Wilson et al., 2009; Taboada et al., 2011), and from lexicon-based methods (Popescu and Etzioni, 2007; Jijkoun et al., 2010; Taboada et al., 2011) to machine learning approaches (Boiy and Moens, 2009; Go et al., 2009). While early attempts focused on classifying overall document opinion (Turney, 2002; Pang et al., 2002), more recent approaches identify opinions expressed about individual product aspects (Popescu and Etzioni, 2007; Fahrni and Klenner, 2008; Mukherjee and Liu, 2012). Identifying opinionated aspects allows for aspect-based comparison across reviews and enables opinion summarization 2 Related Work Aspect-based opinion mining typically consists of three subtasks: sentiment lexicon acquisition, aspect-clue pair identification, and overall review opinion prediction. Most approaches to domainspecific sentiment lexicon acquisition start from a manually compiled set of aspects and opinion clues and then expand it with words satisfying certain co-occurrence or syntactic criteria in a domainspecific corpus (Kanayama and Nasukawa, 2006; Popescu and Etzioni, 2007; F"
W13-2404,W02-1011,0,0.0181113,"pinion mining is being increasingly used to automatically recognize opinions about products in natural language texts. Numerous approaches to opinion mining have been proposed, ranging from domainspecific (Fahrni and Klenner, 2008; Qiu et al., 2009; Choi et al., 2009) to cross-domain approaches (Wilson et al., 2009; Taboada et al., 2011), and from lexicon-based methods (Popescu and Etzioni, 2007; Jijkoun et al., 2010; Taboada et al., 2011) to machine learning approaches (Boiy and Moens, 2009; Go et al., 2009). While early attempts focused on classifying overall document opinion (Turney, 2002; Pang et al., 2002), more recent approaches identify opinions expressed about individual product aspects (Popescu and Etzioni, 2007; Fahrni and Klenner, 2008; Mukherjee and Liu, 2012). Identifying opinionated aspects allows for aspect-based comparison across reviews and enables opinion summarization 2 Related Work Aspect-based opinion mining typically consists of three subtasks: sentiment lexicon acquisition, aspect-clue pair identification, and overall review opinion prediction. Most approaches to domainspecific sentiment lexicon acquisition start from a manually compiled set of aspects and opinion clues and th"
W13-2404,J11-2001,0,0.0296675,"f reviews. We show that a supervised approach to linking opinion clues to aspects is feasible, and that the extracted clues and aspects improve polarity and rating predictions. 1 Introduction For companies, knowing what customers think of their products and services is essential. Opinion mining is being increasingly used to automatically recognize opinions about products in natural language texts. Numerous approaches to opinion mining have been proposed, ranging from domainspecific (Fahrni and Klenner, 2008; Qiu et al., 2009; Choi et al., 2009) to cross-domain approaches (Wilson et al., 2009; Taboada et al., 2011), and from lexicon-based methods (Popescu and Etzioni, 2007; Jijkoun et al., 2010; Taboada et al., 2011) to machine learning approaches (Boiy and Moens, 2009; Go et al., 2009). While early attempts focused on classifying overall document opinion (Turney, 2002; Pang et al., 2002), more recent approaches identify opinions expressed about individual product aspects (Popescu and Etzioni, 2007; Fahrni and Klenner, 2008; Mukherjee and Liu, 2012). Identifying opinionated aspects allows for aspect-based comparison across reviews and enables opinion summarization 2 Related Work Aspect-based opinion min"
W13-2404,P02-1053,0,0.0267542,"s essential. Opinion mining is being increasingly used to automatically recognize opinions about products in natural language texts. Numerous approaches to opinion mining have been proposed, ranging from domainspecific (Fahrni and Klenner, 2008; Qiu et al., 2009; Choi et al., 2009) to cross-domain approaches (Wilson et al., 2009; Taboada et al., 2011), and from lexicon-based methods (Popescu and Etzioni, 2007; Jijkoun et al., 2010; Taboada et al., 2011) to machine learning approaches (Boiy and Moens, 2009; Go et al., 2009). While early attempts focused on classifying overall document opinion (Turney, 2002; Pang et al., 2002), more recent approaches identify opinions expressed about individual product aspects (Popescu and Etzioni, 2007; Fahrni and Klenner, 2008; Mukherjee and Liu, 2012). Identifying opinionated aspects allows for aspect-based comparison across reviews and enables opinion summarization 2 Related Work Aspect-based opinion mining typically consists of three subtasks: sentiment lexicon acquisition, aspect-clue pair identification, and overall review opinion prediction. Most approaches to domainspecific sentiment lexicon acquisition start from a manually compiled set of aspects and"
W13-2404,J09-3003,0,0.178251,"polarity and rating of reviews. We show that a supervised approach to linking opinion clues to aspects is feasible, and that the extracted clues and aspects improve polarity and rating predictions. 1 Introduction For companies, knowing what customers think of their products and services is essential. Opinion mining is being increasingly used to automatically recognize opinions about products in natural language texts. Numerous approaches to opinion mining have been proposed, ranging from domainspecific (Fahrni and Klenner, 2008; Qiu et al., 2009; Choi et al., 2009) to cross-domain approaches (Wilson et al., 2009; Taboada et al., 2011), and from lexicon-based methods (Popescu and Etzioni, 2007; Jijkoun et al., 2010; Taboada et al., 2011) to machine learning approaches (Boiy and Moens, 2009; Go et al., 2009). While early attempts focused on classifying overall document opinion (Turney, 2002; Pang et al., 2002), more recent approaches identify opinions expressed about individual product aspects (Popescu and Etzioni, 2007; Fahrni and Klenner, 2008; Mukherjee and Liu, 2012). Identifying opinionated aspects allows for aspect-based comparison across reviews and enables opinion summarization 2 Related Work A"
W13-2404,C00-2137,0,0.0487357,"Missing"
W13-2404,H05-2017,0,\N,Missing
W13-2404,H05-1043,0,\N,Missing
W13-2404,W11-1704,0,\N,Missing
W13-5001,bejan-harabagiu-2008-linguistic,0,0.0696688,"Missing"
W13-5001,P13-2139,1,0.776575,"Missing"
W13-5001,I13-1005,0,0.0286708,". Our approach fits into the latter group but we represent documents as graphs of events rather than graphs of concepts. In NLP, graph kernels have been used for question type classification (Suzuki, 2005), cross-lingual retrieval (Noh et al., 2009), and recognizing news stories on ˇ the same event (Glavaˇs and Snajder, 2013b). Event-based IR is addressed explicitly by Lin et al. (2007), who compare predicate-argument structures extracted from queries to those extracted from documents. However, queries have to be manually decomposed into semantic roles and can contain only a single predicate. Kawahara et al. (2013) propose a similar approach and demonstrate that ranking based on semantic roles outperforms ranking based on syntactic dependencies. Both these approaches target the problem of syntactic alternation but do not consider the queries made of multiple predicates, such as those expressing temporal relations between events. 3 Kernels on Event Graphs Our approach consists of two steps. First, we construct event graphs from both the document and the query. We then use a graph kernel to measure the query-document similarity and rank the documents. 3.1 Event Graphs An event graph is a mixed graph in wh"
W14-3705,N03-1013,0,0.0359899,"entences and the number of tokens. Additionally, we use a feature indicating if the two mentions are adjacent (no mentions occur in between). Syntactic features: All dependency relations on the path between events in the dependency tree and features that indicate whether one of the features syntactically governs the other. We compute the syntactic features only for pairs of event mentions from the same sentence, using the Stanford dependency parser (De Marneffe et al., 2006). Knowledge-based features: Computed using WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and CatVar (Habash and Dorr, 2003). We use a feature indicating whether one event mention or any of its derivatives (obtained from CatVar) is a WordNet hypernym of (for nominalized mentions) or entailed from (for verb mentions) the other mention (or any of its derivatives). We use an additional feature to indicate the VerbOcean relation between the event mentions, if such exists. Unlike features from previous groups, bloodshed agreed Figure 1: An example of an EHDAG for a narrative not account for global narrative coherence. Chambers and Jurafsky (2008) consider narratives to be chains of temporally ordered events linked by a"
W14-3705,E12-1034,0,0.0492029,"Missing"
W14-3705,W06-1623,0,0.144937,"from previous groups, bloodshed agreed Figure 1: An example of an EHDAG for a narrative not account for global narrative coherence. Chambers and Jurafsky (2008) consider narratives to be chains of temporally ordered events linked by a common protagonist. Limiting a narrative to a sequence of protagonist-sharing events can often be overly restrictive. E.g., an “encounter between Merkel and Holland” may belong to the same “summit” narrative as a “meeting between Obama and Putin,” although they share no protagonists. Several approaches enforce coherence of temporal relations at a document level. Bramsen et al. (2006) represent the temporal structure of a document as a DAG in which vertices denote textual segments and edges temporal precedence. Similarly, Do et al. (2012) enforce coherence using ILP for joint inference on decisions from local event– event and event–time interval decisions. Complementary to Chambers and Jurafsky (2008), who use a linear temporal structure, with EHDAGs we model the hierarchical structure of events with diverse participants. Similarly to Bramsen et al. (2006), we use an ILP formulation of global coherence over local decisions, but consider STC relations between events rather"
W14-3705,P08-1090,0,0.0342476,"ng WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and CatVar (Habash and Dorr, 2003). We use a feature indicating whether one event mention or any of its derivatives (obtained from CatVar) is a WordNet hypernym of (for nominalized mentions) or entailed from (for verb mentions) the other mention (or any of its derivatives). We use an additional feature to indicate the VerbOcean relation between the event mentions, if such exists. Unlike features from previous groups, bloodshed agreed Figure 1: An example of an EHDAG for a narrative not account for global narrative coherence. Chambers and Jurafsky (2008) consider narratives to be chains of temporally ordered events linked by a common protagonist. Limiting a narrative to a sequence of protagonist-sharing events can often be overly restrictive. E.g., an “encounter between Merkel and Holland” may belong to the same “summit” narrative as a “meeting between Obama and Putin,” although they share no protagonists. Several approaches enforce coherence of temporal relations at a document level. Bramsen et al. (2006) represent the temporal structure of a document as a DAG in which vertices denote textual segments and edges temporal precedence. Similarly"
W14-3705,W04-3205,0,0.192755,"in the document, both in the number of sentences and the number of tokens. Additionally, we use a feature indicating if the two mentions are adjacent (no mentions occur in between). Syntactic features: All dependency relations on the path between events in the dependency tree and features that indicate whether one of the features syntactically governs the other. We compute the syntactic features only for pairs of event mentions from the same sentence, using the Stanford dependency parser (De Marneffe et al., 2006). Knowledge-based features: Computed using WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and CatVar (Habash and Dorr, 2003). We use a feature indicating whether one event mention or any of its derivatives (obtained from CatVar) is a WordNet hypernym of (for nominalized mentions) or entailed from (for verb mentions) the other mention (or any of its derivatives). We use an additional feature to indicate the VerbOcean relation between the event mentions, if such exists. Unlike features from previous groups, bloodshed agreed Figure 1: An example of an EHDAG for a narrative not account for global narrative coherence. Chambers and Jurafsky (2008) consider narratives to be chains of te"
W14-3705,C04-1197,0,0.0430997,"one event must be in the same relation with all coreferent mentions of the other event (eqs. 6–9). Let coref (ei , ej ) be a predicate that holds iff mentions e1 and e2 corefer. The coreference constraints are as follows: The hierarchy of events induced from the independent pairwise STC decisions may be globally incoherent. We therefore need to optimize the set of pairwise STC classifications with respect to the set of constraints that enforce global coherence. We perform exact inference using Integer Linear Programming (ILP), an approach that has been proven useful in many NLP applications (Punyakanok et al., 2004; Roth and Yih, 2007; Clarke and Lapata, 2008). We use the lp solve1 solver to optimize the objective function with respect to the constraints. Objective function. Let M = {e1 , e2 , . . . , en } be the set of all event mentions in the news story and P be the set of all considered pairs of event mentions, P = {(ei , ej ) |ei , ej ∈ M, i < j}. Let R = {S UPER S UB, S UB S UPER, N O R EL} be the set of spatiotemporal relation types and let C(ei , ej , r) be the probability, produced by the pairwise classifier, of relation r holding between event mentions ei and ej . We maximize the sum of local"
W14-3705,de-marneffe-etal-2006-generating,0,0.059379,"Missing"
W14-3705,D12-1062,0,0.0799773,"onsider narratives to be chains of temporally ordered events linked by a common protagonist. Limiting a narrative to a sequence of protagonist-sharing events can often be overly restrictive. E.g., an “encounter between Merkel and Holland” may belong to the same “summit” narrative as a “meeting between Obama and Putin,” although they share no protagonists. Several approaches enforce coherence of temporal relations at a document level. Bramsen et al. (2006) represent the temporal structure of a document as a DAG in which vertices denote textual segments and edges temporal precedence. Similarly, Do et al. (2012) enforce coherence using ILP for joint inference on decisions from local event– event and event–time interval decisions. Complementary to Chambers and Jurafsky (2008), who use a linear temporal structure, with EHDAGs we model the hierarchical structure of events with diverse participants. Similarly to Bramsen et al. (2006), we use an ILP formulation of global coherence over local decisions, but consider STC relations between events rather than temporal relations between textual segments. 3 Spatiotemporal Containment Classifier Constructing Coherent Hierarchies As an example, consider the follo"
W14-3705,W13-0119,0,0.0370892,"Missing"
W14-3705,S13-2001,0,0.0784021,"Missing"
W14-3705,glavas-etal-2014-hieve,1,0.679039,"Missing"
W14-3705,C00-2137,0,0.0814194,"Missing"
W14-3705,P13-2139,1,0.889441,"Missing"
W14-3705,S10-1010,0,\N,Missing
W14-3705,2003.mtsummit-systems.9,0,\N,Missing
W15-5303,P95-1017,0,0.177682,"ional approaches to coreference resolution for English were rule-based and heavily influenced by computational theories of discourse such as focusing and centering (Sidner, 1979; Grosz et al., 1983). As annotated coreference corpora became available, primarily within the Message Understanding Conferences (MUC-6 and MUC-7), research focus shifted towards supervised machine learning models. The first learningbased coreference resolution approach dates back to Connolly et al. (1997). The mention-pair model is essentially a binary coreference classifier for pairs of entity mentions, introduced by Aone and Bennett (1995) and McCarthy and Lehnert (1995). It is still at the core of most coreference resolution systems, despite its obvious inability to enforce the transitivity inherent to the coreference relation and the fact that it requires an additional clustering algorithm to build the coreference clusters. Interestingly enough, more complex models such as entity-mention model (McCallum and Wellner, 2003; Daum´e III and Marcu, 2005; Yang et al., 2008a) and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference"
W15-5303,kopec-ogrodniczuk-2012-creating,0,0.0879632,"n coreference resolution for other major languages, including Spanish (Palomar et al., 2001; Sapena et al., 2010), Italian (Kobdani and Sch¨utze, 2010; Poesio et al., 2010), German (Versley, 2006; Wunsch, 2010), Chinese (Converse, 2006; Kong and Zhou, 2010), Japanese (Iida et al., 2003; Iida, 2007), and Arabic (Zitouni et al., 2005; Luo and Zitouni, 2005). On the other hand, research on coreference resolution for Slavic languages has been quite limited, mainly due to the non-existence of manually annotated corpora. The exceptions are the work done for Polish, (Marciniak, 2002; Matysiak, 2007; Kopec and Ogrodniczuk, 2012), Czech (Linh et al., 2009), and Bulgarian (Zhikov et al., 2013). In particular, Kopec and Ogrodniczuk (2012) demonstrate that a rule-based coreference resolution system for Polish significantly outperforms state-of-the-art machine learning models for English, suggesting that the coreference resolution model benefits from morphological complexity of Polish. In this work, we present a mention-pair coreference resolution model for Croatian. Our model accounts for transitivity of coreference relations by encoding transitivity constraints as an ILP optimization problem. Our constrained mention-pai"
W15-5303,W11-1902,0,0.0292016,"morphological preprocessing (MP-M ORPH). Results show that the supervised mention-pair model significantly outperforms both reasonable rule-based baselines. When morphological features are not used, the model exhibits a slightly lower performance, although the difference is not substantial. Enforcing transitivity in an ILP setting marginally improves the overall MUC score, but yields notable 2-point improvement in B 3 score. Precision is consistently higher than recall for all models and both evaluation metrics, which is consistent with the coreference resolution results for other languages (Lee et al., 2011; Kobdani and Sch¨utze, 2011). Overall, our results are over 10 points higher than the state-of-the-art performance for English (Lee et al., 2011) and comparable (higher MUC and lower B 3 score) to the best results obtained for Polish (Kopec and Ogrodniczuk, 2012), suggesting that coreference resolution may be easier task for morphologically complex languages. 6 Conclusion We presented the first coreference resolution model for Croatian. We built a supervised mention-pair model for recognizing identity coreference relations between entity mentions and augmented it with transitivity constraints"
W15-5303,W09-3939,0,0.0408154,"Missing"
W15-5303,H05-1013,0,0.0756267,"Missing"
W15-5303,H05-1083,0,0.088793,"Missing"
W15-5303,Q14-1037,0,0.0229354,"ous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction (Shinyama and Sekine, 2006), question answering (Vicedo and Ferr´andez, 2000; Zheng, 2002), and text summarization (Bergler et al., 2003; Steinberger et al., 2007). Despite being easy to define, coreference resolution is considered to be a rather difficult task, primarily because it heavily relies on external knowledge (e.g., for resolving “U.S. President” and “Barack Obama”, one needs to know that Obama is the president of the USA) (Markert et al., 2003; Durrett and Klein, 2014). Although machine learning-based approaches to anaphora and coreference resolution for English appeared almost two decades ago (Connolly et al., 17 Proceedings of the 5th Workshop on Balto-Slavic Natural Language Processing, pages 17–23, Hissar, Bulgaria, 10–11 September 2015. failed to demonstrate a significant performance improvements over the simple mention-pair model. Besides for English, there is a significant body of work on coreference resolution for other major languages, including Spanish (Palomar et al., 2001; Sapena et al., 2010), Italian (Kobdani and Sch¨utze, 2010; Poesio et al.,"
W15-5303,W03-2606,0,0.0327748,"his attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction (Shinyama and Sekine, 2006), question answering (Vicedo and Ferr´andez, 2000; Zheng, 2002), and text summarization (Bergler et al., 2003; Steinberger et al., 2007). Despite being easy to define, coreference resolution is considered to be a rather difficult task, primarily because it heavily relies on external knowledge (e.g., for resolving “U.S. President” and “Barack Obama”, one needs to know that Obama is the president of the USA) (Markert et al., 2003; Durrett and Klein, 2014). Although machine learning-based approaches to anaphora and coreference resolution for English appeared almost two decades ago (Connolly et al., 17 Proceedings of the 5th Workshop on Balto-Slavic Natural Language Processing, pages 17–23, Hissar, Bulgaria, 10–11 September 2015. failed to demonstrate a significant performance improvements over the simple mention-pair model. Besides for English, there is a significant body of work on coreference resolution for other major languages, including Spanish (Palomar et al., 2001; Sapena et al., 2010), Italian (Kobdani and Sch¨"
W15-5303,P83-1007,0,0.436491,"rence relation and the fact that it requires an additional clustering algorithm to build the coreference clusters. Interestingly enough, more complex models such as entity-mention model (McCallum and Wellner, 2003; Daum´e III and Marcu, 2005; Yang et al., 2008a) and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades (Grosz et al., 1983; Connolly et al., 1997; Ponzetto and Strube, 2006). Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction (Shinyama and Sekine, 2006), question answering (Vicedo and Ferr´andez, 2000; Zheng, 2002), and text summarization (Bergler et al., 2003; Steinberger et al., 2007). Despite being easy to define, coreference resolution is considered to be a rather difficult task, primarily because it heavily relies on external knowledge (e.g., for resolving “U.S. Presiden"
W15-5303,W03-2604,0,0.448941,". The mention-pair model is essentially a binary coreference classifier for pairs of entity mentions, introduced by Aone and Bennett (1995) and McCarthy and Lehnert (1995). It is still at the core of most coreference resolution systems, despite its obvious inability to enforce the transitivity inherent to the coreference relation and the fact that it requires an additional clustering algorithm to build the coreference clusters. Interestingly enough, more complex models such as entity-mention model (McCallum and Wellner, 2003; Daum´e III and Marcu, 2005; Yang et al., 2008a) and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades (Grosz et al., 1983; Connolly et al., 1997; Ponzetto and Strube, 2006). Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction (Shinyama and Sekine, 2006), question"
W15-5303,S10-1018,0,0.0358274,"Missing"
W15-5303,P02-1014,0,0.101915,"ions (as the set of individual binary decisions may conflict the transitivity property of the coreference relation). 4.1 • Indication whether the two mention strings fully match (f1 ); • Indication whether one mention string contains the other (f2 ); • Length of the longest common subsequence between the mentions (f3 ); • Edit distance (i.e., Levenshtein distance) between the mentions (f4 ). Creating Training Instances Overlap features quantify the overlap between the mentions in terms of tokens these mentions share: In this work, we generate training instances using the heuristic proposed by Ng and Cardie (2002), which is, in turn, the extension of the approach by Soon et al. (2001). We thus create a positive 1 Mention-Pair Model • Indications whether there is at least one matching word, lemma, and stem between the tokens of the two mentions (f4 , f5 , and f6 ); • Relative overlap between the mentions, meaA part of this dataset is freely available; cf. Section 5. 19 sured as the number of content lemmas (nouns, adjectives, verbs, and adverbs) found in both mentions, normalized by the token length of both mentions (f7 ). Grammatical features encode some grammatical properties and aim to indicate gramm"
W15-5303,W11-1910,0,0.0539046,"Missing"
W15-5303,D10-1086,0,0.0543677,"Missing"
W15-5303,J01-4005,0,0.112695,"Missing"
W15-5303,P08-1096,0,0.0138681,"ach dates back to Connolly et al. (1997). The mention-pair model is essentially a binary coreference classifier for pairs of entity mentions, introduced by Aone and Bennett (1995) and McCarthy and Lehnert (1995). It is still at the core of most coreference resolution systems, despite its obvious inability to enforce the transitivity inherent to the coreference relation and the fact that it requires an additional clustering algorithm to build the coreference clusters. Interestingly enough, more complex models such as entity-mention model (McCallum and Wellner, 2003; Daum´e III and Marcu, 2005; Yang et al., 2008a) and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades (Grosz et al., 1983; Connolly et al., 1997; Ponzetto and Strube, 2006). Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extractio"
W15-5303,poesio-etal-2010-creating,0,0.0429758,"Missing"
W15-5303,N06-1025,0,0.0414381,"s an additional clustering algorithm to build the coreference clusters. Interestingly enough, more complex models such as entity-mention model (McCallum and Wellner, 2003; Daum´e III and Marcu, 2005; Yang et al., 2008a) and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades (Grosz et al., 1983; Connolly et al., 1997; Ponzetto and Strube, 2006). Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction (Shinyama and Sekine, 2006), question answering (Vicedo and Ferr´andez, 2000; Zheng, 2002), and text summarization (Bergler et al., 2003; Steinberger et al., 2007). Despite being easy to define, coreference resolution is considered to be a rather difficult task, primarily because it heavily relies on external knowledge (e.g., for resolving “U.S. President” and “Barack Obama”, one needs to know that Obama"
W15-5303,J08-3002,0,0.023642,"ach dates back to Connolly et al. (1997). The mention-pair model is essentially a binary coreference classifier for pairs of entity mentions, introduced by Aone and Bennett (1995) and McCarthy and Lehnert (1995). It is still at the core of most coreference resolution systems, despite its obvious inability to enforce the transitivity inherent to the coreference relation and the fact that it requires an additional clustering algorithm to build the coreference clusters. Interestingly enough, more complex models such as entity-mention model (McCallum and Wellner, 2003; Daum´e III and Marcu, 2005; Yang et al., 2008a) and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades (Grosz et al., 1983; Connolly et al., 1997; Ponzetto and Strube, 2006). Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extractio"
W15-5303,recasens-etal-2010-typology,0,0.0611384,"Missing"
W15-5303,R13-1098,0,0.0609934,"Missing"
W15-5303,S10-1017,0,0.0612956,"Missing"
W15-5303,W05-0709,0,0.0289008,"Missing"
W15-5303,N06-1039,0,0.0305155,"and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades (Grosz et al., 1983; Connolly et al., 1997; Ponzetto and Strube, 2006). Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction (Shinyama and Sekine, 2006), question answering (Vicedo and Ferr´andez, 2000; Zheng, 2002), and text summarization (Bergler et al., 2003; Steinberger et al., 2007). Despite being easy to define, coreference resolution is considered to be a rather difficult task, primarily because it heavily relies on external knowledge (e.g., for resolving “U.S. President” and “Barack Obama”, one needs to know that Obama is the president of the USA) (Markert et al., 2003; Durrett and Klein, 2014). Although machine learning-based approaches to anaphora and coreference resolution for English appeared almost two decades ago (Connolly et al"
W15-5303,J01-4004,0,0.385848,"ity property of the coreference relation). 4.1 • Indication whether the two mention strings fully match (f1 ); • Indication whether one mention string contains the other (f2 ); • Length of the longest common subsequence between the mentions (f3 ); • Edit distance (i.e., Levenshtein distance) between the mentions (f4 ). Creating Training Instances Overlap features quantify the overlap between the mentions in terms of tokens these mentions share: In this work, we generate training instances using the heuristic proposed by Ng and Cardie (2002), which is, in turn, the extension of the approach by Soon et al. (2001). We thus create a positive 1 Mention-Pair Model • Indications whether there is at least one matching word, lemma, and stem between the tokens of the two mentions (f4 , f5 , and f6 ); • Relative overlap between the mentions, meaA part of this dataset is freely available; cf. Section 5. 19 sured as the number of content lemmas (nouns, adjectives, verbs, and adverbs) found in both mentions, normalized by the token length of both mentions (f7 ). Grammatical features encode some grammatical properties and aim to indicate grammatical compatibility of the two mentions: • Indication whether the first"
W15-5303,P00-1070,0,0.113559,"Missing"
W16-2705,J92-4003,0,0.138452,"rve the ordering or ranking of the embeddings. To do this, we need to compute two thresholds per dimension (upper and lower) across the whole vocabulary. For each dimension (component) i of is computed the mean of positives values (Ci+ , the upper threshold) and negative values (Ci− , the lower one). Thereafter, the following function is used over each component Cij of vector w ~j: Word Representations have been shown to substantially improve several NLP tasks, among which NER for English and German (Faruqui and Pad´o, 2010). There are two main approaches. One approach is to compute clusters (Brown et al., 1992; Liang, 2005) (Brown Clustering) from unlabeled data and using them as features in NLP models (including NER). Another approach transforms each word into a continuous real-valued vector (Collobert and Weston, 2008) of n dimensions also known as a “word embedding” (Mikolov et al., 2013a). With (Brown) clustering, words that appear in the same or a similar sentence context are assigned to the same cluster. Whereas in word embeddings similar words occur close to each other in Rn (the induced n dimensional vector space). Word Representations work better the more data they are fed. One way to achi"
W16-2705,W02-2004,0,0.299069,"Missing"
W16-2705,W15-3904,0,0.0386154,"Missing"
W16-2705,W09-1119,0,0.0611789,"n using cross-lingual Word Representations. Keywords. NER for Spanish, Word Representations, Conditional Random Fields. 1 Introduction Supervised NER models require large amounts of (manually) labeled data to achieve good performance, data that often is hard to acquire or generate. However, it is possible to take advantage of unlabeled data to learn word representations to enrich and boost supervised NER models learned over small gold standards. In supervised NER the common practice has been to use domain-specific lexicon (list of words related with named entity types) (Carreras et al., 2002; Ratinov and Roth, 2009; Passos et al., 2014). More recently, it has been shown that supervised NER can be boosted via specific word features induced from very large unsupervised word representations (Turian et al., 2010), and in particular, from (i) very large word clusters (Brown et al., 1992; Liang, 2005), and (ii) very large word embeddings (Collobert and Weston, 2008; Mikolov et al., 2013a; Mikolov et al., 2 Related work 2.1 Spanish NER The first results (CoNLL 2002 shared-task1 ) for (supervised) Spanish NER were obtained by Carreras et al. (2002) where a set of selected word features and lexicons (gazetteers)"
W16-2705,P05-1045,0,0.0174015,"ther hand, English Wikipedia. For Spanish this is a novel approach. The experimental results show it achieves competitive performance w.r.t. the current (Deep learningdriven) state-of-the-art for Spanish NER, in particular when using cross- or multi-lingual Word Representations. 4.1 w1 t2 based on word representations in order to take advantage of unlabeled data, as depicted in Figure 1. The classifier was implemented using CRFSuite (Okazaki, 2007), due to its simplicity and the ease with which one can add extra features. Additionally, we experimented with the Stanford CRF classifier for NER (Finkel et al., 2005), for comparison purposes. 4.2 Baseline Features The baseline features were defined over a window of ± 2 tokens. The set of features for each word was: • • • • NER Model We used for our NER experiments a linear chain CRF sequence classifier, whose main properties we briefly recall (for a detailed description of this known model please refer to Sutton and McCallum (2012)). Linear chain CRFs are discrimative probabilistic graphical models that work by estimating the conditional probability of label sequence t given word sequence (sentence) w:   |t |#(F ) X X 1 p(t|w) = exp  θj fj (ti1 , ti ,"
W16-2705,W02-2024,0,0.0121418,"er, all characters are letters or digits. Prefixes and suffixes: four first or latter letters respectively. Digit length: whether the current token has 2 or 4 length. Digit combination: which digit combination the current token has (alphanumeric, slash, comma, period). Whether the current token has just uppercase letter and period mark or contains an uppercase, lowercase, digit, alphanumeric, symbol character. Flags for initial letter capitalized, all letter capitalized, all lower case, all digits, all nonalphanumeric characters, CoNLL 2002 Spanish Corpus The CoNLL 2002 shared task (Tjong Kim Sang, 2002) gave rise to a training and evaluation 36 LOC 6 983 MISC 2 958 ORG 10 490 PER 6 278 Brown Clusters 011100010 011100010 011100010 0111100011010 0111100011010 0111100011010 0111111001111 0111111001111 011101001010 011101001010 011101001010 Table 1: Entities in CoNLL-2002 (Spanish). standard for supervised NER algorithms used ever since: the CoNLL-2002 Spanish corpus. The CoNLL is tagged with four entities: PERSON, ORGANIZATION, LOCATION, MISCELLANEOUS and nine classes: B-PER, I-PER, BORG, I-ORG, B-LOC, I-LOC, B-MISC, I-MISC and O. In this corpus there are 74 683 tokens and 11 755 sentences. Add"
W16-2705,D14-1012,0,0.358457,"niversidad Cat´olica San Pablo, Arequipa, Peru {jenny.copara,jeochoa}@ucsp.edu.pe 2 Data & Web Science Group, Universit¨at Mannheim, Germany {camilo,goran}@informatik.uni-mannheim.de Abstract 2013b; dos Santos and Guimar˜aes, 2015). For English NER, (Passos et al., 2014; Guo et al., 2014) show that (large) word embeddings yield better results than clustering. However, when combined and fed as features to linear chain conditional random field (CRF) sequence classifiers, they yield models comparable to state-of-the-art deep learning approaches, but with the added value of a very large coverage (Guo et al., 2014). In this paper we investigate whether these techniques can be successfully applied to NER in Spanish. In order to do so, we follow Guo et al. (2014)’s approach combining probabilistic graphical models learned from the CoNLL 2002 corpus, with word representations learned from large unlabeled Spanish corpora, while exploring the optimal setting and feature combinations that match state-of-the-art algorithms for NER in Spanish. The paper is organized as follows. In Section 2, we provide a review of Spanish NER, and NER using word representations as features. Section 3 describes the structure of"
W16-2705,P10-1040,0,0.0711068,"imensions with a binarized value different to zero will be considered). Clustering Embeddings For cluster embeddings, 500, 1000, 1500, 2000 and 3000 clusters were computed, to model different levels of granularity (Guo et al., 2014). As features for each word w, we return the cluster assignments at each granularity level. Table 4 shows the clusters of embeddings computed for word “Maria”. The first column denotes the level of granularity. The second column denotes the cluster assigned to “Maria” at Brown clustering The number k of word clusters for Brown clustering was fixed to 1000 following Turian et al. (2010). Sample Brown clusters are shown in Table 2. The cluster is used as feature of each word in the annotated CoNLL 2002. As the reader can see Brown clustering tends to 3 Word Franc¸aise Hamburg Peru latino sueco conservador malogran paralizaban Facebook Twitter Internet http://crscardellino.me/SBWCE/ 37 Granularity 500 1000 1500 2000 3000 k 31 978 1317 812 812 Model Baseline +Binarization +Brown +Prototype +Clustering +Clustering+Prototype +Brown+Clustering +Brown+Clustering+Prototype +Brown+Clustering+Prototype∗ Carreras et al. (2002)† Carreras et al. (2002) Finkel et al. (2005) dos Santos and"
W16-2705,N16-1030,0,0.0206801,"ter embeddings (dos 1 http://www.cnts.ua.ac.be/conll2002/ner/ 34 Proceedings of the Sixth Named Entity Workshop, joint with 54th ACL, pages 34–40, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics Clustering embeddings A clustering method for embeddings based on k-means has been proposed in Yu et al. (2013b). Experiments have shown different numbers for k’s which contains different granularity information. The toolkit Sofia-ml (Sculley, 2010) 2 was used to do so. Santos and Guimar˜aes, 2015), Recurrent Neural Networks (RNNs) with word and character embeddings (Lample et al., 2016; Yang et al., 2016), and a character-based RNN with characters encoded as bytes (Gillick et al., 2015). 2.2 Word Representations Binarized embeddings The idea behind this method is to “reduce” continous word vectors w ~ in standard word embeddings, into discrete bin(w) ~ vectors, that however preserve the ordering or ranking of the embeddings. To do this, we need to compute two thresholds per dimension (upper and lower) across the whole vocabulary. For each dimension (component) i of is computed the mean of positives values (Ci+ , the upper threshold) and negative values (Ci− , the lower one)"
W16-2705,P13-2056,0,0.0211982,"n dimensions also known as a “word embedding” (Mikolov et al., 2013a). With (Brown) clustering, words that appear in the same or a similar sentence context are assigned to the same cluster. Whereas in word embeddings similar words occur close to each other in Rn (the induced n dimensional vector space). Word Representations work better the more data they are fed. One way to achieve this is to input them cross-lingual datasets, provided they overlap in vocabulary and domain. Cross-lingual Word Representations have been shown to improve several NLP tasks, such as model learning(Bhattarai, 2013; Yu et al., 2013a). This is because, among other things, they allow to extend the coverage of possibly limited (in the sense of small or sparsely annotated) resources through Word Representations in other languages. For instance, using English to enrich Chinese (Yu et al., 2013a), or learning a model in English to solve a Text Classification task in German (also German-English, English-French and French-English) (Bhattarai, 2013). 3   U+ , φ(Cij ) = B− ,   0, if Cij ≥ mean(Ci+ ), if Cij ≤ mean(Ci− ), otherwise. Distributional Prototypes This approach is based on the idea that each entity class has a set"
W16-2705,N13-1063,0,0.0173142,"n dimensions also known as a “word embedding” (Mikolov et al., 2013a). With (Brown) clustering, words that appear in the same or a similar sentence context are assigned to the same cluster. Whereas in word embeddings similar words occur close to each other in Rn (the induced n dimensional vector space). Word Representations work better the more data they are fed. One way to achieve this is to input them cross-lingual datasets, provided they overlap in vocabulary and domain. Cross-lingual Word Representations have been shown to improve several NLP tasks, such as model learning(Bhattarai, 2013; Yu et al., 2013a). This is because, among other things, they allow to extend the coverage of possibly limited (in the sense of small or sparsely annotated) resources through Word Representations in other languages. For instance, using English to enrich Chinese (Yu et al., 2013a), or learning a model in English to solve a Text Classification task in German (also German-English, English-French and French-English) (Bhattarai, 2013). 3   U+ , φ(Cij ) = B− ,   0, if Cij ≥ mean(Ci+ ), if Cij ≤ mean(Ci− ), otherwise. Distributional Prototypes This approach is based on the idea that each entity class has a set"
W16-2705,N16-1155,0,\N,Missing
W17-0810,cybulska-vossen-2014-using,0,0.442561,"tion 5 we discuss the results. In Section 6 we describe the comparative analysis. Section 7 concludes the paper. 2 Besides the work at these shared tasks, several authors proposed different schemes for event annotation, considering both the linguistic level and the conceptual one. The NewsReader Project (Vossen et al., 2016; Rospocher et al., 2016; Agerri and Rigau, 2016) is an initiative focused on extracting information about what happened to whom, when, and where, processing a large volume of financial and economic data. Within this project, in addition to description schemes (e.g., ECB+. (Cybulska and Vossen, 2014a)) and multilingual semantically annotated corpus of Wikinews articles (Minard et al., 2016a), van Son et al. (2016) propose a framework for annotating perspectives in texts using four different layers, i.e., events, attribution, factuality, and opinion. In the NewsReader Project the annotation is based on the guidelines to detect and annotate markables and relations among markables (Speranza and Minard, 2014). In the detection and annotation of markables, the authors distinguish among entities and entity mention in order to “handle both the annotation of single mentions and of the coreferenc"
W17-0810,J05-1004,0,0.151362,"ach property is associated with some association rules that specify the constraints related to both its syntactic behaviors and the pertinence and the intension of the property itself. In other words, these association rules contribute to the description of the way in which entity classes can be combined through properties in sentence contexts. To formalize such rules in the form of a set of axioms, we take in consideration the possibility of combining semantic and lexical behaviors, suitable for identifying specific event patterns. Thus, for inOur functional annotation differs from PropBank (Palmer et al., 2005) definitions of semantic roles as we do not delineate our functional roles through a verb-by-verb analysis. More concretely, PropBank adds predicate-argument relations to the syntactic trees of the Penn Treebank, representing these relations as framesets, which describe the different sets of roles required for the different meanings of the verb. In contrast, our analysis aims to describe the focus of an event mention by means of identifying actions, which can involve also other lexical elements in addition to the verb. This is easily demonstrated through the example “fire broke out” from Figur"
W17-0810,L16-1699,0,0.0342139,"Missing"
W17-0810,E12-2021,0,0.0621506,"Missing"
W17-0810,W03-0419,0,0.0269221,"Missing"
W17-0810,L16-1187,0,0.0663059,"Missing"
W17-2906,S16-2016,1,0.866077,"Missing"
W17-2906,E17-2109,1,0.849705,"Missing"
W17-2906,D14-1162,0,0.0907744,"tion to topically label political texts (Karan et al., 2016; Zirn et al., 2016). Existing classification models utilize discrete representation of text (i.e., bag of words) and can thus exploit only monolingual data (i.e., train and predict same language instances ). In contrast, in this work, we aim to exploit multilingual data – topically-coded CMP manifestos in different languages. We propose a classification model that can be trained on multilingual corpus of political texts.To this effect, we induce semantic representations of texts from ubiquitous word embeddings (Mikolov et al., 2013b; Pennington et al., 2014) and induce a joint multilingual embedding space via the linear translation matrices (Mikolov et al., 2013a). We then experiment with two classification models, support vector machines (SVM) and convolutional neural network (CNN) that use embeddings from the joint multilingual space as input. Experimental results offer evidence that topic classifiers leveraging multilingual training sets outperform monolingual classifiers. In this paper, we propose an approach for cross-lingual topical coding of sentences from electoral manifestos of political parties in different languages. To this end, we ex"
W17-2906,W16-2102,1,0.781964,"Missing"
W17-2906,D14-1181,0,0.0034212,"rmed using the linear translation model proposed by Mikolov et al. (2013a), who observed that there exists a linear translation between embedding spaces independently trained on different corpora. Given a set of N word translations pairs {wsi , wti }N i=1 , we learn a translation matrix M that projects the embedding vectors from the source space to the target space. Let S be the matrix composed of embeddings of all source words wsi Convolutional Neural Network Recently, convolutional neural networks (LeCun and Bengio, 1998, CNN) have yielded best performance on many text classification tasks (Kim, 2014; Severyn and Moschitti, 2015). CNN is a feed-forward neural network consisting of one or more convolution layers. Each convolution layer consists of a set of filters matrices (parameters of the model optimized during training). In text classification, the convolution operation is computed sequentially between each filter matrix and each slice (of the same size as filter) of the embedding matrix representing the input text. Each convolution layer is coupled with a pooling layer, in which only the subset of largest convolution scores produced by each filter is retained and used as input either"
W17-2906,D13-1010,0,0.0198065,"difficult to ensure annotation consistency, especially across different countries and languages (Mikhaylov et al., 2012). Nonetheless, manually coded manifestos remain the crucial data source for studies in computational political science (Lowe et al., 2011; Nanni et al., 2016). 2 Related Work The recent adoption of NLP methods had led to significant advances in the field of Computational Social Science (CSS) (Lazer et al., 2009) and political science in particular (Grimmer and Stewart, 2013). Among other tasks, researchers have addressed the identification of political differences from text (Sim et al., 2013; Menini and Tonelli, 2016), positioning of political entities on a leftright spectrum (Slapin and Proksch, 2008; Glavaˇs et al., 2017), as well as the detection of political events (Nanni et al., 2017) and prominent topics (Lauscher et al., 2016) in political texts. For what concerns the analysis of manifestos, 1 https://manifestoproject.wzb.eu/ coding_schemes/mp_v5 42 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 42–46, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics previous studies have focused"
W17-2906,C16-1232,0,0.0605231,"e annotation consistency, especially across different countries and languages (Mikhaylov et al., 2012). Nonetheless, manually coded manifestos remain the crucial data source for studies in computational political science (Lowe et al., 2011; Nanni et al., 2016). 2 Related Work The recent adoption of NLP methods had led to significant advances in the field of Computational Social Science (CSS) (Lazer et al., 2009) and political science in particular (Grimmer and Stewart, 2013). Among other tasks, researchers have addressed the identification of political differences from text (Sim et al., 2013; Menini and Tonelli, 2016), positioning of political entities on a leftright spectrum (Slapin and Proksch, 2008; Glavaˇs et al., 2017), as well as the detection of political events (Nanni et al., 2017) and prominent topics (Lauscher et al., 2016) in political texts. For what concerns the analysis of manifestos, 1 https://manifestoproject.wzb.eu/ coding_schemes/mp_v5 42 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 42–46, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics previous studies have focused on topical segmentation (Gl"
W17-4201,balahur-etal-2010-sentiment,0,0.0652338,"Missing"
W17-4201,S07-1072,0,0.0964553,"Missing"
W17-4201,E17-4007,0,0.020231,"basis of two main goals: headlines that represent the abstract of the main event and headlines that promote one of the details in the news story (Bell, 1991; Nir, 1993). Furthermore, Iarovici and Amel 2 Related work Despite the fact that news values has been widely investigated in Social Science and journalism studies, not much attention has been paid to its automatic classification by the NLP community. In fact, even if news value classification may be applied in several user-oriented applications, e.g., news recommendation systems, and web search engines, few scholars (De Nies et al., 2012; Piotrkowicz et al., 2017) have been focused on this particular topic. Related to our work is the work on predicting 1 Proceedings of the 2017 EMNLP Workshop on Natural Language Processing meets Journalism, pages 1–6 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics emotions in news articles and headlines, which has been investigated from different perspectives and by means of different techniques. Strapparava and Mihalcea (2008) describe an experiment devoted to analyze emotion in news headlines, focusing on six basic emotions and proposing knowledgebased and corpus-based approac"
W17-4201,S07-1013,0,0.0580233,"e Cohen’s κ and F1-macro IAA agreement scores for the 11 news value labels. We observe a moderate agreement of κ ≥ 0.4 (Landis and Koch, 1977) only for the “Bad news”, “Celebrity”, and “Entertainment” news values, suggesting that recognizing news values from headlines is a difficult task even for humans. To obtain the final dataset, we adjudicated the annotations of the three annotators my a majority vote. The adjudicated IAA is moderate/substantial, except for “Magnitude”, “Shareability”, and “Surprise”. Dataset As a starting point, we adopt the dataset proposed for the SemEval-2007 Task 14 (Strapparava and Mihalcea, 2007). The dataset consists of 1250 headlines extracted from major newspapers such as New York Times, CNN, BBC News, and Google News. Each headline has been manually annotated for valence and six emotions (Anger, Disgust, Fear, Joy, Sadness, and Surprise) on a scale from 0 to 100. In this work, we use only the emotion labels, and not the valence labels. News values. On top of the emotion annotations, we added an additional layer of news value labels. Our starting point for the annotation was the news values classification scheme proposed by Harcup and O’Neill (2016). This study proposes a set of fi"
W17-4201,C00-2137,0,0.18962,"two positive emotions, joy and surprise, which are the kernels of two sub-groups: joy is related to “Good news”, “Shareability” and, to a lesser extent, to “Magnitude”, while surprise emotions relates to “Entertainment” and “Surprise” news values. 3 SVM News value Bad news Celebrity Conflict Drama Entertainment Good news Magnitude Power elite Shareability T 0.652 0.553 0.526 0.636 0.832 0.414 0.299 0.596 0.309 the models. Models for the remaining nine news values were trained successfully and outperform a random baseline (the differences are significant at p<0.001; two-sided permutation test (Yeh, 2000)). We can make three main observations. First, there is a considerable variance in performance across the news values: “Bad news” and “Entertainment” seems to be the easiest to predict, whereas “Shareability”, “Magnitude”, and “Celebrity” are more difficult. Secondly, by comparing “T” and “T+E” variants of the models, we observe that adding emotions as features improves leads to further improvements for the “Bad news” and “Entertainment” news values (differences are significant at p<0.05) for CNN, and for SVM also for “Magnitude”, but for other news values adding emotions did not improve the p"
W17-6809,W11-4533,0,0.0171607,"ures. STS measures exploiting visual signal alone are shown to outperform, in some settings, linguistic-only measures by a wide margin, whereas multi-modal measures yield further performance gains. We also show that selective inclusion of visual information may further boost performance in the multi-modal setup. 1 Introduction Semantic textual similarity (Agirre et al., 2012, 2015, inter alia) measures the degree of semantic equivalence between short texts, usually pairs of sentences. Despite the obvious applicability to sentence alignment for machine translation (MT) (Resnik and Smith, 2003; Aziz and Specia, 2011) or plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), cross-lingual STS models were proposed only recently (Agirre et al., 2016; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fa"
W17-6809,S12-1059,0,0.0677119,"Missing"
W17-6809,R11-1055,0,0.106629,"gnitive science clearly suggests that human meaning representations are grounded in our perceptual system and sensori-motor experience (Harnad, 1990; Lakoff and Johnson, 1999; Louwerse, 2011, inter alia), previous STS models relied exclusively on linguistic processing and textual information. To the best of our knowledge, there has not yet been an STS method that leveraged visual information and combined linguistic and visual input into a visually-informed multi-modal STS system. However, such visually-informed models have been successfully used in other tasks such as selectional preferences (Bergsma and Goebel, 2011), detecting semantic similarity and relatedness (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova et al., 2016), to name only a few. Another important property of visual data is their expected language invariance,1 exploited in recent work on multi-modal modeling in cross-lingual settings (Bergsma and Durme, 2011; Kiela et al., 2015; Vuli´c et al., 2016; Specia et al., 2016). Supported by these findings, in this work we show that our multi-modal STS framework may be straightforwardly ext"
W17-6809,S16-1089,0,0.0314869,"Missing"
W17-6809,S13-1005,0,0.014395,"sed STS Measures In the previous section we explained the different levels at which we may combine visual and linguistic representations. However, we still have to define the actual STS measures that compute similarity scores for given pairs of sentences. We propose two simple unsupervised scores for measuring textual similarity. Both scores are agnostic of the actual modality used: this means that we can swap linguistic, visual, and multi-modal vectors as desired without altering the actual STS measure. Optimal aligment similarity. Following the ideas from successful unsupervised STS models (Han et al., 2013; Sultan et al., 2014), we aim to align words between the two sentences at hand. Aiming to devise language-independent STS models (i.e., language-specific tools that could help better align the words are off-limits), we can resort to word similarity measures as the sole information source guiding the alignment process. This STS measure is based on the optimal alignment between the words of the two input sentences. Given the similarity scores for all pairs of words between the sentences S1 and S2 , we are looking for an alignment {(wSi 1 , wSi 2 )}N i=1 (N is the number of aligned pairs, equal"
W17-6809,S16-1116,0,0.0124176,"show that selective inclusion of visual information may further boost performance in the multi-modal setup. 1 Introduction Semantic textual similarity (Agirre et al., 2012, 2015, inter alia) measures the degree of semantic equivalence between short texts, usually pairs of sentences. Despite the obvious applicability to sentence alignment for machine translation (MT) (Resnik and Smith, 2003; Aziz and Specia, 2011) or plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), cross-lingual STS models were proposed only recently (Agirre et al., 2016; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity (Silberer and Lapata, 2012; Bruni et al., 2014;"
W17-6809,P16-4010,0,0.0893966,"in all experiments).2 Example images for the four languages we consider in our experiments (cf. Section 5) are shown in Figure 1. We next run a deep convolutional neural network (CNN) pre-trained on the ImageNet classification task (Russakovsky et al., 2015) and extract the 4096dimensional vector from the pre-softmax layer to represent each image. We opt for the VGG network (Simonyan and Zisserman, 2014) which, according to Kiela et al. (2016), has a slight edge on the two other alternatives – AlexNet (Krizhevsky et al., 2012) and GoogLeNet (Szegedy et al., 2015). We used the MMFeat toolkit (Kiela, 2016) to facilitate the process of image retrieval and CNN-based feature extraction. Visual similarity. Because we retrieve more than one image per word, our visual representation of the word is a set of image embedding vectors. This allows for different visual similarity measures taking as input two sets of image embeddings (Kiela et al., 2015), given in Table 1. 3.3 Multi-Modal Representations In order to compute multi-modal STS scores, one can combine linguistic and visual embeddings of words and sentences in a number of ways. Here, we explore three different levels of combining visual and lingu"
W17-6809,D14-1005,0,0.57417,". These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), predicting phrase compositionality (Roller and Schulte Im Walde, 2013), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova 1 et al., 2016). While still predominantly applied in monolingual settings, representations originating from the visual modality are inherently language-invariable (Bergsma and Durme, 2011; Kiela et al., 2015). As such, they could serve as a natural cross-language bridge in cross-lingual STS. In this work, we investigate unsupervised multi-modal and cross-lingual STS models that leverage visual information from images alongside linguisti"
W17-6809,P14-2135,0,0.277321,"f similarity scores rather than at the embedding level. Thus, it may be applied both for computing word and sentence similarities. Let sim v be the similarity measure (cf. Section 4) for two words or sentences computed using their visual representations, and let sim t be their similarity computed using their linguistic representations. The late-fusion similarity is then computed as the linear combination of the uni-modal similarities, i.e., as a · sim v + b · sim t . The default late-fusion model uses a = b = 0.5. Selective inclusion of visual information. Previous studies (Hill et al., 2013; Kiela et al., 2014) show that visual signal does not improve the semantic representation equally for all concepts. In fact, the inclusion of visual information deteriorates semantic representations for abstract concepts (e.g., honesty, love, freedom). In order to selectively include the visual information, we need a measure reflecting the quality of the visual signal. To this end, we use the image dispersion score (Kiela et al., 2014). A concept’s image dispersion is the cosine distance between image embeddings, averaged over all pairs of images obtained for the concept w: id (w) = 1 X I(w) 2  1 − cos(ei , ej )"
W17-6809,P15-2020,1,0.923477,"Missing"
W17-6809,D16-1043,0,0.672896,"Missing"
W17-6809,D15-1015,1,0.919942,"Missing"
W17-6809,J99-4009,0,0.936816,") measures the degree of semantic equivalence between short texts, usually pairs of sentences. Despite the obvious applicability to sentence alignment for machine translation (MT) (Resnik and Smith, 2003; Aziz and Specia, 2011) or plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), cross-lingual STS models were proposed only recently (Agirre et al., 2016; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), predicting phrase compositionality (Roller and Schulte Im Walde, 2013), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova 1 et al., 2016). W"
W17-6809,D14-1162,0,0.0807577,"ision, as we were unable to consistently retrieve images for whole sentences as queries. 3.1 Linguistic Representations We use the ubiquitous word embeddings as the linguistic representations of words. Aiming to make our approach language-independent, we opted for embedding models that require nothing but the large corpora as input. Due to the common 1 Using a simple example from Vuli´c et al. (2016), bicycles resemble each other irrespective of whether we call them bicycle, v´elo, fiets, bicicletta, or Fahrrad; see also Fig. 1 3 usage, we chose the Skip-Gram (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) embeddings. For the cross-lingual STS setting, the words of the two languages have to be projected to the same embedding space. To achieve this, we employ the translation matrix model of Mikolov et al. (2013), who have shown that the linear mapping can be established between independently trained embedding spaces. Given a set of translation pairs {si , ti }ni=1 , si ∈ Rds , ti ∈ Rdt (with ds and dt being the sizes of source and target embeddings, respectively), we obtain the translation matrix M ∈ Rds ×dt by minimizing the sum: n X ksi M − ti k2 i=1 Once learned, the matrix M is used to proje"
W17-6809,J03-3002,0,0.0405059,"al Similarity (STS) measures. STS measures exploiting visual signal alone are shown to outperform, in some settings, linguistic-only measures by a wide margin, whereas multi-modal measures yield further performance gains. We also show that selective inclusion of visual information may further boost performance in the multi-modal setup. 1 Introduction Semantic textual similarity (Agirre et al., 2012, 2015, inter alia) measures the degree of semantic equivalence between short texts, usually pairs of sentences. Despite the obvious applicability to sentence alignment for machine translation (MT) (Resnik and Smith, 2003; Aziz and Specia, 2011) or plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), cross-lingual STS models were proposed only recently (Agirre et al., 2016; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic"
W17-6809,D13-1115,0,0.125683,"Missing"
W17-6809,S12-1060,1,0.897428,"Missing"
W17-6809,N16-1020,0,0.0485671,"s relied exclusively on linguistic processing and textual information. To the best of our knowledge, there has not yet been an STS method that leveraged visual information and combined linguistic and visual input into a visually-informed multi-modal STS system. However, such visually-informed models have been successfully used in other tasks such as selectional preferences (Bergsma and Goebel, 2011), detecting semantic similarity and relatedness (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova et al., 2016), to name only a few. Another important property of visual data is their expected language invariance,1 exploited in recent work on multi-modal modeling in cross-lingual settings (Bergsma and Durme, 2011; Kiela et al., 2015; Vuli´c et al., 2016; Specia et al., 2016). Supported by these findings, in this work we show that our multi-modal STS framework may be straightforwardly extended to cross-lingual settings. 3 Multi-Modal Concept Representations Our multi-modal STS measures combine – at different fusion levels – linguistic and visual concept representations. We obtain linguistic and visual r"
W17-6809,D12-1130,0,0.369012,"16; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), predicting phrase compositionality (Roller and Schulte Im Walde, 2013), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova 1 et al., 2016). While still predominantly applied in monolingual settings, representations originating from the visual modality are inherently language-invariable (Bergsma and Durme, 2011; Kiela et al., 2015). As such, they could serve as a natural cross-language bridge in cross-lingual STS. In this work, we investigate unsupervised multi-modal and cross-lingual STS models that leverage visua"
W17-6809,W16-2346,0,0.0396104,"Missing"
W17-6809,S14-2039,0,0.0157147,"n the previous section we explained the different levels at which we may combine visual and linguistic representations. However, we still have to define the actual STS measures that compute similarity scores for given pairs of sentences. We propose two simple unsupervised scores for measuring textual similarity. Both scores are agnostic of the actual modality used: this means that we can swap linguistic, visual, and multi-modal vectors as desired without altering the actual STS measure. Optimal aligment similarity. Following the ideas from successful unsupervised STS models (Han et al., 2013; Sultan et al., 2014), we aim to align words between the two sentences at hand. Aiming to devise language-independent STS models (i.e., language-specific tools that could help better align the words are off-limits), we can resort to word similarity measures as the sole information source guiding the alignment process. This STS measure is based on the optimal alignment between the words of the two input sentences. Given the similarity scores for all pairs of words between the sentences S1 and S2 , we are looking for an alignment {(wSi 1 , wSi 2 )}N i=1 (N is the number of aligned pairs, equal to the number of words"
W17-6809,P16-2031,1,0.632732,"Missing"
W17-6809,S15-2045,0,\N,Missing
W17-6809,S12-1051,0,\N,Missing
W17-6809,W13-2609,0,\N,Missing
W17-6809,S16-1081,0,\N,Missing
W18-5203,P11-1051,0,0.0239128,"ArguminSci: A Tool for Analyzing Argumentation and Rhetorical Aspects in Scientific Writing Anne Lauscher,1,2 Goran Glavaˇs,1 and Kai Eckert2 1 Data and Web Science Research Group University of Mannheim, Germany 2 Web-based Information Systems and Services Stuttgart Media University, Germany {anne, goran, simone}@informatik.uni-mannheim.de {lauscher, eckert}@hdm-stuttgart.de Abstract can be supported, such as the attribution of scientific statements to authors (Teufel and Moens, 2000), identification of research trends (McKeown et al., 2016), or automatic summarization of scientific articles (Abu-Jbara and Radev, 2011; Lauscher et al., 2017a). In this work, we present ArguminSci1 a tool that aims to support the holistic analyses of scientific publications in terms of scitorics, including the identification of argumentative components. We make ArguminSci publicly available for download.2 In its core, it relies on separate neural models based on recurrent neural networks with the long short-term memory cells (LSTM) (Hochreiter and Schmidhuber, 1997) pre-trained for each of the five tasks in the area of scientific publication mining that ArguminSci adresses, namely (1) argumentative component identification,"
W18-5203,L16-1492,0,0.237742,"Missing"
W18-5203,W15-1605,0,0.435768,"ument Component Background claim, Own claim, Data (coupled with B-I-O scheme) Discourse Role Background, Challenge, Approach, Outcome, Future work Citation Context Begin citation context, Inside citation context, Outside Subjective Aspect Advantage, Disadvantage, Adv.-disadv., Disadv.-adv., Novelty, Common practice, Limitation Summarization Relevance Totally irrelevant, Should not appear, May appear, Relevant, Very relevant term memory cells (Bi-LSTMs) (Hochreiter and Schmidhuber, 1997), one pre-trained for each of the five annotation tasks on our argumentatively extended Dr. Inventor corpus (Fisas et al., 2015, 2016; Lauscher et al., 2018). Model Descriptions. As ArguminSci addresses (1) two token-level sequence tagging tasks and (2) three sentence-level classification tasks, the system implements two types of models: • Token-level Sequence Labeling: Given a sentence x = (x1 , . . . , xn ) with words xi , we first lookup the vector representations ei (i.e., pre-trained word embeddings) of the words xi . Next, we run a Bi-LSTM and obtain the sentence-contextualized representation hi for each token: Table 1: Labels of ArguminSci’s annotation layers. Dataset. For training our models, we used an extens"
W18-5203,C12-3023,0,0.0232375,"Sci 22 Proceedings of the 5th Workshop on Argument Mining, pages 22–28 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 3.1 from PDF documents. The system embeds several modules for mining scientific text, e.g., for the discourse role characterization of sentences. Saggion et al. (2017) presented MultiScien, a tool that analyzes scientific text collections in English and Spanish and offers a visualization of discourse categories and summaries. Also, several systems analyzing argumentative zones (Teufel et al., 1999) have been made publicly available (e.g., Guo et al., 2012; Simsek et al., 2013). However, to the best of our knowledge, ArguminSci is the first publicly available system that provides fine-grained argumentative analysis of scientific publications, and allows for a joint analysis of scitorics – argumentation and several other rhetorical aspects of scientific language. Annotation Tasks. Our system supports the following aspects of rhetorical analysis (i.e., automatic annotation) of scientific writing: (1) argument component identification, (2) discourse role classification, (3) subjective aspect classification, (4) citation context identification, and"
W18-5203,N18-5005,0,0.0177515,"et of token tags Yaci . The label set is a combination of the standard B-I-O tagging scheme and three types of argumentative components, namely background claim, own claim, and data. Argument Mining Tools. Apart from new research models and approaches, several systems and software tools have been proposed for argument mining in other domains, mainly using machine-learning models at their core. Wachsmuth et al. (2017) developed args.me, an argument search engine that aims to support users in finding arguments and forming opinions on controversial topics.3 Another similar system is ArgumenText (Stab et al., 2018). In contrast to args.me, the search engine of ArgumenText provides access to sentential arguments extracted from large amounts of arbitrary text. The system most similar to ArguminSci is MARGOT (Lippi and Torroni, 2016),4 which extracts argumentative components from arbitrary text provided by the user. However, MARGOT is not tuned for a particular domain and does not perform well on scientific text (i.e., it cannot account for peculiarities of argumentative and rhetorical structures of scientific text). While MARGOT focuses only on argumentative components, ArguminSci allows for parallel anal"
W18-5203,E99-1015,0,0.658765,"insky): [Aôgjum&quot;Inski]. 2 https://github.com/anlausch/ ArguminSci 22 Proceedings of the 5th Workshop on Argument Mining, pages 22–28 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 3.1 from PDF documents. The system embeds several modules for mining scientific text, e.g., for the discourse role characterization of sentences. Saggion et al. (2017) presented MultiScien, a tool that analyzes scientific text collections in English and Spanish and offers a visualization of discourse categories and summaries. Also, several systems analyzing argumentative zones (Teufel et al., 1999) have been made publicly available (e.g., Guo et al., 2012; Simsek et al., 2013). However, to the best of our knowledge, ArguminSci is the first publicly available system that provides fine-grained argumentative analysis of scientific publications, and allows for a joint analysis of scitorics – argumentation and several other rhetorical aspects of scientific language. Annotation Tasks. Our system supports the following aspects of rhetorical analysis (i.e., automatic annotation) of scientific writing: (1) argument component identification, (2) discourse role classification, (3) subjective aspec"
W18-5203,W18-5206,1,0.365107,"Missing"
W18-5203,W99-0311,0,0.230741,"resent their work by embracing and exploiting established practices and specific tools related to the scientific discourse, such as citations (Gilbert, 1977), that facilitate building persuading argumentation lines. Consequently, scientific texts are abundant with different interrelated rhetorical and argumentative layers. In this work, we refer to this set of mutually-related rhetorical aspects of scientific writing as scitorics. Numerous research groups have already proposed computational models for analyzing scientific language with respect to one or multiple of these aspects. For example, Teufel and Moens (1999) presented experiments on the automatic assignment of argumentative zones, i.e., sentential discourse roles, to sentences in scientific articles. Similarly, there has been work on automatic classification of citations with respect to their polarity and purpose (Jha et al., 2017; Lauscher et al., 2017b). It has also been shown that through analyses of scitorics higher-level computational tasks 2 Related Work We divide the overview of related tools and systems into two categories: (1) systems targeting the analysis of scitorics and (2) tools for argument mining (in other domains). Tools for the"
W18-5203,W17-5106,0,0.0395111,"he task is to identify argumentative components in a sentence. That is, given a sentence x = (x1 , . . . , xn ) with individual words xi assign a sequence of labels yaci = (y1 , . . . , yn ) out of the set of token tags Yaci . The label set is a combination of the standard B-I-O tagging scheme and three types of argumentative components, namely background claim, own claim, and data. Argument Mining Tools. Apart from new research models and approaches, several systems and software tools have been proposed for argument mining in other domains, mainly using machine-learning models at their core. Wachsmuth et al. (2017) developed args.me, an argument search engine that aims to support users in finding arguments and forming opinions on controversial topics.3 Another similar system is ArgumenText (Stab et al., 2018). In contrast to args.me, the search engine of ArgumenText provides access to sentential arguments extracted from large amounts of arbitrary text. The system most similar to ArguminSci is MARGOT (Lippi and Torroni, 2016),4 which extracts argumentative components from arbitrary text provided by the user. However, MARGOT is not tuned for a particular domain and does not perform well on scientific text"
W18-5203,N16-1174,0,0.0549487,"d network and apply a softmax function on its output to predict the label probability distribution for each token: yi = softmax(W hi + b) , with W ∈ R2K×|Y |being the weight matrix, b ∈ R|Y |the bias vector, and K being the state size of the LSTMs. • Background claim: A statement of argumentative nature, which is about or closely related to the work of others or common practices in a research field or about background facts related to the topic of the publication. • Sentence-level Classification: The sentencelevel classification builds upon the output of the Bi-LSTM described above: Following Yang et al. (2016), we first obtain a sentence representation by aggregating the individual hidden representations of the words hi using an intrasentence attention mechanism defined as X si = αi hi . • Own claim: A statement of argumentative nature, which related to the authors own work and contribution. • Data: A fact that serves as evidence pro or against a claim. j More details on the argument-extended corpus we use to train our models can be found in the accompanying resource paper (Lauscher et al., 2018). For more details on the original annotation layers of the Dr. Inventor Corpus, we refer the reader to"
W19-4310,D15-1075,0,0.0303085,"´c3 1 Oracle Labs 2 Ubiquitous Knowledge Processing Lab (UKP-TUDA), TU Darmstadt 3 Language Technology Lab, TAL, University of Cambridge 4 Data and Web Science Group, University of Mannheim 1 aishwarya.kamath@oracle.com 2 pfeiffer@ukp.informatik.tu-darmstadt.de 3 {ep490,iv250}@cam.ac.uk 4 goran@informatik.uni-mannheim.de Abstract The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different rel"
W19-4310,N18-1045,0,0.0912161,"rke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 2017; Rei et al., 2018). Our POSTLE method belongs to the first group. Vuli´c and Mrkši´c (2018) proposed LEAR, a retrofitting LE model which displays performance gains on a spectrum of graded and ungraded LE evaluations compared to joint specialization models (Nguyen et al., 2017). However, LEAR still specializes only the vecto"
W19-4310,D18-1024,0,0.0380775,"Lingual LE Specialization Transfer The POSTLE models enable LE specialization of vectors of words unseen in lexical constraints. Conceptually, this also allows for a LE-specialization of a distributional space of another language (possibly without any external constraints), provided a shared bilingual distributional word vector space. To this end, we can resort to any of the methods for inducing shared cross-lingual vector spaces (Ruder et al., 2018). What is more, most recent methods successfully learn the shared space without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018; Chen and Cardie, 2018; Hoshen and Wolf, 2018). Let Xt be the distributional space of some target language for which we have no external lexical constraints and let P (x; θP ) : Rdt 7→ Rds be the (linear) function projecting vectors xt ∈ Xt to the distributional space Xds of the source language with available lexical constraints for which 3 Simply minimizing Euclidean distance also aligns vectors in terms of both direction and size. However, we consistently obtained better results by the objective function from Eq. (6). 75 with m = 2, 048 units and Leaky ReLU (slope 0.2) (Maas et al., 2014) for the generator. The d"
W19-4310,W09-0215,0,0.0509636,"urthermore, transfers with unsupervised (Ar, Co) and supervised bilingual mapping (Sm) yield comparable performance. This implies that a robust LE-specialization of distributional vectors for languages with no lexico-semantic resources is possible even without any bilingual signal or translation effort. 5 able methodology for specializing all distributional word vectors for the LE relation as well. Modeling Lexical Entailment. Extensive research effort in lexical semantics has been dedicated to automatic detection of the fundamental taxonomic LE relation. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et"
W19-4310,W13-3520,0,0.227478,"Missing"
W19-4310,P18-1073,0,0.375664,"LE specialization 2a. POSTLE: LE specialization of all words in the source lang observed with different input distributional spaces in several LE-related tasks such as hypernymy detection and directionality, and graded lexical entailment. What is more, the highest gains are reported for resource-lean data scenarios where a high percentage of words in the datasets is unseen. Finally, we show how to LE-specialize distributional spaces for target languages that lack external lexical knowledge. POSTLE can be coupled with any model for inducing cross-lingual embedding spaces (Conneau et al., 2018; Artetxe et al., 2018; Smith et al., 2017). If this model is unsupervised, the procedure effectively yields a zero-shot LE specialization transfer, and holds promise to support the construction of hierarchical semantic networks for resource-lean languages in future work. Source Lang Target Lang Distributional word vectors LE-specialized word vectors 2b. POSTLE: LE specialization of all words in the target lang Figure 1: High-level overview of a) the POSTLE full vocabulary specialization process; and b) zero-shot crosslingual specialization for LE. This relies on an initial shared cross-lingual word embedding space"
W19-4310,E12-1004,0,0.190005,"n, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 2017; Rei et al., 2018). Our POSTLE method belongs to the first group. Vuli´c and Mrkši´c (2018) proposed LEAR, a retrofitting LE model which displays performance gains on a spectrum of graded and ungraded LE evaluations compared to joint specialization models (Nguyen et al., 2017). However, LEAR still specializes only the vectors of words seen in external resources. The same limitation holds for a family of recent models that embed concept hierarchies (i.e., trees or directed acyclic graphs) in hyperbolic spaces (Nickel and Kiela"
W19-4310,N15-1184,0,0.28703,"Missing"
W19-4310,W11-2501,0,0.321138,"Missing"
W19-4310,I13-1095,0,0.173251,"UDA), TU Darmstadt 3 Language Technology Lab, TAL, University of Cambridge 4 Data and Web Science Group, University of Mannheim 1 aishwarya.kamath@oracle.com 2 pfeiffer@ukp.informatik.tu-darmstadt.de 3 {ep490,iv250}@cam.ac.uk 4 goran@informatik.uni-mannheim.de Abstract The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader"
W19-4310,P05-1014,0,0.395253,"anguages with no lexico-semantic resources is possible even without any bilingual signal or translation effort. 5 able methodology for specializing all distributional word vectors for the LE relation as well. Modeling Lexical Entailment. Extensive research effort in lexical semantics has been dedicated to automatic detection of the fundamental taxonomic LE relation. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni e"
W19-4310,P18-1004,1,0.83187,"Missing"
W19-4310,S12-1012,0,0.0487335,"d (Ar, Co) and supervised bilingual mapping (Sm) yield comparable performance. This implies that a robust LE-specialization of distributional vectors for languages with no lexico-semantic resources is possible even without any bilingual signal or translation effort. 5 able methodology for specializing all distributional word vectors for the LE relation as well. Modeling Lexical Entailment. Extensive research effort in lexical semantics has been dedicated to automatic detection of the fundamental taxonomic LE relation. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or exter"
W19-4310,D17-1185,1,0.917865,"Missing"
W19-4310,P14-2050,0,0.263993,"Missing"
W19-4310,gonzalez-agirre-etal-2012-multilingual,0,0.0233013,"an adversarial unsupervised model fine-tuned with the closed-form Procustes solution (Conneau et al., 2018); 2) an unsupervised self-learning algorithm that iteratively bootstraps new bilingual seeds, initialized according to structural similarities of the monolingual spaces (Artetxe et al., 2018); 3) an orthogonal linear mapping with inverse softmax, supervised by 5K bilingual seeds (Smith et al., 2017). We test POSTLE-specialized Spanish and French word vectors on WN-Hy-ES and WN-Hy-FR, two equally sized datasets (148K word pairs) created by Glavaš and Ponzetto (2017) using the ES WordNet (Gonzalez-Agirre et al., 2012) and the FR WordNet (Sagot and Fišer, 2008). We perform a ranking evaluation: the aim is to rank LE pairs above pairs standing in other relations (meronyms, synonyms, antonyms, and reverse LE). We rank word pairs in the ascending order based on ILE , see Eq. (10). Sm .742 .786 Table 2: Average precision (AP) of POSTLE models in cross-lingual transfer. Results are shown for both POSTLE models ( DFFN and ADV ), two target languages (Spanish and French) and three methods for inducing bilingual vector spaces: Ar (Artetxe et al., 2018), Co (Conneau et al., 2018), and Sm (Smith et al., 2017). settin"
W19-4310,P15-1145,0,0.132792,"ace. Related Work Vector Space Specialization. In general, lexical specialization models fall into two categories: 1) joint optimization models and 2) post-processing or retrofitting models. Joint models integrate external constraints directly into the distributional objective of embedding algorithms such as Skip-Gram and CBOW (Mikolov et al., 2013), or Canonical Correlation Analysis (Dhillon et al., 2015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018) use external constraints to posthoc fine-tune distributional spaces. Effectively, this makes them applicable to any inpu"
W19-4310,K16-1006,0,0.0214983,"The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (Schwartz et al., 2015; Mrkši´c et al., 2017). To mitigate this deficiency, a standard solution is a post-processing step: distributional vectors are gradually refined to satisfy linguistic constraints extracted from external resources such as W"
W19-4310,P16-1193,0,0.0609727,"tion. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 2017; Rei et al., 2018). Our POSTLE method belongs to the first group. Vuli´c and Mrkši´c (2018) proposed LEAR, a retrofitting LE model which displays performance gains on a spectrum of graded and ungraded LE evaluations compared to joint specialization models (Nguyen et al., 2017). Howev"
W19-4310,P13-2078,0,0.0163404,"ny bilingual signal or translation effort. 5 able methodology for specializing all distributional word vectors for the LE relation as well. Modeling Lexical Entailment. Extensive research effort in lexical semantics has been dedicated to automatic detection of the fundamental taxonomic LE relation. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 20"
W19-4310,D18-1043,0,0.0379808,"on Transfer The POSTLE models enable LE specialization of vectors of words unseen in lexical constraints. Conceptually, this also allows for a LE-specialization of a distributional space of another language (possibly without any external constraints), provided a shared bilingual distributional word vector space. To this end, we can resort to any of the methods for inducing shared cross-lingual vector spaces (Ruder et al., 2018). What is more, most recent methods successfully learn the shared space without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018; Chen and Cardie, 2018; Hoshen and Wolf, 2018). Let Xt be the distributional space of some target language for which we have no external lexical constraints and let P (x; θP ) : Rdt 7→ Rds be the (linear) function projecting vectors xt ∈ Xt to the distributional space Xds of the source language with available lexical constraints for which 3 Simply minimizing Euclidean distance also aligns vectors in terms of both direction and size. However, we consistently obtained better results by the objective function from Eq. (6). 75 with m = 2, 048 units and Leaky ReLU (slope 0.2) (Maas et al., 2014) for the generator. The discriminator uses H = 2"
W19-4310,W13-0904,0,0.0234062,"TAL, University of Cambridge 4 Data and Web Science Group, University of Mannheim 1 aishwarya.kamath@oracle.com 2 pfeiffer@ukp.informatik.tu-darmstadt.de 3 {ep490,iv250}@cam.ac.uk 4 goran@informatik.uni-mannheim.de Abstract The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (Schwartz et al., 2015"
W19-4310,N15-1070,0,0.0220684,"is (Dhillon et al., 2015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018) use external constraints to posthoc fine-tune distributional spaces. Effectively, this makes them applicable to any input distributional space, but they modify only vectors of words seen in the external resource. Nonetheless, retrofitting models tend to outperform joint models in the context of both similarity-based (Mrkši´c et al., 2016) and LE specialization (Vuli´c and Mrkši´c, 2018). The recent post-specialization paradigm has been so far applied only to the symmetric semantic similarity relation."
W19-4310,P15-2020,1,0.934011,"Missing"
W19-4310,Q17-1022,1,0.906083,"Missing"
W19-4310,C14-1097,0,0.0609212,"Missing"
W19-4310,P16-2074,0,0.0372945,"015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018) use external constraints to posthoc fine-tune distributional spaces. Effectively, this makes them applicable to any input distributional space, but they modify only vectors of words seen in the external resource. Nonetheless, retrofitting models tend to outperform joint models in the context of both similarity-based (Mrkši´c et al., 2016) and LE specialization (Vuli´c and Mrkši´c, 2018). The recent post-specialization paradigm has been so far applied only to the symmetric semantic similarity relation. Vuli´c et al. (2018)"
W19-4310,P15-1173,0,0.0284036,"et al., 2013), or Canonical Correlation Analysis (Dhillon et al., 2015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018) use external constraints to posthoc fine-tune distributional spaces. Effectively, this makes them applicable to any input distributional space, but they modify only vectors of words seen in the external resource. Nonetheless, retrofitting models tend to outperform joint models in the context of both similarity-based (Mrkši´c et al., 2016) and LE specialization (Vuli´c and Mrkši´c, 2018). The recent post-specialization paradigm has been so far applied only to t"
W19-4310,E14-4008,0,0.21465,"n effort. 5 able methodology for specializing all distributional word vectors for the LE relation as well. Modeling Lexical Entailment. Extensive research effort in lexical semantics has been dedicated to automatic detection of the fundamental taxonomic LE relation. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 2017; Rei et al., 2018)"
W19-4310,N15-1100,0,0.51525,"of specific concepts. True LE pairs should display both a small cosine distance and a negative norm difference. Therefore, in different LE tasks we can rank the candidate pairs in the ascending order of their asymmetric LE distance ILE . The LE distances are trivially transformed into binary LE predictions, using a binarization threshold t: if ILE (x, y) < t, we predict that LE holds between words x and y with vectors x and y. Linguistic Constraints. We use the same set of constraints as LEAR in prior work (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialization (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (parrot, bird), (bird, animal), and (parrot, animal) are in the LE set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. 4 Evaluation and Results We extensively evaluate the proposed POSTLE models on two fundamental LE tasks: 1) predicting graded LE and 2) LE detection (and directiona"
W19-4310,Q16-1030,0,0.0860588,"alization. In general, lexical specialization models fall into two categories: 1) joint optimization models and 2) post-processing or retrofitting models. Joint models integrate external constraints directly into the distributional objective of embedding algorithms such as Skip-Gram and CBOW (Mikolov et al., 2013), or Canonical Correlation Analysis (Dhillon et al., 2015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018) use external constraints to posthoc fine-tune distributional spaces. Effectively, this makes them applicable to any input distributional space, but they modify"
W19-4310,K15-1026,0,0.132958,"(Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (Schwartz et al., 2015; Mrkši´c et al., 2017). To mitigate this deficiency, a standard solution is a post-processing step: distributional vectors are gradually refined to satisfy linguistic constraints extracted from external resources such as WordNet (Fellbaum, 1998) or BabelNet (Navigli and Ponzetto, 2012). This process, termed retrofitting or semantic specialization, is beneficial to language understanding tasks (Faruqui, 2016; Glavaš and Vuli´c, 2018) and is extremely versatile as it can be applied on top of any input distributional space. Retrofitting methods, however, have a major weakness: they only locally"
W19-4310,P16-1226,0,0.289881,"Missing"
W19-4310,D14-1162,0,0.0887635,".uni-mannheim.de Abstract The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (Schwartz et al., 2015; Mrkši´c et al., 2017). To mitigate this deficiency, a standard solution is a post-processing step: distributional vectors are gradually refined to satisfy linguistic constraints extracted from extern"
W19-4310,E17-1007,0,0.300758,"odology for specializing all distributional word vectors for the LE relation as well. Modeling Lexical Entailment. Extensive research effort in lexical semantics has been dedicated to automatic detection of the fundamental taxonomic LE relation. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 2017; Rei et al., 2018). Our POSTLE method bel"
W19-4310,N18-1202,0,0.0487797,"rchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (Schwartz et al., 2015; Mrkši´c et al., 2017). To mitigate this deficiency, a standard solution is a post-processing step: distributional vectors are gradually refined to satisfy linguistic constraints extracted from external resources such as WordNet (Fellbaum, 1998) or BabelNet (Navigli a"
W19-4310,D18-1026,1,0.791735,"Missing"
W19-4310,P06-1101,0,0.0982207,"up, University of Mannheim 1 aishwarya.kamath@oracle.com 2 pfeiffer@ukp.informatik.tu-darmstadt.de 3 {ep490,iv250}@cam.ac.uk 4 goran@informatik.uni-mannheim.de Abstract The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (Schwartz et al., 2015; Mrkši´c et al., 2017). To mitigate this deficiency"
W19-4310,D16-1039,0,0.0443433,"informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 2017; Rei et al., 2018). Our POSTLE method belongs to the first group. Vuli´c and Mrkši´c (2018) proposed LEAR, a retrofitting LE model which displays performance gains on a spectrum of graded and ungraded LE evaluations compared to joint specialization models (Nguyen et al., 2017). However, LEAR still specializes only the vectors of words seen in external resources. The same limitation holds for a family of recent models that embed concept hierarchies (i.e., trees or directed acyclic graphs) in hyperbolic spaces (Nickel and Kiela, 2017; Chamberlain"
W19-4310,P18-2101,1,0.874487,"Missing"
W19-4310,P16-1157,0,0.0247947,"erarchical relationships between concepts. Following previous work (Nickel and Kiela, 2017; Vuli´c and Mrkši´c, 2018), we evaluate graded LE on the standard HyperLex dataset (Vuli´c et al., 2017).5 HyperLex contains 2,616 word pairs (2,163 noun pairs, the rest are verb pairs) rated by humans by 4 We experiment with unsupervised and weakly supervised models for inducing cross-lingual embedding spaces. However, we stress that the POSTLE specialization transfer is equally applicable on top of any method for inducing crosslingual word vectors, some of which may require more bilingual supervision (Upadhyay et al., 2016; Ruder et al., 2018). 5 Graded LE is a phenomenon deeply rooted in cognitive science and linguistics: it captures the notions of concept prototypicality (Rosch, 1973; Medin et al., 1984) and category vagueness (Kamp and Partee, 1995; Hampton, 2007). We refer the reader to the original paper for a more detailed discussion. 76 LEAR DFFN ADV LEAR Spearman’s ρ correlation 0.65 Spearman’s ρ correlation DFFN ADV 0.65 0.55 0.45 0.35 0.55 0.45 0.35 0.25 0.25 0 30 50 70 Percentage of seen HyperLex words 90 100 0 (a) SGNS - BOW 2 30 50 70 Percentage of seen HyperLex words 90 100 (b) GLOVE Figure 2: Spe"
W19-4310,J17-4004,1,0.900171,"Missing"
W19-4310,N18-1048,1,0.863589,"Missing"
W19-4310,N18-1103,1,0.430208,"Missing"
W19-4310,C14-1212,0,0.497525,"Missing"
W19-4310,C04-1146,0,0.537145,"Missing"
W19-4310,Q15-1025,0,0.199464,"cal Correlation Analysis (Dhillon et al., 2015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018) use external constraints to posthoc fine-tune distributional spaces. Effectively, this makes them applicable to any input distributional space, but they modify only vectors of words seen in the external resource. Nonetheless, retrofitting models tend to outperform joint models in the context of both similarity-based (Mrkši´c et al., 2016) and LE specialization (Vuli´c and Mrkši´c, 2018). The recent post-specialization paradigm has been so far applied only to the symmetric semantic"
W19-4310,N18-1101,0,0.020584,"biquitous Knowledge Processing Lab (UKP-TUDA), TU Darmstadt 3 Language Technology Lab, TAL, University of Cambridge 4 Data and Web Science Group, University of Mannheim 1 aishwarya.kamath@oracle.com 2 pfeiffer@ukp.informatik.tu-darmstadt.de 3 {ep490,iv250}@cam.ac.uk 4 goran@informatik.uni-mannheim.de Abstract The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synon"
W19-4310,P14-2089,0,0.16496,"vocabulary words, and unlike joint models, it is computationally inexpensive and applicable to any distributional vector space. Related Work Vector Space Specialization. In general, lexical specialization models fall into two categories: 1) joint optimization models and 2) post-processing or retrofitting models. Joint models integrate external constraints directly into the distributional objective of embedding algorithms such as Skip-Gram and CBOW (Mikolov et al., 2013), or Canonical Correlation Analysis (Dhillon et al., 2015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018)"
W19-4310,D14-1161,0,0.314347,"norms than vectors of specific concepts. True LE pairs should display both a small cosine distance and a negative norm difference. Therefore, in different LE tasks we can rank the candidate pairs in the ascending order of their asymmetric LE distance ILE . The LE distances are trivially transformed into binary LE predictions, using a binarization threshold t: if ILE (x, y) < t, we predict that LE holds between words x and y with vectors x and y. Linguistic Constraints. We use the same set of constraints as LEAR in prior work (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialization (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (parrot, bird), (bird, animal), and (parrot, animal) are in the LE set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. 4 Evaluation and Results We extensively evaluate the proposed POSTLE models on two fundamental LE tasks: 1) predicting graded LE and 2) LE detect"
