2020.msr-1.1,The Third Multilingual Surface Realisation Shared Task ({SR}{'}20): Overview and Evaluation Results,2020,-1,-1,3,0.380407,5966,simon mille,Proceedings of the Third Workshop on Multilingual Surface Realisation,0,"This paper presents results from the Third Shared Task on Multilingual Surface Realisation (SR{'}20) which was organised as part of the COLING{'}20 Workshop on Multilingual Surface Realisation. As in SR{'}18 and SR{'}19, the shared task comprised two tracks: (1) a Shallow Track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (2) a Deep Track where additionally, functional words and morphological information were removed. Moreover, each track had two subtracks: (a) restricted-resource, where only the data provided or approved as part of a track could be used for training models, and (b) open-resource, where any data could be used. The Shallow Track was offered in 11 languages, whereas the Deep Track in 3 ones. Systems were evaluated using both automatic metrics and direct assessment by human evaluators in terms of Readability and Meaning Similarity to reference outputs. We present the evaluation results, along with descriptions of the SR{'}19 tracks, data and evaluation methods, as well as brief summaries of the participating systems. For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume."
2020.lrec-1.1,Neural Mention Detection,2020,-1,-1,2,0.964912,4243,juntao yu,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Mention detection is an important preprocessing step for annotation and interpretation in applications such as NER and coreference resolution, but few stand-alone neural models have been proposed able to handle the full range of mentions. In this work, we propose and compare three neural network-based approaches to mention detection. The first approach is based on the mention detection part of a state of the art coreference resolution system; the second uses ELMO embeddings together with a bidirectional LSTM and a biaffine classifier; the third approach uses the recently introduced BERT model. Our best model (using a biaffine classifier) achieves gains of up to 1.8 percentage points on mention recall when compared with a strong baseline in a HIGH RECALL coreference annotation setting. The same model achieves improvements of up to 5.3 and 6.2 p.p. when compared with the best-reported mention detection F1 on the CONLL and CRAC coreference data sets respectively in a HIGH F1 annotation setting. We then evaluate our models for coreference resolution by using mentions predicted by our best model in start-of-the-art coreference systems. The enhanced model achieved absolute improvements of up to 1.7 and 0.7 p.p. when compared with our strong baseline systems (pipeline system and end-to-end system) respectively. For nested NER, the evaluation of our model on the GENIA corpora shows that our model matches or outperforms state-of-the-art models despite not being specifically designed for this task."
2020.lrec-1.634,A Gold Standard Dependency Treebank for {T}urkish,2020,-1,-1,3,0,17913,tolga kayadelen,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We introduce TWT; a new treebank for Turkish which consists of web and Wikipedia sentences that are annotated for segmentation, morphology, part-of-speech and dependency relations. To date, it is the largest publicly available human-annotated morpho-syntactic Turkish treebank in terms of the annotated word count. It is also the first large Turkish dependency treebank that has a dedicated Wikipedia section. We present the tagsets and the methodology that are used in annotating the treebank and also the results of the baseline experiments on Turkish dependency parsing with this treebank."
2020.acl-main.173,On Faithfulness and Factuality in Abstractive Summarization,2020,54,0,3,0,8653,joshua maynez,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria."
2020.acl-main.577,Named Entity Recognition as Dependency Parsing,2020,29,0,2,0.964912,4243,juntao yu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points."
W19-8012,Recursive {LSTM} Tree Representation for Arc-Standard Transition-Based Dependency Parsing,2019,0,0,2,1,1202,mohab elkaref,"Proceedings of the Third Workshop on Universal Dependencies (UDW, SyntaxFest 2019)",0,None
D19-6301,The Second Multilingual Surface Realisation Shared Task ({SR}{'}19): Overview and Evaluation Results,2019,0,2,3,0.465617,5966,simon mille,Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019),0,"We report results from the SR{'}19 Shared Task, the second edition of a multilingual surface realisation task organised as part of the EMNLP{'}19 Workshop on Multilingual Surface Realisation. As in SR{'}18, the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally, functional words and morphological information were removed. The shallow track was offered in eleven, and the deep track in three languages. Systems were evaluated (a) automatically, using a range of intrinsic metrics, and (b) by human judges in terms of readability and meaning similarity. This report presents the evaluation results, along with descriptions of the SR{'}19 tracks, data and evaluation methods. For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume."
W18-6527,Underspecified {U}niversal {D}ependency Structures as Inputs for Multilingual Surface Realisation,2018,0,1,3,0.493927,5966,simon mille,Proceedings of the 11th International Conference on Natural Language Generation,0,"In this paper, we present the datasets used in the Shallow and Deep Tracks of the First Multilingual Surface Realisation Shared Task (SR{'}18). For the Shallow Track, data in ten languages has been released: Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish. For the Deep Track, data in three languages is made available: English, French and Spanish. We describe in detail how the datasets were derived from the Universal Dependencies V2.0, and report on an evaluation of the Deep Track input quality. In addition, we examine the motivation for, and likely usefulness of, deriving NLG inputs from annotations in resources originally developed for Natural Language Understanding (NLU), and assess whether the resulting inputs supply enough information of the right kind for the final stage in the NLG process."
W18-3601,The First Multilingual Surface Realisation Shared Task ({SR}{'}18): Overview and Evaluation Results,2018,0,11,3,0.493927,5966,simon mille,Proceedings of the First Workshop on Multilingual Surface Realisation,0,"We report results from the SR{'}18 Shared Task, a new multilingual surface realisation task organised as part of the ACL{'}18 Workshop on Multilingual Surface Realisation. As in its English-only predecessor task SR{'}11, the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally, functional words and morphological information were removed. The shallow track was offered in ten, and the deep track in three languages. Systems were evaluated (a) automatically, using a range of intrinsic metrics, and (b) by human judges in terms of readability and meaning similarity. This report presents the evaluation results, along with descriptions of the SR{'}18 tracks, data and evaluation methods. For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume."
P18-1246,Morphosyntactic Tagging with a Meta-{B}i{LSTM} Model over Context Sensitive Token Encodings,2018,16,3,1,1,16528,bernd bohnet,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The rise of neural networks, and particularly recurrent neural networks, has produced significant advances in part-of-speech tagging accuracy. One characteristic common among these models is the presence of rich initial word encodings. These encodings typically are composed of a recurrent character-based representation with dynamically and pre-trained word embeddings. However, these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts. In this paper, we investigate models that use recurrent neural networks with sentence-level context for initial character and word-based representations. In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states."
K18-2011,"82 Treebanks, 34 Models: {U}niversal {D}ependency Parsing with Multi-Treebank Models",2018,13,0,2,0,29069,aaron smith,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We present the Uppsala system for the CoNLL 2018 Shared Task on universal dependency parsing. Our system is a pipeline consisting of three components: the first performs joint word and sentence segmentation; the second predicts part-of-speech tags and morphological features; the third predicts dependency trees from words and tags. Instead of training a single parsing model for each treebank, we trained models with multiple treebanks for one language or closely related languages, greatly reducing the number of models. On the official test run, we ranked 7th of 27 teams for the LAS and MLAS metrics. Our system obtained the best scores overall for word segmentation, universal POS tagging, and morphological features."
W17-6302,Dependency Language Models for Transition-based Dependency Parsing,2017,-1,-1,2,1,4243,juntao yu,Proceedings of the 15th International Conference on Parsing Technologies,0,"In this paper, we present an approach to improve the accuracy of a strong transition-based dependency parser by exploiting dependency language models that are extracted from a large parsed corpus. We integrated a small number of features based on the dependency language models into the parser. To demonstrate the effectiveness of the proposed approach, we evaluate our parser on standard English and Chinese data where the base parser could achieve competitive accuracy scores. Our enhanced parser achieved state-of-the-art accuracy on Chinese data and competitive results on English data. We gained a large absolute improvement of one point (UAS) on Chinese and 0.5 points for English."
W17-3517,Shared Task Proposal: Multilingual Surface Realization Using {U}niversal {D}ependency Trees,2017,0,1,2,0.493927,5966,simon mille,Proceedings of the 10th International Conference on Natural Language Generation,0,"We propose a shared task on multilingual Surface Realization, i.e., on mapping unordered and uninflected universal dependency trees to correctly ordered and inflected sentences in a number of languages. A second deeper input will be available in which, in addition, functional words, fine-grained PoS and morphological information will be removed from the input trees. The first shared task on Surface Realization was carried out in 2011 with a similar setup, with a focus on English. We think that it is time for relaunching such a shared task effort in view of the arrival of Universal Dependencies annotated treebanks for a large number of languages on the one hand, and the increasing dominance of Deep Learning, which proved to be a game changer for NLP, on the other hand."
P16-1015,Generalized Transition-based Dependency Parsing via Control Parameters,2016,26,3,1,1,16528,bernd bohnet,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
W15-2201,Domain Adaptation for Dependency Parsing via Self-Training,2015,29,3,3,1,4243,juntao yu,Proceedings of the 14th International Conference on Parsing Technologies,0,"This paper presents a successful approach for domain adaptation of a dependency parser via self-training. We improve parsing accuracy for out-of-domain texts with a self-training approach that uses confidence-based methods to select additional training samples. We compare two confidence-based methods: The first method uses the parse score of the employed parser to measure the confidence into a parse tree. The second method calculates the score differences between the best tree and alternative trees. With these methods, we were able to improve the labeled accuracy score by 1.6 percentage points on texts from a chemical domain and by 0.6 on average on texts of three web domains. Our improvements on the chemical texts of 1.5% UAS is substantially higher than improvements reported in previous work of 0.5% UAS. For the three web domains, no positive results for self-training have been reported before."
W15-2133,{P}ars{P}er: A Dependency Parser for {P}ersian,2015,16,2,2,0,35106,mojgan seraji,Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015),0,"We present a dependency parser for Persian, called ParsPer, developed using the graph-based parser in the Mate Tools. The parser is trained on the entire Uppsala Persian Dependency Treebank with a ..."
W15-2138,Exploring Confidence-based Self-training for Multilingual Dependency Parsing in an Under-Resourced Language Scenario,2015,33,0,2,1,4243,juntao yu,Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015),0,This paper presents a novel self-training approach that we use to explore a scenario which is typical for under-resourced languages. We apply self-training on small multilingual dependency corpora of nine languages. Our approach employs a confidence-based method to gain additional training data from large unlabeled datasets. The method has been shown effective for five languages out of the nine languages of the SPMRL Shared Task 2014 datasets. We obtained the largest absolute improvement of two percentage points on Korean data. Our selftraining experiments show improvements upon the best state-of-the-art systems of the SPMRL shared task that employs one parser only.
P15-1165,Inverted indexing for cross-lingual {NLP},2015,24,47,5,0,143,anders sogaard,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We present a novel, count-based approach to obtaining inter-lingual word representations based on inverted indexing of Wikipedia. We present experiments applying these representations to 17 datasets in document classification, POS tagging, dependency parsing, and word alignment. Our approach has the advantage that it is simple, computationally efficient and almost parameter-free, and, more importantly, it enables multi-source crosslingual learning. In 14/17 cases, we improve over using state-of-the-art bilingual embeddings."
N15-3012,Visualizing Deep-Syntactic Parser Output,2015,19,1,3,0,2827,juan solercompany,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"xe2x80x9cDeep-syntacticxe2x80x9d dependency structures bridge the gap between the surface-syntactic structures as produced by state-of-the-art dependency parsers and semantic logical forms in that they abstract away from surfacesyntactic idiosyncrasies, but still keep the linguistic structure of a sentence. They have thus a great potential for such downstream applications as machine translation and summarization. In this demo paper, we propose an online version of a deep-syntactic parser that outputs deep-syntactic structures from plain sentences and visualizes them using the Brat tool. Along with the deep-syntactic structures, the user can also inspect the visual presentation of the surface-syntactic structures that serve as input to the deep-syntactic parser and that are produced by the joint tagger and syntactic transition-based parser ran in the pipeline before deep-syntactic parsing takes place."
N15-1042,Data-driven sentence generation with non-isomorphic trees,2015,36,15,2,0.746854,3435,miguel ballesteros,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Abstract structures from which the generation naturally starts often do not contain any func- tional nodes, while surface-syntactic struc- tures or a chain of tokens in a linearized tree contain all of them. Therefore, data-driven linguistic generation needs to be able to cope with the projection between non-isomorphic structures that differ in their topology and number of nodes. So far, such a projection has been a challenge in data-driven genera- tion and was largely avoided. We present a fully stochastic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cas- cade of SVM-classifier based submodules that map in a series of transitions the input struc- tures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered Ancora- UPF corpus."
W14-6105,Exploring Options for Fast Domain Adaptation of Dependency Parsers,2014,36,3,4,0,31547,viktor pekar,Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages,0,"The paper explores different domain-independent techniques to adapt a dependency parser trained on a general-language corpus to parse web texts (online reviews, newsgroup posts, weblogs): co-training, word clusters, and a crowd-sourced dictionary. We examine the relative utility of these techniques as well as different ways to put them together to achieve maximum parsing accuracy. While we find that co-training and word clusters produce the most promising results, there is little additive improvement when combining the two techniques, which suggests that in the absence of large grammatical discrepancies between the training and test domains, they address largely the same problem, that of unknown vocabulary, with word clusters being a somewhat more effective solution for it. Our highest results were achieved by a combination of word clusters and co-training, significantly improving on the baseline, by up to 1.67%. Evaluation of the best configurations on the SANCL-2012 test data (Petrov and McDonald, 2012) showed that they outperform all the shared task submissions that used a single parser to parse test data, averaging the results across all the test sets."
S14-2122,{UB}ham: Lexical Resources and Dependency Parsing for Aspect-Based Sentiment Analysis,2014,22,1,3,0,31547,viktor pekar,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper describes the system developed by the UBham team for the SemEval2014 Aspect-Based Sentiment Analysis task (Task 4). We present an approach based on deep linguistic processing techniques and resources, and explore the parameter space of these techniques applied to the different stages in this task and examine possibilities to exploit interdependencies between them."
C14-1076,Automatic Feature Selection for Agenda-Based Dependency Parsing,2014,35,13,2,0.746854,3435,miguel ballesteros,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"In this paper we present an in-depth study on automatic feature selection for beam-search dependency parsers. The search strategy is inherited from the one implemented in MaltOptimizer, but searches in a much larger set of feature templates that could lead to a higher number of combinations. Our models provide results that are on par with models trained with a larger set of feature templates, and this implies that our models provide faster training and parsing times. Moreover, the results establish the state of the art for some of the languages."
C14-1133,Deep-Syntactic Parsing,2014,39,11,2,0.746854,3435,miguel ballesteros,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"xe2x80x9cDeep-syntacticxe2x80x9d dependency structures that capture the argumentative, attributive and coordinative relations between full words of a sentence have a great potential for a number of NLPapplications. The abstraction degree of these structures is in-between the output of a syntactic dependency parser (connected trees defined over all words of a sentence and language-specific grammatical functions) and the output of a semantic parser (forests of trees defined over individual lexemes or phrasal chunks and abstract semantic role labels which capture the argument structure of predicative elements, dropping all attributive and coordinative dependencies). We propose a parser that delivers deep syntactic structures as output."
Q13-1034,Joint Morphological and Syntactic Analysis for Richly Inflected Languages,2013,50,49,1,1,16528,bernd bohnet,Transactions of the Association for Computational Linguistics,0,"Joint morphological and syntactic analysis has been proposed as a way of improving parsing accuracy for richly inflected languages. Starting from a transition-based model for joint part-of-speech tagging and dependency parsing, we explore different ways of integrating morphological features into the model. We also investigate the use of rule-based morphological analyzers to provide hard or soft lexical constraints and the use of word clusters to tackle the sparsity of lexical features. Evaluation on five morphologically rich languages (Czech, Finnish, German, Hungarian, and Russian) shows consistent improvements in both morphological and syntactic accuracy for joint prediction over a pipeline model, with further improvements thanks to lexical constraints and word clusters. The final results improve the state of the art in dependency parsing for all languages."
I13-1123,Finding Dependency Parsing Limits over a Large {S}panish Corpus,2013,20,9,4,0,18764,muntsa padro,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper studies the performance of different parsers over a large Spanish treebank. The aim of this work is to assess the limitations of state-of-the-art parsers. We want to select the most appropriate parser for Subcategorization Frame acquisition, and we focus our analysis on two aspects: the accuracy drop when parsing out-of-domain data, and the performance over specific labels relevant to our task."
I13-1178,Towards the Annotation of {P}enn {T}ree{B}ank with Information Structure,2013,24,5,1,1,16528,bernd bohnet,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Information Structure (IS) determines the xe2x80x9ccommunicativexe2x80x9d segmentation of the meaning of an utterance, which makes it central to the semanticsxe2x80x90syntaxxe2x80x90 intonation interface and therefore also to NLP. Despite this relevance, IS has not received much attention in the context of the majority of the reference treebanks for data-driven NLP that already contain a semantic and syntactic layers of annotation. We present our work in progress on the annotation of the Penn TreeBank with the thematicity dimension of the IS as defined in the Meaning-Text Theory. We experiment with tagging and transitionbased parsing techniques. Especially the latter achieve acceptable accuracy with even very small training samples, which is promising for languages with scarce resources."
W12-1506,Towards a Surface Realization-Oriented Corpus Annotation,2012,25,6,3,0,82,leo wanner,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"Until recently, deep stochastic surface realization has been hindered by the lack of semantically annotated corpora. This is about to change. Such corpora are increasingly available, e.g., in the context of CoNLL shared tasks. However, recent experiments with CoNLL 2009 corpora show that these popular resources, which serve well for other applications, may not do so for generation. The attempts to adapt them for generation resulted so far in a better performance of the realizers, but not yet in a genuinely semantic generation-oriented annotation schema. Our goal is to initiate a debate on how a generation suitable annotation schema should be defined. We define some general principles of a semantic generation-oriented annotation and propose an annotation schema that is based on these principles. Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them."
W12-1525,The Surface Realisation Task: Recent Developments and Future Plans,2012,13,8,2,0,26421,anja belz,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"The Surface Realisation Shared Task was first run in 2011. Two common-ground input representations were developed and for the first time several independently developed surface realisers produced realisations from the same shared inputs. However, the input representations had several shortcomings which we have been aiming to address in the time since. This paper reports on our work to date on improving the input representations and on our plans for the next edition of the SR Task. We also briefly summarise other related developments in NLG shared tasks and outline how the different ideas may be usefully brought together in the future."
E12-1009,The Best of Both Worlds {--} A Graph-based Completion Model for Transition-based Parsers,2012,40,51,1,1,16528,bernd bohnet,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Transition-based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph configuration is available. In this paper, we describe a model that takes into account complete structures as they become available to rescore the elements of a beam, combining the advantages of transition-based and graph-based approaches. We also propose an efficient implementation that allows for the use of sophisticated features and show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores."
D12-1085,Generating Non-Projective Word Order in Statistical Linearization,2012,33,5,1,1,16528,bernd bohnet,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"We propose a technique to generate non-projective word orders in an efficient statistical linearization system. Our approach predicts liftings of edges in an unordered syntactic tree by means of a classifier, and uses a projective algorithm for tree linearization. We obtain statistically significant improvements on six typologically different languages: English, German, Dutch, Danish, Hungarian, and Czech."
D12-1133,A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing,2012,52,161,1,1,16528,bernd bohnet,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transition-based system for joint part-of-speech tagging and labeled dependency parsing with non-projective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-the-art results for all languages."
C12-2105,Data-driven Dependency Parsing With Empty Heads,2012,24,8,3,0.571429,17921,wolfgang seeker,Proceedings of {COLING} 2012: Posters,0,"Syntactic dependency structures are based on the assumption that there is exactly one node in the structure for each word in the sentence. However representing elliptical constructions (e.g. missing verbs) is problematic as the question where the dependents of the elided material should be attached to has to be solved. In this paper, we present an in-depth study into the challenges of introducing empty heads into dependency structures during automatic parsing. Structurally, empty heads provide an attachment site for the dependents of the non-overt material and thus preserve the linguistically plausible structure of the sentence. We compare three different (computational) approaches to the introduction of empty heads and evaluate them against German and Hungarian data. We then conduct a fine-grained error analysis on the output of one of the approaches to highlight some of the difficulties of the task. We find that while a clearly defined part of the phenomena can be learned by the parser, more involved elliptical structures are still mostly out of reach of the automatic tools."
C12-1052,Stacking of Dependency and Phrase Structure Parsers,2012,36,9,2,0.576461,29565,richard farkas,Proceedings of {COLING} 2012,0,"We investigate the stacking of dependency and phrase structure parsers, i.e. we define features from the output of a phrase structure parser for a dependency parser and vice versa. Our features are based on the original form of the external parses and we also compare this approach to converting phrase structures to dependencies then applying standard stacking on the converted output. The proposed method provides high accuracy gains for both phrase structure and dependency parsing. With the features derived from the phrase structures, we achieved a gain of 0.89 percentage points over a state-of-the-art parser and reach 93.95 UAS, which is the highest reported accuracy score on dependency parsing of the Penn Treebank. The phrase structure parser obtains 91.72 F-score with the features derived from the dependency trees, and this is also competitive with the best reported PARSEVAL scores for the Penn Treebank."
W11-2924,Features for Phrase-Structure Reranking from Dependency Parses,2011,22,6,2,0.576461,29565,richard farkas,Proceedings of the 12th International Conference on Parsing Technologies,0,"Radically different approaches have been proved to be effective for phrase-structure and dependency parsers in the last decade. Here, we aim to exploit the divergence in these approaches and show the utility of features extracted from the automatic dependency parses of sentences for a discriminative phrase-structure parser. Our experiments show a significant improvement over the state-of-the-art German discriminative constituent parser."
W11-2835,{\\textless}{S}tu{M}a{B}a{\\textgreater}: From Deep Representation to Surface,2011,7,11,1,1,16528,bernd bohnet,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"We realize the full generation pipeline, from the deep (= semantic) representation (SemR), over the shallow (= surface-syntactic) representation (SSyntR) to the surface. To account systematically for the non-isomorphic projection between SemR and SSyntR, we introduce an intermediate representation: the so-called deep-syntactic representation (DSyntR), which does not contain yet (all) function words (as SemR), but which already contains grammatical function relation labels (as SSyntR)."
W10-4230,The {UMUS} System for Named Entity Generation at {GREC} 2010,2010,2,0,2,1,14954,benoit favre,Proceedings of the 6th International Natural Language Generation Conference,0,We present the UMUS (Universite du Maine/Universitat Stuttgart) submission for the NEG task at GREC'10. We refined and tuned our 2009 system but we still rely on predicting generic labels and then choosing from the list of expressions that match those labels. We handled recursive expressions with care by generating specific labels for all the possible embeddings. The resulting system performs at a type accuracy of 0.84 an a string accuracy of 0.81 on the development set.
bohnet-wanner-2010-open,Open Soucre Graph Transducer Interpreter and Grammar Development Environment,2010,27,19,1,1,16528,bernd bohnet,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Graph and tree transducers have been applied in many NLP areasâamong them, machine translation, summarization, parsing, and text generation. In particular, the successful use of tree rewriting transducers for the introduction of syntactic structures in statistical machine translation contributed to their popularity. However, the potential of such transducers is limited because they do not handle graphs and because they ÂconsumeÂ the source structure in that they rewrite it instead of leaving it intact for intermediate consultations. In this paper, we describe an open source tree and graph transducer interpreter, which combines the advantages of graph transducers and two-tape Finite State Transducers and surpasses the limitations of state-of-the-art tree rewriting transducers. Along with the transducer, we present a graph grammar development environment that supports the compilation and maintenance of graph transducer grammatical and lexical resources. Such an environment is indispensable for any effort to create consistent large coverage NLP-resources by human experts."
C10-3009,A High-Performance Syntactic and Semantic Dependency Parser,2010,11,59,2,0,18856,anders bjorkelund,Coling 2010: Demonstrations,0,"This demonstration presents a high-performance syntactic and semantic dependency parser. The system consists of a pipeline of modules that carry out the to-kenization, lemmatization, part-of-speech tagging, dependency parsing, and semantic role labeling of a sentence. The system's two main components draw on improved versions of a state-of-the-art dependency parser (Bohnet, 2009) and semantic role labeler (Bjorkelund et al., 2009) developed independently by the authors.n n The system takes a sentence as input and produces a syntactic and semantic annotation using the CoNLL 2009 format. The processing time needed for a sentence typically ranges from 10 to 1000 milliseconds. The predicate--argument structures in the final output are visualized in the form of segments, which are more intuitive for a user."
C10-2129,Informed ways of improving data-driven dependency parsing for {G}erman,2010,28,8,2,0.571429,17921,wolfgang seeker,Coling 2010: Posters,0,"We investigate a series of targeted modifications to a data-driven dependency parser of German and show that these can be highly effective even for a relatively well studied language like German if they are made on a (linguistically and methodologically) informed basis and with a parser implementation that allows for fast and robust training and application. Making relatively small changes to a range of very different system components, we were able to increase labeled accuracy on a standard test set (from the CoNLL 2009 shared task), ignoring gold standard part-of-speech tags, from 87.64% to 89.40%. The study was conducted in less than five weeks and as a secondary project of all four authors. Effective modifications include the quality and combination of auto-assigned morphosyntactic features entering machine learning, the internal feature handling as well as the inclusion of global constraints and a combination of different parsing strategies."
C10-1011,Top Accuracy and Fast Dependency Parsing is not a Contradiction,2010,15,249,1,1,16528,bernd bohnet,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,None
C10-1012,Broad Coverage Multilingual Deep Sentence Generation with a Stochastic Multi-Level Realizer,2010,25,42,1,1,16528,bernd bohnet,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically annotated, or, even better, multilevel annotated corpora. Only then can they deal with such crucial generation issues as sentence planning, linearization and morphologization. Multilevel annotated corpora are increasingly available for multiple languages. We take advantage of them and propose a multilingual deep stochastic sentence realizer that mirrors the state-of-the-art research in semantic parsing. The realizer uses an SVM learning algorithm. For each pair of adjacent levels of annotation, a separate decoder is defined. So far, we evaluated the realizer for Chinese, English, German, and Spanish."
W09-2818,{ICSI}-{CRF}: The Generation of References to the Main Subject and Named Entities Using Conditional Random Fields,2009,2,2,2,1,14954,benoit favre,Proceedings of the 2009 Workshop on Language Generation and Summarisation ({UCNLG}+{S}um 2009),0,"In this paper, we describe our contribution to the Generation Challenge 2009 for the tasks of generating Referring Expressions to the Main Subject References (MSR) and Named Entities Generation (NEG). To generate the referring expressions, we employ the Conditional Random Fields (CRF) learning technique due to the fact that the selection of an expression depends on the selection of the previous references. CRFs fit very well to this task since they are designed for the labeling of sequences. For the MSR task, our system has a String Accuracy of 0.68 and a REG08-Type Accuracy of 0.76 and for the NEG task a String Accuracy of 0.79 and REG08-Type Accuracy of 0.83."
W09-1210,Efficient Parsing of Syntactic and Semantic Dependency Structures,2009,18,39,1,1,16528,bernd bohnet,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"In this paper, we describe our system for the 2009 CoNLL shared task for joint parsing of syntactic and semantic dependency structures of multiple languages. Our system combines and implements efficient parsing techniques to get a high accuracy as well as very good parsing and training time. For the applications of syntactic and semantic parsing, the parsing time and memory footprint are very important. We think that also the development of systems can profit from this since one can perform more experiments in the given time. For the subtask of syntactic dependency parsing, we could reach the second place with an accuracy in average of 85.68 which is only 0.09 points behind the first ranked system. For this task, our system has the highest accuracy for English with 89.88, German with 87.48 and the out-of-domain data in average with 78.79. The semantic role labeler works not as well as our parser and we reached therefore the fourth place (ranked by the macro F1 score) in the joint task for syntactic and semantic dependency parsing."
W09-0631,Generation of Referring Expression with an Individual Imprint,2009,2,5,1,1,16528,bernd bohnet,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"A major outcome of the last Shared Tasks for Referring Expressions Generation was that each human prefers distinct properties, syntax and lexical units for building referring expressions. One of the reasons for this seems to be that entities might be identified faster since the conversation partner has already some knowledge about how his conversation partner builds referring expressions. Therefore, artificial referring expressions should provide such individual preferences as well so that they become human like. With this contribution to the shared task, we follow this idea again. For the development set, we got a very good DICE score of 0.88 for the furniture domain and of 0.79 for the people domain."
W08-1128,{IS}-{G}: The Comparison of Different Learning Techniques for the Selection of the Main Subject References,2008,2,0,1,1,16528,bernd bohnet,Proceedings of the Fifth International Natural Language Generation Conference,0,"The GREC task of the Referring Expression Generation Challenge 2008 is to select appropriate references to the main subject in given texts. This means to select the correct type of the referring expressions such as name, pronoun, common, or elision (empty). We employ for the selection different learning techniques with the aim to find the most appropriate one for the task and the used attributes. As training data, we use the syntactic category of the searched referring expressions and additionally gathered data from the text itself."
W08-1132,The Fingerprint of Human Referring Expressions and their Surface Realization with Graph Transducers,2008,7,17,1,1,16528,bernd bohnet,Proceedings of the Fifth International Natural Language Generation Conference,0,"The algorithm IS-FP takes up the idea from the IS-FBN algorithm developed for the shared task 2007. Both algorithms learn the individual attribute selection style for each human that provided referring expressions to the corpus. The IS-FP algorithm was developed with two additional goals (1) to improve the indentification time that was poor for the FBN algorithm and (2) to push the dice score even higher. In order to generate a word string for the selected attributes, we build based on individual preferences a surface syntactic dependency tree as input. We derive the individual preferences from the training set. Finally, a graph transducer maps the input strucutre to a deep morphologic structure."
2007.mtsummit-ucnlg.6,The induction and evaluation of word order rules using corpora based on the two concepts of topological models,2007,-1,-1,1,1,16528,bernd bohnet,Proceedings of the Workshop on Using corpora for natural language generation,0,None
2007.mtsummit-ucnlg.15,"{IS}-{FBN}, {IS}-{FBS}, {IS}-{IAC}: the adaptation of two classic algorithms for the generation of referring expresssions in order to produce expressions like humans do",2007,-1,-1,1,1,16528,bernd bohnet,Proceedings of the Workshop on Using corpora for natural language generation,0,None
U04-1010,Referring Expression Generation as a Search Problem,2004,15,1,1,1,16528,bernd bohnet,Proceedings of the Australasian Language Technology Workshop 2004,0,"One of the most widely explored issues in natural language generation is the generation of referring expressions (gre): given an entity we want to refer to, how do we work out the content of a referring expression that uniquely identifies the intended referent? Over the last 15 years, a number of authors have proposed a wide range of algorithms for addressing different aspects of this problem, but the different approaches taken have made it very difficult to compare and contrast the algorithms provided in any meaningful way. In this paper, we propose a characterisation of the problem of referring expression generation as a search problem; this allows us to recast existing algorithms in a way that makes their similarities and differences clear."
bohnet-seniv-2004-mapping,Mapping Dependency Structures to Phrase Structures and the Automatic Acquisition of Mapping Rules,2004,9,2,1,1,16528,bernd bohnet,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,This paper describes a simple graph grammar based formalism that is capable to translate dependency structures into phrase structures. A procedure is introduced for the automatic acquisition of mapping rules from corpora which are annotated with both phrase structures and dependency structures. The acquired rules are evaluated by applying them to a corpus annotated with dependency structures.
W03-2315,Deriving the Communicative Structure in Applied {NLG},2003,0,4,2,0,82,leo wanner,Proceedings of the 9th {E}uropean Workshop on Natural Language Generation ({ENLG}-2003) at {EACL} 2003,0,None
W01-0807,On Using a Parallel Graph Rewriting Formalism in Generation,2001,11,17,1,1,16528,bernd bohnet,Proceedings of the {ACL} 2001 Eighth {E}uropean Workshop on Natural Language Generation ({EWNLG}),0,"In this paper, we present a parallel context sensitive graph rewriting formalism for a dependency-oriented generation grammar. The parallel processing of the input structure makes an explicit presentation of all alternative options for its mapping onto the output structure possible. This allows for the selection of the linguistic realization that suits best the communicative and contextual criteria available."
W00-1436,A development Environment for an {MTT}-Based Sentence Generator,2000,4,33,1,1,16528,bernd bohnet,{INLG}{'}2000 Proceedings of the First International Conference on Natural Language Generation,0,"With the rising standard of the state of the art in text generation and the increase of the number of practical generation applications, it becomes more and more important to provide means for the maintenance of the generator, i.e. its extension, modification, and monitoring by grammarians who are not familiar with its internals. However, only a few sentence and text generators developed to date actually provide these means. One of these generators is KPML (Bateman, 1997). KPML comes with a Development Environment and there is no doubt about the contribution of this environment to the popularity of the systemic approach in generation."
