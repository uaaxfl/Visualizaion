2020.paclic-1.35,A corpus-based comparative study of light verbs in three {C}hinese speech communities,2020,-1,-1,1,1,15873,benjamin tsou,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.paclic-1.68,"Bilingual Multi-word Expressions, Multiple-correspondence, and their cultivation from parallel patents: The {C}hinese-{E}nglish case",2020,-1,-1,1,1,15873,benjamin tsou,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.coling-main.309,Using Bilingual Patents for Translation Training,2020,-1,-1,2,0,2821,john lee,Proceedings of the 28th International Conference on Computational Linguistics,0,"While bilingual corpora have been instrumental for machine translation, their utility for training translators has been less explored. We investigate the use of bilingual corpora as pedagogical tools for translation in the technical domain. In a user study, novice translators revised Chinese translations of English patents through bilingual concordancing. Results show that concordancing with an in-domain bilingual corpus can yield greater improvement in translation quality of technical terms than a general-domain bilingual corpus."
W19-8714,Towards a Proactive {MWE} Terminological Platform for Cross-Lingual Mediation in the Age of Big Data,2019,-1,-1,1,1,15873,benjamin tsou,Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019),0,"The emergence of China as a global economic power in the 21st Century has brought about surging needs for cross-lingual and cross-cultural mediation, typically performed by translators. Advances in Artificial Intelligence and Language Engineering have been bolstered by Machine learning and suitable Big Data cultivation. They have helped to meet some of the translator{'}s needs, though the technical specialists have not kept pace with the practical and expanding requirements in language mediation. One major technical and linguistic hurdle involves words outside the vocabulary of the translator or the lexical database he/she consults, especially Multi-Word Expressions (Compound Words) in technical subjects. A further problem is in the multiplicity of renditions of a term in the target language. This paper discusses a proactive approach following the successful extraction and application of sizable bilingual Multi-Word Expressions (Compound Words) for language mediation in technical subjects, which do not fall within the expertise of typical translators, who have inadequate appreciation of the range of new technical tools available to help him/her. Our approach draws on the personal reflections of translators and teachers of translation and is based on the prior R{\&}D efforts relating to 300,000 comparable Chinese-English patents. The subsequent protocol we have developed aims to be proactive in meeting four identified practical challenges in technical translation (e.g. patents). It has broader economic implication in the Age of Big Data (Tsou et al, 2015) and Trade War, as the workload, if not, the challenges, increasingly cannot be met by currently available front-line translators. We shall demonstrate how new tools can be harnessed to spearhead the application of language technology not only in language mediation but also in the {``}teaching{''} and {``}learning{''} of translation. It shows how a better appreciation of their needs may enhance the contributions of the technical specialists, and thus enhance the resultant synergetic benefits."
U19-1021,Difficulty-aware Distractor Generation for Gap-Fill Items,2019,0,0,3,0,12220,chak yeung,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,0,
W15-3401,Augmented Comparative Corpora and Monitoring Corpus in {C}hinese: {LIVAC} and Sketch Search Engine Compared,2015,5,0,1,1,15873,benjamin tsou,Proceedings of the Eighth Workshop on Building and Using Comparable Corpora,0,"The increasing availability of numerous corpora has significantly contributed to the understanding of words in terms of their underlying semantic structures and lexical networks (e.g. COBUILD, WordNet etc.). Through data mining and information retrieval, research in this area has vastly expanded our appreciation that what constitutes lexical knowledge goes beyond synonymy, hyponymy, metonymy, meronymy, grammatical and other collocations. Furthermore, they are fundamental to a universalistic conceptual base of ontologies and knowledge representation which are often enriched by deeper and newer analysis. In this context, each language foregrounds specific features or nodes within this knowledge base by usually non-uniform means. At the same time, the arrival of the age of Big Data has attracted extensive studies on the actual and dynamic use of language as contextualized (ala. Jakobson 1960) within a given society, especially through the mass media. What are foregrounded in this medium tend to have graded cognitive saliency characterizing members of the common speech community, and such shared knowledge is usually at great variance with the thesaurus approach and show noticeable localized features. It is proposed here that the two kinds of knowledge (thesauric vs cognitive-cultural) complement each other in human cognition, and are integral to it. We draw on two large Chinese media databases Sketch (2.1 billion character tokens1) and LIVAC (550 million character tokens2) for illustration and discussion. The Sketch Engine in Chinese shows how apple is, as expected, primarily related to orange, peach, fruit, vegetable, food etc. At the"
Y12-1003,Idiomaticity and Classical Traditions in Some {E}ast {A}sian Languages,2012,-1,-1,1,1,15873,benjamin tsou,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,None
P11-1033,Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora,2011,41,58,4,1,44655,bin lu,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"Most previous work on multilingual sentiment analysis has focused on methods to adapt sentiment resources from resource-rich languages to resource-poor languages. We present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data. We rely on the intuition that the sentiment labels for parallel sentences should be similar and present a model that jointly learns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%--8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines."
2011.tc-1.9,Machine translation between uncommon language pairs via a third common language: the case of patents,2011,7,0,1,1,15873,benjamin tsou,Proceedings of Translating and the Computer 33,0,"This paper proposes to familiarize the MT users with two major areas of development: (1) To improve translation quality between uncommon language pairs, the use of a third language as the pivot. Various techniques have been shown to be promising when parallel corpora for the uncommon language pairs are not readily available. They require the use of two other language pairs involving a common third language pairing with each member of the initial target pair. (2) The surging demands in the field of patent translation and for efforts to bootstrap machine translation in uncommon language pairs (e.g., Japanese and Chinese) via more common language pairs (e.g., Chinese-English and English-Japanese), and the application of the pivot approach to expedite processing."
2011.mtsummit-papers.54,The Cultivation of a {C}hinese-{E}nglish-{J}apanese Trilingual Parallel Corpus from Comparable Patents,2011,4,0,3,1,44655,bin lu,Proceedings of Machine Translation Summit XIII: Papers,0,None
Y10-1102,"A Note on Pseudo-comparatives like {``}John is rich like {X}!{''} and {``}Like {X}, John is rich!{''}",2010,-1,-1,1,1,15873,benjamin tsou,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,None
W10-4110,Mining Large-scale Parallel Corpora from Multilingual Patents: An {E}nglish-{C}hinese example and its application to {SMT},2010,16,14,2,1,44655,bin lu,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
S10-1064,{C}ity{U}-{DAC}: Disambiguating Sentiment-Ambiguous Adjectives within Context,2010,13,7,2,1,44655,bin lu,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"This paper describes our system participating in task 18 of SemEval-2010, i.e. disambiguating Sentiment-Ambiguous Adjectives (SAAs). To disambiguating SAAs, we compare the machine learning-based and lexicon-based methods in our submissions: 1) Maximum entropy is used to train classifiers based on the annotated Chinese data from the NTCIR opinion analysis tasks, and the clause-level and sentence-level classifiers are compared; 2) For the lexicon-based method, we first classify the adjectives into two classes: intensifiers (i.e. adjectives intensifying the intensity of context) and suppressors (i.e. adjectives decreasing the intensity of context), and then use the polarity of context to get the SAAs' contextual polarity based on a sentiment lexicon. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context."
Y09-2038,Towards Bilingual Term Extraction in Comparable Patents,2009,22,13,2,1,44655,bin lu,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"In order to extract bilingual terms in a corpus of comparable patents, we present a novel framework in this paper. The framework includes the following major steps: 1) extract monolingual single-word and multi-word term candidates in monolingual patents; 2) Find parallel sentences in comparable patents; 3) extract bilingual single-word and multi-word term candidates; 4) identify correct bilingual terms using a SVM classifier by integrating both linguistic and statistical information. The experimental results show that the framework can well identify correct bilingual terms from comparable patents, and the SVM classifier can further improve its performance."
2009.mtsummit-wpt.3,The Construction of a {C}hinese-{E}nglish Patent Parallel Corpus,2009,20,8,2,1,44655,bin lu,Proceedings of the Third Workshop on Patent Translation,0,"In this paper, we describe the construction of a parallel Chinese-English patent sentence corpus which is created from noisy parallel patents. First, we use a publicly available sentence aligner to find parallel sentence candidates in the noisy parallel data. Then we compare and evaluate three individual measures and different ensemble techniques to sort the parallel sentence candidates according to the confidence score and filter out those with low scores as the noisy data. The experiment shows that the combination of measures outperforms the individual measures, and that filtering out low-quality sentence pairs is readily justified as it can improve SMT performance. Finally, we arrive at the final corpus consisting of 160K sentence pairs in which about 90% are correct or partially correct alignments."
C08-1058,Extending a Thesaurus with Words from Pan-{C}hinese Sources,2008,13,2,2,1,21796,oi kwong,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In this paper, we work on extending a Chinese thesaurus with words distinctly used in various Chinese communities. The acquisition and classification of such region-specific lexical items is an important step toward the larger goal of constructing a Pan-Chinese lexical resource. In particular, we extend a previous study in three respects: (1) to improve automatic classification by removing duplicated words from the thesaurus, (2) to experiment with classifying words at the subclass level and semantic head level, and (3) to further investigate the possible effects of data heterogeneity between the region-specific words and words in the thesaurus on classification performance. Automatic classification was based on the similarity between a target word and individual categories of words in the thesaurus, measured by the cosine function. Experiments were done on 120 target words from four regions. The automatic classification results were evaluated against a gold standard obtained from human judgements. In general accuracy reached 80% or more with the top 10 (out of 80) and top 100 (out of 1,300) candidates considered at the subclass level and semantic head level respectively, provided that the appropriate data sources were used."
C08-1143,Active Learning with Sampling by Uncertainty and Density for Word Sense Disambiguation and Text Classification,2008,21,87,4,0.444444,5752,jingbo zhu,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper addresses two issues of active learning. Firstly, to solve a problem of uncertainty sampling that it often fails by selecting outliers, this paper presents a new selective sampling technique, sampling by uncertainty and density (SUD), in which a k-Nearest-Neighbor-based density measure is adopted to determine whether an unlabeled example is an outlier. Secondly, a technique of sampling by clustering (SBC) is applied to build a representative initial training data set for active learning. Finally, we implement a new algorithm of active learning with SUD and SBC techniques. The experimental results from three real-world data sets show that our method outperforms competing methods, particularly at the early stages of active learning."
D07-1034,Extending a Thesaurus in the Pan-{C}hinese Context,2007,14,3,2,1,21796,oi kwong,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"In this paper, we address a unique problem in Chinese language processing and report on our study on extending a Chinese thesaurus with region-specific words, mostly from the financial domain, from various Chinese speech communities. With the larger goal of automatically constructing a Pan-Chinese lexical resource, this work aims at taking an existing semantic classificatory structure as leverage and incorporating new words into it. In particular, it is important to see if the classification could accommodate new words from heterogeneous data sources, and whether simple similarity measures and clustering methods could cope with such variation. We use the cosine function for similarity and test it on automatically classifying 120 target words from four regions, using different datasets for the extraction of feature vectors. The automatic classification results were evaluated against human judgement, and the performance was encouraging, with accuracy reaching over 85% in some cases. Thus while human judgement is not straightforward and it is difficult to create a PanChinese lexicon manually, it is observed that combining simple clustering methods with the appropriate data sources appears to be a promising approach toward its automatic construction."
W06-0102,Regional Variation of Domain-Specific Lexical Items: Toward a Pan-{C}hinese Lexical Resource,2006,11,3,2,1,21796,oi kwong,Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper reports on an initial and necessary step toward the construction of a Pan-Chinese lexical resource. We investigated the regional variation of lexical items in two specific domains, finance and sports; and explored how much of such variation is covered in existing Chinese synonym dictionaries, in particular the Tongyici Cilin. The domain-specific lexical items were obtained from subsections of a synchronous Chinese corpus, LIVAC. Results showed that 20-40% of the words from various subcorpora are unique to the individual communities, and as much as 70% of such unique items are not yet covered in the Tongyici Cilin. The results suggested great potential for building a Pan-Chinese lexical resource for Chinese language processing. Our next step would be to explore automatic means for extracting related lexical items from the corpus, and to incorporate them into existing semantic classifications."
tsou-etal-2006-court,Court Stenography-To-Text ({``}{STT}{''}) in {H}ong {K}ong: A Jurilinguistic Engineering Effort,2006,8,0,1,1,15873,benjamin tsou,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Implementation of legal bilingualism in Hong Kong after 1997 has necessitated the production of voluminous and extensive court proceedings and judgments in both Chinese and English. For the former, Cantonese, a dialect of Chinese, is the home language of more than 90{\%} of the population in Hong Kong and so used in the courts. To record speech in Cantonese verbatim, a Chinese Computer-Aided Transcription system has been developed. The transcription system converts stenographic codes into Chinese text, i.e. from phonetic to orthographic representation of the language. The main challenge lies in the resolution of the sever ambiguity resulting from homocode problems in the conversion process. Cantonese Chinese is typified by problematic homonymy, which presents serious challenges. The N-gram statistical model is employed to estimate the most probable character string of the input transcription codes. Domain-specific corpora have been compiled to support the statistical computation. To improve accuracy, scalable techniques such as domain-specific transcription and special encoding are used. Put together, these techniques deliver 96{\%} transcription accuracy."
tsou-kwong-2006-toward,Toward a Pan-{C}hinese Thesaurus,2006,12,4,1,1,15873,benjamin tsou,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper, we propose a corpus-based approach to the construction of a Pan-Chinese lexical resource, starting out with the aim to enrich existing Chinese thesauri in the Pan-Chinese context. The resulting thesaurus is thus expected to contain not only the core senses and usages of Chinese lexical items but also usages specific to individual Chinese speech communities. We introduce the ideas behind the construction of the resource, outline the steps to be taken, and discuss some preliminary analyses. The work is backed up by a unique and large Chinese synchronous corpus containing textual data from various Chinese speech communities including Hong Kong, Beijing, Taipei and Singapore."
W05-1001,Data Homogeneity and Semantic Role Tagging in {C}hinese,2005,12,3,2,1,21796,oi kwong,Proceedings of the {ACL}-{SIGLEX} Workshop on Deep Lexical Acquisition,0,"This paper reports on a study of semantic role tagging in Chinese in the absence of a parser. We tackle the task by identifying the relevant headwords in a sentence as a first step to partially locate the corresponding constituents to be labelled. We also explore the effect of data homogeneity by experimenting with a textbook corpus and a news corpus, representing simple data and complex data respectively. Results suggest that while the headword location method remains to be improved, the homogeneity between the training and testing data is important especially in view of the characteristic syntax-semantics interface in Chinese. We also plan to explore some class-based techniques for the task with reference to existing semantic lexicons, and to modify the method and augment the feature set with more linguistic input."
O05-5009,A Synchronous Corpus-Based Study on the Usage and Perception of Judgement Terms in the Pan-{C}hinese Context,2005,7,2,2,1,21796,oi kwong,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 4, {D}ecember 2005: Special Issue on Selected Papers from {CLSW}-5",0,"This paper reports on a synchronous corpus-based study of the everyday usage of a set of Chinese judgement terms. An earlier study on Hong Kong data found that these terms were more polysemous than their English counterparts within the legal domain, and were even more fuzzily used in general news reportage. The current study further compares their usage in general texts from other Chinese speech communities (Beijing, Taiwan, and Singapore) to explore the regional differences in lexicalisation and perception of the relevant legal concepts. Corpus data revealed the distinctiveness of the Singapore data, and that the contrasting frequency distributions of the terms and senses could be a result of the varied focus in reportage or the use of alternative expressions for the same concepts in individual communities. The analysis will contribute to the construction and enrichment of Pan-Chinese lexico-semantic resources, which will be useful for many natural language processing applications, such as machine translation."
I05-1026,Using Multiple Discriminant Analysis Approach for Linear Text Segmentation,2005,23,4,5,0.444444,5752,jingbo zhu,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Research on linear text segmentation has been an on-going focus in NLP for the last decade, and it has great potential for a wide range of applications such as document summarization, information retrieval and text understanding. However, for linear text segmentation, there are two critical problems involving automatic boundary detection and automatic determination of the number of segments in a document. In this paper, we propose a new domain-independent statistical model for linear text segmentation. In our model, Multiple Discriminant Analysis (MDA) criterion function is used to achieve global optimization in finding the best segmentation by means of the largest word similarity within a segment and the smallest word similarity between segments. To alleviate the high computational complexity problem introduced by the model, genetic algorithms (GAs) are used. Comparative experimental results show that our method based on MDA criterion functions has achieved higher Pk measure (Beeferman) than that of the baseline system using TextTiling algorithm."
I05-1070,Semantic Role Tagging for {C}hinese at the Lexical Level,2005,19,1,2,1,21796,oi kwong,Second International Joint Conference on Natural Language Processing: Full Papers,0,"This paper reports on a study of semantic role tagging in Chinese, in the absence of a parser. We investigated the effect of using only lexical information in statistical training; and proposed to identify the relevant headwords in a sentence as a first step to partially locate the corresponding constituents to be labelled. Experiments were done on a textbook corpus and a news corpus, representing simple data and complex data respectively. Results suggested that in Chinese, simple lexical features are useful enough when constituent boundaries are known, while parse information might be more important for complicated sentences than simple ones. Several ways to improve the headword identification results were suggested, and we also plan to explore some class-based techniques for the task, with reference to existing semantic lexicons."
C04-1145,Morpheme-based Derivation of Bipolar Semantic Orientation of {C}hinese Words,2004,9,50,5,0,52390,raymond yuen,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"The evaluative character of a word is called its semantic orientation (SO). A positive SO indicates desirability (e.g. Good, Honest) and a negative SO indicates undesirability (e.g., Bad, Ugly). This paper presents a method, based on Turney (2003), for inferring the SO of a word from its statistical association with strongly-polarized words and morphemes in Chinese. It is noted that morphemes are much less numerous than words, and that also a small number of fundamental morphemes may be used in the modified system to great advantage. The algorithm was tested on 1,249 words (604 positive and 645 negative) in a corpus of 34 million words, and was run with 20 and 40 polarized words respectively, giving a high precision (79.96% to 81.05%), but a low recall (45.56% to 59.57%). The algorithm was then run with 20 polarized morphemes, or single characters, in the same corpus, giving a high precision of 80.23% and a high recall of 85.03%. We concluded that morphemes in Chinese, as in any language, constitute a distinct sub-lexical unit which, though small in number, has greater linguistic significance than words, as seen by the significant enhancement of results with a much smaller corpus than that required by Turney."
Y03-1022,A Synchronous Corpus-Based Study of Verb-Noun Fluidity in {C}hinese,2003,2,5,2,1,21796,oi kwong,"Proceedings of the 17th Pacific Asia Conference on Language, Information and Computation",0,"The problem of verb-noun categorial ambiguity is critical and relatively unique for non-inflectional languages, especially Chinese. We consider the verb-noun categorial fluidity a continuum and any categorial shift a transitional process. A synchronous corpus-based study was conducted to compare the phenomenon with respect to news texts collected from Hong Kong, Beijing, and Taiwan. It was found that about 15% of the verbs in the Hong Kong and Taiwan texts were undergoing the verb-noun categorial shift; whereas Beijing texts had more than 18% of the verbs undergoing this shift. The results also have important implications on various natural language applications, including lexicography, part-of-speech tagging of Chinese, as well as other natural language processing tasks."
E03-1081,Categorial Fluidity in {C}hinese and its Implications for Part-of-speech Tagging,2003,1,9,2,1,21796,oi kwong,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper discusses the theoretical and practical concerns in part-of-speech (POS) tagging for Chinese. Unlike other languages such as English, Chinese lacks morphological marking in association with categorial alternations. We consider such categorial fluidity a continuum, and any categorial shift a transition, with special focus on the verb-noun shift. Preliminary observations are reported on this phenomenon from empirical data, and we suggest that POS tagging should not only be theoretically valid but also sufficiently capture the extent of categorial fluidity as reflected by the data."
Y02-1003,Identification of {C}hinese Personal Names in Unrestricted Texts,2001,-1,-1,2,1,46433,lawrence cheung,"Proceedings of the 16th Pacific Asia Conference on Language, Information and Computation",0,None
W02-1802,Some Considerations on Guidelines for Bilingual Alignment and Terminology Extraction,2002,4,3,6,1,46433,lawrence cheung,{COLING}-02: The First {SIGHAN} Workshop on {C}hinese Language Processing,0,"Despite progress in the development of computational means, human input is still critical in the production of consistent and useable aligned corpora and term banks. This is especially true for specialized corpora and term banks whose end-users are often professionals with very stringent requirements for accuracy, consistency and coverage. In the compilation of a high quality Chinese-English legal glossary for ELDoS project, we have identified a number of issues that make the role human input critical for term alignment and extraction. They include the identification of low frequency terms, paraphrastic expressions, discontinuous units, and maintaining consistent term granularity, etc. Although manual intervention can more satisfactorily address these issues, steps must also be taken to address intra- and inter-annotator inconsistency."
W02-1404,Alignment and Extraction of Bilingual Legal Terminology from Context Profiles,2002,15,0,2,1,21796,oi kwong,{COLING}-02: {COMPUTERM} 2002: Second International Workshop on Computational Terminology,0,"In this study, we propose a knowledge-independent method for aligning terms and thus extracting translations from a small, domain-specific corpus consisting of parallel English and Chinese court judgments from Hong Kong. With a sentence-aligned corpus, translation equivalences are suggested by analysing the frequency profiles of parallel concordances. The method overcomes the limitations of conventional statistical methods which require large corpora to be effective, and lexical approaches which depend on existing bilingual dictionaries. Pilot testing on a parallel corpus of about 113K Chinese words and 120K English words gives an encouraging 85% precision and 45% recall. Future work includes fine-tuning the algorithm upon the analysis of the errors, and acquiring a translation lexicon for legal terminology by filtering out general terms."
C02-1055,Covering Ambiguity Resolution in {C}hinese Word Segmentation Based on Contextual Information,2002,7,26,3,0,8927,xiao luo,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Covering ambiguity is one of the two basic types of ambiguities in Chinese word segmentation. We regard its resolution as equivalent to word sense disambiguation, and make use of the classical vector space model in information retrieval to formulate the contexts of ambiguous words. A variation form of TFIDF weighting is proposed and a Chinese thesaurus is additionally utilized to cope with data sparseness problem. We select 90 frequent cases of covering ambiguities as the target. The training set includes 77654 sentences, and the test set includes 19242 sentences. The experimental results showed that our model has achieved 96.58% accuracy, outperforming the original form of TFIDF weighting as well as another baseline model, the hidden Markov model."
2001.mtsummit-road.9,Evaluating {C}hinese-{E}nglish translation systems for personal name coverage,2001,-1,-1,1,1,15873,benjamin tsou,Workshop on MT2010: Towards a Road Map for MT,0,"This paper discusses the challenges which Chinese-English machine translation (MT) systems face in translating personal names. We show that the translation of names between Chinese and English is complicated by different factors, including orthographic, phonetic, geographic and social ones. Four existing systems were tested for their capability in translating personal names from Chinese to English. Test data embodying geographic and sociolinguistic differences were obtained from a synchronous Chinese corpus of news media texts. It is obvious that systems vary considerably in their ability to identify personal names in the source language and render them properly in the target language. Given the criticality of personal name translation to the overall intelligibility of a translated text, the coverage of personal names should be one of the important criteria in the evaluation of MT performance. Moreover, name translation, which calls for a hybrid approach, would remain a central issue to the future development of MT systems, especially for online and real-time applications."
Y00-1006,Textual Information Segmentation by Cohesive Ties,2000,11,1,2,1,46432,samuel chan,"Proceedings of the 14th Pacific Asia Conference on Language, Information and Computation",0,This paper proposes a novel approach in clustering texts automatically into coherent segments. A set of mutual linguistic constraints that largely determines the similarity of meaning among lexical items is used and a weight function is devised to incorporate the diversity of linguistic bonds among the text. A computational method of extracting the gist from a higher order structure representing the tremendous diversity of interrelationship among items is presented. Topic boundaries between segments in a text are identified. Our text segmentation is regarded as a process of identifying the shifts from one segment cluster to another. The experimental results show that the combination of these constraints is capable to address the topic shifts of texts.
Y00-1031,Automatic Conversion from Phonetic to Textual Representation of {C}antonese : The Case of {H}ong {K}ong Court Proceedings,2000,9,0,1,1,15873,benjamin tsou,"Proceedings of the 14th Pacific Asia Conference on Language, Information and Computation",0,"The resumption of sovereignty over Hong Kong by China and the implementation of legal bilingualism there have given rise to an urgent need for producing verbatim court records of proceedings conducted in Cantonese, the predominant Chinese dialect spoken by the majority of the population. This has created a challenge to build up the jurilinguistic infrastructure vital for the full implementation of bilingualism and the retention of the Common Law system in Hong Kong. While there are Computer-Aided Transcription (CAT) systems for processing English and Mandarin (Putonghua), none exists for processing Cantonese. This paper discusses the design of a Cantonese CAT system based on the special features of Cantonese speech sounds. The CAT system works on the conventional English-based keyboard to process Cantonese and meets the bilingual requirements of the Hong Kong courts. By utilizing primarily statistical techniques, the system is highly successful in handling the ambiguity resolution of homophonous Chinese characters, a tantalizing problem in the conversion from phonetic to textual representation of Chinese. Additional linguistic analysis and related processing are discussed which could further improve the performance of the system from about 92% to over 94% accuracy."
W00-1206,Enhancement of a {C}hinese Discourse Marker Tagger with {C}4.5,2000,13,10,1,1,15873,benjamin tsou,Second {C}hinese Language Processing Workshop,0,"Discourse markers are complex discontinuous linguistic expressions which are used to explicitly signal the discourse structure of a text. This paper describes efforts to improve an automatic tagging system which identifies and classifies discourse markers in Chinese texts by applying machine learning (ML) to the disambiguation of discourse markers, as an integral part of automatic text summarization via rhetorical structure. Encouraging results are reported."
W00-0402,Mining Discourse Markers for {C}hinese Textual Summarization,2000,31,12,4,1,46432,samuel chan,NAACL-ANLP 2000 Workshop: Automatic Summarization,0,"Discourse markers foreshadow the message thrust of texts and saliently guide their rhetorical structure which are important for content filtering and text abstraction. This paper reports on efforts to automatically identify and classify discourse markers in Chinese texts using heuristic-based and corpus-based data-mining methods, as an integral part of automatic text summarization via rhetorical structure and Discourse Markers. Encouraging results are reported."
Y99-1032,Anaphora Resolution as Lexical Cohesion Identification,1999,12,0,2,1,46432,samuel chan,"Proceedings of the 13th Pacific Asia Conference on Language, Information and Computation",0,"Anaphora,. an important indicator in lexical cohesion, is a discourse level linguistic phenomenon. Most theoretical linguistic approaches to the interpretation of anaphoric expressions propose a treatment on the basis of purely syntactic information. In this article, what we proposed is to cast anaphora resolution as a semantic inference process in which combination of multiple strategies, each exploiting a different linguistic knowledge, is employed to resolve anaphora into a coherent one. We also exhibit how to embed an anaphora resolution into a framework which captures all the salient parameters as well as to remedy, to a certain extent, the inadequacies found in any monolithic resolution systems. The effectiveness of the anaphora resolution considered in this work is exemplified through a set of simulations."
1999.mtsummit-1.31,{MT} evaluation,1999,-1,-1,3,0,50182,margaret king,Proceedings of Machine Translation Summit VII,0,"This panel deals with the general topic of evaluation of machine translation systems. The first contribution sets out some recent work on creating standards for the design of evaluations. The second, by Eduard Hovy. takes up the particular issue of how metrics can be differentiated and systematized. Benjamin K. T'sou suggests that whilst men may evaluate machines, machines may also evaluate men. John S. White focuses on the question of the role of the user in evaluation design, and Yusoff Zaharin points out that circumstances and settings may have a major influence on evaluation design."
P98-2206,{C}hinese Word Segmentation without Using Lexicon and Hand-crafted Training Data,1998,6,86,3,1,4517,maosong sun,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"Chinese word segmentation is the first step in any Chinese NLP system. This paper presents a new algorithm for segmenting Chinese texts without making use of any lexicon and hand-crafted linguistic resource. The statistical data required by the algorithm, that is, mutual information and the difference of t-score between characters, is derived automatically from raw Chinese corpora. The preliminary experiment shows that the segmentation accuracy of our algorithm is acceptable. We hope the gaining of this approach will be beneficial to improving the performance (especially in ability to cope with unknown words and ability to adapt to various domains) of the existing segmenters, though the algorithm itself can also be utilized as a stand-alone segmenter in some NLP applications."
O98-3006,Human Judgment as a Basis for Evaluation of Discourse-Connective-Based Full-Text Abstraction in {C}hinese,1998,10,2,1,1,15873,benjamin tsou,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 3, Number 1, {F}ebruary 1998: Special Issue on the 10th Research on Computational Linguistics International Conference",0,"In Chinese text, discourse connectives constitute a major linguistic device available for a writer to explicitly indicate the structure of a discourse. This set of discourse connectives, consisting of a few hundred entries in modern Chinese, is relatively stable and domain independent. In a recently published paper [T'sou 1996], a computational procedure was introduced to generate the abstract of an input text using mainly the discourse connectives appearing in the text. This paper attempts to demonstrate the validity, of this approach to full-text abstraction by means of an evaluation method, which compares human efforts in text abstraction with the performance of an experimental system called ACFAS. Specifically, our concern is about the relationship between the perceived importance of each individual sentence as judged by human beings and the sentences containing discourse connectives within an argumentative discourse."
C98-2201,{C}hinese Word Segmentation without Using Lexicon and Hand-crafted Training Data,1998,6,86,3,1,4517,maosong sun,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"Chinese word segmentation is the first step in any Chinese NLP system. This paper presents a new algorithm for segmenting Chinese texts without making use of any lexicon and hand-crafted linguistic resource. The statistical data required by the algorithm, that is, mutual information and the difference of t-score between characters, is derived automatically from raw Chinese corpora. The preliminary experiment shows that the segmentation accuracy of our algorithm is acceptable. We hope the gaining of this approach will be beneficial to improving the performance (especially in ability to cope with unknown words and ability to adapt to various domains) of the existing segmenters, though the algorithm itself can also be utilized as a stand-alone segmenter in some NLP applications."
O97-3004,A Synchronous {C}hinese Language Corpus from Different Speech Communities: Construction and Applications,1997,-1,-1,1,1,15873,benjamin tsou,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 2, Number 1, {F}ebruary 1997: Special Issue on Computational Resources for Research in {C}hinese Linguistics",0,None
O97-2002,{C}hinese Word Segmentation and Part-of-Speech Tagging in One Step,1997,-1,-1,3,0.427277,50464,tom lai,{ROCLING} 1997 Poster Papers,0,None
O97-1013,Human Judgment as a Basis for Evaluation of Discourse-Connective-based Full-text Abstraction in {C}hinese,1997,-1,-1,1,1,15873,benjamin tsou,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
Y95-1016,Ambiguity Resolution in {C}hinese Word Segmentation,1995,0,14,2,1,4517,maosong sun,"Proceedings of the 10th Pacific Asia Conference on Language, Information and Computation",0,"A new method for Chinese word segmentation named Conditional FB.. xe2x96xba-1.F. ffi flf.91xc2xb1. At least two possible segmentations which overlap in position exist for the sequence IP, n  and * ffi if we simply match it with a Chinese dictionary. xe2x80xa2 Type II xe2x80x94 Categorial Ambiguity (CA) (2)a. VT Eljf kittf4 b. xc2xb1. Note the sequence --T-1  in (2): the constituents should be combined as a single word in (a), but they should be separated in (b) because of the productivity of expressions such as 1j/17, Basic methods for dealing with segmentation ambiguities so far can be either rule-based [2,3] or statistics-based [4,5]. The former employs the conventional maximal matching strategy, either forward or backward (referred here as RIM and BMM respectively), or both , as a detector of ambiguities, and applies relevant rules from the rule base to solve them. The problems in this approach are (i) the constructions of 0As in texts to be processed are nearly unpredictable, resulting in unwieldy complexity in rule base establislunent and maintenance, and (ii) it always fails in finding CAs. The latter gives segmentation possibilities exhaustively by dictionary lookup as a * This research is supported in part by the Youth Science Foundation of Tsinghua University, Beijing, and by the Language Information Sciences Research Centre, City University of Hong Kong"
C92-3162,A Knowledge-based Machine-aided System for {C}hinese Text Abstraction,1992,4,6,1,1,15873,benjamin tsou,{COLING} 1992 Volume 3: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The production of abstracts from input source texts, using computers, is a subject in natural language processing that has attracted much attention and investigative study. It not only poses interesting theoretical challenges but also promises useful practical applications. At the City Polytechnic of Hong Kong, a large-scale research project on automated Chinese text abstraction has entered its third year. The project aims to investigate the issues related to text abstraction through the building of a prototype system. Recognising the impracticality, at this stage, of attempting to construct a fully automatic system for abstracting random free texts, we have adopted a pragmatic approach by defining three design parameters at the outset : (1) The input texts consist of Chinese newspaper editorials. (2) The universe of discourse is on the safety of the nuclear power plant at Daya Bay (which is situated some 50 Km to the east of Hong Kong). (3) The target system will be fully automatic only at the final text generation stage, but will enlist informant input in the text understanding stage. The result of our investigations and efforts is a prototype known as Machine-Aided Chinese Text Abstraction System (MACTAS) [6], To begin the process of text abstraction in MACTAS, an unsophisticated human informant first reads and understands a given Chinese editorial. Based on his understanding of the editorial, he win go through an open-ended question-answering session with the system. At the conclusion of the human-machine dialog, which draws on a previous project [7], MACTAS will generate an abstract of the editorial in Chinese."
O91-1007,Automatic {C}hinese Text Generation Based On Inference Trees,1991,0,8,2,0,55369,hinglung lin,Proceedings of Rocling {IV} Computational Linguistics Conference {IV},0,None
