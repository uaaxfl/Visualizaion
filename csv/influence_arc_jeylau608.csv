2020.aacl-main.60,W05-0909,0,0.179523,"Missing"
2020.aacl-main.60,P16-1046,0,0.0901812,"Missing"
2020.aacl-main.60,N18-2097,0,0.0139307,"tractive summarization research (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020a). News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More recently, English summarization datasets in other flavours/domains have been developed, e.g. XSum has 226K documents with highly abstractive summaries (Narayan et al., 2018), BIGPATENT is a summarization dataset for the legal domain (Sharma et al., 2019), Reddit TIFU is sourced from social media (Kim et al., 2019), and Cohan et al. (2018) proposed using scientific publications from arXiv and PubMed for abstract summarization. This paper introduces the first large-scale summarization dataset for Indonesian, sourced from the Liputan6.com online news portal over a 10year period. It covers various topics and events that happened primarily in Indonesia, from October 2000 to October 2010. Below, we present details of the dataset, propose benchmark extractive and abstractive summarization methods that leverage both multilingual and monolingual pre-trained BERT models. We further conduct error analysis to better understand the limitat"
2020.aacl-main.60,N19-1395,0,0.0237846,"Missing"
2020.aacl-main.60,D18-1443,0,0.0119254,", and summarization. In this paper, we attempt to bridge this gap by introducing a large-scale Indonesian corpus for text summarization. Neural models have driven remarkable progress in summarization in recent years, particularly for abstractive summarization. One of the first studies was Rush et al. (2015), where the authors proposed an encoder–decoder model with attention to generate headlines for English Gigaword documents (Graff et al., 2003). Subsequent studies introduced pointer networks (Nallapati et al., 2016b; See et al., 2017), summarization with content selection (Hsu et al., 2018; Gehrmann et al., 2018), graph-based attentional models (Tan et al., 2017), and deep reinforcement learning (Paulus et al., 2018). More recently, we have seen the widespread adoption 1 https://www.visualcapitalist.com/ 100-most-spoken-languages/. of pre-trained neural language models for summarization, e.g. BERT (Liu and Lapata, 2019), BART (Lewis et al., 2020), and PEGASUS (Zhang et al., 2020a). Progress in summarization research has been driven by the availability of large-scale English datasets, including 320K CNN/Daily Mail document–summary pairs (Hermann et al., 2015) and 100k NYT articles (Sandhaus, 2008) whic"
2020.aacl-main.60,P18-1013,0,0.0123031,"ext classification, and summarization. In this paper, we attempt to bridge this gap by introducing a large-scale Indonesian corpus for text summarization. Neural models have driven remarkable progress in summarization in recent years, particularly for abstractive summarization. One of the first studies was Rush et al. (2015), where the authors proposed an encoder–decoder model with attention to generate headlines for English Gigaword documents (Graff et al., 2003). Subsequent studies introduced pointer networks (Nallapati et al., 2016b; See et al., 2017), summarization with content selection (Hsu et al., 2018; Gehrmann et al., 2018), graph-based attentional models (Tan et al., 2017), and deep reinforcement learning (Paulus et al., 2018). More recently, we have seen the widespread adoption 1 https://www.visualcapitalist.com/ 100-most-spoken-languages/. of pre-trained neural language models for summarization, e.g. BERT (Liu and Lapata, 2019), BART (Lewis et al., 2020), and PEGASUS (Zhang et al., 2020a). Progress in summarization research has been driven by the availability of large-scale English datasets, including 320K CNN/Daily Mail document–summary pairs (Hermann et al., 2015) and 100k NYT articl"
2020.aacl-main.60,D15-1229,0,0.0571084,"publicly available. Koto (2016) released a dataset for chat summarization by manually annotating chat logs from WhatsApp.14 However, this dataset contains only 300 documents. The largest summarization data to date is IndoSum (Kurniawan and Louvan, 2018), which has approximately 19K news articles with manually-written summaries. Based on our analysis, however, the summaries of IndoSum are highly extractive. Beyond Indonesian, there is only a handful of non-English summarization datasets that are of sufficient size to train modern deep learning summarization methods over, including: (1) LCSTS (Hu et al., 2015), which contains 2 million Chinese short texts constructed from the Sina Weibo microblogging website; and (2) ES-News (Gonzalez et al., 2019), which comprises 270k Spanish news articles with summaries. LCSTS documents are relatively short (less than 140 Chinese characters), while ES-News is not publicly available. Our goal is to create a benchmark corpus for Indonesian text summarization that is both large scale and publicly available. 7 Conclusion We release Liputan6, a large-scale summarization corpus for Indonesian. Our dataset comes with two test sets: a canonical test set and an “Xtreme”"
2020.aacl-main.60,N19-1260,0,0.0121347,"been widely used in abstractive summarization research (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020a). News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More recently, English summarization datasets in other flavours/domains have been developed, e.g. XSum has 226K documents with highly abstractive summaries (Narayan et al., 2018), BIGPATENT is a summarization dataset for the legal domain (Sharma et al., 2019), Reddit TIFU is sourced from social media (Kim et al., 2019), and Cohan et al. (2018) proposed using scientific publications from arXiv and PubMed for abstract summarization. This paper introduces the first large-scale summarization dataset for Indonesian, sourced from the Liputan6.com online news portal over a 10year period. It covers various topics and events that happened primarily in Indonesia, from October 2000 to October 2010. Below, we present details of the dataset, propose benchmark extractive and abstractive summarization methods that leverage both multilingual and monolingual pre-trained BERT models. We further conduct error analysis to bett"
2020.aacl-main.60,L16-1129,1,0.80782,"n text summarization have largely been extractive and used small-scale datasets. Gunawan et al. (2017) developed an unsupervised summarization model over 3K news articles using heuristics such as sentence length, keyword frequency, and title features. In a similar vein, Najibullah (2015) trained a naive Bayes model to extract summary sentences in a 100-article dataset. 605 Aristoteles et al. (2012) and Silvia et al. (2014) apply genetic algorithms to a summarization dataset with less than 200 articles. These studies do not use ROUGE for evaluation, and the datasets are not publicly available. Koto (2016) released a dataset for chat summarization by manually annotating chat logs from WhatsApp.14 However, this dataset contains only 300 documents. The largest summarization data to date is IndoSum (Kurniawan and Louvan, 2018), which has approximately 19K news articles with manually-written summaries. Based on our analysis, however, the summaries of IndoSum are highly extractive. Beyond Indonesian, there is only a handful of non-English summarization datasets that are of sufficient size to train modern deep learning summarization methods over, including: (1) LCSTS (Hu et al., 2015), which contains"
2020.aacl-main.60,2020.coling-main.66,1,0.732015,"Missing"
2020.aacl-main.60,2020.acl-main.703,0,0.022553,"oder–decoder model with attention to generate headlines for English Gigaword documents (Graff et al., 2003). Subsequent studies introduced pointer networks (Nallapati et al., 2016b; See et al., 2017), summarization with content selection (Hsu et al., 2018; Gehrmann et al., 2018), graph-based attentional models (Tan et al., 2017), and deep reinforcement learning (Paulus et al., 2018). More recently, we have seen the widespread adoption 1 https://www.visualcapitalist.com/ 100-most-spoken-languages/. of pre-trained neural language models for summarization, e.g. BERT (Liu and Lapata, 2019), BART (Lewis et al., 2020), and PEGASUS (Zhang et al., 2020a). Progress in summarization research has been driven by the availability of large-scale English datasets, including 320K CNN/Daily Mail document–summary pairs (Hermann et al., 2015) and 100k NYT articles (Sandhaus, 2008) which have been widely used in abstractive summarization research (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020a). News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More recently, English summarization"
2020.aacl-main.60,W04-1013,0,0.131038,"trained with the Adam optimizer for 200,000 steps on 4×V100 16GB GPUs and evaluated every 10,000 steps. For summary generation, we use beam width = 5, trigram blocking, and a length penalty (Wu et al., 2016) to generate at least two sentences and at least 15 words (similar to the extractive model). Henceforth the abstractive model will be referred to as “B ERTA BS”. We additionally experiment with a third variant, “B ERT E XTA BS”, where we use the weights of the fine-tuned BERT in B ERT E XT for the encoder (instead of off-the-shelf BERT weights). 4 Experiment and Results We use three ROUGE (Lin, 2004) F-1 scores as evaluation metrics: R1 (unigram overlap), R2 (bigram overlap), and RL (longest common subsequence overlap). In addition, we also provide BERTS CORE (F-1), as has recently been used for machine translation evaluation (Zhang et al., 2020b).10 We use the development set to select the best checkpoint during training, and report the evaluation scores for the canonical and Xtreme test sets in Table 4. For both test sets, the summarization models are trained using the same training 10 set, but they are tuned with a different development set (see Section 2 for details). In addition to t"
2020.aacl-main.60,D19-1387,0,0.352871,"e the authors proposed an encoder–decoder model with attention to generate headlines for English Gigaword documents (Graff et al., 2003). Subsequent studies introduced pointer networks (Nallapati et al., 2016b; See et al., 2017), summarization with content selection (Hsu et al., 2018; Gehrmann et al., 2018), graph-based attentional models (Tan et al., 2017), and deep reinforcement learning (Paulus et al., 2018). More recently, we have seen the widespread adoption 1 https://www.visualcapitalist.com/ 100-most-spoken-languages/. of pre-trained neural language models for summarization, e.g. BERT (Liu and Lapata, 2019), BART (Lewis et al., 2020), and PEGASUS (Zhang et al., 2020a). Progress in summarization research has been driven by the availability of large-scale English datasets, including 320K CNN/Daily Mail document–summary pairs (Hermann et al., 2015) and 100k NYT articles (Sandhaus, 2008) which have been widely used in abstractive summarization research (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020a). News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More rece"
2020.aacl-main.60,K16-1028,0,0.0456162,"Missing"
2020.aacl-main.60,D18-1206,0,0.0848463,"e English datasets, including 320K CNN/Daily Mail document–summary pairs (Hermann et al., 2015) and 100k NYT articles (Sandhaus, 2008) which have been widely used in abstractive summarization research (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020a). News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More recently, English summarization datasets in other flavours/domains have been developed, e.g. XSum has 226K documents with highly abstractive summaries (Narayan et al., 2018), BIGPATENT is a summarization dataset for the legal domain (Sharma et al., 2019), Reddit TIFU is sourced from social media (Kim et al., 2019), and Cohan et al. (2018) proposed using scientific publications from arXiv and PubMed for abstract summarization. This paper introduces the first large-scale summarization dataset for Indonesian, sourced from the Liputan6.com online news portal over a 10year period. It covers various topics and events that happened primarily in Indonesia, from October 2000 to October 2010. Below, we present details of the dataset, propose benchmark extractive and abstra"
2020.aacl-main.60,D15-1044,0,0.0733956,"h extractive and abstractive summarization models. 1 Introduction Despite having the fourth largest speaker population in the world, with 200 million native speakers,1 Indonesian is under-represented in NLP. One reason is the scarcity of large datasets for different tasks, such as parsing, text classification, and summarization. In this paper, we attempt to bridge this gap by introducing a large-scale Indonesian corpus for text summarization. Neural models have driven remarkable progress in summarization in recent years, particularly for abstractive summarization. One of the first studies was Rush et al. (2015), where the authors proposed an encoder–decoder model with attention to generate headlines for English Gigaword documents (Graff et al., 2003). Subsequent studies introduced pointer networks (Nallapati et al., 2016b; See et al., 2017), summarization with content selection (Hsu et al., 2018; Gehrmann et al., 2018), graph-based attentional models (Tan et al., 2017), and deep reinforcement learning (Paulus et al., 2018). More recently, we have seen the widespread adoption 1 https://www.visualcapitalist.com/ 100-most-spoken-languages/. of pre-trained neural language models for summarization, e.g."
2020.aacl-main.60,P17-1099,0,0.316542,"of large datasets for different tasks, such as parsing, text classification, and summarization. In this paper, we attempt to bridge this gap by introducing a large-scale Indonesian corpus for text summarization. Neural models have driven remarkable progress in summarization in recent years, particularly for abstractive summarization. One of the first studies was Rush et al. (2015), where the authors proposed an encoder–decoder model with attention to generate headlines for English Gigaword documents (Graff et al., 2003). Subsequent studies introduced pointer networks (Nallapati et al., 2016b; See et al., 2017), summarization with content selection (Hsu et al., 2018; Gehrmann et al., 2018), graph-based attentional models (Tan et al., 2017), and deep reinforcement learning (Paulus et al., 2018). More recently, we have seen the widespread adoption 1 https://www.visualcapitalist.com/ 100-most-spoken-languages/. of pre-trained neural language models for summarization, e.g. BERT (Liu and Lapata, 2019), BART (Lewis et al., 2020), and PEGASUS (Zhang et al., 2020a). Progress in summarization research has been driven by the availability of large-scale English datasets, including 320K CNN/Daily Mail document–"
2020.aacl-main.60,P19-1212,0,0.021889,"et al., 2015) and 100k NYT articles (Sandhaus, 2008) which have been widely used in abstractive summarization research (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020a). News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More recently, English summarization datasets in other flavours/domains have been developed, e.g. XSum has 226K documents with highly abstractive summaries (Narayan et al., 2018), BIGPATENT is a summarization dataset for the legal domain (Sharma et al., 2019), Reddit TIFU is sourced from social media (Kim et al., 2019), and Cohan et al. (2018) proposed using scientific publications from arXiv and PubMed for abstract summarization. This paper introduces the first large-scale summarization dataset for Indonesian, sourced from the Liputan6.com online news portal over a 10year period. It covers various topics and events that happened primarily in Indonesia, from October 2000 to October 2010. Below, we present details of the dataset, propose benchmark extractive and abstractive summarization methods that leverage both multilingual and monolingual pre-t"
2020.aacl-main.60,P17-1108,0,0.0354875,"Missing"
2020.aacl-main.60,2020.acl-main.450,0,0.0339976,"Missing"
2020.acl-main.261,Q18-1041,0,0.330064,"ould have been charged with making such a decision, and on what basis. We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines. 1 Introduction 2 NLP tools are increasingly being deployed in the wild with potentially profound societal implications. Alongside the rise in technical capabilities has been a growing awareness of the moral obligation of the field to self-assess issues including: dataset and system bias (Zhao et al., 2017), dataset ethics (Bender and Friedman, 2018), and dual use (Hovy and Spruit, 2016). More recently, there has also been vigorous debate on whether it is ethical for the community to work on certain topics or data types. This paper aims to investigate this issue, focused around the examination of a paper recently published at EMNLP 2019 on automatic prison term prediction by Chen et al. (2019). Specifically, the paper in question proposes a neural model which performs structured prediction of the individual charges laid against an individual, and the prison term associated with each, which can provide an overall prediction of the prison t"
2020.acl-main.261,D19-1667,0,0.496977,"ally profound societal implications. Alongside the rise in technical capabilities has been a growing awareness of the moral obligation of the field to self-assess issues including: dataset and system bias (Zhao et al., 2017), dataset ethics (Bender and Friedman, 2018), and dual use (Hovy and Spruit, 2016). More recently, there has also been vigorous debate on whether it is ethical for the community to work on certain topics or data types. This paper aims to investigate this issue, focused around the examination of a paper recently published at EMNLP 2019 on automatic prison term prediction by Chen et al. (2019). Specifically, the paper in question proposes a neural model which performs structured prediction of the individual charges laid against an individual, and the prison term associated with each, which can provide an overall prediction of the prison term associated with the case. This model was constructed using a large-scale dataset of real-world Chinese court cases. Case Study in Ethical NLP Publication 2.1 Data ethics The first dimension to consider is data ethics: the data source and procedure used to construct a dataset have an immediate impact on the generalisabilty/interpretation of resu"
2020.acl-main.261,P16-2096,0,0.0285247,"decision, and on what basis. We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines. 1 Introduction 2 NLP tools are increasingly being deployed in the wild with potentially profound societal implications. Alongside the rise in technical capabilities has been a growing awareness of the moral obligation of the field to self-assess issues including: dataset and system bias (Zhao et al., 2017), dataset ethics (Bender and Friedman, 2018), and dual use (Hovy and Spruit, 2016). More recently, there has also been vigorous debate on whether it is ethical for the community to work on certain topics or data types. This paper aims to investigate this issue, focused around the examination of a paper recently published at EMNLP 2019 on automatic prison term prediction by Chen et al. (2019). Specifically, the paper in question proposes a neural model which performs structured prediction of the individual charges laid against an individual, and the prison term associated with each, which can provide an overall prediction of the prison term associated with the case. This mod"
2020.acl-main.261,C18-1041,0,0.0498888,"f any mention of ethics approval is certainly troubling given the sensitivity of the data/task. The paper does briefly mention the possibility of demographic bias, without making any attempt to quantify or ameliorate any such bias. Privacy is an interesting question here, as we return to discuss under “data misuse” in Section 2.2, in addition to discussing the legality of using court documents for NLP research. Having said this, we acknowledge that similar datasets have been constructed and used by others (esp. Xiao et al. (2018)), including in major NLP conferences (e.g. Zhong et al. (2018), Hu et al. (2018)). However, this should never be taken as a waiver for data ethic considerations. Also notable here are court proceeding datasets such as that of Aletras et al. (2016), where the use case is 2909 the prediction of the violation of human rights (focusing on torture/degrading treatment, the right to a fair trial, and respect for privacy), which is more clearly aligned with “social good” (although there is more dataset documentation that could have been provided in that paper, along the lines described above). The conversation of what social good is, though, remains an open one (Green, 2019). In"
2020.acl-main.261,D17-1323,0,0.0213424,"been allowed to be published, who should have been charged with making such a decision, and on what basis. We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines. 1 Introduction 2 NLP tools are increasingly being deployed in the wild with potentially profound societal implications. Alongside the rise in technical capabilities has been a growing awareness of the moral obligation of the field to self-assess issues including: dataset and system bias (Zhao et al., 2017), dataset ethics (Bender and Friedman, 2018), and dual use (Hovy and Spruit, 2016). More recently, there has also been vigorous debate on whether it is ethical for the community to work on certain topics or data types. This paper aims to investigate this issue, focused around the examination of a paper recently published at EMNLP 2019 on automatic prison term prediction by Chen et al. (2019). Specifically, the paper in question proposes a neural model which performs structured prediction of the individual charges laid against an individual, and the prison term associated with each, which can p"
2020.acl-main.261,D18-1390,0,0.184232,"g judges). The lack of any mention of ethics approval is certainly troubling given the sensitivity of the data/task. The paper does briefly mention the possibility of demographic bias, without making any attempt to quantify or ameliorate any such bias. Privacy is an interesting question here, as we return to discuss under “data misuse” in Section 2.2, in addition to discussing the legality of using court documents for NLP research. Having said this, we acknowledge that similar datasets have been constructed and used by others (esp. Xiao et al. (2018)), including in major NLP conferences (e.g. Zhong et al. (2018), Hu et al. (2018)). However, this should never be taken as a waiver for data ethic considerations. Also notable here are court proceeding datasets such as that of Aletras et al. (2016), where the use case is 2909 the prediction of the violation of human rights (focusing on torture/degrading treatment, the right to a fair trial, and respect for privacy), which is more clearly aligned with “social good” (although there is more dataset documentation that could have been provided in that paper, along the lines described above). The conversation of what social good is, though, remains an open one"
2020.coling-main.66,S16-1081,0,0.0255512,"ng it against existing resources. Our experiments show that I NDO BERT achieves state-of-the-art performance over most of the tasks in I NDO LEM. 1 Introduction Despite there being over 200M first-language speakers of the Indonesian language, the language is underrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In English, on the other hand, there are ever-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016; Rajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks to benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of which have contributed to rapid progress in the field in recent years. We attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM (“Indonesian Language Evaluation Montage”2 ), a comprehensive dataset encompassing seven NLP tasks and eight sub-datasets, five of which are based on previous work a"
2020.coling-main.66,J08-1001,0,0.0614843,"Missing"
2020.coling-main.66,P16-1046,0,0.0611105,"Missing"
2020.coling-main.66,N19-1423,0,0.364595,"formance over most of the tasks in I NDO LEM. 1 Introduction Despite there being over 200M first-language speakers of the Indonesian language, the language is underrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In English, on the other hand, there are ever-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016; Rajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks to benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of which have contributed to rapid progress in the field in recent years. We attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM (“Indonesian Language Evaluation Montage”2 ), a comprehensive dataset encompassing seven NLP tasks and eight sub-datasets, five of which are based on previous work and three are novel to this work. As part of this, we standardize data splits and evaluation metrics"
2020.coling-main.66,P05-1045,0,0.0259592,"3 tags based on Indonesian tag definition of Adriani et al. (2009). For I NDO LEM, we use the Indonesian POS tagging dataset of Dinakaramani et al. (2014), and 5-fold partitioning of Kurniawan and Aji (2018).15 Named entity recognition (NER). Budi et al. (2005) was the first study on named entity recognition for Indonesian, where roughly 2,000 sentences from a news portal were annotated with three NE classes: person, location, and organization. In other work, Luthfi et al. (2014) utilized Wikipedia and DBPedia to automatically generate an NER corpus, and trained a model with Stanford CRF-NER (Finkel et al., 2005). Rachman et al. (2017) studied LSTM performance over 480 tweets with the same three named entity classes. None of these authors released the datasets used in the research. There are two publicly-available Indonesian NER datasets. The first, NER UI, comprises 2,125 sentences obtained via an annotation assignment in an NLP course at the University of Indonesia in 2016 (Gultom and Wibowo, 2017). The corpus has the same three named entity classes as its predecessors (Budi et al., 2005). The second, NER UGM, comprises 2,343 sentences from news articles, and was constructed at the University of Gaj"
2020.coling-main.66,Y12-1014,0,0.027676,"arch. There are two publicly-available Indonesian NER datasets. The first, NER UI, comprises 2,125 sentences obtained via an annotation assignment in an NLP course at the University of Indonesia in 2016 (Gultom and Wibowo, 2017). The corpus has the same three named entity classes as its predecessors (Budi et al., 2005). The second, NER UGM, comprises 2,343 sentences from news articles, and was constructed at the University of Gajah Mada (Fachri, 2014) based on five named entity classes: person, organization, location, time, and quantity. Dependency parsing. Kamayani and Purwarianti (2011) and Green et al. (2012) pioneered dependency parsing for the Indonesian language. Kamayani and Purwarianti (2011) developed languagespecific dependency labels based on 20 sentences, adapted from Stanford Dependencies (de Marneffe and Manning, 2016). Green et al. (2012) annotated 100 sentences of IDENTIC without dependency labels, and used an ensemble SVM model to build a parser. Later, Rahman et al. (2017) conducted a comparative evaluation over models trained using off-the-shelf tools such as MaltParser (Nivre et al., 2005) on 2,098 annotated sentences from the news domain. However, this corpus is not publicly avai"
2020.coling-main.66,P19-1356,0,0.0256687,"cally for Indonesian.7 However, there is no comprehensive dataset for evaluating NLU systems in the Indonesian language, a void which we seek to fill with I NDO LEM. 3 I NDO BERT Transformers (Vaswani et al., 2017) have driven substantial progress in NLP research based on pretrained models in the last few years. Although attention-based models are data- and GPU-hungry, the full attention mechanisms and parallelism offered by the transformer are highly compatible with the high levels of parallelism that GPU computation offers, and have been shown to be highly effective at capturing the syntax (Jawahar et al., 2019) and sentence semantics of text (Sun et al., 2019). In particular, transformer-based language models (Devlin et al., 2019; Radford et al., 2018; Conneau and Lample, 2019; Raffel et al., 2019) pre-trained on large volumes of text based on simple tasks such as masked word prediction and sentence ordering prediction, have quickly become ubiquitous in NLP and driven substantial empirical gains across tasks including NER (Devlin et al., 2019), POS tagging (Devlin et al., 2019), single document summarization (Liu and Lapata, 2019), syntactic parsing (Kitaev et al., 2019), and discourse analysis (Nie"
2020.coling-main.66,P19-1340,0,0.0276502,"ve at capturing the syntax (Jawahar et al., 2019) and sentence semantics of text (Sun et al., 2019). In particular, transformer-based language models (Devlin et al., 2019; Radford et al., 2018; Conneau and Lample, 2019; Raffel et al., 2019) pre-trained on large volumes of text based on simple tasks such as masked word prediction and sentence ordering prediction, have quickly become ubiquitous in NLP and driven substantial empirical gains across tasks including NER (Devlin et al., 2019), POS tagging (Devlin et al., 2019), single document summarization (Liu and Lapata, 2019), syntactic parsing (Kitaev et al., 2019), and discourse analysis (Nie et al., 2019). However, this effect has been largely observed for high-resource languages such as English. I NDO BERT is a transformer-based model in the style of BERT (Devlin et al., 2019), but trained purely as a masked language model trained using the Huggingface8 framework, following the default configura4 https://universaldependencies.org/ https://github.com/ChineseGLUE/ChineseGLUE 6 https://github.com/sebastianruder/NLP-progress 7 https://github.com/kmkurn/id-nlp-resource 8 https://huggingface.co/ 5 758 Data #train #dev #test 5-Fold Evaluation 7,222 1,530 1,"
2020.coling-main.66,D19-1279,0,0.126981,"rdering task, the fine-tuning procedure is detailed in Table 2. For dependency parsing, we follow Nguyen and Nguyen (2020) in incorporating BERT into the BiAffine dependency parser (Dozat and Manning, 2017) by replacing the word embeddings with the corresponding contextualized representations. Specifically, we generate the BERT embedding of the first WordPiece token as the word embedding, and train the BiAffine parser in its default configuration. In addition, we also benchmark against a pre-existing fine-tuned version of M BERT trained over 75 concatenated UD datasets in different languages (Kondratyuk and Straka, 2019). For summarization, we follow Liu and Lapata (2019) in encoding the document by inserting the tokens [CLS] and [SEP] between sentences. We also apply alternating segment embeddings based on whether the position of a sentence is odd or even. On top of the pre-trained model, we use a second transformer encoder to learn inter-sentential relationships. The input is the encoded [CLS] representation, and the output is the extractive label y ∈ {0, 1} (1 = include in summary; 0 = don’t include). 7 Results Table 3 shows the results for POS tagging and NER. M BERT, M ALAY BERT, and I NDO BERT perform v"
2020.coling-main.66,2020.aacl-main.60,1,0.732015,"Missing"
2020.coling-main.66,L16-1129,1,0.857928,"to pre-trained language models (Liu and Lapata, 2019; Zhang et al., 2019), recent summarization work on English in terms of both extractive and abstractive methods has relied on ever-larger datasets and data-hungry methods. Indonesian (single document) text summarization research has inevitably focused predominantly on extractive methods, based on small datasets. Aristoteles et al. (2012) deployed a genetic algorithm over a 200-document summarization dataset, and Gunawan et al. (2017) performed unsupervised summarization over 3,075 news articles. As an attempt to create a standardized corpus, Koto (2016) released a 300-document chat summarization dataset, and Kurniawan and Louvan (2018) released the IndoSum 19K document–summary dataset. At the time we carried out this work,20 IndoSum was the largest Indonesian summarization corpus in the news domain, manually constructed from CNN Indonesia21 and Kumparan22 documents. IndoSum is a single-document summarization dataset where each article has one abstractive summary. Kurniawan and Louvan (2018) released IndoSum together with the O RACLE — a set of extractive summaries generated automatically by maximizing ROUGE score between sentences of the art"
2020.coling-main.66,N16-1030,0,0.137754,"Missing"
2020.coling-main.66,larasati-2012-identic,0,0.0331061,"n English POS-tagged dataset and noisily projecting the POS tags from English to the Indonesian translations. 9 The existing implementation merges all documents into one text stream https://kompas.com 11 https://koran.tempo.co 12 https://liputan6.com 13 We checkpointed the model at 1M and 2M steps, and found that 2M steps yielded a lower perplexity over the dev set. 14 http://www.panl10n.net/ 10 759 To create a larger and more reliable corpus, Dinakaramani et al. (2014) published a manually-annotated corpus of 260K tokens (10K sentences). The text was sourced from the IDENTIC parallel corpus (Larasati, 2012), which was translated from data in the Penn Treebank corpus. The text is manually annotated with 23 tags based on Indonesian tag definition of Adriani et al. (2009). For I NDO LEM, we use the Indonesian POS tagging dataset of Dinakaramani et al. (2014), and 5-fold partitioning of Kurniawan and Aji (2018).15 Named entity recognition (NER). Budi et al. (2005) was the first study on named entity recognition for Indonesian, where roughly 2,000 sentences from a news portal were annotated with three NE classes: person, location, and organization. In other work, Luthfi et al. (2014) utilized Wikiped"
2020.coling-main.66,2020.acl-main.653,0,0.0297278,"formance by 20 points at the time of writing. In the cross-lingual setting, XGLUE (Liang et al., 2020) was introduced as a benchmark dataset that covers nearly 20 languages. Unlike GLUE, XGLUE includes language generation tasks such as question and headline generation. One of the largest cross-lingual corpora is dependency parsing provided by Universal Dependencies.4 It has consistent annotation of 150 treebanks across 90 languages, constructed through an open collaboration involving many contributors. Recently, other cross-lingual benchmarks have been introduced, such as Hu et al. (2020) and Lewis et al. (2020). While these three cross-lingual benchmarks contain some resources/datasets for Indonesian, the coverage is low and data is limited. Beyond the English and cross-lingual settings, ChineseGLUE5 is a comprehensive NLU collection for Mandarin Chinese, covering eight different tasks. For the Vietnamese language, Nguyen and Nguyen (2020) gathered a dataset covering four tasks (NER, POS tagging, dependency parsing, and language inference), and empirically evaluated them against a monolingual BERT. Elsewhere, there are individual efforts to maintain a systematic catalogue of tasks and datasets, and"
2020.coling-main.66,2020.emnlp-main.484,0,0.0446606,"Missing"
2020.coling-main.66,W04-1013,0,0.103901,"Missing"
2020.coling-main.66,D19-1387,0,0.327715,", and have been shown to be highly effective at capturing the syntax (Jawahar et al., 2019) and sentence semantics of text (Sun et al., 2019). In particular, transformer-based language models (Devlin et al., 2019; Radford et al., 2018; Conneau and Lample, 2019; Raffel et al., 2019) pre-trained on large volumes of text based on simple tasks such as masked word prediction and sentence ordering prediction, have quickly become ubiquitous in NLP and driven substantial empirical gains across tasks including NER (Devlin et al., 2019), POS tagging (Devlin et al., 2019), single document summarization (Liu and Lapata, 2019), syntactic parsing (Kitaev et al., 2019), and discourse analysis (Nie et al., 2019). However, this effect has been largely observed for high-resource languages such as English. I NDO BERT is a transformer-based model in the style of BERT (Devlin et al., 2019), but trained purely as a masked language model trained using the Huggingface8 framework, following the default configura4 https://universaldependencies.org/ https://github.com/ChineseGLUE/ChineseGLUE 6 https://github.com/sebastianruder/NLP-progress 7 https://github.com/kmkurn/id-nlp-resource 8 https://huggingface.co/ 5 758 Data #train #d"
2020.coling-main.66,P16-1100,0,0.0202437,"uate it over I NDO LEM, in addition to benchmarking it against existing resources. Our experiments show that I NDO BERT achieves state-of-the-art performance over most of the tasks in I NDO LEM. 1 Introduction Despite there being over 200M first-language speakers of the Indonesian language, the language is underrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In English, on the other hand, there are ever-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016; Rajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks to benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of which have contributed to rapid progress in the field in recent years. We attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM (“Indonesian Language Evaluation Montage”2 ), a comprehensive dataset encompassing seven NLP tasks and eight sub-datase"
2020.coling-main.66,2020.findings-emnlp.92,0,0.0698743,"ng provided by Universal Dependencies.4 It has consistent annotation of 150 treebanks across 90 languages, constructed through an open collaboration involving many contributors. Recently, other cross-lingual benchmarks have been introduced, such as Hu et al. (2020) and Lewis et al. (2020). While these three cross-lingual benchmarks contain some resources/datasets for Indonesian, the coverage is low and data is limited. Beyond the English and cross-lingual settings, ChineseGLUE5 is a comprehensive NLU collection for Mandarin Chinese, covering eight different tasks. For the Vietnamese language, Nguyen and Nguyen (2020) gathered a dataset covering four tasks (NER, POS tagging, dependency parsing, and language inference), and empirically evaluated them against a monolingual BERT. Elsewhere, there are individual efforts to maintain a systematic catalogue of tasks and datasets, and state-of-the-art methods for each across multiple languages,6 including one specifically for Indonesian.7 However, there is no comprehensive dataset for evaluating NLU systems in the Indonesian language, a void which we seek to fill with I NDO LEM. 3 I NDO BERT Transformers (Vaswani et al., 2017) have driven substantial progress in N"
2020.coling-main.66,P19-1442,0,0.022459,"19) and sentence semantics of text (Sun et al., 2019). In particular, transformer-based language models (Devlin et al., 2019; Radford et al., 2018; Conneau and Lample, 2019; Raffel et al., 2019) pre-trained on large volumes of text based on simple tasks such as masked word prediction and sentence ordering prediction, have quickly become ubiquitous in NLP and driven substantial empirical gains across tasks including NER (Devlin et al., 2019), POS tagging (Devlin et al., 2019), single document summarization (Liu and Lapata, 2019), syntactic parsing (Kitaev et al., 2019), and discourse analysis (Nie et al., 2019). However, this effect has been largely observed for high-resource languages such as English. I NDO BERT is a transformer-based model in the style of BERT (Devlin et al., 2019), but trained purely as a masked language model trained using the Huggingface8 framework, following the default configura4 https://universaldependencies.org/ https://github.com/ChineseGLUE/ChineseGLUE 6 https://github.com/sebastianruder/NLP-progress 7 https://github.com/kmkurn/id-nlp-resource 8 https://huggingface.co/ 5 758 Data #train #dev #test 5-Fold Evaluation 7,222 1,530 1,687 4,477 700 802 170 187 559 100 2,006 425"
2020.coling-main.66,Y17-1012,0,0.0171057,"es, and was constructed at the University of Gajah Mada (Fachri, 2014) based on five named entity classes: person, organization, location, time, and quantity. Dependency parsing. Kamayani and Purwarianti (2011) and Green et al. (2012) pioneered dependency parsing for the Indonesian language. Kamayani and Purwarianti (2011) developed languagespecific dependency labels based on 20 sentences, adapted from Stanford Dependencies (de Marneffe and Manning, 2016). Green et al. (2012) annotated 100 sentences of IDENTIC without dependency labels, and used an ensemble SVM model to build a parser. Later, Rahman et al. (2017) conducted a comparative evaluation over models trained using off-the-shelf tools such as MaltParser (Nivre et al., 2005) on 2,098 annotated sentences from the news domain. However, this corpus is not publicly available. The Universal Dependencies (UD) project16 has released two different Indonesian corpora of relatively small size: (1) 5,593 sentences of UD-Indo-GSD (McDonald et al., 2013);17 and (2) 1,000 sentences of UD-Indo-PUD (Zeman et al., 2018).18 Alfina et al. (2019) found that these corpora contain annotation errors and did not deal adequately with Indonesian morphology. They release"
2020.coling-main.66,P18-2124,0,0.034456,"n addition to benchmarking it against existing resources. Our experiments show that I NDO BERT achieves state-of-the-art performance over most of the tasks in I NDO LEM. 1 Introduction Despite there being over 200M first-language speakers of the Indonesian language, the language is underrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In English, on the other hand, there are ever-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016; Rajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks to benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of which have contributed to rapid progress in the field in recent years. We attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM (“Indonesian Language Evaluation Montage”2 ), a comprehensive dataset encompassing seven NLP tasks and eight sub-datasets, five of which are ba"
2020.coling-main.66,D15-1044,0,0.0158694,"u, sorry, I am a new follower. the new school academic year is January, on the ﬁrst day I may get question about ""tukang keong"". what are you doing beautiful? Figure 1: Example for the next tweet prediction task. To the left is the original Indonesian version and to the right is an English translation. The tweet indicated in bold is the correct next tweet. simply count the proportion of positive and negative polarity aspects, and label the sentence based on the majority sentiment. We discard a review if there is a tie in positive and negative aspects. Summarization. From attention mechanisms (Rush et al., 2015; See et al., 2017) to pre-trained language models (Liu and Lapata, 2019; Zhang et al., 2019), recent summarization work on English in terms of both extractive and abstractive methods has relied on ever-larger datasets and data-hungry methods. Indonesian (single document) text summarization research has inevitably focused predominantly on extractive methods, based on small datasets. Aristoteles et al. (2012) deployed a genetic algorithm over a 200-document summarization dataset, and Gunawan et al. (2017) performed unsupervised summarization over 3,075 news articles. As an attempt to create a s"
2020.coling-main.66,P17-1099,0,0.0134496,"w follower. the new school academic year is January, on the ﬁrst day I may get question about ""tukang keong"". what are you doing beautiful? Figure 1: Example for the next tweet prediction task. To the left is the original Indonesian version and to the right is an English translation. The tweet indicated in bold is the correct next tweet. simply count the proportion of positive and negative polarity aspects, and label the sentence based on the majority sentiment. We discard a review if there is a tie in positive and negative aspects. Summarization. From attention mechanisms (Rush et al., 2015; See et al., 2017) to pre-trained language models (Liu and Lapata, 2019; Zhang et al., 2019), recent summarization work on English in terms of both extractive and abstractive methods has relied on ever-larger datasets and data-hungry methods. Indonesian (single document) text summarization research has inevitably focused predominantly on extractive methods, based on small datasets. Aristoteles et al. (2012) deployed a genetic algorithm over a 200-document summarization dataset, and Gunawan et al. (2017) performed unsupervised summarization over 3,075 news articles. As an attempt to create a standardized corpus,"
2020.coling-main.66,N19-1035,0,0.0239138,"ive dataset for evaluating NLU systems in the Indonesian language, a void which we seek to fill with I NDO LEM. 3 I NDO BERT Transformers (Vaswani et al., 2017) have driven substantial progress in NLP research based on pretrained models in the last few years. Although attention-based models are data- and GPU-hungry, the full attention mechanisms and parallelism offered by the transformer are highly compatible with the high levels of parallelism that GPU computation offers, and have been shown to be highly effective at capturing the syntax (Jawahar et al., 2019) and sentence semantics of text (Sun et al., 2019). In particular, transformer-based language models (Devlin et al., 2019; Radford et al., 2018; Conneau and Lample, 2019; Raffel et al., 2019) pre-trained on large volumes of text based on simple tasks such as masked word prediction and sentence ordering prediction, have quickly become ubiquitous in NLP and driven substantial empirical gains across tasks including NER (Devlin et al., 2019), POS tagging (Devlin et al., 2019), single document summarization (Liu and Lapata, 2019), syntactic parsing (Kitaev et al., 2019), and discourse analysis (Nie et al., 2019). However, this effect has been larg"
2020.coling-main.66,D19-1575,0,0.366639,"the Indonesian language, the language is underrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In English, on the other hand, there are ever-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016; Rajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks to benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of which have contributed to rapid progress in the field in recent years. We attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM (“Indonesian Language Evaluation Montage”2 ), a comprehensive dataset encompassing seven NLP tasks and eight sub-datasets, five of which are based on previous work and three are novel to this work. As part of this, we standardize data splits and evaluation metrics, to enhance reproducibility and robust benchmarking. These tasks are intended to span a broad range of morpho-synta"
2020.coling-main.66,N18-1101,0,0.0473251,"underrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In English, on the other hand, there are ever-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016; Rajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks to benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of which have contributed to rapid progress in the field in recent years. We attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM (“Indonesian Language Evaluation Montage”2 ), a comprehensive dataset encompassing seven NLP tasks and eight sub-datasets, five of which are based on previous work and three are novel to this work. As part of this, we standardize data splits and evaluation metrics, to enhance reproducibility and robust benchmarking. These tasks are intended to span a broad range of morpho-syntactic, semantic, and discourse analysis compet"
2020.coling-main.66,D19-1077,0,0.0231914,"prised of 3–5 tweets, and we model the task via multi-classification (with 5 classes/ranks). We perform inference based on P (r|t), where we decide the final rank based on the highest sum of probabilities from the exhaustive enumeration of document ranks. 763 6.2 BERT Benchmarks To benchmark I NDO BERT, we compare against two pre-existing BERT models: multilingual BERT (“M BERT”), and a monolingual BERT for Malay (“M ALAY BERT”).29 M BERT is trained by concatenating Wikipedia documents for 104 languages including Indonesian, and has been shown to be effective for zero-shot multilingual tasks (Wu and Dredze, 2019; Wang et al., 2019c). M ALAY BERT is a a publicly available model that was trained on Malay documents from Wikipedia, local news sources, social media, and some translations from English. We expect M ALAY BERT to provide better representations than M BERT for the Indonesian language, because Malay and Indonesian are mutually intelligible, with many lexical similarities, but noticeable differences in grammar, pronunciation and vocabulary. For the sequence labelling tasks (POS tagging and NER), sentiment analysis, NTP, and tweet ordering task, the fine-tuning procedure is detailed in Table 2. F"
2020.coling-main.66,K18-2001,0,0.0201268,"6). Green et al. (2012) annotated 100 sentences of IDENTIC without dependency labels, and used an ensemble SVM model to build a parser. Later, Rahman et al. (2017) conducted a comparative evaluation over models trained using off-the-shelf tools such as MaltParser (Nivre et al., 2005) on 2,098 annotated sentences from the news domain. However, this corpus is not publicly available. The Universal Dependencies (UD) project16 has released two different Indonesian corpora of relatively small size: (1) 5,593 sentences of UD-Indo-GSD (McDonald et al., 2013);17 and (2) 1,000 sentences of UD-Indo-PUD (Zeman et al., 2018).18 Alfina et al. (2019) found that these corpora contain annotation errors and did not deal adequately with Indonesian morphology. They released a corrected version of UD-Indo-PUD by fixing annotations for reduplicated-words, clitics, compound words, and noun phrases. We include two UD-based dependency parsing datasets in I NDO LEM: (1) UD-Indo-GSD, and (2) the corrected version of UD-Indo-PUD. As our reference dependency parser model, we use the BiAffine dependency parser (Dozat and Manning, 2017), which has been shown to achieve strong performance for English. 4.2 Semantic Tasks Sentiment a"
2020.tacl-1.20,P18-2073,1,0.601412,"ore document context, using broader and deeper models (e.g., Devlin et al., 2019; Yang et al., 2019). While most language modeling is restricted to individual sentences, models can benefit from using additional context (Khandelwal et al., 2018). However, despite the importance of context, few psycholinguistic or computational studies systematically investigate how context affects acceptability, or the ability of language models to predict human acceptability judgments. Two recent studies that explore the impact of document context on acceptability judgments both identify a compression effect (Bernardy et al., 2018; Bizzoni and Lappin, 2019). Sentences perceived to be low in acceptability when judged without context receive a boost in acceptability when judged within context. Conversely, those with high out-of-context acceptability see a reduction in acceptability when context is presented. It is unclear what causes this compression effect. Is it a result of cognitive load, imposed by additional We study the influence of context on sentence acceptability. First we compare the acceptability ratings of sentences judged in isolation, with a relevant context, and with an irrelevant context. Our results show"
2020.tacl-1.20,W19-0414,1,0.88034,"sing broader and deeper models (e.g., Devlin et al., 2019; Yang et al., 2019). While most language modeling is restricted to individual sentences, models can benefit from using additional context (Khandelwal et al., 2018). However, despite the importance of context, few psycholinguistic or computational studies systematically investigate how context affects acceptability, or the ability of language models to predict human acceptability judgments. Two recent studies that explore the impact of document context on acceptability judgments both identify a compression effect (Bernardy et al., 2018; Bizzoni and Lappin, 2019). Sentences perceived to be low in acceptability when judged without context receive a boost in acceptability when judged within context. Conversely, those with high out-of-context acceptability see a reduction in acceptability when context is presented. It is unclear what causes this compression effect. Is it a result of cognitive load, imposed by additional We study the influence of context on sentence acceptability. First we compare the acceptability ratings of sentences judged in isolation, with a relevant context, and with an irrelevant context. Our results show that context induces a cog"
2020.tacl-1.20,P19-1285,0,0.0240984,"pus, Giga5 ClueWeb, Common Crawl Table 1: Language models and their configurations. order (e.g., x4 and x1 ), and so the model always sees some context words for prediction. As XLNET is trained to work with different factorization orders during training, it has experienced both full/bidirectional context and partial/ unidirectional context, allowing it to adapt to tasks that have access to full context (e.g., most language understanding tasks), as well as those that do not (e.g., left-to-right generation). Another innovation of XLNET is that it incorporates the segment recurrence mechanism of Dai et al. (2019). This mechanism is inspired by truncated backpropagation through time used for training RNNs, where the initial state of a sequence is initialized with the final state from the previous sequence. The segment recurrence mechanism works in a similar way, by caching the hidden states of the transformer blocks from the previous sequence, and allowing the current sequence to attend to them during training. This permits XLNET to model long-range dependencies beyond its maximum sequence length. We use the largest pre-trained model (‘‘XLNetLarge’’),15 which has a similar number of parameters to our B"
2020.tacl-1.20,J15-4004,0,0.0445344,"users are rating the English sentences ≥ 3.0 consistently. For the second and third experiments, we also check that users are selecting the topics appropriately. In each HIT one context paragraph has one real topic (from the topic model), and three fake topics with randomly sampled words as the candidate topics. Users who fail to identify the real topic above a confidence level are filtered out. Across the three experiments, over three quarters of workers passed our filtering conditions. To calibrate for the differences in rating scale between users, we follow the postprocessing procedure of Hill et al. (2015), where we calculate the average rating for each user and the overall average (by taking the mean of all average ratings), and decrease (increase) the ratings of a user by 1.0 if their average rating is greater (smaller) than the overall average by 1.0.7 To reduce the impact of outliers, for each sentence we also remove ratings that are more than 2 standard deviations away from the mean.8 2.2 Results and Discussion We present scatter plots to compare the mean ratings for the three different contexts (H∅ , H+ , and H− ) in Figure 1. The black line represents the diagonal, and the red line repre"
2020.tacl-1.20,P82-1020,0,0.795466,"Missing"
2020.tacl-1.20,N19-1423,0,0.451204,"from raw texts, we have prima facie support for an alternative view of language acquisition that does not rely on a categorical grammaticality component. It is generally assumed that our perception of sentence acceptability is influenced by context. Sentences that may appear odd in isolation can become natural in some environments, and sentences that seem perfectly well formed in some contexts are odd in others. On the computational side, much recent progress in language modeling has been achieved through the ability to incorporate more document context, using broader and deeper models (e.g., Devlin et al., 2019; Yang et al., 2019). While most language modeling is restricted to individual sentences, models can benefit from using additional context (Khandelwal et al., 2018). However, despite the importance of context, few psycholinguistic or computational studies systematically investigate how context affects acceptability, or the ability of language models to predict human acceptability judgments. Two recent studies that explore the impact of document context on acceptability judgments both identify a compression effect (Bernardy et al., 2018; Bizzoni and Lappin, 2019). Sentences perceived to be low"
2020.tacl-1.20,P18-1027,0,0.0180897,"generally assumed that our perception of sentence acceptability is influenced by context. Sentences that may appear odd in isolation can become natural in some environments, and sentences that seem perfectly well formed in some contexts are odd in others. On the computational side, much recent progress in language modeling has been achieved through the ability to incorporate more document context, using broader and deeper models (e.g., Devlin et al., 2019; Yang et al., 2019). While most language modeling is restricted to individual sentences, models can benefit from using additional context (Khandelwal et al., 2018). However, despite the importance of context, few psycholinguistic or computational studies systematically investigate how context affects acceptability, or the ability of language models to predict human acceptability judgments. Two recent studies that explore the impact of document context on acceptability judgments both identify a compression effect (Bernardy et al., 2018; Bizzoni and Lappin, 2019). Sentences perceived to be low in acceptability when judged without context receive a boost in acceptability when judged within context. Conversely, those with high out-of-context acceptability s"
2020.tacl-1.20,C12-1173,0,0.0644447,"Missing"
2020.tacl-1.20,D18-2012,0,0.0554596,"Missing"
2020.tacl-1.20,P12-1101,0,0.0687942,"To modulate for these factors we introduce simple normalization techniques. Table 2 presents five methods to map sentence probabilities to acceptability measures: LP, MeanLP, PenLP, NormLP, and SLOR. LP is the unnormalized log probability. Both MeanLP and PenLP are normalized on sentence length, but PenLP scales length with an exponent (α) to dampen the impact of large values (Wu et al., 2016; Vaswani et al., 2017). We set α = 0.8 in our experiments. NormLP normalizes using unigram Q|s| sentence probability (i.e., Pu (s) = i=0 P (wi )), while SLOR utilizes both length and unigram probability (Pauls and Klein, 2012). When computing sentence probability we have the option of including the context paragraph that the human annotators see (Section 2). We use the superscripts ∅, +, − to denote a model using no context, real context, and random context, respectively (e.g., LSTM∅ , LSTM+ , and LSTM− ). Note that these variants are created at test time, and are all based on the same trained model (e.g., LSTM). For all models except TDLM, incorporating the context paragraph is trivial. We simply prepend it to the target sentence before computing the latter’s probability. For TDLM+ or TDLM− , the context paragraph"
2020.tacl-1.20,P17-1033,1,0.61961,"ion. The two sets of experiments provide insights into the cognitive aspects of sentence processing and central issues in the computational modeling of text and discourse. 1 Introduction Sentence acceptability is the extent to which a sentence appears natural to native speakers of a language. Linguists have often used this property to motivate grammatical theories. Computational language processing has traditionally been more concerned with likelihood—the probability of a sentence being produced or encountered. The question of whether and how these properties are related is a fundamental one. Lau et al. (2017b) experiment with unsupervised language models to predict acceptability, and they obtained an encouraging correlation with human ratings. 296 Transactions of the Association for Computational Linguistics, vol. 8, pp. 296–310, 2020. https://doi.org/10.1162/tacl a 00315 Action Editor: George Foster. Submission batch: 10/2019; Revision batch: 1/2020; Published 6/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. processing demands, or is it the consequence of an attempt to identify a discourse relation between context and sentence? We address these que"
2020.tacl-1.20,P15-1156,1,0.937034,"prediction), and use these models to infer the probabilities of our test sentences. To accommodate sentence length and lexical frequency we experiment with several simple normalization methods, converting probabilities to acceptability measures (Section 3.2). The acceptability measures are the final output of our models; they are what we use to compare to human acceptability ratings. 3.1 Language Models Our first model is an LSTM language model (LSTM: Hochreiter and Schmidhuber, 1997; Mikolov et al., 2010). Recurrent neural network models (RNNs) have been shown to be competitive in this task (Lau et al., 2015; Bernardy et al., 2018), and they serve as our baseline. Our second model is a joint topic and language model (TDLM: Lau et al., 2017a). TDLM combines topic model with language model in a single model, drawing on the idea that the topical context of a sentence can help word prediction in the language model. The topic model is fashioned as an auto-encoder, where the input is the document’s word sequence and it is processed by convolutional layers to produce a topic vector to predict the input words. The language model 10 We follow the procedure detailed in https:// statisticsbyjim.com/regressi"
2020.tacl-1.20,P16-1162,0,0.00980138,"ped to embeddings (along with their positions) and fed to multiple layers of ‘‘transformer blocks’’ before the target word is predicted. Much of its power resides in these transformer blocks: Each provides a multi-headed self-attention unit over all input words, allowing it to capture multiple dependencies between words, while avoiding the need for recurrence. With no need to process a sentence in sequence, the model parallelizes more efficiently, and scales in a way that RNNs cannot. GPT2 is trained on WebText, which consists of over 8 million web documents, and uses Byte Pair Encoding (BPE: Sennrich et al., 2016) for tokenization (casing preserved). BPE produces sub-word units, a middle ground between word and character, and it provides better coverage for unseen words. We use the released medium-sized model (‘‘Medium’’) for our experiments.12 Our second transformer is BERT (Devlin et al., 2019). Unlike GPT2, BERT is not a typical language model, in the sense that it has access to both left and right context words when predicting the target word.13 Hence, it encodes context in a bidirectional manner. To train BERT, Devlin et al. (2019) propose a masked language model objective, where a random proporti"
2020.tacl-1.20,P14-5010,0,0.00379249,"rd units, a middle ground between word and character, and it provides better coverage for unseen words. We use the released medium-sized model (‘‘Medium’’) for our experiments.12 Our second transformer is BERT (Devlin et al., 2019). Unlike GPT2, BERT is not a typical language model, in the sense that it has access to both left and right context words when predicting the target word.13 Hence, it encodes context in a bidirectional manner. To train BERT, Devlin et al. (2019) propose a masked language model objective, where a random proportion of input words are masked 11 We use Stanford CoreNLP (Manning et al., 2014) to tokenize words and sentences. Rare words are replaced by a special UNK symbol. 12 https://github.com/openai/gpt-2. 13 Note that context is burdened with two senses in the paper. It can mean the preceding sentences of a target sentence, or the neighbouring words of a target word. The intended sense should be apparent from the usage. 14 300 https://github.com/google-research/bert. Configuration Architecture Encoding #Param. Casing Size BERTUCS RNN RNN Transformer Transformer Transformer Unidir. Unidir. Unidir. Bidir. Bidir. 60M 80M 340M 340M 340M Uncased Uncased Cased Cased Uncased 0.2GB 0.2"
2020.tacl-1.20,W19-2304,0,0.0184689,"ver, that sentence probability computed this way is not a true probability value: These probabilities do not sum to 1.0 over all sentences. Equation (1), in contrast, does guarantee true probabilities. Intuitively, the sentence probability computed with this bidirectional formulation is a measure 16 Technically we can mask all right context words and predict the target words one at a time, but because the model is never trained in this way, we found that it performs poorly in preliminary experiments. of the model’s confidence in the likelihood of the sentence. To compute the true probability, Wang and Cho (2019) show that we need to sum the pre-softmax weights for each token to score a sentence, and then divide the score by the total score of all sentences. As it is impractical to compute the total score of all sentences (an infinite set), the true sentence probabilities for these bidirectional models are intractable. We use our non-normalized confidence scores as stand-ins for these probabilities. For XLNET, we also compute sentence probability this way, applying bidirectional context, and we denote it as XLNETBI . Note that XLNETUNI and XLNETBI are based on the same trained model. They differ only"
2021.eacl-main.60,D15-1263,0,0.0203611,"1: An example discourse tree, from the RST Discourse Treebank (elab = elaboration). Discourse analysis involves the modelling of the structure of text in a document. It provides a systematic way to understand how texts are segmented hierarchically into discourse units, and the relationships between them. Unlike syntax parsing which models the relationship of words in a sentence, discourse parsing operates at the document-level, and aims to explain the flow of writing. Studies have found that discourse parsing is beneficial for downstream NLP tasks including document-level sentiment analysis (Bhatia et al., 2015) and abstractive summarization (Koto et al., 2019). Rhetorical Structure Theory (RST; Mann and Thompson (1988)) is one of the most widely used discourse theories in NLP (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2016; Yu et al., 2018). RST organizes text spans into a tree, where the leaves represent the basic unit of discourse, known as elementary discourse units (EDUs). EDUs are typically clauses of a sentence. Non-terminal nodes in the tree represent discourse unit relations. In Figure 1, we present an example RST tree with four EDUs spanning two senten"
2021.eacl-main.60,E17-1028,0,0.447664,"ite unit is a supporting sentence for the nucleus unit and contains less prominent information. It is standard practice that the RST tree is trained and evaluated in a right-heavy binarized manner, resulting in three forms of binary nuclearity relationships between EDUs: Nucleus–Satellite, Satellite–Nucleus, and Nucleus–Nucleus. In this work, eighteen coarse-grained relations are considered as discourse labels, consistent with earlier work (Yu et al., 2018).2 Work on RST parsing has been dominated by the bottom-up paradigm (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Braud et al., 2017; Morey et al., 2017; Yu et al., 2018). These methods produce very competitive benchmarks, but in practice it is not a straightforward 1 Code and trained models: https://github.com/ fajri91/NeuralRST-TopDown 2 Details of individual relations can be found at: http: //www.sfu.ca/rst/index.html 1 Introduction 715 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 715–726 April 19 - 23, 2021. ©2021 Association for Computational Linguistics BiAfﬁne Attention Zhang et al., 2020 BiGRU (single EDU) BiGRU (inter-EDU) CNN (kernel=2) BiGRU ("
2021.eacl-main.60,C16-1179,0,0.111074,"ion boundary to split a sequence of discourse units into two sub-sequences of discourse units. This way, we are able to simplify the architecture, in eliminating the decoder as well as reducing the search space for splitting points. Specifically, we Related Work Previous work on RST parsing has been dominated by bottom-up approaches (Hernault et al., 2010; Joty et al., 2013; Li et al., 2016; Braud et al., 2017; Wang et al., 2017). For example, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, wh"
2021.eacl-main.60,P19-1356,0,0.0164152,"and (3) position. The input vector is computed by summing these three embeddings, and fed into BERT (initialized with bert-base). The output of BERT ... MLP Layer Encoder 3.2 ỹEm Figure 4: Architecture of the transformer model. In practice, 1 row of input can have more than two EDUs. gives us a contextualized embedding for each token, and we use the [CLS] embedding as the encoding for each EDU (gEj ). Unlike the LSTM model, we do not incorporate syntax embeddings into the transformer model as we found no empirical benefit (see Section 4.3). This observation is in line with other studies (e.g. Jawahar et al. (2019)) that have found BERT to implicit encode syntactic knowledge. For the segmenter we use a second transformer (initialized with random weights) to capture the inter-EDU relationships for sub-sequences of EDUs during iterative segmentation: {h0Em , .., h0En } = transformer({hEm , .., hEn }) y˜Ej = σ(MLP(h0Ej )) where y˜Ej gives the probability of a segmentation, and hEj is the concatenation of the output of BERT (gEj ) and the EDU type embedding (tEj ). 3.3 Nuclearity and Discourse Relation Prediction In Figure 5, we give an example of the iterative segmentation process to construct the RST tree"
2021.eacl-main.60,W01-1605,0,0.634548,"ntation order O to preserve as many subtrees as possible when an error occurs. We present pseudocode for the proposed dynamic oracle in Algorithm 1. The probability of using the ground truth segmentation or predicted segmentation during training is controlled by the hyper-parameter α ∈ [0, 1] (see Algorithm 1). Intuitively, this hyper-parameter allows the model to alternate between exploring its (possibly erroneous) segmentation or learning from the ground truth segmentation. The oracle reverts to its static variant when α = 0. 4 4.1 Experiments Data We use the English RST Discourse Treebank (Carlson et al., 2001) for our experiments, consistent with recent studies (Ji and Eisenstein, 2014; Li et al., 2014; Feng and Hirst, 2014; Yu et al., 2018). The dataset is based on the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), with 347 documents for training, and the remaining 38 documents for testing. We use the same development set as Yu et al. (2018), which consists of 35 documents selected from the training set. We also use the same 18 discourse labels. Stanford 720 Variant Vanilla +Syntax +Penalty +Syntax+Penalty LSTM Transformer 48.4±0.5 50.0±0.7 49.6±0.5 51.6±0.1 51.3±0.2 51.9±"
2021.eacl-main.60,P14-1002,0,0.676293,"ierarchically into discourse units, and the relationships between them. Unlike syntax parsing which models the relationship of words in a sentence, discourse parsing operates at the document-level, and aims to explain the flow of writing. Studies have found that discourse parsing is beneficial for downstream NLP tasks including document-level sentiment analysis (Bhatia et al., 2015) and abstractive summarization (Koto et al., 2019). Rhetorical Structure Theory (RST; Mann and Thompson (1988)) is one of the most widely used discourse theories in NLP (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2016; Yu et al., 2018). RST organizes text spans into a tree, where the leaves represent the basic unit of discourse, known as elementary discourse units (EDUs). EDUs are typically clauses of a sentence. Non-terminal nodes in the tree represent discourse unit relations. In Figure 1, we present an example RST tree with four EDUs spanning two sentences. In this discourse tree, EDUs are hierarchically connected with arrows and the discourse label elab. The direction of arrows indicates the nuclearity of relations, wherein a “satellite” points to its “nucleus”. The satellite unit is a"
2021.eacl-main.60,N19-1423,0,0.20877,"ation for Computational Linguistics, pages 715–726 April 19 - 23, 2021. ©2021 Association for Computational Linguistics BiAfﬁne Attention Zhang et al., 2020 BiGRU (single EDU) BiGRU (inter-EDU) CNN (kernel=2) BiGRU (ﬁnal encoder) GRU (decoder) stack Kobayashi et al., 2020 BiLSTM (single EDU) Gated MLP grouped by paragraph/ sentence/edu BiLSTM (inter-grouped unit) BiLSTM (inter-EDU) BiLSTM (seq labelling) queue Ours (v2) BERT (partial doc) Splitting via Deep BiAfﬁne Scoring exhaustive search space Ours (v1) BiLSTM (single EDU) use an LSTM (Hochreiter and Schmidhuber, 1997) or pre-trained BERT (Devlin et al., 2019) as the segmenter, enhanced in a number of key ways. Our primary contributions are as follows: (1) we propose a novel top-down approach to RST parsing based on sequence labelling; (2) we explore both traditional sequence models such as LSTMs and also modern pre-trained encoders such as BERT; (3) we demonstrate that adding a weighting mechanism during the splitting of EDU sequences improves performance; and (4) we propose a novel dynamic oracle for training top-down discourse parsers. .... Transformer (seq labelling) queue Figure 2: Comparison of our top-down models with Zhang et al. (2020) and"
2021.eacl-main.60,J95-3005,0,0.408828,"6; Braud et al., 2017; Wang et al., 2017). For example, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, while Liu et al. (2019) employ hierarchical pointer network parsers. Morey et al. (2017) found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging.3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval). Pr"
2021.eacl-main.60,P81-1022,0,0.528388,"Missing"
2021.eacl-main.60,P14-1048,0,0.377429,"texts are segmented hierarchically into discourse units, and the relationships between them. Unlike syntax parsing which models the relationship of words in a sentence, discourse parsing operates at the document-level, and aims to explain the flow of writing. Studies have found that discourse parsing is beneficial for downstream NLP tasks including document-level sentiment analysis (Bhatia et al., 2015) and abstractive summarization (Koto et al., 2019). Rhetorical Structure Theory (RST; Mann and Thompson (1988)) is one of the most widely used discourse theories in NLP (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2016; Yu et al., 2018). RST organizes text spans into a tree, where the leaves represent the basic unit of discourse, known as elementary discourse units (EDUs). EDUs are typically clauses of a sentence. Non-terminal nodes in the tree represent discourse unit relations. In Figure 1, we present an example RST tree with four EDUs spanning two sentences. In this discourse tree, EDUs are hierarchically connected with arrows and the discourse label elab. The direction of arrows indicates the nuclearity of relations, wherein a “satellite” points to its “nucleus”"
2021.eacl-main.60,P13-1048,0,0.0250831,"to gold span boundaries in evaluation. In this paper, we propose a conceptually simpler top-down approach for RST parsing. The core idea is to frame the problem as a sequence labelling task, where the goal is to iteratively find a segmentation boundary to split a sequence of discourse units into two sub-sequences of discourse units. This way, we are able to simplify the architecture, in eliminating the decoder as well as reducing the search space for splitting points. Specifically, we Related Work Previous work on RST parsing has been dominated by bottom-up approaches (Hernault et al., 2010; Joty et al., 2013; Li et al., 2016; Braud et al., 2017; Wang et al., 2017). For example, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and lang"
2021.eacl-main.60,J15-3002,0,0.101885,"del in the remainder of our experiments. We next benchmark our models against state-ofthe-art RST parsers over the test set, as presented in Table 2 (original Parseval) and Table 5 (RSTParseval as additional result). Except Yu et al. (2018), all bottom-up results are from Morey et al. (2017). We present the labelled attachment decision performance for Yu et al. (2018) by running the code of the authors for three runs and taking 9 721 The result is consistent with the test set (see Appendix B) Method S N R F Bottom Up: Feng and Hirst (2014)*† Ji and Eisenstein (2014)*† Surdeanu et al. (2015)*† Joty et al. (2015)* Hayashi et al. (2016)* Li et al. (2016)* Braud et al. (2017)* Yu et al. (2018) (static)‡ Yu et al. (2018) (dynamic)‡ 68.6 64.1 65.3 65.1 65.1 64.5 62.7 71.1 71.4 55.9 54.2 54.2 55.5 54.6 54.0 54.5 59.7 60.3 45.8 46.8 45.1 45.1 44.7 38.1 45.5 48.4 49.2 44.6 46.3 44.2 44.3 44.1 36.6 45.1 47.4 48.1 Top Down: Zhang et al. (2020)* 67.2 55.5 45.3 44.3 Our model Transformer (static)‡ Transformer (dynamic)‡ LSTM (static)‡ LSTM (dynamic)‡ 70.6 70.2 72.7 73.1 59.9 60.1 61.7 62.3 50.6 50.6 50.5 51.5 49.0 49.2 49.4 50.3 78.7 66.8 57.1 #Docs #Spans (0, 50] 21 404 (50, 100] 9 (100, 150] (150, ∞) Type S N"
2021.eacl-main.60,W07-2215,0,0.0483007,"e, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, while Liu et al. (2019) employ hierarchical pointer network parsers. Morey et al. (2017) found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging.3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval). Pre-trained language models (Radford et al., 2018; Devlin et"
2021.eacl-main.60,U19-1010,1,0.885145,"Missing"
2021.eacl-main.60,C12-1059,0,0.0376299,"DU2:2 ) 3.4 n  X where yEi ∈ {0, 1} is the ground truth segmentation label, L(Em:n ) is the cross-entropy loss for an EDU sequence, S is the set of all EDU sequences (based on ground truth segmentation), and β is a scaling hyper-parameter. To summarize, the total training loss of our model is a (weighted) combination of segmentation loss (Lseg ) and nuclearity-discourse prediction loss (Lnuc+dis ): L = λ1 Lseg + λ2 Lnuc+dis 3.5 (2) Dynamic Oracle The training regimen for discourse parsing creates an exposure bias, where the parser may struggle to recover when it makes a mistake at test time. Goldberg and Nivre (2012) propose a dynamic oracle for transition-based dependency parsing to tackle this. The idea is to allow the model during training to use its predictions (instead of ground truth actions), and introduce a dynamic oracle to find the next best/optimal action sequences. It does so by comparing the current state of the constructed tree and the gold-standard tree, and aims to minimize the deviation. As the model is exposed to prediction errors during training time, it has a better chance of recovering from them at test time. We explore a similar idea, and propose a dynamic oracle for our top-down dis"
2021.eacl-main.60,D14-1220,0,0.0850361,"for the proposed dynamic oracle in Algorithm 1. The probability of using the ground truth segmentation or predicted segmentation during training is controlled by the hyper-parameter α ∈ [0, 1] (see Algorithm 1). Intuitively, this hyper-parameter allows the model to alternate between exploring its (possibly erroneous) segmentation or learning from the ground truth segmentation. The oracle reverts to its static variant when α = 0. 4 4.1 Experiments Data We use the English RST Discourse Treebank (Carlson et al., 2001) for our experiments, consistent with recent studies (Ji and Eisenstein, 2014; Li et al., 2014; Feng and Hirst, 2014; Yu et al., 2018). The dataset is based on the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), with 347 documents for training, and the remaining 38 documents for testing. We use the same development set as Yu et al. (2018), which consists of 35 documents selected from the training set. We also use the same 18 discourse labels. Stanford 720 Variant Vanilla +Syntax +Penalty +Syntax+Penalty LSTM Transformer 48.4±0.5 50.0±0.7 49.6±0.5 51.6±0.1 51.3±0.2 51.9±0.4 52.1±0.4 51.8±0.8 configuration as for the LSTM model. We tuned the segmentation loss pena"
2021.eacl-main.60,W16-3616,0,0.0325496,"Missing"
2021.eacl-main.60,D16-1035,0,0.676564,"rse units, and the relationships between them. Unlike syntax parsing which models the relationship of words in a sentence, discourse parsing operates at the document-level, and aims to explain the flow of writing. Studies have found that discourse parsing is beneficial for downstream NLP tasks including document-level sentiment analysis (Bhatia et al., 2015) and abstractive summarization (Koto et al., 2019). Rhetorical Structure Theory (RST; Mann and Thompson (1988)) is one of the most widely used discourse theories in NLP (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2016; Yu et al., 2018). RST organizes text spans into a tree, where the leaves represent the basic unit of discourse, known as elementary discourse units (EDUs). EDUs are typically clauses of a sentence. Non-terminal nodes in the tree represent discourse unit relations. In Figure 1, we present an example RST tree with four EDUs spanning two sentences. In this discourse tree, EDUs are hierarchically connected with arrows and the discourse label elab. The direction of arrows indicates the nuclearity of relations, wherein a “satellite” points to its “nucleus”. The satellite unit is a supporting sente"
2021.eacl-main.60,P19-1410,0,0.612704,"parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, while Liu et al. (2019) employ hierarchical pointer network parsers. Morey et al. (2017) found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging.3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval). Pre-trained language models (Radford et al., 2018; Devlin et al., 2019) have been shown to benefit a multitude of NLP tas"
2021.eacl-main.60,J01-2004,0,0.445184,"). For example, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, while Liu et al. (2019) employ hierarchical pointer network parsers. Morey et al. (2017) found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging.3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval). Pre-trained language models (Radford et"
2021.eacl-main.60,D19-1093,0,0.0152729,"pose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, while Liu et al. (2019) employ hierarchical pointer network parsers. Morey et al. (2017) found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging.3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval). Pre-trained language models (Radford et al., 2018; Devlin et al., 2019) have been shown to benefit a multitude of NLP tasks, including discourse analysis. For example, BERT models have been used for classifying discourse markers (Sileo e"
2021.eacl-main.60,P99-1054,0,0.47651,", 2017; Wang et al., 2017). For example, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, while Liu et al. (2019) employ hierarchical pointer network parsers. Morey et al. (2017) found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging.3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval). Pre-trained language models"
2021.eacl-main.60,D19-1387,0,0.0224675,"want to encode sequences of EDUs that span multiple sentences. In our case, EDU truncation is not an option (since that would produce an incomplete RST tree), and the average number of words per document in our data is 521 (741 word pieces after BERT tokenization), which is much larger than the 512 limit. We therefore break the document into a number of partial documents, each consisting of multiple sentences that fit into the 512 token limit. This way, we allow the model to capture the fine-grained wordto-word relationships across (most) EDUs. Each partial document is then processed based on Liu and Lapata (2019) trick where we use an alternating even/odd segmentation embedding to encode all the EDUs in a document. We illustrate this approach in Figure 4. First, all EDUs are formatted to start with [CLS] and end with [SEP], and words are tokenized using WordPiece. If the document has more than 512 tokens, we break it into multiple partial documents based on EDU boundaries, and pad accordingly (e.g. in Figure 4 we break the example document of 3 EDUs into 2 partial documents), and process each partial document independently with BERT. We also experimented with the second alternative by encoding each ED"
2021.eacl-main.60,P14-5010,0,0.00352362,"of evaluation, we use the standard metrics introduced by Marcu (2000): Span, Nuclearity, Relation, and Full. We report micro-averaged F-1 scores on labelled attachment decisions (original Parseval), following the recommendation of Morey et al. (2017). Additionally, we also present the evaluation with RSTParseval procedure in Appendix A. Table 1: Feature addition study over the development set to find the best configuration for our models. Presented results are the mean and standard deviation of the Full metric (micro-averaged F-score on labelled attachment decisions) over three runs. CoreNLP (Manning et al., 2014) is used for POS tagging.6 4.2 Model Configurations We experiment with two segmentation models — LSTM (Section 3.1) and transformer (Section 3.2) — both implemented in PyTorch framework.7 As EDUs are provided in the dataset, no automatic segmentation of EDU is required in our experiments. For the LSTM model, the dimensionality of the Bi-LSTMs in the encoder is 256, while the segmenter (Bi-LSTM4 ) is 128 (Figure 3). The embedding dimensions of words, POS tags, EDU type, and syntax features are 200, 200, 100, and 1,200, respectively, and we initialize words in EDU with GloVe embedding (Penningto"
2021.eacl-main.60,J93-2004,0,0.0781138,"g training is controlled by the hyper-parameter α ∈ [0, 1] (see Algorithm 1). Intuitively, this hyper-parameter allows the model to alternate between exploring its (possibly erroneous) segmentation or learning from the ground truth segmentation. The oracle reverts to its static variant when α = 0. 4 4.1 Experiments Data We use the English RST Discourse Treebank (Carlson et al., 2001) for our experiments, consistent with recent studies (Ji and Eisenstein, 2014; Li et al., 2014; Feng and Hirst, 2014; Yu et al., 2018). The dataset is based on the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), with 347 documents for training, and the remaining 38 documents for testing. We use the same development set as Yu et al. (2018), which consists of 35 documents selected from the training set. We also use the same 18 discourse labels. Stanford 720 Variant Vanilla +Syntax +Penalty +Syntax+Penalty LSTM Transformer 48.4±0.5 50.0±0.7 49.6±0.5 51.6±0.1 51.3±0.2 51.9±0.4 52.1±0.4 51.8±0.8 configuration as for the LSTM model. We tuned the segmentation loss penalty hyperparameter β (Section 3.4) and the dynamic oracle hyper-parameter α (Section 3.5) based on the development set. Both the LSTM and tr"
2021.eacl-main.60,D17-1136,0,0.433734,"ting sentence for the nucleus unit and contains less prominent information. It is standard practice that the RST tree is trained and evaluated in a right-heavy binarized manner, resulting in three forms of binary nuclearity relationships between EDUs: Nucleus–Satellite, Satellite–Nucleus, and Nucleus–Nucleus. In this work, eighteen coarse-grained relations are considered as discourse labels, consistent with earlier work (Yu et al., 2018).2 Work on RST parsing has been dominated by the bottom-up paradigm (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Braud et al., 2017; Morey et al., 2017; Yu et al., 2018). These methods produce very competitive benchmarks, but in practice it is not a straightforward 1 Code and trained models: https://github.com/ fajri91/NeuralRST-TopDown 2 Details of individual relations can be found at: http: //www.sfu.ca/rst/index.html 1 Introduction 715 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 715–726 April 19 - 23, 2021. ©2021 Association for Computational Linguistics BiAfﬁne Attention Zhang et al., 2020 BiGRU (single EDU) BiGRU (inter-EDU) CNN (kernel=2) BiGRU (ﬁnal encoder) GRU (d"
2021.eacl-main.60,P19-1442,0,0.0228327,"r EDUs) to produce the final encoding: ỹEn Segmenter MLP Layer BI-LSTM4 hEm ... hEn Get Sub-Sequence hE1 hE2 hE3 ... hEq BI-LSTM3 Encoder tE1 ... gE1 Avg Pooling Avg Pooling BI-LSTM1 BI-LSTM2 xi = wi ⊕ pi tEq gEq w {aw 1 , .., ap } = Bi-LSTM1 ({x1 , .., xp }) Avg Pooling Avg Pooling BI-LSTM1 BI-LSTM2 w1 w2 s1 p1 p2 {as1 , ..., asp } = Bi-LSTM2 ({s1 , .., sp }) w gEj = Avg-Pool({aw 1 , .., ap })⊕ Avg-Pool({as1 , .., asp }) ⊕ tEj ... w1 w2 p1 p2 s1 s2 Syntax Feature Syntax Feature Word and POS EDU1 s2 Word and POS ... EDUq Figure 3: Architecture of the LSTM model. 2019) and discourse relations (Nie et al., 2019; Shi and Demberg, 2019). To the best of our knowledge, however, pre-trained models have not been applied in the generation of full discourse trees, which we address here by experimenting with BERT for topdown RST parsing. 3 Top-down RST Parsing We frame RST parsing as a sequence labelling task, where given a sequence of input EDUs, the goal is to find a segmentation boundary to split the sequence into two sub-sequences. This is realized by training a sequence labelling model to predict a binary label for each EDU, and select the EDU with the highest probability to be the segmentation point. A"
2021.eacl-main.60,D14-1162,0,0.0922945,"l., 2014) is used for POS tagging.6 4.2 Model Configurations We experiment with two segmentation models — LSTM (Section 3.1) and transformer (Section 3.2) — both implemented in PyTorch framework.7 As EDUs are provided in the dataset, no automatic segmentation of EDU is required in our experiments. For the LSTM model, the dimensionality of the Bi-LSTMs in the encoder is 256, while the segmenter (Bi-LSTM4 ) is 128 (Figure 3). The embedding dimensions of words, POS tags, EDU type, and syntax features are 200, 200, 100, and 1,200, respectively, and we initialize words in EDU with GloVe embedding (Pennington et al., 2014).8 For hyper-parameters, we use the following: batch size = 4, gradient accumulation = 2, learning rate = 0.001, dropout probability = 0.5, and optimizer = Adam (with epsilon of 1e-6). The loss scaling hyper-parameters (Equation (2)), are tuned based on the development set, and set to λ1 = 1.0, and λ2 = 1.0. For the transformer model, the document length limit is set to 512 tokens, and longer documents are broken into smaller partial documents. As before, we truncate each EDU to the first 50 words. We initialize the transformer in the encoder with bert-base, and the transformer in the segmente"
2021.eacl-main.60,D19-1586,0,0.0205247,"the final encoding: ỹEn Segmenter MLP Layer BI-LSTM4 hEm ... hEn Get Sub-Sequence hE1 hE2 hE3 ... hEq BI-LSTM3 Encoder tE1 ... gE1 Avg Pooling Avg Pooling BI-LSTM1 BI-LSTM2 xi = wi ⊕ pi tEq gEq w {aw 1 , .., ap } = Bi-LSTM1 ({x1 , .., xp }) Avg Pooling Avg Pooling BI-LSTM1 BI-LSTM2 w1 w2 s1 p1 p2 {as1 , ..., asp } = Bi-LSTM2 ({s1 , .., sp }) w gEj = Avg-Pool({aw 1 , .., ap })⊕ Avg-Pool({as1 , .., asp }) ⊕ tEj ... w1 w2 p1 p2 s1 s2 Syntax Feature Syntax Feature Word and POS EDU1 s2 Word and POS ... EDUq Figure 3: Architecture of the LSTM model. 2019) and discourse relations (Nie et al., 2019; Shi and Demberg, 2019). To the best of our knowledge, however, pre-trained models have not been applied in the generation of full discourse trees, which we address here by experimenting with BERT for topdown RST parsing. 3 Top-down RST Parsing We frame RST parsing as a sequence labelling task, where given a sequence of input EDUs, the goal is to find a segmentation boundary to split the sequence into two sub-sequences. This is realized by training a sequence labelling model to predict a binary label for each EDU, and select the EDU with the highest probability to be the segmentation point. After the sequence is seg"
2021.eacl-main.60,N19-1351,0,0.0300142,"Missing"
2021.eacl-main.60,N15-3001,0,0.053505,"Missing"
2021.eacl-main.60,P17-2029,0,0.0925473,"e propose a conceptually simpler top-down approach for RST parsing. The core idea is to frame the problem as a sequence labelling task, where the goal is to iteratively find a segmentation boundary to split a sequence of discourse units into two sub-sequences of discourse units. This way, we are able to simplify the architecture, in eliminating the decoder as well as reducing the search space for splitting points. Specifically, we Related Work Previous work on RST parsing has been dominated by bottom-up approaches (Hernault et al., 2010; Joty et al., 2013; Li et al., 2016; Braud et al., 2017; Wang et al., 2017). For example, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; R"
2021.eacl-main.60,C18-1047,0,0.0811348,"e relationships between them. Unlike syntax parsing which models the relationship of words in a sentence, discourse parsing operates at the document-level, and aims to explain the flow of writing. Studies have found that discourse parsing is beneficial for downstream NLP tasks including document-level sentiment analysis (Bhatia et al., 2015) and abstractive summarization (Koto et al., 2019). Rhetorical Structure Theory (RST; Mann and Thompson (1988)) is one of the most widely used discourse theories in NLP (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2016; Yu et al., 2018). RST organizes text spans into a tree, where the leaves represent the basic unit of discourse, known as elementary discourse units (EDUs). EDUs are typically clauses of a sentence. Non-terminal nodes in the tree represent discourse unit relations. In Figure 1, we present an example RST tree with four EDUs spanning two sentences. In this discourse tree, EDUs are hierarchically connected with arrows and the discourse label elab. The direction of arrows indicates the nuclearity of relations, wherein a “satellite” points to its “nucleus”. The satellite unit is a supporting sentence for the nucleu"
2021.eacl-main.60,2020.acl-main.569,0,0.621727,"t al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Braud et al., 2017; Morey et al., 2017; Yu et al., 2018). These methods produce very competitive benchmarks, but in practice it is not a straightforward 1 Code and trained models: https://github.com/ fajri91/NeuralRST-TopDown 2 Details of individual relations can be found at: http: //www.sfu.ca/rst/index.html 1 Introduction 715 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 715–726 April 19 - 23, 2021. ©2021 Association for Computational Linguistics BiAfﬁne Attention Zhang et al., 2020 BiGRU (single EDU) BiGRU (inter-EDU) CNN (kernel=2) BiGRU (ﬁnal encoder) GRU (decoder) stack Kobayashi et al., 2020 BiLSTM (single EDU) Gated MLP grouped by paragraph/ sentence/edu BiLSTM (inter-grouped unit) BiLSTM (inter-EDU) BiLSTM (seq labelling) queue Ours (v2) BERT (partial doc) Splitting via Deep BiAfﬁne Scoring exhaustive search space Ours (v1) BiLSTM (single EDU) use an LSTM (Hochreiter and Schmidhuber, 1997) or pre-trained BERT (Devlin et al., 2019) as the segmenter, enhanced in a number of key ways. Our primary contributions are as follows: (1) we propose a novel top-down approach"
2021.emnlp-main.833,W19-1909,0,0.0368597,"Missing"
2021.emnlp-main.833,Q17-1010,0,0.128687,"Missing"
2021.emnlp-main.833,2020.findings-emnlp.118,0,0.0605214,"Missing"
2021.emnlp-main.833,P11-2008,0,0.0642259,"Missing"
2021.emnlp-main.833,2020.aacl-main.60,1,0.761881,"e backan Indonesian BERT model for Twitter, and show bone of modern NLP systems, due to their success across various languages and tasks. However, ob- that initializing domain-specific vocabulary with average-pooling of BERT subword embeddings is taining high-quality contextualized representations for specific domains/data sources such as biomedi- more efficient than pretraining from scratch, and more effective than initializing based on word2vec cal, social media, and legal, remains a challenge. Previous studies (Alsentzer et al., 2019; projections (Poerner et al., 2020). We use I N DO BERT (Koto et al., 2020b), a monolingual Chalkidis et al., 2020; Nguyen et al., 2020) have BERT for Indonesian as the domain-general model shown that for domain-specific text, pretraining to develop a pretrained domain-specific model I N from scratch outperforms off-the-shelf BERT. As an alternative approach with lower cost, Gururan- DO BERT WEET for Indonesian Twitter. gan et al. (2020) demonstrated that domain adapThere are two primary reasons to experiment tive pretraining (i.e. pretraining the model on target with Indonesian Twitter. First, despite being the ofdomain text before task fine-tuning) is effective, f"
2021.emnlp-main.833,2020.coling-main.66,1,0.878257,"e backan Indonesian BERT model for Twitter, and show bone of modern NLP systems, due to their success across various languages and tasks. However, ob- that initializing domain-specific vocabulary with average-pooling of BERT subword embeddings is taining high-quality contextualized representations for specific domains/data sources such as biomedi- more efficient than pretraining from scratch, and more effective than initializing based on word2vec cal, social media, and legal, remains a challenge. Previous studies (Alsentzer et al., 2019; projections (Poerner et al., 2020). We use I N DO BERT (Koto et al., 2020b), a monolingual Chalkidis et al., 2020; Nguyen et al., 2020) have BERT for Indonesian as the domain-general model shown that for domain-specific text, pretraining to develop a pretrained domain-specific model I N from scratch outperforms off-the-shelf BERT. As an alternative approach with lower cost, Gururan- DO BERT WEET for Indonesian Twitter. gan et al. (2020) demonstrated that domain adapThere are two primary reasons to experiment tive pretraining (i.e. pretraining the model on target with Indonesian Twitter. First, despite being the ofdomain text before task fine-tuning) is effective, f"
2021.emnlp-main.833,N18-1088,0,0.0516435,"Missing"
2021.emnlp-main.833,2021.ccl-1.108,0,0.0807347,"Missing"
2021.emnlp-main.833,2020.emnlp-demos.2,0,0.134861,"f modern NLP systems, due to their success across various languages and tasks. However, ob- that initializing domain-specific vocabulary with average-pooling of BERT subword embeddings is taining high-quality contextualized representations for specific domains/data sources such as biomedi- more efficient than pretraining from scratch, and more effective than initializing based on word2vec cal, social media, and legal, remains a challenge. Previous studies (Alsentzer et al., 2019; projections (Poerner et al., 2020). We use I N DO BERT (Koto et al., 2020b), a monolingual Chalkidis et al., 2020; Nguyen et al., 2020) have BERT for Indonesian as the domain-general model shown that for domain-specific text, pretraining to develop a pretrained domain-specific model I N from scratch outperforms off-the-shelf BERT. As an alternative approach with lower cost, Gururan- DO BERT WEET for Indonesian Twitter. gan et al. (2020) demonstrated that domain adapThere are two primary reasons to experiment tive pretraining (i.e. pretraining the model on target with Indonesian Twitter. First, despite being the ofdomain text before task fine-tuning) is effective, ficial language of the 5th most populous nation, Inalthough sti"
2021.emnlp-main.833,D11-1141,0,0.189252,"Missing"
2021.emnlp-main.833,S17-2088,0,0.0817598,"Missing"
2021.emnlp-main.833,P16-1162,0,0.142671,"Missing"
2021.emnlp-main.833,W16-3919,0,0.0363854,"Missing"
2021.emnlp-main.833,2020.findings-emnlp.129,0,0.0298012,"othy Baldwin School of Computing and Information Systems The University of Melbourne ffajri@student.unimelb.edu.au, jeyhan.lau@gmail.com, tb@ldwin.net Abstract term in biology. To tackle this problem, Poerner et al. (2020); Tai et al. (2020) proposed simple We present I NDO BERT WEET, the first largemethods to domain-extend the BERT vocabulary: scale pretrained model for Indonesian Twitter Poerner et al. (2020) initialize new vocabulary usthat is trained by extending a monolinguallying a learned projection from word2vec (Mikolov trained Indonesian BERT model with additive et al., 2013), while Tai et al. (2020) use random inidomain-specific vocabulary. We focus in particular on efficient model adaptation under votialization with weight augmentation, substantially cabulary mismatch, and benchmark different increasing the number of model parameters. ways of initializing the BERT embedding layer New vocabulary augmentation has been also confor new word types. We find that initializing ducted for language-adaptive pretraining, mainly with the average BERT subword embedding based on multilingual BERT (M BERT). For inmakes pretraining five times faster, and is more stance, Chau et al. (2020) replace 99 “u"
2021.emnlp-main.833,S18-1005,0,0.0535351,"Missing"
2021.emnlp-main.833,2020.findings-emnlp.240,0,0.0215308,"umber of model parameters. ways of initializing the BERT embedding layer New vocabulary augmentation has been also confor new word types. We find that initializing ducted for language-adaptive pretraining, mainly with the average BERT subword embedding based on multilingual BERT (M BERT). For inmakes pretraining five times faster, and is more stance, Chau et al. (2020) replace 99 “unused” effective than proposed methods for vocabuWordPiece tokens of M BERT with new comlary adaptation in terms of extrinsic evaluation 1 over seven Twitter-based datasets. mon tokens in the target language, while Wang et al. (2020) extend M BERT vocabulary with non1 Introduction overlapping tokens (|VM BERT − Vnew |). These two approaches use random initialization for new WordTransformer-based pretrained language models (Vaswani et al., 2017; Devlin et al., 2019; Liu et al., Piece token embeddings. In this paper, we focus on the task of learning 2019; Radford et al., 2019) have become the backan Indonesian BERT model for Twitter, and show bone of modern NLP systems, due to their success across various languages and tasks. However, ob- that initializing domain-specific vocabulary with average-pooling of BERT subword embe"
2021.emnlp-main.833,2020.aacl-main.85,0,0.596838,"st, Gururan- DO BERT WEET for Indonesian Twitter. gan et al. (2020) demonstrated that domain adapThere are two primary reasons to experiment tive pretraining (i.e. pretraining the model on target with Indonesian Twitter. First, despite being the ofdomain text before task fine-tuning) is effective, ficial language of the 5th most populous nation, Inalthough still not as good as training from scratch. donesian is underrepresented in NLP (notwithstandThe main drawback of domain-adaptive pretrain- ing recent Indonesian benchmarks and datasets ing is that domain-specific words that are not in the (Wilie et al., 2020; Koto et al., 2020a,b)). Secpretrained vocabulary are often tokenized poorly. ond, with a large user base, Twitter is often utilized For instance, in B IO BERT (Lee et al., 2019), Im- to support policymakers, business (Fiarni et al., munoglobulin is tokenized into {I, ##mm, ##uno, 2016), or to monitor elections (Suciati et al., 2019) ##g, ##lo, ##bul, ##in}, despite being a common or health issues (Prastyo et al., 2020). Note that 1 most previous studies that target Indonesian TwitCode and models can be accessed at https:// github.com/indolem/IndoBERTweet ter tend to use traditional machine l"
2021.findings-acl.71,2020.acl-main.175,0,0.0243273,"omitted from this study). ROUGE (Lin, 2004) measures the lexical overlap between the system and reference summary; based on the findings of Graham (2015), we consider 7 variants in this paper: ROUGE1 (unigram), ROUGE-2 (bigram), ROUGE-3 (trigram), ROUGE-L (LCS), ROUGE-S (skipbigram), ROUGE-SU (skip-bigram plus unigram), and ROUGE-W (weighted LCS).4 METEOR (Lavie and Agarwal, 2007) performs word-to-word matching based on word-alignment, and was originally developed for MT but has recently been used for summarization evaluation (See et al., 2017; Chen and Bansal, 2018; Falke and Gurevych, 2019; Amplayo and Lapata, 2020).5 BLEU (Papineni et al., 2002) is a precisionbased metric originally developed for MT, which measures the n-gram match between the reference and system summary. Based on the findings of Graham (2015), we use BLEU-4 according to the SacreBLEU implementation (Post, 2018).6 MoverScore (Zhao et al., 2019) measures the Euclidean distance between two contextualized BERT representations, and relies on soft alignDUC 2001, 2002, 2003 802 4 https://github.com/bheinzerling/pyrouge http://www.cs.cmu.edu/∼alavie/METEOR/ 6 https://github.com/mjpost/sacrebleu 5 ments of words learned by solving an optimisat"
2021.findings-acl.71,N19-1074,0,0.0273267,"n and Goharian, 2016) are omitted from this study). ROUGE (Lin, 2004) measures the lexical overlap between the system and reference summary; based on the findings of Graham (2015), we consider 7 variants in this paper: ROUGE1 (unigram), ROUGE-2 (bigram), ROUGE-3 (trigram), ROUGE-L (LCS), ROUGE-S (skipbigram), ROUGE-SU (skip-bigram plus unigram), and ROUGE-W (weighted LCS).4 METEOR (Lavie and Agarwal, 2007) performs word-to-word matching based on word-alignment, and was originally developed for MT but has recently been used for summarization evaluation (See et al., 2017; Chen and Bansal, 2018; Falke and Gurevych, 2019; Amplayo and Lapata, 2020).5 BLEU (Papineni et al., 2002) is a precisionbased metric originally developed for MT, which measures the n-gram match between the reference and system summary. Based on the findings of Graham (2015), we use BLEU-4 according to the SacreBLEU implementation (Post, 2018).6 MoverScore (Zhao et al., 2019) measures the Euclidean distance between two contextualized BERT representations, and relies on soft alignDUC 2001, 2002, 2003 802 4 https://github.com/bheinzerling/pyrouge http://www.cs.cmu.edu/∼alavie/METEOR/ 6 https://github.com/mjpost/sacrebleu 5 ments of words lear"
2021.findings-acl.71,2020.emnlp-main.751,0,0.147623,"-efficient. In proposing these metrics, the authors measured correlation with human judgments based on English datasets that are not representative of modern summarization systems. For instance, Lin (2004) use DUC1 2001–2003 for ROUGE (meaning summaries were generated with largely outdated extractive summarization systems); Zhao et al. (2019) use the TAC2 dataset for MoverScore (again, featuring summaries from largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et al. (2020b) developed BERTScore based on a machine translation corpus (WMT). In contemporaneous work, Bhandari et al. (2020) address this issue by annotating English CNN/DailyMail summaries produced by recent summarization models, and found disparities over results from TAC. 1 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/ Equally troublingly, ROUGE has become the default summarization evaluation metric for languages other than English (Hu et al., 2015; Scialom et al., 2020; Ladhak et al., 2020; Koto et al., 2020b), despite there being no systematic validation of its efficacy across other languages. The questions we ask in this study, therefore, are twofold: (1) How well do existing automatic metrics p"
2021.findings-acl.71,D15-1013,0,0.0597712,"EN), Indonesian (ID), French (FR), Turkish (TR), Mandarin Chinese (ZH), Russian (RU), German (DE), and Spanish (ES); (2) we evaluate an extensive range of traditional and model-based metrics, and find BERTScore to be the best metric for evaluating both focus and coverage; and (3) we release a manually-annotated multilingual resource for summarization evaluation comprising 4,320 annotations. Data and code used in this paper is available at: https://github.com/ fajri91/Multi SummEval. 2 Related Work As with much of NLP, research on automatic summarization metrics has been highly Englishcentric. Graham (2015) comprehensively evaluated 192 ROUGE variations based on the DUC2004 (English) dataset. Bhandari et al. (2020) released a new (English) evaluation dataset by annotating CNN/DailyMail using simplified Pyramid (Nenkova and Passonneau, 2004). First, semantic content units (SCUs) were manually extracted from the reference, and crowd-workers were then asked to count the number of SCUs in the system summary. Their annotation procedure does not specifically consider focus, but is closely related to the coverage aspect of our work. Similarly, Fabbri et al. (2020) annotated the (English) CNN/DailyMail"
2021.findings-acl.71,P18-1063,0,0.0234824,"., 2010) and RESA (Cohan and Goharian, 2016) are omitted from this study). ROUGE (Lin, 2004) measures the lexical overlap between the system and reference summary; based on the findings of Graham (2015), we consider 7 variants in this paper: ROUGE1 (unigram), ROUGE-2 (bigram), ROUGE-3 (trigram), ROUGE-L (LCS), ROUGE-S (skipbigram), ROUGE-SU (skip-bigram plus unigram), and ROUGE-W (weighted LCS).4 METEOR (Lavie and Agarwal, 2007) performs word-to-word matching based on word-alignment, and was originally developed for MT but has recently been used for summarization evaluation (See et al., 2017; Chen and Bansal, 2018; Falke and Gurevych, 2019; Amplayo and Lapata, 2020).5 BLEU (Papineni et al., 2002) is a precisionbased metric originally developed for MT, which measures the n-gram match between the reference and system summary. Based on the findings of Graham (2015), we use BLEU-4 according to the SacreBLEU implementation (Post, 2018).6 MoverScore (Zhao et al., 2019) measures the Euclidean distance between two contextualized BERT representations, and relies on soft alignDUC 2001, 2002, 2003 802 4 https://github.com/bheinzerling/pyrouge http://www.cs.cmu.edu/∼alavie/METEOR/ 6 https://github.com/mjpost/sacre"
2021.findings-acl.71,N15-1124,1,0.837557,"Missing"
2021.findings-acl.71,L16-1130,0,0.017336,"erlap, and the longest common subsequence (“LCS”) between reference and system summaries. In other work, Saggion et al. (2010) investigated coverage, responsiveness, and pyramids for several extractive models in English, French, and Spanish. To the best of our knowledge, we are the first to systemically quantify the panlinguistic efficacy of evaluation metrics for modern summarization systems. 3 Evaluation Metrics We assess a total of 19 different evaluation metrics that are commonly used in summarization research (noting that lesser-used metrics such as FRESA (Saggion et al., 2010) and RESA (Cohan and Goharian, 2016) are omitted from this study). ROUGE (Lin, 2004) measures the lexical overlap between the system and reference summary; based on the findings of Graham (2015), we consider 7 variants in this paper: ROUGE1 (unigram), ROUGE-2 (bigram), ROUGE-3 (trigram), ROUGE-L (LCS), ROUGE-S (skipbigram), ROUGE-SU (skip-bigram plus unigram), and ROUGE-W (weighted LCS).4 METEOR (Lavie and Agarwal, 2007) performs word-to-word matching based on word-alignment, and was originally developed for MT but has recently been used for summarization evaluation (See et al., 2017; Chen and Bansal, 2018; Falke and Gurevych, 2"
2021.findings-acl.71,P19-1330,0,0.112854,"oped for English are routinely applied to other languages, this is the first attempt to systematically quantify their panlinguistic efficacy. We take a summarization corpus for eight different languages, and manually annotate generated summaries for focus (precision) and coverage (recall). Based on this, we evaluate 19 summarization evaluation metrics, and find that using multilingual BERT within BERTScore performs well across all languages, at a level above that for English. 1 Figure 1: Illustration of focus and coverage. Introduction Although manual evaluation (Nenkova and Passonneau, 2004; Hardy et al., 2019) of text summarization is more reliable and interpretable, most research on text summarization employs automatic evaluations such as ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020b) because they are time- and cost-efficient. In proposing these metrics, the authors measured correlation with human judgments based on English datasets that are not representative of modern summarization systems. For instance, Lin (2004) use DUC1 2001–2003 for ROUGE (meaning summaries were generated with largely outdated extractive summarization"
2021.findings-acl.71,D15-1229,0,0.0202696,"TAC2 dataset for MoverScore (again, featuring summaries from largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et al. (2020b) developed BERTScore based on a machine translation corpus (WMT). In contemporaneous work, Bhandari et al. (2020) address this issue by annotating English CNN/DailyMail summaries produced by recent summarization models, and found disparities over results from TAC. 1 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/ Equally troublingly, ROUGE has become the default summarization evaluation metric for languages other than English (Hu et al., 2015; Scialom et al., 2020; Ladhak et al., 2020; Koto et al., 2020b), despite there being no systematic validation of its efficacy across other languages. The questions we ask in this study, therefore, are twofold: (1) How well do existing automatic metrics perform over languages other than English? and (2) What automatic metric works best across different languages? In this paper, we examine content-based summarization evaluation from the aspects of precision and recall, in the form of focus and coverage to compare system-generated summaries to groundtruth summaries (see Figure 1). As advocated b"
2021.findings-acl.71,2020.aacl-main.60,1,0.788782,"m largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et al. (2020b) developed BERTScore based on a machine translation corpus (WMT). In contemporaneous work, Bhandari et al. (2020) address this issue by annotating English CNN/DailyMail summaries produced by recent summarization models, and found disparities over results from TAC. 1 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/ Equally troublingly, ROUGE has become the default summarization evaluation metric for languages other than English (Hu et al., 2015; Scialom et al., 2020; Ladhak et al., 2020; Koto et al., 2020b), despite there being no systematic validation of its efficacy across other languages. The questions we ask in this study, therefore, are twofold: (1) How well do existing automatic metrics perform over languages other than English? and (2) What automatic metric works best across different languages? In this paper, we examine content-based summarization evaluation from the aspects of precision and recall, in the form of focus and coverage to compare system-generated summaries to groundtruth summaries (see Figure 1). As advocated by Koto et al. (2020a), focus and coverage are more interpretab"
2021.findings-acl.71,2020.coling-main.66,1,0.74331,"m largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et al. (2020b) developed BERTScore based on a machine translation corpus (WMT). In contemporaneous work, Bhandari et al. (2020) address this issue by annotating English CNN/DailyMail summaries produced by recent summarization models, and found disparities over results from TAC. 1 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/ Equally troublingly, ROUGE has become the default summarization evaluation metric for languages other than English (Hu et al., 2015; Scialom et al., 2020; Ladhak et al., 2020; Koto et al., 2020b), despite there being no systematic validation of its efficacy across other languages. The questions we ask in this study, therefore, are twofold: (1) How well do existing automatic metrics perform over languages other than English? and (2) What automatic metric works best across different languages? In this paper, we examine content-based summarization evaluation from the aspects of precision and recall, in the form of focus and coverage to compare system-generated summaries to groundtruth summaries (see Figure 1). As advocated by Koto et al. (2020a), focus and coverage are more interpretab"
2021.findings-acl.71,2020.findings-emnlp.360,0,0.0260883,"aturing summaries from largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et al. (2020b) developed BERTScore based on a machine translation corpus (WMT). In contemporaneous work, Bhandari et al. (2020) address this issue by annotating English CNN/DailyMail summaries produced by recent summarization models, and found disparities over results from TAC. 1 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/ Equally troublingly, ROUGE has become the default summarization evaluation metric for languages other than English (Hu et al., 2015; Scialom et al., 2020; Ladhak et al., 2020; Koto et al., 2020b), despite there being no systematic validation of its efficacy across other languages. The questions we ask in this study, therefore, are twofold: (1) How well do existing automatic metrics perform over languages other than English? and (2) What automatic metric works best across different languages? In this paper, we examine content-based summarization evaluation from the aspects of precision and recall, in the form of focus and coverage to compare system-generated summaries to groundtruth summaries (see Figure 1). As advocated by Koto et al. (2020a), focus and coverage a"
2021.findings-acl.71,2020.tacl-1.20,1,0.822724,"rately high. However, the agreement does vary, and it affects the interpretation of the correlation scores when we assess the automatic metrics later. Note also that we get the lowest score for English, meaning the results for non-English languages are actually more 16 We approved all HITs with at least 30 minutes working time and a minimum quality control score of 5, irrespective of whether they passed the higher quality-control threshold required for the ground truth. 804 17 In MTurk, we did not set a specific location for Chinese because we found there are no workers in China. 18 We follow Lau et al. (2020) in computing one-vs-rest correlation: we randomly isolate a worker’s score (for each sample) and compare it against the mean score of the rest using Pearson’s r, and repeat this for 1000 trials to get the mean correlation. robust.19 Although focus and coverage are positively correlated in Table 1, the distribution of scores varies quite a bit between languages: English annotation variance is higher than the other languages, and has the lowest correlation between focus and coverage (r = 0.57); for French, Russian, and Spanish, summaries generally have low focus and coverage (for more details,"
2021.findings-acl.71,W07-0734,0,0.446038,"ght different languages, and manually annotate generated summaries for focus (precision) and coverage (recall). Based on this, we evaluate 19 summarization evaluation metrics, and find that using multilingual BERT within BERTScore performs well across all languages, at a level above that for English. 1 Figure 1: Illustration of focus and coverage. Introduction Although manual evaluation (Nenkova and Passonneau, 2004; Hardy et al., 2019) of text summarization is more reliable and interpretable, most research on text summarization employs automatic evaluations such as ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020b) because they are time- and cost-efficient. In proposing these metrics, the authors measured correlation with human judgments based on English datasets that are not representative of modern summarization systems. For instance, Lin (2004) use DUC1 2001–2003 for ROUGE (meaning summaries were generated with largely outdated extractive summarization systems); Zhao et al. (2019) use the TAC2 dataset for MoverScore (again, featuring summaries from largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et a"
2021.findings-acl.71,W04-1013,0,0.287754,"zation corpus for eight different languages, and manually annotate generated summaries for focus (precision) and coverage (recall). Based on this, we evaluate 19 summarization evaluation metrics, and find that using multilingual BERT within BERTScore performs well across all languages, at a level above that for English. 1 Figure 1: Illustration of focus and coverage. Introduction Although manual evaluation (Nenkova and Passonneau, 2004; Hardy et al., 2019) of text summarization is more reliable and interpretable, most research on text summarization employs automatic evaluations such as ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020b) because they are time- and cost-efficient. In proposing these metrics, the authors measured correlation with human judgments based on English datasets that are not representative of modern summarization systems. For instance, Lin (2004) use DUC1 2001–2003 for ROUGE (meaning summaries were generated with largely outdated extractive summarization systems); Zhao et al. (2019) use the TAC2 dataset for MoverScore (again, featuring summaries from largely defunct systems; see Peyrard (2019) and Ran"
2021.findings-acl.71,W02-0406,0,0.277386,"d reliable to evaluate focus and coverage based on the source document than the ground-truth summary, we use the ground-truth summary in this research for the following reasons. First, historically, validation of automatic evaluation metrics for summarization has been based primarily on ground-truth summaries (not source documents). Second, ROUGE (Lin, 801 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 801–812 August 1–6, 2021. ©2021 Association for Computational Linguistics 2004) was initially motivated and assessed based on coverage over the DUC datasets3 (Lin and Hovy, 2002) using annotations based on reference summaries (not source documents). Third, although it is certainly possible to generate different summaries for the same source document, we argue that the variance in content is actually not that great, especially for single-document summarization. Lastly, basing human evaluation (of focus and coverage) on the source article leads to more complicated annotation schemes, and has been shown to yield poor annotations (Nenkova and Passonneau, 2004; Fabbri et al., 2020). In summary, this paper makes three contributions: (1) we carry out the first systematic att"
2021.findings-acl.71,D19-1387,0,0.0152401,"nto 1 task, as the only thing that differentiates them is the ordering of the system and reference summaries, which is opaque to the annotators.14 Workers responded by scoring based via a slider button (continuous scale of 1–100).15 For each HIT, we create 10 samples for quality control: 5 samples are random pairs (should be 7 https://github.com/AIPHES/emnlp19-moverscore https://github.com/Tiiiger/bert score 9 Note that both multilingual BERT and XLM were explicitly trained over all eight target languages used in this paper. 10 English, Indonesian and Chinese summaries were generated with the Liu and Lapata (2019) model, and the Dong et al. (2019) model was used for the MLSUM-based languages. 8 803 11 Summaries for all datasets except LCSTS were sourced from the authors of the dataset. For LCSTS, we trained the two models ourselves based on the training data. 12 BERT-based summaries are representative of transformerbased model, and the ROUGE score gap over state-of-the-art models (Zhang et al., 2020a) for English is only ∼2 points. 13 https://www.mturk.com 14 For focus, the first text is the reference and the second text the system summary; for coverage, the order is reversed. 15 See Appendix for the M"
2021.findings-acl.71,2020.acl-main.448,1,0.706094,"verage (for more details, see scatterplots of focus-coverage in Figure 2 of the Appendix). 5.2 Correlation with Automatic Evaluation In Table 2 we present the Pearson correlation between the human annotations and various automatic metrics, broken down across language and focus vs. coverage, and (naively) aggregated across languages in the form of the average correlation. We also include the one-vs-rest annotator correlation (Section 5.1) in the last row, as it can be interpreted as the average performance of a single annotator. Recognizing the sensitivity of Pearson’s correlation to outliers (Mathur et al., 2020), we manually examined the distribution of scores for all language– system combinations for outliers (and present all scatterplots in Figure 2 of the Appendix). The general pattern is consistent across languages: BERTScore performs better than other metrics in terms of both focus and coverage. This finding is consistent with that of Fabbri et al. (2020) wrt expert annotations of relevance (interpreted as the harmonic mean of our focus and coverage). ROUGE-1 and ROUGE-L are overall the best versions of ROUGE, while BLEU-4 performs the worst. For coverage, METEOR tends to be competitive with ROU"
2021.findings-acl.71,N04-1019,0,0.0578396,"ation evaluation methods developed for English are routinely applied to other languages, this is the first attempt to systematically quantify their panlinguistic efficacy. We take a summarization corpus for eight different languages, and manually annotate generated summaries for focus (precision) and coverage (recall). Based on this, we evaluate 19 summarization evaluation metrics, and find that using multilingual BERT within BERTScore performs well across all languages, at a level above that for English. 1 Figure 1: Illustration of focus and coverage. Introduction Although manual evaluation (Nenkova and Passonneau, 2004; Hardy et al., 2019) of text summarization is more reliable and interpretable, most research on text summarization employs automatic evaluations such as ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020b) because they are time- and cost-efficient. In proposing these metrics, the authors measured correlation with human judgments based on English datasets that are not representative of modern summarization systems. For instance, Lin (2004) use DUC1 2001–2003 for ROUGE (meaning summaries were generated with largely outdated ext"
2021.findings-acl.71,P02-1040,0,0.119558,"in, 2004) measures the lexical overlap between the system and reference summary; based on the findings of Graham (2015), we consider 7 variants in this paper: ROUGE1 (unigram), ROUGE-2 (bigram), ROUGE-3 (trigram), ROUGE-L (LCS), ROUGE-S (skipbigram), ROUGE-SU (skip-bigram plus unigram), and ROUGE-W (weighted LCS).4 METEOR (Lavie and Agarwal, 2007) performs word-to-word matching based on word-alignment, and was originally developed for MT but has recently been used for summarization evaluation (See et al., 2017; Chen and Bansal, 2018; Falke and Gurevych, 2019; Amplayo and Lapata, 2020).5 BLEU (Papineni et al., 2002) is a precisionbased metric originally developed for MT, which measures the n-gram match between the reference and system summary. Based on the findings of Graham (2015), we use BLEU-4 according to the SacreBLEU implementation (Post, 2018).6 MoverScore (Zhao et al., 2019) measures the Euclidean distance between two contextualized BERT representations, and relies on soft alignDUC 2001, 2002, 2003 802 4 https://github.com/bheinzerling/pyrouge http://www.cs.cmu.edu/∼alavie/METEOR/ 6 https://github.com/mjpost/sacrebleu 5 ments of words learned by solving an optimisation problem.7 We adapt use the"
2021.findings-acl.71,P19-1502,0,0.0173613,"h as ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020b) because they are time- and cost-efficient. In proposing these metrics, the authors measured correlation with human judgments based on English datasets that are not representative of modern summarization systems. For instance, Lin (2004) use DUC1 2001–2003 for ROUGE (meaning summaries were generated with largely outdated extractive summarization systems); Zhao et al. (2019) use the TAC2 dataset for MoverScore (again, featuring summaries from largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et al. (2020b) developed BERTScore based on a machine translation corpus (WMT). In contemporaneous work, Bhandari et al. (2020) address this issue by annotating English CNN/DailyMail summaries produced by recent summarization models, and found disparities over results from TAC. 1 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/ Equally troublingly, ROUGE has become the default summarization evaluation metric for languages other than English (Hu et al., 2015; Scialom et al., 2020; Ladhak et al., 2020; Koto et al., 2020b), despite there being no s"
2021.findings-acl.71,W18-6319,0,0.0218519,"ROUGE-SU (skip-bigram plus unigram), and ROUGE-W (weighted LCS).4 METEOR (Lavie and Agarwal, 2007) performs word-to-word matching based on word-alignment, and was originally developed for MT but has recently been used for summarization evaluation (See et al., 2017; Chen and Bansal, 2018; Falke and Gurevych, 2019; Amplayo and Lapata, 2020).5 BLEU (Papineni et al., 2002) is a precisionbased metric originally developed for MT, which measures the n-gram match between the reference and system summary. Based on the findings of Graham (2015), we use BLEU-4 according to the SacreBLEU implementation (Post, 2018).6 MoverScore (Zhao et al., 2019) measures the Euclidean distance between two contextualized BERT representations, and relies on soft alignDUC 2001, 2002, 2003 802 4 https://github.com/bheinzerling/pyrouge http://www.cs.cmu.edu/∼alavie/METEOR/ 6 https://github.com/mjpost/sacrebleu 5 ments of words learned by solving an optimisation problem.7 We adapt use the default configuration (n-gram=1) over 5 different pre-trained models, as detailed below. Note that MoverScore is symmetric (i.e. MoverScore(x, y) = MoverScore(y, x)), and as such is not designed to separately evaluate precision and recall."
2021.mrl-1.2,E17-1088,0,0.0199535,"s the ISO-639-3 language code for Na. 6 8 They are tuned on a small subset of de-en or fr-en data. Note that Griko is not included in ISO-639-3, and “grk” is an arbitrary (non-assigned) designator used in this paper. 9 18 Na raw cln njɤ˧ |ɑ˩ʁo˧ |ə˧si˧-ɳɯ˧ ʐwɤ˩qʰv˩mv˩-hĩ˧ lɑ˩ ɲi˩ mæ˩! njɤ˧ ɑ˩ʁo˧ ə˧si˧-ɳɯ˧ ʐwɤ˩qʰv˩mv˩-hĩ˧ lɑ˩ ɲi˩ mæ˩ English raw cln It (i.e. this story) is only what (we’ve) heard our great-grandmother tell. it is only what heard our great grandmother tell Table 2: An example Na–English parallel sentence before and after pre-processing (“raw” vs. “cln”) processing code used in Adams et al. (2017) with minor modifications.10 3.1.2 common problems for endangered languages. 3.2 Baselines Shipibo-Konibo We compare our model against various crosslingual models that are trained on a parallel corpus. First, we compare our model against a recently-proposed word-alignment model based on mBERT (Dou and Neubig, 2021).12 It fine-tunes mBERT on parallel corpora using various crosslingual objectives, and achieves state-of-the-art performance on word alignment tasks across many language pairs. We also include Levy et al. (2017), Luong et al. (2015a), and Sabet et al. (2020) as recent word embedding"
2021.mrl-1.2,2020.lrec-1.356,0,0.0222913,"word (Anastasopoulos et al., 2018). Unlike the Na and Shipibo-Konibo data sets, Griko and Italian are very similar in many ways: they both use the Latin script and have similar syntax. Therefore, the main challenge comes from the data paucity and inconsistent orthography in Griko, both of which are 10 We use white space as the word delimiter and keep all tones in Na sentences, as removing tones increases polysemy in the bilingual dictionary we use for evaluation. For Chinese, we perform word segmentation using the Stanford Word Segmenter (Chang et al., 2008) after data cleaning. 11 Recently, Bustamante et al. (2020) attempted to scrape monolingual data from PDF documents, but the size of the resulting data is still too small (22k sentences) to apply the latest pretraining methods. 12 We used the bert-base-multilingual-cased model, following the original paper. 13 We use Fast Align to generate the alignment. 14 Besides, Wada et al. (2019) show that the mapping models perform very poorly on low-resource conditions. Based on these findings, we did not include them as our baselines. 15 Except for the mBERT baseline, which has its pre-defined vocabulary and word embedding dimension, i.e. 768. 19 (Brown et al."
2021.mrl-1.2,N19-1391,0,0.0192352,"l too small (22k sentences) to apply the latest pretraining methods. 12 We used the bert-base-multilingual-cased model, following the original paper. 13 We use Fast Align to generate the alignment. 14 Besides, Wada et al. (2019) show that the mapping models perform very poorly on low-resource conditions. Based on these findings, we did not include them as our baselines. 15 Except for the mBERT baseline, which has its pre-defined vocabulary and word embedding dimension, i.e. 768. 19 (Brown et al., 1993), and still serve as de facto standard models to generate word alignments (Cao et al., 2020; Aldarmaki and Diab, 2019). For all the baselines, we use the authors’ implementations.16 3.3 Experimental Settings and Evaluation In our experiments, we train cross-lingual embeddings for five low-resource language pairs: Griko– Italian, Shipibo-Konibo–Spanish and Na–{French, Chinese, English}. For the Griko–Italian pair, we evaluate models on a cross-lingual word alignment task and report alignment accuracy (1−AER). We use the gold alignments manually annotated over the 330 Griko–Italian sentences. To produce alignments using Giza++ and Fast Align, we train them on the 330 sentences with or without additional 10k sen"
2021.mrl-1.2,W08-0336,0,0.148786,"Missing"
2021.mrl-1.2,C18-1214,0,0.0233141,"monolingual corpus for the language,11 but for cross-lingual resources there are two parallel corpora aligned with Spanish, which are extracted from the Bible and educational books (Galarreta et al., 2017). Similar to Na, Shipibo-Konibo is an SOV language with very rich morphology (Valenzuela, 1997; Vasquez et al., 2018), whereas Spanish is an SVO language. 3.1.3 Griko Griko is a Greek dialect spoken in southern Italy, and “severely endangered” according to UNESCO. There is no large-scale monolingual corpus of Griko, but there are two Griko–Italian parallel corpora (Zanon Boito et al., 2018; Anastasopoulos et al., 2018), with the smaller one including gold word alignment annotations. However, Griko has never had a consistent orthography, and hence its tokenisation and word segmentation differ across these corpora: the smaller data set is based on orthographic conventions from Italian, while the larger one follows the concept of a phonological word (Anastasopoulos et al., 2018). Unlike the Na and Shipibo-Konibo data sets, Griko and Italian are very similar in many ways: they both use the Latin script and have similar syntax. Therefore, the main challenge comes from the data paucity and inconsistent orthograph"
2021.mrl-1.2,2020.emnlp-main.42,0,0.0366598,"Missing"
2021.mrl-1.2,P18-1073,0,0.0194224,"presentations even in extremely lowresource conditions. Furthermore, our model also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.1 1 Introduction Cross-lingual word embedding learning has the goal of learning representations for words of different languages in a common space (Mikolov et al., 2013b; Conneau et al., 2018; Levy et al., 2017). Cross-lingual representations are beneficial for finding correspondences between languages, and are utilised in many downstream tasks such as machine translation (Lample et al., 2018; Artetxe et al., 2018b) and cross-lingual named entity recognition (Xie et al., 2018). ∗ This work was partially done at Nara Institute of Science and Technology. 1 Our code is available at https://github.com/ twadada/multilingual-nlm 16 Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 16–31 November 11, 2021. ©2021 Association for Computational Linguistics 2 Methodology 2.1 ??? (e.g.) ! ?? (????) = ??? (????) + ?(?@??? , ??) ? ?! ℓ Model Architecture (1) s us1 ..., usN = f (r1s ..., rN ), (2) + ??? ??? ?! ! ??? M +1 ∏ (3) − → p(yit |h i , us ), i=1 − → − → → t hi = − g t ( h i−1 , ri"
2021.mrl-1.2,J82-2005,0,0.663283,"Missing"
2021.mrl-1.2,Q19-1038,0,0.0362526,"Missing"
2021.mrl-1.2,Q17-1010,0,0.0443519,"posed model is based an LSTM2 encoderdecoder model with attention (Luong et al., 2015b), trained with translation and reconstruction objectives (Figure 1). Suppose our model encodes a sentence ⟨xs1 ..., xsN ⟩ in the source language s and det ⟩ in the target language codes a sentence ⟨y1t ..., yM t. The encoder employs bi-directional LSTMs f , which are shared among all languages: ris = E s xsi , = 2.2 Shared Subword Embeddings (4) To incorporate orthographic information into word embeddings, we propose a simple yet effective method to combine word and subword embeddings, inspired by FastText (Bojanowski et al., 2017). For each word wiℓ , we calculate its subword˜wℓ as follows: aware word embedding E i (5) (6) (7) ˜ ℓ = E ℓ + F(Zk∈Q(w ) ), E wi wi i 2 We use LSTM rather than Transformer (Vaswani et al., 2017) because LSTM is less sensitive to hyper-parameters and performs better at translation under extremely low-resource conditions (Zhang et al., 2020). 3 In our preliminary experiments, we have found that learning language-specific decoders improves cross-lingual embeddings for distant languages, e.g. SOV and SVO languages. (8) 4 This simplifies the lexical model proposed by Nguyen and Chiang (2018). Whil"
2021.mrl-1.2,W03-0301,0,0.414717,"Missing"
2021.mrl-1.2,P19-1314,0,0.0598251,"Missing"
2021.mrl-1.2,W15-1521,0,0.227358,"rd embeddings of xsi . Given the en→ − coder states us , the decoders − g t and ← g t translate (when s = t) or reconstruct (when s = t) the input sentence left-to-right and right-to-left. We train separate decoders for each language and direction to allow for the differences of word order.3 Similar to ELMo (Peters et al., 2018), the decoding is performed independently in both directions: t p(y1t ..., yM , EOS) = ? ' ?! ( + ?(?) Figure 1: Our proposed model. where xsi rit = E t yit , ?ℓ + ??, ? ) Attention ((???, ? , ? Our proposed model is based an LSTM2 encoderdecoder model with attention (Luong et al., 2015b), trained with translation and reconstruction objectives (Figure 1). Suppose our model encodes a sentence ⟨xs1 ..., xsN ⟩ in the source language s and det ⟩ in the target language codes a sentence ⟨y1t ..., yM t. The encoder employs bi-directional LSTMs f , which are shared among all languages: ris = E s xsi , = 2.2 Shared Subword Embeddings (4) To incorporate orthographic information into word embeddings, we propose a simple yet effective method to combine word and subword embeddings, inspired by FastText (Bojanowski et al., 2017). For each word wiℓ , we calculate its subword˜wℓ as follows:"
2021.mrl-1.2,D15-1166,0,0.0790593,"rd embeddings of xsi . Given the en→ − coder states us , the decoders − g t and ← g t translate (when s = t) or reconstruct (when s = t) the input sentence left-to-right and right-to-left. We train separate decoders for each language and direction to allow for the differences of word order.3 Similar to ELMo (Peters et al., 2018), the decoding is performed independently in both directions: t p(y1t ..., yM , EOS) = ? ' ?! ( + ?(?) Figure 1: Our proposed model. where xsi rit = E t yit , ?ℓ + ??, ? ) Attention ((???, ? , ? Our proposed model is based an LSTM2 encoderdecoder model with attention (Luong et al., 2015b), trained with translation and reconstruction objectives (Figure 1). Suppose our model encodes a sentence ⟨xs1 ..., xsN ⟩ in the source language s and det ⟩ in the target language codes a sentence ⟨y1t ..., yM t. The encoder employs bi-directional LSTMs f , which are shared among all languages: ris = E s xsi , = 2.2 Shared Subword Embeddings (4) To incorporate orthographic information into word embeddings, we propose a simple yet effective method to combine word and subword embeddings, inspired by FastText (Bojanowski et al., 2017). For each word wiℓ , we calculate its subword˜wℓ as follows:"
2021.mrl-1.2,N18-1031,0,0.0197724,"Text (Bojanowski et al., 2017). For each word wiℓ , we calculate its subword˜wℓ as follows: aware word embedding E i (5) (6) (7) ˜ ℓ = E ℓ + F(Zk∈Q(w ) ), E wi wi i 2 We use LSTM rather than Transformer (Vaswani et al., 2017) because LSTM is less sensitive to hyper-parameters and performs better at translation under extremely low-resource conditions (Zhang et al., 2020). 3 In our preliminary experiments, we have found that learning language-specific decoders improves cross-lingual embeddings for distant languages, e.g. SOV and SVO languages. (8) 4 This simplifies the lexical model proposed by Nguyen and Chiang (2018). While they apply separate output layers to the weighted average of word embeddings and hidden states, we have found that sharing the same output layer (E t ) performed the best, suggesting that the optimal model architecture is different between word and sentence translations. 17 where F(·) denotes the subword encoding function; Zk denotes the k-th subword embedding and Q(wi ) denotes the indices of the subwords included in wi . The subword embeddings Z are shared among all languages, capturing orthographic similarities across languages. For the encoding function F(·), we experiment with two"
2021.mrl-1.2,L18-1697,0,0.0167371,"we train them on the same data, and align each word in a sentence to the closest word in its translation using static or contextualised word embeddings.18 To calculate word similarity, we use cross-domain similarity local scaling (Conneau et al., 2018): 1 ∑ CSLS(x, y) = 2 cos(x, y) − cos(x, yt ) K yt ∈NT (x) − 1 K ∑ each source word in a bilingual dictionary, we extract the k nearest words from the whole target vocabulary and see whether they are listed as translations in the dictionary. We set k to 1 or 5, and report P@1 and P@5. For evaluation, we use a Shipibo-Konibo–Spanish dictionary20 (Maguiño-Valencia et al., 2018) and Na–French– Chinese–English dictionaries (Michaud, 2018). Based on extracting words that are present in the parallel corpora, we identified 79, 262, 215 and 87 word pairs for Shipibo-Konibo–Spanish, Na–French, Na–Chinese, and Na–English.21 To perform BLI with GIZA++ and Fast Align, we use their source-to-target probability table. We also try using the result of bidirectional word alignments, aligning each word to the most frequently aligned words to it.22 For the neural baselines and our model, we use static word embeddings and employ CSLS to measure the word embedding similarities. To obt"
2021.mrl-1.2,J03-1002,0,0.0434436,"d BIS2V trains a Continuous Bag-of-Words (CBOW) model that predicts a target word from the rest of the sentence and its parallel sentence. Sabet et al. (2020) and Marie and Fujita (2019) show that these joint learning models perform better than mapping-based methods, which align monolingual word embeddings cross-lingually.14 Regarding the vocabulary size and word embedding dimension, we always use the same values for all the baselines and our model, to ensure fairness.15 In addition to these neural baselines, we also compare our model against statistical word alignment methods, namely GIZA++ (Och and Ney, 2003) and Fast Align (Dyer et al., 2013). These are pre-neural methods based on the IBM models Shipibo-Konibo is an indigenous language spoken by around 35,000 native speakers in the Amazon region of Peru (Vasquez et al., 2018), and is “definitely endangered” according to the UNESCO’s Atlas of the World’s Languages in Danger (Moseley, 2010). There is no large monolingual corpus for the language,11 but for cross-lingual resources there are two parallel corpora aligned with Spanish, which are extracted from the Bible and educational books (Galarreta et al., 2017). Similar to Na, Shipibo-Konibo is an"
2021.mrl-1.2,P19-1312,0,0.0201844,"Sabet et al. (2020) as recent word embedding baselines, which we denote as SENTID, BIVEC and BIS2V, respectively. All of these baselines are very similar in terms of methodology: SENTID trains a Skip-Gram model that predicts a sentence ID (which is assigned to each set of parallel sentences) from the component words; BIVEC trains a Skip-Gram model that predicts the context cross-lingually based on the wordalignment information;13 and BIS2V trains a Continuous Bag-of-Words (CBOW) model that predicts a target word from the rest of the sentence and its parallel sentence. Sabet et al. (2020) and Marie and Fujita (2019) show that these joint learning models perform better than mapping-based methods, which align monolingual word embeddings cross-lingually.14 Regarding the vocabulary size and word embedding dimension, we always use the same values for all the baselines and our model, to ensure fairness.15 In addition to these neural baselines, we also compare our model against statistical word alignment methods, namely GIZA++ (Och and Ney, 2003) and Fast Align (Dyer et al., 2013). These are pre-neural methods based on the IBM models Shipibo-Konibo is an indigenous language spoken by around 35,000 native speake"
2021.mrl-1.2,2020.acl-main.156,0,0.0813123,"Missing"
2021.mrl-1.2,W03-0320,0,0.167078,"Missing"
2021.mrl-1.2,N18-1202,0,0.055646,"r¯is + hi in Eqn. (5) before the linear transformation, with the dropout rate all set to 0.5. We show that this strong regularisation leads to better cross-lingual representations. denotes a one-hot vector. In cross-lingual tasks, we employ ris and usi as the static and contextualised word embeddings of xsi . Given the en→ − coder states us , the decoders − g t and ← g t translate (when s = t) or reconstruct (when s = t) the input sentence left-to-right and right-to-left. We train separate decoders for each language and direction to allow for the differences of word order.3 Similar to ELMo (Peters et al., 2018), the decoding is performed independently in both directions: t p(y1t ..., yM , EOS) = ? ' ?! ( + ?(?) Figure 1: Our proposed model. where xsi rit = E t yit , ?ℓ + ??, ? ) Attention ((???, ? , ? Our proposed model is based an LSTM2 encoderdecoder model with attention (Luong et al., 2015b), trained with translation and reconstruction objectives (Figure 1). Suppose our model encodes a sentence ⟨xs1 ..., xsN ⟩ in the source language s and det ⟩ in the target language codes a sentence ⟨y1t ..., yM t. The encoder employs bi-directional LSTMs f , which are shared among all languages: ris = E s xsi ,"
2021.mrl-1.2,E17-2025,0,0.0608051,"Missing"
2021.mrl-1.2,P19-1300,1,0.812151,"delimiter and keep all tones in Na sentences, as removing tones increases polysemy in the bilingual dictionary we use for evaluation. For Chinese, we perform word segmentation using the Stanford Word Segmenter (Chang et al., 2008) after data cleaning. 11 Recently, Bustamante et al. (2020) attempted to scrape monolingual data from PDF documents, but the size of the resulting data is still too small (22k sentences) to apply the latest pretraining methods. 12 We used the bert-base-multilingual-cased model, following the original paper. 13 We use Fast Align to generate the alignment. 14 Besides, Wada et al. (2019) show that the mapping models perform very poorly on low-resource conditions. Based on these findings, we did not include them as our baselines. 15 Except for the mBERT baseline, which has its pre-defined vocabulary and word embedding dimension, i.e. 768. 19 (Brown et al., 1993), and still serve as de facto standard models to generate word alignments (Cao et al., 2020; Aldarmaki and Diab, 2019). For all the baselines, we use the authors’ implementations.16 3.3 Experimental Settings and Evaluation In our experiments, we train cross-lingual embeddings for five low-resource language pairs: Griko–"
2021.mrl-1.2,2020.sltu-1.13,0,0.0499318,"Missing"
2021.mrl-1.2,D18-1034,0,0.0217819,", our model also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.1 1 Introduction Cross-lingual word embedding learning has the goal of learning representations for words of different languages in a common space (Mikolov et al., 2013b; Conneau et al., 2018; Levy et al., 2017). Cross-lingual representations are beneficial for finding correspondences between languages, and are utilised in many downstream tasks such as machine translation (Lample et al., 2018; Artetxe et al., 2018b) and cross-lingual named entity recognition (Xie et al., 2018). ∗ This work was partially done at Nara Institute of Science and Technology. 1 Our code is available at https://github.com/ twadada/multilingual-nlm 16 Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 16–31 November 11, 2021. ©2021 Association for Computational Linguistics 2 Methodology 2.1 ??? (e.g.) ! ?? (????) = ??? (????) + ?(?@??? , ??) ? ?! ℓ Model Architecture (1) s us1 ..., usN = f (r1s ..., rN ), (2) + ??? ??? ?! ! ??? M +1 ∏ (3) − → p(yit |h i , us ), i=1 − → − → → t hi = − g t ( h i−1 , ri−1 ), t p(BOS, y1t ..., yM )= M ∏ ← − t s p(yM −i |h M −i , u ),"
2021.mrl-1.2,P18-2037,0,0.0387565,"Missing"
2021.mrl-1.2,N15-1104,0,0.0870183,"Missing"
2021.mrl-1.2,W17-2619,0,0.0616142,"Missing"
2021.mrl-1.2,J96-1001,0,0.692304,"and report the best score; we fine-tune the mBERT baseline for 40,000 steps26 with 20 checkpoints, and train SENTID and BIS2V for 1,000 epochs with 100 checkpoints to ensure convergence. For BIVEC, we increase the training corpus size by 20 times by duplicating the sentences and train the model for 50 epochs with 50 checkpoints.27 For our model, on the other hand, we use a simple early-stopping criterion that doesn’t require external data. First, we build a pseudo bilingual dictionary from the training data. To retrieve pseudo bilingual word pairs, we compute the Dice Coeﬀicient (Dice, 1945; Smadja et al., 1996) and extract pairs of words that appear ≥ 3 times in each language and whose Dice Coeﬀicient is ≥ 0.8 across two languages. We perform model selection based on the BLI performance on this pseudo dictionary. 3.5 Word src–tgt nru–en nru–fr nru–zh +SWave +SWcnn bi multi bi mult bi multi 30.2 27.3 31.6 34.2 28.7 36.9 32.0 29.9 36.7 37.8 29.8 40.1 35.6 32.1 38.5 40.5 30.7 40.8 Table 4: Our model performance (P@1) on BLI when the model is trained on two and four languages (“bi” vs. “multi”). All scores are averaged over three runs. our model. Compared to the neural baselines, our model performs bett"
2021.mrl-1.2,2020.acl-main.146,0,0.0389663,"Missing"
2021.mrl-1.2,P17-1179,0,0.0515976,"Missing"
2021.mrl-1.2,2020.emnlp-main.43,0,0.0841257,"Missing"
2021.naacl-main.175,2020.acl-main.194,0,0.11453,"ress and develture of labelled and unlabelled data. More recently, opment are more important than action on pretrained models and semi-supervised learning climate change, and hence policies like renewhave been combined with great success, e.g. Xie ables or carbon taxes are not worth it. et al. (2020) used BERT along with consistency reg• Justification by Comparison (Justify ularization on unlabeled data, Croce et al. (2020) POLICY): our actions are not as important extended the fine-tuning process of BERT to a genas other countries which pollute more, or there erative adversarial setting, and Chen et al. (2020) are other more important issues than global 2168 warming. Examples of these 7 neutralization techniques are given in Table 2. CCS texts often use multiple NT together in their narrative (hence motivating a multilabel classification task), as seen in the second example in Table 1 where Condemn (POLICY) is used to blame the alarmist greens and Deny-Responsibility (SCIENCE) is used to highlight that global warming is a natural cycle. Similarly, in third example as well we see Condemn (POLICY) is used to accuse the IPCC (Intergovernmental Panel on Climate Change,3 in conjunction with Deny-Respons"
2021.naacl-main.175,D18-1217,0,0.056925,"Missing"
2021.naacl-main.175,P19-1590,0,0.0170891,"sed NLP modSCIENCE): there is no evidence of els is the strong dependency on labelled data. To climate change and no climate change tackle this, one approach is apply transfer learning victims; total denial of any global warming. from pretrained language models (Radford et al., • Condemnation of the Condemner (Condemn POLICY): climate change is 2019; Peters et al., 2018; Yang et al., 2019; Conneau and Lample, 2019; Devlin et al., 2019). Anmisrepresented by scientists or manipulated other approach is semi-supervised learning. Yang by politicians, the media, environmentalists, et al. (2017) and Gururangan et al. (2019) emetc. ployed variational autoencoders, and Clark et al. • Appeal to Higher Loyalties (Loyalties (2018) leveraged cross-view training using a mixPOLICY): economic progress and develture of labelled and unlabelled data. More recently, opment are more important than action on pretrained models and semi-supervised learning climate change, and hence policies like renewhave been combined with great success, e.g. Xie ables or carbon taxes are not worth it. et al. (2020) used BERT along with consistency reg• Justification by Comparison (Justify ularization on unlabeled data, Croce et al. (2020) POLI"
2021.naacl-main.175,2020.acl-main.740,0,0.0462508,"Missing"
2021.naacl-main.175,2020.findings-emnlp.296,0,0.294732,"operated at the article level, relate to the SCIENCE frame, and the last three to and focused on binary detection (presence vs. ab- the POLICY frame, as indicated): sence) (Barr´on-Cedeno et al., 2019; Rashkin et al., • Denial of Responsibility 2017). Da San Martino et al. (2019) argued for (Deny-Responsibility SCIENCE): the need for finer granularity in propaganda detecclimate change is happening, but is a natural tion, both in terms of propaganda sub-types and cycle and human are not responsible. fragment-level detection. In a similar vein, Naka• Denial of Injury1 (Deny-Injury1 mura et al. (2020) proposed fine-grained classes SCIENCE): there are no significant harms of fake news to differentiate between misleading, attributable to climate change, and claims are manipulated, or totally false content. More recently generally overstated. in the climate change domain, Luo et al. (2020) re• Denial of Injury2 (Deny-Injury2 leased a stance-annotated dataset for global warmSCIENCE): there are benefits in rising ing, and proposed an opinion framing task to study rising C02 levels which have a positive effect discourse used in the debate around global warmon the environment. ing. • Denial of Vi"
2021.naacl-main.175,D17-1317,0,0.0529276,"Missing"
2021.naacl-main.175,2020.lrec-1.755,0,0.0845038,"Missing"
2021.naacl-main.175,N18-1202,0,0.0403638,"s in rising ing, and proposed an opinion framing task to study rising C02 levels which have a positive effect discourse used in the debate around global warmon the environment. ing. • Denial of Victim (Deny-Victim One challenge in building supervised NLP modSCIENCE): there is no evidence of els is the strong dependency on labelled data. To climate change and no climate change tackle this, one approach is apply transfer learning victims; total denial of any global warming. from pretrained language models (Radford et al., • Condemnation of the Condemner (Condemn POLICY): climate change is 2019; Peters et al., 2018; Yang et al., 2019; Conneau and Lample, 2019; Devlin et al., 2019). Anmisrepresented by scientists or manipulated other approach is semi-supervised learning. Yang by politicians, the media, environmentalists, et al. (2017) and Gururangan et al. (2019) emetc. ployed variational autoencoders, and Clark et al. • Appeal to Higher Loyalties (Loyalties (2018) leveraged cross-view training using a mixPOLICY): economic progress and develture of labelled and unlabelled data. More recently, opment are more important than action on pretrained models and semi-supervised learning climate change, and hence"
2021.naacl-main.301,D17-1168,0,0.023274,"rmalized Scores 0.8 Accuracy Discourse Connective 0.6 0.4 Spearman Rank Accuracy 1.0 10 12 2 4 6 8 Layer 10 12 Figure 2: Probing task performance on English for each of the seven tasks, plus the average across all tasks. For BART and T5, layers 7–12 are the decoder layers. All results are averaged over three runs, and the vertical line for each data point denotes the standard deviation (noting that most results have low s.d., meaning the bar is often not visible). or, or although (Nie et al., 2019), representing the conceptual relation between the sentences/clauses. monsense and storytelling (Chaturvedi et al., 2017; Liu et al., 2018). 4 4. RST nuclearity prediction. For a given ordered pairing of (potentially complex) EDUs which are connected by an unspecified relation, predict the nucleus/satellite status of each (see Figure 1). 5. RST relation prediction. For a given ordered pairing of (potentially complex) EDUs which are connected by an unspecified relation, predict the relation that holds between them (see Figure 1). 6. RST elementary discourse unit (EDU) segmentation. Chunk a concatenated sequence of EDUs into its component EDUs. Experimental Setup We summarize all data (sources, number of labels,"
2021.naacl-main.301,2020.coling-main.66,1,0.859942,"next sentence. The preceding context takes the form of between 2 and 8 sentences, but the candidates are always single sentences. We outline the 7 pretrained models in Table 1. They comprise 4 encoder-only models: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), and ELECTRA (Clark 2. Sentence ordering. We shuffle 3–7 sentences et al., 2020); 1 decoder-only model: GPT-2 (Radand attempt to reproduce the original order. ford et al., 2019); and 2 encoder–decoder modThis task is based on Barzilay and Lapata els: BART (Lewis et al., 2020) and T5 (Raffel (2008) and Koto et al. (2020), and is assessed et al., 2019). To reduce the confound of model size, based on rank correlation relative to the origiwe use pretrained models of similar size (∼110m nal order. model parameters), with the exception of ALBERT which is designed to be lighter weight. All mod3. Discourse connective prediction. Given two els have 12 transformer layers in total; for BART sentences/clauses, the task is to identify an and T5, this means their encoder and decoder have appropriate discourse marker, such as while, 6 layers each. Further details of the models are 2 provided in the Supplementary Material."
2021.naacl-main.301,2020.acl-main.703,0,0.24701,"s of English verbs. Zhu et al. (2020) applied the model of Feng and Hirst (2014) to parse IMDB documents (Maas et al., 2011) into discourse trees. Using this (potentially noisy) data, probing tasks were conducted by mapping attention layers into single vectors of document-level rhetorical features. These features, however, are unlikely to capture all the intricacies of inter-sentential abstraction as their input is formed based on discourse relations1 and aggregate statistics on the distribution of discourse units. The remarkable development of pretrained language models (Devlin et al., 2019; Lewis et al., 2020; Lan et al., 2020) has raised questions about what precise aspects of language these models do and do not capture. Probing tasks offer a means to perform fine-grained analysis of the capabilities of such models, but most existing work has focused on sentence-level analysis such as syntax (Hewitt and Manning, 2019; Jawahar et al., 2019; de Vries et al., 2020), entities/relations (Papanikolaou et al., 2019), and ontological knowledge (Michael et al., 2020). Less is known about how well such models capture broader discourse in documents. Rhetorical Structure Theory is a framework for capturing h"
2021.naacl-main.301,D14-1224,0,0.074932,"Missing"
2021.naacl-main.301,P18-2045,1,0.8568,"acy Discourse Connective 0.6 0.4 Spearman Rank Accuracy 1.0 10 12 2 4 6 8 Layer 10 12 Figure 2: Probing task performance on English for each of the seven tasks, plus the average across all tasks. For BART and T5, layers 7–12 are the decoder layers. All results are averaged over three runs, and the vertical line for each data point denotes the standard deviation (noting that most results have low s.d., meaning the bar is often not visible). or, or although (Nie et al., 2019), representing the conceptual relation between the sentences/clauses. monsense and storytelling (Chaturvedi et al., 2017; Liu et al., 2018). 4 4. RST nuclearity prediction. For a given ordered pairing of (potentially complex) EDUs which are connected by an unspecified relation, predict the nucleus/satellite status of each (see Figure 1). 5. RST relation prediction. For a given ordered pairing of (potentially complex) EDUs which are connected by an unspecified relation, predict the relation that holds between them (see Figure 1). 6. RST elementary discourse unit (EDU) segmentation. Chunk a concatenated sequence of EDUs into its component EDUs. Experimental Setup We summarize all data (sources, number of labels, and data split) in"
2021.naacl-main.301,2021.ccl-1.108,0,0.0758504,"Missing"
2021.naacl-main.301,P11-1015,0,0.0839363,"s with different pretraining objectives, for different languages, and different model sizes. Our research question in this paper is: How much discourse structure do layers of different pretrained language models capture, and do the findings generalize across languages? There are two contemporaneous related studies that have examined discourse modelling in pretrained language models. Upadhye et al. (2020) analyzed how well two pretrained models capture referential biases of different classes of English verbs. Zhu et al. (2020) applied the model of Feng and Hirst (2014) to parse IMDB documents (Maas et al., 2011) into discourse trees. Using this (potentially noisy) data, probing tasks were conducted by mapping attention layers into single vectors of document-level rhetorical features. These features, however, are unlikely to capture all the intricacies of inter-sentential abstraction as their input is formed based on discourse relations1 and aggregate statistics on the distribution of discourse units. The remarkable development of pretrained language models (Devlin et al., 2019; Lewis et al., 2020; Lan et al., 2020) has raised questions about what precise aspects of language these models do and do not"
2021.naacl-main.301,2020.emnlp-main.552,0,0.0509812,"Missing"
2021.naacl-main.301,N16-1098,0,0.199386,", 2020) N/A #Labels: 15 Split: 900/148/159 (4) RST Nuclearity (5) RST Relation RST-DT (Carlson et al., 2001) #Labels (nuc/rel): 3/18 Split: 16903/1943/2308 CDTB (Li et al., 2014) #Labels (nuc/rel): 3/4 Split: 6159/353/809 Potsdam Commentary (Bourgonje and Stede, 2020) #Labels (nuc/rel): 3/31 Split: 1892/289/355 (6) RST EDU Segmentation RST-DT (Carlson et al., 2001) Split: 312/35/38 docs Potsdam Commentary RST-Spanish Treebank CDTB (Li et al., 2014) (Bourgonje and Stede, 2020) (da Cunha et al., 2011) Split: 2135/105/241 p’graphs Split: 131/20/25 docs Split: 200/34/30 docs (7) Cloze Story Test (Mostafazadeh et al., 2016) Split: 1683/188/1871 N/A N/A RST-Spanish Treebank (da Cunha et al., 2011) #Labels (nuc/rel): 3/29 Split: 2042/307/421 N/A Table 2: A summary of probing tasks and datasets for each of the four languages. “Split” indicates the number of train/development/test instances. Nuclearity and Relation prediction: cause (N) (S) elab (S) EDU1 EDU3 (N) EDU2 ⇨ ⇨ text1 |text2 nuclearity, relation EDU1 |EDU2 SN, elab EDU1 EDU2 |EDU3 NS, cause ⇨ EDU segmentation: EDU1 EDU2 EDU3 EDU1 |EDU2 |EDU3 ⇨ Figure 1: Illustration of the RST discourse probing tasks (Tasks 4–6). To summarize, we introduce 7 discourse-rela"
2021.naacl-main.301,D19-6108,0,0.0176823,"as their input is formed based on discourse relations1 and aggregate statistics on the distribution of discourse units. The remarkable development of pretrained language models (Devlin et al., 2019; Lewis et al., 2020; Lan et al., 2020) has raised questions about what precise aspects of language these models do and do not capture. Probing tasks offer a means to perform fine-grained analysis of the capabilities of such models, but most existing work has focused on sentence-level analysis such as syntax (Hewitt and Manning, 2019; Jawahar et al., 2019; de Vries et al., 2020), entities/relations (Papanikolaou et al., 2019), and ontological knowledge (Michael et al., 2020). Less is known about how well such models capture broader discourse in documents. Rhetorical Structure Theory is a framework for capturing how sentences are connected and describing the overall structure of a document (Mann and Thompson, 1986). A number of studies have used pretrained models to classify discourse markers (Sileo et al., 2019) and discourse relations (Nie et al., 2019; Shi and Demberg, 2019), but few (Koto et al., to appear) have systematically investigated the ability of pretrained models to model discourse 1 structure. Further"
2021.naacl-main.301,D19-1448,0,0.0621788,"Missing"
2021.naacl-main.301,2020.blackboxnlp-1.3,0,0.0261906,"model, leaving open the question of how well these findings generalize to other models with different pretraining objectives, for different languages, and different model sizes. Our research question in this paper is: How much discourse structure do layers of different pretrained language models capture, and do the findings generalize across languages? There are two contemporaneous related studies that have examined discourse modelling in pretrained language models. Upadhye et al. (2020) analyzed how well two pretrained models capture referential biases of different classes of English verbs. Zhu et al. (2020) applied the model of Feng and Hirst (2014) to parse IMDB documents (Maas et al., 2011) into discourse trees. Using this (potentially noisy) data, probing tasks were conducted by mapping attention layers into single vectors of document-level rhetorical features. These features, however, are unlikely to capture all the intricacies of inter-sentential abstraction as their input is formed based on discourse relations1 and aggregate statistics on the distribution of discourse units. The remarkable development of pretrained language models (Devlin et al., 2019; Lewis et al., 2020; Lan et al., 2020)"
2021.naacl-main.301,P18-2119,0,0.0125827,"cept sentence ordering and EDU segmentation as a classification problem, and evaluate using accuracy. During fine-tuning, we add an MLP layer on top of the pretrained model for classification, and only update the MLP parameters (all other layers are frozen). We use the [CLS] embedding for BERT and ALBERT following standard practice, while for other models we perform average pooling to obtain a vector for each sentence, and concatenate them as the input to the MLP.3 7. Cloze story test. Given a 4-sentence story context, pick the best ending from two possible options (Mostafazadeh et al., 2016; Sharma et al., 2018). This task is harder than NSP, as it requires an understanding of com3851 3 BERT and ALBERT performance with average pooling For sentence ordering, we follow Koto et al. (2020) and frame it as a sentence-level sequence labelling task, where the goal is to estimate P (r|s), where r is the rank position and s the sentence. The task has 7 classes, as we have 3–7 sentences (see Section 3). At test time, we choose the label sequence that maximizes the sequence probability. Sentence embeddings are obtained by average pooling. The EDU segmentation task is also framed as a binary sequence labelling t"
2021.naacl-main.301,D19-1586,0,0.0239393,"on sentence-level analysis such as syntax (Hewitt and Manning, 2019; Jawahar et al., 2019; de Vries et al., 2020), entities/relations (Papanikolaou et al., 2019), and ontological knowledge (Michael et al., 2020). Less is known about how well such models capture broader discourse in documents. Rhetorical Structure Theory is a framework for capturing how sentences are connected and describing the overall structure of a document (Mann and Thompson, 1986). A number of studies have used pretrained models to classify discourse markers (Sileo et al., 2019) and discourse relations (Nie et al., 2019; Shi and Demberg, 2019), but few (Koto et al., to appear) have systematically investigated the ability of pretrained models to model discourse 1 structure. Furthermore, existing work relating to For example, they only consider discourse relation labels discourse probing has typically focused exclusively and ignore nuclearity. 3849 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3849–3864 June 6–11, 2021. ©2021 Association for Computational Linguistics Probing Task English Chinese German Spanish (1) 4-way NSP (2) Sen"
2021.naacl-main.301,N19-1351,0,0.0631861,"Missing"
2021.naacl-main.301,2020.emnlp-main.70,0,0.0359673,"= sentence order prediction, “LM” = language model, “DISC” = discriminator, and “DAE” = denoising autoencoder. on the BERT-base model, leaving open the question of how well these findings generalize to other models with different pretraining objectives, for different languages, and different model sizes. Our research question in this paper is: How much discourse structure do layers of different pretrained language models capture, and do the findings generalize across languages? There are two contemporaneous related studies that have examined discourse modelling in pretrained language models. Upadhye et al. (2020) analyzed how well two pretrained models capture referential biases of different classes of English verbs. Zhu et al. (2020) applied the model of Feng and Hirst (2014) to parse IMDB documents (Maas et al., 2011) into discourse trees. Using this (potentially noisy) data, probing tasks were conducted by mapping attention layers into single vectors of document-level rhetorical features. These features, however, are unlikely to capture all the intricacies of inter-sentential abstraction as their input is formed based on discourse relations1 and aggregate statistics on the distribution of discourse"
2021.naacl-main.321,D18-1316,0,0.0768221,"light perturbation to the input data can fool these models into producing wrong predictions (Goodfellow et al., 2014; Kurakin et al., 2016). Research in this area is broadly categorised as adversarial machine learning, and it has two sub-fields: adversarial attack, which seeks to generate adversarial examples that fool target models; and adversarial defence, whose goal is to build models that are less susceptible to adversarial attacks. A number of adversarial attacking methods have been proposed for image recognition (Goodfellow et al., 2014), NLP (Zhang et al., 2020) and speech recognition (Alzantot et al., 2018a). These methods are generally categorised into three types: whitebox, black-box and grey-box attacks. White-box ∗ This work was completed during the employment of the authors in IBM Research Australia. attacks assume full access to the target models and often use the gradients from the target models to guide the craft of adversarial examples. Black-box attacks, on the other hand, assume no knowledge on the architecture of the target model and perform attacks by repetitively querying the target model. Different from the previous two, grey-box attacks train a generative model to generate adver"
2021.naacl-main.321,D19-1419,0,0.0434299,"Missing"
2021.naacl-main.321,D14-1181,0,0.00589953,"Missing"
2021.naacl-main.321,2020.tacl-1.20,1,0.849848,"t al. (2020): Attacking performance. We use the standard classification accuracy (ACC) of the target classifier (C) to measure the attacking performance of adversarial examples. Lower accuracy means better attacking performance. Similarity. To assess the textual and semantic similarity between the original and corresponding adversarial examples, we compute BLEU (Papineni et al., 2002) and USE (Cer et al., 2018).8 For both metrics, higher scores represent better performance. Fluency. To measure the readability of generated adversarial examples, we use the acceptability score (ACPT) proposed by Lau et al. (2020), which is based on normalised sentence probabilities produced by XLNet (Yang et al., 2019). Higher scores indicate better fluency. Transferability. To understand the effectiveness of the adversarial examples in attacking another unseen sentiment classifier (TRF), we evaluate the accuracy of C-BERT using adversarial examples that have been generated for attacking classifiers C-LSTM and C-CNN. Lower accuracy indicates better transferability. Attacking speed. We measure each attacking method on the amount of time it takes on average (in seconds) to generate an adversarial example. 4.3.2 Automati"
2021.naacl-main.321,2020.findings-emnlp.341,0,0.0180024,"training of adversarial attacking, we tune λ1 and λ2 , and learning rate lr. We also test different temperature τ for Gumbel-softmax sampling and found that τ = 0.1 performs the best. All word embeddings are fixed. More hyper-parameter and training configurations are detailed in the supplementary material. 4.3 Attacking Performance Most of the existing adversarial attacking methods have been focusing on improving the attack success rate. Recent study show that with constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points (Morris et al., 2020a). In this paper, we want to understand — given a particular success rate — the quality (e.g. fluency, content/label preservation) of the generated adversarial samples. Therefore, we tuned all attacking methods to achieve the same levels of attack success rates; and compare the quality of generated adversarial examples. 7 Note that results for adver6 Pre-trained BLEU scores are 97.7 and 96.8 on yelp50 using GloVe and counter-fitted embedding, respectively. 7 We can in theory tune different methods to achieve higher success rate, but we choose the strategy to use lower success rates so that al"
2021.naacl-main.321,P02-1040,0,0.110608,"architecture. 4.3.1 Evaluation Metrics In addition to measuring how well the adversarial examples fool the sentiment classifier, we also use a number of automatic metrics to assess other aspects of adversarial examples, following Xu et al. (2020): Attacking performance. We use the standard classification accuracy (ACC) of the target classifier (C) to measure the attacking performance of adversarial examples. Lower accuracy means better attacking performance. Similarity. To assess the textual and semantic similarity between the original and corresponding adversarial examples, we compute BLEU (Papineni et al., 2002) and USE (Cer et al., 2018).8 For both metrics, higher scores represent better performance. Fluency. To measure the readability of generated adversarial examples, we use the acceptability score (ACPT) proposed by Lau et al. (2020), which is based on normalised sentence probabilities produced by XLNet (Yang et al., 2019). Higher scores indicate better fluency. Transferability. To understand the effectiveness of the adversarial examples in attacking another unseen sentiment classifier (TRF), we evaluate the accuracy of C-BERT using adversarial examples that have been generated for attacking clas"
2021.naacl-main.321,D14-1162,0,0.0862032,"d tack, instead of optimising G to produce adversar- be replaced or replaced first. Models with copy ial examples with label distribution {pos: 1.0, neg: mechanism are denoted with the + CPY suffix. 0.0} (from C), label distribution {pos: 0.6, neg: 0.4} is targeted. Generator trained with this additional 4 Experiments and Results constraint is denoted with the + LS suffix. 4.1 Dataset Counter-fitted embeddings (+ CF). Mrkši´c et al. (2016) found that unsupervised word embed- We conduct our experiments using the Yelp review dataset.3 We binarise the ratings,4 use spaCy for dings such as GloVe (Pennington et al., 2014) often tokenisation,5 and keep only reviews with ≤ 50 do not capture synonymy and antonymy relations tokens (hence the dataset is denoted as yelp50). (e.g. cheap and pricey have high similarity). The authors propose to post-process pre-trained word 3 https://www.yelp.com/dataset 4 embeddings with lexical resources (e.g. WordNet) Ratings≥4 is set as positive and ratings≤2 as negative. 5 to produce counter-fitted embeddings that better https://spacy.io 4081 We split the data in a 90/5/5 ratio and downsample the positive class in each set to be equivalent to the negative class, resulting in 407,2"
2021.naacl-main.321,W19-4824,0,0.0488547,"Missing"
2021.naacl-main.321,D16-1058,0,0.0692877,"Missing"
2021.nllp-1.23,2020.acl-main.647,0,0.041008,"Missing"
2021.nllp-1.23,W19-2208,0,0.0655402,", but also the risks associated with naive use of model predictions, including fairness across different user demographics. 1 Introduction which we can make incremental measurable improvements. Text classification of any real-world data can be a challenge for many reasons. In the case of legal text classification, the classes themselves or the legal categorisation of a possible case, can vary from organisation to organisation, and also from court to court; there is no universally agreed-upon set of areas of law neatly defined into a taxonomy (Goncalves and Quaresma, 2005; Sulea et al., 2017a; Soh et al., 2019; Tuggener et al., 2020). Furthermore, a case can span multiple areas of law — for example, a FAMILY LAW matter could also fall under the umbrella of G UARDIANSHIP AND AD MINISTRATION , or a C HARITIES LAW issue may also have aspects regarding E MPLOYEES AND VOLUNTEERS. In addition to the issues surrounding the inherent fuzziness of legal categories, the descriptions of legal issues themselves exhibit a range of language styles: those who seek free legal help are not versed in the legal domain, and may have varying linguistic styles, reflecting their social, cultural, and educational backgroun"
2021.nllp-1.23,N19-1423,0,0.00538772,"ckgrounds, generalise well from small amounts of curated data, and potentially dynamically interact with the help-seeker to clarify the nature of the case. However, in this preliminary work, our aim is to develop initial models as 1 a means to ascertain what biases manifest in our https://www.acnc.gov.au/charity/ given data, and to have a workable model upon 232d6dcbcaa1550da90f825fe6fab643#history 217 Natural Legal Language Processing Workshop 2021, pages 217–227 November 10, 2021. ©2021 Association for Computational Linguistics and 5, which describe the initial fine-tuned B ERT classifiers (Devlin et al., 2019) on the small curated help-seeker data informally describing issues in their own words on matters they believe require legal assistance. As a starting point, we wanted to leverage the patterns of language usage encoded in B ERT given our relatively small data set. The main risk is that while robust results can be achieved by fine-tuning over relatively little labelled data in this manner, the data used in developing the pre-trained models can lead to these models implicitly capturing a variety of biases about the world (Bender et al., 2021). In Section 6, we reveal how these biases manifest fo"
2021.nllp-1.23,2020.lrec-1.155,0,0.0802432,"Missing"
C10-2069,E09-1013,0,0.0280579,"ts above the baseline of simply selecting the highest-ranked topic word. This is the case both when training in-domain over other labelled topics for that topic model, and cross-domain, using only labellings from independent topic models learned over document collections from different domains and genres. 1 trout ﬁsh ﬂy ﬁshing water angler stream rod ﬂies salmon Introduction In the short time since its inception, topic modelling (Blei et al., 2003) has become a mainstream technique for tasks as diverse as multidocument summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). For many of these tasks, the multinomial topics learned by the topic model can be interpreted natively as probabilities, or mapped onto a pre-deﬁned discrete class set. However, for tasks where the learned topics are provided to humans as a ﬁrst-order output, e.g. for use in document collection analysis/navigation, it can be difﬁcult for the end-user to interpret the rich statistical information encoded in the topics. This research is concerned with making topics more readily human interpretable, b"
C10-2069,P10-1044,0,0.0307269,"Missing"
C10-2069,N09-1041,0,0.0366841,"a reranking model, we are able to consistently achieve results above the baseline of simply selecting the highest-ranked topic word. This is the case both when training in-domain over other labelled topics for that topic model, and cross-domain, using only labellings from independent topic models learned over document collections from different domains and genres. 1 trout ﬁsh ﬂy ﬁshing water angler stream rod ﬂies salmon Introduction In the short time since its inception, topic modelling (Blei et al., 2003) has become a mainstream technique for tasks as diverse as multidocument summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). For many of these tasks, the multinomial topics learned by the topic model can be interpreted natively as probabilities, or mapped onto a pre-deﬁned discrete class set. However, for tasks where the learned topics are provided to humans as a ﬁrst-order output, e.g. for use in document collection analysis/navigation, it can be difﬁcult for the end-user to interpret the rich statistical information encoded in the topics. This research is concerned wi"
C10-2069,N10-1012,1,0.787706,"sk is thus termed best word or most representative word selection, as we are selecting the label from the closed set of the top-N topic words in that topic. Naturally, not all topics are equally coherent, however, and the lower the topic coherence, the more difﬁcult the label selection task becomes. For example: oct sept nov aug dec july sun lite adv globe appears to conﬂate months with newspaper names, and no one of these topic words is able to capture the topic accurately. As such, our methodology presupposes an automatic means of rating topics for coherence. Fortunately, recent research by Newman et al. (2010) has shown that this is achievable at levels approaching human performance, meaning that this is not an unreasonable assumption. Labelling topics has applications across a diverse range of tasks. Our original interest in the 605 Coling 2010: Poster Volume, pages 605–613, Beijing, August 2010 problem stems from work in document collection visualisation/navigation, and the realisation that presenting users with topics natively (e.g. as represented by the top-N terms) is ineffective, and would be signiﬁcantly enhanced if we could automatically predict succinct labels for each topic. Another appli"
C10-2069,N04-1041,0,0.0140641,"arginal probability, including when training the ranking model on labelled topics from other document collections. Magatti et al. (2009) proposed a method for labelling topics induced by hierarchical topic modelling, based on ontological alignment with the Google Directory (gDir) hierarchy, and optionally expanding topics based on a thesaurus or WordNet. Preliminary experiments suggest the method has promise, but the method crucially relies on both a hierarchical topic model and a pre-existing ontology, so has limited applicability. Over the general task of labelling a learned semantic class, Pantel and Ravichandran (2004) proposed the use of lexico-semantic patterns involving each member of that class to learn a (usually hypernym) label. The proposed method was shown to perform well over the semantically homogeneous, ﬁne-grained clusters learned by CBC (Pantel and Lin, 2002), but for the coarse-grained, heterogeneous topics learned by topic modelling, it is questionable whether it would work as well. The ﬁrst works to report on human scoring of topics were Chang et al. (2009) and Newman et al. (2010). The ﬁrst study used a novel but synthetic intruder detection task where humans evaluate both topics (that had"
C10-2069,D09-1098,0,0.0135524,"e ranking of words based on these probabilities indicates the importance of a word in a topic, and it is also a feature that we use for predicting the most representative word. We also observe that sometimes the most representative words are generalized concepts of other words. As such, hypernym relations could be another feature that may be relevant to predicting the best word. To this end, we use WordNet to ﬁnd hypernym relations between pairs of words in a topic and obtain a set of boolean-valued relationships for each topic word. Our last feature is the distributional similarity scores of Pantel et al. (2009), as trained over Wikipedia.1 This takes the form of representing the distributional similarity between each pairing of terms sim(wi |wj ); if wi is not in the top-200 most similar terms for a given wj , we assume it to have a similarity of 0. While the above features can be used alone to get a ranking on the ten topic words, we can also use various combinations of features in a reranking model such as support vector regression (SVMrank : Joachims (2006)). Applying the features described above — conditional probabilities, PMI, WordNet hypernym relations, the topic model word rank, and Pantel’s"
C12-1093,P12-1056,0,0.0639088,"at it fails to capture the fact that multiple terms may be involved with the same event (Zanzotto et al., 2011), and requires that at least one term undergoes a sufficiently high jump in relative frequency that the event can be identified. Topic models have been proposed as a means of better capturing events, by way of learning clusters of terms that are associated with a given event, as well as modelling changes in term co-occurrence rather than just term frequency. Most work based on topic modelling has been in the form of retrospective event detection models, however (Kireyev et al., 2009; Diao et al., 2012). Moving to the more general area of the machine learning, several online topic models have been proposed (Hoffman et al., 2010; AlSumait et al., 2008). Hoffman et al. (2010) introduced an online LDA variant that uses variational Bayes as the approximate posterior inference algorithm. The model that is closest in spirit to what we propose is On-Line LDA (OLDA) (AlSumait et al., 2008). Using collapsed Gibbs sampling for approximate inference, OLDA processes documents in an on-line fashion by resampling topic assignments for new documents using parameters from a previously learnt model. We retur"
C12-1093,D11-1024,0,0.0327199,"Missing"
C12-1093,N10-1012,1,0.0594681,"Missing"
C12-1093,N10-1021,0,0.0855016,"Missing"
C12-1093,D12-1134,0,0.0399574,"wish to detect events happening presently in 1 http://webtrends.about.com/od/twitter/a/why_twitter_uses_for_twitter.htm 1520 our time, however, we require on-line event detection models. An example application where real-time responsiveness is critical is earthquake detection (Sakaki et al., 2010), and trend analysis also clearly requires on-line processing in order to be of use (Mathioudakis and Koudas, 2010). Most on-line approaches, however, use a relatively simple keyword-based methodology over a pre-defined set of keywords (Culotta, 2010; Lampos and Cristianini, 2010; Weng and Lee, 2011; Zhao et al., 2012) rather than tackling the more challenging task of open-world event detection. Real-time first story detection (Petrovi´c et al., 2010; Osborne et al., 2012) is the task of detecting the mentions of a breaking story as close as possible in time to its first mention. Here, the system should ideally pick up on the breaking story within seconds or minutes of its first mention in order to have impact, e.g. as an alert system for a newswire agency or intelligence organisation. As such, the methods that are standardly applied to the task tend to be based on analysis of local “burstiness” in the data"
C12-1093,D11-1061,0,\N,Missing
C12-1127,E99-1015,0,\N,Missing
C12-1127,S10-1004,1,\N,Missing
C12-1127,S10-1041,0,\N,Missing
C12-1127,S10-1030,0,\N,Missing
C14-1154,E09-1013,0,0.0784519,"Missing"
C14-1154,P13-1141,0,0.0154691,"pproaches there is a clear connection between (induced) word-senses and tokens, making it possible to identify usages of a specific (new) sense. Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010) consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant wordsenses in corpora, including differences between domains. However, this approach does not identify new senses as it relies on a pre-existing sense inventory. Carpuat et al. (2013) identify words in a domainspecific parallel corpus with novel translations. The method proposed by Lau et al. (2012), and extended by Cook et al. (2013), identifies novel wordsenses using a state-of-the-art word-sense induction (WSI) system. This token-based approach offers a natural account of polysemy and not only identifies word types that have a novel sense, but identifies the token instances of the hypothesized novel senses, without reliance on parallel text or a pre-existing sense inventory. We therefore adopt this method for evaluation on our new dataset, and propose further extensions"
C14-1154,Y11-1028,1,0.838437,"about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not the token instances which give rise to these sense differences. Bamman and Crane (2011) use a parallel Latin–English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al. (2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based approaches"
C14-1154,cook-stevenson-2010-automatically,1,0.937154,"d in research to date; (2) development and evaluation of a new baseline for novel sense detection, reformulations of the method of Lau et al., and a method that incorporates only the expected domain(s) of novel senses; (3) empirical evaluation of the method of Cook et al.; and (4) extension of the novel sense detection method of Cook et al. to automatically acquire information about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not"
C14-1154,W11-2508,0,0.203893,"senses; (3) empirical evaluation of the method of Cook et al.; and (4) extension of the novel sense detection method of Cook et al. to automatically acquire information about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not the token instances which give rise to these sense differences. Bamman and Crane (2011) use a parallel Latin–English corpus to induce word senses and build a WSD system, which they then apply to study diachronic"
C14-1154,S13-2049,0,0.0594711,"Missing"
C14-1154,H05-1053,1,0.75209,"nexpensively constructed in the future (Cook and Hirst, 2012). We conducted some preliminary experiments using domain-specific sports 1632 Method NoveltyDiff NoveltyLLR NoveltyRatio RelevanceAuto RelevanceManual Rank SumDiff,auto Rank SumDiff,manual Upper-bound Baseline F-score BNC–ukWaC SiBol/Port 0.57 0.29 0.67 0.28 0.66 0.28 0.48 0.24 0.45 0.27 0.72 0.30 0.72 0.29 0.72 0.42 0.36 0.20 Table 2: Token-level F-score for the BNC–ukWaC and SiBol/Port datasets using variants of Novelty, Relevance, and Rank Sum. The F-score of an oracle upper-bound and baseline are also shown. and finance corpora (Koeling et al., 2005) and the BNC. However, in these experiments we observed very high NoveltyRatio for many distractors (selected in a similar way to our other experiments). Unlike the case of time difference, in corpora from different domains, an arbitrarily chosen word will tend to cooccur with very different words in the corpora, and NoveltyRatio will consequently be high. To address vocabulary differences between corpora, in their experiments on identifying lexical semantic differences between Dutch dialects, Peirsman et al. (2010) restricted the context words used to represent a target word to those with mod"
C14-1154,S13-2051,1,0.89377,"Missing"
C14-1154,S13-2039,1,0.871209,"Missing"
C14-1154,E12-1060,1,0.877532,"s for identifying new word-senses could benefit applied NLP by helping to keep lexicons up-to-date. In revising dictionaries, lexicographers must identify new word-senses, in addition to new words themselves; methods which identify new word-senses could therefore also help to keep dictionaries current. In this paper, because of the need for lexicon maintenance, we focus on relatively-new word-senses. Specifically, we consider the identification of word-senses that are not attested in a reference corpus, taken to represent standard usage, but that are attested in a focus corpus of newer texts. Lau et al. (2012) introduced the task of novel sense identification. They presented a method for identifying novel word-senses — described here in Section 4 — and evaluated this method on a very small dataset consisting of just five lemmas having a novel sense in a single corpus pair. Cook et al. (2013) extended the method of Lau et al. to incorporate knowledge of the expected domains of new wordsenses, but did not conduct a rigorous empirical evaluation. The remainder of this paper is structured This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings"
C14-1154,J07-4005,1,0.808136,"d senses and build a WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al. (2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based approaches there is a clear connection between (induced) word-senses and tokens, making it possible to identify usages of a specific (new) sense. Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010) consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant wordsenses in corpora, including differences between domains. However, this approach does not identify new senses as it relies on a pre-existing sense inventory. Carpuat et al. (2013) identify words in a domainspecific parallel corpus with novel translations. The method proposed by Lau et al. (2012), and extended by Cook et al. (2013), identifies novel wordsenses using a state-of-the-art word-sense induction (WSI) system. This token-based approach offers a natural account of polysemy and not only identifies word types that have a novel sense, but ide"
C14-1154,S13-2035,0,0.0486422,"Missing"
C14-1154,P11-2053,0,0.252002,"nge in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not the token instances which give rise to these sense differences. Bamman and Crane (2011) use a parallel Latin–English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al. (2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based approaches there is a clear connection between (induced) word-senses and tokens, making it possible to identify usages of a specific (new) sense. Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010) consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant wordsenses in corpora, including differences between domains."
C14-1154,W09-0214,0,0.490803,"arger than has been used in research to date; (2) development and evaluation of a new baseline for novel sense detection, reformulations of the method of Lau et al., and a method that incorporates only the expected domain(s) of novel senses; (3) empirical evaluation of the method of Cook et al.; and (4) extension of the novel sense detection method of Cook et al. to automatically acquire information about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergon"
C14-1154,W11-1102,0,0.0956413,"Missing"
C16-1091,N13-1016,0,0.089093,"el, in particular, the topics themselves, which provide the primary insights into the document collection. The de facto topic representation has been a simple term list, in the form of the top-10 terms in a given topic, ranked in descending order of Pr(wj |ti ). The cognitive overhead in interpreting the topic presented as a list of terms can be high, and has led to interest in the task of generating labels for topics, e.g. in the form of textual descriptions (Mei et al., 2007; Lau et al., 2011; Kou et al., 2015), visual representations of the topic words (Smith et al., to appear), or images (Aletras and Stevenson, 2013). In the former case, for example, rather than the top-10 terms of hschool, student, university, college, teacher, class, education, learn, high, program i, a possible textual label could be simply EDUCATION. Recent work has shown that, in the context of a timed information retrieval (IR) task, automatically-generated textual labels are easier for humans to interpret than the top-10 terms, and lead to equivalent-quality relevance judgements (Aletras et al., 2014). Despite this, the accuracy of state-of-the-art topic generation methods is far from perfect, providing the motivation for this work"
C16-1091,D08-1038,0,0.048096,"of interpreting these topics for end-users, we propose labelling a topic with a succinct phrase that summarises its theme or idea. Using Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Compared to a state-of-the-art topic labelling system, our methodology is simpler, more efficient, and finds better topic labels. 1 Introduction Topic models are a popular approach to detecting trends and traits in document collections, e.g. in tracing the evolution of a scientific field through its publications (Hall et al., 2008), enabling visual navigation over search results (Newman et al., 2010a), interactively labelling document collections (Poursabzi-Sangdeh et al., 2016), or detecting trends in text streams (Wang and McCallum, 2006; AlSumait et al., 2008). They are typically unsupervised, and generate “topics” ti in the form of multinominal distributions over the terms wj of the document collection (Pr(wj |ti )), and topic distributions for each document dk in the collection, in the form of a multinomial distribution over topics (Pr(ti |dk )). Traditionally, this has been carried out based on latent Dirichlet al"
C16-1091,P03-1054,0,0.0052759,"5 In addition to doc2vec, we also experiment with word2vec to generate embeddings for Wikipedia titles. By treating titles as a single token (e.g. concatenating financial crisis into financial crisis) and greedily tokenising the text of all of the Wikipedia articles, we can then generate word embeddings for the titles. For word2vec, we use the skip-gram implementation exclusively.6 For both doc2vec and word2vec, we first pre-process English Wikipedia,7 using Wiki Extractor to clean and extract Wikipedia articles from the original dump.8 We then tokenise words with the Stanford CoreNLP Parser (Klein and Manning, 2003), and lowercase all words. We additionally filter out articles where the article body is made up of less than 40 words, and also disambiguation pages. We also remove titles whose length is longer than 4 words, as they are often too specific or inappropriate as topic labels (e.g. List of Presidents of the United States). For word2vec, we remove any parenthesised sub-component of an article title — e.g. in the case of Democratic Party (United States), we remove (United States) to generate Democratic Party — as we would not expect to find verbatim usages of the full title. This has the potential"
C16-1091,W16-1609,1,0.806617,"of which is no longer publicly available), limiting the general-purpose utility of the method. We propose an alternative approach: precomputing distributed representations of the topic terms and article titles using word2vec and doc2vec. To this end, we train a doc2vec model on the English Wikipedia articles, and represent the embedding of a Wikipedia title by its document embedding. As doc2vec runs word2vec internally, word embeddings are also learnt during the training. Given the top-N topic terms, the topic embedding is represented by these terms’ word embeddings. Based on the findings of Lau and Baldwin (2016) that the simpler dbow has less parameters, trains faster, and performs better than dmpv in several extrinsic tasks, we experiment only with dbow.4 In terms of hyper-parameter settings, we follow the recommendations of Lau and Baldwin (2016).5 In addition to doc2vec, we also experiment with word2vec to generate embeddings for Wikipedia titles. By treating titles as a single token (e.g. concatenating financial crisis into financial crisis) and greedily tokenising the text of all of the Wikipedia articles, we can then generate word embeddings for the titles. For word2vec, we use the skip-gram im"
C16-1091,P11-1154,1,0.376587,"to a human user, a fundamental concern is the best way of presenting the rich information generated by the topic model, in particular, the topics themselves, which provide the primary insights into the document collection. The de facto topic representation has been a simple term list, in the form of the top-10 terms in a given topic, ranked in descending order of Pr(wj |ti ). The cognitive overhead in interpreting the topic presented as a list of terms can be high, and has led to interest in the task of generating labels for topics, e.g. in the form of textual descriptions (Mei et al., 2007; Lau et al., 2011; Kou et al., 2015), visual representations of the topic words (Smith et al., to appear), or images (Aletras and Stevenson, 2013). In the former case, for example, rather than the top-10 terms of hschool, student, university, college, teacher, class, education, learn, high, program i, a possible textual label could be simply EDUCATION. Recent work has shown that, in the context of a timed information retrieval (IR) task, automatically-generated textual labels are easier for humans to interpret than the top-10 terms, and lead to equivalent-quality relevance judgements (Aletras et al., 2014). De"
C16-1091,N10-1012,1,0.906287,"topic with a succinct phrase that summarises its theme or idea. Using Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Compared to a state-of-the-art topic labelling system, our methodology is simpler, more efficient, and finds better topic labels. 1 Introduction Topic models are a popular approach to detecting trends and traits in document collections, e.g. in tracing the evolution of a scientific field through its publications (Hall et al., 2008), enabling visual navigation over search results (Newman et al., 2010a), interactively labelling document collections (Poursabzi-Sangdeh et al., 2016), or detecting trends in text streams (Wang and McCallum, 2006; AlSumait et al., 2008). They are typically unsupervised, and generate “topics” ti in the form of multinominal distributions over the terms wj of the document collection (Pr(wj |ti )), and topic distributions for each document dk in the collection, in the form of a multinomial distribution over topics (Pr(ti |dk )). Traditionally, this has been carried out based on latent Dirichlet allocation (LDA: Blei et al. (2003)) or extensions thereof, but more re"
C16-1091,P16-1110,0,0.0650891,"Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Compared to a state-of-the-art topic labelling system, our methodology is simpler, more efficient, and finds better topic labels. 1 Introduction Topic models are a popular approach to detecting trends and traits in document collections, e.g. in tracing the evolution of a scientific field through its publications (Hall et al., 2008), enabling visual navigation over search results (Newman et al., 2010a), interactively labelling document collections (Poursabzi-Sangdeh et al., 2016), or detecting trends in text streams (Wang and McCallum, 2006; AlSumait et al., 2008). They are typically unsupervised, and generate “topics” ti in the form of multinominal distributions over the terms wj of the document collection (Pr(wj |ti )), and topic distributions for each document dk in the collection, in the form of a multinomial distribution over topics (Pr(ti |dk )). Traditionally, this has been carried out based on latent Dirichlet allocation (LDA: Blei et al. (2003)) or extensions thereof, but more recently there has been interest in deep learning approaches to topic modelling (Ca"
C16-1091,P11-1039,0,\N,Missing
D18-1098,W13-0102,0,0.414812,"offers little insight into the utility of the topics in describing the documents. In this paper, we explore the topic intrusion task — the task of guessing an outlier topic given a document and a set of topics — and propose a method to automate it. We improve upon the state-of-the-art substantially, demonstrating its viability as an alternative method for topic model evaluation. 1 Introduction Topic models have traditionally been evaluated using model perplexity, but there is an increasing trend to use topic coherence as a task-independent evaluation (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; R¨oder et al., 2015). In earlier work (Bhatia et al., 2017), we showed that topic coherence as a standalone evaluation can be misleading, which we illustrated with an adversarial topic model that produces highly coherent topics that collectively tell us little about the content of the document collection. We went on to explore an alternative approach to assessing topics using topic intrusion, based on the manual task of Chang et al. (2009). In the original topic intrusion setup, users are presented with a document, a set of associated topics (from a topic model) and an intr"
D18-1098,K17-1022,1,0.383063,"this paper, we explore the topic intrusion task — the task of guessing an outlier topic given a document and a set of topics — and propose a method to automate it. We improve upon the state-of-the-art substantially, demonstrating its viability as an alternative method for topic model evaluation. 1 Introduction Topic models have traditionally been evaluated using model perplexity, but there is an increasing trend to use topic coherence as a task-independent evaluation (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; R¨oder et al., 2015). In earlier work (Bhatia et al., 2017), we showed that topic coherence as a standalone evaluation can be misleading, which we illustrated with an adversarial topic model that produces highly coherent topics that collectively tell us little about the content of the document collection. We went on to explore an alternative approach to assessing topics using topic intrusion, based on the manual task of Chang et al. (2009). In the original topic intrusion setup, users are presented with a document, a set of associated topics (from a topic model) and an intruder topic, and are tasked to find the intruder. Success in the task demonstrat"
D18-1098,D14-1181,0,0.00303579,"2: mpGOLD vs. System Scores at the model level are triples of (di , tji , yij ) — essentially the task is formulated as a binary classification problem. The architecture of our network is given in Figure 1. The input to our model is a document–topic pair, with each represented as a sequence of words. These words are mapped to embeddings, via embedding matrix W ∈ R|V |×d , where V is the vocabulary and d the dimensionality of the embeddings. The document embeddings Ed ∈ Rk×d (k = document length) and topic embeddings Et ∈ Rm×d (m = number of topic words) are processed via convolutional layers (Kim, 2014; Severyn and Moschitti, 2015) to produce two hidden representations for the document and topic. The convolution operation is performed using feature maps of varying size followed by a max-pooling operation to produce a constant-length vector. The document and topic hidden representations are concatenated and fed to 2 dense layers and ultimately reduced to a sigmoid-activated score. 4.3.1 External IR Feature A good topic model learns common themes in the document collection. A limitation of our network is the lack of global- or collection-level information (as the input consists of only a docu"
D18-1098,E14-1056,1,0.936639,"he utility of the topics in describing the documents. In this paper, we explore the topic intrusion task — the task of guessing an outlier topic given a document and a set of topics — and propose a method to automate it. We improve upon the state-of-the-art substantially, demonstrating its viability as an alternative method for topic model evaluation. 1 Introduction Topic models have traditionally been evaluated using model perplexity, but there is an increasing trend to use topic coherence as a task-independent evaluation (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; R¨oder et al., 2015). In earlier work (Bhatia et al., 2017), we showed that topic coherence as a standalone evaluation can be misleading, which we illustrated with an adversarial topic model that produces highly coherent topics that collectively tell us little about the content of the document collection. We went on to explore an alternative approach to assessing topics using topic intrusion, based on the manual task of Chang et al. (2009). In the original topic intrusion setup, users are presented with a document, a set of associated topics (from a topic model) and an intruder topic, and ar"
D18-1098,D11-1024,0,0.233888,"Missing"
D18-1098,N10-1012,1,0.863488,"ell topic words relate to each other, but offers little insight into the utility of the topics in describing the documents. In this paper, we explore the topic intrusion task — the task of guessing an outlier topic given a document and a set of topics — and propose a method to automate it. We improve upon the state-of-the-art substantially, demonstrating its viability as an alternative method for topic model evaluation. 1 Introduction Topic models have traditionally been evaluated using model perplexity, but there is an increasing trend to use topic coherence as a task-independent evaluation (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; R¨oder et al., 2015). In earlier work (Bhatia et al., 2017), we showed that topic coherence as a standalone evaluation can be misleading, which we illustrated with an adversarial topic model that produces highly coherent topics that collectively tell us little about the content of the document collection. We went on to explore an alternative approach to assessing topics using topic intrusion, based on the manual task of Chang et al. (2009). In the original topic intrusion setup, users are presented with a document, a set of a"
D18-1098,D14-1162,0,0.0811572,"performs mpORIG and mp by a substantial margin, and also has a score close to human judgements. We can attribute this to the fact that nss provides more nuanced system predictions (over the full range [0, 1]), whereas mp tends to be binary.4 For our experiments, we train the model on outputs from all topics models over one dataset, and test it on the other (cross-domain training). We use a single channel for the convolutional networks, pad the documents as necessary (k = 200), and use the top-10 words to represent a topic (i.e. m = 10). Word embeddings are initialised using pre-trained GloVe (Pennington et al., 2014) vectors (d = 100), and their weights are fixed during training. We use kernel windows of width = {3, 5, 7} with 100 feature maps each and two (fully-connected) hidden layers, with dimensionality of 50 and 10. We use a dropout rate of 0.5, 0.5 and 0.25 after the document, topic and first hidden layer, respectively. We set the batch size to 100, and use Adam as the optimizer with a learning rate of 0.001. For activation functions, we use ReLU for the fully-connected layers and sigmoid for the final layer. To reduce variance, we run the models with 8 different seeds for initialisation and take t"
E12-1060,S07-1002,0,0.718641,"assignments — in the form of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words. LDA offers a solution to the question of sense granularity determination via non-parametric formulations, such as a Hierarchical Dirichlet Process (HDP: Teh et al. (2006), Yao and Durme (2011)). Our contributions in this paper are as follows. We first establish the effectiveness of HDP for WSI over both the SemEval-2007 and SemEval2010 WSI datasets (Agirre and Soroa, 2007; Manandhar et al., 2010), and show that the nonparametric formulation is superior to a standard LDA formulation with oracle determination of sense granularity for a given word. We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets. Finally, we apply our method to the novel sense detection task based on a dataset developed in this research, and achieve highly encouraging results. 2 Methodology In topic modelling, documents are assumed to exhibit multiple"
E12-1060,E09-1013,0,0.822927,"s a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and derivative approaches — and use the topic model to determine the appropriate sense granularity. Topic modelling is an unsupervised approach to jointly learn topics — in the form of multinomial probability distributions over words — and per-document topic assignments — in the form of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighte"
E12-1060,cook-stevenson-2010-automatically,1,0.882901,"hese senses is listed in Wordnet 3.0 (Fellbaum, 1998) — yet appear to be in regular usage, particularly in text related to pop culture and online media. The manual identification of such new wordsenses is a challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date. Moreover, lexicons that better reflect contemporary usage could benefit NLP applications that use sense inventories. The challenge of identifying changes in word sense has only recently been considered in computational linguistics. For example, Sagi et al. (2009), Cook and Stevenson (2010), and Gulordava and Baroni (2011) propose type-based models of semantic change. Such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense. Bamman and Crane (2011) use a parallel Latin– English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in word senses. Crucially, in this token-based approach there is a clear connection between word senses and tokens, making it possible to identify usages of a specific sense. Based on the findings in Section 3.2, here we apply the HDP method for WSI to"
E12-1060,de-marneffe-etal-2006-generating,0,0.00362124,"Missing"
E12-1060,H92-1045,0,0.355926,"ruction of semantic space models, e.g. for WSD. Based on these findings, we include dependency relations as additional features in our topic models,2 but just for dependency relations that involve the target word. 2.2 Topic Modelling Topic models learn a probability distribution over topics for each document, by simply aggregating the distributions over topics for each word in the document. In WSI terms, we take this distribution over topics for each target word (“instance” in WSI parlance) as our distribution over senses for that word. 1 Notwithstanding the one sense per discourse heuristic (Gale et al., 1992). 2 We use the Stanford Parser to do part of speech tagging and to extract the dependency relations (Klein and Manning, 2003; De Marneffe et al., 2006). In our initial experiments, we use LDA topic modelling, which requires us to set T , the number of topics to be learned by the model. The LDA generative process is: (1) draw a latent topic z from a document-specific topic distribution P (t = z|d) then; (2) draw a word w from the chosen topic P (w|t = z). Thus, the probability of producing a single copy of word w given a document d is given by: P (w|d) = T ∑ P (w|t = z)P (t = z|d). z=1 In stand"
E12-1060,W11-2508,0,0.0612863,"t 3.0 (Fellbaum, 1998) — yet appear to be in regular usage, particularly in text related to pop culture and online media. The manual identification of such new wordsenses is a challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date. Moreover, lexicons that better reflect contemporary usage could benefit NLP applications that use sense inventories. The challenge of identifying changes in word sense has only recently been considered in computational linguistics. For example, Sagi et al. (2009), Cook and Stevenson (2010), and Gulordava and Baroni (2011) propose type-based models of semantic change. Such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense. Bamman and Crane (2011) use a parallel Latin– English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in word senses. Crucially, in this token-based approach there is a clear connection between word senses and tokens, making it possible to identify usages of a specific sense. Based on the findings in Section 3.2, here we apply the HDP method for WSI to the task of 596 identifying new"
E12-1060,S10-1079,0,0.195775,"Missing"
E12-1060,S10-1011,0,0.457638,"rm of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words. LDA offers a solution to the question of sense granularity determination via non-parametric formulations, such as a Hierarchical Dirichlet Process (HDP: Teh et al. (2006), Yao and Durme (2011)). Our contributions in this paper are as follows. We first establish the effectiveness of HDP for WSI over both the SemEval-2007 and SemEval2010 WSI datasets (Agirre and Soroa, 2007; Manandhar et al., 2010), and show that the nonparametric formulation is superior to a standard LDA formulation with oracle determination of sense granularity for a given word. We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets. Finally, we apply our method to the novel sense detection task based on a dataset developed in this research, and achieve highly encouraging results. 2 Methodology In topic modelling, documents are assumed to exhibit multiple topics, with each documen"
E12-1060,D10-1012,0,0.0734893,"sk. 1 Introduction Word sense induction (WSI) is the task of automatically inducing the different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances. It contrasts with word sense disambiguation (WSD), where a fixed sense inventory is assumed to exist, and token instances of a given word are disambiguated relative to the sense inventory. While WSI is intuitively appealing as a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and derivative approaches — and use t"
E12-1060,S07-1037,0,0.0562894,"Missing"
E12-1060,J07-2002,0,0.0570721,"Missing"
E12-1060,W09-0214,0,0.117489,"Missing"
E12-1060,J98-1004,0,0.680031,"sense detection task. 1 Introduction Word sense induction (WSI) is the task of automatically inducing the different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances. It contrasts with word sense disambiguation (WSD), where a fixed sense inventory is assumed to exist, and token instances of a given word are disambiguated relative to the sense inventory. While WSI is intuitively appealing as a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and"
E12-1060,W11-1102,0,0.612441,"Missing"
E14-1056,W13-0102,0,0.849832,"d found that their measure correlates well with human judgements of observed coherence (where topics were rated in the same manner as Newman et al. (2010), based on a 3-point ordinal scale). To incorporate the evaluation of semantic coherence into the topic model, the authors proposed to record words that co-occur together frequently, and update the counts of all associated words before and after the sampling of a new topic assignment in the Gibbs sampler. This variant of topic model was shown to produce more coherent topics than LDA based on the log conditional probability coherence measure. Aletras and Stevenson (2013a) introduced distributional semantic similarity methods for computing coherence, calculating the distributional similarity between semantic vectors for the top-N topic words using a range of distributional similarity measures such as cosine similarity and the Dice coefficient. To construct the semantic vector space for the topic words, they used English Wikipedia as the reference corpus, and collected words that co-occur in a window of ±5 words. They showed that their method correlates well with the observed coherence rated by human judges. (2010) introduced the notion of topic “coherence”, a"
E14-1056,C10-2069,1,0.630341,"1.2 million New York Times articles from 1994 to 2004 (from the English Gigaword); and (2) W IKI -F ULL, which contains 3.3 million English Wikipedia articles (retrieved November 28th 2009).2 The rationale for choosing the New York Times and English Wikipedia as the reference corpora is to ensure domain consistency with the word intrusion dataset; the full collections are used to more robustly estimate lexical probabilities. 4 task is that it requires human annotations, therefore preventing large-scale evaluation. We begin by proposing a methodology to fully automate the word intrusion task. Lau et al. (2010) proposed a methodology that learns the most representative or best topic word that summarises the semantics of the topic. Observing that the word intrusion task — the task of detecting the least representative word — is the converse of the best topic word selection task, we adapt their methodology to automatically identify the intruder word for the word intrusion task, based on the knowledge that there is a unique intruder word per topic. The methodology works as follows: given a set of topics (including intruder words), we compute the word association features for each of the topN topic word"
E14-1056,N13-1016,0,0.19099,"d found that their measure correlates well with human judgements of observed coherence (where topics were rated in the same manner as Newman et al. (2010), based on a 3-point ordinal scale). To incorporate the evaluation of semantic coherence into the topic model, the authors proposed to record words that co-occur together frequently, and update the counts of all associated words before and after the sampling of a new topic assignment in the Gibbs sampler. This variant of topic model was shown to produce more coherent topics than LDA based on the log conditional probability coherence measure. Aletras and Stevenson (2013a) introduced distributional semantic similarity methods for computing coherence, calculating the distributional similarity between semantic vectors for the top-N topic words using a range of distributional similarity measures such as cosine similarity and the Dice coefficient. To construct the semantic vector space for the topic words, they used English Wikipedia as the reference corpus, and collected words that co-occur in a window of ±5 words. They showed that their method correlates well with the observed coherence rated by human judges. (2010) introduced the notion of topic “coherence”, a"
E14-1056,P11-1154,1,0.788243,"ic method for estimating topic coherence based on pairwise pointwise mutual information (PMI) between the topic words. Mimno et al. (2011) similarly introduced a methodology for computing coherence, replacing PMI with log conditional probability. Musat et al. (2011) incorporated the WordNet hierarchy to capture the relevance of topics, and in Aletras and Stevenson (2013a), the authors proposed the use of distributional similarity for computing the pairwise association of the topic words. One application of these methods has been to remove incoherent topics before generating labels for topics (Lau et al., 2011; Aletras and Stevenson, 2013b). Ultimately, all these methodologies, and also the word intrusion approach, attempt to assess the same quality: the human-interpretability of topics. The relationship between these methodologies, however, is poorly understood, and there is no consensus on what is the best approach for computing the semantic interpretability of topic models. This is a second contribution of this paper: we perform a systematic empirical comparison of the different methods and find appreciable differences between them. We further go on to propose an improved formulation of Newman e"
E14-1056,C12-1093,1,0.34226,"(2009) at near-human levels of accuracy, as a result of which we can perform automatic evaluation of the human-interpretability of topics, as well as topic models. There has been prior work to directly estimate the human-interpretability of topics through automatic means. For example, Newman et al. Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2"
E14-1056,E12-1060,1,0.435095,"(2009) at near-human levels of accuracy, as a result of which we can perform automatic evaluation of the human-interpretability of topics, as well as topic models. There has been prior work to directly estimate the human-interpretability of topics through automatic means. For example, Newman et al. Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2"
E14-1056,N09-1041,0,0.0355475,"valuation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2007). In fields such as the digital humanities, on the other hand, human users interact directly with the output of topic models. It is this context of topic modelling for direct human consumption that we target in this paper. 530 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 530–539, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics word that does not belong to the topic in question. Newman e"
E14-1056,D11-1024,0,0.908556,"t al. (2010) capture topic interpretability using a more direct approach, by asking human users to rate topics (represented by their top-10 topic words) on a 3-point scale based on how coherent the topic words are (i.e. their observed coherence). They proposed several ways of automating the estimation of the observed coherence, and ultimately found that a simple method based on PMI term co-occurrence within a sliding context window over English Wikipedia produces the consistently best result, nearing levels of interannotator agreement over topics learnt from two distinct document collections. Mimno et al. (2011) proposed a closely-related method for evaluating semantic coherence, replacing PMI with log conditional probability. Rather than using Wikipedia for sampling the word cooccurrence counts, Mimno et al. (2011) used the topic-modelled documents, and found that their measure correlates well with human judgements of observed coherence (where topics were rated in the same manner as Newman et al. (2010), based on a 3-point ordinal scale). To incorporate the evaluation of semantic coherence into the topic model, the authors proposed to record words that co-occur together frequently, and update the co"
E14-1056,D08-1038,0,0.0958689,"nstration that we can automate the method of Chang et al. (2009) at near-human levels of accuracy, as a result of which we can perform automatic evaluation of the human-interpretability of topics, as well as topic models. There has been prior work to directly estimate the human-interpretability of topics through automatic means. For example, Newman et al. Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau"
E14-1056,N10-1012,1,0.862842,"e, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2007). In fields such as the digital humanities, on the other hand, human users interact directly with the output of topic models. It is this context of topic modelling for direct human consumption that we target in this paper. 530 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 530–539, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics word that does not belong to the topic in question. Newman et al. (2010) capture topic interpretability using a more direct approach, by asking human users to rate topics (represented by their top-10 topic words) on a 3-point scale based on how coherent the topic words are (i.e. their observed coherence). They proposed several ways of automating the estimation of the observed coherence, and ultimately found that a simple method based on PMI term co-occurrence within a sliding context window over English Wikipedia produces the consistently best result, nearing levels of interannotator agreement over topics learnt from two distinct document collections. Mimno et al."
E17-2111,N13-1016,1,0.897975,"ikolaos Aletras3 and Timothy Baldwin1 1 Computing and Information Systems, The University of Melbourne 2 IBM Research 3 Amazon.com {ionutsorodoc,jeyhan.lau,nikos.aletras}@gmail.com tb@ldwin.net Abstract tras et al., 2014; Aletras et al., 2017); (b) there is a potential bias in presenting the topic based on a fixed cardinality (Lau and Baldwin, 2016); and (c) it can be hard to interpret mixed or incoherent topics (Newman et al., 2010b). Automatic topic labelling methods have been proposed to assist with topic interpretation, e.g. based on text (Lau et al., 2011; Bhatia et al., 2016) or images (Aletras and Stevenson, 2013; Aletras and Mittal, 2017), with recent work showing that the optimal modality (i.e. text or image) for topic labelling varies across topics (Aletras and Mittal, 2017). The focus of this paper is the automatic rating of a textual or image label for a given topic. Our contributions are as follows: 1. we develop and release a novel topic labelling dataset with manually-scored image and text labels for a diverse set of topics; one particular point of divergence from other text–image datasets is that text and image labels are rated on a common scale, and the optimal modality (text vs. image) for"
E17-2111,P14-2103,1,0.88136,"ethods usually involve two main steps: (1) the generation of candidate labels (e.g. text or images) for a given topic; and (2) the ranking of candidate labels by relevance to the topic. Textual labels have been sourced from in a number of different ways, including noun chunks from a reference corpus (Mei et al., 2007), Wikipedia ar701 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 701–706, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ticle titles (Lau et al., 2011; Aletras and Stevenson, 2014; Bhatia et al., 2016), or short text summaries (Cano Basave et al., 2014; Wan and Wang, 2016). Images are often selected from Wikipedia or the web based on querying with topic words (Aletras and Stevenson, 2013; Aletras and Mittal, 2017). Recent work on topic labelling has shown that text or image embeddings can improve candidate label generation and ranking (Bhatia et al., 2016; Aletras and Mittal, 2017). Bhatia et al. (2016) use word2vec (Mikolov et al., 2013) and doc2vec (Le and Mikolov, 2014) to represent topics and candidate textual labels in the same latent semantic space. The most rele"
E17-2111,C16-1091,1,0.294226,"ut Sorodoc1 , Jey Han Lau1,2 , Nikolaos Aletras3 and Timothy Baldwin1 1 Computing and Information Systems, The University of Melbourne 2 IBM Research 3 Amazon.com {ionutsorodoc,jeyhan.lau,nikos.aletras}@gmail.com tb@ldwin.net Abstract tras et al., 2014; Aletras et al., 2017); (b) there is a potential bias in presenting the topic based on a fixed cardinality (Lau and Baldwin, 2016); and (c) it can be hard to interpret mixed or incoherent topics (Newman et al., 2010b). Automatic topic labelling methods have been proposed to assist with topic interpretation, e.g. based on text (Lau et al., 2011; Bhatia et al., 2016) or images (Aletras and Stevenson, 2013; Aletras and Mittal, 2017), with recent work showing that the optimal modality (i.e. text or image) for topic labelling varies across topics (Aletras and Mittal, 2017). The focus of this paper is the automatic rating of a textual or image label for a given topic. Our contributions are as follows: 1. we develop and release a novel topic labelling dataset with manually-scored image and text labels for a diverse set of topics; one particular point of divergence from other text–image datasets is that text and image labels are rated on a common scale, and the"
E17-2111,P16-1110,0,0.0188527,"rating for the label, relative to the topic. Experiments show that this multimodal approach outperforms single-modality topic labelling systems. 1 Introduction LDA-style topic models (Blei et al., 2003) are a popular approach to document clustering, with the “topics” (in the form of multinominal distributions over words) and topic allocations per document (in the form of a multinomial distribution over the topics) providing a powerful document collection visualisation, gisting and navigational aid (Griffiths et al., 2007; Newman et al., 2010a; Chaney and Blei, 2012; Sievert and Shirley, 2014; Poursabzi-Sangdeh et al., 2016). Given its internal structure, an obvious way of presenting a topic t is as a ranked list of the highest-probability terms wi based on Pr(wi |t), often simply based on a fixed “cardinality” (i.e. number of topic words) such as 10. However, this has a number of disadvantages: (a) there is a cognitive load in forming an impression of what concept the topic represents from its topic words (Ale2 Related work Topic labelling methods usually involve two main steps: (1) the generation of candidate labels (e.g. text or images) for a given topic; and (2) the ranking of candidate labels by relevance to"
E17-2111,W14-3110,0,0.143765,"automatically generates a rating for the label, relative to the topic. Experiments show that this multimodal approach outperforms single-modality topic labelling systems. 1 Introduction LDA-style topic models (Blei et al., 2003) are a popular approach to document clustering, with the “topics” (in the form of multinominal distributions over words) and topic allocations per document (in the form of a multinomial distribution over the topics) providing a powerful document collection visualisation, gisting and navigational aid (Griffiths et al., 2007; Newman et al., 2010a; Chaney and Blei, 2012; Sievert and Shirley, 2014; Poursabzi-Sangdeh et al., 2016). Given its internal structure, an obvious way of presenting a topic t is as a ranked list of the highest-probability terms wi based on Pr(wi |t), often simply based on a fixed “cardinality” (i.e. number of topic words) such as 10. However, this has a number of disadvantages: (a) there is a cognitive load in forming an impression of what concept the topic represents from its topic words (Ale2 Related work Topic labelling methods usually involve two main steps: (1) the generation of candidate labels (e.g. text or images) for a given topic; and (2) the ranking of"
E17-2111,N16-1057,1,0.860234,"Missing"
E17-2111,P16-1217,0,0.0185589,"r a given topic; and (2) the ranking of candidate labels by relevance to the topic. Textual labels have been sourced from in a number of different ways, including noun chunks from a reference corpus (Mei et al., 2007), Wikipedia ar701 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 701–706, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ticle titles (Lau et al., 2011; Aletras and Stevenson, 2014; Bhatia et al., 2016), or short text summaries (Cano Basave et al., 2014; Wan and Wang, 2016). Images are often selected from Wikipedia or the web based on querying with topic words (Aletras and Stevenson, 2013; Aletras and Mittal, 2017). Recent work on topic labelling has shown that text or image embeddings can improve candidate label generation and ranking (Bhatia et al., 2016; Aletras and Mittal, 2017). Bhatia et al. (2016) use word2vec (Mikolov et al., 2013) and doc2vec (Le and Mikolov, 2014) to represent topics and candidate textual labels in the same latent semantic space. The most relevant textual labels for a topic are selected from Wikipedia article titles using the cosine si"
E17-2111,P11-1154,1,0.80732,"opic Labelling Ionut Sorodoc1 , Jey Han Lau1,2 , Nikolaos Aletras3 and Timothy Baldwin1 1 Computing and Information Systems, The University of Melbourne 2 IBM Research 3 Amazon.com {ionutsorodoc,jeyhan.lau,nikos.aletras}@gmail.com tb@ldwin.net Abstract tras et al., 2014; Aletras et al., 2017); (b) there is a potential bias in presenting the topic based on a fixed cardinality (Lau and Baldwin, 2016); and (c) it can be hard to interpret mixed or incoherent topics (Newman et al., 2010b). Automatic topic labelling methods have been proposed to assist with topic interpretation, e.g. based on text (Lau et al., 2011; Bhatia et al., 2016) or images (Aletras and Stevenson, 2013; Aletras and Mittal, 2017), with recent work showing that the optimal modality (i.e. text or image) for topic labelling varies across topics (Aletras and Mittal, 2017). The focus of this paper is the automatic rating of a textual or image label for a given topic. Our contributions are as follows: 1. we develop and release a novel topic labelling dataset with manually-scored image and text labels for a diverse set of topics; one particular point of divergence from other text–image datasets is that text and image labels are rated on a"
E17-2111,P14-2050,0,0.0115681,"(Le and Mikolov, 2014) to represent topics and candidate textual labels in the same latent semantic space. The most relevant textual labels for a topic are selected from Wikipedia article titles using the cosine similarity between the topic and article title embeddings. Finally, top labels are re-ranked in a supervised fashion using various features such as the PageRank score of the article in Wikipedia (Brin and Page, 1998), trigram letter ranking (Kou et al., 2015), topic word overlap, and word length of the label. Aletras and Mittal (2017) use pre-computed dependency-based word embeddings (Levy and Goldberg, 2014) to represent the topics and the caption of the images, as well as image embeddings using the output layer of VGG-net (Simonyan and Zisserman, 2014) pretrained on ImageNet (Deng et al., 2009). A concatenation of these three vectors is the input to a simple deep neural network with four hidden layers and a sigmoid output layer to predict the relevance score. Textual or visual modalities for labelling topics have been studied extensively, although independently from one another. Our work differs from the single-modality methods described above in that it uses a joint model to predict the continu"
E17-2111,P14-2101,0,\N,Missing
I13-1080,baldwin-awab-2006-open,1,0.568017,"r example, ‘mk’ would capture context features for the membeli and membelikan variants of the stem beli “buy”. 4 We had also manually grouped stems from other word classes: 48 noun stems were grouped into 13 subclasses; and 27 adjective stems were grouped into 5, giving us a total of 100 stems with the 25 verbs, but we only report on the verb experiments. 5 http://dumps.wikimedia.org/idwiki/ 6 http://www.cs.technion.ac.il/˜gabr/ resources/code/wikiprep 7 http://opennlp.apache.org/ Our experiments showed that OpenNLP’s English models performed better than a rule-based Malay sentence tokeniser (Baldwin and Awab, 2006). Window Size (win): This stipulates the context window size, relative to individual occurrences of the target lexeme, and can take a value of 1–5. 688 M ORPHOLOGY V ERB F RAME D ECOMPOSITION Example Type A: acuh “to heed”, terjemah “translate”, mandi “bathe” ME N+V 1 – – ME N+V 1 + KAN <NPa , NPb &gt; DOto ( [NPa ], [V1 TO( [NP] ) ] ) Example Type B: dengar “hear”, kenang “think of” ME N+V 3 <NPa , NPb &gt; HAPPENto ( [NPb ], [ V3 TO([NPa ] ) ] ) ME N+V 3 + KAN <NPa , NPb &gt; DOto ( [NPa ], [ V3 TO( [NPb ] ) ] ) Table 2: Manually generated verb Types (‘–’ = no attested word form in the text; ‘{. . ."
I13-1080,J03-4004,0,0.0240065,"enchmark system, a method also employed by systems such as Schulte im Walde (2002) for German and Jurgens and Stevens (2010) for English word sense induction. In both cases, we base our experiments on the 735 lexemes identified as being able to be affixed with -kan, and the unigram features from Section 4.1. Note, however, that evaluation is based on the subset of the 735 lexemes which were manually classified into classes and types in Section 3. We employ a bagging approach (sampling with Distributional semantic models are commonly employed in the induction and disambiguation of word senses (McCarthy and Carroll, 2003; Lapata and Brew, 2004; Brody and Lapata, 2009; Lau et al., 2012), and to a lesser extent, in learning syntactic classes and diathesis alternation behaviour (Parisien and Stevenson, 2011; Bonial et al., 2011). We infer lexical similarity and soft word clusters using topic modelling, based on a hierarchical Dirichlet process (HDP: Teh et al. (2006)), a non-parametric extension of latent Dirichlet allocation (LDA: Blei et al. (2003)). LDA is a Bayesian generative topic model that learns latent topics for a collection of documents 689 System Maj. Rand. ON - ALL ON - VERBS L EVIN - HDP .174 .367"
I13-1080,W11-0910,0,0.0917186,"les Brent (1993), in that we rely mainly on linguistic knowledge based on simple lexical features. However, the way linguistic knowledge is learned and applied is quite different, as we will see in Section 3 In terms of the methodology, the studies that we look to are those systems that are built to disambiguate and/or discover syntactico-semantic Levin-style classes, rather than systems that aim to induce valency or syntactic frame information from corpora. These can be built in a supervised fashion as in Lapata and Brew (2004) or tackled as a clustering task as in Schulte im Walde (2006) or Bonial et al. (2011). Lapata and Brew (2004) develop a semi-supervised system that generates, for a given verb and its syntactic frame, a probability distribution over the Levin verb classes. They then use this system to disambiguate tokens using collocation information. Our system, like Schulte im Walde (2006), uses an unsupervised clustering approach. In her approach, Schulte Im Walde employs hierarchical agglomerative clustering over parse features to discover word classes in German, and evaluates using manually-created goldstandard data. 3 Evaluation Data This section describes how we arrive at the two evalua"
I13-1080,J93-2002,0,0.0976834,"eparating unaccusative from unergative stems, which predicts their morphosynatic behaviour. However the facts of -kan seem more intricate than this characterisation. Even though the causative and benefactive constructions uses of kan are the most commonly cited, its usage is much more varied and nuanced, as shown by Kroeger (2007), which is why we chose this morpho-syntactic construction as our case study. Since the early ’90s, the tools and resources employed in valency acquisition tasks have become increasingly sophisticated and lingusitically-rich. One of the earlier examples of this is by Brent (1993), who employs a system based on deterministic morphological cues to identify predefined syntactic patterns from the Brown Corpus. Manning (1993) employs a shallow parser or chunker in order to acquire subcategorisation frames from the New York Times. Schulte im Walde (2002) induces subcategorisation information for German with the use of a lexicalised probabilistic context free grammar (PCFG), and O’Donovan et al. (2005) employ the richly-annotated Penn Treebank in achieving this endeavour. In terms of resources, our work most closely resembles Brent (1993), in that we rely mainly on linguisti"
I13-1080,J05-3003,0,0.0273919,"Missing"
I13-1080,E09-1013,0,0.0309022,"such as Schulte im Walde (2002) for German and Jurgens and Stevens (2010) for English word sense induction. In both cases, we base our experiments on the 735 lexemes identified as being able to be affixed with -kan, and the unigram features from Section 4.1. Note, however, that evaluation is based on the subset of the 735 lexemes which were manually classified into classes and types in Section 3. We employ a bagging approach (sampling with Distributional semantic models are commonly employed in the induction and disambiguation of word senses (McCarthy and Carroll, 2003; Lapata and Brew, 2004; Brody and Lapata, 2009; Lau et al., 2012), and to a lesser extent, in learning syntactic classes and diathesis alternation behaviour (Parisien and Stevenson, 2011; Bonial et al., 2011). We infer lexical similarity and soft word clusters using topic modelling, based on a hierarchical Dirichlet process (HDP: Teh et al. (2006)), a non-parametric extension of latent Dirichlet allocation (LDA: Blei et al. (2003)). LDA is a Bayesian generative topic model that learns latent topics for a collection of documents 689 System Maj. Rand. ON - ALL ON - VERBS L EVIN - HDP .174 .367 .114 .065 L EVIN - NOHDP .057 .111 T YPES - HDP"
I13-1080,schulte-im-walde-2002-subcategorisation,0,0.172537,"s much more varied and nuanced, as shown by Kroeger (2007), which is why we chose this morpho-syntactic construction as our case study. Since the early ’90s, the tools and resources employed in valency acquisition tasks have become increasingly sophisticated and lingusitically-rich. One of the earlier examples of this is by Brent (1993), who employs a system based on deterministic morphological cues to identify predefined syntactic patterns from the Brown Corpus. Manning (1993) employs a shallow parser or chunker in order to acquire subcategorisation frames from the New York Times. Schulte im Walde (2002) induces subcategorisation information for German with the use of a lexicalised probabilistic context free grammar (PCFG), and O’Donovan et al. (2005) employ the richly-annotated Penn Treebank in achieving this endeavour. In terms of resources, our work most closely resembles Brent (1993), in that we rely mainly on linguistic knowledge based on simple lexical features. However, the way linguistic knowledge is learned and applied is quite different, as we will see in Section 3 In terms of the methodology, the studies that we look to are those systems that are built to disambiguate and/or discov"
I13-1080,S10-1080,0,0.0191369,"orted are based on the median of 11 random assignments. We use pairwise precision (pP ), recall (pR), and F-score (pF1 ) to evaluate our generated clusters, relative to the gold-standard word classes, as described by Schulte im Walde (2006). 5 Modelling Distributional Similarity Results We perform two experiments. First, we apply the hierarchical Dirichlet process (HDP) to produce topic probabilities, over which we perform HAC. Second, we perform HAC over the raw unigram features (NoHDP), as our benchmark system, a method also employed by systems such as Schulte im Walde (2002) for German and Jurgens and Stevens (2010) for English word sense induction. In both cases, we base our experiments on the 735 lexemes identified as being able to be affixed with -kan, and the unigram features from Section 4.1. Note, however, that evaluation is based on the subset of the 735 lexemes which were manually classified into classes and types in Section 3. We employ a bagging approach (sampling with Distributional semantic models are commonly employed in the induction and disambiguation of word senses (McCarthy and Carroll, 2003; Lapata and Brew, 2004; Brody and Lapata, 2009; Lau et al., 2012), and to a lesser extent, in lea"
I13-1080,J06-2001,0,0.455149,"e specific morpho-syntactic behaviour of the kan-affixed verb is very much determined by the type of stem it attaches to, and its resulting behaviour varies from stem type to stem type (Kroeger, 2007; Vamarasi, 1999; Arka, 1993). The spectrum of variation induced by the affixing of -kan is not observed on all types of stems, and so being able to identify these superordinate types, representing the same morpho-syntactic variation, would assist greatly in accelerating lexicon development. It has been shown that Levin classes can be successfully induced employing unsupervised methods (Schulte im Walde, 2006; Kipper et al., 2006). We investigate the viability of automatiIn this study we investigate how we can learn both: (a) syntactic classes that capture the range of predicate argument structures (PASs) of a word and the syntactic alternations it participates in, but ignore large semantic differences in the component words; and (b) syntactico-semantic classes that capture PAS and alternation properties, but are also semantically coherent (a la Levin classes). We focus on Indonesian as our case study, a language that is spoken by more than 165 million speakers, but is nonetheless relatively under"
I13-1080,kipper-etal-2006-extending,0,0.035871,"rpho-syntactic behaviour of the kan-affixed verb is very much determined by the type of stem it attaches to, and its resulting behaviour varies from stem type to stem type (Kroeger, 2007; Vamarasi, 1999; Arka, 1993). The spectrum of variation induced by the affixing of -kan is not observed on all types of stems, and so being able to identify these superordinate types, representing the same morpho-syntactic variation, would assist greatly in accelerating lexicon development. It has been shown that Levin classes can be successfully induced employing unsupervised methods (Schulte im Walde, 2006; Kipper et al., 2006). We investigate the viability of automatiIn this study we investigate how we can learn both: (a) syntactic classes that capture the range of predicate argument structures (PASs) of a word and the syntactic alternations it participates in, but ignore large semantic differences in the component words; and (b) syntactico-semantic classes that capture PAS and alternation properties, but are also semantically coherent (a la Levin classes). We focus on Indonesian as our case study, a language that is spoken by more than 165 million speakers, but is nonetheless relatively under-resourced in terms of"
I13-1080,J04-1003,0,0.196135,"reebank in achieving this endeavour. In terms of resources, our work most closely resembles Brent (1993), in that we rely mainly on linguistic knowledge based on simple lexical features. However, the way linguistic knowledge is learned and applied is quite different, as we will see in Section 3 In terms of the methodology, the studies that we look to are those systems that are built to disambiguate and/or discover syntactico-semantic Levin-style classes, rather than systems that aim to induce valency or syntactic frame information from corpora. These can be built in a supervised fashion as in Lapata and Brew (2004) or tackled as a clustering task as in Schulte im Walde (2006) or Bonial et al. (2011). Lapata and Brew (2004) develop a semi-supervised system that generates, for a given verb and its syntactic frame, a probability distribution over the Levin verb classes. They then use this system to disambiguate tokens using collocation information. Our system, like Schulte im Walde (2006), uses an unsupervised clustering approach. In her approach, Schulte Im Walde employs hierarchical agglomerative clustering over parse features to discover word classes in German, and evaluates using manually-created golds"
I13-1080,E12-1060,1,0.841477,"(2002) for German and Jurgens and Stevens (2010) for English word sense induction. In both cases, we base our experiments on the 735 lexemes identified as being able to be affixed with -kan, and the unigram features from Section 4.1. Note, however, that evaluation is based on the subset of the 735 lexemes which were manually classified into classes and types in Section 3. We employ a bagging approach (sampling with Distributional semantic models are commonly employed in the induction and disambiguation of word senses (McCarthy and Carroll, 2003; Lapata and Brew, 2004; Brody and Lapata, 2009; Lau et al., 2012), and to a lesser extent, in learning syntactic classes and diathesis alternation behaviour (Parisien and Stevenson, 2011; Bonial et al., 2011). We infer lexical similarity and soft word clusters using topic modelling, based on a hierarchical Dirichlet process (HDP: Teh et al. (2006)), a non-parametric extension of latent Dirichlet allocation (LDA: Blei et al. (2003)). LDA is a Bayesian generative topic model that learns latent topics for a collection of documents 689 System Maj. Rand. ON - ALL ON - VERBS L EVIN - HDP .174 .367 .114 .065 L EVIN - NOHDP .057 .111 T YPES - HDP .281 .261 .271 .14"
I13-1080,P93-1032,0,\N,Missing
I17-1075,C12-1064,0,0.241795,"ion 744 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 744–753, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Partition Training Development Test Related Work Early work in geolocation prediction operated at the user-level. Backstrom et al. (2010) developed a methodology to predict the location of a user on Facebook by measuring the relationship between geography and friendship networks, and Cheng et al. (2010) proposed a content-based prediction system to predict a Twitter user’s location based purely on his/her tweet messages. Han et al. (2012) introduced tweet-level prediction, where they first extract location indicative words by leveraging the geotagged tweets and then train a classifier for geolocation prediction using the location indicative words as features. Extending on this, systems were developed to better rank these location indicative or geospatial words by locality (Chang et al., 2012; Laere et al., 2014; Han et al., 2014). More recently, Han et al. (2016) proposed a shared task for Twitter geolocation prediction, offering a benchmark dataset on the task. Hashing is an effective method to compress data for fast access a"
I17-1075,W16-3929,0,0.337912,"Missing"
I17-1075,D14-1181,0,0.0031887,"in hours or bins. This preference of bins should be different to tweets from a distant location (e.g. East Asia). Assuming each bin follows a Gaussian distribution, then the goal of the network is to learn where ri is the output value and µi and σi are the parameters for bin i. Let B be the total number of bins, the feature vector generated by a RBF network is given as follows: frbf = [r0 , r1 , ..., rB−1 ] where frbf ∈ RB . 3.2.3 Convolutional Network Location is a user self-declared field in the metadata. As it is free-form text, we use a standard character-level convolution neural network (Kim, 2014) to process it. The network architecture is simpler compared to the text network (Section 3.2.1): it has no recurrent and self-attention layers, and max-over-time pooling is performed over all spans. Let xt ∈ RE denote the character embedding of the t-th character in the tweet. A tweet of T characters is represented by a concatenation of its character vectors: x0:T −1 = x0 ⊕ x1 ⊕ ... ⊕ xT −1 . We 7 As an example, 17:25 is converted to 0.726. UTC offset minimum is assumed -12 and maximum +14 based on: https://en.wikipedia.org/wiki/ List_of_UTC_time_offsets. 8 747 use convolutional filters and m"
I17-1075,W16-3930,1,0.898224,"Missing"
I17-1075,W16-3931,0,0.195993,"Missing"
I17-1075,W16-3928,0,0.753432,"d friendship networks, and Cheng et al. (2010) proposed a content-based prediction system to predict a Twitter user’s location based purely on his/her tweet messages. Han et al. (2012) introduced tweet-level prediction, where they first extract location indicative words by leveraging the geotagged tweets and then train a classifier for geolocation prediction using the location indicative words as features. Extending on this, systems were developed to better rank these location indicative or geospatial words by locality (Chang et al., 2012; Laere et al., 2014; Han et al., 2014). More recently, Han et al. (2016) proposed a shared task for Twitter geolocation prediction, offering a benchmark dataset on the task. Hashing is an effective method to compress data for fast access and analysis. Broadly there are two types of hashing techniques: data-independent techniques which design arbitrary functions to generate hashes, and data-dependent techniques that leverage pairwise similarity in the training data (Chi and Zhu, 2017). Locality-sensitive hashing (lsh: Indyk and Motwani (1998)) is a widelyknown data-independent hashing method that uses randomised projections to generate hashcodes. It preserves data"
I17-1075,N15-1153,1,0.917785,"Missing"
K17-1022,W13-0102,0,0.679191,"network relations (Wang and Blei, 2011). This has led to a wealth of topic models of different types, and the need for methods to evaluate different styles of topic model over the same document collections. Test data perplexity is the obvious solution, but it has been shown to correlate poorly with direct human assessment of topic model quality (Chang et al., 2009), motivating the need for automatic topic model evaluation methods which emulate human assessment. Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al., 2014). We challenge this assumption, and demonstrate that topic model evaluation should operate at both the topic and document levels. Our primary contributions are as follows: (1) we empirically demonstrate that there can be large discrepancies between topic- and document-level topic model evaluation; (2) we demonstrate that previ"
K17-1022,D11-1024,0,0.355379,"Missing"
K17-1022,N10-1012,1,0.918028,"t basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of documentlevel topic allocations, and provide empirical evidence for its robustness. 1 Introduction Topic models such as latent Dirichlet allocation (Blei et al., 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al., 2010a; Chaney and Blei, 2012; Smith et al., 2017). One property of LDA-style topic models that has contributed to their popularity is that they are highly configurable, and can be structured to capture a myriad of statistical dependencies, such as between topics (Blei and Lafferty, 2006), between documents associated with the same individual (Rosen-Zvi et al., 2004), or between documents associated with individuals in different network relations (Wang and Blei, 2011). This has led to a wealth of topic models of different types, and the need for methods to evaluate different styles of topic model o"
K17-1022,N16-1057,1,0.647912,"cs. Topics highlighted in pink (yellow) are those incorrectly selected by the system (humans) as intruder topics. 8 annotators and the automated method in intruder topic selection. To further understand how the topics relate to the documents in different topic models, we present documents with the corresponding topics for different topic models in Table 8. In the human annotation task, we use the top-10 most probable words to represent a topic. We use 10 words as it is the standard approach to visualising topics, but this is an important hyper-parameter which needs to be investigated further (Lau and Baldwin, 2016), which we leave to future work. Conclusion We demonstrate empirically that there can be large discrepancies between topic coherence and document–topic associations. By way of designing an artificial topic model, we showed that a topic model can simultaneously produce topics that are coherent but be largely undescriptive of the document collection. We propose a method to automatically predict document-level topic quality and found encouraging correlation with manual evaluation, suggesting that it can be used as an alternative approach for extrinsic topic model evaluation. 213 Document lda Topi"
K17-1022,E14-1056,1,0.963902,"lei, 2011). This has led to a wealth of topic models of different types, and the need for methods to evaluate different styles of topic model over the same document collections. Test data perplexity is the obvious solution, but it has been shown to correlate poorly with direct human assessment of topic model quality (Chang et al., 2009), motivating the need for automatic topic model evaluation methods which emulate human assessment. Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al., 2014). We challenge this assumption, and demonstrate that topic model evaluation should operate at both the topic and document levels. Our primary contributions are as follows: (1) we empirically demonstrate that there can be large discrepancies between topic- and document-level topic model evaluation; (2) we demonstrate that previously-proposed doc"
K17-1022,P14-5010,0,0.0056709,"Missing"
K17-1022,Q17-1001,0,0.017275,"ysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of documentlevel topic allocations, and provide empirical evidence for its robustness. 1 Introduction Topic models such as latent Dirichlet allocation (Blei et al., 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al., 2010a; Chaney and Blei, 2012; Smith et al., 2017). One property of LDA-style topic models that has contributed to their popularity is that they are highly configurable, and can be structured to capture a myriad of statistical dependencies, such as between topics (Blei and Lafferty, 2006), between documents associated with the same individual (Rosen-Zvi et al., 2004), or between documents associated with individuals in different network relations (Wang and Blei, 2011). This has led to a wealth of topic models of different types, and the need for methods to evaluate different styles of topic model over the same document collections. Test data"
N10-1012,N09-1003,0,0.207692,"Missing"
N10-1012,E09-1013,0,0.0054854,"oduction There has traditionally been strong interest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each This research seeks to fill the gap between topic evaluation in computational linguistics and machine learning, in developing techniques to perform intrinsic qualitative evaluation of learned topics. That is, we develop methods for evaluating the quality of a given topic, in terms of its coherence to a human. After learning topics from a collection of news articles and a collection of books, we ask humans to decide whether individual learned t"
N10-1012,N09-1067,0,0.0165068,"Missing"
N10-1012,N09-1041,0,0.0913028,"rest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each This research seeks to fill the gap between topic evaluation in computational linguistics and machine learning, in developing techniques to perform intrinsic qualitative evaluation of learned topics. That is, we develop methods for evaluating the quality of a given topic, in terms of its coherence to a human. After learning topics from a collection of news articles and a collection of books, we ask humans to decide whether individual learned topics are coherent, in terms of their interpretability an"
N10-1012,O97-1002,0,0.036404,"Missing"
N10-1012,J98-1006,0,0.00953055,"is the better combination method. 102 We also experimented with the median, and trialled filtering the set of senses in a variety of ways, e.g. using only the first sense (the sense with the highest prior) for a given word, or using only the word senses associated with the POS with the highest prior. In all cases, the overall trend was for the correlation with the human scores to drop relative to the mean, so we only present the numbers for the mean in this paper. count of nodes includes the beginning and ending word nodes. Leacock-Chodorow (LC H) The measure of semantic similarity devised by Leacock et al. (1998) finds the shortest path between two WordNet synsets (sp(c1 , c2 )) using hypernym and synonym relationships. This path length is then scaled by the maximum depth of WordNet (D), and the log likelihood taken: simlch (c1 , c2 ) = − log sp(c1 , c2 ) 2·D Wu-Palmer (W U P) Wu and Palmer (1994) proposed to scale the depth of the two synset nodes (depthc1 and depthc2 ) by the depth of their LCS (depth(lcsc1 ,c2 )): simwup (c1 , c2 ) = 2 · depth(lcsc1 ,c2 ) depthc1 + depthc2 + 2 · depth(lcsc1 ,c2 ) The scaling means that specific terms (deeper in the hierarchy) that are close together are more semant"
N10-1012,P98-2127,0,0.0272583,"Missing"
N10-1012,W04-1013,0,0.0367688,"me possibly counter-intuitive results, where in some cases humans preferred models with higher perplexity. This type of result shows the need for further exploring measures other than 101 perplexity for evaluating topic models. In earlier work, we carried out preliminary experimentation using pointwise mutual information and Google results to evaluate topic coherence over the same set of topics as used in this research (Newman et al., 2009). Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al., 2002) and automatic summarisation (Lin, 2004). Here, the development of automated methods with high correlation with human subjects has opened the door to large-scale automated evaluation of system outputs, revolutionising the respective fields. While our aspirations are more modest, the basic aim is the same: to develop a fully-automated method for evaluating a well-grounded task, which achieves near-human correlation. 3 Topic Modelling In order to evaluate topic modelling, we require a topic model and set of topics for a given document collection. While the evaluation methodology we describe generalises to any method which generates se"
N10-1012,W08-2106,0,0.0201307,"ile statistical evaluation of topic models is reasonably well understood, there has been much less work on evaluating the intrinsic semantic quality of topics learned by topic models, which could have a far greater impact on the overall value of topic modeling for end-user applications. Some researchers have started to address this problem, including Mei et al. (2007) who presented approaches for automatic labeling of topics (which is core to the question of coherence and semantic interpretability), and Griffiths and Steyvers (2006) who applied topic models to word sense discrimination tasks. Misra et al. (2008) used topic modelling to identify semantically incoherent documents within a document collection (vs. coherent topics, as targeted in this research). Chang et al. (2009) presented the first human-evaluation of topic models by creating a task where humans were asked to identify which word in a list of five topic words had been randomly switched with a word from another topic. This work showed some possibly counter-intuitive results, where in some cases humans preferred models with higher perplexity. This type of result shows the need for further exploring measures other than 101 perplexity for"
N10-1012,P02-1040,0,0.105865,"Missing"
N10-1012,J98-1004,0,0.0199885,"Missing"
N10-1012,P08-2068,0,0.0128048,"pture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each This research seeks to fill the gap between topic evaluation in computational linguistics and machine learning, in developing techniques to perform intrinsic qualitative evaluation of learned topics. That is, we develop methods for evaluating the quality of a given topic, in terms of its coherence to a human. After learning topics from a collection of news articles and a collection of books, we ask humans to decide whether individual learned topics are coherent, in terms of their interpretability and association with a single over-arching semantic concept. We then propose models to p"
N10-1012,widdows-ferraro-2008-semantic,0,0.0587849,"inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best. 1 Introduction There has traditionally been strong interest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each This research seeks to fill the gap between topic evaluation in computational linguistics and machine learning, in developing techniques to perform intrinsic qualitative evaluation of learned topics. That is, we develop me"
N10-1012,P94-1019,0,\N,Missing
N10-1012,J06-1003,0,\N,Missing
N10-1012,C98-2122,0,\N,Missing
N16-1057,W13-0102,0,0.759356,"clustering, in which “topics” (multinomial distributions over terms) and topic allocations (multinomial distributions over topics per document) are jointly learned. When the topic model output is to be presented 1 https://github.com/jhlau/ topic-coherence-sensitivity Since then, several methodologies have been introduced to automate the evaluation of topic coherence. Newman et al. (2010) found that aggregate pairwise PMI scores over the top-N topic words correlated well with human ratings. Mimno et al. (2011) proposed replacing PMI with conditional probability based on co-document frequency. Aletras and Stevenson (2013) showed that coherence can be measured by a classical distributional similarity approach. More recently, Lau et al. (2014) proposed a methodology to automate the word intrusion task directly. Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of topic words, or its cardinality. These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009), N = 5, whereas for Newman et al. (2010), Aletras and Stevenson (2013) and Lau"
N16-1057,E14-1056,1,0.633524,"r document) are jointly learned. When the topic model output is to be presented 1 https://github.com/jhlau/ topic-coherence-sensitivity Since then, several methodologies have been introduced to automate the evaluation of topic coherence. Newman et al. (2010) found that aggregate pairwise PMI scores over the top-N topic words correlated well with human ratings. Mimno et al. (2011) proposed replacing PMI with conditional probability based on co-document frequency. Aletras and Stevenson (2013) showed that coherence can be measured by a classical distributional similarity approach. More recently, Lau et al. (2014) proposed a methodology to automate the word intrusion task directly. Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of topic words, or its cardinality. These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009), N = 5, whereas for Newman et al. (2010), Aletras and Stevenson (2013) and Lau et al. (2014), N = 10. 483 Proceedings of NAACL-HLT 2016, pages 483–487, c San Diego, California, June 12-17, 2016. 2016 A"
N16-1057,D11-1024,0,0.5959,"lity.1 1 Introduction Latent Dirichlet Allocation (“LDA”: Blei et al. (2003)) is an approach to document clustering, in which “topics” (multinomial distributions over terms) and topic allocations (multinomial distributions over topics per document) are jointly learned. When the topic model output is to be presented 1 https://github.com/jhlau/ topic-coherence-sensitivity Since then, several methodologies have been introduced to automate the evaluation of topic coherence. Newman et al. (2010) found that aggregate pairwise PMI scores over the top-N topic words correlated well with human ratings. Mimno et al. (2011) proposed replacing PMI with conditional probability based on co-document frequency. Aletras and Stevenson (2013) showed that coherence can be measured by a classical distributional similarity approach. More recently, Lau et al. (2014) proposed a methodology to automate the word intrusion task directly. Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of topic words, or its cardinality. These methodologies evaluate coherence over the top-N topic words, where N is selected arb"
N16-1057,N10-1012,1,0.782452,"substantially more stable and robust evaluation. We release the code and the datasets used in this research, for reproducibility.1 1 Introduction Latent Dirichlet Allocation (“LDA”: Blei et al. (2003)) is an approach to document clustering, in which “topics” (multinomial distributions over terms) and topic allocations (multinomial distributions over topics per document) are jointly learned. When the topic model output is to be presented 1 https://github.com/jhlau/ topic-coherence-sensitivity Since then, several methodologies have been introduced to automate the evaluation of topic coherence. Newman et al. (2010) found that aggregate pairwise PMI scores over the top-N topic words correlated well with human ratings. Mimno et al. (2011) proposed replacing PMI with conditional probability based on co-document frequency. Aletras and Stevenson (2013) showed that coherence can be measured by a classical distributional similarity approach. More recently, Lau et al. (2014) proposed a methodology to automate the word intrusion task directly. Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of"
N19-1163,W14-4012,0,0.0182807,"Missing"
N19-1163,C18-1284,0,0.077695,"each sequential posts to represent the hypothesis. Ruchansky et al. (2017) integrate textual, user response, and source information into their neural models and achieve better performance. Most of these works focus on detection accuracy, and so largely ignore the timing of the detection. Ma et al. (2015) develop a dynamic time 1615 series structure to incorporate temporal information to the features to understand the whole life cycle of rumours. Zhao et al. (2015) propose a detection model using a set of regular expressions to find posts that question or rebut the rumour to detect it earlier. Dungs et al. (2018) present an approach that checks for a rumour after 5 or 10 retweets. These models are interested in early rumour detection, although the checkpoint for triggering a detection is pre-determined, and succeeding posts after the checkpoint are usually ignored. On a similar note but a different task, Farajtabar et al. (2017) experiment with reinforcement learning by combining it with a point process network activity model to detect fake news and found some success. where K is the number of words in the post. Henceforth W in all equations are model parameters. To capture the temporal relationship b"
N19-1163,D14-1181,0,0.00347468,"classification: p = softmax(Wp hN + bp ) (2) where p 2 R2 , i.e. p0 (p1 ) gives the probability of the positive (negative) class.3 3.2 Rumor Detection Module (RDM) RDM contains three layers: a word embedding layer that maps input words into vectors, a maxpooling layer that extracts important features of a post, and a GRU (Cho et al., 2014) that processes the sequential posts of an event. In the word embedding layer, we map words in post xi into vectors, yielding vectors eji for each word. To capture the most salient features of a post, we apply a max pooling operation (Collobert et al., 2011; Kim, 2014; Lau et al., 2017), producing a fixed size vector mi : 1) Checkpoint Module (CM) Q⇤ (s, a) = Es0 "" [r + max Qi (s0 , a0 )|s, a] 0 a where r is the reward value, the discount rate, and the optimal action in all action sequence a0 is selected to maximise the expected value of r + Q⇤ (s0 , a0 ). The optimal action-value function obeys the Bellman equation and is used for iterative value update: Qi+1 (s, a) = E[r + max Qi (s0 , a0 )|s, a] 0 a The above iterative algorithm will converge and reach the optimal action value function, i.e. Qi ! Q⇤ when q ! 1 (Sutton et al., 1998). 3 Although sigmoid a"
N19-1163,P17-1033,1,0.839094,"ion: p = softmax(Wp hN + bp ) (2) where p 2 R2 , i.e. p0 (p1 ) gives the probability of the positive (negative) class.3 3.2 Rumor Detection Module (RDM) RDM contains three layers: a word embedding layer that maps input words into vectors, a maxpooling layer that extracts important features of a post, and a GRU (Cho et al., 2014) that processes the sequential posts of an event. In the word embedding layer, we map words in post xi into vectors, yielding vectors eji for each word. To capture the most salient features of a post, we apply a max pooling operation (Collobert et al., 2011; Kim, 2014; Lau et al., 2017), producing a fixed size vector mi : 1) Checkpoint Module (CM) Q⇤ (s, a) = Es0 "" [r + max Qi (s0 , a0 )|s, a] 0 a where r is the reward value, the discount rate, and the optimal action in all action sequence a0 is selected to maximise the expected value of r + Q⇤ (s0 , a0 ). The optimal action-value function obeys the Bellman equation and is used for iterative value update: Qi+1 (s, a) = E[r + max Qi (s0 , a0 )|s, a] 0 a The above iterative algorithm will converge and reach the optimal action value function, i.e. Qi ! Q⇤ when q ! 1 (Sutton et al., 1998). 3 Although sigmoid activation is more a"
N19-1163,I17-2043,0,0.0707369,"er 7) question the veracity of the original message. Had the rumour been identified timely and rebutted, its propagation could have been contained. Most studies (Qazvinian et al., 2011; Zhang et al., 2015) consider rumour detection as a binary classification problem, where they extract various 1614 Proceedings of NAACL-HLT 2019, pages 1614–1623 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics features to capture rumour indicative signals for detecting a rumour, and a few recent works explore deep learning approaches to enhance detection accuracy (Long et al., 2017; Ruchansky et al., 2017). In all these studies, however, the timeliness of the rumour detection is not evaluated. There are a few exceptions. In Ma et al. (2015) and Kwon et al. (2017), the authors define a checkpoint (e.g. number of posts or time elapsed after the source message) in the timeline and use all the posts prior to this checkpoint to classify a rumour. The checkpoint is often a pre-determined value for all rumours, and so does not capture the variation of propagation patterns for different rumours. The focus of our paper is on early rumour detection. That is, our aim is to identif"
N19-1163,P17-1066,0,0.309144,"on developing handcrafted features for machine learning algorithms (Qazvinian et al., 2011). Takahashi and Igata (2012) propose a method for rumour detection on Twitter using cue words and tweets statistics. Yang et al. (2012) apply two new types of features — client-based and location-based features — to rumour detection on Sina Weibo. Beyond this, user-based (Liang et al., 2015) and topic-based (Yang et al., 2015) features have also been explored. Friggeri et al. (2014) demonstrate that there are structural differences in the propagation of rumours and non-rumours, and Wu et al. (2015) and Ma et al. (2017) experiment with using these propagation patterns extensively to improve detection. More recently, deep learning models are explored for the task. Compared to traditional machine learning approaches, these deep learning models tend to rely less on sophisticated handcrafted features. Ma et al. (2016) introduce a rumour detection model for microblogs based on recurrent networks. The input to their model is simple tf-idf features but it outperforms models leveraging handcrafted features. Sampson et al. (2016) show that implicit linkages between conversation fragments improve detection accuracy. L"
N19-1163,D14-1162,0,0.0822425,"h and 200K iterations respectively. We can see that loss declines steadily after 20K iterations and converges 5 Results For T WITTER, words are tokenised using white spaces, and stopword list is based on NLTK (Bird et al., 2009). For W EIBO, Jieba is used for tokenisation: https://pypi. org/project/jieba/; and stopword list is a customised list based on: http://blog.sina.com.cn/s/blog_ a19ab3770102wjav.html. 6 For W EIBO, the embeddings are pre-trained using word2vec (Mikolov et al., 2013) on a separate Weibo data set we collected. For T WITTER, the embeddings are pretrained GloVe embeddings (Pennington et al., 2014). Unknown words are initialised as zero vectors. Bucketing Strategy Recall that we explore 3 different methods to group posts in order to process them in batches (Section 3.4). Here we evaluate them on rumour classification accuracy over the validation set of T WITTER. Note that we do not use CM here (and hence no reinforcement learning is involved) — we simply use all posts of an event to perform rumour classification with RDM. In terms of metrics we use standard accuracy, precision, recall and F1 scores. Results are presented in Table 2. We see FN produces the best performance, and so FN is"
N19-1163,D11-1147,0,0.524254,"ource message started a claim about the cause of Michael Brown’s shooting, and it was published shortly after the shooting happened. It claimed that he was shot ten times by the police for stealing candy. The message was retweeted by multiple users on T WITTER, and within 24 hours there were about 900K users involved, either by reposting, commenting, or questioning the original source message. From Figure 1, we see some users (e.g. User 7) question the veracity of the original message. Had the rumour been identified timely and rebutted, its propagation could have been contained. Most studies (Qazvinian et al., 2011; Zhang et al., 2015) consider rumour detection as a binary classification problem, where they extract various 1614 Proceedings of NAACL-HLT 2019, pages 1614–1623 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics features to capture rumour indicative signals for detecting a rumour, and a few recent works explore deep learning approaches to enhance detection accuracy (Long et al., 2017; Ruchansky et al., 2017). In all these studies, however, the timeliness of the rumour detection is not evaluated. There are a few exceptions. In Ma et al. (2015) and"
P11-1154,E09-1013,0,0.0186437,"fed into a supervised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method. 1 Introduction Topic modelling is an increasingly popular framework for simultaneously soft-clustering terms and documents into a fixed number of “topics”, which take the form of a multinomial distribution over terms in the document collection (Blei et al., 2003). It has been demonstrated to be highly effective in a wide range of tasks, including multidocument summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). One standard way of interpreting a topic is to use the marginal probabilities p(wi |tj ) associated with each term wi in a given topic tj to extract out the 10 terms with highest marginal probability. This results in term lists such as:1 stock market investor fund trading investment firm exchange companies share 1 Here and throughout the paper, we will represent a topic tj via its ranking of top-10 topic terms, based on p(wi |tj ). which are clearly associat"
P11-1154,N10-1125,0,0.0206364,"than a benchmark method. 1 Introduction Topic modelling is an increasingly popular framework for simultaneously soft-clustering terms and documents into a fixed number of “topics”, which take the form of a multinomial distribution over terms in the document collection (Blei et al., 2003). It has been demonstrated to be highly effective in a wide range of tasks, including multidocument summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). One standard way of interpreting a topic is to use the marginal probabilities p(wi |tj ) associated with each term wi in a given topic tj to extract out the 10 terms with highest marginal probability. This results in term lists such as:1 stock market investor fund trading investment firm exchange companies share 1 Here and throughout the paper, we will represent a topic tj via its ranking of top-10 topic terms, based on p(wi |tj ). which are clearly associated with the domain of stock market trading. The aim of this research is to automatically generate topic labels which explicitly identify"
P11-1154,N09-1041,0,0.102661,"ion of association measures and lexical features, optionally fed into a supervised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method. 1 Introduction Topic modelling is an increasingly popular framework for simultaneously soft-clustering terms and documents into a fixed number of “topics”, which take the form of a multinomial distribution over terms in the document collection (Blei et al., 2003). It has been demonstrated to be highly effective in a wide range of tasks, including multidocument summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). One standard way of interpreting a topic is to use the marginal probabilities p(wi |tj ) associated with each term wi in a given topic tj to extract out the 10 terms with highest marginal probability. This results in term lists such as:1 stock market investor fund trading investment firm exchange companies share 1 Here and throughout the paper, we will represent a topic tj via its ranking of top-10 topic te"
P11-1154,C10-2069,1,0.620712,"Missing"
P11-1154,N10-1012,1,0.447059,"ng et al. (2009) were one of the first to propose human labelling of topic models, in the form of synthetic intruder word and topic detection tasks. In the intruder word task, they include a term w with low marginal probability p(w|t) for topic t into the topN topic terms, and evaluate how well both humans and their model are able to detect the intruder. The potential applications for automatic labelling of topics are many and varied. In document collection visualisation, e.g., the topic model can be used as the basis for generating a two-dimensional representation of the document collection (Newman et al., 2010a). Regions where documents have a high marginal probability p(di |tj ) of being associated with a given topic can be explicitly labelled with the learned label, rather than just presented as an unlabelled region, or presented with a dense “term cloud” from the original topic. In topic modelbased selectional preference learning (Ritter et al., ` S´eaghdha, 2010), the learned topics can 2010; O be translated into semantic class labels (e.g. DAYS OF THE WEEK ), and argument positions for individual predicates can be annotated with those labels for greater interpretability/portability. In dynamic"
P11-1154,P10-1045,0,0.0159018,"Missing"
P11-1154,N04-1041,0,0.00580004,"We reimplement their method and present an empirical comparison in Section 5.3. In other work, Magatti et al. (2009) proposed a method for labelling topics induced by a hierarchical topic model. Their label candidate set is the Google Directory (gDir) hierarchy, and label selection takes the form of ontological alignment with gDir. The experiments presented in the paper are highly preliminary, although the results certainly show promise. However, the method is only applicable to a hierarchical topic model and crucially relies on a pre-existing ontology and the class labels contained therein. Pantel and Ravichandran (2004) addressed the more specific task of labelling a semantic class by applying Hearst-style lexico-semantic patterns to each member of that class. When presented with semantically homogeneous, fine-grained nearsynonym clusters, the method appears to work well. With topic modelling, however, the top-ranking topic terms tended to be associated and not lexically similar to one another. It is thus highly questionable whether their method could be applied to topic models, but it would certainly be interesting to investigate whether our model could conversely be applied to the labelling of sets of near"
P11-1154,P10-1044,0,0.0135116,"Missing"
P11-1154,D08-1027,0,0.136267,"Missing"
P14-1025,P13-1141,0,0.0173254,"ve of context, we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,W04-3204,0,0.195409,"Missing"
P14-1025,S07-1002,0,0.0143677,"nto a multinomial distribution over words, based on simple maximum likelihood estimation.6 We then calculate the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in"
P14-1025,P06-1012,0,0.0155071,"Missing"
P14-1025,E09-1005,0,0.0121583,"he flexibility and robustness of our methodology. Future work could pursue a more sophisticated methodology, using non-linear combinations of sim(si , tj ) for computing the affinity measures or multiple features in a supervised context. We contend, however, that these extensions are ultimately a preliminary demonstration to the flexibility and robustness of our methodology. A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. where f (tj ) is the frequency of topic tj in the corpus. The intuition behind novelty is that a target lemma with a novel sense should have a (somewhat-)frequent topic that has low association with any sense. That we use the frequency rather than the probability of the topic here is deliberate, as topics with a higher raw number of occurrences (whether as a low-probability topic for a high-frequency word, or a high-probability topic for a low-frequency word) are indicative of a novel word sense. For each of our t"
P14-1025,cook-stevenson-2010-automatically,1,0.840918,"Missing"
P14-1025,I13-1041,1,0.812251,"one dataset), but HDP-WSI is better at inducing the overall sense distribution. It is important to bear in mind that MKWC in these experiments makes use of full-text parsing in calculating the distributional similarity thesaurus, and the WordNet graph structure in calculating the similarity between associated words and different senses. Our method, on the other hand, uses no parsing, and only the synset definitions (and not the graph structure) of WordNet.8 The non-reliance on parsing is significant in terms of portability to text sources which are less amenable to parsing (such as Twitter: (Baldwin et al., 2013)), and the non-reliance on the graph structure of WordNet is significant in terms of portability to conventional “flat” sense inventories. While comparable results on a different dataset have been achieved with a proximity thesaurus (McCarthy et al., 2007) compared to a dependency one,9 it is not stated how both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies. Given that both systems compute a continuousvalued prevalence score for each sense of a target lemma, a distribution of senses can be obtained"
P14-1025,N06-1017,0,0.0586004,"A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability dist"
P14-1025,S07-1060,0,0.466214,"Missing"
P14-1025,D07-1109,0,0.017322,"tems, The University of Melbourne ♦ University of Cambridge jeyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable se"
P14-1025,E14-4042,1,0.869137,"Missing"
P14-1025,E09-1013,0,0.0180149,"let and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata a"
P14-1025,W11-2508,0,0.125836,"Missing"
P14-1025,D07-1108,0,0.0240323,"k@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition"
P14-1025,S13-2039,1,0.835591,"we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,I08-1073,1,0.825054,"d in Table 3. HDP-WSI consistently achieves lower JS divergence, indicating that the distribution of senses that it finds is closer to the gold standard distribution. Testing for statistical significance over the paired JS divergence values for each lemma using the Wilcoxon signed-rank test, the result for FINANCE is significant (p &lt; 0.05) but the results for the other two datasets are not (p &gt; 0.1 in each case). 8 McCarthy et al. (2004b) obtained good results with definition overlap, but their implementation uses the relation structure alongside the definitions (Banerjee and Pedersen, 2002). Iida et al. (2008) demonstrate that further extensions using distributional data are required when applying the method to resources without hierarchical relations. 9 The thesauri used in the reimplementation of MKWC in this paper were obtained from http://webdocs.cs. ualberta.ca/˜lindek/downloads.htm. 263 wide a window is needed for the proximity thesaurus. This could be a significant issue with Twitter data, where context tends to be limited. In the next section, we demonstrate the robustness of the method in experimenting with two new datasets, based on Twitter and a web corpus, and the Macmillan English Dict"
P14-1025,O97-1002,0,0.0490006,"n for Computational Linguistics and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. tively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date ("
P14-1025,P10-1116,0,0.0142022,"lbourne ♦ University of Cambridge jeyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further"
P14-1025,N09-2059,1,0.960189,"senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). Background and Related Work There has been a considerable amount of research on representing word senses and disambiguat"
P14-1025,P98-2127,0,0.0433121,"ing predominant senses 259 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259–270, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. tively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automa"
P14-1025,P12-3005,1,0.65009,"ary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of Twitter (from a crawl over the time period Jan 3–Feb 28, 2013 using the Twitter Streaming API) and ukWaC, after language identification using langid.py (Lui and Baldwin, 2012) and POS tagging (based on the CMU ARK Twitter POS tagger v2.0 (Owoputi et al., 2012) for Twitter, and the POS tags provided with the corpus for ukWaC). Amazon Mechanical Turk (AMT) was then used to 5-way sense-tag each usage relative to Macmillan, including allowing the annotators the option to label a usage as “Other” in instances where the usage was not captured by any of the Macmillan senses. After quality control over the annotators/annotations (see 5.1 Learning Sense Distributions As in Section 4, we evaluate in terms of WSD accuracy (Table 4) and JS divergence over the gold-standard sen"
P14-1025,S13-2049,0,0.0136682,"the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution over words, and per-usage topic assignments (= probabilistic sense assignm"
P14-1025,S10-1011,0,0.00982955,"ds, based on simple maximum likelihood estimation.6 We then calculate the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution"
P14-1025,C04-1177,1,0.882843,". Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses — i.e., senses that are in the sense inventory but not attested in a given corpus. In contrast to the previous work of McCarthy et al. (2004a) on this topic which uses the sense ranking score from McCarthy et al. (2004b) to remove low-frequency senses from WordNet, we focus on finding senses that are unattested in the corpus on the premise that, given accurate disambiguation, rare senses in a corpus contribute to correct interpretation. Corpus instances of a word can also correspond to senses that are not present in a given sense inventory. This can be due to, for example, words taking on new meanings over time (e.g. the relaUnsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to the"
P14-1025,H05-1053,1,0.623299,"r of usages assigned to topic tj ), and T is the number of topics. The intuition behind the approach is that the predominant sense should be the sense that has relatively high similarity (in terms of lexical overlap) with high-probability topic(s). 4 WordNet annotations. The authors evaluated their method in terms of WSD accuracy over a given corpus, based on assigning all instances of a target word with the predominant sense learned from that corpus. For the remainder of the paper, we denote their system as MKWC. To compare our system (HDP-WSI) with MKWC, we apply it to the three datasets of Koeling et al. (2005). For each dataset, we use HDP to induce topics for each target lemma, compute the similarity between the topics and the WordNet senses (Equation (1)), and rank the senses based on the prevalence scores (Equation (2)). In addition to the WSD accuracy based on the predominant sense inferred from a particular corpus, we additionally compute: (1) AccUB , the upper bound for the first sense-based WSD accuracy (using the gold standard predominant sense for disambiguation);7 and (2) ERR, the error rate reduction between the accuracy for a given system (Acc) and the upper bound (AccUB ), calculated a"
P14-1025,P04-1036,1,0.84242,". Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses — i.e., senses that are in the sense inventory but not attested in a given corpus. In contrast to the previous work of McCarthy et al. (2004a) on this topic which uses the sense ranking score from McCarthy et al. (2004b) to remove low-frequency senses from WordNet, we focus on finding senses that are unattested in the corpus on the premise that, given accurate disambiguation, rare senses in a corpus contribute to correct interpretation. Corpus instances of a word can also correspond to senses that are not present in a given sense inventory. This can be due to, for example, words taking on new meanings over time (e.g. the relaUnsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to the"
P14-1025,J04-1003,0,0.0384531,"ta, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usages of words in context (WSD) as, in order to produce computational systems that understand and produce natural language, it is essential to"
P14-1025,J07-4005,1,0.874451,"e learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren’t attested in the corpus, and identifying novel senses in the corpus which aren’t captured in the sense inventory. 1 Introduction The automatic determination of word sense information has been a long-term pursuit of the NLP community (Agirre and Edmonds, 2006; Navigli, 2009). Word sense distributions tend to be Zipfian, and as such, a simple but surprisingly highaccuracy back-off heuristic for word sense disambiguation (WSD) is to tag each instance of a given word with its predominant sense (McCarthy et al., 2007). Such an approach requires knowledge of predominant senses; however, word sense distributions — and predominant senses too — vary from corpus to corpus. Therefore, methods for automatically learning predominant senses 259 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259–270, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy e"
P14-1025,E12-1060,1,0.881316,"ity of Cambridge jeyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method f"
P14-1025,H93-1061,0,0.648686,"Missing"
P14-1025,S13-2051,1,0.840589,"we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,E06-1016,0,0.357917,"Missing"
P14-1025,S13-2035,0,0.0261491,"we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,W11-1102,0,0.0307236,"Missing"
P14-1025,P10-4014,0,0.10309,"Missing"
P14-1025,S07-1006,0,0.0131677,"(Gella et al., to appear) based on text from ukWaC (Ferraresi et al., 2008) and Twitter, and annotated using the Macmillan English Dictionary10 (henceforth “Macmillan”). For the purposes of this research, the choice of Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of Twitter (from a crawl ov"
P14-1025,W04-2807,0,0.0704841,"move to a new dataset (Gella et al., to appear) based on text from ukWaC (Ferraresi et al., 2008) and Twitter, and annotated using the Macmillan English Dictionary10 (henceforth “Macmillan”). For the purposes of this research, the choice of Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of T"
P14-1025,N13-1079,0,0.0120313,"eyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distri"
P14-1025,W09-0214,0,0.0845474,"Missing"
P14-1025,S07-1053,0,\N,Missing
P14-1025,N06-2015,0,\N,Missing
P14-1025,C98-2122,0,\N,Missing
P15-1156,P03-1054,0,0.0369933,"1 RNNLM 0.44 0.46 0.55 0.33 0.57 Table 3: Pearson’s r of acceptability measure and mean sentence rating for all experimented models in ENWIKI. Boldface indicates the best performing measure. on the original RNNLM with 600 neurons in the hidden layer, trained on BNC-100M (Table 2 column: “RNNLM”).12 We see that RNNLM performs very well. It outperforms the other models, achieving a correlation of 0.53. 3.7 PCFG Parser (Supervised) Although we are interested in unsupervised models, for purposes of comparison we experimented with a constituent PCFG parser for our task. We use the Stanford Parser (Klein and Manning, 2003a; Klein and Manning, 2003b), and tested both the unlexicalised and lexicalised PCFG parser with the supplied model. To compute the log probability of test sentences, we experimented with both top-1 and top-1K best parses. We found that the unlexicalised variant gives better performance, but we saw little difference between using the top-1 and the top-1K best parses for computing log probability. In Table 2 (column: “PCFG”), we report results for the unlexicalised variant based on the top-1 best parse. The supervised PCFG parser performs poorly. This is not surprising, given that the parser is"
P15-1156,D14-1082,0,0.0220048,"n(wxy ) + αP0 (wxy ) ; n+α n(wx ) + αP0 (wx ) n(wy ) + αP0 (wy ) P (H1 |H − ) = × n+α n+1+α In recent years, we have seen a resurgence in the use of neural networks for deep machine learning and NLP. Rather than designing structures or handcrafting features that seem intuitive for a task, deep learning introduces an entirely general architecture for machine learning. It has yielded some impressive results for NLP tasks: automatic speech recognition, parsing, part of speech tagging, and named entity recognition, to name a few (Seide et al., 2011; Mikolov et al., 2011a; Collobert et al., 2011; Chen and Manning, 2014). We experiment with a recurrent neural network language model (RNNLM: (Elman, 1998; Mikolov, 2012)) for our task. We choose this model because it has an internal state that keeps track of previously observed sequences, which is well suited for natural language problems. To train the model, we use stochastic gradient descent combined with back propagation through time. RNNLM is optimised to reduce the error in predicting the following word, based on the current word and its history (represented in a compressed dimension in the size of the hidden layer). Full details of RNNLM can be found in th"
P15-1156,W13-2604,1,0.876413,"measures for predicting the acceptability of a sentence. Notations: SLOR is the syntactic log-odds ratio, introduced by Pauls and Klein (2012); ξ is the sentence (|ξ |is the sentence length); Pm (ξ) is the probability of the sentence given by the model; Pu (ξ) is the unigram probability of the sentence. Note that the negative sign in Norm LP (Div) is given to reverse the sign change introduced by the division of log unigram probabilities. Unsupervised Models Lexical N -gram Model Lexical N -gram models were variously explored in tasks related to acceptability estimation (Heilman et al., 2014; Clark et al., 2013; Pauls and Klein, 2012). We use an N -gram model with Kneser-Ney interpolation (Goodman, 2001), and we train bigram, trigram, and 4-gram models on BNC-100M. The trained models are then used to compute the acceptability measures of the test sentences. The results are detailed in Table 2 (columns: “2-gram”, “3-gram” and “4-gram”).4 In general across all models, the Norm LP (Div) and SLOR measures consistently produce the best correlations. We see a significant improvement when the context window is increased from 2-gram to 3-gram, but not so from 3-gram to 4-gram (2-gram best: 0.34; 3-gram best"
P15-1156,P07-1094,0,0.0144041,"do not present model perplexity values in the results, as we did not find any correlation between perplexity and task performance. of the lexical N -gram model does not correspond to a better representation of grammatical structure (insofar as the size of the dataset is fixed), and a more sophisticated model is necessary. 3.2 Bayesian HMM Seeing that local context is insufficient for predicting acceptability, we explore various Bayesian models that incorporate richer latent structures. We chose a Bayesian implementation because its “rich gets richer” dynamics tends to work well for languages (Goldwater and Griffiths, 2007; Goldwater et al., 2009; Newman et al., 2012; Lau et al., 2012). Lexical N -grams model the generation of a word based on its preceding words. We introduce a layer of latent variables on top of the words, which can be interpreted as the word classes, and we model the transitions between the latent variables and observed words using Markov processes. In this model we first generate a (latent) word class based on its preceding word classes, and we then generate the word based on its word class. Figure 1(b) illustrates the structure of a second order Hidden Markov model (HMM). 1620 wi−2 wi−1 wi"
P15-1156,P14-2029,0,0.504764,"able 1: Acceptability measures for predicting the acceptability of a sentence. Notations: SLOR is the syntactic log-odds ratio, introduced by Pauls and Klein (2012); ξ is the sentence (|ξ |is the sentence length); Pm (ξ) is the probability of the sentence given by the model; Pu (ξ) is the unigram probability of the sentence. Note that the negative sign in Norm LP (Div) is given to reverse the sign change introduced by the division of log unigram probabilities. Unsupervised Models Lexical N -gram Model Lexical N -gram models were variously explored in tasks related to acceptability estimation (Heilman et al., 2014; Clark et al., 2013; Pauls and Klein, 2012). We use an N -gram model with Kneser-Ney interpolation (Goodman, 2001), and we train bigram, trigram, and 4-gram models on BNC-100M. The trained models are then used to compute the acceptability measures of the test sentences. The results are detailed in Table 2 (columns: “2-gram”, “3-gram” and “4-gram”).4 In general across all models, the Norm LP (Div) and SLOR measures consistently produce the best correlations. We see a significant improvement when the context window is increased from 2-gram to 3-gram, but not so from 3-gram to 4-gram (2-gram bes"
P15-1156,E12-1060,1,0.547697,"y correlation between perplexity and task performance. of the lexical N -gram model does not correspond to a better representation of grammatical structure (insofar as the size of the dataset is fixed), and a more sophisticated model is necessary. 3.2 Bayesian HMM Seeing that local context is insufficient for predicting acceptability, we explore various Bayesian models that incorporate richer latent structures. We chose a Bayesian implementation because its “rich gets richer” dynamics tends to work well for languages (Goldwater and Griffiths, 2007; Goldwater et al., 2009; Newman et al., 2012; Lau et al., 2012). Lexical N -grams model the generation of a word based on its preceding words. We introduce a layer of latent variables on top of the words, which can be interpreted as the word classes, and we model the transitions between the latent variables and observed words using Markov processes. In this model we first generate a (latent) word class based on its preceding word classes, and we then generate the word based on its word class. Figure 1(b) illustrates the structure of a second order Hidden Markov model (HMM). 1620 wi−2 wi−1 wi (a) Lexical 3-gram wi+1 ti−2 ti−1 ti ti+1 si−2 si−1 si si+1 si−2"
P15-1156,C12-1127,1,0.386881,"as we did not find any correlation between perplexity and task performance. of the lexical N -gram model does not correspond to a better representation of grammatical structure (insofar as the size of the dataset is fixed), and a more sophisticated model is necessary. 3.2 Bayesian HMM Seeing that local context is insufficient for predicting acceptability, we explore various Bayesian models that incorporate richer latent structures. We chose a Bayesian implementation because its “rich gets richer” dynamics tends to work well for languages (Goldwater and Griffiths, 2007; Goldwater et al., 2009; Newman et al., 2012; Lau et al., 2012). Lexical N -grams model the generation of a word based on its preceding words. We introduce a layer of latent variables on top of the words, which can be interpreted as the word classes, and we model the transitions between the latent variables and observed words using Markov processes. In this model we first generate a (latent) word class based on its preceding word classes, and we then generate the word based on its word class. Figure 1(b) illustrates the structure of a second order Hidden Markov model (HMM). 1620 wi−2 wi−1 wi (a) Lexical 3-gram wi+1 ti−2 ti−1 ti ti+1 si−"
P15-1156,P12-1101,0,0.0168103,"eley Parser approach, where we replace low frequency or OOV words using the UNK signature. We capture additional surface characteristics of the original word by attaching features at the end of the signature (e.g. the OOV word 1949 would be replaced by the signature UNK-NUM).3 3 3.1 Acc. Measure Equation LogProb log Pm (ξ) Mean LP Norm LP (Div) log Pm (ξ) |ξ| log Pm (ξ) − log Pu (ξ) Norm LP (Sub) log Pm (ξ) − log Pu (ξ) SLOR log Pm (ξ) − log Pu (ξ) |ξ| Table 1: Acceptability measures for predicting the acceptability of a sentence. Notations: SLOR is the syntactic log-odds ratio, introduced by Pauls and Klein (2012); ξ is the sentence (|ξ |is the sentence length); Pm (ξ) is the probability of the sentence given by the model; Pu (ξ) is the unigram probability of the sentence. Note that the negative sign in Norm LP (Div) is given to reverse the sign change introduced by the division of log unigram probabilities. Unsupervised Models Lexical N -gram Model Lexical N -gram models were variously explored in tasks related to acceptability estimation (Heilman et al., 2014; Clark et al., 2013; Pauls and Klein, 2012). We use an N -gram model with Kneser-Ney interpolation (Goodman, 2001), and we train bigram, trigra"
P16-1143,W04-3204,0,0.117847,"Missing"
P16-1143,N15-1132,0,0.200608,"the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on how best to use the MFS heuristic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of Lau et al. (2014), due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository containing sense glosses. The task we are interested in — namely, sense distribution learning — is in principle very similar to identifying the MFS. Indeed, of these methods for identifying the MFS, some of them are 1514 explicitly described in terms of s"
P16-1143,P11-2087,0,0.0263279,"Missing"
P16-1143,S07-1060,0,0.165989,"nt sense (MFS) heuristic (McCarthy et al., 2007), which assigns each usage of a given word-type to its most frequent sense. Given the popularity of the MFS heuristic, much of the past work on unsupervised techniques has focused on identifying the most frequent sense. The original method of this kind was proposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on"
P16-1143,D07-1109,0,0.185536,"o the most frequent sense heuristic has been particularly strong when used with domain-specific data (Koeling et al., 2005; Chan and Ng, 2006; Lau et al., 2014). Introduction Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see Agirre and Edmonds (2007) and Navigli (2009)). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for performing WSD (Postma et al., 2015; Chen et al., 2014; Boyd-Graber et al., 2007; Brody et al., 2006), In addition, there is great scope to use these techniques to improve existing sense frequency resources, which are currently limited by the bottleneck of requiring manual sense annotation. The most prominent example of such a resource is W ORD N ET (Fellbaum, 1998), where the sense 1513 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1513–1524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics frequency data is based on S EM C OR (Miller et al., 1993), a 220,000 word corpus that has been ma"
P16-1143,P06-1013,0,0.127078,"heuristic has been particularly strong when used with domain-specific data (Koeling et al., 2005; Chan and Ng, 2006; Lau et al., 2014). Introduction Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see Agirre and Edmonds (2007) and Navigli (2009)). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for performing WSD (Postma et al., 2015; Chen et al., 2014; Boyd-Graber et al., 2007; Brody et al., 2006), In addition, there is great scope to use these techniques to improve existing sense frequency resources, which are currently limited by the bottleneck of requiring manual sense annotation. The most prominent example of such a resource is W ORD N ET (Fellbaum, 1998), where the sense 1513 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1513–1524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics frequency data is based on S EM C OR (Miller et al., 1993), a 220,000 word corpus that has been manually tagged with W"
P16-1143,W10-0701,0,0.0326124,"our English Wikipedia corpus. The remaining lemmas were then split into 5 groups of approximately equal size based on S EM C OR frequency. The S EM C OR frequencies contained in each group are summarised in Table 1. From each of the S EM C OR frequency groups, we randomly sampled 10 lemmas, giving 50 lemmas in total. Then for each lemma, we randomly sampled 100 usages to be annotated from English Wikipedia. This was done in the same way as the sampling of lemma usages for L EX S EM TM (see Section 6). We obtained crowdsourced sense annotations for each lemma using Amazon Mechanical Turk (AMT: Callison-Burch and Dredze (2010)). The sentences for each lemma were split into 4 batches (25 sentences per batch). In addition, two control sentences18 were created for each lemma, and added to each corresponding batch. Each batch of 27 items was annotated separately by 10 annotators. For each item to be annotated, annotators were provided with the sentence containing the lemma, the gloss for each sense as listed in W ORD N ET 3.019 and a list of hypernyms and synonyms for each sense. Annotators were asked to assign each item to exactly one sense. From these crowdsourced annotations, our gold-standard sense distributions we"
P16-1143,P06-1012,0,0.19513,", 2014), semi-automatic dictionary construction (Cook et al., 2013), lexical simplification (Biran et al., 2011), and textual entailment (Shnarch et al., 2011). Automatically acquired sense distributions themselves are also used to improve unsupervised WSD, for example by providing a most frequent sense heuristic (McCarthy et al., 2004b; Jin et al., 2009) or by improving unsupervised usage sampling strategies (Agirre and Martinez, 2004). Furthermore, the improvement due to the most frequent sense heuristic has been particularly strong when used with domain-specific data (Koeling et al., 2005; Chan and Ng, 2006; Lau et al., 2014). Introduction Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see Agirre and Edmonds (2007) and Navigli (2009)). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for performing WSD (Postma et al., 2015; Chen et al., 2014; Boyd-Graber et al., 2007; Brody et al., 2006), In addition, there is great scope to use these techniques to improve existing sense frequency"
P16-1143,C14-1035,0,0.0254343,"others implicitly learn sense distributions by calculating some kind of scores used to rank senses. The state-of-the-art technique of Lau et al. (2014) that we are building upon involves performing unsupervised word sense induction (WSI), which itself is implemented using nonparametric HDP (Teh et al., 2006) topic models, as detailed in Section 3. The WSI component, HDP-WSI, is based on the work of Lau et al. (2012), which at the time was state-of-the-art. Since then, however, other competitive WSI approaches have been developed, involving complex structures such as multi-layer topic models (Chang et al., 2014), or complex word embedding based approaches (Neelakantan et al., 2014). We have not used these approaches in this work on account of their complexity and likely computational cost, however we believe they are worth future exploration. On the other hand, because HDP-WSI is implemented using topic models, it can be customised by replacing HDP with newer, more efficient topic modelling algorithms. Recent work has produced more advanced topic modelling approaches, some of which are extensions of existing approaches using more advanced learning algorithms or expanded models (Buntine and Mishra, 20"
P16-1143,D14-1110,0,0.186671,"e improvement due to the most frequent sense heuristic has been particularly strong when used with domain-specific data (Koeling et al., 2005; Chan and Ng, 2006; Lau et al., 2014). Introduction Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see Agirre and Edmonds (2007) and Navigli (2009)). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for performing WSD (Postma et al., 2015; Chen et al., 2014; Boyd-Graber et al., 2007; Brody et al., 2006), In addition, there is great scope to use these techniques to improve existing sense frequency resources, which are currently limited by the bottleneck of requiring manual sense annotation. The most prominent example of such a resource is W ORD N ET (Fellbaum, 1998), where the sense 1513 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1513–1524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics frequency data is based on S EM C OR (Miller et al., 1993), a 220,000 wo"
P16-1143,C00-1027,0,0.202938,"Missing"
P16-1143,N13-1132,0,0.0174577,"ch lemma were split into 4 batches (25 sentences per batch). In addition, two control sentences18 were created for each lemma, and added to each corresponding batch. Each batch of 27 items was annotated separately by 10 annotators. For each item to be annotated, annotators were provided with the sentence containing the lemma, the gloss for each sense as listed in W ORD N ET 3.019 and a list of hypernyms and synonyms for each sense. Annotators were asked to assign each item to exactly one sense. From these crowdsourced annotations, our gold-standard sense distributions were created using MACE (Hovy et al., 2013), which is a generalpurpose tool for inferring item labels from multiannotator, multi-item tasks. It provides a Bayesian framework for modelling item annotations, modelling the individual biases of each annotator, and 17 We chose to restrict our scope in this evaluation to nouns because much of the prior work has also focussed on nouns, and these are the words we would expect others to care the most about disambiguating, since they are more often context bearing. Also, introducing other POS would require a greater quantity of expensive annotated data. 18 These were created manually, to be as c"
P16-1143,N09-2059,1,0.917589,"oyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on how best to use the MFS heuristic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of Lau et al. (2014), due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository containing sense glosses. The task we are interested in — namely, sense distribution learning — is in principle very similar to identifying the MFS. Indeed, of these methods for identifying the MFS, some of them are 1514 explicitly described in terms of sense distribution learning (Chan a"
P16-1143,H05-1053,1,0.832069,"ew/527 8 For simplicity we use HCA to refer to both the topic modelling algorithm implemented by Buntine and Mishra (2014) as well as the corresponding software suite, whereas elsewhere HCA often only refers to the software. 1516 HCA Topic Model Convergence 104 log2 (perplexity) Topic Model Training Time (s) Training Time for HDP-WSI vs. HCA-WSI 103 102 HDP-WSI HCA-WSI 0 20 40 Number of Lemma Usages (1000’s) 14 12 10 0 Figure 1: Comparison of the time taken to train the topic models of HDP-WSI and HCA-WSI for each lemma in the BNC dataset. For each method, one data point is plotted per lemma. Koeling et al. (2005),9 which was also used by Lau et al. (2014). This dataset consists of 40 English lemmas, and for each lemma it contains a set of usages of varying size from the BNC and a gold-standard sense distribution that was created by hand-annotating a subset of the usages with W ORD N ET 1.7 senses. Using this dataset, we can calculate the quality of a candidate sense distribution by calculating its Jensen Shannon divergence (JSD) with respect to the corresponding gold-standard distribution. JSD is a measure of dissimilarity between two probability distributions, so a lower JSD score means the distribut"
P16-1143,J04-1003,0,0.0262866,"iments is available via: https://github.com/awbennett/LexSemTm 2 Background and Related Work Given the difficulty and expense of obtaining large-scale and robust annotated data, unsupervised approaches to problems involving word learning and recognising word senses have long been studied in NLP. Perhaps the most famous such problem is word sense disambiguation (WSD), for which many unsupervised solutions have been proposed. Some methods are very complex, performing WSD separately for each word usage using information such as word embeddings of surrounding words (Chen et al., 2014) or POStags (Lapata and Brew, 2004). On the other hand, most approaches make use of the difficult-to-beat most frequent sense (MFS) heuristic (McCarthy et al., 2007), which assigns each usage of a given word-type to its most frequent sense. Given the popularity of the MFS heuristic, much of the past work on unsupervised techniques has focused on identifying the most frequent sense. The original method of this kind was proposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar appro"
P16-1143,N07-1044,0,0.0165171,"lly similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on how best to use the MFS heuristic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of Lau et al. (2014), due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository containing sense glosses. The task we are interested in — namely, sense di"
P16-1143,P04-1036,1,0.835389,"posed. Some methods are very complex, performing WSD separately for each word usage using information such as word embeddings of surrounding words (Chen et al., 2014) or POStags (Lapata and Brew, 2004). On the other hand, most approaches make use of the difficult-to-beat most frequent sense (MFS) heuristic (McCarthy et al., 2007), which assigns each usage of a given word-type to its most frequent sense. Given the popularity of the MFS heuristic, much of the past work on unsupervised techniques has focused on identifying the most frequent sense. The original method of this kind was proposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapt"
P16-1143,J07-4005,1,0.0500043,"btaining large-scale and robust annotated data, unsupervised approaches to problems involving word learning and recognising word senses have long been studied in NLP. Perhaps the most famous such problem is word sense disambiguation (WSD), for which many unsupervised solutions have been proposed. Some methods are very complex, performing WSD separately for each word usage using information such as word embeddings of surrounding words (Chen et al., 2014) or POStags (Lapata and Brew, 2004). On the other hand, most approaches make use of the difficult-to-beat most frequent sense (MFS) heuristic (McCarthy et al., 2007), which assigns each usage of a given word-type to its most frequent sense. Given the popularity of the MFS heuristic, much of the past work on unsupervised techniques has focused on identifying the most frequent sense. The original method of this kind was proposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method"
P16-1143,H93-1061,0,0.858704,"Missing"
P16-1143,E06-1016,0,0.0192742,"roposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on how best to use the MFS heuristic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of Lau et al. (2014), due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository co"
P16-1143,D14-1113,0,0.02152,"kind of scores used to rank senses. The state-of-the-art technique of Lau et al. (2014) that we are building upon involves performing unsupervised word sense induction (WSI), which itself is implemented using nonparametric HDP (Teh et al., 2006) topic models, as detailed in Section 3. The WSI component, HDP-WSI, is based on the work of Lau et al. (2012), which at the time was state-of-the-art. Since then, however, other competitive WSI approaches have been developed, involving complex structures such as multi-layer topic models (Chang et al., 2014), or complex word embedding based approaches (Neelakantan et al., 2014). We have not used these approaches in this work on account of their complexity and likely computational cost, however we believe they are worth future exploration. On the other hand, because HDP-WSI is implemented using topic models, it can be customised by replacing HDP with newer, more efficient topic modelling algorithms. Recent work has produced more advanced topic modelling approaches, some of which are extensions of existing approaches using more advanced learning algorithms or expanded models (Buntine and Mishra, 2014), while others are more novel, involving variations such as neural n"
P16-1143,E12-1060,1,0.603835,"eed, of these methods for identifying the MFS, some of them are 1514 explicitly described in terms of sense distribution learning (Chan and Ng, 2005; Chan and Ng, 2006; Lau et al., 2014), while the others implicitly learn sense distributions by calculating some kind of scores used to rank senses. The state-of-the-art technique of Lau et al. (2014) that we are building upon involves performing unsupervised word sense induction (WSI), which itself is implemented using nonparametric HDP (Teh et al., 2006) topic models, as detailed in Section 3. The WSI component, HDP-WSI, is based on the work of Lau et al. (2012), which at the time was state-of-the-art. Since then, however, other competitive WSI approaches have been developed, involving complex structures such as multi-layer topic models (Chang et al., 2014), or complex word embedding based approaches (Neelakantan et al., 2014). We have not used these approaches in this work on account of their complexity and likely computational cost, however we believe they are worth future exploration. On the other hand, because HDP-WSI is implemented using topic models, it can be customised by replacing HDP with newer, more efficient topic modelling algorithms. Re"
P16-1143,S15-2058,0,0.0349866,"Missing"
P16-1143,P14-1025,1,0.218445,"f languages, which could be extremely computationally expensive. To make things worse, domain differences could require learning numerous distributions per word. Despite this, though, we would not want to make these techniques scalable at the expense of sense distribution quality. Therefore, we would like to understand the tradeoff between the accuracy and computation time of these techniques, and optimise this tradeoff. This could be particularly critical in applying them in an industrial setting. The current state-of-the-art technique for unsupervised sense distribution learning is HDP-WSI (Lau et al., 2014). In order to address the above concerns, we provide a series of investigations exploring how to best optimise HDP-WSI for largescale application. We then use our optimised technique to produce L EX S EM TM,1 a semantic and sense frequency dataset of unprecedented size, spanning the entire vocabulary of English. Finally, we use crowdsourced data to produce a new set of gold-standard sense distributions to accompany L EX S EM TM. We use these to investigate the quality of the sense frequency data in L EX S EM TM with respect to S EM C OR. 1 L EX S EM TM, as well as code for accessing L EX S EM"
P16-1143,P11-2098,0,0.0658532,"Missing"
P16-1143,N15-1074,0,0.0151679,"however we believe they are worth future exploration. On the other hand, because HDP-WSI is implemented using topic models, it can be customised by replacing HDP with newer, more efficient topic modelling algorithms. Recent work has produced more advanced topic modelling approaches, some of which are extensions of existing approaches using more advanced learning algorithms or expanded models (Buntine and Mishra, 2014), while others are more novel, involving variations such as neural networks (Larochelle and Murray, 2011; Cao et al., 2015), or incorporating distributional similarity of words (Xie et al., 2015). Of these approaches, we chose to experiment with that of Buntine and Mishra (2014) because a working implementation was readily available, it has previously shown very strong performance in terms of accuracy and speed, and it is similar to HDP and thus easy to incorporate into our work. 3 HDP-WSI Sense Learning HDP-WSI (Lau et al., 2014) is a state-of-theart unsupervised method for learning sense distributions, given a sense repository with per-sense glosses. It takes as input a collection of example usages of the target lemma2 and the glosses 2 Except where stated otherwise, a lemma usage i"
P17-1033,W13-0102,0,0.018532,"topic, we replace s with Bt before computing the softmax over the vocabulary. wise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window (Newman et al., 2010b; Lau et al., 2014).13 Topic models are traditionally evaluated using model perplexity. There are various ways to estimate test perplexity (Wallach et al., 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al. (2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al. (2014) summarises these methods to understand their differences. We propose using automatic topic coherence as a means to evaluate the topic model aspect of tdlm. In terms of datasets, we use the same document collections (APNEWS, IMDB and BNC) as the language model experiments (Section 4). We use the same hyper-parameter settings for tdlm and do not tune them. Based on the findings of Lau and Baldwin (2016), we average topic coherence over the top5/10/15/20 topic words. To aggregate topic coherence scores for a model,"
P17-1033,P03-1054,0,0.0199484,"g, we have additional minibatches for the documents. We start the document classification training after the topic and language models have completed training in each epoch. We use 20 NEWS in this experiment, which is a popular dataset for text classification. 20 NEWS is a collection of forum-like messages from 20 newsgroups categories. We use the “bydate” version of the dataset, where the train and test partition is separated by a specific date. We sample 2K documents from the training set to create the development set. For preprocessing we tokenise words and sentence using Stanford CoreNLP (Klein and Manning, 2003), and lowercase all words. As with previous experiments (Section 4) we additionally filter low/high frequency word types and stopwords. Preprocessed dataset statistics are presented in Table 5. For comparison, we use the same two topic ntm: ntm is a neural topic model proposed by Cao et al. (2015). The document-topic and topicword multinomials are expressed from a neural network perspective using differentiable functions. Model hyper-parameters are tuned using development loss. Topic model performance is presented in Table 4. There are two models of tdlm (tdlm-small and tdlm-large), which spec"
P17-1033,W14-4012,0,0.190012,"Missing"
P17-1033,N16-1057,1,0.799751,"does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al. (2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al. (2014) summarises these methods to understand their differences. We propose using automatic topic coherence as a means to evaluate the topic model aspect of tdlm. In terms of datasets, we use the same document collections (APNEWS, IMDB and BNC) as the language model experiments (Section 4). We use the same hyper-parameter settings for tdlm and do not tune them. Based on the findings of Lau and Baldwin (2016), we average topic coherence over the top5/10/15/20 topic words. To aggregate topic coherence scores for a model, we calculate the mean coherence over topics. For comparison, we use the following topic models: lda: We use a LDA model as a baseline topic model. We use the same LDA models as were used to learn topic distributions for lstm+lda (Section 4). Following Lau et al. (2014), we compute topic coherence using normalised PMI (“NPMI”) scores. Given the top-n words of a topic, coherence is computed based on the sum of pair13 We use this toolkit to compute topic coherence: https://github.com/"
P17-1033,E14-1056,1,0.793704,"distribution for a particular topic is therefore trivial: we can do so by treating s as having maximum weight (1.0) for the topic of interest, and no weight (0.0) for all other topics. Let Bt denote the topic output vector for the t-th topic. To generate the multinomial distribution over word types for the t-th topic, we replace s with Bt before computing the softmax over the vocabulary. wise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window (Newman et al., 2010b; Lau et al., 2014).13 Topic models are traditionally evaluated using model perplexity. There are various ways to estimate test perplexity (Wallach et al., 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al. (2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al. (2014) summarises these methods to understand their differences. We propose using automatic topic coherence as a means to evaluate the topic model aspect of tdlm. In terms of datasets, we use the same"
P17-1033,P11-1015,0,0.226548,"Missing"
P17-1033,D08-1038,0,0.0526451,"datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics. 1 Introduction Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006). A myriad of variants of the classical LDA method (Blei et al., 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009). Separately, language models have long been a foundational component of any NLP task involving generation or textual normalisation of a noisy input (including speech, OCR and the processing of social media text). The primary purpose of a language model is to predict the probability of a 2 Related Work Griffiths et al."
P17-1033,D11-1024,0,0.126281,"Missing"
P17-1033,N10-1012,1,0.90555,"e that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics. 1 Introduction Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006). A myriad of variants of the classical LDA method (Blei et al., 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009). Separately, language models have long been a foundational component of any NLP task involving generation or textual normalisation of a noisy input (including speech, OCR and the processing of social media text). The primary purpose of a language model is to predict the probability of a 2 Related Work Griffiths et al. (2004) propose a mod"
P17-1033,N16-1036,0,0.0174909,"ci i In the case where we use a filters, we have d ∈ Ra , and this constitutes the vector representation of the document generated by the convolutional and max-over-time pooling network. The topic vectors are stored in two lookup tables A ∈ Rk×a (input vector) and B ∈ Rk×b (output vector), where k is the number of topics, and a and b are the dimensions of the topic vectors. To align the document vector d with the topics, we compute an attention vector which is used to 2 The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016). We explored various attention styles (including traditional schemes which use one vector for a topic), but found this approach to work best. 1 A non-linear function is typically used here, but preliminary experiments suggest that the identity function works best for tdlm. 357 We use a gating unit similar to a GRU (Cho et al., 2014; Chung et al., 2014) to allow tdlm to learn the degree of influence of topical information on the language model: ument collections from 3 sources: APNEWS, IMDB and BNC. APNEWS is a collection of Associated Press5 news articles from 2009 to 2016. IMDB is a set of m"
P17-1033,P16-1125,0,0.0617449,"Missing"
P18-1181,W15-0705,1,0.794367,"typical rhyming scheme being ABAB CDCD EFEF GG. There are a number of variants, however, mostly seen in the quatrains; e.g. AABB or ABBA are also common. We build our sonnet dataset from the latest image of Project Gutenberg.4 We first create a 3 There are other forms of sonnets, but the Shakespearean sonnet is the dominant one. Hereinafter “sonnet” is used to specifically mean Shakespearean sonnets. 4 https://www.gutenberg.org/. Partition #Sonnets #Words Train Dev Test 2685 335 335 367K 46K 46K Table 1: SONNET dataset statistics. (generic) poetry document collection using the GutenTag tool (Brooke et al., 2015), based on its inbuilt poetry classifier and rule-based structural tagging of individual poems. Given the poems, we use word and character statistics derived from Shakespeare’s 154 sonnets to filter out all non-sonnet poems (to form the “BACKGROUND” dataset), leaving the sonnet corpus (“SONNET”).5 Based on a small-scale manual analysis of SONNET, we find that the approach is sufficient for extracting sonnets with high precision. BACKGROUND serves as a large corpus (34M words) for pre-training word embeddings, and SONNET is further partitioned into training, development and testing sets. Statis"
P18-1181,W14-4012,0,0.155437,"Missing"
P18-1181,D10-1051,0,0.80635,"rains with stress and rhyme patterns that are indistinguishable from human-written poems and rated highly by an expert; • a vanilla language model trained over our sonnet corpus, surprisingly, captures meter implicitly at human-level performance; • while crowd workers rate the poems generated by our best model as nearly indistinguishable from published poems by humans, an expert annotator found the machine-generated poems to lack readability and emotion, and our best model to be only comparable to a vanilla language model on these dimensions; • most work on poetry generation focuses on meter (Greene et al., 2010; Ghazvininejad et al., 2016; Hopkins and Kiela, 2017); our results suggest that future research should look beyond meter and focus on improving readability. In this, we develop a new annotation framework for the evaluation of machine-generated poems, and release both a novel data of sonnets and the full source code associated with this research.2 Noting that there are many notable divergences from this in the work of particular poets (e.g. Walt Whitman) and poetry types (such as free verse or haiku). 2 https://github.com/jhlau/deepspeare 1948 Proceedings of the 56th Annual Meeting of the Asso"
P18-1181,P17-1016,0,0.423851,"istinguishable from human-written poems and rated highly by an expert; • a vanilla language model trained over our sonnet corpus, surprisingly, captures meter implicitly at human-level performance; • while crowd workers rate the poems generated by our best model as nearly indistinguishable from published poems by humans, an expert annotator found the machine-generated poems to lack readability and emotion, and our best model to be only comparable to a vanilla language model on these dimensions; • most work on poetry generation focuses on meter (Greene et al., 2010; Ghazvininejad et al., 2016; Hopkins and Kiela, 2017); our results suggest that future research should look beyond meter and focus on improving readability. In this, we develop a new annotation framework for the evaluation of machine-generated poems, and release both a novel data of sonnets and the full source code associated with this research.2 Noting that there are many notable divergences from this in the work of particular poets (e.g. Walt Whitman) and poetry types (such as free verse or haiku). 2 https://github.com/jhlau/deepspeare 1948 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), p"
P18-1181,D14-1181,0,0.00252216,"s and preceding context improves performance substantially, reducing perplexity by almost 10 points from LM to LM∗∗ . The inferior performance of LM∗∗ -C compared to LM∗∗ demonstrates that our approach of processing context with recurrent networks with selective encoding is more effective than convolutional networks. The full model LM∗∗ +PM+RM, which learns stress 20 In Zhang and Lapata (2014), the authors use a series of convolutional networks with a width of 2 words to convert 5/7 poetry lines into a fixed size vector; here we use a standard convolutional network with max-pooling operation (Kim, 2014) to process the context. Pentameter Model To assess the pentameter model, we use the attention weights to predict stress patterns for words in the test data, and compare them against stress patterns in the CMU pronunciation dictionary.21 Words that have no coverage or have nonalternating patterns given by the dictionary are discarded. We use accuracy as the metric, and a predicted stress pattern is judged to be correct if it matches any of the dictionary stress patterns. To extract a stress pattern for a word from the model, we iterate through the pentameter (10 time steps), and append the app"
P18-1181,W09-2005,0,0.105737,"d with this research.2 Noting that there are many notable divergences from this in the work of particular poets (e.g. Walt Whitman) and poetry types (such as free verse or haiku). 2 https://github.com/jhlau/deepspeare 1948 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1948–1958 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work Early poetry generation systems were generally rule-based, and based on rhyming/TTS dictionaries and syllable counting (Gerv´as, 2000; Wu et al., 2009; Netzer et al., 2009; Colton et al., 2012; Toivanen et al., 2013). The earliest attempt at using statistical modelling for poetry generation was Greene et al. (2010), based on a language model paired with a stress model. Neural networks have dominated recent research. Zhang and Lapata (2014) use a combination of convolutional and recurrent networks for modelling Chinese poetry, which Wang et al. (2016) later simplified by incorporating an attention mechanism and training at the character level. For English poetry, Ghazvininejad et al. (2016) introduced a finite-state acceptor to explicitly model rhythm in conjunc"
P18-1181,E17-2025,0,0.0475363,"), and optimise the model with standard categorical cross-entropy loss. We use dropout as regularisation (Srivastava et al., 2014), and apply it to the encoder/decoder LSTM outputs and word embedding lookup. The same regularisation method is used for the pentameter and rhyme models. As our sonnet data is relatively small for training a neural language model (367K words; see Table 1), we pre-train word embeddings and reduce parameters further by introducing weight-sharing between output matrix Wout and embedding matrix Wwrd via a projection matrix Wprj (Inan et al., 2016; Paulus et al., 2017; Press and Wolf, 2017): the pentameter model learns to attend to the appropriate characters to predict the 10 binary stress symbols sequentially.11 As punctuation is not pronounced, we preprocess each sonnet line to remove all punctuation, leaving only spaces and letters. Like the language model, the pentameter model is fashioned as an encoder–decoder network. In the encoder, we embed the characters using the shared embedding matrix Wchr and feed them to the shared bidirectional character-level LSTM (Equation (1)) to produce the character encodings for the sentence: uj = [~uj ; u~j ]. In the decoder, it attends to"
P18-1181,P11-2014,0,0.504514,"ncluded in the dictionary are discarded. Rhyme is determined by extracting the final stressed phoneme for the paired words, and testing if their phoneme patterns match. We predict rhyme for a word pair by feeding them to the rhyme model and computing cosine similarity; if a word pair is assigned a score > 0.8,23 it is considered to rhyme. As a baseline (Rhyme-BL), we first extract for each word the last vowel and all following consonants, and predict a word pair as rhyming if their extracted sequences match. The extracted sequence can be interpreted as a proxy for the last syllable of a word. Reddy and Knight (2011) propose an unsupervised model for learning rhyme schemes in poems via EM. There are two latent variables: φ specifies the distribution of rhyme schemes, and θ defines 23 0.8 is empirically found to be the best threshold based on development data. the pairwise rhyme strength between two words. The model’s objective is to maximise poem likelihood over all possible rhyme scheme assignments under the latent variables φ and θ. We train this model (Rhyme-EM) on our data24 and use the learnt θ to decide whether two words rhyme.25 Table 2 details the rhyming results. The rhyme model performs very str"
P18-1181,P17-1099,0,0.0158761,"that it can simply ignore u∗t to predict the alternating stresses based on gt . For this reason we use only u∗t to compute the stress probability: P (S − ) = σ(We u∗t + be ) P which gives the loss Lent = t − log P (St? ) for the whole sequence, where St? is the target stress at time step t. We find the decoder still has the tendency to attend to the same characters, despite the incorporation of position information. To regularise the model further, we introduce two loss penalties: repeat and coverage loss. The repeat loss penalises the model when it attends to previously attended characters (See et al., 2017), and is computed as follows: Lrep = XX t min(fjt , j t−1 X fjt ) t=1 By keeping a sum of attention weights over all previous time steps, we penalise the model when it focuses on characters that have non-zero history weights. The repeat loss discourages the model from focussing on the same characters, but does not assure that the appropriate characters receive attention. Observing that stresses are aligned with the vowels of a syllable, we therefore penalise the model when vowels are ignored: Lcov = X j∈V ReLU(C − 10 X fjt ) t=1 where V is a set of positions containing vowel characters, and C"
P18-1181,D14-1074,0,0.589919,"Association for Computational Linguistics (Long Papers), pages 1948–1958 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work Early poetry generation systems were generally rule-based, and based on rhyming/TTS dictionaries and syllable counting (Gerv´as, 2000; Wu et al., 2009; Netzer et al., 2009; Colton et al., 2012; Toivanen et al., 2013). The earliest attempt at using statistical modelling for poetry generation was Greene et al. (2010), based on a language model paired with a stress model. Neural networks have dominated recent research. Zhang and Lapata (2014) use a combination of convolutional and recurrent networks for modelling Chinese poetry, which Wang et al. (2016) later simplified by incorporating an attention mechanism and training at the character level. For English poetry, Ghazvininejad et al. (2016) introduced a finite-state acceptor to explicitly model rhythm in conjunction with a recurrent neural language model for generation. Hopkins and Kiela (2017) improve rhythm modelling with a cascade of weighted state transducers, and demonstrate the use of character-level language model for English poetry. A critical difference over our work is"
P18-1181,P17-1101,0,0.0192447,"a number of variations in addition to the standard pattern (Greene et al., 2010), but our model uses only the standard pattern as it is the dominant one. 1949 (a) Language model (b) Pentameter model (c) Rhyme model Figure 2: Architecture of the language, pentameter and rhyme models. Colours denote shared weights. ter model to sample meter-conforming sentences and the rhyme model to enforce rhyme. The architecture of the joint model is illustrated in Figure 2. We train all the components together by treating each component as a sub-task in a multitask learning setting.8 a selective mechanism (Zhou et al., 2017) to each hi . By defining the representation of the whole context h = [~hC ; h~1 ] (where C is the number of words in the context), the selective mechanism filters the hidden states hi using h as follows: 4.1 where denotes element-wise product. Hereinafter W, U and b are used to refer to model parameters. The intuition behind this procedure is to selectively filter less useful elements from the context words. In the decoder, we embed words xt in the current line using the encoder-shared embedding matrix (Wwrd ) to produce wt . In addition to the word embeddings, we also embed the characters of"
P18-2073,E87-1007,0,0.483838,"Missing"
P18-2073,A00-2019,0,0.341576,"Missing"
P18-2073,P14-2029,0,0.0542392,"Missing"
P18-2073,D07-1012,0,0.0598687,"Missing"
P18-2073,P07-2045,0,0.00510625,"ence and then take the mean over all sentences. We found a small difference: 0.71 for h− and 0.76 for h+ . We also calculate one-vs-rest correlation, where for each The Influence of Document Context on Acceptability Ratings Our goal is to construct a dataset of sentences annotated with acceptability ratings, judged with and without document context. To obtain sentences and their document context, we extracted 100 random articles from the English Wikipedia and sampled a sentence from each article. To generate a set of sentences with varying degrees of acceptability we used the Moses MT system (Koehn et al., 2007) to translate each sentence from English to 4 target languages — Czech, Spanish, German and French — and then back to English.3 We chose these 4 languages because preliminary experiments found that they produce sentences with different sorts of grammatical, semantic, and lexical infelicities. Note that we only translate the sentences; the document context is not modified. To gather acceptability judgements we used Amazon Mechanical Turk and asked workers to judge acceptability using a 4-point scale.4 We ran the annotation task twice: first where we presented sentences without context, and seco"
P18-2073,P17-1033,1,0.804602,"Missing"
P18-2073,P15-1156,1,0.859699,"ed systems to model acceptability.1 1 Introduction Sentence acceptability is defined as the extent to which a sentence is well formed or natural to native speakers of a language. It encompasses semantic, syntactic and pragmatic plausibility and other non-linguistic factors such as memory limitation. Grammaticality, by contrast, is the syntactic well-formedness of a sentence. Grammaticality as characterised by formal linguists is a theoretical concept that is difficult to elicit from non-expert assessors. In the research presented here we are interested in predicting acceptability judgements.2 Lau et al. (2015, 2016) present unsupervised probabilistic methods to predict sentence acceptability, where sentences were judged independently of context. In this paper we extend this 1 Annotated data (with acceptability ratings) is available at: https://github.com/GU-CLASP/BLL2018. 2 See Lau et al. (2016) for a detailed discussion of the relationship between acceptability and grammaticality. They provide motivation for measuring acceptability rather than grammaticality in their crowd source surveys and modelling experiments. 456 Proceedings of the 56th Annual Meeting of the Association for Computational Lin"
Q14-1003,P10-1010,0,0.0108201,"nd estimates the proportion of the document that is written in each language. Detecting multilingual documents has a variety of applications. Most natural language processing techniques presuppose monolingual input data, so inclusion of data in foreign languages introduces noise, and can degrade the performance of NLP systems (Alex et al., 2007; Cook and Lui, 2012). Automatic detection of multilingual documents can be used as a pre-filtering step to improve the quality of input data. Detecting multilingual documents is also important for acquiring linguistic data from the web (Scannell, 2007; Abney and Bird, 2010), and has applications in mining bilingual texts for statistical machine translation from online resources (Resnik, 1999; Nie et al., 1999; Ling et al., 2013). There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such as English (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013). King and Abney (2013, p1118) specifically mention the need for an automatic method “to examine a multilingual document, and with high accuracy, list the languages that are present in the"
Q14-1003,D07-1016,0,0.0480532,"Missing"
Q14-1003,N10-1027,1,0.505424,") we show that our method is able to identify multilingual documents in real-world data. 2 Background Most language identification research focuses on language identification for monolingual documents (Hughes et al., 2006). In monolingual LangID, the task is to assign each document D a unique language Li ∈ L. Some work has reported near-perfect accuracy for language identification of large documents in a small number of languages (Cavnar and Trenkle, 1994; McNamee, 2005). However, in order to attain such accuracy, a large number of simplifying assumptions have to be made (Hughes et al., 2006; Baldwin and Lui, 2010a). In this work, we tackle the assumption that each document is monolingual, i.e. it contains text from a single language. In language identification, documents are modeled as a stream of characters (Cavnar and Trenkle, 1994; Kikui, 1996), often approximated by the corresponding stream of bytes (Kruengkrai et al., 2005; Baldwin and Lui, 2010a) for robustness over variable character encodings. In this work, we follow Baldwin and Lui (2010a) in training a single model for languages that naturally use multiple encodings 28 (e.g. UTF8, Big5 and GB encodings for Chinese), as issues of encoding are"
Q14-1003,W12-2108,0,0.145694,"including measures based on rank order statistics (Cavnar and Trenkle, 1994), information theory (Baldwin and Lui, 2010a), string kernels (Kruengkrai et al., 2005) and vector space models (Prager, 1999a; McNamee, 2005). Language identification has been applied in domains such as USENET messages (Cavnar and Trenkle, 1994), web pages (Kikui, 1996; Martins and Silva, 2005; Liu and Liang, 2008), web search queries (Ceylan and Kim, 2009; Bosca and Dini, 2010), mining the web for bilingual text (Resnik, 1999; Nie et al., 1999), building minority language corpora (Ghani et al., 2004; Scannell, 2007; Bergsma et al., 2012) as well as a largescale database of Interlinear Glossed Text (Xia et al., 2010), and the construction of a large-scale multilingual web crawl (Callan and Hoy, 2009). 2.1 Multilingual Documents Language identification over documents that contain text from more than one language has been identified as an open research question (Hughes et al., 2006). Common examples of multilingual documents are web pages that contain excerpts from another language, and documents from multilingual organizations such as the European Union. character byte English the French pour Italian di German auf Dutch voo Jap"
Q14-1003,P09-1120,0,0.016262,"nstette, 1995; Lui and Baldwin, 2011; Tiedemann and Ljubeˇsi´c, 2012), and compressive models (Teahan, 2000). The nearest-prototype methods vary primarily in the distance measure used, including measures based on rank order statistics (Cavnar and Trenkle, 1994), information theory (Baldwin and Lui, 2010a), string kernels (Kruengkrai et al., 2005) and vector space models (Prager, 1999a; McNamee, 2005). Language identification has been applied in domains such as USENET messages (Cavnar and Trenkle, 1994), web pages (Kikui, 1996; Martins and Silva, 2005; Liu and Liang, 2008), web search queries (Ceylan and Kim, 2009; Bosca and Dini, 2010), mining the web for bilingual text (Resnik, 1999; Nie et al., 1999), building minority language corpora (Ghani et al., 2004; Scannell, 2007; Bergsma et al., 2012) as well as a largescale database of Interlinear Glossed Text (Xia et al., 2010), and the construction of a large-scale multilingual web crawl (Callan and Hoy, 2009). 2.1 Multilingual Documents Language identification over documents that contain text from more than one language has been identified as an open research question (Hughes et al., 2006). Common examples of multilingual documents are web pages that co"
Q14-1003,U12-1014,1,0.72294,"move this monolingual assumption, and address the problem of language identification in documents that may contain text from more than one language from the candidate set. We propose a method that concurrently detects that a document is multilingual, and estimates the proportion of the document that is written in each language. Detecting multilingual documents has a variety of applications. Most natural language processing techniques presuppose monolingual input data, so inclusion of data in foreign languages introduces noise, and can degrade the performance of NLP systems (Alex et al., 2007; Cook and Lui, 2012). Automatic detection of multilingual documents can be used as a pre-filtering step to improve the quality of input data. Detecting multilingual documents is also important for acquiring linguistic data from the web (Scannell, 2007; Abney and Bird, 2010), and has applications in mining bilingual texts for statistical machine translation from online resources (Resnik, 1999; Nie et al., 1999; Ling et al., 2013). There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such a"
Q14-1003,hughes-etal-2006-reconsidering,1,0.689156,"nt a method for identifying multilingual documents, the languages contained therein and the relative proportion of the document in each language; (2) we show that our method outperforms state-of-the-art methods for language identification in multilingual documents; (3) we show that our method is able to estimate the proportion of the document in each language to a high degree of accuracy; and (4) we show that our method is able to identify multilingual documents in real-world data. 2 Background Most language identification research focuses on language identification for monolingual documents (Hughes et al., 2006). In monolingual LangID, the task is to assign each document D a unique language Li ∈ L. Some work has reported near-perfect accuracy for language identification of large documents in a small number of languages (Cavnar and Trenkle, 1994; McNamee, 2005). However, in order to attain such accuracy, a large number of simplifying assumptions have to be made (Hughes et al., 2006; Baldwin and Lui, 2010a). In this work, we tackle the assumption that each document is monolingual, i.e. it contains text from a single language. In language identification, documents are modeled as a stream of characters ("
Q14-1003,C96-2110,0,0.21726,"he task is to assign each document D a unique language Li ∈ L. Some work has reported near-perfect accuracy for language identification of large documents in a small number of languages (Cavnar and Trenkle, 1994; McNamee, 2005). However, in order to attain such accuracy, a large number of simplifying assumptions have to be made (Hughes et al., 2006; Baldwin and Lui, 2010a). In this work, we tackle the assumption that each document is monolingual, i.e. it contains text from a single language. In language identification, documents are modeled as a stream of characters (Cavnar and Trenkle, 1994; Kikui, 1996), often approximated by the corresponding stream of bytes (Kruengkrai et al., 2005; Baldwin and Lui, 2010a) for robustness over variable character encodings. In this work, we follow Baldwin and Lui (2010a) in training a single model for languages that naturally use multiple encodings 28 (e.g. UTF8, Big5 and GB encodings for Chinese), as issues of encoding are not the focus of this research. The document representation used for language identification generally involves estimating the relative distributions of particular byte sequences, selected such that their distributions differ between lang"
Q14-1003,N13-1131,0,0.474412,"ts can be used as a pre-filtering step to improve the quality of input data. Detecting multilingual documents is also important for acquiring linguistic data from the web (Scannell, 2007; Abney and Bird, 2010), and has applications in mining bilingual texts for statistical machine translation from online resources (Resnik, 1999; Nie et al., 1999; Ling et al., 2013). There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such as English (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013). King and Abney (2013, p1118) specifically mention the need for an automatic method “to examine a multilingual document, and with high accuracy, list the languages that are present in the document”. We introduce a method that is able to detect multilingual documents, and simultaneously identify each language present as well as estimate the proportion of the document written in that language. We achieve this with a probabilistic mixture model, using a document representation developed for monolingual language identification (Lui and Baldwin, 2011). The model posits that each document is genera"
Q14-1003,P13-1018,0,0.0260113,"ge processing techniques presuppose monolingual input data, so inclusion of data in foreign languages introduces noise, and can degrade the performance of NLP systems (Alex et al., 2007; Cook and Lui, 2012). Automatic detection of multilingual documents can be used as a pre-filtering step to improve the quality of input data. Detecting multilingual documents is also important for acquiring linguistic data from the web (Scannell, 2007; Abney and Bird, 2010), and has applications in mining bilingual texts for statistical machine translation from online resources (Resnik, 1999; Nie et al., 1999; Ling et al., 2013). There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such as English (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013). King and Abney (2013, p1118) specifically mention the need for an automatic method “to examine a multilingual document, and with high accuracy, list the languages that are present in the document”. We introduce a method that is able to detect multilingual documents, and simultaneously identify each language present as well as estimate the pro"
Q14-1003,I11-1062,1,0.71337,"h as English (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013). King and Abney (2013, p1118) specifically mention the need for an automatic method “to examine a multilingual document, and with high accuracy, list the languages that are present in the document”. We introduce a method that is able to detect multilingual documents, and simultaneously identify each language present as well as estimate the proportion of the document written in that language. We achieve this with a probabilistic mixture model, using a document representation developed for monolingual language identification (Lui and Baldwin, 2011). The model posits that each document is generated as samples from an unknown mixture of languages from the training set. We introduce a Gibbs sampler to map samples to languages for any given set of languages, and use this to select the set of languages that maximizes the posterior probability of the document. 27 Transactions of the Association for Computational Linguistics, 2 (2014) 27–40. Action Editor: Kristina Toutanova. c Submitted 1/2013; Revised 7/2013; Published 2/2014. 2014 Association for Computational Linguistics. Our method is able to learn a language identifier for multilingual d"
Q14-1003,D09-1026,0,0.0510119,"uish between languages. The exact set of features is selected from the training data using Information Gain (IG), an information-theoretic metric developed as a splitting criterion for decision trees (Quinlan, 1993). IGbased feature selection combined with a naive Bayes classifier has been shown to be particularly effective for language identification (Lui and Baldwin, 2011). 3.2 Generative Mixture Models Generative mixture models are popular for text modeling tasks where a mixture of influences governs the content of a document, such as in multi-label document classification (McCallum, 1999; Ramage et al., 2009), and topic modeling (Blei et al., 2003). Such models normally assume full exchangeability between tokens (i.e. the bag-of-words assumption), and label each token with a single discrete label. Multi-label text classification, topic modeling and our model for language identification in multilingual documents share the same fundamental representation of the latent structure of a document. Each label is modeled with a probability distribution over tokens, and each document is modeled as a probabilistic mixture of labels. As presented in Griffiths and Steyvers (2004), the probability of the ith to"
Q14-1003,P99-1068,0,0.27484,"pplications. Most natural language processing techniques presuppose monolingual input data, so inclusion of data in foreign languages introduces noise, and can degrade the performance of NLP systems (Alex et al., 2007; Cook and Lui, 2012). Automatic detection of multilingual documents can be used as a pre-filtering step to improve the quality of input data. Detecting multilingual documents is also important for acquiring linguistic data from the web (Scannell, 2007; Abney and Bird, 2010), and has applications in mining bilingual texts for statistical machine translation from online resources (Resnik, 1999; Nie et al., 1999; Ling et al., 2013). There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such as English (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013). King and Abney (2013, p1118) specifically mention the need for an automatic method “to examine a multilingual document, and with high accuracy, list the languages that are present in the document”. We introduce a method that is able to detect multilingual documents, and simultaneously identify each langua"
Q14-1003,C12-1160,0,0.189363,"Missing"
Q14-1003,P12-1102,0,0.696165,"detection of multilingual documents can be used as a pre-filtering step to improve the quality of input data. Detecting multilingual documents is also important for acquiring linguistic data from the web (Scannell, 2007; Abney and Bird, 2010), and has applications in mining bilingual texts for statistical machine translation from online resources (Resnik, 1999; Nie et al., 1999; Ling et al., 2013). There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such as English (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013). King and Abney (2013, p1118) specifically mention the need for an automatic method “to examine a multilingual document, and with high accuracy, list the languages that are present in the document”. We introduce a method that is able to detect multilingual documents, and simultaneously identify each language present as well as estimate the proportion of the document written in that language. We achieve this with a probabilistic mixture model, using a document representation developed for monolingual language identification (Lui and Baldwin, 2011). The model posits that"
Q14-1003,C00-2137,0,0.031814,"rt both the documentlevel micro-average, as well as the language-level macro-average. For consistency with Baldwin and Lui (2010a), the macro-averaged F-score we report is the average of the per-class F-scores, rather than the harmonic mean of the macro-averaged precision and recall; as such, it is possible for the F-score to not fall between the precision and recall values. As is common practice, we compute the F-score for β = 1, giving equal importance to precision and recall.2 We tested the difference in performance for statistical significance using an approximate randomization procedure (Yeh, 2000) with 10000 iterations. Within each table of results (Tables 2, 3 and 2 Intuitively, it may seem that the maximal precision and recall should be achieved when precision and recall are balanced. However, because of the multi-label nature of the task and variable number of labels assigned to a given document by our models, it is theoretically possible and indeed common in our results for the maximal macro-averaged F-score to be achieved when macro-averaged precision and recall are not balanced. 33 4), all differences between systems are statistically significant at a p &lt; 0.05 level. To evaluate"
Q14-1003,U10-1003,1,\N,Missing
S13-2039,S07-1002,0,0.060288,"model? and (4) given the rapid pace of language evolution on real-time social media such as Twitter and Facebook, is it reasonable to assume a static sense inventory? Given this backdrop, there has been a recent growth of interest in the task of word sense induction (WSI), where the word sense representation for a given word is automatically inferred from a given data source, and word usages are labelled (often probabilistically) according to that data source. While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (Agirre and Soroa, 2007; Manandhar et al., 2010), leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled “Evaluating Word Sense Induction & Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of"
S13-2039,E12-1060,1,0.867931,"iven data source, and word usages are labelled (often probabilistically) according to that data source. While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (Agirre and Soroa, 2007; Manandhar et al., 2010), leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled “Evaluating Word Sense Induction & Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of novel word sense detection. The core machinery of our system is driven by a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). In LDA, the model learns latent topics for a collection of documents, and associates these latent topics with every document in the collection. A topic is represented by a multinomial distribution of words, and th"
S13-2039,S13-2051,1,0.837682,"Missing"
S13-2039,S10-1011,0,0.0389883,"rapid pace of language evolution on real-time social media such as Twitter and Facebook, is it reasonable to assume a static sense inventory? Given this backdrop, there has been a recent growth of interest in the task of word sense induction (WSI), where the word sense representation for a given word is automatically inferred from a given data source, and word usages are labelled (often probabilistically) according to that data source. While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (Agirre and Soroa, 2007; Manandhar et al., 2010), leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled “Evaluating Word Sense Induction & Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of novel word sense detectio"
S13-2039,S13-2035,0,0.122581,"induce the ranking based on the sense probabilities assigned to the senses, such that snippets that have the highest probability of the induced sense are ranked highest, and snippets with lower sense probabilities 3 Our implementation can be accessed via https:// github.com/jhlau/hdp-wsi. 219 are ranked lower. Two classes of evaluation are used in the shared task: 1. cluster quality measures: Jaccard Index (JI), RandIndex (RI), Adjusted RandIndex (ARI) and F1; 2. diversification of search results: Subtopic Recall@K and Subtopic Precision@r. Details of the evaluation measures are described in Navigli and Vannella (2013). The idea behind the second form of evaluation (i.e. diversification of search results) is that search engine results should cluster the results based on senses (of the query term in the documents) given an ambiguous query. For example, if a user searches for apple, the search engine may return results related to both the computer brand sense and the fruit sense of apple. Given this assumption, the best WSI/WSD system is the one that can correctly identify the diversity of senses in the snippets. Figure 1: Subtopic Recall@K for all participating systems. Cluster quality, subtopic recall@K and"
S13-2051,S12-1027,0,0.0225917,"he weights of the induced senses and that of the gold senses. For clustering comparison, fuzzy normalised mutual information (FNMI) and fuzzy b-cubed (FBC) are used. Note that the WSD systems participating in this shared task are not evaluated with clustering comparison metrics, as they do not induce senses/clusters in the same manner as WSI systems. WSI systems produce senses that are different to the gold standard sense inventory (WordNet 3.1), and the induced senses are mapped to the gold standard senses using the 80/20 validation setting. Details of this mapping procedure are described in Jurgens (2012). Results for all test instances are presented in Table 3. Note that many baselines are used, only some of which we present in this paper, namely: (1) R AN DOM — label instances with one of three random induced senses; (2) S EMCOR MFS — label instances with the most frequently occurring sense in Semcor; (3) T EST MFS — label instances with the most frequently occurring sense in the test dataset. To benchmark our method, we present one or two of the best 309 systems from each team. Looking at Table 3, our system performs encouragingly well. Although not the best system, we achieve results close"
S13-2051,E12-1060,1,0.721911,"ed to grade senses rather than selecting a single sense like most word sense disambiguation (WSD) settings. The evaluation measures are designed to assess how well a system perceives the different senses in a contextual instance. We adopt a previously-proposed WSI methodology for the task, which is based on a Hierarchical Dirichlet Process (HDP), a nonparametric topic model. Our system requires no parameter tuning, uses the English ukWaC as an external resource, and achieves encouraging results over the shared task. 1 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012), and also applied to SemEval-2013 Task 11 on WSI for web snippet clustering (Lau et al., to appear). The core machinery of our system is driven by a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). In LDA, the model learns latent topics for a collection of documents, and associates these latent topics with every document in the collection. A topic is represented by a multinomial distribution of words, and the association of topics with documents is represented by a multinomial distribution of topics, a distribution for each document. The generative process of LDA for drawing"
S13-2051,S13-2039,1,0.837606,"Missing"
U19-1002,W14-3346,0,0.0275103,"Missing"
U19-1002,P84-1044,0,0.320586,"Missing"
U19-1002,C08-1048,0,0.0516628,"es users to provide keywords or titles and a rhyming template so the system can expand words and fill them into the user-selected template. This method is used in several successful poetry generation services, for instance, the haiku generation system of Wu et al. (2009) extracts rules from corpus and expand keywords from users to generate haiku sentences based on rules. Another influential approach for Chinese quatrain generation is statistical machine translation (SMT). It is first utilised to generate Chinese couplets, which can be regarded as a special quatrain comprised of two sentences (Jiang and Zhou, 2008). They take the first sentence as input from users and generate an N-best list of second sentence candidates via a phrase-based SMT decoder. This method is extended by He et al. (2012) to generate Chinese quatrains where users are asked to provide the first sentence as input and then the system “translates” three following sentences based on the previous sentences. Recently, neural networks are the predominant technique in the literature. Zhang and Lapata (2014) propose using recurrent neural networks (RNN) to generate the poem sentence by sentence. However, the model they provide is rather co"
U19-1002,P18-1181,1,0.896971,"ZPP 飞流直下三千尺， *P*ZPPZ 疑是银河落九天。 *ZPPZZP Waterfall on Mount Lu Li Bai The Sunlit Censer peak exhales a wreath of cloud. Like an unpended stream the waterfall sounds loud. Its torrent dashes down three thousand feet from high. As if the Silver River fell from azure sky. Table 1: A 7-character Tang Quatrain by Li Bai. tone is indicated in Table 1 where ‘P’ indicates “Ping”, ‘Z’ refers to “Ze” and ‘*’ means this character can be either tone. Following all these constraints, a well-written Chinese quatrain are full of rhythm while expressing abundant emotions. 1.1 Motivation Inspired by Deep-speare (Lau et al., 2018), a joint neural network model that is trained on English sonnets to generate quatrains in iambic pentameter (Halle and Keyser, 1971), we seek to adapt this model to poetry in other languages. The abundance of Chinese quatrain and its important cultural status makes it a natural choice. We analyse the similarity between characteristics of English sonnets and Chinese quatrains and found it is possible to adapt the model to Chinese as the language model can be modified to handle Chinese characters. Lau et al. (2018) found that a vanilla language model can learn meter automatically at human level"
U19-1002,D14-1074,0,0.0254334,"ation (SMT). It is first utilised to generate Chinese couplets, which can be regarded as a special quatrain comprised of two sentences (Jiang and Zhou, 2008). They take the first sentence as input from users and generate an N-best list of second sentence candidates via a phrase-based SMT decoder. This method is extended by He et al. (2012) to generate Chinese quatrains where users are asked to provide the first sentence as input and then the system “translates” three following sentences based on the previous sentences. Recently, neural networks are the predominant technique in the literature. Zhang and Lapata (2014) propose using recurrent neural networks (RNN) to generate the poem sentence by sentence. However, the model they provide is rather complex as it has one CNN and two RNNs, and it still observes theme drift when generating long sequences. To solve these problems, Wang et al. (2016) propose a simpler neural model which treats the whole poem as a character sequence. This approach can be easily extended to generate other genres such as Song Iambics or Haiku and they utilise the attention mechanism (Bahdanau et al., 2014) to avoid theme drift. The closest work to our study is Deep-speare, which is"
U19-1002,P17-1101,0,0.0559112,"Missing"
U19-1010,D15-1263,0,0.0737828,"arser of Joty et al. (2013) to RST-parse product reviews. By extracting graph-based features, important aspects are identified in the review and included in the summary based on a template-based generation framework. Although the experiment shows that the RST can be beneficial for content selection, the proposed feature is rule-based and highly tailored to review documents. Instead, in this work, we extract a latent representation of the discourse directly from the Yu et al. (2018) parser, and incorporate this into the abstractive summarizer. 2.2 Discourse Analysis for Document Classification Bhatia et al. (2015) show that discourse analyses produced by an RST parser can improve document-level sentiment analysis. Based on DPLP (Discourse Parsing from Linear Projection) — an RST parser by Ji and Eisenstein (2014) — they recursively propagate sentiment scores up to the root via a neural network. A similar idea was proposed by Lee et al. (2018), where a recursive neural network is used to learn a discourse-aware representation. Here, DPLP is utilized to obtain discourse structures, and a recursive neural network is applied to the doc2vec (Le and Mikolov, 2014) representations for each EDU. The proposed a"
U19-1010,N18-2097,0,0.020144,"nslation models, in largely eschewing symbolic 5 https://github.com/XuezheMax/ NeuroNLP2 analysis and learning purely from training data. Pioneering work such as Rush et al. (2015), for instance, assumes the neural architecture is able to learn main sentence identification, discourse structure analysis, and paraphrasing all in one model. Studies such as Gehrmann et al. (2018); Hsu et al. (2018) attempt to incorporate additional supervision (e.g. content selection) to improve summarization. Although there are proposals that extend sequence-to-sequence models based on discourse structure — e.g. Cohan et al. (2018) include an additional attention layer for document sections — direct incorporation of discourse information is rarely explored. Hare and Borchardt (1984) observe four core activities involved in creating a summary: (1) topic sentence identification; (2) deletion of unnecessary details; (3) paragraph collapsing; and (4) paraphrasing and insertion of connecting words. Current approaches (Nallapati et al., 2016; See et al., 2017) capture topic sentence identification by leveraging the pointer network to do content selection, but the model is left to largely figure out the rest by providing it wi"
U19-1010,P81-1022,0,0.618278,"Missing"
U19-1010,P12-1007,0,0.161621,"course parser in sequence training; (2) we empirically demonstrate that a latent representation of discourse structure enhances the summaries generated by an abstractive summarizer; and (3) we show that discourse structure is an essential factor in modelling the popularity of online petitions. 2 Related Work Discourse parsing, especially in the form of RST parsing, has been the target of research over a long period of time, including pre-neural feature engi1 The details of each relation can be found on the RST website http://www.sfu.ca/rst/index.html neering approaches (Hernault et al., 2010; Feng and Hirst, 2012; Ji and Eisenstein, 2014). Two approaches have been proposed to construct discourse parses: (1) bottom-up construction, where EDU merge operations are applied to single units; and (2) transition parser approaches, where the discourse tree is constructed as a sequence of parser actions. Neural sequence models have also been proposed. In early work, Li et al. (2016a) applied attention in an encoder–decoder framework and slightly improved on a classical featureengineering approach. The current state of the art is a neural transition-based discourse parser (Yu et al., 2018) which incorporates imp"
U19-1010,D18-1443,0,0.0190638,"in the original document; it is closer to how humans summarize, in that it generates paraphrases and blends multiple sentences in a coherent manner. Current sequence-to-sequence models for abstractive summarization work like neural machine translation models, in largely eschewing symbolic 5 https://github.com/XuezheMax/ NeuroNLP2 analysis and learning purely from training data. Pioneering work such as Rush et al. (2015), for instance, assumes the neural architecture is able to learn main sentence identification, discourse structure analysis, and paraphrasing all in one model. Studies such as Gehrmann et al. (2018); Hsu et al. (2018) attempt to incorporate additional supervision (e.g. content selection) to improve summarization. Although there are proposals that extend sequence-to-sequence models based on discourse structure — e.g. Cohan et al. (2018) include an additional attention layer for document sections — direct incorporation of discourse information is rarely explored. Hare and Borchardt (1984) observe four core activities involved in creating a summary: (1) topic sentence identification; (2) deletion of unnecessary details; (3) paragraph collapsing; and (4) paraphrasing and insertion of connect"
U19-1010,D14-1168,0,0.0247245,"n, Ono et al. (1994), O’Donnell (1997), and Marcu (1997) suggest introducing penalty scores for each EDU based on the nucleus–satellite structure. In recent work, Schrimpf (2018) utilizes the topic relation to divide documents into sentences with similar topics. Every chunk of sentences is then summarized in extractive fashion, resulting in a concise summary that covers all of the topics discussed in the passage. Although the idea of using discourse information in summarization is not new, most work to date has focused on extractive summarization, where our focus is abstractive summarization. Gerani et al. (2014) used the parser of Joty et al. (2013) to RST-parse product reviews. By extracting graph-based features, important aspects are identified in the review and included in the summary based on a template-based generation framework. Although the experiment shows that the RST can be beneficial for content selection, the proposed feature is rule-based and highly tailored to review documents. Instead, in this work, we extract a latent representation of the discourse directly from the Yu et al. (2018) parser, and incorporate this into the abstractive summarizer. 2.2 Discourse Analysis for Document Clas"
U19-1010,P18-1013,0,0.0162431,"t; it is closer to how humans summarize, in that it generates paraphrases and blends multiple sentences in a coherent manner. Current sequence-to-sequence models for abstractive summarization work like neural machine translation models, in largely eschewing symbolic 5 https://github.com/XuezheMax/ NeuroNLP2 analysis and learning purely from training data. Pioneering work such as Rush et al. (2015), for instance, assumes the neural architecture is able to learn main sentence identification, discourse structure analysis, and paraphrasing all in one model. Studies such as Gehrmann et al. (2018); Hsu et al. (2018) attempt to incorporate additional supervision (e.g. content selection) to improve summarization. Although there are proposals that extend sequence-to-sequence models based on discourse structure — e.g. Cohan et al. (2018) include an additional attention layer for document sections — direct incorporation of discourse information is rarely explored. Hare and Borchardt (1984) observe four core activities involved in creating a summary: (1) topic sentence identification; (2) deletion of unnecessary details; (3) paragraph collapsing; and (4) paraphrasing and insertion of connecting words. Current"
U19-1010,P14-1002,0,0.565893,"nce training; (2) we empirically demonstrate that a latent representation of discourse structure enhances the summaries generated by an abstractive summarizer; and (3) we show that discourse structure is an essential factor in modelling the popularity of online petitions. 2 Related Work Discourse parsing, especially in the form of RST parsing, has been the target of research over a long period of time, including pre-neural feature engi1 The details of each relation can be found on the RST website http://www.sfu.ca/rst/index.html neering approaches (Hernault et al., 2010; Feng and Hirst, 2012; Ji and Eisenstein, 2014). Two approaches have been proposed to construct discourse parses: (1) bottom-up construction, where EDU merge operations are applied to single units; and (2) transition parser approaches, where the discourse tree is constructed as a sequence of parser actions. Neural sequence models have also been proposed. In early work, Li et al. (2016a) applied attention in an encoder–decoder framework and slightly improved on a classical featureengineering approach. The current state of the art is a neural transition-based discourse parser (Yu et al., 2018) which incorporates implicit syntax features obta"
U19-1010,P13-1048,0,0.0734756,"and Marcu (1997) suggest introducing penalty scores for each EDU based on the nucleus–satellite structure. In recent work, Schrimpf (2018) utilizes the topic relation to divide documents into sentences with similar topics. Every chunk of sentences is then summarized in extractive fashion, resulting in a concise summary that covers all of the topics discussed in the passage. Although the idea of using discourse information in summarization is not new, most work to date has focused on extractive summarization, where our focus is abstractive summarization. Gerani et al. (2014) used the parser of Joty et al. (2013) to RST-parse product reviews. By extracting graph-based features, important aspects are identified in the review and included in the summary based on a template-based generation framework. Although the experiment shows that the RST can be beneficial for content selection, the proposed feature is rule-based and highly tailored to review documents. Instead, in this work, we extract a latent representation of the discourse directly from the Yu et al. (2018) parser, and incorporate this into the abstractive summarizer. 2.2 Discourse Analysis for Document Classification Bhatia et al. (2015) show t"
U19-1010,D14-1220,0,0.0969918,"eural RST parser. It is most closely related to the work of Bhatia et al. (2015) and Lee et al. (2018), but intuitively, our discourse representations contain richer information, and we evaluate over more tasks such as popularity prediction of online petitions. 3 Discourse Feature Extraction To incorporate discourse information into our models (for summarization or document regression), we use the RST parser developed by Yu et al. (2018) to extract shallow and latent discourse features. The parser is competitive with other traditional parsers that use heuristic features (Feng and Hirst, 2012; Li et al., 2014; Ji and Eisenstein, 2014) and other neural network-based parsers (Li et al., 2016b). 3.1 Shallow Discourse Features Given a discourse tree produced by the RST parser (Yu et al., 2018), we compute several shallow features for an EDU: (1) the nuclearity score; (2) the relation score for each relation; and (3) the node type and that of its sibling. Intuitively, the nuclearity score measures how informative an EDU is, by calculating the (relative) number of ancestor nodes that are nuclei:2 P x∈ancestor(e) 1nucleus (x) h(root) where e is an EDU; h(x) gives the height from node x;3 and 1nucleus (x)"
U19-1010,D16-1035,0,0.527009,"been the target of research over a long period of time, including pre-neural feature engi1 The details of each relation can be found on the RST website http://www.sfu.ca/rst/index.html neering approaches (Hernault et al., 2010; Feng and Hirst, 2012; Ji and Eisenstein, 2014). Two approaches have been proposed to construct discourse parses: (1) bottom-up construction, where EDU merge operations are applied to single units; and (2) transition parser approaches, where the discourse tree is constructed as a sequence of parser actions. Neural sequence models have also been proposed. In early work, Li et al. (2016a) applied attention in an encoder–decoder framework and slightly improved on a classical featureengineering approach. The current state of the art is a neural transition-based discourse parser (Yu et al., 2018) which incorporates implicit syntax features obtained from a bi-affine dependency parser (Dozat and Manning, 2017). In this work, we employ this discourse parser to generate discourse representations. 2.1 Discourse and Summarization Research has shown that discourse parsing is valuable for summarization. Via the RST tree, the salience of a given text can be determined from the nuclearit"
U19-1010,W04-1013,0,0.0188753,"–100 words for beam search. Our experiment has two pointer–generator network baselines: (1) one without the coverage mechanism (“PG”); and (2) one with the coverage mechanism (“PG+Cov”; Section 4.1). For each baseline, we incorporate the latent and shallow discourse features separately in 3 ways (Section 4.2), giving us 6 additional results. We train the models for approximately 240,000270,000 iterations (13 epochs). When we include the coverage mechanism (second baseline), we train for an additional 3,000–3,500 iterations using the coverage penalty, following See et al. (2017). We use ROUGE (Lin, 2004) as our evaluation metric, which is a standard measure based on overlapping n-grams between the generated summary and the reference summary. We assess unigram (R-1), bigram (R-2), and longest-commonsubsequence (R-L) overlap, and present F1, recall and precision scores in Table 1. For the first baseline (PG), we see that incorporating discourse features consistently improves recall and F1. This observation is consistent irrespective of how (e.g. M1 or M2) and what (e.g. shallow or latent features) we add. These improvements do come at the expense of precision, with the exception of M2-latent (w"
U19-1010,W97-0713,0,0.637797,"ssical featureengineering approach. The current state of the art is a neural transition-based discourse parser (Yu et al., 2018) which incorporates implicit syntax features obtained from a bi-affine dependency parser (Dozat and Manning, 2017). In this work, we employ this discourse parser to generate discourse representations. 2.1 Discourse and Summarization Research has shown that discourse parsing is valuable for summarization. Via the RST tree, the salience of a given text can be determined from the nuclearity structure. In extractive summarization, Ono et al. (1994), O’Donnell (1997), and Marcu (1997) suggest introducing penalty scores for each EDU based on the nucleus–satellite structure. In recent work, Schrimpf (2018) utilizes the topic relation to divide documents into sentences with similar topics. Every chunk of sentences is then summarized in extractive fashion, resulting in a concise summary that covers all of the topics discussed in the passage. Although the idea of using discourse information in summarization is not new, most work to date has focused on extractive summarization, where our focus is abstractive summarization. Gerani et al. (2014) used the parser of Joty et al. (201"
U19-1010,C94-1056,0,0.468496,"framework and slightly improved on a classical featureengineering approach. The current state of the art is a neural transition-based discourse parser (Yu et al., 2018) which incorporates implicit syntax features obtained from a bi-affine dependency parser (Dozat and Manning, 2017). In this work, we employ this discourse parser to generate discourse representations. 2.1 Discourse and Summarization Research has shown that discourse parsing is valuable for summarization. Via the RST tree, the salience of a given text can be determined from the nuclearity structure. In extractive summarization, Ono et al. (1994), O’Donnell (1997), and Marcu (1997) suggest introducing penalty scores for each EDU based on the nucleus–satellite structure. In recent work, Schrimpf (2018) utilizes the topic relation to divide documents into sentences with similar topics. Every chunk of sentences is then summarized in extractive fashion, resulting in a concise summary that covers all of the topics discussed in the passage. Although the idea of using discourse information in summarization is not new, most work to date has focused on extractive summarization, where our focus is abstractive summarization. Gerani et al. (2014)"
U19-1010,D14-1162,0,0.0895044,"f examples and yˆi (yi ) the predicted (true) value. Note that in both cases, lower numbers are better. Similar to the abstractive summarization task, we experiment with incorporating the discourse features of the petition text to the petition regression model, under the hypothesis that discourse structure should benefit the model. 5.1 Deep Regression Model As before, our model is based on the model of Subramanian et al. (2018). The input is a concatenation of the petition’s title and content words, and the output is the log number of signatures. The input sequence is mapped to GloVe vectors (Pennington et al., 2014) and processed by several convolution filters with max-pooling to create a fixed-width hidden representation, which is then fed to fully connected layers and ultimately activated by an exponential linear unit to predict the output. The model is optimized with mean squared error (MSE). In addition to the MSE loss, the authors include an auxiliary ordinal regression objective that predicts the scale of signatures (e.g. {10, 100, 1000, 10000, 100000}), and found that it improves performance. Our model is based on the best model that utilizes both the MSE and ordinal regression loss. 5.2 Incorpora"
U19-1010,D15-1044,0,0.0387593,"sk of creating a concise version of a document that encapsulates its core content. Unlike extractive summarization, abstractive summarization has the ability to create new sentences that are not in the original document; it is closer to how humans summarize, in that it generates paraphrases and blends multiple sentences in a coherent manner. Current sequence-to-sequence models for abstractive summarization work like neural machine translation models, in largely eschewing symbolic 5 https://github.com/XuezheMax/ NeuroNLP2 analysis and learning purely from training data. Pioneering work such as Rush et al. (2015), for instance, assumes the neural architecture is able to learn main sentence identification, discourse structure analysis, and paraphrasing all in one model. Studies such as Gehrmann et al. (2018); Hsu et al. (2018) attempt to incorporate additional supervision (e.g. content selection) to improve summarization. Although there are proposals that extend sequence-to-sequence models based on discourse structure — e.g. Cohan et al. (2018) include an additional attention layer for document sections — direct incorporation of discourse information is rarely explored. Hare and Borchardt (1984) observ"
U19-1010,W18-0313,0,0.0150495,"l., 2018) which incorporates implicit syntax features obtained from a bi-affine dependency parser (Dozat and Manning, 2017). In this work, we employ this discourse parser to generate discourse representations. 2.1 Discourse and Summarization Research has shown that discourse parsing is valuable for summarization. Via the RST tree, the salience of a given text can be determined from the nuclearity structure. In extractive summarization, Ono et al. (1994), O’Donnell (1997), and Marcu (1997) suggest introducing penalty scores for each EDU based on the nucleus–satellite structure. In recent work, Schrimpf (2018) utilizes the topic relation to divide documents into sentences with similar topics. Every chunk of sentences is then summarized in extractive fashion, resulting in a concise summary that covers all of the topics discussed in the passage. Although the idea of using discourse information in summarization is not new, most work to date has focused on extractive summarization, where our focus is abstractive summarization. Gerani et al. (2014) used the parser of Joty et al. (2013) to RST-parse product reviews. By extracting graph-based features, important aspects are identified in the review and in"
U19-1010,P17-1099,0,0.457801,"example of a discourse tree, from (Yu et al., 2018); elab = elaboration; attr = attribute. work, we use our reimplementation of the state of the art neural RST parser of Yu et al. (2018), which is based on eighteen relations: purp, cont, attr, evid, comp, list, back, same, topic, mann, summ, cond, temp, eval, text, cause, prob, elab.1 This research investigates the impact of discourse representations obtained from an RST parser on natural language generation and document classification. We primarily experiment with an abstractive summarization model in the form of a pointer–generator network (See et al., 2017), focusing on two factors: (1) whether summarization benefits from discourse parsing; and (2) how a pointer–generator network guides the summarization model when discourse information is provided. For document classification, we investigate the content-based popularity prediction of online petitions with a deep regression model (Subramanian et al., 2018). We argue that document structure is a key predictor of the societal influence (as measured by signatures to the petition) of a document such as a petition. Our primary contributions are as follows: (1) we are the first to incorporate a neural"
U19-1010,P18-2030,1,0.940627,"ch investigates the impact of discourse representations obtained from an RST parser on natural language generation and document classification. We primarily experiment with an abstractive summarization model in the form of a pointer–generator network (See et al., 2017), focusing on two factors: (1) whether summarization benefits from discourse parsing; and (2) how a pointer–generator network guides the summarization model when discourse information is provided. For document classification, we investigate the content-based popularity prediction of online petitions with a deep regression model (Subramanian et al., 2018). We argue that document structure is a key predictor of the societal influence (as measured by signatures to the petition) of a document such as a petition. Our primary contributions are as follows: (1) we are the first to incorporate a neural discourse parser in sequence training; (2) we empirically demonstrate that a latent representation of discourse structure enhances the summaries generated by an abstractive summarizer; and (3) we show that discourse structure is an essential factor in modelling the popularity of online petitions. 2 Related Work Discourse parsing, especially in the form"
U19-1010,C18-1047,0,0.335395,"The Nucleus unit is considered more prominent than the Satellite, indicating that the Satellite is a supporting sentence for the Nucleus. Nuclearity relationships between two EDUs can take the following three forms: Nucleus–Satellite, Satellite–Nucleus, and Nucleus–Nucleus. In this elab attr EDU-1 EDU-2 elab EDU-3 EDU-4 EDU-1: American Telephone & Telegraph Co. said it EDU-2: will lay off 75 to 85 technicians here, effective Nov. 1 EDU-3: The workers install, maintain and repair its branch, EDU-4: which are large intracompany telephone networks Figure 1: An example of a discourse tree, from (Yu et al., 2018); elab = elaboration; attr = attribute. work, we use our reimplementation of the state of the art neural RST parser of Yu et al. (2018), which is based on eighteen relations: purp, cont, attr, evid, comp, list, back, same, topic, mann, summ, cond, temp, eval, text, cause, prob, elab.1 This research investigates the impact of discourse representations obtained from an RST parser on natural language generation and document classification. We primarily experiment with an abstractive summarization model in the form of a pointer–generator network (See et al., 2017), focusing on two factors: (1) whe"
W16-1609,S14-2010,0,0.012764,"est score in each row. Domain DLS headlines ans-forums ans-students belief images .83 .74 .77 .74 .86 doc2vec dbow dmpv .77 .78 .66 .65 .65 .60 .76 .75 .78 .75 word2vec sg cbow .74 .69 .62 .52 .69 .64 .72 .59 .73 .69 Description Dimension of word vectors Left/right context window size Minimum frequency threshold for word types Threshold to downsample high frequency words No. of negative word samples Number of training epochs Semantic Textual Similarity The Semantic Textual Similarity (STS) task is a shared task held as part of *SEM and SemEval over a number of iterations (Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). In STS, the goal is to automatically predict the similarity of a pair of sentences in the range [0, 5], where 0 indicates no similarity whatsoever and 5 indicates semantic equivalence. The top systems utilise word alignment, and further optimise their scores using supervised learning (Agirre et al., 2015). Word embeddings are employed, although sentence embeddings are often taken as the average of word embeddings (e.g. Sultan et al. (2015)). We evaluate doc2vec and word2vec embeddings over the English STS sub-task of SemEval2015 (Agirre et al., 2015). The dataset has 5"
W16-1609,N13-1092,0,0.0151087,"Missing"
W16-1609,S15-2027,0,0.0310091,"tic Textual Similarity (STS) task is a shared task held as part of *SEM and SemEval over a number of iterations (Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). In STS, the goal is to automatically predict the similarity of a pair of sentences in the range [0, 5], where 0 indicates no similarity whatsoever and 5 indicates semantic equivalence. The top systems utilise word alignment, and further optimise their scores using supervised learning (Agirre et al., 2015). Word embeddings are employed, although sentence embeddings are often taken as the average of word embeddings (e.g. Sultan et al. (2015)). We evaluate doc2vec and word2vec embeddings over the English STS sub-task of SemEval2015 (Agirre et al., 2015). The dataset has 5 domains, and each domain has 375–750 annotated pairs. Sentences are much shorter than our previous task, at an average of only 13 words in each test sentence. As the dataset is also much smaller, we combine sentences from all 5 domains and also sentences from previous years (2012–2014) to form the training data. We use the headlines domain from 2014 as development, and test on all 2015 domains. For pre-processing, we tokenise and lowercase the words using Stanfor"
W16-1609,P16-1158,1,0.826708,"Missing"
W16-1609,C14-1179,0,0.00949912,"dings were first proposed by Bengio et al. (2003), in the form of a feed-forward neural network language model. Modern methods use a simpler and more efficient neural architecture to learn word vectors (word2vec: Mikolov et al. (2013b); GloVe: Pennington et al. (2014)), based on objective functions that are designed specifically to produce high-quality vectors. Neural embeddings learnt by these methods have been applied in a myriad of NLP applications, including initialising neural network models for objective visual recognition (Frome et al., 2013) or machine translation (Zhang et al., 2014; Li et al., 2014), as well as directly modelling word-to-word relationships (Mikolov et al., 1 The term doc2vec was popularised by Gensim ˇ uˇrek and Sojka, 2010), a widely-used implementation of (Reh˚ paragraph vectors: https://radimrehurek.com/gensim/ 2 The authors of Gensim found dbow outperforms dmpv: https://github.com/piskvorky/gensim/blob/ develop/docs/notebooks/doc2vec-IMDB.ipynb 3 https://groups.google.com/forum/#!topic/ gensim/bEskaT45fXQ 4 For a detailed discussion on replicating the results of Le and Mikolov (2014), see: https://groups.google.com/ forum/#!topic/word2vec-toolkit/Q49FIrNOQRo 78 Proce"
W16-1609,P14-1011,0,0.0121569,"duction Neural embeddings were first proposed by Bengio et al. (2003), in the form of a feed-forward neural network language model. Modern methods use a simpler and more efficient neural architecture to learn word vectors (word2vec: Mikolov et al. (2013b); GloVe: Pennington et al. (2014)), based on objective functions that are designed specifically to produce high-quality vectors. Neural embeddings learnt by these methods have been applied in a myriad of NLP applications, including initialising neural network models for objective visual recognition (Frome et al., 2013) or machine translation (Zhang et al., 2014; Li et al., 2014), as well as directly modelling word-to-word relationships (Mikolov et al., 1 The term doc2vec was popularised by Gensim ˇ uˇrek and Sojka, 2010), a widely-used implementation of (Reh˚ paragraph vectors: https://radimrehurek.com/gensim/ 2 The authors of Gensim found dbow outperforms dmpv: https://github.com/piskvorky/gensim/blob/ develop/docs/notebooks/doc2vec-IMDB.ipynb 3 https://groups.google.com/forum/#!topic/ gensim/bEskaT45fXQ 4 For a detailed discussion on replicating the results of Le and Mikolov (2014), see: https://groups.google.com/ forum/#!topic/word2vec-toolkit/Q4"
W16-1609,P14-5010,0,0.00835232,"ate doc2vec and word2vec embeddings over the English STS sub-task of SemEval2015 (Agirre et al., 2015). The dataset has 5 domains, and each domain has 375–750 annotated pairs. Sentences are much shorter than our previous task, at an average of only 13 words in each test sentence. As the dataset is also much smaller, we combine sentences from all 5 domains and also sentences from previous years (2012–2014) to form the training data. We use the headlines domain from 2014 as development, and test on all 2015 domains. For pre-processing, we tokenise and lowercase the words using Stanford CoreNLP (Manning et al., 2014). As a benchmark, we include results from the overall top-performing system in the competition, referred to as “DLS” (Sultan et al., 2015). Note, however, that this system is supervised and highly customised to the task, whereas our methods are completely unsupervised. Results are presented in Table 2. Unsurprisingly, we do not exceed the overall performance of the supervised benchmark system DLS, although doc2vec outperforms DLS over ngram .61 .50 .65 .67 .62 Table 2: Pearson’s r of the STS task across 5 domains. DLS is the overall best system in the competition. Boldface indicates the best r"
W16-1609,D14-1162,0,0.121305,"Missing"
W16-1609,N15-1099,1,0.679102,"Missing"
W17-1002,W04-1013,0,0.0146504,"coder, so ot = Wo xt + Uo ht−1 + bo jt = Wj xt + Uj ht−1 + bj (1) ct = ct−1 ∗ σ(ft ) + tanh(jt ) ∗ σ(it ) ht = tanh(ct ) ∗ σ(ot ) 2 The unnormalised attention weights are computed by combining ht−1 with the convolutional embedding of each source word via dot product. where it , ft and ot are input, forget and output gates, respectively; jt , ct and ht represent the new 8 Combination add-input add-hidden stack-input stack-hidden mlp-input mlp-hidden Equation i0t = it + d h0t = ht + d i0t = [it ; d] h0t = [ht ; d] i0t = tanh(Wi it + Wd d + b) h0t = tanh(Wi ht + Wd d + b) using the ROUGE metric (Lin, 2004), following the same evaluation style of benchmark systems (Rush et al., 2015; Chopra et al., 2016). For outof-domain experiments, we use the same models trained from G IGAWORD, but tune using DUC 03 and test on DUC 04; DUC 03 and DUC 04 each have 500 examples. For the doc2vec encoder, we train using G IGA WORD and infer document vectors for validation and test examples using the trained model. Valid and test examples are excluded from the doc2vec training data. Table 1: Incorporation of doc2vec signal in the decoder. d denotes the doc2vec vector; it (ht ) is the input (hidden) vector at time"
W17-1002,D15-1044,0,0.571169,"the document vector to a recurrent neural network decoder. With this decoupled architecture, we decrease the number of parameters in the decoder substantially, and shorten its training time. Experiments show that the decoupled model achieves comparable performance with state-of-the-art models for in-domain documents, but less well for out-of-domain documents. 1 Introduction Abstractive document summarization is a challenging natural language understanding task. Abstractive methods first encode the original document into a high-level representation, and then decode it into the target summary. Rush et al. (2015) proposed the task of headline generation as the first step towards abstractive summarization. Instead of using the full document, the authors experimented with using the first sentence as input, with the aim of generating a coherent headline given the sentence. 2 Attentive Recurrent Neural Network: A Joint Encoder–decoder Architecture The attentive recurrent neural network is composed of an attentive encoder and a recurrent decoder (Chopra et al., 2016), where the encoder is 1 The training time is decreased from 4 days (with full G I GAWORD ) for the coupled model (Rush et al., 2015) to 2 day"
W17-1002,N16-1012,0,0.13036,"nh(jt ) ∗ σ(it ) ht = tanh(ct ) ∗ σ(ot ) 2 The unnormalised attention weights are computed by combining ht−1 with the convolutional embedding of each source word via dot product. where it , ft and ot are input, forget and output gates, respectively; jt , ct and ht represent the new 8 Combination add-input add-hidden stack-input stack-hidden mlp-input mlp-hidden Equation i0t = it + d h0t = ht + d i0t = [it ; d] h0t = [ht ; d] i0t = tanh(Wi it + Wd d + b) h0t = tanh(Wi ht + Wd d + b) using the ROUGE metric (Lin, 2004), following the same evaluation style of benchmark systems (Rush et al., 2015; Chopra et al., 2016). For outof-domain experiments, we use the same models trained from G IGAWORD, but tune using DUC 03 and test on DUC 04; DUC 03 and DUC 04 each have 500 examples. For the doc2vec encoder, we train using G IGA WORD and infer document vectors for validation and test examples using the trained model. Valid and test examples are excluded from the doc2vec training data. Table 1: Incorporation of doc2vec signal in the decoder. d denotes the doc2vec vector; it (ht ) is the input (hidden) vector at time t; and “[·; ·]” denotes vector concatenation. 4.2 input, new context and new hidden state, respecti"
W17-1002,N16-1149,1,0.834556,"Table 1: Incorporation of doc2vec signal in the decoder. d denotes the doc2vec vector; it (ht ) is the input (hidden) vector at time t; and “[·; ·]” denotes vector concatenation. 4.2 input, new context and new hidden state, respectively; ∗ is the elementwise vector product; and σ is the sigmoid activation function. Given an input word and previous hidden state, the decoder predicts the next word and generates the summary one word at a time. To generate summaries that are related to the document, we incorporate the doc2vec input document signal to the decoder using several methods proposed by Hoang et al. (2016). There are two layers where we can incorporate doc2vec: in the input layer (input), or hidden layer (hidden). There are three methods of incorporation: addition (add), stacking (stack), or via a multilayer perceptron (mlp). Table 1 illustrates the 6 possible approaches to incorporation. Note that add requires doc2vec to have the same vector dimensionality as the layer it is combined with, and stack-hidden doubles the hidden size (assuming they have the same dimensions), resulting in a large output projection matrix and longer training time. 4 4.1 Hyper-parameter tuning For the encoder, we exp"
W17-1002,W16-1609,1,\N,Missing
W18-6119,D15-1075,0,0.0188086,"xt Only We experiment with two methods for constructing our text embeddings: an attentional approach, and a benchmark approach using a simple paragraph vector representation. 3 139 https://www.tensorflow.org/ Let G be a feed-forward network with relu activations. We define the representation for each word as follows: 4.1.1 Decompositional Attentional Model Parikh et al. (2016) proposed a decompositional attentional model for identifying near-duplicate questions. It is based on a bag-of-words model, and has been shown to perform well over the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015; Tomar et al., 2017). We adapt their architecture for our task, running it on question/answer pairs instead of entailment pairs. Note that, in our case, the best answer is in no way expected to be a near-duplicate of the question, and rather, the attention mechanism over word embeddings is used to bridge the “lexical gap” between questions and answers (Shtok et al., 2012), as well as to automatically determine the sorts of answer words that are likely to align with particular question words. Henceforth we refer to our adapted model as “decatt”. The model works as follows: first it attends wor"
W18-6119,P14-1092,0,0.0608892,"Missing"
W18-6119,W16-1616,0,0.0184598,"P Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 137–147 c Brussels, Belgium, Nov 1, 2018. 2018 Association for Computational Linguistics compared using cosine similarity. The resulting score was used as a feature in a learn-to-rank setup, together with a set of hand-crafted features including metadata, which did not have a positive effect on the results. In another approach, Bao and Wu (2016) mapped questions and answers to multiple lower dimensional layers of variable size. They then used a 3-way tensor transformation to combine the layers and produce one output layer. Nassif et al. (2016) used stacked bidirectional LSTMs with a multilayer perceptron on top, with the addition of a number of extra features including a small number of metadata features, to classify and re-rank answers. Although the model performed well, it was no better than a far simpler classification model using only features based on text (Belinkov et al., 2015). Compared to these past deep learning approaches for answer retrieval, our work differs in that we include metadata features directly within our deep learning model. We include a large number of such features and show, contrary to the results of previ"
W18-6119,N16-1084,0,0.0610742,"Missing"
W18-6119,D16-1244,0,0.052512,"Missing"
W18-6119,D14-1162,0,0.0829498,"5 Experiments To train our models, we used the Adam Optimiser (Kingma and Ba, 2014). For decatt, we used dropout over F, G, H after every dense layer. For the doc2vec MLP, we included batch normalisation before, and dropout after, each dense layer. For testing, we picked the best model according to the validation results after the end of each epoch. The parameters for decatt were initialised with a Gaussian distribution of mean 0 and variance 0.01, and for the doc2vec MLP we used Glorot normal initialization. For Stack Overflow, the parameters for Word embeddings were pretrained using GloVe (Pennington et al., 2014) with the F ULL data (by combining all questions and answers in the sequence they appeared) for 50 epochs. Word embeddings were set to 150 dimensions. The co-occurrence weighting function’s maximum value xmax was kept at the default of 10. For S EM E VAL, we used pretrained Common Crawl cased embeddings with 840G tokens and 300 dimensions (Pennington et al., 2014). To train the decatt model for Stack Overflow we split the data into 3 partitions based on the size of the question/answer text, with separate partitions where the total length of the question/answer text was: (1) ≤ 500 words; (2) >"
W18-6119,S17-2058,0,0.015578,"e develop two novel benchmark datasets for cQA answer ranking/selection; • we adapt a deep learning method proposed for near-duplicate/paraphrase detection, and achieve state-of-the-art results for text-based answer selection; and • we demonstrate that metadata is critical in identifying preferred answers, but at the same time text-based representations complement metadata to achieve the best overall results for the task. The data and code used in this research will be made available on acceptance. 2 Related work The work that is most closely related to ours is Bogdanova and Foster (2016) and Koreeda et al. (2017). In this first case, Le and Mikolov’s paragraph2vec was used to convert question–answer pairs into fixed-size vectors in a word-embedding vector space, which were then fed into a simple feed-forward neural network. In the second case, a decompositional attentional model is applied to the SemEval question–comment re-ranking task, and achieved respectable results for text alone. We improve on the standalone results for these two methods through better training and hyperparameter optimisation. We additionally extend both methods by incorporating metadata features in the training of the neural mo"
W18-6119,W16-1609,1,0.923095,"on and answer by summing them: v1 = Plb k=1 exp(ei,k ) la X v1,i ; i=1 v2 = lb X v2,i j=1 Finally, we concatenate both vectors, vtext = [v1 ; v2 ]. This text vector is used as the input in the classification network H. 4.1.2 Paragraph Vectors Our second approach uses the method of Bogdanova and Foster (2016), who achieved state-ofthe-art performance on the Yahoo! Answers corpus of Jansen et al. (2014). The method, which we will refer to as “doc2vec”, works by independently constructing vector representations of both the question and answer texts, using paragraph vectors (Le and Mikolov, 2014; Lau and Baldwin, 2016) in the same vector space. The training is unsupervised, only requiring an unlabelled pretraining corpus to learn the vectors. The doc2vec method is an extension of word2vec (Mikolov et al., 2013) for learning document embeddings. The document embeddings are generated along with word embeddings in the same vector space. word2vec learns word embeddings that can separate the words appearing in contexts of the target word from randomly sampled words, while doc2vec learns document embeddings that can separate the words appearing in the document from randomly sampled words. Given the doc2vec questi"
W18-6119,W17-4121,0,0.0313843,"Missing"
W18-6119,P14-5010,0,0.0103402,"and Foster (2016). The two evaluation datasets, which we denote as “S MALL” and “L ARGE”, contain 10K and 70K questions, respectively, each with a predefined 50/25/25 split between train, val, and test questions. On average, there are approximately six answers per question. In addition to the sampled sub-sets, we also used the full Stack Overflow dump (containing a full month of questions and answers) for pretraining; we will refer to this dataset as “F ULL”. This full dataset consists of approximately 300K questions and 1M answers. In all cases, we tokenised the text using Stanford CoreNLP (Manning et al., 2014). Stack Overflow contains rich metadata, including user-level information and question- and answer-specific data. We leverage this metadata in our model, as detailed in Section 4.2. Summary statistics of S MALL, L ARGE and F ULL are presented in Table 1. In addition to the Stack Overflow dataset, we also experiment with an additional complementary dataset: the S EM E VAL 2017 Task 3A QuestionComment reranking dataset (Nakov et al., 2017). 4 Methodology We treat the answer ranking problem as a classification problem, where given a question/answer pair, the model tries to predict how likely the"
W18-6119,P10-1125,0,0.0365423,"SemEval question–comment re-ranking task, and achieved respectable results for text alone. We improve on the standalone results for these two methods through better training and hyperparameter optimisation. We additionally extend both methods by incorporating metadata features in the training of the neural model, instead of extracting neural features for use in a non-deep learning model, as is commonly done in re-ranking tasks (Koreeda et al., 2017). In addition to this, there is a variety of other recent work on deep learning methods for answer ranking or best answer selection. For instance, Wang et al. (2010) used a network based on restricted Bolzmann machines (Hinton, 2002), using binary vectors of the most frequent words in the training data as input. This model was trained by trying to reconstruct question vectors from answer vectors, then at test time question vectors were compared against answer vectors to determine their relevance. Elsewhere, Zhou et al. (2016) used Denoising Auto-Encoders (Vincent et al., 2008) to learn how to map both questions and answers to lowdimensional representations, which were then 138 (2009), Wang et al. (2009) and Joty et al. (2016). In this kind of approach, qu"
