2021.iwslt-1.1,{FINDINGS} {OF} {THE} {IWSLT} 2021 {EVALUATION} {CAMPAIGN},2021,-1,-1,10,0,832,antonios anastasopoulos,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2021) featured this year four shared tasks: (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Multilingual speech translation, (iv) Low-resource speech translation. A total of 22 teams participated in at least one of the tasks. This paper describes each shared task, data and evaluation metrics, and reports results of the received submissions."
2021.iwslt-1.15,Maastricht University{'}s Multilingual Speech Translation System for {IWSLT} 2021,2021,-1,-1,2,1,5771,danni liu,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"This paper describes Maastricht University{'}s participation in the IWSLT 2021 multilingual speech translation track. The task in this track is to build multilingual speech translation systems in supervised and zero-shot directions. Our primary system is an end-to-end model that performs both speech transcription and translation. We observe that the joint training for the two tasks is complementary especially when the speech translation data is scarce. On the source and target side, we use data augmentation and pseudo-labels respectively to improve the performance of our systems. We also introduce an ensembling technique that consistently improves the quality of transcriptions and translations. The experiments show that the end-to-end system is competitive with its cascaded counterpart especially in zero-shot conditions."
2021.eacl-tutorials.3,Tutorial Proposal: End-to-End Speech Translation,2021,-1,-1,1,1,5714,jan niehues,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts,0,"Speech translation is the translation of speech in one language typically to text in another, traditionally accomplished through a combination of automatic speech recognition and machine translation. Speech translation has attracted interest for many years, but the recent successful applications of deep learning to both individual tasks have enabled new opportunities through joint modeling, in what we today call {`}end-to-end speech translation.{'} In this tutorial we will introduce the techniques used in cutting-edge research on speech translation. Starting from the traditional cascaded approach, we will given an overview on data sources and model architectures to achieve state-of-the art performance with end-to-end speech translation for both high- and low-resource languages. In addition, we will discuss methods to evaluate analyze the proposed solutions, as well as the challenges faced when applying speech translation models for real-world applications."
2021.eacl-main.70,Continuous Learning in Neural Machine Translation using Bilingual Dictionaries,2021,-1,-1,1,1,5714,jan niehues,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"While recent advances in deep learning led to significant improvements in machine translation, neural machine translation is often still not able to continuously adapt to the environment. For humans, as well as for machine translation, bilingual dictionaries are a promising knowledge source to continuously integrate new knowledge. However, their exploitation poses several challenges: The system needs to be able to perform one-shot learning as well as model the morphology of source and target language. In this work, we proposed an evaluation framework to assess the ability of neural machine translation to continuously learn new phrases. We integrate one-shot learning methods for neural machine translation with different word representations and show that it is important to address both in order to successfully make use of bilingual dictionaries. By addressing both challenges we are able to improve the ability to translate new, rare words and phrases from 30{\%} to up to 70{\%}. The correct lemma is even generated by more than 90{\%}."
2021.dravidianlangtech-1.7,Unsupervised Machine Translation On {D}ravidian Languages,2021,-1,-1,3,0,11147,sai koneru,Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages,0,"Unsupervised Neural Machine translation (UNMT) is beneficial especially for under-resourced languages such as from the Dravidian family. They learn to translate between the source and target, relying solely on only monolingual corpora. However, UNMT systems fail in scenarios that occur often when dealing with low resource languages. Recent works have achieved state-of-the-art results by adding auxiliary parallel data with similar languages. In this work, we focus on unsupervised translation between English and Kannada by using limited amounts of auxiliary data between English and other Dravidian languages. We show that transliteration is essential in unsupervised translation between Dravidian languages, as they do not share a common writing system. We explore several model architectures that use the auxiliary data in order to maximize knowledge sharing and enable UNMT for dissimilar language pairs. We show from our experiments it is crucial for Kannada and reference languages to be similar. Further, we propose a method to measure language similarity to choose the most beneficial reference languages."
2021.acl-long.101,Improving Zero-Shot Translation by Disentangling Positional Information,2021,-1,-1,2,1,5771,danni liu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Multilingual neural machine translation has shown the capability of directly translating between language pairs unseen in training, i.e. zero-shot translation. Despite being conceptually attractive, it often suffers from low output quality. The difficulty of generalizing to new translation directions suggests the model representations are highly specific to those language pairs seen in training. We demonstrate that a main factor causing the language-specific representations is the positional correspondence to input tokens. We show that this can be easily alleviated by removing residual connections in an encoder layer. With this modification, we gain up to 18.5 BLEU points on zero-shot translation while retaining quality on supervised directions. The improvements are particularly prominent between related languages, where our proposed model outperforms pivot-based translation. Moreover, our approach allows easy integration of new languages, which substantially expands translation coverage. By thorough inspections of the hidden layer outputs, we show that our approach indeed leads to more language-independent representations."
2020.iwslt-1.1,{FINDINGS} {OF} {THE} {IWSLT} 2020 {EVALUATION} {CAMPAIGN},2020,-1,-1,16,0,11026,ebrahim ansari,Proceedings of the 17th International Conference on Spoken Language Translation,0,"The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2020) featured this year six challenge tracks: (i) Simultaneous speech translation, (ii) Video speech translation, (iii) Offline speech translation, (iv) Conversational speech translation, (v) Open domain translation, and (vi) Non-native speech translation. A total of teams participated in at least one of the tracks. This paper introduces each track{'}s goal, data and evaluation metrics, and reports the results of the received submissions."
2020.iwslt-1.30,Adapting End-to-End Speech Recognition for Readable Subtitles,2020,26,0,2,1,5771,danni liu,Proceedings of the 17th International Conference on Spoken Language Translation,0,"Automatic speech recognition (ASR) systems are primarily evaluated on transcription accuracy. However, in some use cases such as subtitling, verbatim transcription would reduce output readability given limited screen size and reading time. Therefore, this work focuses on ASR with output compression, a task challenging for supervised approaches due to the scarcity of training data. We first investigate a cascaded system, where an unsupervised compression model is used to post-edit the transcribed speech. We then compare several methods of end-to-end speech recognition under output length constraints. The experiments show that with limited data far less than needed for training a model from scratch, we can adapt a Transformer-based ASR model to incorporate both transcription and compression capabilities. Furthermore, the best performance in terms of WER and ROUGE scores is achieved by explicitly modeling the length constraints within the end-to-end ASR system."
2020.amta-research.3,Machine Translation with Unsupervised Length-Constraints,2020,25,1,1,1,5714,jan niehues,Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track),0,"We have seen significant improvements in machine translation due to the usage of deep learning. While the improvements in translation quality are impressive, the encoder-decoder architecture enables many more possibilities. In this paper, we explore one of these, the generation of constraint translation. We focus on length constraints, which are essential if the translation should be displayed in a given format. In this work, we propose an end-to-end approach for this task. Compared to a traditional method that first translates and then performs sentence compression, the text compression is learned completely unsupervised. By combining the idea with zero-shot multilingual machine translation, we are also able to perform unsupervised monolingual sentence compression. In order to fulfill the length constraints, we investigated several methods to integrate the constraints into the model. Using the presented technique, we are able to significantly improve the translation quality under constraints. Furthermore, we are able to perform unsupervised monolingual sentence compression."
W19-8671,Modeling Confidence in Sequence-to-Sequence Models,2019,11,0,1,1,5714,jan niehues,Proceedings of the 12th International Conference on Natural Language Generation,0,"Recently, significant improvements have been achieved in various natural language processing tasks using neural sequence-to-sequence models. While aiming for the best generation quality is important, ultimately it is also necessary to develop models that can assess the quality of their output. In this work, we propose to use the similarity between training and test conditions as a measure for models{'} confidence. We investigate methods solely using the similarity as well as methods combining it with the posterior probability. While traditionally only target tokens are annotated with confidence measures, we also investigate methods to annotate source tokens with confidence. By learning an internal alignment model, we can significantly improve confidence projection over using state-of-the-art external alignment tools. We evaluate the proposed methods on downstream confidence estimation for machine translation (MT). We show improvements on segment-level confidence estimation as well as on confidence estimation for source tokens. In addition, we show that the same methods can also be applied to other tasks using sequence-to-sequence models. On the automatic speech recognition (ASR) task, we are able to find 60{\%} of the errors by looking at 20{\%} of the data."
W19-5202,Improving Zero-shot Translation with Language-Independent Constraints,2019,26,0,2,1,5766,ngocquan pham,Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers),0,"An important concern in training multilingual neural machine translation (NMT) is to translate between language pairs unseen during training, i.e zero-shot translation. Improving this ability kills two birds with one stone by providing an alternative to pivot translation which also allows us to better understand how the model captures information between languages. In this work, we carried out an investigation on this capability of the multilingual NMT models. First, we intentionally create an encoder architecture which is independent with respect to the source language. Such experiments shed light on the ability of NMT encoders to learn multilingual representations, in general. Based on such proof of concept, we were able to design regularization methods into the standard Transformer model, so that the whole architecture becomes more robust in zero-shot conditions. We investigated the behaviour of such models on the standard IWSLT 2017 multilingual dataset. We achieved an average improvement of 2.23 BLEU points across 12 language pairs compared to the zero-shot performance of a state-of-the-art multilingual system. Additionally, we carry out further experiments in which the effect is confirmed even for language pairs with multiple intermediate pivots."
Q19-1020,Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation,2019,2,17,3,0.559566,10828,matthias sperber,Transactions of the Association for Computational Linguistics,0,"Speech translation has traditionally been approached through cascaded models consisting of a speech recognizer trained on a corpus of transcribed speech, and a machine translation system trained on parallel texts. Several recent works have shown the feasibility of collapsing the cascade into a single, direct model that can be trained in an end-to-end fashion on a corpus of translated speech. However, experiments are inconclusive on whether the cascade or the direct model is stronger, and have only been conducted under the unrealistic assumption that both are trained on equal amounts of data, ignoring other available speech recognition and machine translation corpora. In this paper, we demonstrate that direct speech translation models require more data to perform well than cascaded models, and although they allow including auxiliary data through multi-task training, they are poor at exploiting such data, putting them at a severe disadvantage. As a remedy, we propose the use of end- to-end trainable models with two attention mechanisms, the first establishing source speech to source text alignments, the second modeling source to target text alignment. We show that such models naturally decompose into multi-task{--}trainable recognition and translation tasks and propose an attention-passing technique that alleviates error propagation issues in a previous formulation of a model with two attention stages. Our proposed model outperforms all examined baselines and is able to exploit auxiliary training data much more effectively than direct attentional models."
D19-5535,Incremental processing of noisy user utterances in the spoken language understanding task,2019,15,0,2,0,26594,stefan constantin,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),0,"The state-of-the-art neural network architectures make it possible to create spoken language understanding systems with high quality and fast processing time. One major challenge for real-world applications is the high latency of these systems caused by triggered actions with high executions times. If an action can be separated into subactions, the reaction time of the systems can be improved through incremental processing of the user utterance and starting subactions while the utterance is still being uttered. In this work, we present a model-agnostic method to achieve high quality in processing incrementally produced partial utterances. Based on clean and noisy versions of the ATIS dataset, we show how to create datasets with our method to create low-latency natural language understanding components. We get improvements of up to 47.91 absolute percentage points in the metric F1-score."
W18-6422,The Karlsruhe Institute of Technology Systems for the News Translation Task in {WMT} 2018,2018,0,1,2,1,5766,ngocquan pham,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We present our experiments in the scope of the news translation task in WMT 2018, in directions: EnglishâGerman. The core of our systems is the encoder-decoder based neural machine translation models using the transformer architecture. We enhanced the model with a deeper architecture. By using techniques to limit the memory consumption, we were able to train models that are 4 times larger on one GPU and improve the performance by 1.2 BLEU points. Furthermore, we performed sentence selection for the newly available ParaCrawl corpus. Thereby, we could improve the effectiveness of the corpus by 0.5 BLEU points."
W18-2712,Towards one-shot learning for rare-word translation with external experts,2018,0,3,2,1,5766,ngocquan pham,Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,0,"Neural machine translation (NMT) has significantly improved the quality of automatic translation models. One of the main challenges in current systems is the translation of rare words. We present a generic approach to address this weakness by having external models annotate the training data as Experts, and control the model-expert interaction with a pointer network and reinforcement learning. Our experiments using phrase-based models to simulate Experts to complement neural machine translation models show that the model can be trained to copy the annotations into the output consistently. We demonstrate the benefit of our proposed framework in outof domain translation scenarios with only lexical resources, improving more than 1.0 BLEU point in both translation directions English-Spanish and German-English."
W18-2606,Robust and Scalable Differentiable Neural Computer for Question Answering,2018,0,6,2,0,28407,jorg franke,Proceedings of the Workshop on Machine Reading for Question Answering,0,"Deep learning models are often not easily adaptable to new tasks and require task-specific adjustments. The differentiable neural computer (DNC), a memory-augmented neural network, is designed as a general problem solver which can be used in a wide range of tasks. But in reality, it is hard to apply this model to new tasks. We analyze the DNC and identify possible improvements within the application of question answering. This motivates a more robust and scalable DNC (rsDNC). The objective precondition is to keep the general character of this model intact while making its application more reliable and speeding up its required training time. The rsDNC is distinguished by a more robust training, a slim memory unit and a bidirectional architecture. We not only achieve new state-of-the-art performance on the bAbI task, but also minimize the performance variance between different initializations. Furthermore, we demonstrate the simplified applicability of the rsDNC to new tasks with passable results on the CNN RC task without adaptions."
L18-1318,Automated Evaluation of Out-of-Context Errors,2018,6,0,2,0,4238,patrick huber,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We present a new approach to evaluate computational models for the task of text understanding by the means of out-of-context error detection. Through the novel design of our automated modification process, existing large-scale data sources can be adopted for a vast number of text understanding tasks. The data is thereby altered on a semantic level, allowing models to be tested against a challenging set of modified text passages that require to comprise a broader narrative discourse. Our newly introduced task targets actual real-world problems of transcription and translation systems by inserting authentic out-of-context errors. The automated modification process is applied to the 2016 TEDTalk corpus. Entirely automating the process allows the adoption of complete datasets at low cost, facilitating supervised learning procedures and deeper networks to be trained and tested. To evaluate the quality of the modification algorithm a language model and a supervised binary classification model are trained and tested on the altered dataset. A human baseline evaluation is examined to compare the results with human performance. The outcome of the evaluation task indicates the difficulty to detect semantic errors for machine-learning algorithms and humans, showing that the errors cannot be identified when limited to a single sentence."
L18-1616,{KIT}-Multi: A Translation-Oriented Multilingual Embedding Corpus,2018,0,0,2,1,5767,thanhle ha,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-2020,{KIT} Lecture Translator: Multilingual Speech Translation with One-Shot Learning,2018,0,2,4,0,30723,florian dessloch,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"In today{'}s globalized world we have the ability to communicate with people across the world. However, in many situations the language barrier still presents a major issue. For example, many foreign students coming to KIT to study are initially unable to follow a lecture in German. Therefore, we offer an automatic simultaneous interpretation service for students. To fulfill this task, we have developed a low-latency translation system that is adapted to lectures and covers several language pairs. While the switch from traditional Statistical Machine Translation to Neural Machine Translation (NMT) significantly improved performance, to integrate NMT into the speech translation framework required several adjustments. We have addressed the run-time constraints and different types of input. Furthermore, we utilized one-shot learning to easily add new topic-specific terms to the system. Besides better performance, NMT also enabled us increase our covered languages through multilingual NMT. {\%} Combining these techniques, we are able to provide an adapted speech translation system for several European languages."
W17-4708,Exploiting Linguistic Resources for Neural Machine Translation Using Multi-task Learning,2017,20,18,1,1,5714,jan niehues,Proceedings of the Second Conference on Machine Translation,0,"Linguistic resources such as part-of-speech (POS) tags have been extensively used in statistical machine translation (SMT) frameworks and have yielded better performances. However, usage of such linguistic annotations in neural machine translation (NMT) systems has been left under-explored. n In this work, we show that multi-task learning is a successful and a easy approach to introduce an additional knowledge into an end-to-end neural attentional model. By jointly training several natural language processing (NLP) tasks in one system, we are able to leverage common information and improve the performance of the individual task. n We analyze the impact of three design decisions in multi-task learning: the tasks used in training, the training schedule, and the degree of parameter sharing across the tasks, which is defined by the network architecture. The experiments are conducted for an German to English translation task. As additional linguistic resources, we exploit POS information and named-entities (NE). Experiments show that the translation quality can be improved by up to 1.5 BLEU points under the low-resource condition. The performance of the POS tagger is also improved using the multi-task learning scheme."
W17-4734,The {QT}21 Combined Machine Translation System for {E}nglish to {L}atvian,2017,0,0,5,0.512486,30412,janthorsten peter,Proceedings of the Second Conference on Machine Translation,0,None
W17-4736,The Karlsruhe Institute of Technology Systems for the News Translation Task in {WMT} 2017,2017,0,2,2,1,5766,ngocquan pham,Proceedings of the Second Conference on Machine Translation,0,None
W17-3202,Analyzing Neural {MT} Search and Model Performance,2017,3,1,1,1,5714,jan niehues,Proceedings of the First Workshop on Neural Machine Translation,0,"In this paper, we offer an in-depth analysis about the modeling and search performance. We address the question if a more complex search algorithm is necessary. Furthermore, we investigate the question if more complex models which might only be applicable during rescoring are promising. By separating the search space and the modeling using n-best list reranking, we analyze the influence of both parts of an NMT system independently. By comparing differently performing NMT systems, we show that the better translation is already in the search space of the translation systems with less performance. This results indicate that the current search algorithms are sufficient for the NMT systems. Furthermore, we could show that even a relatively small $n$-best list of 50 hypotheses already contain notably better translations."
D17-1145,Neural Lattice-to-Sequence Models for Uncertain Inputs,2017,0,6,3,1,10828,matthias sperber,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"The input to a neural sequence-to-sequence model is often determined by an up-stream system, e.g. a word segmenter, part of speech tagger, or speech recognizer. These up-stream models are potentially error-prone. Representing inputs through word lattices allows making this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form. In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoder-decoder model. We integrate lattice posterior scores into this architecture by extending the TreeLSTM{'}s child-sum and forget gates and introducing a bias term into the attention mechanism. We experiment with speech translation lattices and report consistent improvements over baselines that translate either the 1-best hypothesis or the lattice without posterior scores."
W16-2314,The Karlsruhe Institute of Technology Systems for the News Translation Task in {WMT} 2016,2016,24,4,3,1,5767,thanhle ha,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2320,The {QT}21/{H}im{L} Combined Machine Translation System,2016,5,6,13,0.512486,30412,janthorsten peter,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the joint submission of the QT21 and HimL projects for the Englishxe2x86x92Romanian translation task of the ACL 2016 First Conference on Machine Translation (WMT 2016). The submission is a system combination which combines twelve different statistical machine translation systems provided by the different groups (RWTH Aachen University, LMU Munich, Charles University in Prague, University of Edinburgh, University of Sheffield, Karlsruhe Institute of Technology, LIMSI, University of Amsterdam, Tilde). The systems are combined using RWTHxe2x80x99s system combination approach. The final submission shows an improvement of 1.0 BLEU compared to the best single system on newstest2016."
W16-2208,Using Factored Word Representation in Neural Network Language Models,2016,19,5,1,1,5714,jan niehues,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,None
N16-3017,Lecture Translator - Speech translation framework for simultaneous lecture translation,2016,11,4,3,0.833333,30108,markus muller,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,None
C16-1172,Pre-Translation for Neural Machine Translation,2016,20,27,1,1,5714,jan niehues,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Recently, the development of neural machine translation (NMT) has significantly improved the translation quality of automatic machine translation. While most sentences are more accurate and fluent than translations by statistical machine translation (SMT)-based systems, in some cases, the NMT system produces translations that have a completely different meaning. This is especially the case when rare words occur. When using statistical machine translation, it has already been shown that significant gains can be achieved by simplifying the input in a preprocessing step. A commonly used example is the pre-reordering approach. In this work, we used phrase-based machine translation to pre-translate the input into the target language. Then a neural machine translation system generates the final hypothesis using the pre-translation. Thereby, we use either only the output of the phrase-based machine translation (PBMT) system or a combination of the PBMT output and the source sentence. We evaluate the technique on the English to German translation task. Using this approach we are able to outperform the PBMT system as well as the baseline neural MT system by up to 2 BLEU points. We analyzed the influence of the quality of the initial system on the final result."
C16-1292,Lightly Supervised Quality Estimation,2016,19,1,3,1,10828,matthias sperber,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Evaluating the quality of output from language processing systems such as machine translation or speech recognition is an essential step in ensuring that they are sufficient for practical use. However, depending on the practical requirements, evaluation approaches can differ strongly. Often, reference-based evaluation measures (such as BLEU or WER) are appealing because they are cheap and allow rapid quantitative comparison. On the other hand, practitioners often focus on manual evaluation because they must deal with frequently changing domains and quality standards requested by customers, for which reference-based evaluation is insufficient or not possible due to missing in-domain reference data (Harris et al., 2016). In this paper, we attempt to bridge this gap by proposing a framework for lightly supervised quality estimation. We collect manually annotated scores for a small number of segments in a test corpus or document, and combine them with automatically predicted quality scores for the remaining segments to predict an overall quality estimate. An evaluation shows that our framework estimates quality more reliably than using fully automatic quality estimation approaches, while keeping annotation effort low by not requiring full references to be available for the particular domain."
W15-4917,Stripping Adjectives: Integration Techniques for Selective Stemming in {SMT} Systems,2015,25,0,2,1,36550,isabel slawik,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"In this paper we present an approach to reduce data sparsity problems when translating from morphologically rich languages into less inflected languages by selectively stemming certain word types. We develop and compare three different integration strategies: replacing words with their stemmed form, combined input using alternative lattice paths for the stemmed and surface forms and a novel hidden combination strategy, where we replace the stems in the stemmed phrase table by the observed surface forms in the test data. This allows us to apply advanced models trained on the surface forms of the words. We evaluate our approach by stemming German adjectives in two Germanxe2x86x92English translation scenarios: a low-resource condition as well as a large-scale state-of-the-art translation system. We are able to improve between 0.2 and 0.4 BLEU points over our baseline and reduce the number of out-of-vocabulary words by up to 16.5%."
W15-3008,The Karlsruhe Institute of Technology Translation Systems for the {WMT} 2015,2015,30,2,3,1,2962,eunah cho,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"In this paper, the KIT systems submitted to the Shared Translation Task are presented. We participated in two translation directions: from German to English and from English to German. Both translations are generated using phrase-based translation systems. The performance of the systems was boosted by using language models built based on different tokens such as word, part-of-speech, and automacally generated word clusters. The difference in word order between German and English is addressed by part-of-speech and syntactic tree-based reordering models. In addition to a discriminative word lexicon, we used hypothesis rescoring using the ListNet algorithm after generating the translation with the phrase-based system. We evaluated the rescoring using only the baseline features as well as using additional computational complex features."
W15-3012,The {KIT}-{LIMSI} Translation System for {WMT} 2015,2015,30,1,4,1,5767,thanhle ha,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper presented the joined submission of KIT and LIMSI to the English to German translation task of WMT 2015. In this year submission, we integrated a neural network-based translation model into a phrase-based translation model by rescoring the n-best lists. Since the computation complexity is one of the main issues for continuous space models, we compared two techniques to reduce the computation cost. We investigated models using a structured output layer as well as models trained with noise contrastive estimation. Furthermore, we evaluated a new method to obtain the best log-linear combination in the rescoring phase. Using these techniques, we were able to improve the BLEU score of the baseline phrase-based system by 1.4 BLEU points."
W15-3030,{L}ist{N}et-based {MT} Rescoring,2015,21,4,1,1,5714,jan niehues,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"The log-linear combination of different features is an important component of SMT systems. It allows for the easy integartion of models into the system and is used during decoding as well as for nbest list rescoring. With the recent success of more complex models like neural network-based translation models, n-best list rescoring attracts again more attention. In this work, we present a new technique to train the log-linear model based on the ListNet algorithm. This technique scales to many features, considers the whole list and not single entries during learning and can also be applied to more complex models than a log-linear combination. Using the new learning approach, we improve the translation quality of a largescale system by 0.8 BLEU points during rescoring and generate translations which are up to 0.3 BLEU points better than other learning techniques such as MERT or MIRA."
2015.iwslt-papers.3,Source discriminative word lexicon for translation disambiguation,2015,-1,-1,2,1,36851,teresa herrmann,Proceedings of the 12th International Workshop on Spoken Language Translation: Papers,0,None
2015.iwslt-papers.8,Punctuation insertion for real-time spoken language translation,2015,-1,-1,2,1,2962,eunah cho,Proceedings of the 12th International Workshop on Spoken Language Translation: Papers,0,None
2015.iwslt-evaluation.1,The {IWSLT} 2015 Evaluation Campaign,2015,18,51,2,0,10592,mauro cettolo,Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Campaign,0,None
2015.iwslt-evaluation.9,The {KIT} translation systems for {IWSLT} 2015,2015,-1,-1,2,1,5767,thanhle ha,Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Campaign,0,None
2015.eamt-1.18,Stripping Adjectives: Integration Techniques for Selective Stemming in {SMT} Systems,2015,25,0,2,1,36550,isabel slawik,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,"In this paper we present an approach to reduce data sparsity problems when translating from morphologically rich languages into less inflected languages by selectively stemming certain word types. We develop and compare three different integration strategies: replacing words with their stemmed form, combined input using alternative lattice paths for the stemmed and surface forms and a novel hidden combination strategy, where we replace the stems in the stemmed phrase table by the observed surface forms in the test data. This allows us to apply advanced models trained on the surface forms of the words. We evaluate our approach by stemming German adjectives in two Germanxe2x86x92English translation scenarios: a low-resource condition as well as a large-scale state-of-the-art translation system. We are able to improve between 0.2 and 0.4 BLEU points over our baseline and reduce the number of out-of-vocabulary words by up to 16.5%."
W14-3307,The {KIT}-{LIMSI} Translation System for {WMT} 2014,2014,27,1,3,0,14186,quoc do,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes the joined submission of LIMSI and KIT to the Shared Translation Task for the German-toEnglish direction. The system consists of a phrase-based translation system using a pre-reordering approach. The baseline system already includes several models like conventional language models on different word factors and a discriminative word lexicon. This system is used to generate a k-best list. In a second step, the list is reranked using SOUL language and translation models (Le et al., 2011). Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score."
W14-3313,The Karlsruhe Institute of Technology Translation Systems for the {WMT} 2014,2014,24,3,5,1,36851,teresa herrmann,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"In this paper, we present the KIT systems participating in the Shared Translation Task translating between English$German and English$French. All translations are generated using phrase-based translation systems, using different kinds of word-based, part-ofspeech-based and cluster-based language models trained on the provided data. Additional models include bilingual language models, reordering models based on part-of-speech tags and syntactic parse trees, as well as a lexicalized reordering model. In order to make use of noisy web-crawled data, we apply filtering and data selection methods for language modeling. A discriminative word lexicon using source context information proved beneficial for all translation directions."
W14-3330,{LIMSI} @ {WMT}{'}14 Medical Translation Task,2014,-1,-1,8,0,36855,nicolas pecheux,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,None
herrmann-etal-2014-manual,Manual Analysis of Structurally Informed Reordering in {G}erman-{E}nglish Machine Translation,2014,15,1,2,1,36851,teresa herrmann,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Word reordering is a difficult task for translation. Common automatic metrics such as BLEU have problems reflecting improvements in target language word order. However, it is a crucial aspect for humans when deciding on translation quality. This paper presents a detailed analysis of a structure-aware reordering approach applied in a German-to-English phrase-based machine translation system. We compare the translation outputs of two translation systems applying reordering rules based on parts-of-speech and syntax trees on a sentence-by-sentence basis. For each sentence-pair we examine the global translation performance and classify local changes in the translated sentences. This analysis is applied to three data sets representing different genres. While the improvement in BLEU differed substantially between the data sets, the manual evaluation showed that both global translation performance as well as individual types of improvements and degradations exhibit a similar behavior throughout the three data sets. We have observed that for 55-64{\%} of the sentences with different translations, the translation produced using the tree-based reordering was considered to be the better translation. As intended by the investigated reordering model, most improvements are achieved by improving the position of the verb or being able to translate a verb that could not be translated before."
E14-4009,Tight Integration of Speech Disfluency Removal into {SMT},2014,15,4,2,1,2962,eunah cho,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"Speech disfluencies are one of the main challenges of spoken language processing. Conventional disfluency detection systems deploy a hard decision, which can have a negative influence on subsequent applications such as machine translation. In this paper we suggest a novel approach in which disfluency detection is integrated into the translation process. We train a CRF model to obtain a disfluency probability for each word. The SMT decoder will then skip the potentially disfluent word based on its disfluency probability. Using the suggested scheme, the translation score of both the manual transcript and ASR output is improved by around 0.35 BLEU points compared to the CRF hard decision system."
2014.iwslt-papers.4,Machine translation of multi-party meetings: segmentation and disfluency removal strategies,2014,-1,-1,2,1,2962,eunah cho,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"Translating meetings presents a challenge since multi-speaker speech shows a variety of disfluencies. In this paper we investigate the importance of transforming speech into well-written input prior to translating multi-party meetings. We first analyze the characteristics of this data and establish oracle scores. Sentence segmentation and punctuation are performed using a language model, turn information, or a monolingual translation system. Disfluencies are removed by a CRF model trained on in-domain and out-of-domain data. For comparison, we build a combined CRF model for punctuation insertion and disfluency removal. By applying these models, multi-party meetings are transformed into fluent input for machine translation. We evaluate the models with regard to translation performance and are able to achieve an improvement of 2.1 to 4.9 BLEU points depending on the availability of turn information."
2014.iwslt-papers.10,Lexical translation model using a deep neural network architecture,2014,-1,-1,2,1,5767,thanhle ha,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"In this paper we combine the advantages of a model using global source sentence contexts, the Discriminative Word Lexicon, and neural networks. By using deep neural networks instead of the linear maximum entropy model in the Discriminative Word Lexicon models, we are able to leverage dependencies between different source words due to the non-linearity. Furthermore, the models for different target words can share parameters and therefore data sparsity problems are effectively reduced. By using this approach in a state-of-the-art translation system, we can improve the performance by up to 0.5 BLEU points for three different language pairs on the TED translation task."
2014.iwslt-evaluation.1,Report on the 11th {IWSLT} evaluation campaign,2014,-1,-1,2,0.16178,10592,mauro cettolo,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"The paper overviews the 11th evaluation campaign organized by the IWSLT workshop. The 2014 evaluation offered multiple tracks on lecture transcription and translation based on the TED Talks corpus. In particular, this year IWSLT included three automatic speech recognition tracks, on English, German and Italian, five speech translation tracks, from English to French, English to German, German to English, English to Italian, and Italian to English, and five text translation track, also from English to French, English to German, German to English, English to Italian, and Italian to English. In addition to the official tracks, speech and text translation optional tracks were offered, globally involving 12 other languages: Arabic, Spanish, Portuguese (B), Hebrew, Chinese, Polish, Persian, Slovenian, Turkish, Dutch, Romanian, Russian. Overall, 21 teams participated in the evaluation, for a total of 76 primary runs submitted. Participants were also asked to submit runs on the 2013 test set (progress test set), in order to measure the progress of systems with respect to the previous year. All runs were evaluated with objective metrics, and submissions for two of the official text translation tracks were also evaluated with human post-editing."
2014.iwslt-evaluation.7,Combined spoken language translation,2014,55,6,11,0.655738,3519,markus freitag,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"EU-BRIDGE is a European research project which is aimed at developing innovative speech translation technology. One of the collaborative efforts within EU-BRIDGE is to produce joint submissions of up to four different partners to the evaluation campaign at the 2014 International Workshop on Spoken Language Translation (IWSLT). We submitted combined translations to the GermanâEnglish spoken language translation (SLT) track as well as to the GermanâEnglish, EnglishâGerman and EnglishâFrench machine translation (MT) tracks. In this paper, we present the techniques which were applied by the different individual translation systems of RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show the combination approach developed at RWTH Aachen University which combined the individual systems. The consensus translations yield empirical gains of up to 2.3 points in BLEU and 1.2 points in TER compared to the best individual system."
2014.iwslt-evaluation.17,The {KIT} translation systems for {IWSLT} 2014,2014,-1,-1,3,1,36550,isabel slawik,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, we present the KIT systems participating in the TED translation tasks of the IWSLT 2014 machine translation evaluation. We submitted phrase-based translation systems for all three official directions, namely EnglishâGerman, GermanâEnglish, and EnglishâFrench, as well as for the optional directions EnglishâChinese and EnglishâArabic. For the official directions we built systems both for the machine translation as well as the spoken language translation track. This year we improved our systems{'} performance over last year through n-best list rescoring using neural network-based translation and language models and novel preordering rules based on tree information of multiple syntactic levels. Furthermore, we could successfully apply a novel phrase extraction algorithm and transliteration of unknown words for Arabic. We also submitted a contrastive system for GermanâEnglish built with stemmed German adjectives. For the SLT tracks, we used a monolingual translation system to translate the lowercased ASR hypotheses with all punctuation stripped to truecased, punctuated output as a preprocessing step to our usual translation system."
2014.amta-researchers.17,Combining techniques from different {NN}-based language models for machine translation,2014,23,1,1,1,5714,jan niehues,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"This paper presents two improvements of language models based on Restricted Boltzmann Machine (RBM) for large machine translation tasks. In contrast to other continuous space approach, RBM based models can easily be integrated into the decoder and are able to directly learn a hidden representation of the n-gram. Previous work on RBM-based language models do not use a shared word representation and therefore, they might suffer of a lack of generalization for larger contexts. Moreover, since the training step is very time consuming, they are only used for quite small copora. In this work we add a shared word representation for the RBM-based language model by factorizing the weight matrix. In addition, we propose an efficient and tailored sampling algorithm that allows us to drastically speed up the training process. Experiments are carried out on two German to English translation tasks and the results show that the training time could be reduced by a factor of 10 without any drop in performance. Furthermore, the RBM-based model can also be trained on large size corpora."
W13-3204,Letter N-Gram-based Input Encoding for Continuous Space Language Models,2013,41,15,2,0,40882,henning sperr,Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,0,"We present a letter-based encoding for words in continuous space language models. We represent the words completely by letter n-grams instead of using the word index. This way, similar words will automatically have a similar representation. With this we hope to better generalize to unknown or rare words and to also capture morphological information. We show their influence in the task of machine translation using continuous space language models based on restricted Boltzmann machines. We evaluate the translation quality as well as the training time on a German-to-English translation task of TED and university lectures as well as on the news translation task translating from English to German. Using our new approach a gain in BLEU score by up to 0.4 points can be achieved."
W13-2210,The {K}arlsruhe {I}nstitute of {T}echnology Translation Systems for the {WMT} 2013,2013,14,5,4,1,2962,eunah cho,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes the phrase-based SMT systems developed for our participation in the WMT13 Shared Translation Task. Translations for English$German and English$French were generated using a phrase-based translation system which is extended by additional models such as bilingual, fine-grained part-ofspeech (POS) and automatic cluster language models and discriminative word lexica (DWL). In addition, we combined reordering models on different sentence"
W13-2223,Joint {WMT} 2013 Submission of the {QUAERO} Project,2013,34,4,9,0.384615,27027,stephan peitz,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes the joint submission of the QUAERO project for the German!English translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation (WMT 2013). The submission was a system combination of the output of four different translation systems provided by RWTH Aachen University, Karlsruhe Institute of Technology (KIT), LIMSI-CNRS and SYSTRAN Software, Inc. The translations were joined using the RWTHxe2x80x99s system combination approach. Experimental results show improvements of up to 1.2 points in BLEU and 1.2 points in TER compared to the best single translation."
W13-2264,An {MT} Error-Driven Discriminative Word Lexicon using Sentence Structure Features,2013,13,10,1,1,5714,jan niehues,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"The Discriminative Word Lexicon (DWL) is a maximum-entropy model that predicts the target word probability given the source sentence words. We present two ways to extend a DWL to improve its ability to model the word translation probability in a phrase-based machine translation (PBMT) system. While DWLs are able to model the global source information, they ignore the structure of the source and target sentence. We propose to include this structure by modeling the source sentence as a bag-of-n-grams and features depending on the surrounding target words. Furthermore, as the standard DWL does not get any feedback from the MT system, we change the DWL training process to explicitly focus on addressing MT errors."
W13-0805,Combining Word Reordering Methods on different Linguistic Abstraction Levels for Statistical Machine Translation,2013,21,20,2,1,36851,teresa herrmann,"Proceedings of the Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"We describe a novel approach to combining lexicalized, POS-based and syntactic treebased word reordering in a phrase-based machine translation system. Our results show that each of the presented reordering methods leads to improved translation quality on its own. The strengths however can be combined to achieve further improvements. We present experiments on German-English and GermanFrench translation. We report improvements of 0.7 BLEU points by adding tree-based and lexicalized reordering. Up to 1.1 BLEU points can be gained by POS and tree-based reordering over a baseline with lexicalized reordering. A human analysis, comparing subjective translation quality as well as a detailed error analysis show the impact of our presented tree-based rules in terms of improved sentence quality and reduction of errors related to missing verbs and verb positions."
2013.iwslt-papers.11,Analyzing the potential of source sentence reordering in statistical machine translation,2013,-1,-1,3,1,36851,teresa herrmann,Proceedings of the 10th International Workshop on Spoken Language Translation: Papers,0,"We analyze the performance of source sentence reordering, a common reordering approach, using oracle experiments on German-English and English-German translation. First, we show that the potential of this approach is very promising. Compared to a monotone translation, the optimally reordered source sentence leads to improvements of up to 4.6 and 6.2 BLEU points, depending on the language. Furthermore, we perform a detailed evaluation of the different aspects of the approach. We analyze the impact of the restriction of the search space by reordering lattices and we can show that using more complex rule types for reordering results in better approximation of the optimally reordered source. However, a gap of about 3 to 3.8 BLEU points remains, presenting a promising perspective for research on extending the search space through better reordering rules. When evaluating the ranking of different reordering variants, the results reveal that the search for the best path in the lattice performs very well for German-English translation. For English-German translation there is potential for an improvement of up to 1.4 BLEU points through a better ranking of the different reordering possibilities in the reordering lattice."
2013.iwslt-evaluation.1,Report on the 10th {IWSLT} evaluation campaign,2013,-1,-1,2,0.16178,10592,mauro cettolo,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"The paper overviews the tenth evaluation campaign organized by the IWSLT workshop. The 2013 evaluation offered multiple tracks on lecture transcription and translation based on the TED Talks corpus. In particular, this year IWSLT included two automatic speech recognition tracks, on English and German, three speech translation tracks, from English to French, English to German, and German to English, and three text translation track, also from English to French, English to German, and German to English. In addition to the official tracks, speech and text translation optional tracks were offered involving 12 other languages: Arabic, Spanish, Portuguese (B), Italian, Chinese, Polish, Persian, Slovenian, Turkish, Dutch, Romanian, Russian. Overall, 18 teams participated in the evaluation for a total of 217 primary runs submitted. All runs were evaluated with objective metrics on a current test set and two progress test sets, in order to compare the progresses against systems of the previous years. In addition, submissions of one of the official machine translation tracks were also evaluated with human post-editing."
2013.iwslt-evaluation.16,{EU}-{BRIDGE} {MT}: text translation of talks in the {EU}-{BRIDGE} project,2013,52,8,9,0.789474,3519,markus freitag,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes one of the collaborative efforts within EUBRIDGE to further advance the state of the art in machine translation between two European language pairs, EnglishâFrench and GermanâEnglish. Four research institutions involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the machine translation track of the evaluation campaign at the 2013 International Workshop on Spoken Language Translation (IWSLT). We present the methods and techniques to achieve high translation quality for text translation of talks which are applied at RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show how we have been able to considerably boost translation performance (as measured in terms of the metrics BLEU and TER) by means of system combination. The joint setups yield empirical gains of up to 1.4 points in BLEU and 2.8 points in TER on the IWSLT test sets compared to the best single systems."
2013.iwslt-evaluation.24,The {KIT} translation systems for {IWSLT} 2013,2013,-1,-1,3,0,41933,thanle ha,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, we present the KIT systems participating in all three official directions, namely EnglishâGerman, GermanâEnglish, and EnglishâFrench, in translation tasks of the IWSLT 2013 machine translation evaluation. Additionally, we present the results for our submissions to the optional directions EnglishâChinese and EnglishâArabic. We used phrase-based translation systems to generate the translations. This year, we focused on adapting the systems towards ASR input. Furthermore, we investigated different reordering models as well as an extended discriminative word lexicon. Finally, we added a data selection approach for domain adaptation."
W12-3140,Joint {WMT} 2012 Submission of the {QUAERO} Project,2012,37,5,5,0.833333,3519,markus freitag,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes the joint QUAERO submission to the WMT 2012 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT Germanxe2x86x92English task. Each group translated the data sets with their own systems and finally the RWTH system combination combined these translations in our final submission. Experimental results show improvements of up to 1.7 points in Bleu and 3.4 points in Ter compared to the best single system."
W12-3144,The Karlsruhe Institute of Technology Translation Systems for the {WMT} 2012,2012,17,7,1,1,5714,jan niehues,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes the phrase-based SMT systems developed for our participation in the WMT12 Shared Translation Task. Translations for Englishxe2x86x94German and Englishxe2x86x94French were generated using a phrase-based translation system which is extended by additional models such as bilingual, fine-grained part-of-speech (POS) and automatic cluster language models and discriminative word lexica. In addition, we explicitly handle out-of-vocabulary (OOV) words in German, if we have translations for other morphological forms of the same stem. Furthermore, we extended the POS-based reordering approach to also use information from syntactic trees."
federico-etal-2012-iwslt,The {IWSLT} 2011 Evaluation Campaign on Automatic Talk Translation,2012,18,30,7,0,3526,marcello federico,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We report here on the eighth evaluation campaign organized in 2011 by the IWSLT workshop series. That IWSLT 2011 evaluation focused on the automatic translation of public talks and included tracks for speech recognition, speech translation, text translation, and system combination. Unlike in previous years, all data supplied for the evaluation has been publicly released on the workshop website, and is at the disposal of researchers interested in working on our benchmarks and in comparing their results with those published at the workshop. This paper provides an overview of the IWSLT 2011 evaluation campaign, and describes the data supplied, the evaluation infrastructure made available to participants, and the subjective evaluation carried out."
2012.iwslt-papers.3,Continuous space language models using restricted Boltzmann machines,2012,20,20,1,1,5714,jan niehues,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,"We present a novel approach for continuous space language models in statistical machine translation by using Restricted Boltzmann Machines (RBMs). The probability of an n-gram is calculated by the free energy of the RBM instead of a feedforward neural net. Therefore, the calculation is much faster and can be integrated into the translation process instead of using the language model only in a re-ranking step. Furthermore, it is straightforward to introduce additional word factors into the language model. We observed a faster convergence in training if we include automatically generated word classes as an additional word factor. We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures. Instead of replacing the conventional n-gram-based language model, we trained the RBM-based language model on the more important but smaller in-domain data and combined them in a log-linear way. With this approach we could show improvements of about half a BLEU point on the translation task."
2012.iwslt-papers.15,Segmentation and punctuation prediction in speech language translation using a monolingual translation system,2012,16,24,2,1,2962,eunah cho,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,"In spoken language translation (SLT), finding proper segmentation and reconstructing punctuation marks are not only significant but also challenging tasks. In this paper we present our recent work on speech translation quality analysis for German-English by improving sentence segmentation and punctuation. From oracle experiments, we show an upper bound of translation quality if we had human-generated segmentation and punctuation on the output stream of speech recognition systems. In our oracle experiments we gain 1.78 BLEU points of improvements on the lecture test set. We build a monolingual translation system from German to German implementing segmentation and punctuation prediction as a machine translation task. Using the monolingual translation system we get an improvement of 1.53 BLEU points on the lecture test set, which is a comparable performance against the upper bound drawn by the oracle experiments."
2012.iwslt-evaluation.3,The {KIT} translation systems for {IWSLT} 2012,2012,-1,-1,4,1,14105,mohammed mediani,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, we present the KIT systems participating in the English-French TED Translation tasks in the framework of the IWSLT 2012 machine translation evaluation. We also present several additional experiments on the English-German, English-Chinese and English-Arabic translation pairs. Our system is a phrase-based statistical machine translation system, extended with many additional models which were proven to enhance the translation quality. For instance, it uses the part-of-speech (POS)-based reordering, translation and language model adaptation, bilingual language model, word-cluster language model, discriminative word lexica (DWL), and continuous space language model. In addition to this, the system incorporates special steps in the preprocessing and in the post-processing step. In the preprocessing the noisy corpora are filtered by removing the noisy sentence pairs, whereas in the postprocessing the agreement between a noun and its surrounding words in the French translation is corrected based on POS tags with morphological information. Our system deals with speech transcription input by removing case information and punctuation except periods from the text translation model."
2012.amta-papers.19,Detailed Analysis of Different Strategies for Phrase Table Adaptation in {SMT},2012,-1,-1,1,1,5714,jan niehues,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"This paper gives a detailed analysis of different approaches to adapt a statistical machine translation system towards a target domain using small amounts of parallel in-domain data. Therefore, we investigate the differences between the approaches addressing adaptation on the two main steps of building a translation model: The candidate selection and the phrase scoring. For the latter step we characterized the differences by four key aspects. We performed experiments on two different tasks of speech translation and analyzed the influence of the different aspects on the overall translation quality. On both tasks we could show significant improvements by using the presented adaptation techniques."
W11-2124,Wider Context by Using Bilingual Language Models in Machine Translation,2011,18,60,1,1,5714,jan niehues,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"In past Evaluations for Machine Translation of European Languages, it could be shown that the translation performance of SMT systems can be increased by integrating a bilingual language model into a phrase-based SMT system. In the bilingual language model, target words with their aligned source words build the tokens of an n-gram based language model. We analyzed the effect of bilingual language models and show where they could help to better model the translation process. We could show improvements of translation quality on German-to-English and Arabic-to-English. In addition, for the Arabic-to-English task, training an extra bilingual language model on the POS tags instead of the surface word forms led to further improvements."
W11-2142,Joint {WMT} Submission of the {QUAERO} Project,2011,25,1,7,0.833333,3519,markus freitag,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes the joint QUAERO submission to the WMT 2011 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT Germanxe2x86x92English task. Each group translated the data sets with their own systems. Then RWTH system combination combines these translations to a better one. In this paper, we describe the single systems of each group. Before we present the results of the system combination, we give a short description of the RWTH Aachen system combination approach."
W11-2145,The Karlsruhe Institute of Technology Translation Systems for the {WMT} 2011,2011,13,11,3,1,36851,teresa herrmann,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes the phrase-based SMT systems developed for our participation in the WMT11 Shared Translation Task. Translations for Englishxe2x86x94German and Englishxe2x86x94French were generated using a phrase-based translation system which is extended by additional models such as bilingual and fine-grained POS language models, POS-based reordering, lattice phrase extraction and discriminative word alignment. Furthermore, we present a special filtering method for the English-French Giga corpus and the phrase scoring step in the training is parallelized."
2011.iwslt-papers.6,Using {W}ikipedia to translate domain-specific terms in {SMT},2011,22,12,1,1,5714,jan niehues,Proceedings of the 8th International Workshop on Spoken Language Translation: Papers,0,"When building a university lecture translation system, one important step is to adapt it to the target domain. One problem in this adaptation task is to acquire translations for domain specific terms. In this approach we tried to get these translations from Wikipedia, which provides articles on very specific topics in many different languages. To extract translations for the domain specific terms, we used the interlanguage links of Wikipedia . We analyzed different methods to integrate this corpus into our system and explored methods to disambiguate between different translations by using the text of the articles. In addition, we developed methods to handle different morphological forms of the specific terms in morphologically rich input languages like German. The results show that the number of out-of-vocabulary (OOV) words could be reduced by 50{\%} on computer science lectures and the translation quality could be improved by more than 1 BLEU point."
2011.iwslt-evaluation.9,The {KIT} {E}nglish-{F}rench translation systems for {IWSLT} 2011,2011,9,21,3,1,14105,mohammed mediani,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper presents the KIT system participating in the EnglishâFrench TALK Translation tasks in the framework of the IWSLT 2011 machine translation evaluation. Our system is a phrase-based translation system using POS-based reordering extended with many additional features. First of all, a special preprocessing is devoted to the Giga corpus in order to minimize the effect of the great amount of noise it contains. In addition, the system gives more importance to the in-domain data by adapting the translation and the language models as well as by using a wordcluster language model. Furthermore, the system is extended by a bilingual language model and a discriminative word lexicon. The automatic speech transcription input usually has no or wrong punctuation marks, therefore these marks were especially removed from the source training data for the SLT system training."
2011.iwslt-evaluation.15,Advances on spoken language translation in the Quaero program,2011,25,2,8,0,43221,karim boudahmane,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"The Quaero program is an international project promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within the program framework, research organizations and industrial partners collaborate to develop prototypes of innovating applications and services for access and usage of multimedia data. One of the topics addressed is the translation of spoken language. Each year, a project-internal evaluation is conducted by DGA to monitor the technological advances. This work describes the design and results of the 2011 evaluation campaign. The participating partners were RWTH, KIT, LIMSI and SYSTRAN. Their approaches are compared on both ASR output and reference transcripts of speech data for the translation between French and German. The results show that the developed techniques further the state of the art and improve translation quality."
W10-1719,The Karlsruhe Institute for Technology Translation System for the {ACL}-{WMT} 2010,2010,9,7,1,1,5714,jan niehues,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,This paper describes our phrase-based Statistical Machine Translation (SMT) system for the WMT10 Translation Task. We submitted translations for the German to English and English to German translation tasks. Compared to state-of-the-art phrase-based systems we preformed additional preprocessing and used a discriminative word alignment approach. The word reordering was modeled using POS information and we extended the translation model with additional features.
2010.iwslt-evaluation.11,The {KIT} translation system for {IWSLT} 2010,2010,25,14,1,1,5714,jan niehues,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, we present the KIT systems participating in the TED translation tasks of the IWSLT 2015 machine translation evaluation. We submitted phrase-based translation systems for three directions, namely English!German, German!English, and English!Vietnamese. For the official directions (English!German and German!English), we built systems both for the machine translation (MT) as well as the spoken language translation (SLT) tracks. This year we improved our systemsxe2x80x99 performance over last year through n-best list rescoring using neural networkbased translation and language models and novel discriminative models based on different source-side features and classification methods. For the SLT tracks, we used a monolingual translation system to translate the lowercased ASR hypotheses with all punctuation stripped to truecased, punctuated output as a preprocessing step to our usual translation system. In addition to punctuation insertion, we also trained that system for sentence boundary insertion since the SLTxe2x80x99s data this year come with no sentence boundary."
2010.eamt-1.29,Domain Adaptation in Statistical Machine Translation using Factored Translation Models,2010,17,22,1,1,5714,jan niehues,Proceedings of the 14th Annual conference of the European Association for Machine Translation,0,"In recent years the performance of SMT increased in domains with enough training data. But under real-world conditions, it is often not possible to collect enough parallel data. We propose an approach to adapt an SMT system using small amounts of parallel in-domain data by introducing the corpus identifier (corpus id) as an additional target factor. Then we added features to model the generation of the tags and features to judge a sequence of tags. Using this approach we could improve the translation performance in two domains by up to 1 BLEU point when translating from German to English."
W09-0413,"The {U}niversit{\\\a}t {K}arlsruhe Translation System for the {EACL}-{WMT} 2009""",2009,6,12,1,1,5714,jan niehues,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,In this paper we describe the statistical machine translation system of the Universitat Karlsruhe developed for the translation task of the Fourth Workshop on Statistical Machine Translation. The state-of-the-art phrase-based SMT system is augmented with alternative word reordering and alignment mechanisms as well as optional phrase table modifications. We participate in the constrained condition of German-English and English-German as well as in the constrained condition of French-English and English-French.
W09-0435,A {POS}-Based Model for Long-Range Reorderings in {SMT},2009,18,73,1,1,5714,jan niehues,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"In this paper we describe a new approach to model long-range word reorderings in statistical machine translation (SMT). Until now, most SMT approaches are only able to model local reorderings. But even the word order of related languages like German and English can be very different. In recent years approaches that reorder the source sentence in a preprocessing step to better match target sentences according to POS(Part-of-Speech)-based rules have been applied successfully. We enhance this approach to model long-range reorderings by introducing discontinuous rules.n n We tested this new approach on a German-English translation task and could significantly improve the translation quality, by up to 0.8 BLEU points, compared to a system which already uses continuous POS-based rules to model short-range reorderings."
W08-0303,Discriminative Word Alignment via Alignment Matrix Modeling,2008,22,46,1,1,5714,jan niehues,Proceedings of the Third Workshop on Statistical Machine Translation,0,"In this paper a new discriminative word alignment method is presented. This approach models directly the alignment matrix by a conditional random field (CRF) and so no restrictions to the alignments have to be made. Furthermore, it is easy to add features and so all available information can be used. Since the structure of the CRFs can get complex, the inference can only be done approximately and the standard algorithms had to be adapted. In addition, different methods to train the model have been developed. Using this approach the alignment quality could be improved by up to 23 percent for 3 different language pairs compared to a combination of both IBM4-alignments. Furthermore the word alignment was used to generate new phrase tables. These could improve the translation quality significantly."
2008.iwslt-papers.5,Simultaneous {G}erman-{E}nglish lecture translation.,2008,17,12,4,0,47108,muntsin kolss,Proceedings of the 5th International Workshop on Spoken Language Translation: Papers,0,"In an increasingly globalized world, situations in which people of different native tongues have to communicate with each other become more and more frequent. In many such situations, human interpreters are prohibitively expensive or simply not available. Automatic spoken language translation (SLT), as a cost-effective solution to this dilemma, has received increased attention in recent years. For a broad number of applications, including live SLT of lectures and oral presentations, these automatic systems should ideally operate in real time and with low latency. Large and highly specialized vocabularies as well as strong variations in speaking style {--} ranging from read speech to free presentations suffering from spontaneous events {--} make simultaneous SLT of lectures a challenging task. This paper presents our progress in building a simultaneous German-English lecture translation system. We emphasize some of the challenges which are particular to this language pair and propose solutions to tackle some of the problems encountered."
W07-0727,The {ISL} Phrase-Based {MT} System for the 2007 {ACL} Workshop on Statistical Machine Translation,2007,13,10,3,0,14461,matthias paulik,Proceedings of the Second Workshop on Statistical Machine Translation,0,"In this paper we describe the Interactive Systems Laboratories (ISL) phrase-based machine translation system used in the shared task Machine Translation for European Languages of the ACL 2007 Workshop on Statistical Machine Translation. We present results for a system combination of the ISL syntax-augmented MT system and the ISL phrase-based system by combining and rescoring the n-best lists of the two systems. We also investigate the combination of two of our phrase-based systems translating from different source languages, namely Spanish and German, into their common target language, English."
