2014.lilt-9.4,W09-2508,0,0.0506063,"Missing"
2014.lilt-9.4,W05-1210,0,0.0303535,"Missing"
2014.lilt-9.4,bentivogli-etal-2010-building,1,0.769235,"on). In both competitions, most of the approaches implement Machine Learning methods, that try to exploit training set data for learning. Since the work we present in this paper focuses in particular on Textual Entailment, the data we consider for our analysis include a sample of pairs extracted from RTE-5 data set (Bentivogli et al. 2009b). More specifically, in order to compare our results with the literature, we created our reference data joining the data sets annotated by Sammons et al. (2010) (composed of 210 pairs from RTE-5 test set: 107 entailment, 37 contradiction, 66 unknown) and by Bentivogli et al. (2010) (composed of 90 pairs from RTE-5: 30 entailment, 30 contradiction, 30 unknown). Since the two data sets have a lot of pairs in common, joining the two results in 243 pairs, divided into 117 positive (i.e. entailment), and 126 negative (i.e. 51 contradiction and 75 unknown) pairs. With respect to RTE-5 sub tasks (IE, IR and QA), such pairs are distributed as follows: 91 QA, 74 IE and 75 IR. From now on, we consider this data set as the reference data for our study (we will refer to it as “RTE-5-SAMPLE”), on which the annotation and the experiments described in the next sections are carried out"
2014.lilt-9.4,W07-1401,1,0.832017,"Missing"
2014.lilt-9.4,J96-2004,0,0.0267783,"reets in panic during the 30 second tremor. A student dormitory was said to be one of the buildings badly damaged. [. . . ] One student told Rai state TV that he managed to escape the building before the roof collapsed. H: A powerful earthquake strikes central Italy. To assess the validity of the proposed annotation, a subset of RTE-5SAMPLE (i.e. 90 pairs from RTE-5: 30 entailment, 30 contradiction, 30 unknown, Bentivogli et al. (2010)) has been independently annotated by another annotator with linguistic skills. To measure the inter-rater agreement we calculate the Cohen’s kappa coefficient (Carletta 1996), that is generally thought to be a more robust measure than simple percent agreement calculation since  takes into account the agreement occurring by chance. More specifically, Cohen’s kappa measures the agreement between two raters who each classifies N items into C mutually exclusive categories. The equation for  is: Pr(a) Pr(e) (1) = , 1 Pr(e) where Pr(a) is the relative observed agreement among raters, and Pr(e) is the hypothetical probability of chance agreement, using the observed data to calculate the probabilities of each observer randomly saying each category. If the raters are in"
2014.lilt-9.4,W07-1409,0,0.0779889,"Missing"
2014.lilt-9.4,P11-2057,0,0.0129903,"enomena+Ontology Decomposing Semantic Inferences / 99 ports the entailment or the contradiction judgment in a certain pair. For instance, in example 14 the phenomenon syntax:modifier supports the entailment relation (respected traditional healer ) healer ), but if T and H were inverted, it would have triggered a negative judgment (i.e. healer ; respected traditional healer ). As in Garoufi (2007), our study confirms that a huge amount of background knowledge and reasoning is required to face the RTE task, given the fact that phenomena belonging to the category reasoning are the most frequent. LoBue and Yates (2011) have attempted to characterize them proposing 20 categories of common-sense knowledge that are prevalent in TE. Their categories can be loosely organized into formbased categories (e.g. cause and e↵ect, simultaneous conditions) and content-based categories (e.g. arithmetic, has parts). While some of their fine-grained categories can be mapped to ours (e.g. arithmetic=quantity and has parts= meronymy), we plan to extend our annotation of the reasoning phenomena adopting some of the labels they propose, to subcategorize the phenomena we annotated as reasoning:general inference. 5 Analyzing sema"
2014.lilt-9.4,P08-1118,0,0.0719097,"Missing"
2014.lilt-9.4,C10-1087,0,0.0411756,"Missing"
2014.lilt-9.4,P10-1123,0,0.0630776,"Missing"
2014.lilt-9.4,P10-1122,0,0.101117,"ysis is that it is not fully clear what a system can actually learn from the available data sets. In the light of the above considerations, the purpose of this paper is to carry out a deep analysis of Textual Entailment (TE) data sets. We investigate the relations among two relevant aspects of semantic inferences: the logical dimension, i.e. the capacity of the inference to prove the conclusion from its premises, and the linguistic dimension, i.e. the linguistic devices that are used to accomplish the goal of the inference. With respect to other studies - see, for instance, Garoufi (2007) and Sammons et al. (2010) - that have annotated and investigated TE datasets, we take a data oriented and neutral approach. As an example, we do not assign a polarity to single linguistic phenomena, and we do not impose specific categorizations on positive and negative entailment, rather we expect to derive such distinctions from observations. According to this perspective, we aim at understanding whether there are regularities (i.e. relevant patterns) that might be learned combining the two dimensions. In the paper we show that the sparseness of the linguistic phenomena in current data sets and their distribution in"
2014.lilt-9.4,P07-1058,0,0.0343697,"are referred as monothematic pairs. In this work we decided to switch the terminology to be compliant with the theoretical framework we propose. 100 / Elena Cabrio and Bernardo Magnini TE systems. Macro categories are defined referring to widely accepted linguistic categories in the literature (Garoufi 2007), and to the inference types typically addressed in RTE systems: lexical, syntactic, lexical-syntactic, discourse and reasoning. Moreover, we assume that humans have knowledge about the linguistic phenomena relevant to TE, and that such knowledge can be expressed through entailment rules (Szpektor et al. 2007). An entailment rule is either a directional or bidirectional relation between two sides of a pattern, corresponding to text fragments with variables (typically phrases or parse sub-trees, according to the granularity of the phenomenon they formalize). The left-hand side of the pattern (LHS) entails the rights-hand side (RHS) of the same pattern under the same variable instantiation. In addition, a rule may be defined by a set of constraints, representing variable typing (e.g. PoS, NE type) and relations between variables, which have to be satisfied for the rule to be correctly applied. For in"
2014.lilt-9.4,W05-1206,0,0.0918016,"Missing"
2020.coling-main.42,W17-5526,0,0.0655602,"Missing"
2020.coling-main.42,C18-1105,0,0.0126029,"ne of research by applying semi-supervised learning (Cho et al., 2019) and also data selection (Do and Gaspers, 2019) mechanism. Generative Models. Most of the proposed models are discriminative, among the few works carried out for generative models for SF and IC, (Raymond and Riccardi, 2007; Yogatama et al., 2017) have shown that a generative model is relatively better than a discriminative model in a situation where data is scarce. One possible direction for generative models is to apply data augmentation to automatically create additional training data (Yoo et al., 2019; Zhao et al., 2019; Hou et al., 2018; Kurata et al., 2016; Peng et al., 2020; Kim et al., 2019). The main challenge for data augmentation is to generate diverse and fluent synthetic utterance, which preserve the semantics of the original utterance. Evaluation of SF and IC on more complex dataset. Existing neural approaches typically evaluated on single-intent utterance, however in a real-world scenario users may indicate multiple-intent in an utterance e.g. ”Show me all flights from Atlanta to London and get the cost” (Gangadharaiah and Narayanaswamy, 2019) or even expressing multiple sentences in one single turn. While most dat"
2020.coling-main.42,2020.acl-main.128,0,0.0316061,"rios, so that different aspects of the model can be captured. For example, as neural models 488 are data hungry, more work is still needed on transfer learning scenarios, where evaluation is carried out with less or without labeled data (zero-shot) for a particular target domain. In addition, most models for SF and IC are evaluated on English, which means that more effort is still needed to make models that work well for other languages. Some recent works have started exploring zero-shot cross lingual methods (Qin et al., 2020; Liu et al., 2020a; Liu et al., 2019) and also few-shot scenarios (Hou et al., 2020) and the room for improvement for these scenarios is still large. In short, designing a data efficient model that is portable across domains and languages is still a challenging problem for the coming future. Leveraging unlabeled data from live traffic. In real situations, personal digital assistants such as Google Home, Apple Siri and Amazon Alexa, receive live traffic data from real users. This large amount of unlabeled data from live traffic is a potential data source for model training, in addition to in-house annotated data. Unlabeled live data are likely different from in-house data, as"
2020.coling-main.42,N18-3019,0,0.0200015,"n contextual embedding, ELMo (Peters et al., 2018), before fine-tuning on DT . As we need to train from scratch the whole model when adding a new domain, data-driven approaches, especially MTL-based, need increasing training time as the number of domains grows. The alternative strategy, the model-driven approach, alleviates the problem by enabling model reusability. Although different domains have different slot schemas, slots such as date, time and location can be shared. In model driven adaptation ”expert” models (Figure 2 Middle) are first trained on these reusable slots (Kim et al., 2017; Jha et al., 2018) and the outputs of the expert models are used to guide the training of MT for a new target domain. This way the training time of MT is faster, as it is proportional to the DT data size, instead of the larger data size of the whole DS and DT . In this model-driven settings, Kim et al. (2017) do not treat each expert model on each DS equally, instead they use attention mechanism to learn a weighted combinations from the feedback of the expert models. Jha et al. (2018) use a similar model as Kim et al. (2017), however they do not use attention mechanism. For training the expert models, instead o"
2020.coling-main.42,P17-1060,0,0.112555,"on §3 and Section §4 are designed to be trained on a single domain (e.g. banking, restaurant reservation) and require relatively large labeled data to perform well. In practice, new intents and slots are regularly added to a system to support new tasks and domains, requiring data and time intensive processes. Hence, methods to train models for new domains with limited or without labeled data are needed. We refer to this situation as the domain scaling problem. Figure 2: Left: Data-driven approach (Jaech et al., 2016; Hakkani-T¨ur et al., 2016). Middle: ModelDriven Approach with expert models (Kim et al., 2017). Right: Zero-shot model (Bapna et al., 2017). 5.1 Transfer Learning Models for SF and IC A common approach to deal with domain scaling is transfer learning (TF).3 In the TF setup we have K source domains DS1 , DS2 , . . . , DSK and a target domain DTK+1 , and we assume abundance of data in DS and limited data in DT . Instead of training a target model MT for DT from scratch, TF aims to adapt the learned model MS from DS to produce a model MT trained on DT . TF is typically applied with various parameter sharing and training mechanisms. For SF and IC two approaches are proposed, namely data-dr"
2020.coling-main.42,N19-3014,0,0.0183937,"t al., 2019) and also data selection (Do and Gaspers, 2019) mechanism. Generative Models. Most of the proposed models are discriminative, among the few works carried out for generative models for SF and IC, (Raymond and Riccardi, 2007; Yogatama et al., 2017) have shown that a generative model is relatively better than a discriminative model in a situation where data is scarce. One possible direction for generative models is to apply data augmentation to automatically create additional training data (Yoo et al., 2019; Zhao et al., 2019; Hou et al., 2018; Kurata et al., 2016; Peng et al., 2020; Kim et al., 2019). The main challenge for data augmentation is to generate diverse and fluent synthetic utterance, which preserve the semantics of the original utterance. Evaluation of SF and IC on more complex dataset. Existing neural approaches typically evaluated on single-intent utterance, however in a real-world scenario users may indicate multiple-intent in an utterance e.g. ”Show me all flights from Atlanta to London and get the cost” (Gangadharaiah and Narayanaswamy, 2019) or even expressing multiple sentences in one single turn. While most datasets for slot filling and intent classification are single"
2020.coling-main.42,D16-1223,0,0.0261695,"applying semi-supervised learning (Cho et al., 2019) and also data selection (Do and Gaspers, 2019) mechanism. Generative Models. Most of the proposed models are discriminative, among the few works carried out for generative models for SF and IC, (Raymond and Riccardi, 2007; Yogatama et al., 2017) have shown that a generative model is relatively better than a discriminative model in a situation where data is scarce. One possible direction for generative models is to apply data augmentation to automatically create additional training data (Yoo et al., 2019; Zhao et al., 2019; Hou et al., 2018; Kurata et al., 2016; Peng et al., 2020; Kim et al., 2019). The main challenge for data augmentation is to generate diverse and fluent synthetic utterance, which preserve the semantics of the original utterance. Evaluation of SF and IC on more complex dataset. Existing neural approaches typically evaluated on single-intent utterance, however in a real-world scenario users may indicate multiple-intent in an utterance e.g. ”Show me all flights from Atlanta to London and get the cost” (Gangadharaiah and Narayanaswamy, 2019) or even expressing multiple sentences in one single turn. While most datasets for slot fillin"
2020.coling-main.42,D18-1417,0,0.0637101,"Missing"
2020.coling-main.42,D19-1129,0,0.0228488,"lso important to test models in different scenarios, so that different aspects of the model can be captured. For example, as neural models 488 are data hungry, more work is still needed on transfer learning scenarios, where evaluation is carried out with less or without labeled data (zero-shot) for a particular target domain. In addition, most models for SF and IC are evaluated on English, which means that more effort is still needed to make models that work well for other languages. Some recent works have started exploring zero-shot cross lingual methods (Qin et al., 2020; Liu et al., 2020a; Liu et al., 2019) and also few-shot scenarios (Hou et al., 2020) and the room for improvement for these scenarios is still large. In short, designing a data efficient model that is portable across domains and languages is still a challenging problem for the coming future. Leveraging unlabeled data from live traffic. In real situations, personal digital assistants such as Google Home, Apple Siri and Amazon Alexa, receive live traffic data from real users. This large amount of unlabeled data from live traffic is a potential data source for model training, in addition to in-house annotated data. Unlabeled live da"
2020.coling-main.42,2020.acl-main.3,0,0.0635515,"her zero-shot approaches explore the use of slot value examples (Shah et al., 2019; Guerini et al., 2018). Shah et al. (2019) showing that a combination of a small number of slot values examples with a slot description performs better than (Bapna et al., 2017; Lee and Jha, 2019) on the SNIPS dataset. Zero-shot models are typically trained on a per-slot basis (Figure 2 Right), meaning that if we have N slots, then the model will output N predictions, therefore, a merging mechanism is needed in case there are prediction overlaps. In order to alleviate the problem of having multiple predictions, Liu et al. (2020b) propose a coarse-to-fine approach, in which the model learns the slot entity pattern (coarsely) to identify a particular token is an entity or not. After that, the model performs a single prediction of the slot type (fine) based on the similarity between the feature representation and the slot description. Takeaways on scaling to new domains: • Both data driven methods, MTL and pre-train fine tuning, improve performance when data in DT is limited. Both are also flexible, as virtually many tasks from different domains can be plugged into these methods. As the number of domains grow, pre-trai"
2020.coling-main.42,W19-5911,1,0.833591,"of data in DS and limited data in DT . Instead of training a target model MT for DT from scratch, TF aims to adapt the learned model MS from DS to produce a model MT trained on DT . TF is typically applied with various parameter sharing and training mechanisms. For SF and IC two approaches are proposed, namely data-driven and model- driven. As for data-driven techniques, typically we combine data from DS and DT and we partition the parameters in the model into parts that are task-specific and parameters that are shared across tasks. Some studies (Jaech et al., 2016; Hakkani-T¨ur et al., 2016; Louvan and Magnini, 2019) apply this technique using multi-task learning (MTL) (Caruana, 1997) and the models are trained simultaneously on DS and DT (Figure 2 Left). Results have shown that MTL is particularly effective relative to single-task learning (STL) when the data in DT is scarce and the benefits over STL diminish as more data is available. Another technique that is typically used in data-driven approaches is based on pre-train and fine-tune mechanisms. Goyal et al. (2018) train a joint model of SF and IC, MS , on large DS , then fine-tune MS by replacing the output layer corresponding with the label space fr"
2020.coling-main.42,P15-2130,0,0.122576,"Missing"
2020.coling-main.42,P19-1550,0,0.0190755,"zero-shot models perform prediction on a per-slot basis, potential disadvantages are model accuracy when there is a prediction overlap and the model can also be computationally inefficient when dealing with many slots. 6 State of the Art and Beyond Based on the results in Table 2 and 3, it is evident that neural models have achieved outstanding performance on ATIS and SNIPS, showing that it is relatively easy for neural models to capture patterns that recognize slots and intents. ATIS, in particular, is already overused for SF and IC evaluations and recent analysis (B´echet and Raymond, 2018; Niu and Penn, 2019) have shown that the dataset is relatively simple and the room for performance improvement is tiny. A similar trend in performance can be noted for other datasets, such as SNIPS, and it is likely that performance improvement can be quickly saturated. However, it does not mean these models have solved SF and IC, or NLU problems in general, rather that the model has merely solved the datasets. Nevertheless, there are still a number of issues in SF and IC that need further investigation: Portable and Data Efficient Models. Instead of evaluating models with the typical leaderboard setup with fixed"
2020.coling-main.42,D14-1162,0,0.0881936,"assification (IC)) and filling the correct value for the slots of the frame (i.e. slot filling (SF)). In recent years, neural-network based models have achieved the state of the art for a wide range of natural language processing tasks, including SF and IC. Various neural architectures have been experimented on SF and IC, including RNN-based (Mesnil et al., 2013) and attention-based (Liu and Lane, 2016) approaches, till the more recent transformers models (Chen et al., 2019). Input representations have also evolved from static word embeddings (Mikolov et al., 2010; Collobert and Weston, 2008; Pennington et al., 2014) to contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). Such progress allows to better address dialogue phenomena involving SF and IC, including context modeling, handling out-of-vocabulary words, long-distance dependency between words, and to better exploit the synergy between SF and IC through joint models. In addition to rapid progresses in the research community, the demand for commercial conversational AI is also growing fast, shown by a variety of available solutions, such as Microsoft LUIS, Google Dialogflow, RASA, and Amazon Alexa. These solutions also use variou"
2020.coling-main.42,N18-1202,0,0.189054,"ts of the frame (i.e. slot filling (SF)). In recent years, neural-network based models have achieved the state of the art for a wide range of natural language processing tasks, including SF and IC. Various neural architectures have been experimented on SF and IC, including RNN-based (Mesnil et al., 2013) and attention-based (Liu and Lane, 2016) approaches, till the more recent transformers models (Chen et al., 2019). Input representations have also evolved from static word embeddings (Mikolov et al., 2010; Collobert and Weston, 2008; Pennington et al., 2014) to contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). Such progress allows to better address dialogue phenomena involving SF and IC, including context modeling, handling out-of-vocabulary words, long-distance dependency between words, and to better exploit the synergy between SF and IC through joint models. In addition to rapid progresses in the research community, the demand for commercial conversational AI is also growing fast, shown by a variety of available solutions, such as Microsoft LUIS, Google Dialogflow, RASA, and Amazon Alexa. These solutions also use various kinds of semantic frame representations as part of th"
2020.coling-main.42,D19-1214,0,0.103439,"t, and also domain. In a recent approach by Wang et al. (2018) propose a bi-model based structure to learn the crossimpact between SF and IC. They argue that a single model for two tasks can hurt performance, and, instead of sharing parameters, they use two-task networks to learn the cross-impact between the two tasks and only share the hidden state of the other task. In the model, every hidden state h1t in the first network is combined with the hidden state of the second network h2t , and vice versa. Training is also done asynchronously, as each model has a separate loss function. Qin et al. Qin et al. (2019) use a self-attentive shared encoder to produce better context-aware representations, then apply IC at the token level and use this information to guide the SF task. They argue that previous work based on single utterance-level intent prediction is more prone to error propagation. If some token-level intent is incorrectly predicted, the other correct token-level prediction can still be useful for corresponding SF. For the final IC prediction, they use a voting mechanism to take into account the IC prediction on each token. Chen et al. (2019) use a Transformer (Vaswani et al., 2017) model for j"
2020.coling-main.42,N19-1380,0,0.0240194,"Missing"
2020.coling-main.42,P19-1547,0,0.0532661,"rained on label representations that leverage natural language descriptions of the slots (Bapna et al., 2017; Lee and Jha, 2019). Assuming that accurate slot descriptions are provided, slots with different names although semantically similar would have similar description as well. Thus, having trained a model for the D ESTI NATION slot with its descriptions, it is now possible to recognize the slot A RRIVAL L OCATION without training on it, but only supplying the corresponding slot description. In addition to slot description, other zero-shot approaches explore the use of slot value examples (Shah et al., 2019; Guerini et al., 2018). Shah et al. (2019) showing that a combination of a small number of slot values examples with a slot description performs better than (Bapna et al., 2017; Lee and Jha, 2019) on the SNIPS dataset. Zero-shot models are typically trained on a per-slot basis (Figure 2 Right), meaning that if we have N slots, then the model will output N predictions, therefore, a merging mechanism is needed in case there are prediction overlaps. In order to alleviate the problem of having multiple predictions, Liu et al. (2020b) propose a coarse-to-fine approach, in which the model learns th"
2020.coling-main.42,J00-3003,0,0.62865,"Missing"
2020.coling-main.42,P17-2007,0,0.0615936,"Missing"
2020.coling-main.42,N18-2050,0,0.119857,"ith attention mechanism commonly used for neural machine translation. The shared encoder is a bi-directional LSTM, and the last hidden state of the encoder is then used by the decoder to generate a sequence of slot labels, while for IC there is a separate decoder. The attention mechanism is used to learn alignments between slot labels in the decoder and words in the encoder. Hakkani-T¨ur et al. (2016) also adopt parameter sharing similar to Zhang and Wang (2016), but instead of using GRU they use a shared LSTM and perform predictions for slots, intent, and also domain. In a recent approach by Wang et al. (2018) propose a bi-model based structure to learn the crossimpact between SF and IC. They argue that a single model for two tasks can hurt performance, and, instead of sharing parameters, they use two-task networks to learn the cross-impact between the two tasks and only share the hidden state of the other task. In the model, every hidden state h1t in the first network is combined with the hidden state of the second network h2t , and vice versa. Training is also done asynchronously, as each model has a separate loss function. Qin et al. Qin et al. (2019) use a self-attentive shared encoder to produ"
2020.coling-main.42,2020.emnlp-main.410,0,0.053664,"Missing"
2020.coling-main.42,C18-1182,0,0.0251128,"possible, semantic features such as NE are more beneficial than syntactic features for SF. When NE is used, it can boost the model performance for SF significantly. • The slot filling task is related to Named Entity Recognition (NER) (Grishman and Sundheim, 1996) task as slot values can be a named entity such as airline name, city name etc. If the slot filling task is modeled as a sequence tagging problem, basically recent neural models proposed for NER can be used for slot filling and vice versa. To know more about the recent development of neural NER models, one can consult the survey from Yadav and Bethard (2018). • The main disadvantage of independent models is that they do not exploit the interaction between intent and slots and may introduce error propagation when they are used in a pipeline. 4 Joint Models for SF and IC Figure 1: Left: Shared Bi-GRU encoder (Zhang and Wang, 2016). Middle: Slot-Gate Mechanism (Goo et al., 2018). Right: BERT Based (Chen et al., 2019). In Section 3 we reported approaches that treat SF and IC independently. However, as the two tasks always appear together in an utterance and they share information, it is intuitive to think that they can benefit each other. For instanc"
2020.coling-main.42,D19-1375,0,0.0146932,"rks explore this line of research by applying semi-supervised learning (Cho et al., 2019) and also data selection (Do and Gaspers, 2019) mechanism. Generative Models. Most of the proposed models are discriminative, among the few works carried out for generative models for SF and IC, (Raymond and Riccardi, 2007; Yogatama et al., 2017) have shown that a generative model is relatively better than a discriminative model in a situation where data is scarce. One possible direction for generative models is to apply data augmentation to automatically create additional training data (Yoo et al., 2019; Zhao et al., 2019; Hou et al., 2018; Kurata et al., 2016; Peng et al., 2020; Kim et al., 2019). The main challenge for data augmentation is to generate diverse and fluent synthetic utterance, which preserve the semantics of the original utterance. Evaluation of SF and IC on more complex dataset. Existing neural approaches typically evaluated on single-intent utterance, however in a real-world scenario users may indicate multiple-intent in an utterance e.g. ”Show me all flights from Atlanta to London and get the cost” (Gangadharaiah and Narayanaswamy, 2019) or even expressing multiple sentences in one single tu"
2020.insights-1.3,W19-5911,1,0.873025,"Missing"
2020.insights-1.3,D11-1033,0,0.0304656,"e of the challenges in transfer learning is to deal with the data distribution mismatch between the source (DS ) and the target data (DT ) (Rosenstein et al., 2005). One solution to alleviate the impact of the mismatch is using data selection, a process for selecting relevant training instances from the source data. Data selection (DS) has been applied in the context of domain adaptation to address changes in the data distribution for various NLP tasks, such as sentiment analysis and POS Tagging (Ruder and Plank, 2017; Liu et al., 2019; Blitzer et al., 2007; Remus, 2012), machine translation (Axelrod et al., 2011), dependency parsing (Søgaard, 2011) and Named Entity Recognition (NER) (Murthy et al., 2018; Zhao et al., 2018). To our knowledge, all existing previous works apply 15 Proceedings of the First Workshop on Insights from Negative Results in NLP, pages 15–21 c Online, November 19, 2020. 2020 Association for Computational Linguistics ISBN 978-1-952148-66-8 instance x. The features are calculated between the representation of XS instances and XT . We use term distribution as the representation of the instances. We use the same similarity and diversity measures as Ruder and Plank (2017). The weight"
2020.insights-1.3,P18-2064,0,0.0130636,"n the source (DS ) and the target data (DT ) (Rosenstein et al., 2005). One solution to alleviate the impact of the mismatch is using data selection, a process for selecting relevant training instances from the source data. Data selection (DS) has been applied in the context of domain adaptation to address changes in the data distribution for various NLP tasks, such as sentiment analysis and POS Tagging (Ruder and Plank, 2017; Liu et al., 2019; Blitzer et al., 2007; Remus, 2012), machine translation (Axelrod et al., 2011), dependency parsing (Søgaard, 2011) and Named Entity Recognition (NER) (Murthy et al., 2018; Zhao et al., 2018). To our knowledge, all existing previous works apply 15 Proceedings of the First Workshop on Insights from Negative Results in NLP, pages 15–21 c Online, November 19, 2020. 2020 Association for Computational Linguistics ISBN 978-1-952148-66-8 instance x. The features are calculated between the representation of XS instances and XT . We use term distribution as the representation of the instances. We use the same similarity and diversity measures as Ruder and Plank (2017). The weights θ are learned through BO by taking into account the performance on the validation set when"
2020.insights-1.3,P07-1056,0,0.132053,"scale across different tasks, domains, and languages. One of the challenges in transfer learning is to deal with the data distribution mismatch between the source (DS ) and the target data (DT ) (Rosenstein et al., 2005). One solution to alleviate the impact of the mismatch is using data selection, a process for selecting relevant training instances from the source data. Data selection (DS) has been applied in the context of domain adaptation to address changes in the data distribution for various NLP tasks, such as sentiment analysis and POS Tagging (Ruder and Plank, 2017; Liu et al., 2019; Blitzer et al., 2007; Remus, 2012), machine translation (Axelrod et al., 2011), dependency parsing (Søgaard, 2011) and Named Entity Recognition (NER) (Murthy et al., 2018; Zhao et al., 2018). To our knowledge, all existing previous works apply 15 Proceedings of the First Workshop on Insights from Negative Results in NLP, pages 15–21 c Online, November 19, 2020. 2020 Association for Computational Linguistics ISBN 978-1-952148-66-8 instance x. The features are calculated between the representation of XS instances and XT . We use term distribution as the representation of the instances. We use the same similarity an"
2020.insights-1.3,P16-2067,0,0.0275668,"sian Optimization (BO) (Brochu et al., 2010), to evaluate the effectiveness of data selection on both the STDD and DTDD scenarios. Specifically, for DTDD we combine data selection and multi-task learning. Given XS , the framework performs data selection based on a score S derived from a set of features. The top m examples are then used to train MTT . In case of STDD, the MTT is a single task sequence tagging model, where we use a biLSTM-CRF model (Lample et al., 2016). As for DTDD, MTT is a hard parameter sharing MTL model, which has been applied to many NLP tasks (Søgaard and Goldberg, 2016; Plank et al., 2016; Changpinyo et al., 2018; Schulz et al., 2018). The performance on the validation set of the target task is then used by the BO optimizer to update the weight of the scoring features. Following Ruder and Plank (2017), the selection process is based on a score S computed as the linear combination of weighted features, which include both similarity and diversity features: Sθ (x) = θ&gt; · φ(x), where θ represents the weight for each feature and φ(x) denotes the feature values of each 3.1 Datasets For NER we use the OntoNotes 5.0 (Pradhan et al., 2012) dataset, which consists of several sections: n"
2020.insights-1.3,C18-1251,0,0.0390324,"Missing"
2020.insights-1.3,W12-4501,0,0.0230907,"ed to many NLP tasks (Søgaard and Goldberg, 2016; Plank et al., 2016; Changpinyo et al., 2018; Schulz et al., 2018). The performance on the validation set of the target task is then used by the BO optimizer to update the weight of the scoring features. Following Ruder and Plank (2017), the selection process is based on a score S computed as the linear combination of weighted features, which include both similarity and diversity features: Sθ (x) = θ&gt; · φ(x), where θ represents the weight for each feature and φ(x) denotes the feature values of each 3.1 Datasets For NER we use the OntoNotes 5.0 (Pradhan et al., 2012) dataset, which consists of several sections: newswire (NW), talkshows broadcast (BC), telephone conversation (TC), news broadcast (BN), articles from web sources (WB), and articles from magazines (MZ). We use different OntoNotes sections as different domains in our experiments. As for ST we use three datasets: ATIS (Price, 1990), MIT-R, and MIT-M (Liu et al., 2013), that are widely used as benchmarks for spoken language understanding. Each dataset contains utterances annotated with domain-specific slot labels, which are typically more fine-grained than NER labels. For example, in the utteranc"
2020.insights-1.3,D17-1038,0,0.0158831,"lessons learned for DTDD are the following: 1. We observe that MTL performs better than single-task learning (STL) for low-resource slot tagging, confirming the finding from Louvan 6 Conclusion In this paper we investigated the benefit of data selection for transfer learning in several scenarios of increasing complexity. We apply an existing model-agnostic state of the art data selection framework, and carried on experiments on two semantic sequence tagging tasks, NER and Slot Tagging, and two transfer learning scenarios, STDD (Same 5 We embed the sentence in source and target with InferSent (Conneau et al., 2017) and compute cosine similarity between the centroid of the target and each of the sentence in source. 18 Tasks Different Domains), and DTDD (Different Tasks Different Domains). For the STDD scenario, selection methods show potential when the target domain has the highest similarity to the source domains, based on Jensen Shannon Divergence. As for the DTDD scenario in which we use related tasks (NER and ST) from distant domains (news and conversational domains), using selection does not bring advantage over using all the source data. A possible cause is that, because of data sparsity on the tar"
2020.insights-1.3,2020.acl-main.268,0,0.0613043,"Missing"
2020.insights-1.3,N18-2006,0,0.0194692,"to evaluate the effectiveness of data selection on both the STDD and DTDD scenarios. Specifically, for DTDD we combine data selection and multi-task learning. Given XS , the framework performs data selection based on a score S derived from a set of features. The top m examples are then used to train MTT . In case of STDD, the MTT is a single task sequence tagging model, where we use a biLSTM-CRF model (Lample et al., 2016). As for DTDD, MTT is a hard parameter sharing MTL model, which has been applied to many NLP tasks (Søgaard and Goldberg, 2016; Plank et al., 2016; Changpinyo et al., 2018; Schulz et al., 2018). The performance on the validation set of the target task is then used by the BO optimizer to update the weight of the scoring features. Following Ruder and Plank (2017), the selection process is based on a score S computed as the linear combination of weighted features, which include both similarity and diversity features: Sθ (x) = θ&gt; · φ(x), where θ represents the weight for each feature and φ(x) denotes the feature values of each 3.1 Datasets For NER we use the OntoNotes 5.0 (Pradhan et al., 2012) dataset, which consists of several sections: newswire (NW), talkshows broadcast (BC), telepho"
2020.insights-1.3,P11-2120,0,0.0324287,"to deal with the data distribution mismatch between the source (DS ) and the target data (DT ) (Rosenstein et al., 2005). One solution to alleviate the impact of the mismatch is using data selection, a process for selecting relevant training instances from the source data. Data selection (DS) has been applied in the context of domain adaptation to address changes in the data distribution for various NLP tasks, such as sentiment analysis and POS Tagging (Ruder and Plank, 2017; Liu et al., 2019; Blitzer et al., 2007; Remus, 2012), machine translation (Axelrod et al., 2011), dependency parsing (Søgaard, 2011) and Named Entity Recognition (NER) (Murthy et al., 2018; Zhao et al., 2018). To our knowledge, all existing previous works apply 15 Proceedings of the First Workshop on Insights from Negative Results in NLP, pages 15–21 c Online, November 19, 2020. 2020 Association for Computational Linguistics ISBN 978-1-952148-66-8 instance x. The features are calculated between the representation of XS instances and XT . We use term distribution as the representation of the instances. We use the same similarity and diversity measures as Ruder and Plank (2017). The weights θ are learned through BO by taking"
2020.insights-1.3,P16-2038,0,0.025739,"Plank (2017), based on Bayesian Optimization (BO) (Brochu et al., 2010), to evaluate the effectiveness of data selection on both the STDD and DTDD scenarios. Specifically, for DTDD we combine data selection and multi-task learning. Given XS , the framework performs data selection based on a score S derived from a set of features. The top m examples are then used to train MTT . In case of STDD, the MTT is a single task sequence tagging model, where we use a biLSTM-CRF model (Lample et al., 2016). As for DTDD, MTT is a hard parameter sharing MTL model, which has been applied to many NLP tasks (Søgaard and Goldberg, 2016; Plank et al., 2016; Changpinyo et al., 2018; Schulz et al., 2018). The performance on the validation set of the target task is then used by the BO optimizer to update the weight of the scoring features. Following Ruder and Plank (2017), the selection process is based on a score S computed as the linear combination of weighted features, which include both similarity and diversity features: Sθ (x) = θ&gt; · φ(x), where θ represents the weight for each feature and φ(x) denotes the feature values of each 3.1 Datasets For NER we use the OntoNotes 5.0 (Pradhan et al., 2012) dataset, which consists of"
2020.insights-1.3,N18-2056,0,0.0580326,"Missing"
2020.lrec-1.259,S12-1051,0,0.0864548,"Missing"
2020.lrec-1.259,S14-2004,0,0.116637,"Missing"
2020.lrec-1.259,Q16-1026,0,0.0214798,"the output vectors of BLSTM into a CRF layer, as it is depicted in Figure 1. For each token in the input sequence, first a character-level representation is computed by a CNN with character embeddings as inputs. Then the characterlevel representation vector is concatenated with the word embedding vector to feed the BLSTM network. The CNN for Character-level Representation is an effective approach to extract morphological information (like the prefix or suffix of a word) from characters of words and encode it into neural representations. In NeuroNLP2 the CNN is similar to the one proposed in (Chiu and Nichols, 2016), except that it uses only character embeddings as inputs, without character type. At the second layer each input sequence is presented both forwards and backwards to a bidirectional LSTM, whose output allows to capture past and future information. LSTMs (Hochreiter and Schmidhuber, 1997) are variants of recurrent neural networks (RNNs) designed to cope with gradient vanishing problems. A LSTM unit is composed of three multiplicative gates which control the proportions of information to forget and to pass on to the next time step. The basic idea is to present each sequence forwards and backwar"
2020.lrec-1.259,doddington-etal-2004-automatic,0,0.253517,"to generate new names (e.g., for food names, salmon tacos is a potential food name given the existence of salmon and tacos). 3 4 2112 20/04/2018 https://github.com/kyzhouhzau/BERT-NER I O would O like O to O order O a O salami B-FOOD pizza I-FOOD and O two O mozzarella B-FOOD cheese I-FOOD sandwiches I-FOOD Table 2: Example of IOB annotation of food nominal entities. Nominal entity recognition has been approached with systems based on linguistic knowledge, including morphosyntactic information, chunking, and head identification (Pianta and Tonelli, 2010). In the framework of the ACE program (Doddington et al., 2004) there has been several attempts to develop supervised systems for nominal entities (Haghighi and Klein, 2010), which, however, had to face the problem of the scarcity of annotated data, and, for this reason, were developed for few entity types. Similarly to what is done for named entities, nominal entity recognition has been approached as a sequence labeling task. Given an utterance U = {t1 , t2 , ..., tn } and a set of entity categories C = {c1 , c2 , ..., cm }, the task is to label the tokens in U that refer to entities belonging to the categories in C. As an example, using the IOB format ("
2020.lrec-1.259,I05-5002,0,0.0337384,"Missing"
2020.lrec-1.259,P15-1033,0,0.0184464,"M unit is composed of three multiplicative gates which control the proportions of information to forget and to pass on to the next time step. The basic idea is to present each sequence forwards and backwards to two separate LSTMs and then to concatenate the output to capture past and future information, respectively. The LSTM’s hidden state takes information only from the past, knowing nothing about the future. However, for many tasks it is beneficial to have access to both past (left) and fu2111 ture (right) contexts. A possible solution, whose effectiveness has been proven by previous work (Dyer et al., 2015), is provided by bi-directional LSTMs (BLSTM). (Ma and Hovy, 2016) apply a dropout layer on both the input and output vectors of the BLSTM. Finally, the third layer implemented in NeuroNLP2 is a Conditional Random Fields (CRF) based decoder, which considers dependencies between entity labels in their context and then jointly decodes the best chain of labels for a given input sentence. For example, in NER with standard IOB annotation, an I-token can not follow an O, a constraint which is captured by the CFR layer. Conditional Random Fields (Lafferty et al., 2001) offer several advantages over h"
2020.lrec-1.259,W07-1401,1,0.279345,"5 1] [1 0 0] 0.345 0.398 0.259 0.174 0.396 0.274 0.357 0.111 - M RR1,3 [-1 0 1] -0.213 -0.028 -0.184 -0.015 -0.018 Test M RR2,3 [0 0.5 1] 0.346 0.231 0.356 0.298 0.318 M RR1 [1 0 0] 0.407 0.179 0.357 0.165 0.132 Table 6: Results on the development and test sets for compatibility relation detection. 6.1. Datasets used for Textual Entailment We have tested the performance of a neural approach, based on BERT, on two RTE datasets available for Italian. RTE3 Italian. This is the Italian translation of the RTE-3 dataset carried out during the EU project EXCITEMENT 7 . The RTE-3 dataset for English (Giampiccolo et al., 2007) consists of 1600 text-hypothesis pairs, equally divided into a development set and a test set. While the length of the hypotheses (h) was the same as in the RTE1a and RTE2 datasets, a certain number of texts (t) were longer than in previous datasets, up to a paragraph. Four applications – namely IE, IR, QA and SUM – were considered as settings or contexts for the pairs generation, and 200 pairs were selected for each application in each dataset. RTE Evalita 2009. This is the dataset developed for Evalita 2009 (Bos et al., 2009) tasks. Pairs of texts have be taken from Italian Wikipedia articl"
2020.lrec-1.259,N10-1061,0,0.0133615,"lmon and tacos). 3 4 2112 20/04/2018 https://github.com/kyzhouhzau/BERT-NER I O would O like O to O order O a O salami B-FOOD pizza I-FOOD and O two O mozzarella B-FOOD cheese I-FOOD sandwiches I-FOOD Table 2: Example of IOB annotation of food nominal entities. Nominal entity recognition has been approached with systems based on linguistic knowledge, including morphosyntactic information, chunking, and head identification (Pianta and Tonelli, 2010). In the framework of the ACE program (Doddington et al., 2004) there has been several attempts to develop supervised systems for nominal entities (Haghighi and Klein, 2010), which, however, had to face the problem of the scarcity of annotated data, and, for this reason, were developed for few entity types. Similarly to what is done for named entities, nominal entity recognition has been approached as a sequence labeling task. Given an utterance U = {t1 , t2 , ..., tn } and a set of entity categories C = {c1 , c2 , ..., cm }, the task is to label the tokens in U that refer to entities belonging to the categories in C. As an example, using the IOB format (Inside-Outside-Beginning, (Ramshaw and Marcus, 1995)), the sentence “I would like to order a salami pizza and"
2020.lrec-1.259,N15-1097,0,0.153153,"the results on the DPD dataset of NeuroNLP2 + NNg , compared to the others, show that NNg correctly generalizes nominal entities from the gazetteer, improving both Recall and Precision with respect to the multi-token approach. 5. Lexical Relations among Words This section addresses the capacity of neural models to detect semantic relations (e.g., synonymy, semantic similarity, entailment, compatibility) between words (or phrases, like the nominal expressions described in Section 4.2). We focus our experiments on the compatibility relation, and adopt the definition of compatibility proposed by Kruszewski and Baroni (2015): two linguistic expressions w1 and w2 are compatible iff, in a reasonably normal state of affairs, they can both truthfully refer to the same thing. If they cannot, then they are incompatible. Under this definition compatibility is a symmetric relation, which is different both from subsumption, which in not symmetric, from semantic similarity (Agirre et al., 2012) (two expressions can be compatible although not semantically similar, like aperitif and chips, and from textual entailment (Dagan et al., 2005), as entailment is not a symmetric relation. 5.1. Task definition The task is defined as"
2020.lrec-1.259,N16-1030,0,0.0338423,"Italian. 2.3. A Sequence Labeling Neural Architecture: NeuroNLP2 In this section we introduce NeuroNLP2 (Ma and Hovy, 2016), a reference neural architecture for sequence labeling in NLP that achieved state-of-the-art performance for named entity recognition for English on the ConLL-2003 dataset. Specifically, we describe the most recent implementation of the system in Pytorch distributed by the authors2 . We selected this system not only for its state-ofthe-art performance and for code availability, but also for the peculiar structure of the network, which is common to other works, including (Lample et al., 2016). The system is composed of three layers (Figure 1): (i) a CNN that allows to extract information from the input text without any pre-processing; (ii) a bidirectional LSTM layer that presents each sequence forwards and backwards to two sep1 https://github.com/google-research/bert/ blob/master/multilingual.md 2 https://github.com/XuezheMax/NeuroNLP2 Figure 1: The main NeuroNLP2 structure. Dashed arrows indicate dropout layers applied on both the input and output vectors of BLSTM. arate LSTMs; (iii) a CRF layer that decodes the best label sequence. NeuroNLP2 constructs a neural network model by"
2020.lrec-1.259,P16-1101,0,0.0307357,"proportions of information to forget and to pass on to the next time step. The basic idea is to present each sequence forwards and backwards to two separate LSTMs and then to concatenate the output to capture past and future information, respectively. The LSTM’s hidden state takes information only from the past, knowing nothing about the future. However, for many tasks it is beneficial to have access to both past (left) and fu2111 ture (right) contexts. A possible solution, whose effectiveness has been proven by previous work (Dyer et al., 2015), is provided by bi-directional LSTMs (BLSTM). (Ma and Hovy, 2016) apply a dropout layer on both the input and output vectors of the BLSTM. Finally, the third layer implemented in NeuroNLP2 is a Conditional Random Fields (CRF) based decoder, which considers dependencies between entity labels in their context and then jointly decodes the best chain of labels for a given input sentence. For example, in NER with standard IOB annotation, an I-token can not follow an O, a constraint which is captured by the CFR layer. Conditional Random Fields (Lafferty et al., 2001) offer several advantages over hidden Markov models and stochastic grammars for such tasks, includ"
2020.lrec-1.259,W19-5807,1,0.855983,"Missing"
2020.lrec-1.259,D14-1162,0,0.0826463,"Missing"
2020.lrec-1.259,N18-1202,0,0.0365747,"tc. Three families of word embeddings can be identified: • Bag of words based. The original word order independent models like Word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). • Attention (Transformer) based. Embeddings generated by BERT (Devlin et al., 2018), which has produced state-of-the-art results to date in downstream 2110 tasks like NER, Q&A, classification etc. BERT takes into account the order of words in a sentence but is based on attention mechanism as opposed to sequence models like ELMo. • RNN family based. Sequence models (ELMo) that produce word embeddings (Peters et al., 2018). ELMo uses stacked bidirectional LSTMs to generate word embeddings that have different properties based on the layer that generates them. 2.2. BERT BERT (Devlin et al., 2018) is a deep learning model that has given state-of-the-art results on a wide variety of natural language processing tasks. It stands for Bidirectional Encoder Representations for Transformers. It has been pretrained on Wikipedia and BooksCorpus and requires taskspecific fine-tuning. BERT is available pre-trained on domain-specific corpora. E.g., Clinical BERT (BERT pre-trained on a corpus of clinical notes) and sciBERT (Pr"
2020.lrec-1.259,S10-1036,0,0.00948324,"ine tokens of one entity name with tokens of another entity name to generate new names (e.g., for food names, salmon tacos is a potential food name given the existence of salmon and tacos). 3 4 2112 20/04/2018 https://github.com/kyzhouhzau/BERT-NER I O would O like O to O order O a O salami B-FOOD pizza I-FOOD and O two O mozzarella B-FOOD cheese I-FOOD sandwiches I-FOOD Table 2: Example of IOB annotation of food nominal entities. Nominal entity recognition has been approached with systems based on linguistic knowledge, including morphosyntactic information, chunking, and head identification (Pianta and Tonelli, 2010). In the framework of the ACE program (Doddington et al., 2004) there has been several attempts to develop supervised systems for nominal entities (Haghighi and Klein, 2010), which, however, had to face the problem of the scarcity of annotated data, and, for this reason, were developed for few entity types. Similarly to what is done for named entities, nominal entity recognition has been approached as a sequence labeling task. Given an utterance U = {t1 , t2 , ..., tn } and a set of entity categories C = {c1 , c2 , ..., cm }, the task is to label the tokens in U that refer to entities belongin"
2020.lrec-1.259,P19-1493,0,0.0162728,"ted in Devlin et al. (2018). In particular, some aspects of the SENTIPOLC 2016 dataset are difficult to address with BERT. For example, the fact that the dataset is strongly unbalanced, usually an important aspect to take into account with a supervised system like BERT. To reduce this effect we down-sample the most common polarity, but even in this case, the result is not competitive with the state of the art. On the other hand, is important to notice that in both cases (SENTIPOLC 2016 and ABSITA 2018), the models were not fine-tuned on Italian, but only on the task. According to the paper by Pires et al. (2019), multilingual BERT is able to perform some cross-lingual adaptation but it is reasonable to think that in a task more related to semantic a deeper process of fine-tuning is needed. dataset SENTIPOLC 2016 - Task 2 ABSITA 2018 - Task ACD ABSITA 2018 - Task ACP BERT 52.17 74.05 68.13 SotA 66.38 81.08 76.73 Table 8: Application of BERT to Sentiment Analysis. 8. Text Classification Finally, we focus on text classification applied to radiological reports in Italian. Radiological reporting generates a large amount of free-text clinical narratives, a potentially valuable source of information for imp"
2020.lrec-1.259,W18-5446,0,0.0181501,"rmation extraction from Italian texts. We carried on experiments using available datasets on both sequence tagging (i.e., named entity recognition, nominal entity recognition) and classification tasks (i.e., lexical relations among words, semantic relations among sentences, sentiment analysis, text classification). We consider this paper as a contribution in the direction of developing benchmarks encompassing a variety of tasks in order to favour models that share general linguistic knowledge across tasks. This is very much in the spirit of GLUE, the General Language Understanding Evaluation (Wang et al., 2018), a collection of resources for training, evaluating, and analyzing natural language understanding systems. The paper is structured as follows. Section 2 reports basic notions about deep learning for NLP that will be used for our experiments. Sections 3 and 4 focus on sequence tagging tasks, named entity recognition and nominal entity recognition, respectively. Sections 4-7 report on classification tasks: lexical relations, textual entailment, sentiment analysis and text classification. Finally, Section 9 discusses our achievement and proposes work for the future. 2. Deep Learning for NLP This"
2020.lrec-1.407,gavrilidou-etal-2012-meta,1,0.919419,"Missing"
2020.lrec-1.407,2020.lrec-1.420,1,0.860379,"Missing"
2020.lrec-1.407,L18-1213,1,0.894888,"Missing"
2020.lrec-1.407,piperidis-etal-2014-meta,1,0.824391,"ween 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META-NET results (Rehm and Uszkoreit, 2012), funded a"
2020.lrec-1.407,piperidis-2012-meta,1,0.92358,"n 34 European countries. META-NET was, between 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META"
2020.lrec-1.407,L16-1251,1,0.865781,"Missing"
2020.lrec-1.407,2020.lrec-1.413,1,0.785764,"Missing"
2020.paclic-1.20,N06-1003,0,0.113844,"19), to the use of generative models (Goodfellow et al., 2014) for generating synthetic data. Recently, data augmentation has been applied to various NLP tasks, including text classification (Wei and Zou, 2019; Wang and Yang, 2015), parsing (Sahin and Steedman, 2018; Vania et al., 2019), and machine translation (Fadaee et al., 2017). Augmentation techniques for NLP tasks range from operations on tokens (e.g., substituting, deleting) (Wang and Yang, 2015; Kobayashi, 2018; Wei and Zou, 2019), to manipulation of the sentence structure (Sahin and Steedman, 2018), to paraphrase-based augmentation (Callison-Burch et al., 2006). Data augmentation has been also experimented in the context of slot filling and intent classification. Particularly, recent methods have focused on the application of generative models to produce synthetic utterances. Hou et al. (2018) proposes a method that separates the utterance generation from the slot values realization. A sequence to sequence based model is used to generate utterances for a given intent with slot values placeholders (i.e., delexicalized), and then words in the training data that occur in similar contexts of the placeholder are inserted as the slot values. Zhao et al. ("
2020.paclic-1.20,N19-1423,0,0.0281906,"and rotate (Sahin and Steedman, 2018) operations based on a dependency parse structure. We investigate the effect of lightweight augmentation both on typical biLSTM-based joint SF and IC models, and on large pre-trained LM transformers based models, in both cases with a limited data setting. Our contributions are as follows: • We present a lightweight text span and sentence level augmentation for SF and IC. We show that, despite its simplicity, lightweight augmentation is competitive with more complex, deep learning-based, augmentation. • We show that big self-supervised models, such as BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), and ALBERT (Lan et al., 2020) can perform well under a low data regime, and still benefit from lightweight augmentation. • The combination of our span based augmentation and transfer learning (e.g. BERT finetuning) yields the best performance for most cases. 2 Lightweight Data Augmentation Given the original training data D, DA aims to generate additional training data D0 . For each sentence S in D, an augmentation operation is applied N times, which can be empirically determined. Each augmented sentence S 0 is added to D0 , and the union of D and D0 is then used"
2020.paclic-1.20,2020.acl-main.225,0,0.0178464,"then sample a sp0 from SP 0 and replace sp in S with sp0 to produce S 0 . Notice that the slot values in sp0 are not necessary synonyms of the original slot value, although their slot label must be the same to preserve semantic compatibility. 2.2 Slot Substitution with Language Model (S LOT-S UB -LM) Our second lightweight method, S LOT-S UB -LM, shares the goal with S LOT-S UB, i.e., to substitute sp with sp0 . However, we do not use D to look for substitute candidates, instead we use a large pre-trained language model to generate the slot value candidates, using the fill-in-the-blank style (Donahue et al., 2020). The expectation is that large pre-trained LMs, being trained on massive amount of data, can produce a sensible text span given a particular sentence context, and possibly produce slot values that do not occur in D. While we use BERT for our purpose, virtually any pre-trained LM can be used for S LOT-S UB -LM. Existing works on DA using LMs (Kobayashi, 2018; Kumar et al., 2020) are applied on text classification to replace random tokens in the text, which is not directly applicable to SF. Our approach focuses on spans conveying slot values, and include a filtering mechanism to reject retrieve"
2020.paclic-1.20,P17-2090,0,0.17712,"hey suffer in data scarcity situations, which regularly happen when new domains are added to the system to support new functionalities. One of the methods proposed to alleviate data scarcity is data augmentation (DA), which aims to automatically increase the size of the training data by applying data transformations, ranging from simple word substitution to sentence generation. Recently, DA has shown promising potential for several NLP tasks, including text classification (Wei and Zou, 2019; Wang and Yang, 2015), parsing (Sahin and Steedman, 2018; Vania et al., 2019), and machine translation (Fadaee et al., 2017). As for SF and IC, DA approaches typically generate synthetic utterances by leveraging Seq2Seq (Hou et al., 2018; Zhao et al., 2019; Kurata et al., 2016), Conditional VAE (Yoo et al., 2019), or pre-trained Natural Language Generation (NLG) models (Peng et al., 2020). Such approaches make use of in-domain data, and are relatively heavyweight, as they require training neural models, which may involve several phases to generate, filter, and rank the produced augmented data, thus requiring more computation time. It is also relatively challenging for deep learningbased models to generate semantica"
2020.paclic-1.20,P18-1082,0,0.0215577,"e LM to predict the new tokens in the span. For instance, we give “show me the round trip flight from Atlanta to Denver” to the LM for blank prediction. Practically, blank tokens are encoded as special [MASK] tokens3 to let the pre-trained LM performing prediction. The decoding of the new tokens is carried out iteratively from left to right (Figure 1 Middle) and, to produce the surface form of a token, we apply nucleus sampling (Holtzman et al., 2020) using the top-p portion of the probability mass. Nucleus sampling has been empirically shown to be better than beam search, and top-k sampling (Fan et al., 2018) to produce fluent and diverse texts. Filtering. While pre-trained LMs are expected to generate sensible replacements for a span in the utterance, a possible issue is that the new slot span is not semantically consistent with the original one. For example, for the original span “cheapest” in “show me the cheapest round trip flight from Atlanta to Denver”, the LM could output “earliest” as a substitution, which does not fit the slot label COST RELATIVE . To mitigate this issue, we use a binary sentence classifier as a filter (S LOT-S UB LM+Filter) to decide whether S and S 0 are semantically co"
2020.paclic-1.20,N18-2118,0,0.0192441,"derstanding component is responsible for parsing the user utterance into a semantic representation. This is often modeled as a semantic frame (Tur and De Mori, 2011), and typically involves slot filling and intent classification. For example, in the utterance “book in Southern Shores for 8 at Ariston Cafe”, the intent is BOOKING A RESTAURANT and the corresponding slot values and slot names are “Southern Shores” (C ITY NAME), “8” (N UMBER O F P EOPLE), and “Ariston Cafe” (R ESTAURANT NAME). Bernardo Magnini Fondazione Bruno Kessler magnini@fbk.eu Although neural-based models (Qin et al., 2019; Goo et al., 2018; Mesnil et al., 2015) have achieved stellar performance in slot filling (SF) and intent classification (IC), their performance depend on the availability of large labeled datasets. Consequently, they suffer in data scarcity situations, which regularly happen when new domains are added to the system to support new functionalities. One of the methods proposed to alleviate data scarcity is data augmentation (DA), which aims to automatically increase the size of the training data by applying data transformations, ranging from simple word substitution to sentence generation. Recently, DA has shown"
2020.paclic-1.20,N18-1108,0,0.0295555,"transfer learning (e.g. BERT finetuning) yields the best performance for most cases. 2 Lightweight Data Augmentation Given the original training data D, DA aims to generate additional training data D0 . For each sentence S in D, an augmentation operation is applied N times, which can be empirically determined. Each augmented sentence S 0 is added to D0 , and the union of D and D0 is then used to train the model for SF and IC. We describe the lightweight DA operations in the following subsections. 2.1 Slot Substitution (S LOT-S UB) Our first lightweight method, slot substitution, is similar to Gulordava et al. (2018), which is based on substituting a token in a sentence with another token with a consistent syntactic annotation (i.e., part-of-speech or morphology tags). However, unlike Gulordava et al. (2018), our method is not limited to single tokens. As slot filling is a semantic task, rather than syntactic, we can naturally extend the method from single tokens (i.e., slot names composed by a single token) to multiple tokens (i.e., slot names composed by multiple tokens, or spans1 ), still preserving the semantics associated to a certain slot. Practically, for slot substitution we take advantage of the"
2020.paclic-1.20,H90-1021,0,0.317486,"Missing"
2020.paclic-1.20,C18-1105,0,0.164986,"(e.g. BERT finetuning) yields the best performance for most cases. 2 Lightweight Data Augmentation Given the original training data D, DA aims to generate additional training data D0 . For each sentence S in D, an augmentation operation is applied N times, which can be empirically determined. Each augmented sentence S 0 is added to D0 , and the union of D and D0 is then used to train the model for SF and IC. We describe the lightweight DA operations in the following subsections. 2.1 Slot Substitution (S LOT-S UB) Our first lightweight method, slot substitution, is similar to Gulordava et al. (2018), which is based on substituting a token in a sentence with another token with a consistent syntactic annotation (i.e., part-of-speech or morphology tags). However, unlike Gulordava et al. (2018), our method is not limited to single tokens. As slot filling is a semantic task, rather than syntactic, we can naturally extend the method from single tokens (i.e., slot names composed by a single token) to multiple tokens (i.e., slot names composed by multiple tokens, or spans1 ), still preserving the semantics associated to a certain slot. Practically, for slot substitution we take advantage of the"
2020.paclic-1.20,N18-2072,0,0.306423,"ith S LOT-S UB, i.e., to substitute sp with sp0 . However, we do not use D to look for substitute candidates, instead we use a large pre-trained language model to generate the slot value candidates, using the fill-in-the-blank style (Donahue et al., 2020). The expectation is that large pre-trained LMs, being trained on massive amount of data, can produce a sensible text span given a particular sentence context, and possibly produce slot values that do not occur in D. While we use BERT for our purpose, virtually any pre-trained LM can be used for S LOT-S UB -LM. Existing works on DA using LMs (Kobayashi, 2018; Kumar et al., 2020) are applied on text classification to replace random tokens in the text, which is not directly applicable to SF. Our approach focuses on spans conveying slot values, and include a filtering mechanism to reject retrieved slot spans that are not semantically compatible. Generating New Slot Values. Given an utterance consisting of one or more slot value spans, we “blank” one of the span and then let the LM to predict the new tokens in the span. For instance, we give “show me the round trip flight from Atlanta to Denver” to the LM for blank prediction. Practically, blank toke"
2020.paclic-1.20,2020.lifelongnlp-1.3,0,0.0229482,".e., to substitute sp with sp0 . However, we do not use D to look for substitute candidates, instead we use a large pre-trained language model to generate the slot value candidates, using the fill-in-the-blank style (Donahue et al., 2020). The expectation is that large pre-trained LMs, being trained on massive amount of data, can produce a sensible text span given a particular sentence context, and possibly produce slot values that do not occur in D. While we use BERT for our purpose, virtually any pre-trained LM can be used for S LOT-S UB -LM. Existing works on DA using LMs (Kobayashi, 2018; Kumar et al., 2020) are applied on text classification to replace random tokens in the text, which is not directly applicable to SF. Our approach focuses on spans conveying slot values, and include a filtering mechanism to reject retrieved slot spans that are not semantically compatible. Generating New Slot Values. Given an utterance consisting of one or more slot value spans, we “blank” one of the span and then let the LM to predict the new tokens in the span. For instance, we give “show me the round trip flight from Atlanta to Denver” to the LM for blank prediction. Practically, blank tokens are encoded as spe"
2020.paclic-1.20,D16-1223,0,0.0184927,"proposed to alleviate data scarcity is data augmentation (DA), which aims to automatically increase the size of the training data by applying data transformations, ranging from simple word substitution to sentence generation. Recently, DA has shown promising potential for several NLP tasks, including text classification (Wei and Zou, 2019; Wang and Yang, 2015), parsing (Sahin and Steedman, 2018; Vania et al., 2019), and machine translation (Fadaee et al., 2017). As for SF and IC, DA approaches typically generate synthetic utterances by leveraging Seq2Seq (Hou et al., 2018; Zhao et al., 2019; Kurata et al., 2016), Conditional VAE (Yoo et al., 2019), or pre-trained Natural Language Generation (NLG) models (Peng et al., 2020). Such approaches make use of in-domain data, and are relatively heavyweight, as they require training neural models, which may involve several phases to generate, filter, and rank the produced augmented data, thus requiring more computation time. It is also relatively challenging for deep learningbased models to generate semantically preserving synthetic utterances in limited data settings. In this paper, we show that lightweight augmentation, a set of simple DA methods that produc"
2020.paclic-1.20,D19-1214,0,0.0114743,"spoken language understanding component is responsible for parsing the user utterance into a semantic representation. This is often modeled as a semantic frame (Tur and De Mori, 2011), and typically involves slot filling and intent classification. For example, in the utterance “book in Southern Shores for 8 at Ariston Cafe”, the intent is BOOKING A RESTAURANT and the corresponding slot values and slot names are “Southern Shores” (C ITY NAME), “8” (N UMBER O F P EOPLE), and “Ariston Cafe” (R ESTAURANT NAME). Bernardo Magnini Fondazione Bruno Kessler magnini@fbk.eu Although neural-based models (Qin et al., 2019; Goo et al., 2018; Mesnil et al., 2015) have achieved stellar performance in slot filling (SF) and intent classification (IC), their performance depend on the availability of large labeled datasets. Consequently, they suffer in data scarcity situations, which regularly happen when new domains are added to the system to support new functionalities. One of the methods proposed to alleviate data scarcity is data augmentation (DA), which aims to automatically increase the size of the training data by applying data transformations, ranging from simple word substitution to sentence generation. Rece"
2020.paclic-1.20,D18-1545,0,0.14358,"ce depend on the availability of large labeled datasets. Consequently, they suffer in data scarcity situations, which regularly happen when new domains are added to the system to support new functionalities. One of the methods proposed to alleviate data scarcity is data augmentation (DA), which aims to automatically increase the size of the training data by applying data transformations, ranging from simple word substitution to sentence generation. Recently, DA has shown promising potential for several NLP tasks, including text classification (Wei and Zou, 2019; Wang and Yang, 2015), parsing (Sahin and Steedman, 2018; Vania et al., 2019), and machine translation (Fadaee et al., 2017). As for SF and IC, DA approaches typically generate synthetic utterances by leveraging Seq2Seq (Hou et al., 2018; Zhao et al., 2019; Kurata et al., 2016), Conditional VAE (Yoo et al., 2019), or pre-trained Natural Language Generation (NLG) models (Peng et al., 2020). Such approaches make use of in-domain data, and are relatively heavyweight, as they require training neural models, which may involve several phases to generate, filter, and rank the produced augmented data, thus requiring more computation time. It is also relati"
2020.paclic-1.20,D19-1102,0,0.0909786,"ity of large labeled datasets. Consequently, they suffer in data scarcity situations, which regularly happen when new domains are added to the system to support new functionalities. One of the methods proposed to alleviate data scarcity is data augmentation (DA), which aims to automatically increase the size of the training data by applying data transformations, ranging from simple word substitution to sentence generation. Recently, DA has shown promising potential for several NLP tasks, including text classification (Wei and Zou, 2019; Wang and Yang, 2015), parsing (Sahin and Steedman, 2018; Vania et al., 2019), and machine translation (Fadaee et al., 2017). As for SF and IC, DA approaches typically generate synthetic utterances by leveraging Seq2Seq (Hou et al., 2018; Zhao et al., 2019; Kurata et al., 2016), Conditional VAE (Yoo et al., 2019), or pre-trained Natural Language Generation (NLG) models (Peng et al., 2020). Such approaches make use of in-domain data, and are relatively heavyweight, as they require training neural models, which may involve several phases to generate, filter, and rank the produced augmented data, thus requiring more computation time. It is also relatively challenging for"
2020.paclic-1.20,D15-1306,0,0.187372,"ification (IC), their performance depend on the availability of large labeled datasets. Consequently, they suffer in data scarcity situations, which regularly happen when new domains are added to the system to support new functionalities. One of the methods proposed to alleviate data scarcity is data augmentation (DA), which aims to automatically increase the size of the training data by applying data transformations, ranging from simple word substitution to sentence generation. Recently, DA has shown promising potential for several NLP tasks, including text classification (Wei and Zou, 2019; Wang and Yang, 2015), parsing (Sahin and Steedman, 2018; Vania et al., 2019), and machine translation (Fadaee et al., 2017). As for SF and IC, DA approaches typically generate synthetic utterances by leveraging Seq2Seq (Hou et al., 2018; Zhao et al., 2019; Kurata et al., 2016), Conditional VAE (Yoo et al., 2019), or pre-trained Natural Language Generation (NLG) models (Peng et al., 2020). Such approaches make use of in-domain data, and are relatively heavyweight, as they require training neural models, which may involve several phases to generate, filter, and rank the produced augmented data, thus requiring more"
2020.paclic-1.20,D19-1670,0,0.191957,"F) and intent classification (IC), their performance depend on the availability of large labeled datasets. Consequently, they suffer in data scarcity situations, which regularly happen when new domains are added to the system to support new functionalities. One of the methods proposed to alleviate data scarcity is data augmentation (DA), which aims to automatically increase the size of the training data by applying data transformations, ranging from simple word substitution to sentence generation. Recently, DA has shown promising potential for several NLP tasks, including text classification (Wei and Zou, 2019; Wang and Yang, 2015), parsing (Sahin and Steedman, 2018; Vania et al., 2019), and machine translation (Fadaee et al., 2017). As for SF and IC, DA approaches typically generate synthetic utterances by leveraging Seq2Seq (Hou et al., 2018; Zhao et al., 2019; Kurata et al., 2016), Conditional VAE (Yoo et al., 2019), or pre-trained Natural Language Generation (NLG) models (Peng et al., 2020). Such approaches make use of in-domain data, and are relatively heavyweight, as they require training neural models, which may involve several phases to generate, filter, and rank the produced augmented data"
2020.paclic-1.20,D19-1375,0,0.112648,"One of the methods proposed to alleviate data scarcity is data augmentation (DA), which aims to automatically increase the size of the training data by applying data transformations, ranging from simple word substitution to sentence generation. Recently, DA has shown promising potential for several NLP tasks, including text classification (Wei and Zou, 2019; Wang and Yang, 2015), parsing (Sahin and Steedman, 2018; Vania et al., 2019), and machine translation (Fadaee et al., 2017). As for SF and IC, DA approaches typically generate synthetic utterances by leveraging Seq2Seq (Hou et al., 2018; Zhao et al., 2019; Kurata et al., 2016), Conditional VAE (Yoo et al., 2019), or pre-trained Natural Language Generation (NLG) models (Peng et al., 2020). Such approaches make use of in-domain data, and are relatively heavyweight, as they require training neural models, which may involve several phases to generate, filter, and rank the produced augmented data, thus requiring more computation time. It is also relatively challenging for deep learningbased models to generate semantically preserving synthetic utterances in limited data settings. In this paper, we show that lightweight augmentation, a set of simple"
2021.sigdial-1.25,W19-5932,0,0.339048,"data, moving from rule-based to learning methods for dialogue state updating, and addressing long-term dependency, a crucial aspect in dialogue. Furthermore, encouraged by the considerable success in modeling single domain dialogues (Henderson et al., 2014c; Wen et al., 2017; Mrkˇsi´c et al., 2017a), research on DST has recently moved toward building models that can handle multiple domains (Wu et al., 2019; Zhang et al., 2019; Zhong et al., 2018; Heck et al., 2020), and that are flexible enough to be adapted to new domains (Rastogi et al., 2020; Balaraman and Magnini, 2021; Lin et al., 2020; Gao et al., 2019). Although such rapid signs of progress have generated an impressive amount of research in DST, including several datasets and experimental material, to the best of our knowledge, such a massive amount of recent work has been only poorly documented (Williams et al., 2016a; Chen et al., 2017), and there is not an updated survey of the field. This paper intends to fill such a gap, providing 239 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 239–251 July 29–31, 2021. ©2021 Association for Computational Linguistics and then using an update mec"
2021.sigdial-1.25,P16-1154,0,0.0149534,"track slot values even if they are not defined in the ontology. Two major approaches for dynamic ontology models are: i) copy the slot value from the user input to the output; and ii) generate the slot value as the output. Figure 3 presents the schema of a model using the combination of both approaches. One significant difference between static ontology and dynamic ontology models is that while the output vocabulary in the static ontology is limited (i.e., equal to # of slot-values), in a dynamic ontology setting the output vocabulary is much larger. Copy and pointer networks. Copy mechanism (Gu et al., 2016) and pointer networks (Vinyals et al., 2015) are the main approaches in neural networks to make predictions on the input tokens. They both rely on the attention mechanism (Bahdanau et al., 2015) to obtain scores over the input tokens. (Xu and Hu, 2018) proposed an end-to-end DST architecture based on pointer networks, showing efficient tracking of unseen slot values in a datadriven approach on the DSTC2 dataset. However, since pointer networks can only make predictions on the input tokens, they cannot be directly applied for all slots and require postprocessing of predicted values. (Wu et al.,"
2021.sigdial-1.25,2020.sigdial-1.4,0,0.0437895,"Missing"
2021.sigdial-1.25,W14-4337,0,0.0956635,"Missing"
2021.sigdial-1.25,W13-4065,0,0.0172545,"Missing"
2021.sigdial-1.25,W14-4340,0,0.498104,"dberg, 2017; Chen et al., 2017), has been pushed forward by neural networkbased approaches. The DST task actually merges some aspects of natural language understanding in dialogues, although it is more complex than the standard slot filling task. In fact, while slot filling involves predicting the slot-value pairs referred in a particular turn in dialogue (Louvan and Magnini, 2020), DST involves predicting the slot-value pairs at the dialogue level until the current turn. The complexity of DST has driven research to propose various neural approaches, including recurrent neural networks-based (Henderson et al., 2014c; Henderson et al., 2014; Wen et al., 2017; Xu and Hu, 2018; Ren et al., 2018), attention-based models (Wu et al., 2019; Xu and Hu, 2018; Nouri and Hosseini-Asl, 2018), and the very recent transformer-based models (Heck et al., 2020; Kim et al., 2020; Zhang et al., 2019; Lee et al., 2019; Rastogi et al., 2020; Balaraman and Magnini, 2021; Lin et al., 2020). In addition, the rapid progress of NLP has provided technologies to address several DST challenges, including predicting slot-values that are not present in training data, moving from rule-based to learning methods for dialogue state updat"
2021.sigdial-1.25,2020.acl-main.53,0,0.533529,"hile slot filling involves predicting the slot-value pairs referred in a particular turn in dialogue (Louvan and Magnini, 2020), DST involves predicting the slot-value pairs at the dialogue level until the current turn. The complexity of DST has driven research to propose various neural approaches, including recurrent neural networks-based (Henderson et al., 2014c; Henderson et al., 2014; Wen et al., 2017; Xu and Hu, 2018; Ren et al., 2018), attention-based models (Wu et al., 2019; Xu and Hu, 2018; Nouri and Hosseini-Asl, 2018), and the very recent transformer-based models (Heck et al., 2020; Kim et al., 2020; Zhang et al., 2019; Lee et al., 2019; Rastogi et al., 2020; Balaraman and Magnini, 2021; Lin et al., 2020). In addition, the rapid progress of NLP has provided technologies to address several DST challenges, including predicting slot-values that are not present in training data, moving from rule-based to learning methods for dialogue state updating, and addressing long-term dependency, a crucial aspect in dialogue. Furthermore, encouraged by the considerable success in modeling single domain dialogues (Henderson et al., 2014c; Wen et al., 2017; Mrkˇsi´c et al., 2017a), research on DST has re"
2021.sigdial-1.25,P19-1546,0,0.231283,"the slot-value pairs referred in a particular turn in dialogue (Louvan and Magnini, 2020), DST involves predicting the slot-value pairs at the dialogue level until the current turn. The complexity of DST has driven research to propose various neural approaches, including recurrent neural networks-based (Henderson et al., 2014c; Henderson et al., 2014; Wen et al., 2017; Xu and Hu, 2018; Ren et al., 2018), attention-based models (Wu et al., 2019; Xu and Hu, 2018; Nouri and Hosseini-Asl, 2018), and the very recent transformer-based models (Heck et al., 2020; Kim et al., 2020; Zhang et al., 2019; Lee et al., 2019; Rastogi et al., 2020; Balaraman and Magnini, 2021; Lin et al., 2020). In addition, the rapid progress of NLP has provided technologies to address several DST challenges, including predicting slot-values that are not present in training data, moving from rule-based to learning methods for dialogue state updating, and addressing long-term dependency, a crucial aspect in dialogue. Furthermore, encouraged by the considerable success in modeling single domain dialogues (Henderson et al., 2014c; Wen et al., 2017; Mrkˇsi´c et al., 2017a), research on DST has recently moved toward building models th"
2021.sigdial-1.25,2020.acl-main.703,0,0.0260871,"domains, pre-trained language models were used to encode the user input representation and domain/slot representations (Lee et al., 2019; Kim et al., 2020; Heck et al., 2020; Rastogi et al., 2020; Balaraman and Magnini, 2021). In addition, the schema guided dataset enabled models to be able to predict dialogue states for any domains that adopt the proposed schema, paving the way for further progress in zero-shot learning approaches for DST (Rastogi et al., 2020; Balaraman and Magnini, 2021; Gao et al., 2019). Finally, (Lin et al., 2020) used the pre-trained T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) language model, and proposed a minimalist transfer learning approach called MinTL. Unlike other models that predict the dialogue state, MinTL generates the change in the dialogue state as a Levenshtein belief state. This unique approach showed more robust results in low resource domains. 246 7.2 Data Augmentation and Data-efficient Models The high cost of data acquisition for annotated dialogues has pushed researchers to look for alternative options. Among them, data augmentation allows generating additional training from existing data. In addition, the cost of dialogue collection makes model"
2021.sigdial-1.25,2020.emnlp-main.273,0,0.308543,"n and Magnini, 2020), DST involves predicting the slot-value pairs at the dialogue level until the current turn. The complexity of DST has driven research to propose various neural approaches, including recurrent neural networks-based (Henderson et al., 2014c; Henderson et al., 2014; Wen et al., 2017; Xu and Hu, 2018; Ren et al., 2018), attention-based models (Wu et al., 2019; Xu and Hu, 2018; Nouri and Hosseini-Asl, 2018), and the very recent transformer-based models (Heck et al., 2020; Kim et al., 2020; Zhang et al., 2019; Lee et al., 2019; Rastogi et al., 2020; Balaraman and Magnini, 2021; Lin et al., 2020). In addition, the rapid progress of NLP has provided technologies to address several DST challenges, including predicting slot-values that are not present in training data, moving from rule-based to learning methods for dialogue state updating, and addressing long-term dependency, a crucial aspect in dialogue. Furthermore, encouraged by the considerable success in modeling single domain dialogues (Henderson et al., 2014c; Wen et al., 2017; Mrkˇsi´c et al., 2017a), research on DST has recently moved toward building models that can handle multiple domains (Wu et al., 2019; Zhang et al., 2019; Z"
2021.sigdial-1.25,N18-1187,0,0.0257365,"Missing"
2021.sigdial-1.25,P18-2069,0,0.0300303,"Missing"
2021.sigdial-1.25,2020.coling-main.42,1,0.522817,"as downstream components, like the dialog manager, rely on the dialogue state to choose the next action of the system. In recent years the performance of several natural language processing (NLP) tasks, including dialogue state tracking (Goldberg, 2017; Chen et al., 2017), has been pushed forward by neural networkbased approaches. The DST task actually merges some aspects of natural language understanding in dialogues, although it is more complex than the standard slot filling task. In fact, while slot filling involves predicting the slot-value pairs referred in a particular turn in dialogue (Louvan and Magnini, 2020), DST involves predicting the slot-value pairs at the dialogue level until the current turn. The complexity of DST has driven research to propose various neural approaches, including recurrent neural networks-based (Henderson et al., 2014c; Henderson et al., 2014; Wen et al., 2017; Xu and Hu, 2018; Ren et al., 2018), attention-based models (Wu et al., 2019; Xu and Hu, 2018; Nouri and Hosseini-Asl, 2018), and the very recent transformer-based models (Heck et al., 2020; Kim et al., 2020; Zhang et al., 2019; Lee et al., 2019; Rastogi et al., 2020; Balaraman and Magnini, 2021; Lin et al., 2020). I"
2021.sigdial-1.25,P15-2130,0,0.0508869,"Missing"
2021.sigdial-1.25,P17-1163,0,0.0427233,"Missing"
2021.sigdial-1.25,P18-2018,0,0.0339053,"Missing"
2021.sigdial-1.25,Q17-1022,0,0.0431993,"Missing"
2021.sigdial-1.25,D18-1299,0,0.0283462,"Missing"
2021.sigdial-1.25,N18-3006,0,0.067366,"Missing"
2021.sigdial-1.25,E17-1042,0,0.0465883,"Missing"
2021.sigdial-1.25,Q15-1025,0,0.0334868,"data sets for the dialogue state tracking task. Figure 2: Left: model with softmax activation to predict over all slot-values, Right: model using value representations to predict the score. StateNet, a DST sharing the parameters for all slots, thus reducing the number of model parameters. StateNet combines a n-gram input feature representation with a slot representation, and uses long short term memory (LSTM) to encode them into a single vector. The value representation is then compared with the encoded vector to obtain the score for each slot-value. A semantically specialised Paragram-SL999 (Wieting et al., 2015) was used to encode the tokens. Compared with fully statistical NBT, StateNet achieves high performance even with a rule-based update function. RNN and latency in DST. A relevant issue for DST models is prediction time, due to the number of dialogue states they have to consider at each dialogue turn. (Zhong et al., 2018) combined both a shared representation and a slot-specific representation in the Global-Locally Self Attentive Dialogue State Tracker (GLAD). The GLAD model consists of an RNN-based global module, to learn global features, and a local module that learns slot-specific features."
2021.sigdial-1.25,P19-1078,0,0.0880776,"to track either single or multiple domains and to scale to new domains, both in terms of knowledge transfer and zero-shot learning. We cover a period from 2013 to 2020, showing a significant increase of multiple domain methods, most of them utilizing pre-trained language models. 1 Introduction Task-oriented dialogue systems enable users to accomplish tasks, such as ticket booking, restaurant reservation, and customer support, by interacting in natural language. The ability to accurately track the user’s requirements during the dialogue is crucial to enable a consistent and effective dialogue (Wu et al., 2019). Dialogue systems track such information using a dialogue state tracker (DST) component, where a dialogue state is represented with slot-value pairs, each denoting a specific user’s requirement. The accurate tracking of this information is crucial, as downstream components, like the dialog manager, rely on the dialogue state to choose the next action of the system. In recent years the performance of several natural language processing (NLP) tasks, including dialogue state tracking (Goldberg, 2017; Chen et al., 2017), has been pushed forward by neural networkbased approaches. The DST task actu"
2021.sigdial-1.25,P18-1134,0,0.144456,"networkbased approaches. The DST task actually merges some aspects of natural language understanding in dialogues, although it is more complex than the standard slot filling task. In fact, while slot filling involves predicting the slot-value pairs referred in a particular turn in dialogue (Louvan and Magnini, 2020), DST involves predicting the slot-value pairs at the dialogue level until the current turn. The complexity of DST has driven research to propose various neural approaches, including recurrent neural networks-based (Henderson et al., 2014c; Henderson et al., 2014; Wen et al., 2017; Xu and Hu, 2018; Ren et al., 2018), attention-based models (Wu et al., 2019; Xu and Hu, 2018; Nouri and Hosseini-Asl, 2018), and the very recent transformer-based models (Heck et al., 2020; Kim et al., 2020; Zhang et al., 2019; Lee et al., 2019; Rastogi et al., 2020; Balaraman and Magnini, 2021; Lin et al., 2020). In addition, the rapid progress of NLP has provided technologies to address several DST challenges, including predicting slot-values that are not present in training data, moving from rule-based to learning methods for dialogue state updating, and addressing long-term dependency, a crucial aspect i"
2021.sigdial-1.25,2020.nlp4convai-1.13,0,0.0199933,"user behavior. 3.3 3.6 MultiWoZ MultiWoZ is the first widely used multi-domain dialogue dataset for the DST task. It is collected using Wizard-of-Oz and consists of dialogues in 7 domains: restaurant, hotel, attraction, taxi, hospital, and police. 10,438 dialogues were released, out of which 3,406 are single-domain dialogues and 7,032 are multi-domain dialogues (Ramadan et al., 2018). Each of the multi-domain dialogues consists of at least 2 up to 5 domains. MultiWoZ has seen various versions, with several error corrections (Ramadan et al., 2018; Budzianowski et al., 2018; Eric et al., 2020; Zang et al., 2020). Machine-to-Machine Machine-to-Machine (M2M) dialogues are collected using a bootstrapping approach (Shah et al., 2018) based on dialogue simulators, and are then converted into natural language by crowd workers. The dataset consists of single domain dialogues for restaurant reservation and movie booking including, respectively, 2,240, 768, and 120K dialogues (Shah et al., 2018; Liu et al., 2018). 241 Among the datasets discussed in this study, DSTC2 and WoZ2.0 are the most used datasets for training single domain models, while MultiWoz is widely used for multi-domain models. 3.7 slot-value r"
2021.sigdial-1.25,P18-1135,0,0.194869,"). In addition, the rapid progress of NLP has provided technologies to address several DST challenges, including predicting slot-values that are not present in training data, moving from rule-based to learning methods for dialogue state updating, and addressing long-term dependency, a crucial aspect in dialogue. Furthermore, encouraged by the considerable success in modeling single domain dialogues (Henderson et al., 2014c; Wen et al., 2017; Mrkˇsi´c et al., 2017a), research on DST has recently moved toward building models that can handle multiple domains (Wu et al., 2019; Zhang et al., 2019; Zhong et al., 2018; Heck et al., 2020), and that are flexible enough to be adapted to new domains (Rastogi et al., 2020; Balaraman and Magnini, 2021; Lin et al., 2020; Gao et al., 2019). Although such rapid signs of progress have generated an impressive amount of research in DST, including several datasets and experimental material, to the best of our knowledge, such a massive amount of recent work has been only poorly documented (Williams et al., 2016a; Chen et al., 2017), and there is not an updated survey of the field. This paper intends to fill such a gap, providing 239 Proceedings of the 22nd Annual Meetin"
A92-1003,P90-1029,0,0.0312712,"Missing"
A92-1003,J87-1005,0,0.0801603,"Missing"
A92-1003,J89-1001,0,0.0407361,"Missing"
A92-1003,P89-1024,0,0.0335843,"Missing"
A92-1003,P88-1003,0,\N,Missing
atserias-etal-2004-cross,habash-dorr-2002-handling,0,\N,Missing
atserias-etal-2004-cross,briscoe-carroll-2002-robust,1,\N,Missing
atserias-etal-2004-cross,H92-1045,0,\N,Missing
atserias-etal-2004-cross,magnini-cavaglia-2000-integrating,1,\N,Missing
atserias-etal-2004-cross,agirre-etal-2004-exploring,1,\N,Missing
bentivogli-etal-2010-building,W07-1409,0,\N,Missing
bentivogli-etal-2010-building,W09-2508,0,\N,Missing
bentivogli-etal-2010-building,W09-2507,0,\N,Missing
bentivogli-etal-2010-building,W07-1401,1,\N,Missing
bentivogli-etal-2010-building,P08-1118,0,\N,Missing
bentivogli-etal-2010-building,P07-1058,1,\N,Missing
bongelli-etal-2012-corpus,rosenthal-etal-2010-towards,0,\N,Missing
bongelli-etal-2012-corpus,W08-0606,0,\N,Missing
bongelli-etal-2012-corpus,W09-1401,0,\N,Missing
bongelli-etal-2012-corpus,W10-3001,0,\N,Missing
C04-1163,W02-1304,1,\N,Missing
C04-1163,P00-1064,0,\N,Missing
C04-1163,A00-1031,0,\N,Missing
C10-2012,W07-1401,1,0.872285,"Missing"
C10-2012,bentivogli-etal-2010-building,1,0.868684,"posal on qualitative evaluation takes advantage of previous work on specialized entailment engines and monothematic datasets. A monothematic pair is defined (Magnini and Cabrio, 2009) as a T,H pair in which a certain phenomenon relevant to the entailment relation is highlighted and isolated. The main idea is to create the monothematic pairs basing on the phenomena that are actually present in the original RTE pairs, so that the actual distribution of the linguistic phenomena involved in the entailment relation emerges. For the decomposition procedure, we refer to the methodology described in (Bentivogli et al., 2010), consisting of a number of steps carried out manually. The starting point is a [T,H] pair taken from one of the RTE data sets, that should be decomposed in a number of monothematic pairs [T, Hi ], where T is the original Text and Hi are the Hypotheses created for each linguistic phenomenon relevant for judging the entailment relation in [T,H]. In details, the procedure for the creation of monothematic pairs is composed of the following steps: 1. Individuate the phenomena contributing to the entailment decision in [T,H]. 2. For each linguistic phenomenon i: (a) Detect a general entailment rule"
C10-2012,E06-1052,0,0.0130344,"ognizing Textual Entailment (RTE) has been proposed as a task whose aim is to capture major semantic inference needs across applications in Computational Linguistics (Dagan et al., 2009). Systems are asked to automatically judge whether the meaning of a portion of text, referred as Text (T), entails the meaning of another text, referred as Hypothesis (H). This evaluation provides useful cues for researchers and developers aiming at the integration of TE components in larger applications (see, for instance, the use of a TE engine in the QALL-ME project system1 , the use in relation extraction (Romano et al., 2006), and in reading comprehension systems (Nielsen et al., 2009)). Although the RTE evaluations showed progresses in TE technologies, we think that there is 1 3. Although the ability to detect and manage single phenomena seems to be a crucial feature of high performing systems, very little is known about how systems manage to combine such results in a global score for a pair. The mechanism underlying such composition may shed light on meaning composition related to TE tasks. 4. Finally, we are interested in the relation between the above mentioned items over the different kinds of pairs represent"
C10-2012,L08-1000,0,\N,Missing
C16-2028,C14-2026,0,0.0311821,"peline to be integrable with T EXT P RO -AL, it has to be able to produce an output in the IOB2 format5 and to assign a confidence score to each labeled sequence.6 2. Annotation editor. This is a tool for manually inserting and revising linguistic annotations on a corpus. Required basic functions are the possibility to define a set of categories to be used for a certain annotation task and the capability to annotate a sequence of tokens with a certain category. Several open source annotation tools are available (e.g. Callisto,7 WebAnno,8 Brat,9 and CAT10 ); among these, we selected MTEqual11 (Girardi et al., 2014) (a tool developed for assessing the quality of machine translations) to integrate it in the current demonstrator, as it offers good editing features for online revisions. The use of MTEqual allows us to experiment the T EXT P RO -AL approach virtually on any sequence labeling annotation task. 3. AL package. This is a package for Active Learning which optimizes the selection of samples (from a large pool of unlabeled data) to be given for revision to the annotator. Only a small number of packages for AL are available (e.g. JCLAL12 ) and we preferred our own implementation, which is specificall"
C16-2028,pianta-etal-2008-textpro,0,0.037317,"for each task the ML classifier already implements corresponding feature extractors (e.g. orthographic features for named entity recognition). We also assume that there are no hard coded linguistic categories for the NLP tasks (e.g. Person for NER), so that the pipeline builds a model for a labeling task by taking the categories directly from the training data. There are several linguistic pipelines of this type available, including CoreNLP1 developed at 1 http://stanfordnlp.github.io/CoreNLP/ 132 Stanford University, the OpenNLP pipeline2 and LingPipe3 . For our demonstrator we use TextPro4 (Pianta et al., 2008), a pipeline for English and Italian including several annotation layers, such as part-of-speech tagging, lemmatization, named entities recognition, dependency parsing and event extraction. In order for a pipeline to be integrable with T EXT P RO -AL, it has to be able to produce an output in the IOB2 format5 and to assign a confidence score to each labeled sequence.6 2. Annotation editor. This is a tool for manually inserting and revising linguistic annotations on a corpus. Required basic functions are the possibility to define a set of categories to be used for a certain annotation task and"
C16-2028,W07-1516,0,0.22685,"fidence score to each annotated instance (step 4). In step 2b the manually supervised instance is stored in the Global Memory of the system (together with the revisions performed by the annotator). In step 5 a single instance is selected from the unlabeled dataset. The selected instance, as well as its relevant context, is removed from the unlabeled set and is presented to the human annotator to be revised. Active Learning has been successfully experimented for a large variety of sequence labeling annotation tasks (an incomplete list includes (Shen et al., 2004) for Named Entity Recognition, (Ringger et al., 2007) for Part-of-Speech Tagging, and (Schohn and Cohn, 2000) for Text Classification), which guarantees the high portability of the approach we propose. 3 System Description T EXT P RO -AL integrates four components in a single platform: (1) a ML-based NLP pipeline; (2) a webbased annotation editor for manually revising linguistic annotations; (3) an AL package which selects samples to be annotated from a large pool of non annotated documents and then re-trains the pipeline; (4) a visualizer of the internal learning status of the pipeline for the task at hand. 1. NLP pipeline. This is a set of too"
C16-2028,P04-1075,0,0.329965,"nlabeled documents and to assigns an estimated confidence score to each annotated instance (step 4). In step 2b the manually supervised instance is stored in the Global Memory of the system (together with the revisions performed by the annotator). In step 5 a single instance is selected from the unlabeled dataset. The selected instance, as well as its relevant context, is removed from the unlabeled set and is presented to the human annotator to be revised. Active Learning has been successfully experimented for a large variety of sequence labeling annotation tasks (an incomplete list includes (Shen et al., 2004) for Named Entity Recognition, (Ringger et al., 2007) for Part-of-Speech Tagging, and (Schohn and Cohn, 2000) for Text Classification), which guarantees the high portability of the approach we propose. 3 System Description T EXT P RO -AL integrates four components in a single platform: (1) a ML-based NLP pipeline; (2) a webbased annotation editor for manually revising linguistic annotations; (3) an AL package which selects samples to be annotated from a large pool of non annotated documents and then re-trains the pipeline; (4) a visualizer of the internal learning status of the pipeline for th"
C16-2028,W09-1906,0,0.0345613,"nference on Computational Linguistics: System Demonstrations, pages 131–135, Osaka, Japan, December 11-17 2016. Figure 1: T EXT P RO -AL architecture. These criteria are typically applied in an iterative way, following a re-training procedure, where instances are selected from a (usually large) pool of unlabeled texts. Although there are experimental evidences that AL allows for a significant reduction of the amount of training needed to achieve a certain performance (e.g. calculated in terms of F-measure), there is less experience and less consensus about the use of AL in practical contexts (Tomanek and Olsson, 2009). In our implementation, the AL cycle (see Figure 1) starts with a human annotator providing supervision on a sample that has been tagged automatically by the system (step 1): the annotator is asked to either confirm the annotation (in case it is correct) or to revise it. The annotated instance is stored in a batch (step 2a), where it is accumulated with other instances for re-training and, as a result, a new model is produced (step 3). This model is used to automatically annotate a set of unlabeled documents and to assigns an estimated confidence score to each annotated instance (step 4). In"
cabrio-etal-2008-qall,W03-2120,0,\N,Missing
cabrio-etal-2008-qall,W99-0310,0,\N,Missing
cattoni-etal-2012-knowledgestore,magnini-etal-2006-cab,0,\N,Missing
cattoni-etal-2012-knowledgestore,pianta-etal-2008-textpro,1,\N,Missing
E06-1003,W04-3221,0,0.0854834,"ecently emerged as a new field of application for knowledge acquisition techniques (see, among others, (Buitelaar et al., 2005)). Although there is no a univocally accepted definition for the OP task, a useful approximation has been suggested (Bontcheva and Cunningham, 2003) as Ontology Driven Information Extraction, where, in place of a template to be filled, the goal of the task is the extraction and classification of instances of concepts and relations defined in a Ontology. The task has been approached in a variety of similar perspectives, including term clustering (e.g. (Lin, 1998a) and (Almuhareb and Poesio, 2004)) and term categorization (e.g. (Avancini et al., 2003)). A rather different task is Ontology Learning (OL), where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (see, for instance, (Velardi et al., 2005)). In this paper OP is defined in the following scenario. Given a set of terms T = t1 , t2 , ..., tn , a document collection D, where terms in T are supposed to appear, and a set of predefined classes C = c1 , c2 , ..., cm denoting concepts in an Ontology, each term ti has to be assigned to the proper class in C. F"
E06-1003,P98-2127,0,0.83705,"from texts has recently emerged as a new field of application for knowledge acquisition techniques (see, among others, (Buitelaar et al., 2005)). Although there is no a univocally accepted definition for the OP task, a useful approximation has been suggested (Bontcheva and Cunningham, 2003) as Ontology Driven Information Extraction, where, in place of a template to be filled, the goal of the task is the extraction and classification of instances of concepts and relations defined in a Ontology. The task has been approached in a variety of similar perspectives, including term clustering (e.g. (Lin, 1998a) and (Almuhareb and Poesio, 2004)) and term categorization (e.g. (Avancini et al., 2003)). A rather different task is Ontology Learning (OL), where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (see, for instance, (Velardi et al., 2005)). In this paper OP is defined in the following scenario. Given a set of terms T = t1 , t2 , ..., tn , a document collection D, where terms in T are supposed to appear, and a set of predefined classes C = c1 , c2 , ..., cm denoting concepts in an Ontology, each term ti has to be a"
E06-1003,W04-3206,1,0.168489,"Missing"
E06-1003,C02-1130,0,0.460085,"ation between two words, e.g. “the ant is an insect” or “ants and other insects”. However, such phrases do not appear frequently in a text corpus. For this reason, some approaches use the Web (Schlobach et al., 2004). (Velardi et al., 2005) experimented several head-matching heuristics according to which if a term1 is in the head of term2 , then there is an “is-a” relation between them: For example “Christmas tree” is a kind of “tree”. Context feature approaches use a corpus to extract features from the context in which a semantic class tends to appear. Contextual features may be superficial (Fleischman and Hovy, 2002) or syntactic (Lin, 1998a), (Almuhareb and Poesio, 2004). Comparative evaluation in (Cimiano and V¨olker, 2005) shows that syntactic features lead to better performance. Feature weights can be calculated either by Machine Learning algorithms (Fleischman and Hovy, 2002) or by statistical measures, like Point Wise Mutual Information or the Jaccard coefficient (Lin, 1998a). A hybrid approach using both pattern-based, term structure, and contextual feature methods is presented in (Cimiano et al., 2005). State-of-the-art approaches may be divided in two classes, according to different use of traini"
E06-1003,C98-2122,0,\N,Missing
forner-etal-2010-evaluating,lamel-etal-2008-question,0,\N,Missing
jezek-etal-2014-pas,D07-1002,0,\N,Missing
jezek-etal-2014-pas,E06-2001,0,\N,Missing
jezek-etal-2014-pas,J02-3001,0,\N,Missing
jezek-etal-2014-pas,lenci-etal-2012-lexit,0,\N,Missing
jezek-etal-2014-pas,jezek-quochi-2010-capturing,1,\N,Missing
jezek-etal-2014-pas,E12-1085,0,\N,Missing
kouylekov-magnini-2006-building,W99-0501,0,\N,Missing
kouylekov-magnini-2006-building,W04-3206,0,\N,Missing
L16-1339,W14-2907,0,0.026183,"Missing"
L16-1339,E06-2001,0,0.0103573,"verb substitution process and finally will provide description of the crowsourcing plaftorm setting. 5.1. The T-PAS Resource We extracted S1 from the annotated corpus of T-PAS resource3 for the source verb. The T-PAS resource is an inventory of Typed Predicate Argument Structures for Italian manually acquired from corpora following the Corpus Pattern Analysis (CPA) methodology (Hanks, 2004). T-PASs are semantically motivated and are identified through inspection and annotation of actual uses of the analyzed verbs in a corpus of sentences extracted from a a reduced version of the ItWAC corpus (Baroni and Kilgarriff, 2006). An example of T-PAS for the Italian verb divorare (Engl. to devour) is given in (3): (3) T-PAS#2 of the verb divorare (Eng. to devour): [[Human]] divorare [[Document]] Each T-PAS corresponds to a distinct verb sense, for example in (3) the sense is “read eagerly”. After analyzing a sample of 250 concordances of the verb in the corpus, the lexicographer defines each T-PAS recognizing its relevant structure and identifying the Semantic Types (STs) for each argument slots by generalizing over the lexical sets observed in the concordances. For instance, in (3) [[Document]] generalizes over libro"
L16-1339,W04-3205,0,0.0819183,"f values for a given property, of which they may specify the two poles (or bounds), e.g. to like / to dislike (a person). Converses (or relational opposites) describe the same action from a different perspective: e.g. to give / to receive (a present). Finally, reversives denote a change (literal movement or abstract change) in opposite direction between two states: one term indicates a change from a state to another and viceversa: e.g. to build / to destroy a building or to wrap / to unwrap an object. This category includes the group of restitutives (Cruse, 1986), cases in which, according to Chklovski and Pantel (2004), the opposition relation systematically interacts with the happens-before relation, as for the pairs to 2138 damage / to repair. Also Fellbaum (1998) has noted that the relation between the verbs in these pairs seems one of entailment (Fellbaum, 1998, p.75); for example one can only unwrap something which has been previously wrapped. To distinguish different types of opposition is relevant for several reasons. Consider for example the following sentences, where according to our definition to remain - to leave are complementaries and to increase - to decrease are antonyms: (1) (a) John did not"
L16-1339,E14-4044,0,0.051078,"Missing"
L16-1339,W13-1202,0,0.0445099,"Missing"
L16-1339,P13-2130,0,0.0288829,"adictory event pairs and create a large-scale database of Japanese contradictory event pairs by asking the crowd to write and evaluate in-domain contradictory sentences. In their study, the authors classify contradictory event pairs in a taxonomy which includes also “binary event pairs”, i.e. events that contradict each other for presenting e.g.“mutually exclusive antonyms”, such as single and married. Other related works include experiments that use crowdsourcing not to directly investigate opposition relations but that are connected to other aspects of our work. For example, in the study of Fossati et al. (2013), authors apply crowdsourcing to perform FrameNet annotation. They compare two approaches: in the first they ask the crowd to select the frame evoked by a given predicate in a sentence and then to identify the frame elements typically involved in the identified frame (2-steps approach); in the second approach they start from the frame elements annotation to identify the frame expressed in a sentence (1-step approach). Among other results, they notice that crowdsourcing can produce usable results for the annotation of frame element. Feizabadi and Pad´o (2014) also report on a study on the annot"
L16-1339,E14-1057,0,0.0477158,"Missing"
L16-1339,J13-3004,0,0.0751831,"Missing"
L16-1339,sabou-etal-2014-corpus,0,0.0315855,"hether opposition relations are exclusive, that is, whether it is possible for a pair of verb senses to have characteristics that belong to more than one type of opposition. We expect our experiment to provide us with valuable insights in this regard. 3. Crowdsourcing: Related Works As mentioned in Section 1, for the acquisition of opposition relations, we collected data using crowdsourcing, a methodology already used to acquire information on large scale which can be used to enrich lexical resources. In order to design properly the task we have tried to follow the best practices suggested in Sabou et al. (2014)1 , in particular for what concerns, e.g.: • project definition: main task need to be decomposed in simple tasks, suitable task setting has to be selected, rewarding need to be determined, tasks should be simple and intuitive; • data preparation: the interface and the instruction should be clear, task need to be design in order to prevent and reduce cheating (crowdsourcing platforms offer some functionality that can help in this sense); • project execution: it is important to attract and retain contributors and to filter cheating workers in order to improve quality (it is possible e.g. to embe"
L16-1339,W15-0813,0,0.0314485,"h each other and/or are saliently different across a dimension of meaning) and on classifying the pairs into one of the different type of oppositions the authors distinguish (i.e. antipodals, complementaries, disjoint, gradable opposites, reversibles). Specifically, observing the collected data the authors find that annotators agree markedly on identifying contrasting word pairs while there is a variation in the agreement in the questions that aim at identifying the different kinds of opposites. They also verify that contrasting pairs can be classified into more than one kinds. More recently, Takabatake et al. (2015) use crowdsourcing in order to collect contradictory event pairs and create a large-scale database of Japanese contradictory event pairs by asking the crowd to write and evaluate in-domain contradictory sentences. In their study, the authors classify contradictory event pairs in a taxonomy which includes also “binary event pairs”, i.e. events that contradict each other for presenting e.g.“mutually exclusive antonyms”, such as single and married. Other related works include experiments that use crowdsourcing not to directly investigate opposition relations but that are connected to other aspect"
L18-1085,S16-1165,0,0.113779,"oit not only the surface forms of temporal expressions, but also their meaning. For instance, applications in event / timeline extraction (Minard et al., 2015; Cornegruta and Vlachos, 2016; Spitz and Gertz, 2016), question answering (Llorens et al., 2015) and (temporal) information retrieval (Kanhabua et al., 2015) can exploit temporal tagging output. Thus, temporal tagging has become a vibrant research area, and several new temporal taggers have been made available and new strategies have been developed. However, as was shown in previous work (Mazur and Dale, 2010; Str¨otgen and Gertz, 2013; Bethard et al., 2016; Tabassum et al., 2016), different types of documents pose different challenges for temporal tagging such that domain-sensitive normalization strategies are required (Str¨otgen and Gertz, 2016). To judge the performance of temporal taggers and new methods, evaluations need to be performed on diverse text types, e.g., on news articles and narrative-style Wikipedia documents. In contrast to many natural language processing tasks, there has also been some effort towards multilinguality in the context of temporal tagging, e.g., research competitions were organized not only for English but covered"
L18-1085,D16-1200,0,0.0241421,"ublicly available to further boost research in temporal tagging. Keywords: temporal tagging, corpus annotation, TIMEX3 1. Introduction Temporal tagging – the extraction and normalization of temporal expressions from texts – is an important task towards improved natural language understanding. Once temporal expressions have been detected in a text, their semantics can be assigned to them in a standard format so that applications can exploit not only the surface forms of temporal expressions, but also their meaning. For instance, applications in event / timeline extraction (Minard et al., 2015; Cornegruta and Vlachos, 2016; Spitz and Gertz, 2016), question answering (Llorens et al., 2015) and (temporal) information retrieval (Kanhabua et al., 2015) can exploit temporal tagging output. Thus, temporal tagging has become a vibrant research area, and several new temporal taggers have been made available and new strategies have been developed. However, as was shown in previous work (Mazur and Dale, 2010; Str¨otgen and Gertz, 2013; Bethard et al., 2016; Tabassum et al., 2016), different types of documents pose different challenges for temporal tagging such that domain-sensitive normalization strategies are required ("
L18-1085,S10-1063,0,0.0325517,"atives in German, AncientTimes (Str¨otgen et al., 2014) and WikiWarsDE (Str¨otgen and Gertz, 2011), have been manually annotated but following the English TimeML guidelines without further specifying language1 536 http://github.com/JannikStroetgen/KRAUTS specific adaptations. WikiWarsDE is the German counterpart of the English WikiWars corpus (Mazur and Dale, 2010) and AncientTimes is a small multilingual corpus containing documents about history. Driven by the above-mentioned shared tasks, many temporal taggers have been developed. Some of these can process several languages, such as TIPSem (Llorens et al., 2010) for English and Spanish, TimePro (Mirza and Minard, 2014) (a module of TextPro2 ) for English, Italian and French, and HeidelTime (Str¨otgen and Gertz, 2013) for 13 languages, including German, as well as its automatic extension as a baseline temporal tagger for more than 200 languages (Str¨otgen and Gertz, 2015). Str¨otgen et al. (2014) performed an evaluation of HeidelTime on two German corpora of narratives: WikiWarsDE and AncientTimes. They reported F1-scores of 87.7 and 78.0 for strict match, and value F1-scores3 of 80.4 and 82.2 on WikiWarsDE and AncientTimes, respectively. Our work con"
L18-1085,S15-2134,0,0.0642709,"temporal tagging, corpus annotation, TIMEX3 1. Introduction Temporal tagging – the extraction and normalization of temporal expressions from texts – is an important task towards improved natural language understanding. Once temporal expressions have been detected in a text, their semantics can be assigned to them in a standard format so that applications can exploit not only the surface forms of temporal expressions, but also their meaning. For instance, applications in event / timeline extraction (Minard et al., 2015; Cornegruta and Vlachos, 2016; Spitz and Gertz, 2016), question answering (Llorens et al., 2015) and (temporal) information retrieval (Kanhabua et al., 2015) can exploit temporal tagging output. Thus, temporal tagging has become a vibrant research area, and several new temporal taggers have been made available and new strategies have been developed. However, as was shown in previous work (Mazur and Dale, 2010; Str¨otgen and Gertz, 2013; Bethard et al., 2016; Tabassum et al., 2016), different types of documents pose different challenges for temporal tagging such that domain-sensitive normalization strategies are required (Str¨otgen and Gertz, 2016). To judge the performance of temporal ta"
L18-1085,D10-1089,0,0.0721884,"n a standard format so that applications can exploit not only the surface forms of temporal expressions, but also their meaning. For instance, applications in event / timeline extraction (Minard et al., 2015; Cornegruta and Vlachos, 2016; Spitz and Gertz, 2016), question answering (Llorens et al., 2015) and (temporal) information retrieval (Kanhabua et al., 2015) can exploit temporal tagging output. Thus, temporal tagging has become a vibrant research area, and several new temporal taggers have been made available and new strategies have been developed. However, as was shown in previous work (Mazur and Dale, 2010; Str¨otgen and Gertz, 2013; Bethard et al., 2016; Tabassum et al., 2016), different types of documents pose different challenges for temporal tagging such that domain-sensitive normalization strategies are required (Str¨otgen and Gertz, 2016). To judge the performance of temporal taggers and new methods, evaluations need to be performed on diverse text types, e.g., on news articles and narrative-style Wikipedia documents. In contrast to many natural language processing tasks, there has also been some effort towards multilinguality in the context of temporal tagging, e.g., research competition"
L18-1085,S15-2132,1,0.899791,"Missing"
L18-1085,L16-1699,1,0.88122,"Missing"
L18-1085,D15-1063,1,0.936733,"Missing"
L18-1085,strotgen-etal-2014-extending,1,0.879933,"Missing"
L18-1085,D16-1030,0,0.0182735,"ce forms of temporal expressions, but also their meaning. For instance, applications in event / timeline extraction (Minard et al., 2015; Cornegruta and Vlachos, 2016; Spitz and Gertz, 2016), question answering (Llorens et al., 2015) and (temporal) information retrieval (Kanhabua et al., 2015) can exploit temporal tagging output. Thus, temporal tagging has become a vibrant research area, and several new temporal taggers have been made available and new strategies have been developed. However, as was shown in previous work (Mazur and Dale, 2010; Str¨otgen and Gertz, 2013; Bethard et al., 2016; Tabassum et al., 2016), different types of documents pose different challenges for temporal tagging such that domain-sensitive normalization strategies are required (Str¨otgen and Gertz, 2016). To judge the performance of temporal taggers and new methods, evaluations need to be performed on diverse text types, e.g., on news articles and narrative-style Wikipedia documents. In contrast to many natural language processing tasks, there has also been some effort towards multilinguality in the context of temporal tagging, e.g., research competitions were organized not only for English but covered further languages such"
L18-1085,S13-2001,0,0.0355106,"temporal expressions in the corpus, we developed annotation guidelines for German temporal tagging by using the guidelines defined for Italian (Caselli and Sprugnoli, 2015) as a starting point. Overall, the corpus contains 192 documents with 1,140 annotated temporal expressions, and the corpus as well as the annotation guidelines have been made publicly available to further boost research in temporal tagging.1 2. Related Work The task of temporal processing has gained interest in recent years, in particular thanks to the TempEval tasks at SemEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Llorens et al., 2015; Bethard et al., 2016). Temporal tagging is a subtask of temporal processing and consists of the identification of temporal expressions in texts and their normalization to some standard format. Str¨otgen and Gertz (2016) present a complete overview of the task as well as a survey of the resources, tools, etc. They focus on the description of domainsensitive temporal tagging and multilingual taggers. The annotation of temporal expressions follows in most cases the TimeML annotation guidelines (Pustejovsky et al., 2003) developed first for English. They have then been adap"
L18-1085,S07-1014,0,0.0486161,"e weekly newspaper D IE Z EIT. For annotating temporal expressions in the corpus, we developed annotation guidelines for German temporal tagging by using the guidelines defined for Italian (Caselli and Sprugnoli, 2015) as a starting point. Overall, the corpus contains 192 documents with 1,140 annotated temporal expressions, and the corpus as well as the annotation guidelines have been made publicly available to further boost research in temporal tagging.1 2. Related Work The task of temporal processing has gained interest in recent years, in particular thanks to the TempEval tasks at SemEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Llorens et al., 2015; Bethard et al., 2016). Temporal tagging is a subtask of temporal processing and consists of the identification of temporal expressions in texts and their normalization to some standard format. Str¨otgen and Gertz (2016) present a complete overview of the task as well as a survey of the resources, tools, etc. They focus on the description of domainsensitive temporal tagging and multilingual taggers. The annotation of temporal expressions follows in most cases the TimeML annotation guidelines (Pustejovsky et al., 2003) develope"
L18-1684,L16-1160,0,0.0155872,"not (e.g. ciononostante); • the syntactic category: adverbs, prepositions, subordinating or coordinating conjunctions; • the semantic relation(s) which the connective indicates, according to the PDTB 3.0 schema of relations (Webber et al., 2016); 2 http://www.treccani.it/enciclopedia/connettivi (Enciclopediadell’Italiano)/ 4328 • possible alignments with lexica of connectives in German; • examples of usage of the connective for each semantic relations it indicates. The examples in the first version of the resource are translation of the German examples already present in the DimLex resource (Scheffler and Stede, 2016; Stede, 2002; Stede and Umbach, 1998). In adopting a corpus-based approach we aim at enriching LICO with data-driven examples and validating the information in the resource. Related lexica. LICO has been inspired by the DimLex project for German (Scheffler and Stede, 2016; Stede, 2002; Stede and Umbach, 1998)3 , an XML-encoded resource that provides information on orthographic variants, syntactic category, semantic relations in terms of PDTB3.0 (Webber et al., 2016) sense tags, and usage examples for 274 connectives. DimLex is used for automatic discourse parsing, and also for semiautomatic t"
L18-1684,C04-1061,0,0.0614395,". In adopting a corpus-based approach we aim at enriching LICO with data-driven examples and validating the information in the resource. Related lexica. LICO has been inspired by the DimLex project for German (Scheffler and Stede, 2016; Stede, 2002; Stede and Umbach, 1998)3 , an XML-encoded resource that provides information on orthographic variants, syntactic category, semantic relations in terms of PDTB3.0 (Webber et al., 2016) sense tags, and usage examples for 274 connectives. DimLex is used for automatic discourse parsing, and also for semiautomatic text annotation using the ConAno tool (Stede and Heintze, 2004). A similar repository for French is LEXCONN (Roze et al., 2012)4 , which contains more than 300 connectives with their syntactic categories and discourse relations from Segmented Discourse Representation Theory (Asher and Lascarides, 2003). The lexicon has been constructed manually, using a corpus as empirical support. LICO is freely distributed under a CC-BY licence5 and can be browsed with DIMLEX and LEXCONN at http:// connective-lex.info/. 4. Enriching Connectives of Contrast in LICO We aim at enriching the connectives of contrast in LICO with examples from corpora. Collecting these exampl"
L18-1684,P98-2202,0,0.582145,"and documents from Wikipedia. The resulting resource represents a valuable tool for both linguistic analyses of discourse relations and the training of a classifier for NLP applications. Keywords: discourse connectives, contrast relation, corpus examples 1. Introduction per ends with concluding observations and hints for further work. Discourse relations and the linguistic elements marking them in text, commonly referred to as discourse connectives, have recently been at the core of several annotation efforts for multiple languages (including English, German, French, Italian, Portuguese, see Stede and Umbach (1998), Roze et al. (2012) among others). In this paper, we present the results of the effort of enriching the preexisting resource LICO, a lexicon of Italian connectives retrieved from lexicographic sources (Feltracco et al., 2016), with real corpus data, thus allowing us to both extend the resource and validate the lexicon. Our goal in this contribution is limited to the class of connectives marking contrast, and the additional relations such connectives might convey, some of them being polysemous. The motivation beyond our effort is that connectives can only be interpreted and disambiguated when"
L18-1684,W16-1704,0,0.0620912,"essions; iii) adverbs or adverbial expressions; iv) prepositions or prepositional expressions. In line with this definition, Stede (2012) distinguishes connectives as never inflected, closed-class lexical items, which belong to the above mentioned syntactic categories. He also specifies that these lexical elements can only be interpreted successfully when they appear in a relation between two discourse segments. Ferrari (2010) also proposes a non hierarchical classification for connectives depending on the “type of logical relation they convey”, e.g. temporal and causal. The PDTB 3.0 project (Webber et al., 2016) proposes a hierarchical classification composed by three levels (Table 1). GENCY.Cause represents an asymmetric relation between two arguments: being one the cause, and the other the result. The subtype CONTINGENCY.Cause.Reason is used if the argument introduced by the connective (Arg2) is the reason for the situation in the other argument (Arg1) (e.g. “I stayed at home because it was raining”), while CONTINGENCY.Cause.Result is used if it represents the result/effect (e.g. “It was raining, therefore I stayed at home”). Notice that not every type has a further subtype: for example, the argume"
magnini-cavaglia-2000-integrating,H93-1052,0,\N,Missing
magnini-cavaglia-2000-integrating,W93-0303,0,\N,Missing
magnini-cavaglia-2000-integrating,C90-2067,0,\N,Missing
magnini-cavaglia-2000-integrating,P91-1019,0,\N,Missing
magnini-cavaglia-2000-integrating,J98-1004,0,\N,Missing
magnini-etal-2002-towards,W01-1204,0,\N,Missing
magnini-etal-2008-evaluation,W04-0812,1,\N,Missing
magnini-etal-2008-evaluation,bosco-etal-2008-comparing,1,\N,Missing
magnini-etal-2008-evaluation,W96-0102,0,\N,Missing
magnini-etal-2008-evaluation,W96-0213,0,\N,Missing
magnini-etal-2008-evaluation,bosco-etal-2000-building,1,\N,Missing
magnini-etal-2008-evaluation,W01-0521,0,\N,Missing
magnini-etal-2008-evaluation,A00-1031,0,\N,Missing
magnini-etal-2008-evaluation,W03-0419,0,\N,Missing
magnini-etal-2008-evaluation,P99-1065,0,\N,Missing
magnini-etal-2008-evaluation,D07-1096,0,\N,Missing
P02-1054,breck-etal-2000-evaluate,0,0.030976,"Missing"
P02-1054,J93-1003,0,0.0542797,"Missing"
P02-1054,W01-1204,0,0.0395133,"Missing"
P02-1054,W01-1205,0,0.0377541,"Missing"
P14-5008,W07-1401,1,0.865172,"esources for these languages (assuming that the EDAs themselves are largely language-independent). These are provided by the language-independent knowledge acquisition tools which we offer alongside the platform (cf. Section 3.2). EOP Evaluation Results for the three EDAs included in the EOP platform are reported in Table 1. Each line represents an EDA, the language and the dataset on which the EDA was evaluated. For brevity, we omit here the knowledge resources used for each EDA, even though knowledge configuration clearly affects performance. The evaluations were performed on RTE-3 dataset (Giampiccolo et al., 2007), where the goal is to maximize accuracy. We (manually) translated it to German and Italian for evaluations: in both cases the results fix a reference for the two languages. The two new datasets for German and English are available both as part of the EOP distribution and independently5 . The transformation-based EDA was also evaluated on RTE-6 dataset (Bentivogli et al., 2010), in which the goal is to maximize the F1 measure. The results of the included EDAs are higher than median values of participated systems in RTE-3, and they are competing with state-of-the-arts in RTE-6 results. To the b"
P14-5008,P98-2127,0,0.00988239,"otivates our offer of the EOP platform as a library. They also require a system that provides good quality at a reasonable efficiency as well as guidance as to the best choice of parameters. The latter point is realized through our results archive in the official EOP Wiki on the EOP site. Table 1: EDAs results for specific domains. Particularly, the EOP platform includes a language independent tool to build Wikipedia resources (Shnarch et al., 2009), as well as a language-independent framework for building distributional similarity resources like DIRT (Lin and Pantel, 2002) and Lin similarity(Lin, 1998). 3.3 Use Case 2: Textual Entailment Development. This category covers researchers who are interested in Recognizing Textual Entailment itself, for example with the goal of developing novel algorithms for detecting entailment. In contrast to the first category, this group need to look ”under the hood” of the EOP platform and access the source code of the EOP. For this reason, we have spent substantial effort to provide the code in a well-structured and well-documented form. A subclass of this group is formed by researchers who want to set up a RTE infrastructure for languages in which it does"
P14-5008,P09-1051,1,0.820811,"art of or all of the semantic processing, such as Question Answering or Intelligent Tutoring. Such users require a system that is as easy to deploy as possible, which motivates our offer of the EOP platform as a library. They also require a system that provides good quality at a reasonable efficiency as well as guidance as to the best choice of parameters. The latter point is realized through our results archive in the official EOP Wiki on the EOP site. Table 1: EDAs results for specific domains. Particularly, the EOP platform includes a language independent tool to build Wikipedia resources (Shnarch et al., 2009), as well as a language-independent framework for building distributional similarity resources like DIRT (Lin and Pantel, 2002) and Lin similarity(Lin, 1998). 3.3 Use Case 2: Textual Entailment Development. This category covers researchers who are interested in Recognizing Textual Entailment itself, for example with the goal of developing novel algorithms for detecting entailment. In contrast to the first category, this group need to look ”under the hood” of the EOP platform and access the source code of the EOP. For this reason, we have spent substantial effort to provide the code in a well-s"
P14-5008,P12-1030,1,0.845888,"raphrasing patterns at the predicate-argument level that cannot be captured by purely lexical rules. Formally, each syntactic rule consists of two dependency tree fragments plus a mapping from the variables of the LHS tree to the variables of the RHS tree.4 2.3 In the EOP we include a transformation based inference system that adopts the knowledge based transformations of Bar-Haim et al. (2007), while incorporating a probabilistic model to estimate transformation confidences. In addition, it includes a search algorithm which finds an optimal sequence of transformations for any given T/H pair (Stern et al., 2012). Edit distance EDA involves using algorithms casting textual entailment as the problem of mapping the whole content of T into the content of H. Mappings are performed as sequences of editing operations (i.e., insertion, deletion and substitution) on text portions needed to transform T into H, where each edit operation has a cost associated with it. The underlying intuition is that the probability of an entailment relation between T and H is related to the distance between them; see Kouylekov and Magnini (2005) for a comprehensive experimental study. Configuration Files The EC components can b"
P14-5008,P07-2045,0,\N,Missing
P14-5008,C98-2122,0,\N,Missing
P19-3013,W13-4065,0,0.0160924,"ialogue scenarios can be fitted into. However, the reduction of the dialogue flow complexity brings some restrictions to the flexibility of the dialogue management. We provide a list of restrictions and their alternatives in FASTDial dialogue flow as follows: • system initiative dialogue flow: once the user intent is identified, the system tries to fill all necessary slots and in case of an unexpected user utterance, it changes the state to either repeating the question or finalizing the dialogue. This would prevent the system answering the mid-dialogue user questions, i.e. requestable slots (Henderson et al., 2013). As an example, in the Money Transfer intent, the user may ask about her account balance before specifying the amount of money she wants to send. The DM cannot change the intent to Account Balance query in this scenario. 2.3 The task of the Natural Language Understanding component is twofold: first to recognize the user intent and then fill the corresponding FSDS that has been loaded by the Intent Module and active at the moment. While understanding the intent type is implemented as a model interface, the slot recognition task is handled by each slot type separately. The intent identification"
P19-3013,P16-4012,0,0.0280452,"umans in a number of scenarios, including - but not limited to - virtual coaches, personal assistants and automatic help desks. However, when dealing with applications or commercial scenarios, technological complexity should be abstracted away since domain knowledge is often held by non tech savvy. Moreover, systems should be ‘transparent’ to easily allow for modification or scaling when needed, such as error fixing or new intents/objects/requirements integration. To this end, several solutions have appeared on the market. On one side, there are open source tools/frameworks, such as OpenDial (Lison and Kennington, 2016), PyDial (Ultes et al., 2017) and DeepPavlov (Burtsev et al., 2018) that are very flexible and allow many integrations. While these tools are designed with the target of computer scientists in mind, they would still need domain expertise to design proper dialogues. On the other side of the spectrum, several commercial tools 75 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 75–80 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ity can be quickly obtained also by non-experts since t"
P19-3013,W18-5711,1,0.855909,"Missing"
P19-3013,P17-4013,0,0.0181288,"ding - but not limited to - virtual coaches, personal assistants and automatic help desks. However, when dealing with applications or commercial scenarios, technological complexity should be abstracted away since domain knowledge is often held by non tech savvy. Moreover, systems should be ‘transparent’ to easily allow for modification or scaling when needed, such as error fixing or new intents/objects/requirements integration. To this end, several solutions have appeared on the market. On one side, there are open source tools/frameworks, such as OpenDial (Lison and Kennington, 2016), PyDial (Ultes et al., 2017) and DeepPavlov (Burtsev et al., 2018) that are very flexible and allow many integrations. While these tools are designed with the target of computer scientists in mind, they would still need domain expertise to design proper dialogues. On the other side of the spectrum, several commercial tools 75 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 75–80 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ity can be quickly obtained also by non-experts since the technical implementation o"
piperidis-etal-2014-meta,wittenburg-etal-2010-resource,1,\N,Missing
piperidis-etal-2014-meta,choukri-etal-2012-using,1,\N,Missing
piperidis-etal-2014-meta,piperidis-2012-meta,1,\N,Missing
piperidis-etal-2014-meta,gavrilidou-etal-2012-meta,1,\N,Missing
piperidis-etal-2014-meta,broeder-etal-2010-data,0,\N,Missing
piperidis-etal-2014-meta,soria-etal-2012-flarenet,1,\N,Missing
piperidis-etal-2014-meta,federmann-etal-2012-meta,1,\N,Missing
R15-1051,P14-5008,1,0.881173,"Missing"
R15-1051,S12-1051,0,0.0431745,"farm, it’s a Maine Coon, it’s the largest domesticated breed of cat. Text2: Felix is the largest domesticated animal in John’s farm. Introduction In the last decade text-to-text semantic inference has been a relevant topic in Computational Linguistics. Driven by the assumption that language understanding crucially depends on the ability to recognize semantic relations among portions of text, several text-to-text inference tasks have been proposed, including recognizing paraphrasing (Dolan and Brockett., 2005), recognizing textual entailment (RTE) (Dagan et al., 2005), and semantic similarity (Agirre et al., 2012). A common characteristic of such tasks is that the input are two portions of text, let’s call them T ext1 and T ext2, and the output is a semantic relation between the two texts, possibly with a degree of confidence of the system. For instance, given the following text fragments: shows a case of ”non-entailment”. It is worth to notice that in both the examples, although the entailment judgment is different, still there is an high degree of lexical alignments between words in T ext1 and T ext2 (e.g. M ax −→ M ax, pig −→ animal, cat −→ animal). In the paper we systematically investigate the rel"
R15-1051,marelli-etal-2014-sick,0,0.0241724,"ng application derived text fragments, and it is balanced between positive and negative pairs (about 1600 in total). (1) RTE-3 ita. The Italian RTE-3 data-set1 is the translation of the English one. The goal is to monitor the behaviour of the RID while changing the language. RTE-5 eng. The RTE-5 data-set (Bentivogli et al., 2009) is similar to RTE-3, although T 1 pairs are usually much longer, which, in our terms, means that a higher number of alignments can be potentially generated by the same number of pairs. SICK eng. Finally the SICK data-set (Sentences Involving Compositional Knowledge) (Marelli et al., 2014) has been recently used to highlight distributional properties. SICK is not balanced (1299 positive and 3201 negative pairs), and T 1 and T 2, differently from RTE pairs, have similar length. (2) For a resource with negative polarity (e.g. antonyms) the RID is expected to be the difference between the Resource Impact on negative and on positive pairs (equation 3). RID(LR− ,D) = RI(LR,Dn ) − RI(LR,Dp ) 4.2 P i∈D LexAl(T 1i , T 2i ) P i∈D |T 1i |∗ |T 2i | Sources for Lexical Alignments We carried out experiments using six different sources of lexical alignments, whose use is quite diffused in th"
R15-1051,S15-1022,0,0.0253741,"Missing"
R15-1051,W14-3348,0,0.0104485,"ible the experimental setting, and we calculated accuracy and F1 for the two algorithms using the training section of the data-sets3 . Antonyms. The fifth source of alignment are antonyms (e.g. man and woman). Antonyms are provided by WordNet for English and by MultiWordNet for Italian. The expected polarity of alignments based on antonyms is negative, as we assume that they increase the opposition between T 1 and T 2. Paraphrase Tables. The sixth source of alignment are paraphrase tables (e.g. can be modified and may be revised). We built paraphrase tables from the Meteor translation tables (Denkowski and Lavie, 2014). The idea is that if an n-gram ns in the source language s is translated into n-gram nt in the target language t, and if nt has multiple translations back into s, then all these translations are potential paraphrases of each other. The probability of translation from one language to another can be used to compute the probability that two n-grams in language s are paraphrases of each other. To compute this probability we use all shared translations into the target language t of the two n-grams (both in source language s). There EDITS (Negri et al., 2009), is a distance-based RTE algorithm base"
R15-1051,I05-5002,0,0.117629,"Missing"
R15-1051,W07-1401,1,0.781959,"he dataset D. We use |T 1 |∗ |T 2 |(|T |is the number of tokens in text T) as potential number of potential alignments (Dagan et al., 2012, page 52), although there might be other options, such as |T 1 |+ |T 2|, and max(|T 1|, |T 2|). RI ranges from 0, when no alignment is found, to 1, when all potential alignments are returned by LR. In this section we apply the model described in Section 3 to different data-sets and resources, taking advantage of different sources of lexical and phrase alignments. RI(LR,D) P i∈D LexAl(T 1i , T 2i ) = P i∈D |T 1i |∗ |T 2i | 4.1 RTE-3 eng. The RTE-3 data-set (Giampiccolo et al., 2007) for English has been used in the context of the Recognizing Textual Entailment shared tasks. It has been constructed mainly using application derived text fragments, and it is balanced between positive and negative pairs (about 1600 in total). (1) RTE-3 ita. The Italian RTE-3 data-set1 is the translation of the English one. The goal is to monitor the behaviour of the RID while changing the language. RTE-5 eng. The RTE-5 data-set (Bentivogli et al., 2009) is similar to RTE-3, although T 1 pairs are usually much longer, which, in our terms, means that a higher number of alignments can be potent"
R15-1051,pianta-etal-2008-textpro,0,0.0712539,"itution if two words are aligned; 1 for substitution if two words are not aligned; 1 for insertion; 0 for deletion. The algorithm is normalized on the number of words of T 1 and T 2, after stop 2 http://www.excitement-project.eu/index.php/results/178public-resources 3 We will investigate the behavior of the RID between test and training data-sets in future work. 392 5 words are removed. As for linguistic processing, the Edit Distance algorithm needs tokenization, lemmatization and Part-of-Speech tagging (in order to access resources). We used TreeTagger (Schmid, 1995) for English and TextPro (Pianta et al., 2008) for Italian. In addition we removed stop words, including some very common verbs. Results Table 2 and Table 3 report the results of the experiments on the four data-sets and the seven sources of alignment (including the 0-Knowledge baseline) described in Section 46 . For each resource we show the RID of the resource (given the very low values, RIDs are shown multiplied by a 104 factor), and the accuracy achieved both by the EDITS and the P1EDA algorithms. The last row of the tables shows the Pearson correlation between the RID and the accuracy of the algorithms for each data-set, calculated a"
R15-1051,N03-1013,0,0.0186531,"hird source considers the hyperonymy relation (e.g. dog and mammal): as for synonymy we use WordNet and MultiWordNet, counting as an alignment all the cases where two lemmas are in the hypernym hierarchy, at any distance. The expected polarity of alignments based on hypernyms is positive. Morphological Derivations. The fourth source of alignment are morphological derivations (e.g. invention and invent). As for English, derivations are covered again by WordNet, while for Italian we used MorphoDerivIT, a resource developed within the EXCITEMENT project2 , which has the same structure of CATVAR (Habash and Dorr, 2003) for English. The expected polarity of alignments based on morphological derivations is positive. 4.3 Algorithms In order to verify our hypothesis that the RID index is correlated with the capacity of a system to correctly recognize textual entailment, we run experiments using two different RTE algorithms, i.e. EDITS and P1EDA, which take advantage of lexical resources in different ways. The two algorithms are both supervised, in the sense that they use training data to build a model. As the goal of our experiments is to monitor the behavior of the RID index in different settings, rather than"
R15-1051,2003.mtsummit-systems.9,0,\N,Missing
rehm-etal-2014-strategic,P07-2045,0,\N,Missing
rehm-etal-2014-strategic,piperidis-etal-2014-meta,1,\N,Missing
rehm-etal-2014-strategic,piperidis-2012-meta,1,\N,Missing
S01-1027,magnini-cavaglia-2000-integrating,1,0.739707,"stical techniques, for the three tasks we participated in, i.e. English &apos;all words&apos;, English &apos;lexical sample&apos; and Italian &apos;lexical sample&apos;. The main lexical resource for domains is &quot;WordNet Domains&quot;, an extension of English Wordnet 1.6 (Fellbaum, 1998) developed at ITC-irst, where synsets have been annotated with domain information. 2 WordN et Domains The basic lexical resource we used in SENSEVAL2 is &quot;WordNet Domains&quot;, an extension of WoRDNET 1.6 where each synset has been annotated with at least one domain label, selected from a set of about two hundred labels hierarchically organized (see (Magnini and Cavaglia, 2000) for the annotation methodology and for the evaluation of the resource). The information from the domains that we added is complementary to what is already in WoRDNET. First of all a domain may include synsets of different syntactic categories: for instance MEDICINE groups together senses from Nouns, such as doctor#i and hospi tal#i, and from Verbs such as operate#7. Second, a domain may include senses from different WoRDNET sub-hierarchies (i.e. deriving from different &quot;unique beginners&quot; or from different &quot;lexicographer files&quot;). For example, SPORT contains senses such as athlete#i, deriving f"
S01-1027,W00-0804,1,0.924982,"focuses on the role of domain information. The hypothesis is that domain labels (such as MEDICINE, ARCHITECTURE and SPORT) provide a natural and powerful way to establish semantic relations among word senses, which can be profitably used during the disambiguation process. In particular, domains constitute a fundamental feature of text coherence, such that word senses occurring in a coherent portion of text tend to maximize domain similarity. The importance of domain information in WSD has been remarked in several works, including (Gonzalo et al., 1998) and (Buitelaar and Sacaleanu, 2001). In (Magnini and Strapparava, 2000) we introduced &quot;Word Domain Disambiguation&quot; (WDD) as a variant of WSD where for each word in a text a domain label (among those allowed by the word) has to be chosen instead of a sense label. We also argued that WDD can be applied to disambiguation tasks that do not require fine grained sense distinctions, such as information retrieval and content-based user modeling. For SENSEVAL111 2 the goal was to evaluate the role of domain information in WSD: no other syntactic or semantic information has been used (e.g. semantic relations in WoRDNET) except domain labels. Three systems have been impleme"
S07-1001,P00-1064,0,0.0127234,"ets the occurrence identifier, the sense tag (if in training), and the list of features that apply to the occurrence. 5 http://ixa2.si.ehu.es/semeval-clir/ 6 http://en.wikipedia.org/wiki/ Information retrieval 4 Allocation. Using topic-specific synset similarity measures, they create predictions for each word in each document using only word frequency information. The disambiguation process took aprox. 12 hours on a cluster of 48 machines (dual Xeons with 4GB of RAM). Note that contrary to the specifications, this team returned WordNet 2.1 senses, so we had to map automatically to 1.6 senses (Daude et al., 2000). UNIBA This team uses a a knowledge-based WSD system that attempts to disambiguate all words in a text by exploiting WordNet relations. The main assumption is that a specific strategy for each Part-Of-Speech (POS) is better than a single strategy. Nouns are disambiguated basically using hypernymy links. Verbs are disambiguated according to the nouns surrounding them, and adjectives and adverbs use glosses. ORGANIZERS In addition to the regular participants, and out of the competition, the organizers run a regular supervised WSD system trained on Semcor. The system is based on a single k-NN cl"
S07-1001,P97-1010,0,\N,Missing
S07-1001,D07-1007,0,\N,Missing
S07-1001,P07-1005,0,\N,Missing
S07-1001,W99-0624,0,\N,Missing
S07-1041,S07-1012,0,0.549591,"Missing"
S07-1041,W06-0504,1,\N,Missing
S07-1041,P98-1012,0,\N,Missing
S07-1041,C98-1012,0,\N,Missing
S15-2132,S13-2002,0,0.0321759,"Missing"
S15-2132,girardi-etal-2014-cromer,1,0.731015,"types). Some examples of target entities are Steve Jobs (PERSON), Apple Inc. (ORGANISATION), Airbus A380 (PRODUCT), and Nasdaq (FINANCIAL). The annotation procedure for the creation of gold standard timelines for the target entities required one person month. It consisted of four steps, as described below. Entity annotation. All occurrences of the target entities in the four corpora were marked following (Tonelli et al., 2014). Cross-document co-reference was annotated according to the NewsReader crossdocument annotation guidelines (Speranza and Minard, 2014). For this task, we used CROMER3 (Girardi et al., 2014), a tool designed specifically for cross-document annotation. 2 3 http://en.wikinews.org. https://hlt.fbk.eu/technologies/cromer 780 Event and time anchor annotation. Using CROMER, the corpora were annotated with events following the NewsReader cross-document annotation guidelines (Speranza and Minard, 2014). The annotation of events as defined in (Tonelli et al., 2014) was restricted by limiting the annotation to events that could be placed on a timeline. Thus, we did not annotate adjectival events, cognitive events, counter-factual events (which certainly did not happen), uncertain events (w"
S15-2132,R09-1032,0,0.0178229,"he aim of the Cross-Document Event Ordering task is to build timelines from English news articles. To provide focus to the timeline creation, the task is presented as an ordering task in which events involving a particular target entity are to be ordered chronologically. The task focuses on cross-document event coreference resolution and cross-document temporal relation extraction. Additionally, it has also been the focus of the 6th i2b2 NLP Challenge for clinical records (Sun et al., 2013). The cross-document aspect, however, has not often been explored. One example is the work described in (Ji et al., 2009) using the ACE 2005 training corpora. Here the authors link pre-defined events involving the same centroid entities (i.e. entities frequently participating in events) on a timeline. Nominal coreference resolution has been the topic of SemEval 2010 Task on Coreference Resolution in Multiple Languages (Recasens et al., 2010). TimeLine is a pilot task that goes beyond the above-mentioned evaluation exercises by addressing coreference resolution for events and temporal relation extraction at a cross document level. This task was motivated by work done in the NewsReader project1 . The goal of the N"
S15-2132,S15-2132,1,0.106103,"Missing"
S15-2132,W09-2411,0,0.11861,"Missing"
S15-2132,S13-2003,0,0.0583595,"Missing"
S15-2132,P11-2061,0,0.189729,"m the other corpora. On the other hand, on average, Stock Market timelines contain events from a higher number of different documents, i.e. 9.1, versus 6.2 for Airbus and 5.7 for GM. 5 2004 2005-06-05 2011-01 2011-08-24 2011-10-06 fighting keynote leave step_down described BEFORE SIMULTANEOUS Explicit relations Evaluation Methodology Implicit relations The evaluation methodology of this task is based on the evaluation metric used for TempEval-3 (UzZaman et al., 2013) to evaluate relations in terms of recall, precision and F1 -score. The metric captures the temporal awareness of an annotation (UzZaman and Allen, 2011). Temporal awareness is defined as the performance of an annotation as identifying and categorizing temporal relations, which implies the correct recognition and classification of the temporal entities involved in the relations. We calculate the Precision by checking the number of reduced system relations that can be verified from the reference annotation temporal closure graph, out of number of temporal relations in the reduced system relations. Similarly, we calculate the Recall by checking the number of reduced reference annotation relaFigure 2: Explicit and implicit relations resulting fro"
S15-2132,S13-2001,0,0.277531,"step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents. Four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs, the best of which obtained an F1 -score of 7.85 in the main track (timeline creation from raw text). 1 • TempEval-1 (2007): Temporal Relation Identification (Verhagen et al., 2009) • TempEval-2 (2010): Evaluating Events, Time Expressions, and Temporal Relations (Verhagen et al., 2010) • TempEval-3 (2013): Temporal Annotation (UzZaman et al., 2013) Introduction In any domain, it is important that professionals have access to high quality knowledge for taking wellinformed decisions. As daily tasks of information professionals revolve around reconstructing a chain of previous events, an insightful way of presenting information to them is by means of timelines. The aim of the Cross-Document Event Ordering task is to build timelines from English news articles. To provide focus to the timeline creation, the task is presented as an ordering task in which events involving a particular target entity are to be ordered chronologically. The task f"
S15-2132,S10-1010,0,\N,Missing
S15-2132,S10-1001,0,\N,Missing
S16-1121,S12-1051,0,0.0177356,"hip between two chunks, as an interpretation of the similarity. Given an input pair of sentences, participant systems were asked to: (i) identify the chunks in each sentence; (ii) align chunks across the two sentences; (iii) indicate the relation between the aligned chunks and (iv) specify the similarity score of each alignment. The iSTS task has already been the object of an evaluation campaign in 2015, as a subtask of the SemEval-2015 Task 2: Semantic Textual Similarity (Agirre et al., 2015). More in general, shared tasks for the identification and measurement of STS were organized in 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013) and 2014 (Agirre et al., 2014). Bernardo Magnini Fondazione Bruno Kessler Povo-Trento, Italy magnini@fbk.eu Data provided to participants include three datasets: image captions (Images), pairs of sentences from news headlines (Headlines), and a question-answer dataset collected and annotated during the evaluation of the BEETLE II tutorial dialogue system (Student Answers) (Agirre et al., 2015). For each dataset, two subtracks were released: the first with raw input data (SYS), the second with data split in gold standard chunks (GS). Given these input data, particip"
S16-1121,S13-1004,0,0.0170037,"n interpretation of the similarity. Given an input pair of sentences, participant systems were asked to: (i) identify the chunks in each sentence; (ii) align chunks across the two sentences; (iii) indicate the relation between the aligned chunks and (iv) specify the similarity score of each alignment. The iSTS task has already been the object of an evaluation campaign in 2015, as a subtask of the SemEval-2015 Task 2: Semantic Textual Similarity (Agirre et al., 2015). More in general, shared tasks for the identification and measurement of STS were organized in 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013) and 2014 (Agirre et al., 2014). Bernardo Magnini Fondazione Bruno Kessler Povo-Trento, Italy magnini@fbk.eu Data provided to participants include three datasets: image captions (Images), pairs of sentences from news headlines (Headlines), and a question-answer dataset collected and annotated during the evaluation of the BEETLE II tutorial dialogue system (Student Answers) (Agirre et al., 2015). For each dataset, two subtracks were released: the first with raw input data (SYS), the second with data split in gold standard chunks (GS). Given these input data, participants were required to identi"
S16-1121,S16-1082,0,0.0818188,"tic Textual Similarity” as well as the results of the submitted runs. We use a single neural network classification model for predicting the alignment at chunk level, the relation type of the alignment and the similarity scores. Our best run was ranked as first in one the subtracks (i.e. raw input data, Student Answers), among 12 runs submitted, and the approach proved to be very robust across the different datasets. 1 Introduction The Semantic Textual Similarity (STS) task measures the degree of equivalence between the meaning of two texts, usually sentences. In the Interpretable STS (iSTS) (Agirre et al., 2016) the similarity is calculated at chunk level, and systems are asked to provide the type of the relationship between two chunks, as an interpretation of the similarity. Given an input pair of sentences, participant systems were asked to: (i) identify the chunks in each sentence; (ii) align chunks across the two sentences; (iii) indicate the relation between the aligned chunks and (iv) specify the similarity score of each alignment. The iSTS task has already been the object of an evaluation campaign in 2015, as a subtask of the SemEval-2015 Task 2: Semantic Textual Similarity (Agirre et al., 201"
S16-1121,marelli-etal-2014-sick,0,0.0677663,"Missing"
S16-1121,D14-1162,0,0.0849087,"by calculating the element wise mean of each vector). We use Mikolov word2vec (Mikolov et al., 2013) with 100 dimensions using ukWaC, GigaWords (NYT), Europarl V.7, Training Set (JRC) corpora. The system computes the chunk-to-chunk similarity by calculating the cosine similarity between the two chunk vectors with three different models: the first uses the already described vectors (one feature); the second uses vectors representations extracted with a different corpus and a different parametres -i.e. Google News, with 300 dimensions of the vectors- (one feature); the third uses GloVe vectors (Pennington et al., 2014) with 300 dimensions (one feature). Baseline feature. The baseline output - provided by the organizers (Agirre et al., 2016) - was also exploited, i.e. we consider if the chunks are evaluated as aligned, if chunk1 is not aligned, if chunk2 is not aligned (3 features). Composition of the input data. The last three features refer to the datasets. The system takes into consideration if the chunks are extracted from Headline, Images, or Student Answers dataset. 785 #features Chunk tags Token and lemma overlap WordNet relations and similarity Word embedding Cosine Similarity Baseline feature Compos"
S16-1121,S14-2010,0,\N,Missing
tanev-etal-2004-multilingual,P02-1054,1,\N,Missing
tanev-etal-2004-multilingual,P02-1006,0,\N,Missing
W00-0804,W00-0103,0,0.0293694,"Missing"
W00-0804,magnini-cavaglia-2000-integrating,1,0.684897,"Missing"
W00-0804,P99-1020,0,0.0528559,"Missing"
W00-0804,kilgarriff-yallop-2000-whats,0,\N,Missing
W00-1102,breck-etal-2000-evaluate,0,0.0214217,"Missing"
W00-1102,W98-0705,0,0.0335564,"query expansions for text retrieval is a debated topic. Voorhees (1998) argues that WordNet derived query expansions are effective for very short queries, while they do not bring any improvements for long queries. From a number of experiments (Mandala et al., 1998) conclude that WordNet query expansions can increase recall but degrade precision performances. Three reasons are suggested to explain this behavior: (i) the lack of relations among terms of different parts of speech in WordNet; (ii) many semantic relations are not present in WordNet; (iii) proper names are not included in WordNet. (Gonzalo et al., 1998) pointed out some more weaknesses of WordNet for Information Retrieval purposes, in particular the lack of domain information and the fact that sense distinctions are excessively fine-grained for the task. A related topic of query expansion is query I~anslation, which is performed in Cross-Language Information Retrieval (Verdejo et al. 2000). 1.1 Basic k e y w o r d s The idea is that this level of keywords should reflect as much as possible the words used by an average user to query a web search engine. Given a question expressed with a natural language sentence, its basic keywords are derive"
W00-1102,magnini-cavaglia-2000-integrating,1,0.698557,"tor&quot;) and &quot;luce elettrica&apos;&quot; (&quot;electric light&quot;) appear. In the experiment reported in section 3 Italian synonyms have been manually extracted from the ItalianWordnet database (Roventini et al., 2000), a further extension of the Italian Wordnet produced by the EuroWordNet project (Vossen, 1998). Once the correct synset for a basic keyword is selected, its synonyms are added to the expansion list. In the near future we plan to automate the process of synset selection using word domain disambiguation, a variant of word sense disambiguation based on subject field code information added to WordNet (Magnini and Cavaglih, 2000). ) scopritore (discoverer), ideatore (artificer) i n v e n z i o n e (invention) I s y ~ ) s c o p e r t a (discoverer) derivation daivaaca ) i n v e n t a r e (invenO I synyn~ ) s c o p r i r e (discover) Figure 2: Lexical chain for &quot;inventore&quot; (&quot;inventor&quot;) 2 Query compositions We wanted to take advantage of the &quot;advanced&quot; capabilities of the search engine. In particular we experimented the &quot;Boolean phraase&quot; modality, which allows the user to submit queries with keywords composed by means of logical operators. However we quickly realised that realistic choices were restricted to disjoint com"
W00-1102,W98-0704,0,0.0264412,"e experiment: morphological derivations and synonym expansions. Both of them try to expand a &quot;basic-keyword&quot;, that is a keyword direcdy derived from a natural language question. The language used in the experiments is Italian. Some of the problems that we faced with in this work have been already discussed in previous works in the literature. The use of query expansions for text retrieval is a debated topic. Voorhees (1998) argues that WordNet derived query expansions are effective for very short queries, while they do not bring any improvements for long queries. From a number of experiments (Mandala et al., 1998) conclude that WordNet query expansions can increase recall but degrade precision performances. Three reasons are suggested to explain this behavior: (i) the lack of relations among terms of different parts of speech in WordNet; (ii) many semantic relations are not present in WordNet; (iii) proper names are not included in WordNet. (Gonzalo et al., 1998) pointed out some more weaknesses of WordNet for Information Retrieval purposes, in particular the lack of domain information and the fact that sense distinctions are excessively fine-grained for the task. A related topic of query expansion is"
W00-1102,roventini-etal-2000-italwordnet,1,0.808052,"ica?&quot; (&quot;Who invented the electric light?&quot;) might be one among &quot;Lo scopntore della lute elettrica fu Edison&quot; (&quot;The discoverer of electric light was Edison&quot;), &quot;&apos;L&apos;inventore della illuminazione elettrica fu Edison&quot; (&quot;The inventor of electric illumination was Edison&quot;), &quot;La scopritore della illuminazione elettrica fu Edison&quot; (&quot;The discoverer of electric illumination was Edison&quot;), where different synonyms of &quot;inventore&quot; (&quot;inventor&quot;) and &quot;luce elettrica&apos;&quot; (&quot;electric light&quot;) appear. In the experiment reported in section 3 Italian synonyms have been manually extracted from the ItalianWordnet database (Roventini et al., 2000), a further extension of the Italian Wordnet produced by the EuroWordNet project (Vossen, 1998). Once the correct synset for a basic keyword is selected, its synonyms are added to the expansion list. In the near future we plan to automate the process of synset selection using word domain disambiguation, a variant of word sense disambiguation based on subject field code information added to WordNet (Magnini and Cavaglih, 2000). ) scopritore (discoverer), ideatore (artificer) i n v e n z i o n e (invention) I s y ~ ) s c o p e r t a (discoverer) derivation daivaaca ) i n v e n t a r e (invenO I"
W00-1102,verdejo-etal-2000-evaluating,0,0.0267446,"on performances. Three reasons are suggested to explain this behavior: (i) the lack of relations among terms of different parts of speech in WordNet; (ii) many semantic relations are not present in WordNet; (iii) proper names are not included in WordNet. (Gonzalo et al., 1998) pointed out some more weaknesses of WordNet for Information Retrieval purposes, in particular the lack of domain information and the fact that sense distinctions are excessively fine-grained for the task. A related topic of query expansion is query I~anslation, which is performed in Cross-Language Information Retrieval (Verdejo et al. 2000). 1.1 Basic k e y w o r d s The idea is that this level of keywords should reflect as much as possible the words used by an average user to query a web search engine. Given a question expressed with a natural language sentence, its basic keywords are derived selecting the lernmas for each content word of the question. Verbs are transformed in their corresponding nominalization. Furthermore we decided to consider collocations and multiwords as single keywords, as most of the currently available search engines allow the user to specify &quot;phrases&quot; in a very simple way. In the experiments presented"
W02-1109,E99-1001,0,\N,Missing
W02-1109,C96-1071,0,\N,Missing
W02-1109,A97-1030,0,\N,Missing
W02-1109,P98-1045,0,\N,Missing
W02-1109,C98-1045,0,\N,Missing
W02-1304,W02-1304,1,0.0511956,"Missing"
W02-1304,W00-1702,1,0.907606,"Missing"
W02-1304,W01-0703,1,0.824628,"Missing"
W02-1304,W99-0603,0,0.0765567,"Missing"
W02-1304,W00-1322,1,0.899983,"Missing"
W02-1304,J98-1001,0,0.0564481,"Missing"
W02-1304,J98-1006,0,0.0986902,"Missing"
W02-1304,magnini-cavaglia-2000-integrating,1,0.726135,"Missing"
W02-1304,W00-1326,1,0.886817,"Missing"
W02-1304,P98-2247,0,0.048344,"Missing"
W02-1304,S01-1029,1,0.792942,"Missing"
W02-1304,W97-0201,0,0.436936,"Missing"
W02-1304,S01-1017,1,\N,Missing
W02-1304,W00-1325,0,\N,Missing
W02-1304,P00-1064,0,\N,Missing
W02-1304,W00-0706,1,\N,Missing
W02-1304,P95-1026,0,\N,Missing
W02-1304,W02-0801,1,\N,Missing
W02-1304,C98-2242,0,\N,Missing
W04-0805,S01-1007,0,0.0421649,"Missing"
W04-0805,A00-1031,0,0.0967241,"Missing"
W04-0805,magnini-cavaglia-2000-integrating,1,0.773214,"st set senses were clustered in order to compute mixed- and coarsegrained scores, this year we decided to return just the fine-grained measure, where an automatically tagged instance is correct only if the sense corresponds to the one assigned by humans, and wrong otherwise (i.e. one-to-one mapping). There are different sense clustering methods, but grouping meanings according to some sort of similarity is always an arbitrary decision. We intended to calculate a domain-based coarse-grained score, where word senses were clustered according to the domain information provided in WordNet Domains (Magnini and Cavaglià, 2000). Unfortunately, this approach would have been significant with nouns, but not with adjectives and verbs, that belong mostly to the generic Factotum domain, so we discarded the idea. All the six participating systems were supervised, which means they all used the training data set and no one utilized either unlabelled instances or the lexical database. UNED used also SemCor as an additional source of training examples. IRST-Kernels system exploited Kernel methods for pattern abstraction and combination of different knowledge sources, in particular paradigmatic and syntagmatic information, and"
W04-0861,W04-0828,1,0.681474,"Missing"
W04-0861,W04-0837,1,0.823382,"Missing"
W04-0861,P94-1013,0,0.0414439,"Catalonia. The integration was carried out by the TALP group.   Naive Bayes (NB) is the well–known Bayesian algorithm that classifies an example by choosing the class that maximizes the product, over all features, of the conditional probability of the class given the feature. The provider of this module is IXA. Conditional probabilities were smoothed by Laplace correction.  Decision List (DL) are lists of weighted classification rules involving the evaluation of one single feature. At classification time, the algorithm applies the rule with the highest weight that matches the test example (Yarowsky, 1994). The provider is IXA and they also applied smoothing to generate more robust decision lists.  In the Vector Space Model method (cosVSM), each example is treated as a binary-valued feature vector. For each sense, one centroid vector is obtained from training. Centroids are compared with the vectors representing test examples, using the cosine similarity function, and the closest centroid is used to classify the example. No smoothing is required for this method provided by IXA. 2 The WSD Modules Support Vector Machines (SVM) find the hyperplane (in a high dimensional feature space) that separa"
W04-2214,magnini-cavaglia-2000-integrating,1,0.421647,"nd Amsler, 1986). As regards the usage of Domain hierarchies in the field of multilingual lexicography, an example is given by the EuroWordNet Domain-ontology, a language independent domain hierarchy to which interlingual concepts (ILI-records) can be assigned (Vossen, 1998). In the same line, see also the SIMPLE domain hierarchy (SIMPLE, 2000). Large domain hierarchies are also available on the Internet, mainly meant for classifying web documents. See for instance the Google and Yahoo directories. A large-scale application of a domain hierarchy to a lexicon is represented by WORDNET DOMAINS (Magnini and Cavaglià, 2000). WORDNET DOMAINS is a lexical resource developed at ITCirst where each WordNet synset (Fellbaum, 1998) is annotated with one or more domain labels selected from a domain hierarchy which was specifically created to this purpose. As the WORDNET DOMAINS Hierarchy (WDH) is language-independent, it has been possible to exploit it in the framework of MultiWordNet (Pianta et al., 2002), a multilingual lexical database developed at ITC-irst in which the Italian component is strictly aligned with the English WordNet. In MultiWordNet, the domain information has been automatically transferred from Engli"
W04-2214,W02-1304,1,0.736051,"from English to Italian, resulting in a Italian version of WORDNET DOMAINS. For instance, as the English synset {court, tribunal, judicature} was annotated with the domain LAW, also the Italian synset {corte, tribunale}, which is aligned with the corresponding English synset, results automatically annotated with the LAW domain. This procedure can be applied to any other WordNet (or part of it) aligned with Princeton WordNet (see for instance the Spanish WordNet). It is worth noticing that two of the main ongoing projects addressing the construction of multilingual resources, that is MEANING (Rigau et al. 2002) and BALKANET (see web site), make use of WORDNET DOMAINS. Finally, WORDNET DOMAINS is being profitably used by the NLP community mainly for Word Sense Disambiguation tasks in various languages. Another application of domain hierarchies can be found in the field of corpus creation. In many existing corpora (see for instance the BNC, the ANC, the Brown and LOB Corpora) domain is one of the most used criteria for text selection and/or classification. Given that a domain hierarchy is language independent, if the same domain hierarchy is used to build reference corpora for different languages, the"
W04-2214,J98-1004,0,0.00723395,"e the domain of a text is its broad topic. In this work we will assume that also these two points of view on domains are strictly intertwined. By their nature, domains can be organized in hierarchies based on a relation of specificity. For instance we can say that TENNIS is a more specific domain than SPORT, or that ARCHITECTURE is more general than TOWN PLANNING. Domain hierarchies can be usefully integrated into other linguistic resources and are also profitably used in many Natural Language Processing (NLP) tasks such as Word Sense Disambiguation (Magnini et al. 2002), Text Categorization (Schutze, 1998), Information Retrieval (Walker and Amsler, 1986). As regards the usage of Domain hierarchies in the field of multilingual lexicography, an example is given by the EuroWordNet Domain-ontology, a language independent domain hierarchy to which interlingual concepts (ILI-records) can be assigned (Vossen, 1998). In the same line, see also the SIMPLE domain hierarchy (SIMPLE, 2000). Large domain hierarchies are also available on the Internet, mainly meant for classifying web documents. See for instance the Google and Yahoo directories. A large-scale application of a domain hierarchy to a lexicon is"
W04-3249,C00-1066,0,0.0320323,", domain detection allows a number of useful simplifications in text processing applications, such as, for instance, in Word Sense Disambiguation (WSD). In this paper we introduce Domain Relevance Estimation (DRE) a fully unsupervised technique for domain detection. Roughly speaking, DRE can be viewed as a text categorization (TC) problem (Sebastiani, 2002), even if we do not approach the problem in the standard supervised setting requiring category labeled training data. In fact, recently, unsupervised approaches to TC have received more and more attention in the literature (see for example (Ko and Seo, 2000). We assume a pre-defined set of categories, each defined by means of a list of related terms. We call such categories domains and we consider them as a set of general topics (e.g. S PORT, M EDICINE, P OLITICS) that cover the main disciplines and areas of human activity. For each domain, the list of related words is extracted from W ORD N ET D O MAINS (Magnini and Cavagli`a, 2000), an extension of W ORD N ET in which synsets are annotated with domain labels. We have identified about 40 domains (out of 200 present in W ORD N ET D OMAINS) and we will use them for experiments throughout the paper"
W04-3249,magnini-cavaglia-2000-integrating,1,0.883007,"Missing"
W04-3249,S01-1005,0,0.012662,"- Table 2: W ORD N ET senses and domains for the word “bank”. from act#2, and playing field#1 from location#1. Domains may group senses of the same word into thematic clusters, which has the important sideeffect of reducing the level of ambiguity when we are disambiguating to a domain. Table 2 shows an example. The word “bank” has ten different senses in W ORD N ET 1.6: three of them (i.e. bank#1, bank#3 and bank#6) can be grouped under the E CONOMY domain, while bank#2 and bank#7 both belong to G EOGRAPHY and G EOL OGY. Grouping related senses is an emerging topic in WSD (see, for instance (Palmer et al., 2001)). Finally, there are W ORD N ET synsets that do not belong to a specific domain, but rather appear in texts associated with any domain. For this reason, a FACTOTUM label has been created that basically includes generic synsets, which appear frequently in different contexts. Thus the FACTOTUM domain can be thought of as a “placeholder” for all other domains. 3 Domain Relevance Estimation for Texts The basic idea of domain relevance estimation for texts is to exploit lexical coherence inside texts. From the domain point of view lexical coherence is equivalent to domain coherence, i.e. the fact"
W06-0504,W04-3221,0,0.0225213,"more general Ontology Population from text (cf. Buitelaar et al. 2005); in particular, mentions are well defined and there are systems for automatic mention recognition. Although there is no univocally accepted definition for the OP task, a useful approximation has been suggested by (Bontcheva and Cunningham, 2005) as Ontology Driven Information Extraction with the goal of extracting and classifying instances of concepts and relations defined in a Ontology, in place of filling a template. A similar task has been approached in a variety of perspectives, including term clustering (Lin, 1998 and Almuhareb and Poesio, 2004) and term categorization (Avancini et al. 2003). A rather different task is Ontology Learning, where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (Velardi et al. 2005). However, since mentions have been introduced as an evolution of the traditional Named Entity Recognition task (see Tanev and Magnini, 2006), they guarantee a reasonable level of difficulty, which makes OPTM challenging both for the Computational Linguistic side and the Knowledge Representation community. Second, there already exist annotated data"
W06-0504,P98-2127,0,0.00489203,"espect to the more general Ontology Population from text (cf. Buitelaar et al. 2005); in particular, mentions are well defined and there are systems for automatic mention recognition. Although there is no univocally accepted definition for the OP task, a useful approximation has been suggested by (Bontcheva and Cunningham, 2005) as Ontology Driven Information Extraction with the goal of extracting and classifying instances of concepts and relations defined in a Ontology, in place of filling a template. A similar task has been approached in a variety of perspectives, including term clustering (Lin, 1998 and Almuhareb and Poesio, 2004) and term categorization (Avancini et al. 2003). A rather different task is Ontology Learning, where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (Velardi et al. 2005). However, since mentions have been introduced as an evolution of the traditional Named Entity Recognition task (see Tanev and Magnini, 2006), they guarantee a reasonable level of difficulty, which makes OPTM challenging both for the Computational Linguistic side and the Knowledge Representation community. Second, the"
W06-0504,magnini-etal-2006-cab,0,0.0259059,"Missing"
W06-0504,E06-1003,1,0.677523,"assifying instances of concepts and relations defined in a Ontology, in place of filling a template. A similar task has been approached in a variety of perspectives, including term clustering (Lin, 1998 and Almuhareb and Poesio, 2004) and term categorization (Avancini et al. 2003). A rather different task is Ontology Learning, where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (Velardi et al. 2005). However, since mentions have been introduced as an evolution of the traditional Named Entity Recognition task (see Tanev and Magnini, 2006), they guarantee a reasonable level of difficulty, which makes OPTM challenging both for the Computational Linguistic side and the Knowledge Representation community. Second, there already exist annotated data with mentions, delivered under the ACE (Automatic Content Extraction) initiative (Ferro et al. 2005, Linguistic Data Consortium 2004), which makes the exploitation of machine learning based approaches possible. Finally, having a limited scope with respect to OLP, the OPTM task allows for a better estimation of performance; in particular, it is possible to evaluate more easily the recall"
W06-0504,C98-2122,0,\N,Missing
W06-2713,ide-romary-2002-standards,0,\N,Missing
W07-1401,W03-0906,0,\N,Missing
W07-1401,W05-1203,0,\N,Missing
W07-1401,W05-1209,0,\N,Missing
W07-1401,W05-1206,0,\N,Missing
W07-1401,N04-1019,0,\N,Missing
W07-1401,P98-1013,0,\N,Missing
W07-1401,C98-1013,0,\N,Missing
W07-1401,W05-1201,0,\N,Missing
W07-1401,W05-1210,0,\N,Missing
W07-1401,W04-3206,1,\N,Missing
W10-3114,bentivogli-etal-2010-building,1,0.917704,"omposing RTE pairs The qualitative evaluation we propose takes advantage of previous work on monothematic datasets. A monothematic pair (Magnini and Cabrio, 2009) is defined as a [T,H] pair in which a certain phenomenon relevant to the entailment relation is highlighted and isolated. The main idea is to create such monothematic pairs on the basis of the phenomena which are actually present in the original RTE pairs, so that the actual distribution of the linguistic phenomena involved in the entailment relation emerges. For the decomposition procedure, we refer to the methodology described in (Bentivogli et al., 2010), consisting of a number of steps carried out manually. The starting point is a [T,H] pair taken from one of the RTE datasets, that should be decomposed in a number of monothematic pairs [T, Hi ]mono , where T is the original Text and Hi are the Hypotheses created for each linguistic phenomenon relevant for judging the entailment relation in [T,H]. In detail, the procedure for the creation of monothematic pairs is composed of the following steps: 1. Individuate the linguistic phenomena which contribute to the entailment in [T,H]. 2. For each phenomenon i: (a) Individuate a general entailment r"
W10-3114,P08-1118,0,0.17408,"Missing"
W10-3114,D08-1002,0,0.284597,"results for it, focusing on contradiction caused by negation, antonymy, and paraphrases. Voorhees (2008) carries out an analysis of RTE3 extended task, examining systems’ abilities to detect contradiction and providing explanations of their reasoning when making entailment decisions. Beside defining the categories of construction from which contradiction may arise, Marneffe et al. (2008) provide the annotation of the RTE datasets (RTE-1 and RTE-2) for contradiction. Furthermore, they also collect contradiction “in the wild” (e.g. from newswire, Wikipedia) to sample naturally occurring ones.6 Ritter et al. (2008) extend (Marneffe et al., 2008)’s analysis to a class of contradiction that can only be detected using backgroud knowledge, and describe a case study of contradiction detection based on functional relations. They also automatically generate a corpus of seeming contradiction from the Web text.7 Furthermore, some of the systems presented in the previous editions of the RTE challenges attempted specic strategies to focus on the phenomenon of negation. For instance, (Snow et al., 2006) presents a framework for recognizing textual entailment that focuses on the use of syntactic heuristics to recogn"
W10-3114,N06-1005,0,0.028248,"y also collect contradiction “in the wild” (e.g. from newswire, Wikipedia) to sample naturally occurring ones.6 Ritter et al. (2008) extend (Marneffe et al., 2008)’s analysis to a class of contradiction that can only be detected using backgroud knowledge, and describe a case study of contradiction detection based on functional relations. They also automatically generate a corpus of seeming contradiction from the Web text.7 Furthermore, some of the systems presented in the previous editions of the RTE challenges attempted specic strategies to focus on the phenomenon of negation. For instance, (Snow et al., 2006) presents a framework for recognizing textual entailment that focuses on the use of syntactic heuristics to recognize false entailment. Among the others, heuristics concerning negation mismatch and antonym match are defined. In (Tatu et al., 2007) the logic representation of sentences with negated concepts was altered to mark as negated the entire scope of the negation. (Ferrandez et al., 2009) propose a system facing the entailment recognition by computing shallow lexical deductions and richer inferences based on semantics, and features relating to negation are extracted. In (Iftene et al., 2"
W10-3114,P08-1008,0,0.0903552,"VENSES produces a pretty high number of false negatives, meaning that if the system is not able to find evidences of entailment, it assigns the contradiction value to the pairs (for this system, being able to correctly detect all the phenomena contributing to entailment in a pair is fundamental, otherwise it will be marked as contradiction). 6 Related Work Condoravdi et al. (2003) first proposed contradiction detection as an important NLP task, then (Harabagiu et al., 2006) provided the first em92 pirical results for it, focusing on contradiction caused by negation, antonymy, and paraphrases. Voorhees (2008) carries out an analysis of RTE3 extended task, examining systems’ abilities to detect contradiction and providing explanations of their reasoning when making entailment decisions. Beside defining the categories of construction from which contradiction may arise, Marneffe et al. (2008) provide the annotation of the RTE datasets (RTE-1 and RTE-2) for contradiction. Furthermore, they also collect contradiction “in the wild” (e.g. from newswire, Wikipedia) to sample naturally occurring ones.6 Ritter et al. (2008) extend (Marneffe et al., 2008)’s analysis to a class of contradiction that can only"
W10-3114,D09-1082,0,0.052783,"Missing"
W10-3114,E06-1052,0,\N,Missing
W10-3114,W03-0906,0,\N,Missing
W10-3114,W07-1404,0,\N,Missing
W10-3114,W07-1400,0,\N,Missing
W10-3114,P08-1000,0,\N,Missing
W11-0135,W09-2508,0,0.0438474,"Missing"
W11-0135,W05-1210,0,0.0492314,"Missing"
W11-0135,bentivogli-etal-2010-building,1,0.716619,"h several approaches to face this task have been experimented, and progresses in TE technologies have been shown in RTE evaluation campaigns, a renewed interest is rising in the TE community towards a deeper and better understanding of the core phenomena involved in textual inference. In line with this direction, we are convinced that crucial progress may derive from a focus on decomposing the complexity of the TE task into basic phenomena and on their combination. This belief demonstrated to be shared by the RTE community, and a number of recently published works (e.g. Sammons et al. (2010), Bentivogli et al. (2010)) agree that incremental advances in local entailment phenomena are needed to make significant progress in the main task, which is perceived as omnicomprehensive and not fully understood yet. According to this premise, the aim of this work is to systematize and delve into the work done so far in component-based TE, focusing on the aspects that contribute to highlight a common framework and to define a clear research direction that deserves further investigation. Basing on the original definition of TE, that allows to fomulate textual inferences in an application independent way and to take adv"
W11-0135,C10-2012,1,0.848805,"onsist of removing one module at a time from a system, and rerunning the system on the test set with the other modules, except the one tested. The results obtained were not satisfactory, since the impact of a certain resource on system performances is really dependent on how it is used by the system. In some cases, resources like WordNet demonstrated to be very useful, while for other systems their contribution is limited or even damaging, as observed also in Sammons et al. (2010). To provide a more detailed evaluation of the capabilities of a TE system to address specific inference types, in Cabrio and Magnini (2010) we propose a methodology for a qualitative evaluation of TE systems, that takes advantage of the decomposition of T-H pairs into monothematic pairs (described in Section 3). The assumption is that the more a system is able to correctly solve the linguistic phenomena underlying the entailment relation separately, the more the system should be able to correctly judge more complex pairs, in which different phenomena are present and interact in a complex way. According to such assumption, the higher the accuracy of a system on the monothematic pairs and the compositional strategy, the better its"
W11-0135,W07-1409,0,0.0727207,"Missing"
W11-0135,W09-2507,0,0.063703,"Missing"
W11-0135,P08-1118,0,0.0555024,"Missing"
W11-0135,P10-1122,0,0.550759,"t (the text T). Although several approaches to face this task have been experimented, and progresses in TE technologies have been shown in RTE evaluation campaigns, a renewed interest is rising in the TE community towards a deeper and better understanding of the core phenomena involved in textual inference. In line with this direction, we are convinced that crucial progress may derive from a focus on decomposing the complexity of the TE task into basic phenomena and on their combination. This belief demonstrated to be shared by the RTE community, and a number of recently published works (e.g. Sammons et al. (2010), Bentivogli et al. (2010)) agree that incremental advances in local entailment phenomena are needed to make significant progress in the main task, which is perceived as omnicomprehensive and not fully understood yet. According to this premise, the aim of this work is to systematize and delve into the work done so far in component-based TE, focusing on the aspects that contribute to highlight a common framework and to define a clear research direction that deserves further investigation. Basing on the original definition of TE, that allows to fomulate textual inferences in an application indep"
W11-0135,W07-1412,0,0.0188736,"a global judgment for a pair. The definition presented above provides a strong interpretation of the compositional framework for TE, that can be described as a continuum that tends towards systems developed combining identifiable and separable components addressing specific inference types. A number of works in the literature can be placed along this continuum, according to how much they get closer to this interpretation. Systems addressing TE exploiting machine learning techniques with a variety of features, including lexical-syntactic and semantic features (e.g. Kozareva and Montoyo (2006), Zanzotto et al. (2007)) tend towards the opposite extreme of this framework, since even if linguistic features are used, they bring information about a specific aspect relevant to the inference task but they do not provide an independent judgment on it. These systems are not modular, and it is difficult to assess the contribution of a certain feature in providing the correct overall judgment for a pair. A step closer towards the direction of component-based TE is done by Bar-Haim et al. (2008), that model semantic inference as application of entailment rules specifying the generation of entailed sentences from a so"
W11-0135,W07-1401,1,\N,Missing
W12-4006,N03-1003,0,0.0609443,"earch Paraphrase Corpus2 , pairs of sentences are extracted from news sources on the web, and manually annotated. As for rule repositories collected using distributional properties, DIRT (Discovery of Inference Rules from Text)3 is a collection of inference rules 1 http://www.aclweb.org/aclwiki/index. php?title=RTE_Knowledge_Resources 2 http://research.microsoft.com/en-us/ downloads 3 http://www.aclweb.org/aclwiki/index. php?title=DIRT_Paraphrase_Collection (Lin and Pantel, 2001), obtained extracting binary relations between a verb and an object-noun (or a small clause) from dependency trees. Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus. Since the data sets cited so far are paraphrase collections, rules are bidirectional, while one of the peculiarities of the entailment relation is the directionality, addressed in our work. Aharon et al. (2010) presented FRED, an algorithm for generating entailment rules between predicates from FrameNet. Moreover, the TEASE collection of entailment rules (Szpektor et al., 2004) consists of 136 tem"
W12-4006,bentivogli-etal-2010-building,1,0.934026,", their concrete use reflects this limitation. For instance, rule 2 (extracted from DIRT) fails if applied to “The mathematician established the validity of the conjecture”, where the sense of establish is not a synonym of create (but of prove, demonstrate), decreasing system’s precision. Moreover, these rules often suffer from lack of directionality, and from low accuracy (i.e. the strength of association of the two sides of the rule is often weak, and not well defined). Such observations are also in line with the discussion on ablation tests carried out at the last RTE evaluation campaigns (Bentivogli et al., 2010). Additional constraints specifying the variable types are therefore required to correctly instantiate them. In this work, we propose to take advantage of Collaboratively Constructed Semantic Resources (CSRs) (namely, Wikipedia) to mine information useful to context-rich entailment rule acquisition. More specifically, we take advantage of material obtained through Wikipedia revisions, which provides at the same time real textual variations from which we may extrapolate the relevant syntactic context, and several simplifications with respect to alternative resources. We consider T-H pairs where"
W12-4006,D08-1021,0,0.456579,"lity: 0.8 where the variables may be instantiated by any textual element with a specified syntactic relation with the verb. Both kinds of rules are typically acquired either from structured sources (e.g. WordNet (Fellbaum, 1998)), or from unstructured sources according for instance to distributional properties (e.g. DIRT (Lin and Pantel, 2001)). Entailment rules should typically be applied only in specific contexts, defined in (Szpektor et al., 2007) as relevant contexts. Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (Callison-Burch, 2008)), but most do not. Because of a lack of an adequate representation of the linguistic context in which the 34 Proceedings of the 3rd Workshop on the People’s Web Meets NLP, ACL 2012, pages 34–43, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics rules can be successfully applied, their concrete use reflects this limitation. For instance, rule 2 (extracted from DIRT) fails if applied to “The mathematician established the validity of the conjecture”, where the sense of establish is not a synonym of create (but of prove, demonstrate), decreasing system’s pr"
W12-4006,P03-1054,0,0.00673419,"ki 09 as the Hypothesis (see Examples 2 and 3). Example 2. T: The Oxford Companion to Philosophy says ”there is no single defining position that all anarchists hold [...]” H: According to the Oxford Companion to Philosophy ”there is no single defining position that all anarchists hold [...] ” Example 3. T: Bicycles are used by all socio-economic groups because of their convenience [...]. H: Bicycles are used by all socio-economic groups due to their convenience [...]. 4.3 Step 3: extraction of entailment rules Pairs in set b are collected in a data set, and processed with the Stanford parser (Klein and Manning, 2003); chunks are extracted from each pair using the script chunklink.pl.6 The assumption underlying our approach is that the difference between T and H (i.e. the editing made by the user on a specific structure) can be extracted from such pairs and identified as an entailment rule. The rule extraction algorithm was implemented to this purpose. In details, for each sentence pair the algorithm iteratively compares the chunks of T and H to extract the ones that differ. It can be the case that several chunks of H are identical to a given chunk of T, as in: T:&lt;NP&gt;[The DT][Oxford NNP][Companion NNP] &lt;/N"
W12-4006,W06-3907,0,0.0219747,"Missing"
W12-4006,max-wisniewski-2010-mining,0,0.18489,"edia revision history in NLP tasks has been previously investigated by a few works. In (Zanzotto and Pennacchiotti, 2010), two versions of Wikipedia and semi-supervised machine learning methods are used to extract large TE data sets similar to the ones provided for the RTE challenges. (Yatskar et al., 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications. Nelken and Yamangil (2008) compare different versions of the same document to collect users’ editorial choices, for automated text correction, sentence compression and text summarization systems. (Max and Wisniewski, 2010) use the revision history of French Wikipedia to create a corpus of natural rewritings, including spelling corrections, reformulations, and other local text transformations. In (Dutrey et al., 2011), a subpart of this corpus is analyzed to define a typology of local modifications. Because of its high coverage, Wikipedia is used by the TE community for lexical-semantic rules acquisition, named entity recognition, geographical information1 (e.g. (Mehdad et al., 2009), (Mirkin et al., 2009), (Iftene and Moruz, 2010)), i.e. to provide TE systems with world and background knowledge. However, so far"
W12-4006,I05-5011,0,0.211263,"create Y probability: 0.8 where the variables may be instantiated by any textual element with a specified syntactic relation with the verb. Both kinds of rules are typically acquired either from structured sources (e.g. WordNet (Fellbaum, 1998)), or from unstructured sources according for instance to distributional properties (e.g. DIRT (Lin and Pantel, 2001)). Entailment rules should typically be applied only in specific contexts, defined in (Szpektor et al., 2007) as relevant contexts. Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. (Sekine, 2005), (Callison-Burch, 2008)), but most do not. Because of a lack of an adequate representation of the linguistic context in which the 34 Proceedings of the 3rd Workshop on the People’s Web Meets NLP, ACL 2012, pages 34–43, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics rules can be successfully applied, their concrete use reflects this limitation. For instance, rule 2 (extracted from DIRT) fails if applied to “The mathematician established the validity of the conjecture”, where the sense of establish is not a synonym of create (but of prove, demonstrate)"
W12-4006,C08-1107,0,0.0193952,"hrase collections, rules are bidirectional, while one of the peculiarities of the entailment relation is the directionality, addressed in our work. Aharon et al. (2010) presented FRED, an algorithm for generating entailment rules between predicates from FrameNet. Moreover, the TEASE collection of entailment rules (Szpektor et al., 2004) consists of 136 templates provided as input, plus all the learned templates. Their web-based extraction algorithm is applied to acquire verb-based expressions. No directionality of the pairs is specified, but additional guessing mechanisms it are proposed. In (Szpektor and Dagan, 2008), two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al., 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch, 2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects: their goal is paraphrase extraction, while we are extracting directional entailment rules; as textual resources for p"
W12-4006,P07-1058,0,0.0413325,"Missing"
W12-4006,N10-1056,0,0.14007,"escribes in details the steps for context-rich rules acquisition from Wikipedia pairs. Section 5 reports about the experiments on causality and temporal expressions and the obtained results. Finally, Section 6 concludes the paper and suggests directions for future improvements. 35 2 Related work The use of Wikipedia revision history in NLP tasks has been previously investigated by a few works. In (Zanzotto and Pennacchiotti, 2010), two versions of Wikipedia and semi-supervised machine learning methods are used to extract large TE data sets similar to the ones provided for the RTE challenges. (Yatskar et al., 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications. Nelken and Yamangil (2008) compare different versions of the same document to collect users’ editorial choices, for automated text correction, sentence compression and text summarization systems. (Max and Wisniewski, 2010) use the revision history of French Wikipedia to create a corpus of natural rewritings, including spelling corrections, reformulations, and other local text transformations. In (Dutrey et al., 2011), a subpart of this corpus is analyzed to define a typology of local modifications. B"
W12-4006,P10-2045,0,\N,Missing
W12-4006,W10-3504,0,\N,Missing
W12-4006,W04-3206,0,\N,Missing
W15-0803,W14-2907,0,0.0686625,"Missing"
W15-0803,E06-2001,0,0.0128937,"r et al., 2010) and VerbNet (Schuler, 2005). They differ from T-PAS because the structures they identify are not acquired from corpora following a systematic proce3 18 2. an inventory of about 230 corpus-derived semantic types (STs) for nouns (HUMAN, ARTIFACT, EVENT, etc.), relevant for disambiguation of the verb in context, which was obtained by applying the CPA procedure to the analysis of concordances for ca 1500 English and Italian verbs; 3. a corpus of sentences that instantiate T-PAS, tagged with lexical unit (verb) and pattern number. The reference corpus is a reduced version of ItWAC (Baroni and Kilgarriff, 2006). Pattern acquisition and ST tagging involves the following steps: 1) choose a target verb and create a sample of 250 concordances in the corpus; 2) while browsing the corpus lines, identify the variety of relevant syntagmatic structures corresponding to the minimal contexts where all words are disambiguated; 3) identify the typing constraint of each argument slot of the structure by inspecting the lexical set of fillers: such constraints are crucial to distinguish dure. Another important resource is PDEV (Hanks and Pustejovsky, 2005), a pattern dictionary of English verbs which is the main pr"
W15-0803,W13-1202,0,0.0686171,"Missing"
W15-0803,lenci-etal-2012-lexit,0,0.0303674,"eate a sample of 250 concordances in the corpus; 2) while browsing the corpus lines, identify the variety of relevant syntagmatic structures corresponding to the minimal contexts where all words are disambiguated; 3) identify the typing constraint of each argument slot of the structure by inspecting the lexical set of fillers: such constraints are crucial to distinguish dure. Another important resource is PDEV (Hanks and Pustejovsky, 2005), a pattern dictionary of English verbs which is the main product of the CPA procedure applied to English. As for Italian, a complementary project is LexIt (Lenci et al., 2012), a resource providing automatically acquired distributional information about verbs, adjectives and nouns. Differently from T-PAS, LexIt does not convey an inventory of patterns and the categories used for classifying the semantics of arguments are not corpus-driven. Inventory of senses such as MultiWordNet (Pianta et al., 2002) and Senso Comune (Oltramari et al., 2013) are resources to which T-PAS can be successfully linked with the goal of populating the former with corpus-driven patternbased sense distinctions for verbs. 3 Figure 2: Selected pattern for the verb divorare. Figure 3: Example"
W15-0803,N13-1091,0,0.0275291,"It is worth to note that one the outcome of this work is that event coreference plays a crucial role in detecting textual oppositions, very much as similarity features are relevant to establish opposition at the lexical level. The Recognizing Textual Entailment initiative (Dagan et al., 2009) addressed contradiction under the so called ”three-way” evaluation schema (i.e. entailment, contradiction, unknown). Specific techniques for detecting contradiction include the use of ”negative alignments” among portions of text (Magnini et al., 2014) and methods for detecting the polarity of predicates (Lotan et al., 2013). As far as applications are concerned, there is an increasing interest in detecting various kinds of oppositions in large document repositories. Few examples include recent approaches that address inconsistencies in Wikipedia (Cabrio et al., 2014), approaches to estimate the truth of a certain fact (Martinez-Gomez et al., 2014), and the automatic reconstruction of consistent story-lines on a certain topic of interest. 4 Annotation Schema for Opposite Relations For Italian, to the best of our knowledge, there are no annotation schemas that identify different types of opposition applied to verb"
W15-0803,P14-5008,1,0.873898,"Missing"
W15-0803,J13-3004,0,\N,Missing
W18-5036,N16-1030,0,0.729699,"cludes two main components: a neural classifier (NNg ) trained solely on the entity names in a gazetteer, described in Section 4.1, and the entity tagger that applies the neural classifier to a user utterance, described in Section 4.2. 4.1 NNg Classifier The NNg classifier is the core of the gazetteerbased approach. It is implemented using a multilayer bidirectional LSTM (Schuster and Paliwal, 1997) that classifies an input sequence of tokens either as entity or non-entity for a certain entity category, with a certain degree of confidence. We base our NNg classifier on the system proposed in (Lample et al., 2016), which was modified to match the peculiarities of the gazetteer-based approach: (i) we extend it as a 3-layer biLSTM with 120 units per layer and a single dropout layer 319 TRUE (0.75) separators, as a white space, a comma or the and conjunction, so to mimic how multiple entities are usually expressed in sentences. Alternatively, t1 and t2 can be tokens randomly extracted from a generic corpus, so as to mimic cases when the entity is expressed in isolation. For example, if the initial positive example is black and white t-shirt, the possible negative sub-sequences that are generate are: |blac"
W18-5036,W17-2605,0,0.0609867,"Missing"
W18-5036,doddington-etal-2004-automatic,0,0.0535401,"ling task (see, for instance, the Conll shared tasks on named entities recognition (Tjong Kim Sang and De Meulder, 2003)). Given an utterance U = {t1 , t2 , ..., tn } and a set of entity categories C = {c1 , c2 , ..., cm }, the task is to label the tokens in U that refer to entities belonging to the categories in C. As an example, using the IOB format (Inside, Outside, Beginning) (Ramshaw and Marcus, 1995), the utterance ”I would like to order a salami pizza and two mozzarella cheese sandwiches”, would be labeled as shown in Table 1. We refer to the Automatic Content Extraction program - ACE (Doddington et al., 2004), where 318 I O would O like O to O order O a O salami B-FOOD pizza I-FOOD and O two O mozzarella B-FOOD cheese I-FOOD sandwiches I-FOOD Table 1: IOB annotation of food entities inside user request. two main entity classes are distinguished: named entities and nominal entities. We focus on the latter, as this is more relevant for utterance understanding in the e-commerce scenario. Nominal entities are noun phrase expressions describing an entity. They can be composed by a single name (e.g. pasta, carpet, parka) or by more than one token (e.g. capri sofa bed beige, red jeans skinny fit, lightwe"
W18-5036,pianta-etal-2008-textpro,0,0.0219593,"em that uses a terminological-driven and rule-based named entity recognizer, taking advantage of both entity dictionaries and rules based on chunks. The core strategy is that a chunk in a text is recognized as belonging to a category C if any of its tokens are present in the gazetteer for category C. The approach in (Eftimov et al., 2017) is tailored to a single domain/language and involves merging successive chunks into a single one based on the rules imposed by the algorithm. We extended the approach by adding morphological features and the possible PoS of a word, for which we used TextPro (Pianta et al., 2008), (see Algorithm 2). We assume that the dictionary+chunk algorithm is particularly suitable for compositional entities. In fact, actual entities in a text can still be recognized even if the perfect match is not present in the original dictionary. For example, the tarAlgorithm 1 NNg Tagger 1: for sub-sequence in utterance do 2: if sub-sequence is an entity then 3: add sub-sequence to entity-list 4: else 5: discard sub-sequence 6: order entity-list by confidence-score 7: for element in entity-list do 8: if element not overlap previous elements then 9: tag element as entity 10: else 11: discard"
W18-5036,W95-0107,0,0.049792,"e such names. For the purposes of this paper, it is relevant to notice that taking advantage of e-commerce website catalogs, it 3.1 Entity Recognition Entity recognition has been largely approached as a sequence labeling task (see, for instance, the Conll shared tasks on named entities recognition (Tjong Kim Sang and De Meulder, 2003)). Given an utterance U = {t1 , t2 , ..., tn } and a set of entity categories C = {c1 , c2 , ..., cm }, the task is to label the tokens in U that refer to entities belonging to the categories in C. As an example, using the IOB format (Inside, Outside, Beginning) (Ramshaw and Marcus, 1995), the utterance ”I would like to order a salami pizza and two mozzarella cheese sandwiches”, would be labeled as shown in Table 1. We refer to the Automatic Content Extraction program - ACE (Doddington et al., 2004), where 318 I O would O like O to O order O a O salami B-FOOD pizza I-FOOD and O two O mozzarella B-FOOD cheese I-FOOD sandwiches I-FOOD Table 1: IOB annotation of food entities inside user request. two main entity classes are distinguished: named entities and nominal entities. We focus on the latter, as this is more relevant for utterance understanding in the e-commerce scenario. N"
W18-5036,W17-5526,0,0.0683486,"Missing"
W18-5036,W03-0419,0,0.351253,"Missing"
W18-5036,P17-1019,0,0.0323576,"Missing"
W18-5704,W17-5510,0,0.0294401,"testing for each independent variable, where we provided to the subjects of the experiment the transcripts of some conversations between a human user and CH1. Before starting the experiment, the user received a short text describing the task. 26 4.1 Interaction Strategies versational agent functionality of adapting its language to that of the user. The agent will start using the user’s frequent expressions in order to align its lexicon. For example, it should align its linguistic register or reuse the same words used by the user in the generation of the following turn (Branigan et al., 2010; Duplessis et al., 2017). In Table 1 we give, as an example, the transcript used as stimulus material for the empathy variable. Five strategies, together with their linguistic parameters, were analyzed. The transcripts of the experimental condition were realized by two expert linguists, following the substitution/insertion instructions described in Section 3. Empathy can be defined as the ability of a conversational agent to adapt to the user feelings and also to provide flexible emotionally-coloured responses for different purposes (Callejas et al., 2011). There exist many different ways in which emotions are define"
W18-5704,dybkjaer-etal-2004-usability,0,0.290994,", where two slightly different transcripts of the same interaction with a conversational agent are presented to the user. Through a series of pilot experiments we prove that this methodology allows fast and cheap experimentation/evaluation, focusing on aspects that are overlooked by current methods. 1 Introduction The evaluation of task-oriented conversational agents is usually focused on measuring their effectiveness, either at the single turn level - see for example (Wen et al., 2015; Frampton and Lemon, 2006; Chen et al., 2013) - or at the level of the whole interaction - e.g success rate (Dybkjaer et al., 2004). Still, as conversational agents are becoming more complex and human-like (Bowden et al., 2017; Romero et al., 2017; Cercas Curry et al., 2017), these evaluation methodologies may not suffice. In this paper, we present a framework for evaluating interaction strategies of conversational agents during their development phase. Our approach combines in a novel way methodologies already tested and validated, and is based on 2 Related Works Several frameworks to evaluate dialogue systems have been proposed. So far, evaluation mainly focused on implemented components/systems and Proceedings of the 2"
W18-5704,P06-1024,0,0.218532,"quality of experience of agent’s interaction strategies. The methodology is based on a within subject design, where two slightly different transcripts of the same interaction with a conversational agent are presented to the user. Through a series of pilot experiments we prove that this methodology allows fast and cheap experimentation/evaluation, focusing on aspects that are overlooked by current methods. 1 Introduction The evaluation of task-oriented conversational agents is usually focused on measuring their effectiveness, either at the single turn level - see for example (Wen et al., 2015; Frampton and Lemon, 2006; Chen et al., 2013) - or at the level of the whole interaction - e.g success rate (Dybkjaer et al., 2004). Still, as conversational agents are becoming more complex and human-like (Bowden et al., 2017; Romero et al., 2017; Cercas Curry et al., 2017), these evaluation methodologies may not suffice. In this paper, we present a framework for evaluating interaction strategies of conversational agents during their development phase. Our approach combines in a novel way methodologies already tested and validated, and is based on 2 Related Works Several frameworks to evaluate dialogue systems have b"
W18-5704,W13-2305,0,0.0131236,"results; (ii) the possible measured improvements of the system can still be biased by confounding variables; (iii) it is difficult for wizards to provide consistent responses across sessions; (iv) ‘behavior instructions’ should be prepared and given to the wizard and possibly to each single user1 (v) these ’behavior instructions’ cannot describe every single reaction, but must try to control typical situations. Evaluation in related fields. Our design leverages in a novel way elements used in several fields. Two variants testing with controlled stimulus material. In the MT field, the work by (Graham et al., 2013) used a ‘within subject’ design where each evaluator was sometimes presented with a small random textual variation (control condition) of a translation they were already exposed to (experimental condition). This methodology was used to evaluate the quality of raters’ judgments. Closely to our approach, the MT evaluation campaign presented in (Bojar et al., 2016) used expert annotators for pairwise system comparisons denoting whether a system A was judged better than, worse than, or equivalent to another system B. In this case the two conditions were presented simultaneously, side by side, rath"
W18-5704,P12-1104,1,0.828713,"guiding the user during the interaction strongly affects its naturalness. On the other hand Wizards require significant training so to respond in a way that is credible and consistent. 25 ity being tested), including the outcome (e.g. success of the interaction) so that, if one version is preferred over the other, we can conclude that the effect of preference is solely due to the variable of interest (e.g. the “formality level” of the language, the empathy of the agent) and not to other factors. The procedure for setting up an experiment is: and focused on ecological validity is presented in (Guerini et al., 2012). This approach, however, uses a between-subject design, where subjects are presented with just one stimulus material. Transcripts and ‘third party’ evaluation. Two approaches that use transcripts of the conversation, instead of a direct interaction with the agent, are presented in (Jurˇc´ıcˇ ek et al., 2011; Yang et al., 2010). These works compared lab experiments with crowdsourced ones - in the scenario of spoken dialogue systems - showing that the results in the former (direct interaction with the system) are comparable with the results in the latter (third party users reading transcription"
W18-5704,C10-2011,0,0.0331076,"e manifestation of the user emotion, which can be processed considering linguistic (Balahur et al., 2014) and paralinguistic cues (Schuller et al., 2013). Formality in linguistics is expressed through the choice of lexical expressions. According to the context, the speaker can use a specific linguistic register, style and lexicon (Heylighen and Dewaele, 1999). In order to detect the formality of a text there exist different strategies. One is to detect the average of deixis for each grammatical category of words (Heylighen and Dewaele, 1999); another is to use words length and latinate affix (Brooke et al., 2010). Facing is the ability to tackle situations in which the conversational agent has not a proper or preset answer (Morrissey and Kirakowski, 2013). We can observe two kinds of facing for unexpected users’ input: (i) the agent is not able to recognize the intention and makes resort to a default answer, e.g. “Sorry I do not understand, could you repeat?”; (ii) the agent is able to recognize the intention and it provides a suitable/contextual answer even if it is not endowed with the skills to solve it. Vocabulary Extension concerns agent’s ability to learn new words during the conversation and us"
W18-5704,D16-1230,0,0.0158156,"validation of stimulus material and made resort to traditional evaluation procedures for the final evaluation. Finally, in the realm of persuasive NLG a crowdsourced approach based on A/B testing followed different criteria taken from other research fields, such as machine translation (Wen et al., 2016), human-computer interaction (Allen et al., 2001), user experience and interfaces design (Skantze, 2005). The fact that these methodologies are not designed to evaluate dialogue system, can affect the results - for example, machine translation metrics do not correlate well with human judgments (Liu et al., 2016). Another common aspect of these approaches is that they rely on a complete implementation of the system to evaluate aspects such as efficiency (Raux et al., 2006), quality (Shawar and Atwell, 2007) or both (Silvervarg and J¨onsson, 2011), while in the case of our interaction strategies it would be useful to have a simulation approach that allows to predict the possible impact of such strategies. In the following we first discuss standard methodologies for implemented systems, then methodologies using simulation, and finally evaluation in related fields that inspired our approach. Evaluation o"
W18-5704,W07-0313,0,0.110197,"ting followed different criteria taken from other research fields, such as machine translation (Wen et al., 2016), human-computer interaction (Allen et al., 2001), user experience and interfaces design (Skantze, 2005). The fact that these methodologies are not designed to evaluate dialogue system, can affect the results - for example, machine translation metrics do not correlate well with human judgments (Liu et al., 2016). Another common aspect of these approaches is that they rely on a complete implementation of the system to evaluate aspects such as efficiency (Raux et al., 2006), quality (Shawar and Atwell, 2007) or both (Silvervarg and J¨onsson, 2011), while in the case of our interaction strategies it would be useful to have a simulation approach that allows to predict the possible impact of such strategies. In the following we first discuss standard methodologies for implemented systems, then methodologies using simulation, and finally evaluation in related fields that inspired our approach. Evaluation of implemented systems. Among the metrics used for evaluating specific components of a system we can briefly mention: (i) fluency/grammaticality of the generated sentences in the NLG step of the inte"
W18-5704,W01-1614,0,0.160093,"focused on implemented components/systems and Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, 978-1-948087-75-9 24 Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 24–32 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics ISBN 978-1-948087-75-9 lin et al., 2018; Alfonseca, 2017) Evaluation through simulation. If the system is still at an early stage of development, a viable solution is to use WoZ experiments (Dahlb¨ack et al., 1993; Paek, 2001; Raux et al., 2006), in which the interaction is simulated and users are prepared on how to behave. Still, this approach suffers of some main drawbacks: (i) the need for conducting several time-consuming interactions to get stable results; (ii) the possible measured improvements of the system can still be biased by confounding variables; (iii) it is difficult for wizards to provide consistent responses across sessions; (iv) ‘behavior instructions’ should be prepared and given to the wizard and possibly to each single user1 (v) these ’behavior instructions’ cannot describe every single reactio"
W18-5704,P14-1017,0,0.0155566,"losely to our approach, the MT evaluation campaign presented in (Bojar et al., 2016) used expert annotators for pairwise system comparisons denoting whether a system A was judged better than, worse than, or equivalent to another system B. In this case the two conditions were presented simultaneously, side by side, rather than in a random sequential order as in (Graham et al., 2013). Other seminal approaches - using direct comparison of stimulus materials via pairwise comparison - is presented in the realm of affective NLG (Van Der Sluis and Mellish, 2010), and in the domain of persuasive NLP (Tan et al., 2014). Still, both works used this procedure just for the validation of stimulus material and made resort to traditional evaluation procedures for the final evaluation. Finally, in the realm of persuasive NLG a crowdsourced approach based on A/B testing followed different criteria taken from other research fields, such as machine translation (Wen et al., 2016), human-computer interaction (Allen et al., 2001), user experience and interfaces design (Skantze, 2005). The fact that these methodologies are not designed to evaluate dialogue system, can affect the results - for example, machine translation"
W18-5704,W11-2704,0,0.015621,"eraction, think and digit the input at each turn and read the corresponding wizard response; at the same time the Wizard needs to do the same. 5 Advantages With the initial evidence, provided by the experiments, we can reasonably state that the framework we are proposing has some important advantages: Cheap and Fast. The evaluation can be obtained using platform such as CrowdFlower or AMT, choosing high level and possibly native speaker contributors. Crowdsourcing approaches make it quick and cheap to run evaluation experiments as compared to ecological ones, see for example what reported in (Reiter, 2011). Flexibility. The framework gives the possibility to define the dependent and independent variables that better match the strategies and modalities of interaction that need to be evaluated. Moreover, using crowdsourcing approaches together with hand curated transcripts we can easily experiment several variables/versions of the conversational agents or control for multiple mixed effects (e.g. linguistic style * empathy). We can also test different levels of a strategy, for example to find the optimal formality level. Experiment design. the adoption of a pairwise comparison of the two versions"
W18-5704,N03-1026,0,0.0162953,"uld be useful to have a simulation approach that allows to predict the possible impact of such strategies. In the following we first discuss standard methodologies for implemented systems, then methodologies using simulation, and finally evaluation in related fields that inspired our approach. Evaluation of implemented systems. Among the metrics used for evaluating specific components of a system we can briefly mention: (i) fluency/grammaticality of the generated sentences in the NLG step of the interaction, that can be done either manually (Wen et al., 2015) or in a semiautomatic way, as in (Riezler et al., 2003); (ii) slots correctly realized, an automatic evaluation of the NLG component (Scheffler and Young, 2002; Frampton and Lemon, 2006); (iii) slots correctly recognized, an automatic technique used to evaluate the NLU component (Levin and Pieraccini, 1997; Chen et al., 2013). Among the metrics used for evaluating whole interactions there is success rate. It can be based on objective automatic measures or on a subjective evaluation made by users evaluating the system according to guidelines provided by the experimenter (Dybkjaer et al., 2004). Finally, a framework worth mentioning is PARADISE (Wal"
W18-5704,P97-1035,0,0.554782,"03); (ii) slots correctly realized, an automatic evaluation of the NLG component (Scheffler and Young, 2002; Frampton and Lemon, 2006); (iii) slots correctly recognized, an automatic technique used to evaluate the NLU component (Levin and Pieraccini, 1997; Chen et al., 2013). Among the metrics used for evaluating whole interactions there is success rate. It can be based on objective automatic measures or on a subjective evaluation made by users evaluating the system according to guidelines provided by the experimenter (Dybkjaer et al., 2004). Finally, a framework worth mentioning is PARADISE (Walker et al., 1997) that is specifically devoted to spoken dialogue systems (while in our work we consider text based interactions only). This work focuses on metrics such as task success rate and dialogue cost (e.g. dialogue time, number of utterances, agent response delay) to evaluate the quality of a system. With regard to spoken dialogue systems, the use of crowdsourcing for collecting preference judgments has already been explored, for example in (Trippas et al., 2017; Chuk1 e.g. ‘pretend you are sad because ...’ so to trigger the desired system response, such as empathy. In fact, if the user were totally ‘"
W18-5704,D15-1199,0,0.117548,"the impact on the quality of experience of agent’s interaction strategies. The methodology is based on a within subject design, where two slightly different transcripts of the same interaction with a conversational agent are presented to the user. Through a series of pilot experiments we prove that this methodology allows fast and cheap experimentation/evaluation, focusing on aspects that are overlooked by current methods. 1 Introduction The evaluation of task-oriented conversational agents is usually focused on measuring their effectiveness, either at the single turn level - see for example (Wen et al., 2015; Frampton and Lemon, 2006; Chen et al., 2013) - or at the level of the whole interaction - e.g success rate (Dybkjaer et al., 2004). Still, as conversational agents are becoming more complex and human-like (Bowden et al., 2017; Romero et al., 2017; Cercas Curry et al., 2017), these evaluation methodologies may not suffice. In this paper, we present a framework for evaluating interaction strategies of conversational agents during their development phase. Our approach combines in a novel way methodologies already tested and validated, and is based on 2 Related Works Several frameworks to evalua"
W18-5711,E17-2026,0,0.0426682,"learning (MTL) (Caruana, 1997) in which a joint model is trained on a target (main) task and several auxiliary tasks simultaneously to learn better feature representations across tasks. This technique has shown potential on various NLP tasks and offer flexibility as it allows transfer learning across different domains and tasks (Yang et al., 2017). On slot filling, Jaech et al. (2016) train a single slot filling model on different domains and show that MTL is particulary useful in low resource scenarios. Identifying beneficial auxiliary task for the target task is important when applying MTL (Bingel and Søgaard, 2017). In this work, we investigate the effectiveness of Named Entity Recognition (NER) as an auxiliary task for slot filling. We propose NER because of two main reasons. First, the slot values are typically named entities, for example airline name, city name, etc. Second, the state of the art performance of models for NER have been relatively high (Lample et al., 2016; Ma and Hovy, 2016). Therefore, we expect that the Most of the current dialogue systems depend on an NLU component to extract semantic information from an utterance. Such semantic information is often represented as a semantic frame"
W18-5711,N18-2118,0,0.151332,"ular attribute (an entity, time, etc) of the utterance. Table 1 shows an example of a semantic frame for the sentence ”Show me the prices of all flights from Atlanta to Washington DC” with Begin/In/Out (BIO) representation. We focus on slot filling, a task of automatically extracting slots for a given utterance. This task can be treated as a sequence labeling problem and the most successful approach is to employ a conditional random fields (CRF) on top of a deep recurrent neural networks (RNN). In general, there are two ways of training a slot filling model: (i) train a domain-specific model (Goo et al., 2018; Wang et al., 2018) or (ii) train a model that performs well across domains using domain adaptation or transfer learning techniques (Hakkani-T¨ur 74 Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 74–80 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics ISBN 978-1-948087-75-9 learned features of NER can improve the slot filling performance. Finally, NER corpus is relatively easier to obtain compared to domain specific slot filling datasets. We are interested to answer the following questions: •"
W18-5711,P16-1101,0,0.327313,"a single slot filling model on different domains and show that MTL is particulary useful in low resource scenarios. Identifying beneficial auxiliary task for the target task is important when applying MTL (Bingel and Søgaard, 2017). In this work, we investigate the effectiveness of Named Entity Recognition (NER) as an auxiliary task for slot filling. We propose NER because of two main reasons. First, the slot values are typically named entities, for example airline name, city name, etc. Second, the state of the art performance of models for NER have been relatively high (Lample et al., 2016; Ma and Hovy, 2016). Therefore, we expect that the Most of the current dialogue systems depend on an NLU component to extract semantic information from an utterance. Such semantic information is often represented as a semantic frame which contains the domain, intent of the user, and predefined attributes (slots). Each word of the utterance is labeled with a slot, which defines a particular attribute (an entity, time, etc) of the utterance. Table 1 shows an example of a semantic frame for the sentence ”Show me the prices of all flights from Atlanta to Washington DC” with Begin/In/Out (BIO) representation. We focu"
W18-5711,N18-3019,0,0.17312,"ry task improves slot filling performance and achieve competitive performance compared with state-of-theart. In particular, NER is effective when supervised at the lower layer of the model. For low-resource scenarios, we found that MTL is effective for one dataset. 1 Domain Intent airline search airfare Utterance show me the prices of all flights from Atlanta to Washington DC Slot Label O O O O O O O O B-fromloc.city name O B-toloc.city name I-toloc.city name Table 1: An example of a semantic frame with its coressponding domain, intent and slots. Introduction et al., 2016; Jaech et al., 2016; Jha et al., 2018; Kim et al., 2017). One popular transfer learning technique is multi-task learning (MTL) (Caruana, 1997) in which a joint model is trained on a target (main) task and several auxiliary tasks simultaneously to learn better feature representations across tasks. This technique has shown potential on various NLP tasks and offer flexibility as it allows transfer learning across different domains and tasks (Yang et al., 2017). On slot filling, Jaech et al. (2016) train a single slot filling model on different domains and show that MTL is particulary useful in low resource scenarios. Identifying ben"
W18-5711,P11-1157,0,0.080286,"Missing"
W18-5711,W13-3516,0,0.0797092,"Missing"
W18-5711,P17-1060,0,0.130475,"lot filling performance and achieve competitive performance compared with state-of-theart. In particular, NER is effective when supervised at the lower layer of the model. For low-resource scenarios, we found that MTL is effective for one dataset. 1 Domain Intent airline search airfare Utterance show me the prices of all flights from Atlanta to Washington DC Slot Label O O O O O O O O B-fromloc.city name O B-toloc.city name I-toloc.city name Table 1: An example of a semantic frame with its coressponding domain, intent and slots. Introduction et al., 2016; Jaech et al., 2016; Jha et al., 2018; Kim et al., 2017). One popular transfer learning technique is multi-task learning (MTL) (Caruana, 1997) in which a joint model is trained on a target (main) task and several auxiliary tasks simultaneously to learn better feature representations across tasks. This technique has shown potential on various NLP tasks and offer flexibility as it allows transfer learning across different domains and tasks (Yang et al., 2017). On slot filling, Jaech et al. (2016) train a single slot filling model on different domains and show that MTL is particulary useful in low resource scenarios. Identifying beneficial auxiliary t"
W18-5711,P15-1046,0,0.129865,"lling performance? Inspired by recent work of Søgaard and Goldberg (2016), we investigate the effect of supervising NER on different layers of the model. Our hypothesis is that a more “general” feature is better learned on the lower layer in order to support a task which depends on a more “specific” feature. In addition, we also experiment on crossdomain slot filling models by jointly training slot filling datasets from similar domains using a MTL setup. We explore two techniques to measure similarity between domains: domain similarity by Ruder and Plank (2017a) and label embedding mapping by Kim et al. (2015). We experiment with three datasets from different domains. Our experiments show that for all datasets, using NER as an auxiliary task is beneficial for the slot filling performance. NER is consistently helpful when it is supervised at the lower layer. On the low resource scenario, we found mixed results, in which MTL is only effective for 1 dataset. 2 Slot Filling Model 2.2 Multi-Task Learning One simple technique to perform MTL is by training the target and auxiliary tasks simultaneously. In this setting, the parameters of the model are shared across tasks, pushing the model to learn feature"
W18-5711,D17-1035,0,0.0116136,"orrelation Analysis (CCA). The idea is to construct matrix representation where rows are labels and columns are words in the vocabulary. The cell value in the matrix is the pointwise mutual information (PMI) between the label and the word. After that, we perform rank-k SVD on the matrix and normalized the rows of the matrix. Each row with k dimension of the matrix is the label embedding of a particular label. We use the cosine similarity between two label embedding representations to obtain the nearest neighbor. Implementation. We use the existing BiLSTMCRF sequence tagger implementation from Reimers and Gurevych (2017) for all experiments.2 We use the pre-trained word embedding from (Komninos and Manandhar, 2016). We set the LSTM hidden units to 100. The word and character embeddings dimensions are set to 300 and 30 respectively. We use dropout rate of 0.25. We train the model using the Adam optimizer (Kingma and Ba, 2014) for 25 epochs with early stopping on the target task. For each epoch, we 1 2 76 https://groups.csail.mit.edu/sls/downloads/ https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf Aux. Task Model Target Task SF NER ATIS MIT-R MIT-M Bi-model based (Wang et al., 2018) Slot gated model (Goo et al"
W18-5711,D17-1038,0,0.335018,"NER on the lower layer of the MTL model to the slot filling performance? Inspired by recent work of Søgaard and Goldberg (2016), we investigate the effect of supervising NER on different layers of the model. Our hypothesis is that a more “general” feature is better learned on the lower layer in order to support a task which depends on a more “specific” feature. In addition, we also experiment on crossdomain slot filling models by jointly training slot filling datasets from similar domains using a MTL setup. We explore two techniques to measure similarity between domains: domain similarity by Ruder and Plank (2017a) and label embedding mapping by Kim et al. (2015). We experiment with three datasets from different domains. Our experiments show that for all datasets, using NER as an auxiliary task is beneficial for the slot filling performance. NER is consistently helpful when it is supervised at the lower layer. On the low resource scenario, we found mixed results, in which MTL is only effective for 1 dataset. 2 Slot Filling Model 2.2 Multi-Task Learning One simple technique to perform MTL is by training the target and auxiliary tasks simultaneously. In this setting, the parameters of the model are shar"
W18-5711,N16-1175,0,0.0201101,"s and columns are words in the vocabulary. The cell value in the matrix is the pointwise mutual information (PMI) between the label and the word. After that, we perform rank-k SVD on the matrix and normalized the rows of the matrix. Each row with k dimension of the matrix is the label embedding of a particular label. We use the cosine similarity between two label embedding representations to obtain the nearest neighbor. Implementation. We use the existing BiLSTMCRF sequence tagger implementation from Reimers and Gurevych (2017) for all experiments.2 We use the pre-trained word embedding from (Komninos and Manandhar, 2016). We set the LSTM hidden units to 100. The word and character embeddings dimensions are set to 300 and 30 respectively. We use dropout rate of 0.25. We train the model using the Adam optimizer (Kingma and Ba, 2014) for 25 epochs with early stopping on the target task. For each epoch, we 1 2 76 https://groups.csail.mit.edu/sls/downloads/ https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf Aux. Task Model Target Task SF NER ATIS MIT-R MIT-M Bi-model based (Wang et al., 2018) Slot gated model (Goo et al., 2018) Recurrent Attention (Liu and Lane, 2016) Adversarial(Liu and Lane, 2017a) - - 96.89 95."
W18-5711,N16-1030,0,0.261044,"h et al. (2016) train a single slot filling model on different domains and show that MTL is particulary useful in low resource scenarios. Identifying beneficial auxiliary task for the target task is important when applying MTL (Bingel and Søgaard, 2017). In this work, we investigate the effectiveness of Named Entity Recognition (NER) as an auxiliary task for slot filling. We propose NER because of two main reasons. First, the slot values are typically named entities, for example airline name, city name, etc. Second, the state of the art performance of models for NER have been relatively high (Lample et al., 2016; Ma and Hovy, 2016). Therefore, we expect that the Most of the current dialogue systems depend on an NLU component to extract semantic information from an utterance. Such semantic information is often represented as a semantic frame which contains the domain, intent of the user, and predefined attributes (slots). Each word of the utterance is labeled with a slot, which defines a particular attribute (an entity, time, etc) of the utterance. Table 1 shows an example of a semantic frame for the sentence ”Show me the prices of all flights from Atlanta to Washington DC” with Begin/In/Out (BIO) rep"
W18-5711,P16-2038,0,0.199377,"r to the one proposed by Kim et al. (2016). We then feed xi to a bidirectional LSTM (biLSTM) wordlevel encoder to incorporate the contextual information of wi . The output of the backward and forward LSTM at each time step is then concatenated and fed into a CRF layer. The CRF layer computes the final output, e.g. the tag of each input. We use one hidden layer between biLSTM and CRF as it has been shown by Lample et al. (2016) that it can improve performance. • What is the effect of supervising NER on the lower layer of the MTL model to the slot filling performance? Inspired by recent work of Søgaard and Goldberg (2016), we investigate the effect of supervising NER on different layers of the model. Our hypothesis is that a more “general” feature is better learned on the lower layer in order to support a task which depends on a more “specific” feature. In addition, we also experiment on crossdomain slot filling models by jointly training slot filling datasets from similar domains using a MTL setup. We explore two techniques to measure similarity between domains: domain similarity by Ruder and Plank (2017a) and label embedding mapping by Kim et al. (2015). We experiment with three datasets from different domai"
W18-5711,W03-0419,0,0.379171,"Missing"
W18-5711,N18-2050,0,0.0912738,"entity, time, etc) of the utterance. Table 1 shows an example of a semantic frame for the sentence ”Show me the prices of all flights from Atlanta to Washington DC” with Begin/In/Out (BIO) representation. We focus on slot filling, a task of automatically extracting slots for a given utterance. This task can be treated as a sequence labeling problem and the most successful approach is to employ a conditional random fields (CRF) on top of a deep recurrent neural networks (RNN). In general, there are two ways of training a slot filling model: (i) train a domain-specific model (Goo et al., 2018; Wang et al., 2018) or (ii) train a model that performs well across domains using domain adaptation or transfer learning techniques (Hakkani-T¨ur 74 Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 74–80 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics ISBN 978-1-948087-75-9 learned features of NER can improve the slot filling performance. Finally, NER corpus is relatively easier to obtain compared to domain specific slot filling datasets. We are interested to answer the following questions: • Does NER help the pe"
W19-5807,biggio-etal-2010-entity,0,0.0372032,"ood I O would O like O to O order O a O salami B-FOOD pizza I-FOOD and O two O mozzarella B-FOOD cheese I-FOOD sandwiches I-FOOD Table 1: IOB annotation of food entities inside user request. names, spanish baked salmon, roasted salmon and hot smoked salmon), which makes it possible to combine tokens of one entity name with tokens of another entity name to generate new names (e.g. for food names, salmon tacos is a potential food name given the existence of salmon and tacos). In the framework of the ACE program there have been several attempts to develop supervised systems for nominal entities (Biggio et al., 2010); these systems, however, had to face the problem of the scarcity of annotated data, and, for this reason, were developed for few entity types. In this paper we make use of an end-to-end state-of-art entity recognition system (described in Section 2), and investigate the combination with gazetteers under several integration methods, which are described in Section 3 and 4. Datasets for our experiments are described in Section 5, while results are presented and discussed in Section 6. 2 Core Entity Recognition System In order to investigate the use of gazetteers in combination with neural models"
W19-5807,N16-1031,0,0.0154319,"for entity recognition. We provide experimental evidences on two datasets (named entities and nominal entities) and two languages (English and Italian), showing that extracting features from a rich model of the gazetteer and then concatenating such features with the input embeddings of a neural model is the best strategy in all our experimental settings, significantly outperforming more conventional approaches. 1 Introduction In the recent years a number of neural architectures have been successfully applied to several sequence labelling tasks, including, among others, part-of-speech tagging (Choi, 2016), named entity recognition (Ma and Hovy, 2016), and semantic role labeling (He et al., 2017). It has been shown that these architectures can achieve state-of-art performance with an end-to-end configuration, i.e. without recurring either to linguistic features or to external knowledge sources (e.g. gazetteers). However, experiments have been often conducted over datasets with large amount of training data and in a rather limited spectrum of experimental conditions. Overall, we think that there has not been much discussion about the use of gazetteers together with neural models, and that a deep"
W19-5807,doddington-etal-2004-automatic,0,0.125008,"iscussion about the use of gazetteers together with neural models, and that a deeper investigation is necessary. In this paper we focus on the role of gazetteers for entity recognition. The following are our two main research questions: (i) As neural networks architectures are highly modular, which is the best way to integrate information from gazetteers? (ii) What is the impact of the size of both training data and gazetteers over the performance of a neural model for entity recognition? As mentioned, we focus on entity recognition and refer to the Automatic Content Extraction program - ACE (Doddington et al., 2004). In this context, entity recognition has been approached as a sequence labeling task. Given an utterance U = {t1 , t2 , ..., tn } and a set of entity categories C = {c1 , c2 , ..., cm }, the task is to label the tokens in U that refer to entities belonging to the categories in C. As an example, using the IOB format (Inside-Outside-Beginning, (Ramshaw and Marcus, 1995)), the sentence “I would like to order a salami pizza and two mozzarella cheese sandwiches” could be labeled as shown in Table 1. ACE distinguishes two main entity classes: named entities and nominal entities, and we consider bot"
W19-5807,P15-1033,0,0.0422003,"t character type. At the second layer each input sequence is presented both forwards and backwards to a bidirectional LSTM, whose output allows to capture past and future information. LSTMs (Hochreiter and Schmidhuber, 1997) are variants of recurrent neural networks (RNNs) designed to cope with gradient vanishing problems. The LSTMs hidden state takes information only from the past, knowing nothing about the future. However, for many tasks it is beneficial to have access to both past (left) and future (right) contexts. A possible solution, whose effectiveness has been proven by previous work (Dyer et al., 2015), is provided by bi-directional LSTMs (BLSTM). (Ma and Hovy, 2016) apply a dropout layer on both the input and output vectors of the BLSTM. Finally, the third layer implemented by NeuroNLP2 is a Conditional Random Fields (CRF) based decoder, which considers dependencies between entity labels in their context and then jointly decodes the best chain of labels for a given input sentence. For example, in NER with standard IOB annotation, an I-token can not follow an O, a constraint which is captured by the CFR layer. Conditional Random Fields (Lafferty et al., 2001) offer several advantages over h"
W19-5807,P05-1045,0,0.280919,"tities recognition (NER) tools for frequent categories (i.e. persons as “Barack Obama”, locations as “New York”, and organizations as “IBM”) have been developed for many languages. Several datasets are available for training purposes (e.g. the Conll-2003 datasets (Tjong Kim Sang and De Meulder, 2003)). It has been a common practice of NER systems to make use of gazetteers (i.e. lists of entity names), considering the presence of a token in certain gazetteer as an additional feature for the classifier (see, for instance, the use of the Stanford NER useGazettes parameter for the CRF classifier (Finkel et al., 2005)). Nominal entities, on the other hand, are noun phrase expressions describing an entity. Differently from named entities, nominal entities are typically compositional, as they do allow morphological and syntactic variations (e.g. for food I O would O like O to O order O a O salami B-FOOD pizza I-FOOD and O two O mozzarella B-FOOD cheese I-FOOD sandwiches I-FOOD Table 1: IOB annotation of food entities inside user request. names, spanish baked salmon, roasted salmon and hot smoked salmon), which makes it possible to combine tokens of one entity name with tokens of another entity name to genera"
W19-5807,W18-5036,1,0.854171,"than named entities. The idea is to build a neural classifier trained solely on gazetteers, that classifies a subsequence of tokens on the input sentence as belonging to a certain entity class with a certain confidence. Then we use the output of such classifier as a feature to be integrated within the NeuroNLP2 system. The neural architecture of the entity classifier, the features it uses, and the methodology to generate synthetic negative examples are briefly presented in the following. Architecture of the NNg Classifier. We used the neural gazetteer-based approach (called NNg ) proposed by (Guerini et al., 2018). The NNg classifier is implemented using a multilayer bidirectional LSTM that classifies an input sequence of TRUE (0.75) Softmax Dropout BLSTM 3 BLSTM 2 BLSTM 1 Input layer black and white t-shirt Figure 2: Structure of the neural gazetteer entity recognition (NNg ). The input layer concatenates the features in a single vector. tokens either as an entity of a certain gazetteer or a non-entity, with a degree of confidence, i.e. the system classifies the sequences in number-ofgazetteers plus one (non-entity) different classes. The NNg classifier is based on the system proposed in (Lample et al"
W19-5807,P17-1044,0,0.0255997,"es and nominal entities) and two languages (English and Italian), showing that extracting features from a rich model of the gazetteer and then concatenating such features with the input embeddings of a neural model is the best strategy in all our experimental settings, significantly outperforming more conventional approaches. 1 Introduction In the recent years a number of neural architectures have been successfully applied to several sequence labelling tasks, including, among others, part-of-speech tagging (Choi, 2016), named entity recognition (Ma and Hovy, 2016), and semantic role labeling (He et al., 2017). It has been shown that these architectures can achieve state-of-art performance with an end-to-end configuration, i.e. without recurring either to linguistic features or to external knowledge sources (e.g. gazetteers). However, experiments have been often conducted over datasets with large amount of training data and in a rather limited spectrum of experimental conditions. Overall, we think that there has not been much discussion about the use of gazetteers together with neural models, and that a deeper investigation is necessary. In this paper we focus on the role of gazetteers for entity r"
W19-5807,N16-1030,0,0.100816,"t al., 2018). The NNg classifier is implemented using a multilayer bidirectional LSTM that classifies an input sequence of TRUE (0.75) Softmax Dropout BLSTM 3 BLSTM 2 BLSTM 1 Input layer black and white t-shirt Figure 2: Structure of the neural gazetteer entity recognition (NNg ). The input layer concatenates the features in a single vector. tokens either as an entity of a certain gazetteer or a non-entity, with a degree of confidence, i.e. the system classifies the sequences in number-ofgazetteers plus one (non-entity) different classes. The NNg classifier is based on the system proposed in (Lample et al., 2016). The core is still a bidirectional LSTM, but it has a 3-layer BLSTM with 128 units per layer and with a single dropout layer (with a dropout probability of 0.5) between the third BLSTM and the output layer (a softmax layer). The topology of the network is depicted in Figure 2. NNg Classifier Features. The NNg classifier combines several features: word embeddings, char-based embedding, and nine handcrafted features. Word embeddings are similar to those used for NeuroNLP2. For English we used Stanfords publicly available 50-dimensions embeddings, while for Italian we use 50-dimensional embeddin"
W19-5807,P16-1101,0,0.520102,"rimental evidences on two datasets (named entities and nominal entities) and two languages (English and Italian), showing that extracting features from a rich model of the gazetteer and then concatenating such features with the input embeddings of a neural model is the best strategy in all our experimental settings, significantly outperforming more conventional approaches. 1 Introduction In the recent years a number of neural architectures have been successfully applied to several sequence labelling tasks, including, among others, part-of-speech tagging (Choi, 2016), named entity recognition (Ma and Hovy, 2016), and semantic role labeling (He et al., 2017). It has been shown that these architectures can achieve state-of-art performance with an end-to-end configuration, i.e. without recurring either to linguistic features or to external knowledge sources (e.g. gazetteers). However, experiments have been often conducted over datasets with large amount of training data and in a rather limited spectrum of experimental conditions. Overall, we think that there has not been much discussion about the use of gazetteers together with neural models, and that a deeper investigation is necessary. In this paper w"
W19-5807,N18-1202,0,0.0305308,"icant when either the size of the training data or of the gazetteers are reduced. As a general comment on the use of gazetteers for neural NER, our experiments highlight that gazetteers are much more useful for nominal entities (e.g. food names) than for named entities (e.g. person names). In this respect, the paper shows that the NNg approach significantly helps to identifying compositional variants of nominal entities. In the paper we have based our experiments on the neural model described in (Ma and Hovy, 2016). However, very recently, new models (e.g. BERT (Devlin et al., 2018) and ELMO (Peters et al., 2018)) have been proposed, which have further improved performance on Named Entity Recognition. Based on such expectations, we run a preliminary experiment using the multilingual BERT model on the DPD dataset: results, probably due to the poor performance of the model on Italian food, are still significantly lower than NeuroNLP2, and additional work seems to be necessary to properly take advantage of the full capacity of the BERT model. Finally, as for future work, we intend to apply the current models to a larger number of scenarios, including utterance understanding for conversational agents. Ref"
W19-5807,W95-0107,0,0.121121,"the impact of the size of both training data and gazetteers over the performance of a neural model for entity recognition? As mentioned, we focus on entity recognition and refer to the Automatic Content Extraction program - ACE (Doddington et al., 2004). In this context, entity recognition has been approached as a sequence labeling task. Given an utterance U = {t1 , t2 , ..., tn } and a set of entity categories C = {c1 , c2 , ..., cm }, the task is to label the tokens in U that refer to entities belonging to the categories in C. As an example, using the IOB format (Inside-Outside-Beginning, (Ramshaw and Marcus, 1995)), the sentence “I would like to order a salami pizza and two mozzarella cheese sandwiches” could be labeled as shown in Table 1. ACE distinguishes two main entity classes: named entities and nominal entities, and we consider both of them for our experiments. The first entity class, named entities, roughly corresponds to proper names, and named entities recognition (NER) tools for frequent categories (i.e. persons as “Barack Obama”, locations as “New York”, and organizations as “IBM”) have been developed for many languages. Several datasets are available for training purposes (e.g. the Conll-2"
W19-5807,W03-0419,0,0.740374,"Missing"
W19-5807,N16-1032,0,0.190661,"per focuses on extracting location names from informal and unstructured texts by identifying referent boundaries. The core of the approach is a statistical language model consisting of a probability distribution over sequences of words (collocations) that represent location names in gazetteers. The algorithm uses the relative likelihood of an observed word sequence to decide the boundaries of a location name in tweets. This is similar in spirit to the NNg used in our approach, although we make use of a neural model rather than a statistical model. A second work that it is worth to mention is (Yang et al., 2016), which addresses the problem of using gazetteers when training a neural network with few data. In fact, particularly for massive data scenarios like NER on Twitter, collecting a large amount of high quality gazetteers can alleviate the problem of training data scarcity. The paper shows that large gazetteers may cause a sideeffect called ‘feature under-training’, i.e. gazetteer features overwhelm the training data and may degrade performance. To solve this problem, the authors propose a dropout conditional random fields, which decreases the influence of gazetteer features with a high weight. 8"
W19-5911,N18-2118,0,0.0268747,"tion Language understanding in task-oriented dialogue systems involves recognizing information (i.e., slot filling) expressed in an utterance to accomplish a particular dialogue task. For example, in a flight booking scenario, the utterance ”show me all Delta flights from Milan to New York” contains information belonging to slots in the flight domain, namely airline name (Delta), origin (Milan), and destination (New York). Slots are usually predefined and domain-specific, e.g. in a hotel domain slots can be different, such as room type, length of stay etc. Although recent neural based models (Goo et al., 2018; Wang et al., 2018; Liu and Lane, 2016) have shown remarkable performance in slot filling, they are still based on large labeled data, which means that training a separate model for each domain involves a resource intensive process. Thus, as more domains are added to the system, methods that can 85 Proceedings of the SIGDial 2019 Conference, pages 85–91 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics Sentence what is the most expensive flight from boston to dallas ATIS Slot O O O B-COST REL I-COST REL O O B-FROM LOC O B-TO LOC NER O O O O O O O B-GPE"
W19-5911,N18-3018,0,0.0129056,"r slouvan@fbk.eu Bernardo Magnini Fondazione Bruno Kessler magnini@fbk.eu Abstract generalize slot filling to new domains with limited labeled data (i.e., low-resource settings) are preferable. Existing works in low resource slot filling are mostly based on transfer learning (Mou et al., 2016), whose aim is to leverage relatively large resources in a source domain (DS ) for a source task (TS ), to help a task (TT ) in a target domain (DT ), where less data are available. Depending on how the adaptation is performed, there are two notable approaches: data-driven adaptation (Jaech et al., 2016; Goyal et al., 2018; Kim et al., 2016), and model-driven adaptation (Kim et al., 2017; Jha et al., 2018). Essentially, both approaches produce a model on the target domain performing training on the same task (slot filling, in our case), i.e., assuming (TS = TT ), although from different domains, i.e. (DS 6= DT ). All of these approaches assume that slot filling datasets for the source domain are available, and little effort has been devoted in finding and exploiting cheaper TS , which is crucial in a situation where a slot filling dataset in DS is not ready yet (cold-start). Accordingly, we attempt to leverage"
W19-5911,P16-1101,0,0.0421057,"(Pradhan et al., 2013) and Parallel Meaning Bank (PMB) (Abzianidze et al., 2017) respectively. This is beneficial in a cold-start situation in which no slot filling dataset is already available in DS . 2 Approach Slot filling is often modeled as a sequence labeling problem. Given a sequence of words x = (x1 , x2 , ..., xn ) as input, a model M predicts the corresponding slot labels y = (y1 , y2 , ..., yn ) as output. 2.1 Base Model State-of-the-art models on sequence labeling are typically built based on bi-directional LSTM (biLSTM), on top of which there is a CRF model (Lample et al., 2016; Ma and Hovy, 2016). The bi-LSTM takes x as input and each word xi is represented as an embedding ei = [wi ; ci ] composed of the concatenation of a word embedding wi and character embeddings ci . The bi-LSTM → − layer produces the forward output state hi and the ← − backward output state hi . The concatenation of → − ← − the output states, hi = [ hi ; hi ], is then fed to a 86 feed-forward (FF) layer, followed by a CRF as the final output layer that predicts a slot label yi by taking into account the mixture of context information captured by the last FF layer and the slot prediction yi−1 from the previous word"
W19-5911,N18-1054,0,0.0214339,"we focus on using NER and SemTag, our study has shed light on the potential use of non-conversational tasks in general to help low resource slot filling. tic formalism. In this work, we leverage non-conversational tasks as auxiliary tasks in a multi-task learning (MTL) (Caruana, 1997) setup. Given appropriate auxiliary tasks, MTL has shown to be particularly effective in which labeled data is scarce and has been applied to various NLP tasks such as parsing (Søgaard and Goldberg, 2016), POS tagging (Yang et al., 2016), neural machine translation (Luong et al., 2016), and opinion role labeling (Marasovic and Frank, 2018). While there are potentially many non-conversational tasks that we can use as auxiliary tasks, we focus on those that assign semantic class categories to a word, as they are similar in nature to slot filling. In particular, in this work we choose Named Entity Recognition (NER) and the recently introduced Semantic Tagging (SemTag) (Abzianidze and Bos, 2017), motivated by the following rationales: • Both NER and SemTag are semantically related to slot filling. As illustrated in Table 1, slot labels may correspond to either NER or SemTag labels. In addition, SemTag complements NER as its labels"
W19-5911,N18-3019,0,0.195686,"eralize slot filling to new domains with limited labeled data (i.e., low-resource settings) are preferable. Existing works in low resource slot filling are mostly based on transfer learning (Mou et al., 2016), whose aim is to leverage relatively large resources in a source domain (DS ) for a source task (TS ), to help a task (TT ) in a target domain (DT ), where less data are available. Depending on how the adaptation is performed, there are two notable approaches: data-driven adaptation (Jaech et al., 2016; Goyal et al., 2018; Kim et al., 2016), and model-driven adaptation (Kim et al., 2017; Jha et al., 2018). Essentially, both approaches produce a model on the target domain performing training on the same task (slot filling, in our case), i.e., assuming (TS = TT ), although from different domains, i.e. (DS 6= DT ). All of these approaches assume that slot filling datasets for the source domain are available, and little effort has been devoted in finding and exploiting cheaper TS , which is crucial in a situation where a slot filling dataset in DS is not ready yet (cold-start). Accordingly, we attempt to leverage nonconversational source tasks (TS 6= TT ) i.e., tasks that use widely available non-"
W19-5911,P17-1060,0,0.0704959,"bk.eu Abstract generalize slot filling to new domains with limited labeled data (i.e., low-resource settings) are preferable. Existing works in low resource slot filling are mostly based on transfer learning (Mou et al., 2016), whose aim is to leverage relatively large resources in a source domain (DS ) for a source task (TS ), to help a task (TT ) in a target domain (DT ), where less data are available. Depending on how the adaptation is performed, there are two notable approaches: data-driven adaptation (Jaech et al., 2016; Goyal et al., 2018; Kim et al., 2016), and model-driven adaptation (Kim et al., 2017; Jha et al., 2018). Essentially, both approaches produce a model on the target domain performing training on the same task (slot filling, in our case), i.e., assuming (TS = TT ), although from different domains, i.e. (DS 6= DT ). All of these approaches assume that slot filling datasets for the source domain are available, and little effort has been devoted in finding and exploiting cheaper TS , which is crucial in a situation where a slot filling dataset in DS is not ready yet (cold-start). Accordingly, we attempt to leverage nonconversational source tasks (TS 6= TT ) i.e., tasks that use wi"
W19-5911,C16-1193,0,0.0303697,"nardo Magnini Fondazione Bruno Kessler magnini@fbk.eu Abstract generalize slot filling to new domains with limited labeled data (i.e., low-resource settings) are preferable. Existing works in low resource slot filling are mostly based on transfer learning (Mou et al., 2016), whose aim is to leverage relatively large resources in a source domain (DS ) for a source task (TS ), to help a task (TT ) in a target domain (DT ), where less data are available. Depending on how the adaptation is performed, there are two notable approaches: data-driven adaptation (Jaech et al., 2016; Goyal et al., 2018; Kim et al., 2016), and model-driven adaptation (Kim et al., 2017; Jha et al., 2018). Essentially, both approaches produce a model on the target domain performing training on the same task (slot filling, in our case), i.e., assuming (TS = TT ), although from different domains, i.e. (DS 6= DT ). All of these approaches assume that slot filling datasets for the source domain are available, and little effort has been devoted in finding and exploiting cheaper TS , which is crucial in a situation where a slot filling dataset in DS is not ready yet (cold-start). Accordingly, we attempt to leverage nonconversational s"
W19-5911,D16-1046,0,0.0756897,"Missing"
W19-5911,J05-1004,0,0.0200283,"little effort has been devoted in finding and exploiting cheaper TS , which is crucial in a situation where a slot filling dataset in DS is not ready yet (cold-start). Accordingly, we attempt to leverage nonconversational source tasks (TS 6= TT ) i.e., tasks that use widely available non-conversational resources, to help slot filling. These resources are cheaper to obtain compared to domain-specific slot filling datasets, and many of them are annotated with rich linguistic knowledge, which is potentially useful for slot filling (Chen et al., 2016). Among these resources, we mention PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), which consist of annotated documents with verb and frame-based semantic roles, respectively; CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and OntoNotes (Pradhan et al., 2013), which provide named entity information; and Abstract Meaning Representation (AMR) (Banarescu et al., 2013), which provides a graph-based semanSlot filling is a core operation for utterance understanding in task-oriented dialogue systems. Slots are typically domain-specific, and adding new domains to a dialogue system involves data and time-intensive processes. A popular technique t"
W19-5911,N16-1030,0,0.0111224,"for example OntoNotes (Pradhan et al., 2013) and Parallel Meaning Bank (PMB) (Abzianidze et al., 2017) respectively. This is beneficial in a cold-start situation in which no slot filling dataset is already available in DS . 2 Approach Slot filling is often modeled as a sequence labeling problem. Given a sequence of words x = (x1 , x2 , ..., xn ) as input, a model M predicts the corresponding slot labels y = (y1 , y2 , ..., yn ) as output. 2.1 Base Model State-of-the-art models on sequence labeling are typically built based on bi-directional LSTM (biLSTM), on top of which there is a CRF model (Lample et al., 2016; Ma and Hovy, 2016). The bi-LSTM takes x as input and each word xi is represented as an embedding ei = [wi ; ci ] composed of the concatenation of a word embedding wi and character embeddings ci . The bi-LSTM → − layer produces the forward output state hi and the ← − backward output state hi . The concatenation of → − ← − the output states, hi = [ hi ; hi ], is then fed to a 86 feed-forward (FF) layer, followed by a CRF as the final output layer that predicts a slot label yi by taking into account the mixture of context information captured by the last FF layer and the slot prediction yi−1 fr"
W19-5911,W13-3516,0,0.156136,"Missing"
W19-5911,W12-4501,0,0.0896358,"Missing"
W19-5911,H90-1020,0,0.534457,"Missing"
W19-5911,W95-0107,0,0.391217,"ocess. Thus, as more domains are added to the system, methods that can 85 Proceedings of the SIGDial 2019 Conference, pages 85–91 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics Sentence what is the most expensive flight from boston to dallas ATIS Slot O O O B-COST REL I-COST REL O O B-FROM LOC O B-TO LOC NER O O O O O O O B-GPE O B-GPE B-QUE B-ENS B-DEF B-TOP B-IST B-CON B-REL B-GPE O B-GPE SemTag Table 1: An example of slot filling annotation from the ATIS (Airline Travel Information System) dataset and author-annotated NER and SemTag in IOB format (Ramshaw and Marcus, 1995). Some ATIS slots correspond to NER or SemTag labels, such as FROM LOC and TO LOC with GPE in NER and SemTag. Some slot tags can also be composed of several SemTag labels such as COST REL which is composed of TOP (superlative positive) and IST (intersective adjective). Although NER has been already used in slot filling models, most of these approaches (Mesnil et al., 2013, 2015; Zhang and Wang, 2016; Gong et al., 2019; Louvan and Magnini, 2018) use and incorporate ground truth NER labels or output of NER systems as features to train a slot filling model, our work differs in the method of learn"
W19-5911,P16-2038,0,0.308155,"re. (ii) We show that MTL models with NER and SemTag strongly improve single-task slot filling models on three well known datasets. While we focus on using NER and SemTag, our study has shed light on the potential use of non-conversational tasks in general to help low resource slot filling. tic formalism. In this work, we leverage non-conversational tasks as auxiliary tasks in a multi-task learning (MTL) (Caruana, 1997) setup. Given appropriate auxiliary tasks, MTL has shown to be particularly effective in which labeled data is scarce and has been applied to various NLP tasks such as parsing (Søgaard and Goldberg, 2016), POS tagging (Yang et al., 2016), neural machine translation (Luong et al., 2016), and opinion role labeling (Marasovic and Frank, 2018). While there are potentially many non-conversational tasks that we can use as auxiliary tasks, we focus on those that assign semantic class categories to a word, as they are similar in nature to slot filling. In particular, in this work we choose Named Entity Recognition (NER) and the recently introduced Semantic Tagging (SemTag) (Abzianidze and Bos, 2017), motivated by the following rationales: • Both NER and SemTag are semantically related to slot filling."
W19-5911,tiedemann-2012-parallel,0,0.041304,"Missing"
W19-5911,W03-0419,0,0.454414,"Missing"
W19-5911,N18-2050,0,0.267547,"rstanding in task-oriented dialogue systems involves recognizing information (i.e., slot filling) expressed in an utterance to accomplish a particular dialogue task. For example, in a flight booking scenario, the utterance ”show me all Delta flights from Milan to New York” contains information belonging to slots in the flight domain, namely airline name (Delta), origin (Milan), and destination (New York). Slots are usually predefined and domain-specific, e.g. in a hotel domain slots can be different, such as room type, length of stay etc. Although recent neural based models (Goo et al., 2018; Wang et al., 2018; Liu and Lane, 2016) have shown remarkable performance in slot filling, they are still based on large labeled data, which means that training a separate model for each domain involves a resource intensive process. Thus, as more domains are added to the system, methods that can 85 Proceedings of the SIGDial 2019 Conference, pages 85–91 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics Sentence what is the most expensive flight from boston to dallas ATIS Slot O O O B-COST REL I-COST REL O O B-FROM LOC O B-TO LOC NER O O O O O O O B-GPE O B-GPE B-QUE B-ENS"
W97-0805,J96-4006,0,0.0245649,"Missing"
