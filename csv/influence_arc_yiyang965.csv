2020.acl-main.78,K16-1002,0,0.0407476,"Missing"
2020.acl-main.78,P19-1590,0,0.0209218,"layer. The sigmoid and tanh are two activation functions. We model the Decoder by the LSTM network. Leveraging Unlabeled Financial Documents. Various machine learning models, including SVM (Cesa-Bianchi et al., 2006), representation learning (Dai and Le, 2015), and adversarial training (Miyato et al., 2017), have been used to solve the semi-supervised text classification. Recently, VAEbased methods have been successfully used in semisupervised learning and utilize unlabeled data to model the generating process of underlying data (Kingma and Welling, 2014; Miao et al., 2016; Xie and Ma, 2019; Gururangan et al., 2019). In addition, previous work (Xu et al., 2017) proposes to incorporate labels into the decoder RNN for better text classification performance. In this work, we use the semi-supervised variational autoencoder (SemiVAE) (Kingma et al., 2014; Yang et al., 2019) to exploit these data, which ˆ i |y i , zi )] log pθ (Di , y i ) ≥Eqφ (zi |Di ,yi ) [log pθ (D + log pθ (y i ) − KL [qφ (zi |Di , y i )||p(zi )] = −L(Dl ), (10) where KL [qφ (zi |Di , y i )||p(zi )] is the KL divergence between the latent posterior qφ (zi |Di , y i ) and the prior distribution p(zi ) that should be minimized. Note that we"
2020.acl-main.78,D19-1154,0,0.0232378,"Missing"
2020.acl-main.78,N09-1031,0,0.0323269,". SemiVAE allows effective learning of latent representation from both labeled and unlabeled data, and multi-head attention mechanism produces the direct visualization of informative words associated with multi-label predictive outcomes. Learning the model parameters is effective and scalable under the variational inference method. This paper contributes to the burgeoning body of research on using NLP techniques in key financial applications. For example, the prior study leverages the textual features in firm annual reports to predict a firm’s stock price volatility using firm annual reports (Kogan et al., 2009) and earnings announcement transcripts (Qin and Yang, 2019). Other researches make use of news articles and social media data to predict financial markets variables, such as stock return, firm performance, default prediction and market sentiment (Tetlock, 2007; Schumaker and Chen, 2009; Ding et al., 2015; Luo et al., 2018). It is worth emphasizing that the pre-requisites of using NLP in key financial applications are effective and transparent. In many cases, it requires extensive domain expertise to annotate the variable of interests. Moreover, 846 Proceedings of the 58th Annual Meeting of the"
2020.acl-main.78,P19-1038,1,0.84538,"n from both labeled and unlabeled data, and multi-head attention mechanism produces the direct visualization of informative words associated with multi-label predictive outcomes. Learning the model parameters is effective and scalable under the variational inference method. This paper contributes to the burgeoning body of research on using NLP techniques in key financial applications. For example, the prior study leverages the textual features in firm annual reports to predict a firm’s stock price volatility using firm annual reports (Kogan et al., 2009) and earnings announcement transcripts (Qin and Yang, 2019). Other researches make use of news articles and social media data to predict financial markets variables, such as stock return, firm performance, default prediction and market sentiment (Tetlock, 2007; Schumaker and Chen, 2009; Ding et al., 2015; Luo et al., 2018). It is worth emphasizing that the pre-requisites of using NLP in key financial applications are effective and transparent. In many cases, it requires extensive domain expertise to annotate the variable of interests. Moreover, 846 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 846–852 c"
2020.acl-main.78,N19-1325,0,0.0255236,"nd adversarial training (Miyato et al., 2017), have been used to solve the semi-supervised text classification. Recently, VAEbased methods have been successfully used in semisupervised learning and utilize unlabeled data to model the generating process of underlying data (Kingma and Welling, 2014; Miao et al., 2016; Xie and Ma, 2019; Gururangan et al., 2019). In addition, previous work (Xu et al., 2017) proposes to incorporate labels into the decoder RNN for better text classification performance. In this work, we use the semi-supervised variational autoencoder (SemiVAE) (Kingma et al., 2014; Yang et al., 2019) to exploit these data, which ˆ i |y i , zi )] log pθ (Di , y i ) ≥Eqφ (zi |Di ,yi ) [log pθ (D + log pθ (y i ) − KL [qφ (zi |Di , y i )||p(zi )] = −L(Dl ), (10) where KL [qφ (zi |Di , y i )||p(zi )] is the KL divergence between the latent posterior qφ (zi |Di , y i ) and the prior distribution p(zi ) that should be minimized. Note that we utilize the KL cost annealing method (Bowman et al., 2016; Sønderby et al., 2016) to smooth the training process by gradually increasing the weight β of KL cost from 0 to 1. In the case of each unlabeled document Di ∈ Du , the corresponding risks yˆi are pre"
2020.acl-main.78,C18-1330,0,0.0275398,"abel information, we propose a useful way to encode labels into low dimensional vectors in the training process. We first get label embedding matrix Ei as follows:  Linear(y i ), if Di ∈ Dl Ei = (3) Classifier(Di ), if Di ∈ Du where Ei ∈ Rd×Li and Li is the number of y i . y i are the observed operational risks of i-th document, and the Linear is a fully-connected layer. The Classifier is a semi-supervised classifier, which can predict risk labels and learn the corresponding label embedding based on both labeled and unlabeled document representation. Inspired by prior work (Rai et al., 2015; Yang et al., 2018; Wang et al., 2018), we incorporate two final states fi and mi into label embedding Ei through another BiLSTM, which is beneficial to learn the specific label embedding of i-th document: ˆ i = Bi-LSTM(Ei , (fi , mi )), E (4) ˆ i ∈ R2d×Li . where E Multi-head Attention. The document vector usually involves rich semantics in multiple semantic spaces. However, the traditional attention mechanisms only focus on a specific semantic space of document representation to learn the weights of 847 words, which ignores the influence of other semantic spaces. In our work, we utilize the multi-head attenti"
2020.acl-main.78,P18-1216,0,0.0286192,"e propose a useful way to encode labels into low dimensional vectors in the training process. We first get label embedding matrix Ei as follows:  Linear(y i ), if Di ∈ Dl Ei = (3) Classifier(Di ), if Di ∈ Du where Ei ∈ Rd×Li and Li is the number of y i . y i are the observed operational risks of i-th document, and the Linear is a fully-connected layer. The Classifier is a semi-supervised classifier, which can predict risk labels and learn the corresponding label embedding based on both labeled and unlabeled document representation. Inspired by prior work (Rai et al., 2015; Yang et al., 2018; Wang et al., 2018), we incorporate two final states fi and mi into label embedding Ei through another BiLSTM, which is beneficial to learn the specific label embedding of i-th document: ˆ i = Bi-LSTM(Ei , (fi , mi )), E (4) ˆ i ∈ R2d×Li . where E Multi-head Attention. The document vector usually involves rich semantics in multiple semantic spaces. However, the traditional attention mechanisms only focus on a specific semantic space of document representation to learn the weights of 847 words, which ignores the influence of other semantic spaces. In our work, we utilize the multi-head attention mechanism (Vaswan"
2020.acl-main.79,P17-1116,0,0.013809,"“black-box” GNN-based models by investigating the effect of individual nodes. 1 Introduction Identifying geographic locations of users in online social networks (OSN) has become a key Internet service for many downstream applications, including location-based targeted advertising, emergency location identification, political election campaign, local event/place recommendation, natural disaster response, and remediation, etc. (Zheng et al., 2018). As such, the problem of user geolocation (UG) has received a great deal of research attention in the past decade (Han et al., 2012; Do et al., 2017; Miura et al., 2017; Rahimi et al., 2018; Bakerman et al., 2018). Earlier efforts (Amitay et al., 2004; Wing and Baldridge, 2011; Han et al., 2012; Roller et al., 2012; Ahmed et al., 2013; Han et al., 2014; Chong and Lim, 2017) mainly focused on extracting indicative information from user-posted contents. These approaches rely on informative words that can link users to their specific locations via various natural language processing techniques such as topic modeling and other statistical models. More recently, researchers aimed at incorporating multi-aspect information, especially the user mention/interaction n"
2020.acl-main.79,D10-1124,0,0.117294,"Missing"
2020.acl-main.79,C12-1064,0,0.0354399,"n attempt to uncover the so-called “black-box” GNN-based models by investigating the effect of individual nodes. 1 Introduction Identifying geographic locations of users in online social networks (OSN) has become a key Internet service for many downstream applications, including location-based targeted advertising, emergency location identification, political election campaign, local event/place recommendation, natural disaster response, and remediation, etc. (Zheng et al., 2018). As such, the problem of user geolocation (UG) has received a great deal of research attention in the past decade (Han et al., 2012; Do et al., 2017; Miura et al., 2017; Rahimi et al., 2018; Bakerman et al., 2018). Earlier efforts (Amitay et al., 2004; Wing and Baldridge, 2011; Han et al., 2012; Roller et al., 2012; Ahmed et al., 2013; Han et al., 2014; Chong and Lim, 2017) mainly focused on extracting indicative information from user-posted contents. These approaches rely on informative words that can link users to their specific locations via various natural language processing techniques such as topic modeling and other statistical models. More recently, researchers aimed at incorporating multi-aspect information, espe"
2020.acl-main.79,D19-1480,0,0.0346222,"identification (Rahimi et al., 2015; Do et al., 2017; Rahimi et al., 2017, 2018; Hamouni et al., 2019). For example, (Rahimi et al., 2018; Wu et al., 2019) employ Graph Convolutional Networks (GCNs) (Kipf and Welling, 2017) or simplified GCN (Wu et al., 2019) to learn network structures for user geolocation. In addition, graph representation-based methods (Tang et al., 2015; Grover and Leskovec, 2016; Kipf and Welling, 2017; Hamilton et al., 2017; Qiu et al., 2018) have also been widely used for user geolocation (Do et al., 2017; Miura et al., 2017; Rahimi et al., 2018; Hamouni et al., 2019; Huang and Carley, 2019). However, the existing methods lack model transparency and fail to provide meaningful explanations regarding the model behavior and prediction results, which prevents them from safety-critical applications. For example, when locating an emergency for a specific region, it would be more meaningful to explain why such prediction is made, rather than simply presenting numerical ranking values. To address such limitations, we propose a general framework to explain the behavior of user geolocation models and the prediction results, by utilizing the influence function (Hampel et al., 2011; Koh and"
2020.acl-main.79,P15-2104,0,0.200527,"(Amitay et al., 2004; Wing and Baldridge, 2011; Han et al., 2012; Roller et al., 2012; Ahmed et al., 2013; Han et al., 2014; Chong and Lim, 2017) mainly focused on extracting indicative information from user-posted contents. These approaches rely on informative words that can link users to their specific locations via various natural language processing techniques such as topic modeling and other statistical models. More recently, researchers aimed at incorporating multi-aspect information, especially the user mention/interaction network to boost the performance of geolocation identification (Rahimi et al., 2015; Do et al., 2017; Rahimi et al., 2017, 2018; Hamouni et al., 2019). For example, (Rahimi et al., 2018; Wu et al., 2019) employ Graph Convolutional Networks (GCNs) (Kipf and Welling, 2017) or simplified GCN (Wu et al., 2019) to learn network structures for user geolocation. In addition, graph representation-based methods (Tang et al., 2015; Grover and Leskovec, 2016; Kipf and Welling, 2017; Hamilton et al., 2017; Qiu et al., 2018) have also been widely used for user geolocation (Do et al., 2017; Miura et al., 2017; Rahimi et al., 2018; Hamouni et al., 2019; Huang and Carley, 2019). However, th"
2020.acl-main.79,P17-2033,0,0.0365831,"ge, 2011; Han et al., 2012; Roller et al., 2012; Ahmed et al., 2013; Han et al., 2014; Chong and Lim, 2017) mainly focused on extracting indicative information from user-posted contents. These approaches rely on informative words that can link users to their specific locations via various natural language processing techniques such as topic modeling and other statistical models. More recently, researchers aimed at incorporating multi-aspect information, especially the user mention/interaction network to boost the performance of geolocation identification (Rahimi et al., 2015; Do et al., 2017; Rahimi et al., 2017, 2018; Hamouni et al., 2019). For example, (Rahimi et al., 2018; Wu et al., 2019) employ Graph Convolutional Networks (GCNs) (Kipf and Welling, 2017) or simplified GCN (Wu et al., 2019) to learn network structures for user geolocation. In addition, graph representation-based methods (Tang et al., 2015; Grover and Leskovec, 2016; Kipf and Welling, 2017; Hamilton et al., 2017; Qiu et al., 2018) have also been widely used for user geolocation (Do et al., 2017; Miura et al., 2017; Rahimi et al., 2018; Hamouni et al., 2019; Huang and Carley, 2019). However, the existing methods lack model transpar"
2020.acl-main.79,P18-1187,0,0.203549,"d models by investigating the effect of individual nodes. 1 Introduction Identifying geographic locations of users in online social networks (OSN) has become a key Internet service for many downstream applications, including location-based targeted advertising, emergency location identification, political election campaign, local event/place recommendation, natural disaster response, and remediation, etc. (Zheng et al., 2018). As such, the problem of user geolocation (UG) has received a great deal of research attention in the past decade (Han et al., 2012; Do et al., 2017; Miura et al., 2017; Rahimi et al., 2018; Bakerman et al., 2018). Earlier efforts (Amitay et al., 2004; Wing and Baldridge, 2011; Han et al., 2012; Roller et al., 2012; Ahmed et al., 2013; Han et al., 2014; Chong and Lim, 2017) mainly focused on extracting indicative information from user-posted contents. These approaches rely on informative words that can link users to their specific locations via various natural language processing techniques such as topic modeling and other statistical models. More recently, researchers aimed at incorporating multi-aspect information, especially the user mention/interaction network to boost the p"
2020.acl-main.79,D12-1137,0,0.111295,"cial networks (OSN) has become a key Internet service for many downstream applications, including location-based targeted advertising, emergency location identification, political election campaign, local event/place recommendation, natural disaster response, and remediation, etc. (Zheng et al., 2018). As such, the problem of user geolocation (UG) has received a great deal of research attention in the past decade (Han et al., 2012; Do et al., 2017; Miura et al., 2017; Rahimi et al., 2018; Bakerman et al., 2018). Earlier efforts (Amitay et al., 2004; Wing and Baldridge, 2011; Han et al., 2012; Roller et al., 2012; Ahmed et al., 2013; Han et al., 2014; Chong and Lim, 2017) mainly focused on extracting indicative information from user-posted contents. These approaches rely on informative words that can link users to their specific locations via various natural language processing techniques such as topic modeling and other statistical models. More recently, researchers aimed at incorporating multi-aspect information, especially the user mention/interaction network to boost the performance of geolocation identification (Rahimi et al., 2015; Do et al., 2017; Rahimi et al., 2017, 2018; Hamouni et al., 2019"
2020.acl-main.79,P11-1096,0,0.0307162,"g geographic locations of users in online social networks (OSN) has become a key Internet service for many downstream applications, including location-based targeted advertising, emergency location identification, political election campaign, local event/place recommendation, natural disaster response, and remediation, etc. (Zheng et al., 2018). As such, the problem of user geolocation (UG) has received a great deal of research attention in the past decade (Han et al., 2012; Do et al., 2017; Miura et al., 2017; Rahimi et al., 2018; Bakerman et al., 2018). Earlier efforts (Amitay et al., 2004; Wing and Baldridge, 2011; Han et al., 2012; Roller et al., 2012; Ahmed et al., 2013; Han et al., 2014; Chong and Lim, 2017) mainly focused on extracting indicative information from user-posted contents. These approaches rely on informative words that can link users to their specific locations via various natural language processing techniques such as topic modeling and other statistical models. More recently, researchers aimed at incorporating multi-aspect information, especially the user mention/interaction network to boost the performance of geolocation identification (Rahimi et al., 2015; Do et al., 2017; Rahimi e"
2020.coling-main.541,C18-1239,0,0.0182473,"anguage models in the domain of FinTech. Exhaustive quantitative experiments demonstrate that not only does this approach improve the model accuracy when compared to the current stateof-the-art and human performance, but it also generates counterfactual explanations which are significantly more plausible based on human trials. 1 Introduction and Related Work In recent years, large-scale, pre-trained transformer models have led to massive improvements on a wide range of natural language processing (NLP) tasks (Devlin et al., 2018; Liu et al., 2019), including financial technology applications (Duan et al., 2018; Yang et al., 2018; Xing et al., 2019; Yang et al., 2020). However, this impressive ability also coincides with an inherent lack of robustness and transparency, which undermines human trust in the prediction outcome. In the highly sensitive (and financially lucrative) area of FinTech, explainable financial text classification remains an open, and highly alluring question. To tackle this problem, this paper advances a novel approach which first applies robust transformer models (by leveraging adversarial training) on a real-world, up-to-date, self-collected mergers and acquisitions (M&A) datas"
2020.coling-main.541,2020.lrec-1.220,0,0.0302879,"re typically focuses on predicting likely M&A acquirers and targets, in this work we address a distinct but related task: namely, whether a merger and acquisition rumor is likely going to prove to be correct. 1.2 Visualization-based Explanations To interpret a model’s prediction, prior efforts have focused on either incorporating pre-hoc analysis into the experimental design (Brunner et al., 2020), or developing post-hoc analysis algorithms to select or modify particular instances of the dataset to explain the behavior of models (Keane and Smyth, 2020; Kenny and Keane, 2019). Recent research (Grimsley et al., 2020) shows that transformer models can not be perfectly explained from their intrinsic architecture, and a further work (Brunner et al., 2020) provides strong evidence that self-attention distributions are not directly interpretable. For this reason, model-agnostic, post-hoc explanation methods have come to the fore among these works for explaining text classification models, as they are easy to understand and do not require access to the data or the model (Keane and Smyth, 2020). Towards post-hoc explanation in NLP tasks, (Murdoch et al., 2018) proposes a popular way named contextual decompositio"
2020.coling-main.541,D14-1181,0,0.00320645,"testing set. may be more than one counterfactual explanation corresponding with the original text instance. 4 Experiment 1: Financial Text Classification with Robust Transformers In this section we describe the results of a comprehensive evaluation of classification accuracy, comparing a variety of different classification baselines (including a human baseline) to our adversarial transformer approach. 4.1 Methods Used The baselines used can be grouped into several distinct categories: human evaluations – traditional machine learning approaches (SVM) – classical deep learning approaches (CNN (Kim, 2014), BiGRU (Bahdanau et al., 2014) , and HAN (Yang et al., 2016)) – and various transformer approaches with/without pruning strategies. These transformer-based models are generally considered to provide the current stateof-the-art in text classification. We reproduce these baselines based on the Transformers.2 Acquiring a human baseline As a baseline, we asked 26 participants which were experts in economics and finance to predict M&A events by completing 50 M&A evaluation questionnaires. The participants consisted of Ph.D. students, and academics from the fields of economics/finance. All particip"
2020.coling-main.541,D19-3002,0,0.0256036,"ble 4 where we highlight the changing parts. Based on the 500 testing examples, we guarantee that there is at least one counterfactual instance corresponding with the original input. We gain insight into which aspects are causally relevant by comparing the original context to the revised context which can flip the classifier’s prediction. 5.2 Human Evaluation for the Explanation We implement interpretation experiments on the optimal fine-tuned transformer classifier. While an explainable model trained with supervised learning is a common method to interpret the results of text classification (Wallace et al., 2019), the self-supervised learning explainable frameworks have been scarcely found. Meanwhile, the work in (Kaushik et al., 2020) consider similar types of edits to generate counterfactually-revised data, however, all of the instances are generated by human which greatly limits the expansibility of the method. To comprehensively evaluate the performance of our method, we consider a state-of-the-art example-based explanation framework for comparison, namely HotFlip (Ebrahimi et al., 2017), which uses gradients to identify important words and then flip it with the adversarial word which can cause th"
2020.coling-main.541,N16-1174,0,0.0569409,"lanation corresponding with the original text instance. 4 Experiment 1: Financial Text Classification with Robust Transformers In this section we describe the results of a comprehensive evaluation of classification accuracy, comparing a variety of different classification baselines (including a human baseline) to our adversarial transformer approach. 4.1 Methods Used The baselines used can be grouped into several distinct categories: human evaluations – traditional machine learning approaches (SVM) – classical deep learning approaches (CNN (Kim, 2014), BiGRU (Bahdanau et al., 2014) , and HAN (Yang et al., 2016)) – and various transformer approaches with/without pruning strategies. These transformer-based models are generally considered to provide the current stateof-the-art in text classification. We reproduce these baselines based on the Transformers.2 Acquiring a human baseline As a baseline, we asked 26 participants which were experts in economics and finance to predict M&A events by completing 50 M&A evaluation questionnaires. The participants consisted of Ph.D. students, and academics from the fields of economics/finance. All participants were either native English speakers or had a high degree"
2020.emnlp-main.516,D19-1403,0,0.0312291,"on community, as the low-level features in images are transferable across classes that enables learning from only a few examples from the unseen class. The existing approaches (Snell et al., 2017; Vinyals et al., 2016) typically focus on metric learning. Snell et al. (2017) learn a prototype representation for each class and classifies test points based on the nearest prototypes. Vinyals et al. (2016) compute support set aware similarities between a test point and the target classes. These methods have been adapted with some success to NLP tasks including text classification (Yu et al., 2018; Geng et al., 2019; Bao et al., 2020), machine translation (Gu et al., 2018), and relation classification (Han et al., 2018). Recently, Wang et al. (2019) show that simple feature transformations followed by nearest neighbor search can perform competitively with the state-of-theart meta-learning methods on standard computer vision classification datasets. Inspired by this approach, we evaluate the performance of nearest neighbor based classification against meta-learning methods. Few-shot NER A few approaches have been proposed for few-shot NER. Hofer et al. (2018) explore different pre-training and fine-tuning"
2020.emnlp-main.516,D18-1398,0,0.0234158,"ferable across classes that enables learning from only a few examples from the unseen class. The existing approaches (Snell et al., 2017; Vinyals et al., 2016) typically focus on metric learning. Snell et al. (2017) learn a prototype representation for each class and classifies test points based on the nearest prototypes. Vinyals et al. (2016) compute support set aware similarities between a test point and the target classes. These methods have been adapted with some success to NLP tasks including text classification (Yu et al., 2018; Geng et al., 2019; Bao et al., 2020), machine translation (Gu et al., 2018), and relation classification (Han et al., 2018). Recently, Wang et al. (2019) show that simple feature transformations followed by nearest neighbor search can perform competitively with the state-of-theart meta-learning methods on standard computer vision classification datasets. Inspired by this approach, we evaluate the performance of nearest neighbor based classification against meta-learning methods. Few-shot NER A few approaches have been proposed for few-shot NER. Hofer et al. (2018) explore different pre-training and fine-tuning strategies to recognize entities in medical text with a f"
2020.emnlp-main.516,D18-1514,0,0.0304134,"om only a few examples from the unseen class. The existing approaches (Snell et al., 2017; Vinyals et al., 2016) typically focus on metric learning. Snell et al. (2017) learn a prototype representation for each class and classifies test points based on the nearest prototypes. Vinyals et al. (2016) compute support set aware similarities between a test point and the target classes. These methods have been adapted with some success to NLP tasks including text classification (Yu et al., 2018; Geng et al., 2019; Bao et al., 2020), machine translation (Gu et al., 2018), and relation classification (Han et al., 2018). Recently, Wang et al. (2019) show that simple feature transformations followed by nearest neighbor search can perform competitively with the state-of-theart meta-learning methods on standard computer vision classification datasets. Inspired by this approach, we evaluate the performance of nearest neighbor based classification against meta-learning methods. Few-shot NER A few approaches have been proposed for few-shot NER. Hofer et al. (2018) explore different pre-training and fine-tuning strategies to recognize entities in medical text with a few examples. Fritzler et al. (2019) and Hou 6372"
2020.emnlp-main.516,2020.acl-main.128,0,0.404318,"ty class. It is (i) a challenging problem since the target tag set CT (j) can be different from any source tag set CS . To this end, few-shot NER systems need to learn to generalize to unseen entity classes using only a few labeled examples. Formally, the task of K-shot NER is defined as follows: given an input sentence x = {xt }Tt=1 and a K-shot support set for the target tag set CT , find the best tag sequence y = {yt }T1 for x. The Kshot support set contains K entity examples (not tokens) for each entity class given by CT . 2.2 A standard evaluation setup Prior work (Fritzler et al., 2019; Hou et al., 2020) on few-shot NER followed few-shot classification literature and adopted the episode evaluation methodology. Specifically, a NER system is evaluated with respect to multiple evaluation episodes. An episode includes a sampled K-shot support set of labeled examples and a few sampled K-shot test sets. In addition to these prior practices, we propose a more realistic evaluation setting by sampling only the support sets and testing the model on the standard test sets from NER benchmarks. Test set construction In the episode evaluation setting, test sets are sampled such that the different entity cl"
2020.emnlp-main.516,W17-4418,0,0.113074,"Missing"
2020.emnlp-main.516,D14-1162,0,0.0849816,"score results on five-shot NER for both tag set extension and domain transfer tasks. We report standard deviations from runs with five different support sets sampled from the validation sets. The best results are in bold. BERT hyper-parameter values provided by Hugging Face8 . Specifically, our BiLSTM-NER models adopt one-layer word-level BiLSTM model and one-layer character-level uni-directional LSTM model. LSTM hidden sizes are 50 and 200 and input embedding sizes are 30 and 100 for the character-level and word-level models respectively. We use the pre-trained 100-dimensional GloVe vectors (Pennington et al., 2014) to initialize the word embeddings for all BiLSTM-NER models. SGD and Adam (Kingma and Ba, 2014) are utilized to optimize the BiLSTM-based and BERTbased models with learning rates 0.015 and 5 × 10−5 respectively. We tune other parameters required by different few-shot learning methods on the source domain development sets. The transition re-normalizing temperature τ is chosen from {0.01, 0.005, 0.001}. 4.4 Results The results for one-shot NER and five-shot NER are summarized in Table 2 and Table 3 respectively. 8 As shown, our NNShot and S TRUCT S HOT perform significantly better than all prev"
2020.emnlp-main.516,C18-1327,0,0.0371391,"TA) few-shot NER systems (Fritzler et al., 2019). PrototypicalNet+P&D (Hou et al., 2020) improves upon Prototypical Network by using the pair-wise embedding and dependency transfer mechanism.7 SimBERT is a nearest neighbor classifier based on the pre-trained BERT encoder without fine-tuning on any NER data. Finally, we include our proposed NNShot and S TRUCT S HOT described in § 3. We use the IO tagging scheme for all of the experiments, as we find that it performs much better than BIO scheme for all the considered methods. Parameter tuning We adopt the best hyperparameter values reported by (Yang et al., 2018) for the BiLSTM-NER models and use the default 7 6 Available at: http://conll.cemantix.org/ 2012/data.html Experimental settings Hou et al. (2020) show that Matching Network (Vinyals et al., 2016) preforms worse than Prototypical Network on their evaluation for few-shot NER. 6369 Tag Set Extension System Group A Domain Transfer Group B Group C Ave. CoNLL I2B2 WNUT Ave. BiLSTM-based systems Prototypical Network 4.0±1.6 NNShot (ours) 15.7±7.1 S TRUCT S HOT (ours) 18.9±9.4 5.4±1.9 25.1±7.1 31.9±5.1 5.2±1.5 22.7±7.1 22.0±3.4 4.9 21.2 24.3 18.7±9.2 46.4±11.7 53.1±9.9 2.2±1.0 7.5±2.9 10.5±2.6 5.5±2."
2020.emnlp-main.516,N18-1109,0,0.0257827,"the computer vision community, as the low-level features in images are transferable across classes that enables learning from only a few examples from the unseen class. The existing approaches (Snell et al., 2017; Vinyals et al., 2016) typically focus on metric learning. Snell et al. (2017) learn a prototype representation for each class and classifies test points based on the nearest prototypes. Vinyals et al. (2016) compute support set aware similarities between a test point and the target classes. These methods have been adapted with some success to NLP tasks including text classification (Yu et al., 2018; Geng et al., 2019; Bao et al., 2020), machine translation (Gu et al., 2018), and relation classification (Han et al., 2018). Recently, Wang et al. (2019) show that simple feature transformations followed by nearest neighbor search can perform competitively with the state-of-theart meta-learning methods on standard computer vision classification datasets. Inspired by this approach, we evaluate the performance of nearest neighbor based classification against meta-learning methods. Few-shot NER A few approaches have been proposed for few-shot NER. Hofer et al. (2018) explore different pre-train"
2021.acl-long.109,D19-1383,0,0.193055,"Missing"
2021.acl-long.109,N19-1423,0,0.129429,"tated notes from MIMIC-III (Johnson et al., 2016), comprising over 100K annotated sentences. This will be, to our knowledge, one of the largest annotated datasets for clinical NLP, which tend to be smaller due to the expense of expert annotators. We evaluate machine learning methods to tackle this task. Similar to prior work on multi-aspect extractive summarization, we employ sentencelevel multi-label classification techniques (Hayashi et al., 2020). Our proposed architecture consists of passing a sentence, and its neighboring sentences on its left and right, through a pre-trained BERT model (Devlin et al., 2019) with minor modifications. Since there is limited annotated data but a wealth of unlabeled in-domain clinical notes, we also explore the impact of unsupervised learning on this task. We develop a method for tasktargeted pre-training data selection, in which a model trained on the downstream task selects unlabeled document segments for fine-tuning a BERT model. We find that this focused pre-training is much faster than pre-training on all available data and achieves competitive results. Our results show that unsupervised pre-training of any form is critical to improving results. Our code is ava"
2021.acl-long.109,2020.acl-main.740,0,0.0796845,"Missing"
2021.acl-long.109,W19-1906,0,0.146483,"ically 1 https://github.com/asappresearch/clip As they are built on top of MIMIC-III, which PhysioNet maintains, access to our annotations requires the completion of an ethics course and a Data Use Agreement. 1366 2 important problem (Mullenbach et al., 2021). 2 Related Work and Datasets Clinical information extraction There has been a wealth of previous work on extracting information from clinical notes, much of which also follows an extractive summarization approach. For example, Were et al. (2010) extracts items such as patient smoking status and obesity comorbidities from discharge notes. Liang et al. (2019) created a hybrid system of regex-based heuristics, neural network models trained on pre-existing datasets, and models such as support vector machines for disease-specific extractive summarization. Liu et al. (2018b) developed a pseudo-labelling, semi-supervised approach, using intrinsic correlation between notes, to train extractive summarization models for disease-specific summaries. We differ from these efforts in that we do not aim to generate general-purpose or disease-specific summaries, rather we focus on extracting specific action items from discharge notes to facilitate care transfer."
2021.acl-long.109,D19-1387,0,0.0364247,"released datasets, such as those from the n2c2 shared tasks. For comparison, 500 documents were annotated for adverse drug event extraction (Henry et al., 2020a), 150 documents for family history extraction (Liu et al., 2018a), and 100 documents for clinical concept normalization (Henry et al., 2020b). One of the largest annotated clinical datasets, emrQA, is built on 2,425 clinical notes (Pampari et al., 2018). Summarization Prior summarization work, which we build on, uses pre-trained transformer models to construct sentence representations that are contextualized with the entire document (Liu and Lapata, 2019; Hayashi et al., 2020). Liu and Lapata (2019) evaluate on three benchmark summarization datasets consisting of news articles. They are shorter, with average document lengths from 400-800 words, whereas MIMIC-III discharge notes average over 1,400 words. Liu and Lapata (2019) evaluate with ROUGE scores standard in summarization, whereas we take advantage of having ground truth extracted sentences and evaluate with classification metrics, providing a substantially different task. Liang et al. (2019) develop a disease-specific summary dataset, but it is not public and their methods involve combi"
2021.acl-long.109,2021.acl-long.109,1,0.0547219,"es competitive results. Our results show that unsupervised pre-training of any form is critical to improving results. Our code is available as open-source software 1 , and our annotations are available via PhysioNet 2 , to fully enable reproduction of our results and to provide a benchmark for evaluating future advances in clinical NLP in the context of this clinically 1 https://github.com/asappresearch/clip As they are built on top of MIMIC-III, which PhysioNet maintains, access to our annotations requires the completion of an ethics course and a Data Use Agreement. 1366 2 important problem (Mullenbach et al., 2021). 2 Related Work and Datasets Clinical information extraction There has been a wealth of previous work on extracting information from clinical notes, much of which also follows an extractive summarization approach. For example, Were et al. (2010) extracts items such as patient smoking status and obesity comorbidities from discharge notes. Liang et al. (2019) created a hybrid system of regex-based heuristics, neural network models trained on pre-existing datasets, and models such as support vector machines for disease-specific extractive summarization. Liu et al. (2018b) developed a pseudo-labe"
2021.acl-long.109,K16-1028,0,0.118394,"Missing"
2021.acl-long.109,D18-1258,0,0.128356,"e-trained neural networks will identify and exploit such information as needed (Tenney et al., 2019). Although on different tasks, we note that our dataset of 718 annotated documents is larger than recently released datasets, such as those from the n2c2 shared tasks. For comparison, 500 documents were annotated for adverse drug event extraction (Henry et al., 2020a), 150 documents for family history extraction (Liu et al., 2018a), and 100 documents for clinical concept normalization (Henry et al., 2020b). One of the largest annotated clinical datasets, emrQA, is built on 2,425 clinical notes (Pampari et al., 2018). Summarization Prior summarization work, which we build on, uses pre-trained transformer models to construct sentence representations that are contextualized with the entire document (Liu and Lapata, 2019; Hayashi et al., 2020). Liu and Lapata (2019) evaluate on three benchmark summarization datasets consisting of news articles. They are shorter, with average document lengths from 400-800 words, whereas MIMIC-III discharge notes average over 1,400 words. Liu and Lapata (2019) evaluate with ROUGE scores standard in summarization, whereas we take advantage of having ground truth extracted sente"
2021.acl-long.109,P17-2034,0,0.156822,"ith monitoring of his labs and reinstitution once the kidney function improves”), as these are likely to require PCP action. Training set statistics Dataset comparison and phenomena analysis To distinguish the contribution of our dataset in the context of existing text summarization datasets, we performed a manual quantitative comparison between CLIP and the summarization datasets CNN (Hermann et al., 2015) and WikiASP (Hayashi et al., 2020). For WikiASP, we chose sentences from the “Event” genre of summary, as our dataset describes hospital stays which could be considered events. Inspired by Suhr et al. (2017), we identified five phenomena to compare across datasets quantification (in the numerical sense, as in “300 mg” or “twenty-three people”), temporal expressions, conditional expressions, imperative mood or second-person statements, and out of vocabulary (OOV) terms 3 . We gathered 100 sentences from the summaries of each dataset and counted the occurrences of each phenomena. We see that CLIP has a relative wealth of imperative and second-person statements, which is not surprising due to the prevalence of patientdirected language in “Patient instructions”-labeled sentences. CLIP and WikiASP bot"
2021.acl-long.109,P19-1214,0,0.323692,"hich provides maximal control over the size of the selected dataset. Further, directly applying TAPT to our case may not work well as it does not distinguish positive and negative samples in the in-domain dataset, so the surfaced sentences from TAPT may be less relevant. Our approach benefits from using an encoding method that is trained on the task we are targeting. After selecting data, we pre-train a BERTContext model on the targeted dataset, pulling in neighboring sentences of the targeted sentences. As auxiliary tasks, we used masked language modeling (MLM) and a sentence switching task (Wang et al., 2019). For MLM, we mask tokens in the context sentence only, independently with probability 0.15. For sentence switching, with probability 0.25 we swap the focus sentence with another randomly chosen sentence from the same document, and predict whether the focus sentence was swapped using the context sentences. Cross entropy losses for both tasks are computed and summed to compute the total loss for an instance. These tasks encourage the model to learn how to incorporate information from the context sentences into its representation. Figure 1 depicts the entire process. This process can be repeated"
2021.acl-long.109,2020.emnlp-demos.6,0,0.0487738,"Missing"
2021.emnlp-main.304,2020.acl-main.485,0,0.0412572,"n ing classification methods to demonstrate the via- NLP using gold-standard demographic data (i.e., bility of the approach – that is, to validate that the with known demographics of the authors) is to-date text samples captured can indeed serve as a reason- underexplored. This combination of downstream able proxy of the users’ survey-based responses for dependent variables and known demographics is an 3755 important step towards analyzing NLP fairness issues in real-world social contexts with clear normative goals, while considering the lived experiences of the community members they affect (Blodgett et al., 2020; Taylor et al., 2018). Finally, other teams developing language resources can adapt the process outlined to other domains such as security, e-commerce, finance, etc. We recognize that this is one of a handful of forays into rich psychometric NLP. Our hope is that future work can improve upon the methods and best practices for examining the interplay between survey-based constructs and their manifestations in user-generated text. While we recognize that the questions asked and approach undertaken could be further enhanced, we believe this constitutes an important first step toward aligning sur"
2021.emnlp-main.304,D15-1075,0,0.0289063,"nd to the multi-class scenario for race, and may also be applied to the other demographic variables, making this a rich data set for future fair NLP research. In addition, because the gold-standard labels are continuous (e.g., a numeracy score), this data set can facilitate development of new fairness metrics that merge calibration (Pleiss et al., 2017) with class-label-focused fairness assessments such as DI and xAUC. 5 Related Work Over the past thirty years, significant efforts have been made to develop a robust and burgeoning set of language resources for various linguistic and NLP tasks (Bowman et al., 2015; Guzmán et al., 2019). Gold-standard testbeds have been developed for sentiment analysis and emotion detection (Wiebe et al., 2005; Thelwall et al., 2010). Personality traits manifested in text have also received attention (Luyckx and Daelemans, 2008). More (1) recent work has explored construction of corpora 3754 Figure 4: Plot of model performance (AUC) against fairness (DI on left, ∆xAUC on right) for examining depression and cyberbullying, including annotating self-disclosures of personal information which may trigger bullying (Rakib and Soon, 2018), and testbeds for modeling empathy and"
2021.emnlp-main.304,D18-1507,0,0.0924606,"y. et al., 2015; Shing et al., 2020; Resnik et al., 2021). 3748 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3748–3758 c November 7–11, 2021. 2021 Association for Computational Linguistics In this paper we describe our efforts to construct a testbed for psychometric natural language processing (NLP). In the same vein as prior work on constructing language resources for sentiment, emotion, affect, and personality traits (Wiebe et al., 2005; Thelwall et al., 2010; Luyckx and Daelemans, 2008), and more recent work on modeling empathy and distress (Buechel et al., 2018; AbdulMageed et al., 2017), we describe our approach and resulting testbed related to psychometric dimensions such as trust, anxiety, literacy, and numeracy in the health context. Figure 1 presents a motivating example describing the goal of our work. Given a well-established survey-based scale for “trust in visiting the physician’s office,” how can we obtain a similar score based on user-generated text? Further, how do we ensure that our NLP-based scores are fair and unbiased? The resulting testbed is comprised of usergenerated text from 8,502 individuals for four key health-related psychome"
2021.emnlp-main.304,D19-2004,0,0.0283165,"-reported survey-based responses for the psychometric dimensions of interest (Buechel et al., 2018). Hence, the text is accompanied by surveybased quantifications from the individuals that can serve as a gold-standard proxy of what we hope to measure by applying NLP methods. the psychometric dimensions of interest. Further, our testbed also includes the users’ survey-based responses to related psychometric dimensions, as well as demographic data. We use the latter to explore the fairness of our text classifiers - an important direction for current and future NLP research (Bender et al., 2021; Chang et al., 2019). 6 Conclusion The results of our work have important implications for several stakeholder groups. NLP research focused on constructing novel empirical methods can use the constructed testbed to build new models for psychometric NLP. The inclusion of demographic, text, target psychometric, and secondary psychometric data in the testbed could allow development of rich deep learning architectures that incorporate user models (Ahmad et al., 2021), psychometric embeddings, structural equation model-based encoders, and multi-task learning across the four parallel target psychometric dimensions (Ahm"
2021.emnlp-main.304,I17-2042,0,0.0271737,"m level Strapparava and Mihalcea, 2007) and newer empatterns), each with 256 filters and ReLU activa- pathy and distress prediction tasks (Buechel et al., tion, followed by a global max pooling layer and 2018; Gibson et al., 2015). 3753 The binary classification task yielded similar results, with BERT outperforming the LSTM and CNN models in terms of AUC and F1 , and the LSTMs/CNNs in turn outperforming the FFNN and LR models (Table 5). Further, the best F1 scores in the 0.68 to 0.77 range are comparable to results from prior studies classifying binary discretized labels (Gibson et al., 2015; Khanpour et al., 2017; Yates et al., 2017). The above regression and classification analysis underscores the effectiveness of our survey-text collection process and suggests that NLP-based modeling of psychometric dimensions such as literacy, numeracy, trust, and anxiety in health-related contexts might be possible and practical. 4.2 For anxiety, subjective literacy, and trust in physicians, DI is generally close to 1, suggesting greater equity. For numeracy there is more variation across scores, in particular with respect to BERT. DI is much lower for BERT (less than 0.7) relative to FFNN (0.88) and WordCNN (1.0)"
2021.emnlp-main.304,luyckx-daelemans-2008-personae,0,0.159205,"and remation retrieval and behavior modeling (Abbasi ∗ Authors listed alphabetically. et al., 2015; Shing et al., 2020; Resnik et al., 2021). 3748 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3748–3758 c November 7–11, 2021. 2021 Association for Computational Linguistics In this paper we describe our efforts to construct a testbed for psychometric natural language processing (NLP). In the same vein as prior work on constructing language resources for sentiment, emotion, affect, and personality traits (Wiebe et al., 2005; Thelwall et al., 2010; Luyckx and Daelemans, 2008), and more recent work on modeling empathy and distress (Buechel et al., 2018; AbdulMageed et al., 2017), we describe our approach and resulting testbed related to psychometric dimensions such as trust, anxiety, literacy, and numeracy in the health context. Figure 1 presents a motivating example describing the goal of our work. Given a well-established survey-based scale for “trust in visiting the physician’s office,” how can we obtain a similar score based on user-generated text? Further, how do we ensure that our NLP-based scores are fair and unbiased? The resulting testbed is comprised of u"
2021.emnlp-main.304,D19-1632,0,0.026863,"scenario for race, and may also be applied to the other demographic variables, making this a rich data set for future fair NLP research. In addition, because the gold-standard labels are continuous (e.g., a numeracy score), this data set can facilitate development of new fairness metrics that merge calibration (Pleiss et al., 2017) with class-label-focused fairness assessments such as DI and xAUC. 5 Related Work Over the past thirty years, significant efforts have been made to develop a robust and burgeoning set of language resources for various linguistic and NLP tasks (Bowman et al., 2015; Guzmán et al., 2019). Gold-standard testbeds have been developed for sentiment analysis and emotion detection (Wiebe et al., 2005; Thelwall et al., 2010). Personality traits manifested in text have also received attention (Luyckx and Daelemans, 2008). More (1) recent work has explored construction of corpora 3754 Figure 4: Plot of model performance (AUC) against fairness (DI on left, ∆xAUC on right) for examining depression and cyberbullying, including annotating self-disclosures of personal information which may trigger bullying (Rakib and Soon, 2018), and testbeds for modeling empathy and distress (Buechel et a"
2021.emnlp-main.304,W18-0604,0,0.109778,"Missing"
2021.emnlp-main.304,W17-5205,0,0.058692,"Missing"
2021.emnlp-main.304,D14-1162,0,0.0851982,"assification tasks, respectively. We evaluated the data set against five NLP models:linear/logistic regression (LR), feed forward neural network (FFNN), word CNN, word LSTM, and BERT (Devlin et al., 2018). LR and FFNN were each run with a maximum of 50,000 word unigram, bigram, and trigram features. FFNN contained three dense layers each with 256 units, ReLU activation, L2 regularization of 0.001, each followed by a dropout layer with value of 0.5. Word CNNs and LSTMs both used the GloVe Common Crawl (840B token) 300 dimension word embedFor the regression tasks, consistent with prior redings (Pennington et al., 2014). The word LSTM search, BERT outperformed the LSTMs and CNNs, had two bidirectional layers with 128 units, each and the LSTMs attained better results than the with dropout and recurrent dropout of 0.2, followed feature-based FFNN and regression models (Table by a 64 unit dense layer. Following prior studies 4). Further, our highest Pearson’s r values, in the (Buechel et al., 2018; Majumder et al., 2017), the 0.48 to 0.61 range, are on par with those attained for word CNN was a concatenation of three single con- the well-established emotion intensity prediction volutional layers of kernel size"
2021.emnlp-main.304,2020.acl-main.723,0,0.0503657,"14). In social science repredefined measurement framework. Effectively search, psychometric dimensions are latent concollecting and measuring relevant psychometric distructs that are known to be important antecedents, mensions in a timely, unobtrusive, and open-ended moderators, mediators, and consequents for immanner could be invaluable in many real-world portant humanistic behaviors and outcomes. For settings (Gefen and Larsen, 2017), including inforexample, constructs such as threat severity and remation retrieval and behavior modeling (Abbasi ∗ Authors listed alphabetically. et al., 2015; Shing et al., 2020; Resnik et al., 2021). 3748 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3748–3758 c November 7–11, 2021. 2021 Association for Computational Linguistics In this paper we describe our efforts to construct a testbed for psychometric natural language processing (NLP). In the same vein as prior work on constructing language resources for sentiment, emotion, affect, and personality traits (Wiebe et al., 2005; Thelwall et al., 2010; Luyckx and Daelemans, 2008), and more recent work on modeling empathy and distress (Buechel et al., 2018; AbdulMageed e"
2021.emnlp-main.304,S07-1013,0,0.102702,"nd the LSTMs attained better results than the with dropout and recurrent dropout of 0.2, followed feature-based FFNN and regression models (Table by a 64 unit dense layer. Following prior studies 4). Further, our highest Pearson’s r values, in the (Buechel et al., 2018; Majumder et al., 2017), the 0.48 to 0.61 range, are on par with those attained for word CNN was a concatenation of three single con- the well-established emotion intensity prediction volutional layers of kernel size 1, 2, and 3 (i.e., to problem (Mohammad and Bravo-Marquez, 2017; capture word unigram, bigram, and trigram level Strapparava and Mihalcea, 2007) and newer empatterns), each with 256 filters and ReLU activa- pathy and distress prediction tasks (Buechel et al., tion, followed by a global max pooling layer and 2018; Gibson et al., 2015). 3753 The binary classification task yielded similar results, with BERT outperforming the LSTM and CNN models in terms of AUC and F1 , and the LSTMs/CNNs in turn outperforming the FFNN and LR models (Table 5). Further, the best F1 scores in the 0.68 to 0.77 range are comparable to results from prior studies classifying binary discretized labels (Gibson et al., 2015; Khanpour et al., 2017; Yates et al., 20"
2021.emnlp-main.304,D17-1322,0,0.0279884,"Mihalcea, 2007) and newer empatterns), each with 256 filters and ReLU activa- pathy and distress prediction tasks (Buechel et al., tion, followed by a global max pooling layer and 2018; Gibson et al., 2015). 3753 The binary classification task yielded similar results, with BERT outperforming the LSTM and CNN models in terms of AUC and F1 , and the LSTMs/CNNs in turn outperforming the FFNN and LR models (Table 5). Further, the best F1 scores in the 0.68 to 0.77 range are comparable to results from prior studies classifying binary discretized labels (Gibson et al., 2015; Khanpour et al., 2017; Yates et al., 2017). The above regression and classification analysis underscores the effectiveness of our survey-text collection process and suggests that NLP-based modeling of psychometric dimensions such as literacy, numeracy, trust, and anxiety in health-related contexts might be possible and practical. 4.2 For anxiety, subjective literacy, and trust in physicians, DI is generally close to 1, suggesting greater equity. For numeracy there is more variation across scores, in particular with respect to BERT. DI is much lower for BERT (less than 0.7) relative to FFNN (0.88) and WordCNN (1.0), suggesting that BER"
2021.findings-emnlp.221,P19-1635,0,0.0152328,"ithout using sophisticated mathematical operations. To obtain the embedding of an Out-ofVocabulary (OOV) number, we propose an interpolation method that uses the weighted average of its two neighbors’ embeddings based on cosine similarity. We experiment our method on several numeracy-related tasks, including evaluating embeddings on their ability to capture magnitude (Naik et al., 2019), and solving numeracy tasks (list maximum, decoding, and addition) (Wallace et al., 2019). We also apply our method in a downstream financial NLP task that predicts the magnitude of numbers in market comments (Chen et al., 2019). Experiments show that our approach is efficient, achieving comparable, and even better performance than existing numeracy-preserving methods. to interpolate or extrapolate to OOV numerals. The main reason that causes such poor performance with number-intensive tasks is that the existing word embedding methods are not specifically designed to capture numerical relationships. To handle number embeddings specifically, some new NLP models are proposed. One closely related work to ours is DICE (Sundararaman et al., 2020) which devises an independent-of-corpus and deterministic approach to assign"
2021.findings-emnlp.221,D16-1101,0,0.0632687,"Missing"
2021.findings-emnlp.221,P18-1196,0,0.0481412,"Missing"
2021.findings-emnlp.221,2020.emnlp-main.384,0,0.0325138,"stream financial NLP task that predicts the magnitude of numbers in market comments (Chen et al., 2019). Experiments show that our approach is efficient, achieving comparable, and even better performance than existing numeracy-preserving methods. to interpolate or extrapolate to OOV numerals. The main reason that causes such poor performance with number-intensive tasks is that the existing word embedding methods are not specifically designed to capture numerical relationships. To handle number embeddings specifically, some new NLP models are proposed. One closely related work to ours is DICE (Sundararaman et al., 2020) which devises an independent-of-corpus and deterministic approach to assign embeddings for numbers. However, DICE derives numerical embedding based on engineered mathematical operations, which could be computationally costly for encoding a large number of numbers. Our work differs from DICE in that we infer numeracypreserving embeddings automatically from a specially designed knowledge graph. Compared to DICE, our approach is simple and efficient yet achieves comparable or even better performance. 2 3 Related Work Methods Numbers are ubiquitous and numeracy plays an im- The high-level idea of"
2021.findings-emnlp.221,2021.naacl-main.53,0,0.0985152,"following a small number embedding or contextualized transformer-based language may indicate Admitted. While numeracy is critical models, fail to learn numeracy. As the result, in such domains where numbers are prevalent, most the performance of these models is limited existing NLP models are not designed explicitly to when they are applied to number-intensive applications in clinical and financial domains. In handle numbers. Numbers are either directly disthis work, we propose a simple number embedcarded in pre-processing, or treated as a UNK token ding approach based on knowledge graph. We (Thawani et al., 2021). Prior literature also shows construct a knowledge graph consisting of numthat neither traditional word embeddings such as ber entities and magnitude relations. Knowlword2vec nor the contextualized transformer-based edge graph embedding method is then applied language model such as BERT can handle numbers to obtain number vectors. Our approach is easy and process numeracy tasks effectively (Naik et al., to implement, and experiment results on various numeracy-related NLP tasks demonstrate 2019; Wallace et al., 2019). the effectiveness and efficiency of our method. One straightforward way to e"
2021.findings-emnlp.221,D19-1534,0,0.32875,"essing, or treated as a UNK token ding approach based on knowledge graph. We (Thawani et al., 2021). Prior literature also shows construct a knowledge graph consisting of numthat neither traditional word embeddings such as ber entities and magnitude relations. Knowlword2vec nor the contextualized transformer-based edge graph embedding method is then applied language model such as BERT can handle numbers to obtain number vectors. Our approach is easy and process numeracy tasks effectively (Naik et al., to implement, and experiment results on various numeracy-related NLP tasks demonstrate 2019; Wallace et al., 2019). the effectiveness and efficiency of our method. One straightforward way to encode numbers in NLP tasks is to map a number’s value directly 1 Introduction to its embedding (e.g., “twenty-four” embeds to Numeracy is the ability to reason and to apply nu- [24]). Still, this strategy performs poorly while the NLP task involves a large amount of numbers merical concepts, and numbers play a key role in with a wide range (Wallace et al., 2019). Therenatural language understanding. For example, infore, encoding numbers into high-dimensional vecvestors will probably react differently to the news “AAP"
2021.findings-emnlp.221,P18-2100,0,0.0657987,"Missing"
2021.findings-emnlp.221,N09-1031,0,0.0414685,"Work Methods Numbers are ubiquitous and numeracy plays an im- The high-level idea of our approach NEKG is to portant role in NLP applications and domains such preserve numeracy and numeric semantics (e.g., as financial and clinical documents (Spithourakis magnitude, addition) via knowledge graph. Knowlet al., 2016; Rajkomar et al., 2018; Qin and Yang, edge graph is a network of entities, their seman2019). However, most of existing work simply tic types, properties, and relationships, built based ignores the numbers in the pre-processing step on entity-relation triples (Popping, 2003). In our (Kogan et al., 2009) and thus leads to suboptimal method, we make use of a simply structured knowlperformance. See (Thawani et al., 2021) for an edge graph consisting of only numbers and their overview. Spithourakis and Riedel (2018) studies magnitude relationships. We embed the knowldifferent strategies to model numerals, and Jiang edge graph in a vector space using a graph emet al. (2019) proposes a joint learning model for bedding method for obtaining embeddings of the handling numbers in text. Still, a recent work (Naik number entities. Numbers that are not in the et al., 2019) shows that common word embeddin"
2021.findings-emnlp.221,P19-1329,0,0.0925268,"f n ∈ OOV : 08: embd = Interpolation (n) 09: else : 10: embd = model.get_embeddings (n) (Bordes et al., 2013), to embed the number entities into a vector space. In this way, numeracypreserved embeddings can be obtained directly without using sophisticated mathematical operations. To obtain the embedding of an Out-ofVocabulary (OOV) number, we propose an interpolation method that uses the weighted average of its two neighbors’ embeddings based on cosine similarity. We experiment our method on several numeracy-related tasks, including evaluating embeddings on their ability to capture magnitude (Naik et al., 2019), and solving numeracy tasks (list maximum, decoding, and addition) (Wallace et al., 2019). We also apply our method in a downstream financial NLP task that predicts the magnitude of numbers in market comments (Chen et al., 2019). Experiments show that our approach is efficient, achieving comparable, and even better performance than existing numeracy-preserving methods. to interpolate or extrapolate to OOV numerals. The main reason that causes such poor performance with number-intensive tasks is that the existing word embedding methods are not specifically designed to capture numerical relatio"
2021.findings-emnlp.221,P19-1038,1,0.882286,"Missing"
2021.naacl-main.239,D18-1547,0,0.39184,"age understanding cise procedural requirements. These actions difof user needs (Wu et al., 2019; Rastogi et al., fer from typical dialogue acts because tracking 2020b; Liang et al., 2020). However, selecting them necessitates striking a balance between exactions in real life requires not only obeying user ternal user requests and internally-imposed guiderequests, but also following practical policy limilines. Thus, the major difference between tations which may be at odds with those requests. ABCD and other dialogue datasets, such as MulFor example, while a user may ask for a refund on tiWOZ (Budzianowski et al., 2018), is that it asks their purchase, an agent should only honor such a the agent to adhere to a set of policies while simulrequest if it is valid with regards to the store’s retaneously dealing with customer requests. turn policy. Described in actions, before an agent While the prevalent data collection paradigm 1 All code and data will be available at this location. involves Wizard-of-Oz techniques, our situation 3002 1 Introduction Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3002–3017 June"
2021.naacl-main.239,D19-1459,0,0.104972,"d dialogue systems trained on such datasets are intended for solving user issues. The resolution of these issues implicitly requires taking actions, where an action is a non-utterance decision that depends on both user and system inputs. Despite the tremendous number of dialogues, examples in previous benchmarks fixate on the single knowledge base (KB) lookup action where the agent searches for an item that matches the user’s desires and is available in the KB. By sticking to this sole interaction, conversations can be generated through rules (Weston et al., 2016), paraphrased from templates (Byrne et al., 2019) or taken from static text scenarios (Zhang et al., 2018), leading to dialogues that are predominantly homogeneous in nature. Many datasets have scaled to more domains as well (Eric et al., 2017; Budzianowski et al., 2018; Peskov et al., 2019) Since each new domain introduces a KB lookup requiring different slotvalues, the number of unique actions grows as a linear function of the number of domains covered. Rather than expanding wider, ABCD instead focuses deeper by increasing the count and diversity of actions within a single domain. Exploring Other Avenues Multiple aspects are explored by co"
2021.naacl-main.239,P19-1455,0,0.0576183,"Missing"
2021.naacl-main.239,N19-1423,0,0.0254529,"Missing"
2021.naacl-main.239,W17-5506,0,0.0237083,"t depends on both user and system inputs. Despite the tremendous number of dialogues, examples in previous benchmarks fixate on the single knowledge base (KB) lookup action where the agent searches for an item that matches the user’s desires and is available in the KB. By sticking to this sole interaction, conversations can be generated through rules (Weston et al., 2016), paraphrased from templates (Byrne et al., 2019) or taken from static text scenarios (Zhang et al., 2018), leading to dialogues that are predominantly homogeneous in nature. Many datasets have scaled to more domains as well (Eric et al., 2017; Budzianowski et al., 2018; Peskov et al., 2019) Since each new domain introduces a KB lookup requiring different slotvalues, the number of unique actions grows as a linear function of the number of domains covered. Rather than expanding wider, ABCD instead focuses deeper by increasing the count and diversity of actions within a single domain. Exploring Other Avenues Multiple aspects are explored by conversational datasets attempting to mimic reality. Rashkin et al. (2019) studies the ability of a dialogue model to handle empathy, while Zhou et al. (2018) focuses on commonsense reasoning. Ano"
2021.naacl-main.239,2020.acl-main.54,0,0.0159319,"step selection, utterance ranking, ample, workers would be presented with 55 difand intent classification. For both tasks, we experferent classes for Intent Classification and asked imented with two types of frameworks, a pipeline to choose the right one. Since humans typically version and an end-to-end version. The pipeline struggle when choosing from large collections of version trains each subtask separately while the items, fine-tuned models performed roughly on end-to-end optimizes all tasks jointly (Liang et al., par or better compared to humans in this unnat2020; Rastogi et al., 2020a; Ham et al., 2020). ural setting. On the other hand, human evaluation for the overall CDS task was judged by measuring The pipeline model uses a BERT model trained the success rate in a standard conversational scewith the RAdam optimizer (Liu et al., 2020). narios where behavioral instincts are activated, so To test the performance of different pretrained humans were able to excel on this environment. models under the end-to-end framework, we 3009 f (xi , dj ) = h&gt; ctx hcand exp(f (xi , dj )) Pjrank = Σd0j exp f (xi , d0j ) Model Human Pipeline BERT-base AlBERT RoBERTa RoBERTa-Large BERT-base w/o Action Info BE"
2021.naacl-main.239,P17-1162,0,0.0286639,"al datasets attempting to mimic reality. Rashkin et al. (2019) studies the ability of a dialogue model to handle empathy, while Zhou et al. (2018) focuses on commonsense reasoning. Another approach is to augment dialogues with multi-modality including audio (Castro et al., 2019) or visual (Das et al., 2017a) components. Other researchers have explored grounding conversations with external data sources such as personas (Zhang et al., 2018), online reviews (Ghazvininejad et al., 2018) or large knowledge bases (Dinan et al., 2019). Intricate dialogues can also appear when studying collaboration (He et al., 2017; Kim et al., 2019) or negotiation (Lewis et al., 2017; He et al., 2018) which strongly encourage interaction with the other participant. In comparison, ABCD aims to make dialogue more realistic by considering distinct constraints from policies. Traditional Dialogue Datasets In recent years, Dialogues with Policies Procedural actions foldialogue datasets have grown in size from hunlowing strict guidelines naturally emerge in diadreds of conversations to the tens of thoulogue research geared towards real-world applisands (Henderson et al., 2014; Budzianowski 3003 Subflows Actions recover-userna"
2021.naacl-main.239,D18-1256,1,0.849231,"the ability of a dialogue model to handle empathy, while Zhou et al. (2018) focuses on commonsense reasoning. Another approach is to augment dialogues with multi-modality including audio (Castro et al., 2019) or visual (Das et al., 2017a) components. Other researchers have explored grounding conversations with external data sources such as personas (Zhang et al., 2018), online reviews (Ghazvininejad et al., 2018) or large knowledge bases (Dinan et al., 2019). Intricate dialogues can also appear when studying collaboration (He et al., 2017; Kim et al., 2019) or negotiation (Lewis et al., 2017; He et al., 2018) which strongly encourage interaction with the other participant. In comparison, ABCD aims to make dialogue more realistic by considering distinct constraints from policies. Traditional Dialogue Datasets In recent years, Dialogues with Policies Procedural actions foldialogue datasets have grown in size from hunlowing strict guidelines naturally emerge in diadreds of conversations to the tens of thoulogue research geared towards real-world applisands (Henderson et al., 2014; Budzianowski 3003 Subflows Actions recover-username,1 recover-password,1 reset-2fa,1 status-service-added,2 status-servic"
2021.naacl-main.239,W14-4337,0,0.0997016,"Missing"
2021.naacl-main.239,P19-1651,0,0.0141288,"pting to mimic reality. Rashkin et al. (2019) studies the ability of a dialogue model to handle empathy, while Zhou et al. (2018) focuses on commonsense reasoning. Another approach is to augment dialogues with multi-modality including audio (Castro et al., 2019) or visual (Das et al., 2017a) components. Other researchers have explored grounding conversations with external data sources such as personas (Zhang et al., 2018), online reviews (Ghazvininejad et al., 2018) or large knowledge bases (Dinan et al., 2019). Intricate dialogues can also appear when studying collaboration (He et al., 2017; Kim et al., 2019) or negotiation (Lewis et al., 2017; He et al., 2018) which strongly encourage interaction with the other participant. In comparison, ABCD aims to make dialogue more realistic by considering distinct constraints from policies. Traditional Dialogue Datasets In recent years, Dialogues with Policies Procedural actions foldialogue datasets have grown in size from hunlowing strict guidelines naturally emerge in diadreds of conversations to the tens of thoulogue research geared towards real-world applisands (Henderson et al., 2014; Budzianowski 3003 Subflows Actions recover-username,1 recover-passwo"
2021.naacl-main.239,D17-1259,0,0.108614,"al. (2019) studies the ability of a dialogue model to handle empathy, while Zhou et al. (2018) focuses on commonsense reasoning. Another approach is to augment dialogues with multi-modality including audio (Castro et al., 2019) or visual (Das et al., 2017a) components. Other researchers have explored grounding conversations with external data sources such as personas (Zhang et al., 2018), online reviews (Ghazvininejad et al., 2018) or large knowledge bases (Dinan et al., 2019). Intricate dialogues can also appear when studying collaboration (He et al., 2017; Kim et al., 2019) or negotiation (Lewis et al., 2017; He et al., 2018) which strongly encourage interaction with the other participant. In comparison, ABCD aims to make dialogue more realistic by considering distinct constraints from policies. Traditional Dialogue Datasets In recent years, Dialogues with Policies Procedural actions foldialogue datasets have grown in size from hunlowing strict guidelines naturally emerge in diadreds of conversations to the tens of thoulogue research geared towards real-world applisands (Henderson et al., 2014; Budzianowski 3003 Subflows Actions recover-username,1 recover-password,1 reset-2fa,1 status-service-add"
2021.naacl-main.239,P19-1534,0,0.0185817,"2018), leading to dialogues that are predominantly homogeneous in nature. Many datasets have scaled to more domains as well (Eric et al., 2017; Budzianowski et al., 2018; Peskov et al., 2019) Since each new domain introduces a KB lookup requiring different slotvalues, the number of unique actions grows as a linear function of the number of domains covered. Rather than expanding wider, ABCD instead focuses deeper by increasing the count and diversity of actions within a single domain. Exploring Other Avenues Multiple aspects are explored by conversational datasets attempting to mimic reality. Rashkin et al. (2019) studies the ability of a dialogue model to handle empathy, while Zhou et al. (2018) focuses on commonsense reasoning. Another approach is to augment dialogues with multi-modality including audio (Castro et al., 2019) or visual (Das et al., 2017a) components. Other researchers have explored grounding conversations with external data sources such as personas (Zhang et al., 2018), online reviews (Ghazvininejad et al., 2018) or large knowledge bases (Dinan et al., 2019). Intricate dialogues can also appear when studying collaboration (He et al., 2017; Kim et al., 2019) or negotiation (Lewis et al"
2021.naacl-main.239,D19-1218,0,0.0166738,"to take an action, respond with text or end the task. When the next step is an action xbt+1 , the model should predict the button with its slots and values as in AST. If the agent speaks in the next step xat+1 , the model should rank the true utterance highest, as measured by recall metrics.1 Finally, the model should recognize when to end the conversation. Rewarding the model only when it predicts every step correctly is counter-productive because minor variations in sentence order do not alter overall customer satisfaction. Therefore, CDS is scored using a variation on Cascading Evaluation (Suhr et al., 2019). Rather than receiving a single score for each conversation, cascaded evaluation allows the model to receive “partial credit” whenever it successfully predicts each successive step in the chat. This score is calculated on every turn, and the model is evaluated based on the percent of remaining steps correctly predicted, averaged across all available turns. (See Appendix C for more details.) 6.3 the task into predicting enumerable and nonenumerable values. The ontology lists out all |E| enumerable values, so the prediction head penum simply maps the hidden state henc into the appropriate dimen"
2021.naacl-main.239,P19-1078,0,0.121315,"er since prior steps may driven in no small part by the usefulness of these influence future decision states. (See Figure 1) tools, whereby actions are taken on behalf of the To more closely model real customer service user to accomplish their desired targets (Amaagents, we present the Action-Based Conversazon, 2019; Google, 2019). Research into tasktions Dataset (ABCD) consisting of 10,042 conoriented dialogue has concurrently made tremenversations containing numerous actions with predous progress on natural language understanding cise procedural requirements. These actions difof user needs (Wu et al., 2019; Rastogi et al., fer from typical dialogue acts because tracking 2020b; Liang et al., 2020). However, selecting them necessitates striking a balance between exactions in real life requires not only obeying user ternal user requests and internally-imposed guiderequests, but also following practical policy limilines. Thus, the major difference between tations which may be at odds with those requests. ABCD and other dialogue datasets, such as MulFor example, while a user may ask for a refund on tiWOZ (Budzianowski et al., 2018), is that it asks their purchase, an agent should only honor such a t"
2021.naacl-main.239,2020.emnlp-main.409,0,0.0240744,"Missing"
2021.naacl-main.239,P18-1205,0,0.130971,"for solving user issues. The resolution of these issues implicitly requires taking actions, where an action is a non-utterance decision that depends on both user and system inputs. Despite the tremendous number of dialogues, examples in previous benchmarks fixate on the single knowledge base (KB) lookup action where the agent searches for an item that matches the user’s desires and is available in the KB. By sticking to this sole interaction, conversations can be generated through rules (Weston et al., 2016), paraphrased from templates (Byrne et al., 2019) or taken from static text scenarios (Zhang et al., 2018), leading to dialogues that are predominantly homogeneous in nature. Many datasets have scaled to more domains as well (Eric et al., 2017; Budzianowski et al., 2018; Peskov et al., 2019) Since each new domain introduces a KB lookup requiring different slotvalues, the number of unique actions grows as a linear function of the number of domains covered. Rather than expanding wider, ABCD instead focuses deeper by increasing the count and diversity of actions within a single domain. Exploring Other Avenues Multiple aspects are explored by conversational datasets attempting to mimic reality. Rashki"
2021.naacl-main.239,2020.acl-demos.30,0,0.0763094,"Missing"
D13-1007,P06-2005,0,0.233763,"cized by Eisenstein (2013b), who argues that it strips away important social meanings. In recent work, normalization has been shown to yield improvements for part-of-speech tagging (Han et al., 2013), parsing (Zhang et al., 2013), and machine translation (Hassan and Menezes, 2013). As we will show in Section 7, accurate automated normalization can also improve our understanding of the nature of social media language. Supervised methods Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al., 2007a) and machine translation (Aw et al., 2006), as well as hybrid combinations of spelling correction and speech recognition (Kobus et al., 2008; Beaufort et al., 2010). This work sought to balance language models (favoring words that fit in context) with transformation models (favoring words that are similar to the observed text). Our approach can also be seen as a noisy channel model, but unlike this prior work, no labeled data is required. Unsupervised methods Cook and Stevenson (2009) manually identify several word formation types within a noisy channel framework. They parametrize each formation type with a small num62 ber of scalar v"
D13-1007,P10-1079,0,0.0670416,"has been shown to yield improvements for part-of-speech tagging (Han et al., 2013), parsing (Zhang et al., 2013), and machine translation (Hassan and Menezes, 2013). As we will show in Section 7, accurate automated normalization can also improve our understanding of the nature of social media language. Supervised methods Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al., 2007a) and machine translation (Aw et al., 2006), as well as hybrid combinations of spelling correction and speech recognition (Kobus et al., 2008; Beaufort et al., 2010). This work sought to balance language models (favoring words that fit in context) with transformation models (favoring words that are similar to the observed text). Our approach can also be seen as a noisy channel model, but unlike this prior work, no labeled data is required. Unsupervised methods Cook and Stevenson (2009) manually identify several word formation types within a noisy channel framework. They parametrize each formation type with a small num62 ber of scalar values, so that all legal transformations of a given type are equally likely. The scalar parameters are then estimated usin"
D13-1007,N10-1083,0,0.137776,"ork. There are two main sources of information to be exploited: local context, and surface similarity between the observed strings and normalization candidates. We treat the local context using standard language modeling techniques; we treat string similarity with a log-linear model that includes features for both surface similarity and word-word pairs. Because labeled examples of normalized text are not available, this model cannot be trained in the standard supervised fashion. Nor can we apply dynamic programming techniques for unsupervised training of locally-normalized conditional models (Berg-Kirkpatrick et al., 2010), as their complexity is quadratic in the size of label space; in normalization, the label space is the vocabulary itself, with at least 104 elements. Instead, we present a new training approach using Monte Carlo techniques to compute an approximate gradient on the feature weights. This training method may be applicable in other unsupervised learning problems with a large label space. This model is implemented in a normalization system called UN LOL (unsupervised normalization in a LOg-Linear model). It is a lightweight proba61 Proceedings of the 2013 Conference on Empirical Methods in Natural"
D13-1007,D11-1052,0,0.0421122,"s, which each put weight on different orthographic rules. Because the loadings are constrained to be non-negative, the factorization can be seen as sparsely assigning varying amounts of each style to each author. We choose the factorization that minimizes the Frobenius norm of the reconstruction error, using the NIMFA software package (http://nimfa.biolab.si/). The resulting styles are shown in Table 3, for k = 10; other values of k give similar overall results with more or less detail. The styles incorporate a number of linguistic phenomena, including: expressive lengthening (styles 7-9; see Brody and Diakopoulos, 2011); g- and t-dropping (style 5, see Eisenstein 2013a) ; th-stopping (style 6); and the dropping of several word-final vowels (styles 1-3). Some of these styles, such as t-dropping and th-stopping, have direct analogues in spoken language varieties (Tagliamonte and Temple, 2005; Green, 2002), while others, like expressive lengthening, seem more unique to social media. The relationships between these orthographic styles and social variables such as geography and demograph2 We tried adding these rules as features and retraining the normalization system, but this hurt performance. style 1. you; o-dr"
D13-1007,D11-1120,0,0.00581709,"idgeon soo, noo, doo, oohh, loove, thoo, helloo mee, ive, retweet, bestie, lovee, nicee, heey, likee, iphone, homie, ii, damnit ima, outta, needa, shoulda, woulda, mm, comming, tomm, boutt, ppreciate Table 3: Orthographic styles induced from automatically normalized Twitter text ics must be left to future research, but they offer a promising generalization of prior work that has focused almost exclusively on exclusively on lexical variation (Argamon et al., 2007; Eisenstein et al., 2010; Eisenstein et al., 2011), with a few exceptions for character-level features (Brody and Diakopoulos, 2011; Burger et al., 2011). Note that style 10 is largely the result of mistaken normalizations. The tokens ima, outta, and needa all refer to multi-word expressions in standard English, and are thus outside the scope of the normalization task as defined by Han et al. (2013). UN LOL has produced incorrect single-token normalizations for these terms: i/ima, out/outta, and need/needa. But while these normalizations are wrong, the resulting style nonetheless captures a coherent orthographic phenomenon. 8 Conclusion We have presented a unified, unsupervised statistical model for normalizing social media text, attaining the"
D13-1007,C10-2022,0,0.258123,"ing words that are similar to the observed text). Our approach can also be seen as a noisy channel model, but unlike this prior work, no labeled data is required. Unsupervised methods Cook and Stevenson (2009) manually identify several word formation types within a noisy channel framework. They parametrize each formation type with a small num62 ber of scalar values, so that all legal transformations of a given type are equally likely. The scalar parameters are then estimated using expectation maximization. This work stands apart from most of the other unsupervised models, which are pipelines. Contractor et al. (2010) use string edit distance to identify closely-related candidate orthographic forms and then decode the message using a language model. Gouws et al. (2011) refine this approach by mining an “exception dictionary” of stronglyassociated word pairs such as you/u. Like Contractor et al. (2010), we apply string edit distance, and like Gouws et al. (2011), we capture strongly related word pairs. However, rather than applying these properties as filtering steps in a pipeline, we add them as features in a unified log-linear model. Recent approaches have sought to improve accuracy by bringing more exter"
D13-1007,W09-2010,0,0.226101,"s Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al., 2007a) and machine translation (Aw et al., 2006), as well as hybrid combinations of spelling correction and speech recognition (Kobus et al., 2008; Beaufort et al., 2010). This work sought to balance language models (favoring words that fit in context) with transformation models (favoring words that are similar to the observed text). Our approach can also be seen as a noisy channel model, but unlike this prior work, no labeled data is required. Unsupervised methods Cook and Stevenson (2009) manually identify several word formation types within a noisy channel framework. They parametrize each formation type with a small num62 ber of scalar values, so that all legal transformations of a given type are equally likely. The scalar parameters are then estimated using expectation maximization. This work stands apart from most of the other unsupervised models, which are pipelines. Contractor et al. (2010) use string edit distance to identify closely-related candidate orthographic forms and then decode the message using a language model. Gouws et al. (2011) refine this approach by mining"
D13-1007,D10-1124,1,0.175652,"Missing"
D13-1007,P11-1137,1,0.14876,"ol, fone, dese, dha, shid, dhat, dat’s idk, fuckk, okk, backk, workk, badd, andd, goodd, bedd, elidgible, pidgeon soo, noo, doo, oohh, loove, thoo, helloo mee, ive, retweet, bestie, lovee, nicee, heey, likee, iphone, homie, ii, damnit ima, outta, needa, shoulda, woulda, mm, comming, tomm, boutt, ppreciate Table 3: Orthographic styles induced from automatically normalized Twitter text ics must be left to future research, but they offer a promising generalization of prior work that has focused almost exclusively on exclusively on lexical variation (Argamon et al., 2007; Eisenstein et al., 2010; Eisenstein et al., 2011), with a few exceptions for character-level features (Brody and Diakopoulos, 2011; Burger et al., 2011). Note that style 10 is largely the result of mistaken normalizations. The tokens ima, outta, and needa all refer to multi-word expressions in standard English, and are thus outside the scope of the normalization task as defined by Han et al. (2013). UN LOL has produced incorrect single-token normalizations for these terms: i/ima, out/outta, and need/needa. But while these normalizations are wrong, the resulting style nonetheless captures a coherent orthographic phenomenon. 8 Conclusion We ha"
D13-1007,W13-1102,1,0.429537,"ex systems. We use the output of UN LOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation. 1 Introduction Social media language can differ substantially from other written text. Many of the attempts to characterize and overcome this variation have focused on normalization: transforming social media language into text that better matches standard datasets (Sproat et al., 2001; Liu et al., 2011). Because there is little available training data, and because social media language changes rapidly (Eisenstein, 2013b), fully supervised training is generally not considered appropriate for this task. However, due to the extremely high-dimensional output space — arbitrary sequences of words across the vocabulary — it is a very challenging problem for unsupervised learning. Perhaps it is for these reasons that the most successful systems are pipeline architectures that cobble together a diverse array of techniques and resources, including statistical language models, dependency parsers, string edit distances, off-the-shelf spellcheckers, and curated slang dictionaries (Liu et al., 2011; Han and Baldwin, 2011"
D13-1007,N13-1037,1,0.428495,"ex systems. We use the output of UN LOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation. 1 Introduction Social media language can differ substantially from other written text. Many of the attempts to characterize and overcome this variation have focused on normalization: transforming social media language into text that better matches standard datasets (Sproat et al., 2001; Liu et al., 2011). Because there is little available training data, and because social media language changes rapidly (Eisenstein, 2013b), fully supervised training is generally not considered appropriate for this task. However, due to the extremely high-dimensional output space — arbitrary sequences of words across the vocabulary — it is a very challenging problem for unsupervised learning. Perhaps it is for these reasons that the most successful systems are pipeline architectures that cobble together a diverse array of techniques and resources, including statistical language models, dependency parsers, string edit distances, off-the-shelf spellcheckers, and curated slang dictionaries (Liu et al., 2011; Han and Baldwin, 2011"
D13-1007,P11-2008,1,0.154112,"Missing"
D13-1007,W11-2210,0,0.0291899,"ed. Unsupervised methods Cook and Stevenson (2009) manually identify several word formation types within a noisy channel framework. They parametrize each formation type with a small num62 ber of scalar values, so that all legal transformations of a given type are equally likely. The scalar parameters are then estimated using expectation maximization. This work stands apart from most of the other unsupervised models, which are pipelines. Contractor et al. (2010) use string edit distance to identify closely-related candidate orthographic forms and then decode the message using a language model. Gouws et al. (2011) refine this approach by mining an “exception dictionary” of stronglyassociated word pairs such as you/u. Like Contractor et al. (2010), we apply string edit distance, and like Gouws et al. (2011), we capture strongly related word pairs. However, rather than applying these properties as filtering steps in a pipeline, we add them as features in a unified log-linear model. Recent approaches have sought to improve accuracy by bringing more external resources and complex architectures to bear. Han and Baldwin (2011) begin with a set of string similarity metrics, and then apply dependency parsing t"
D13-1007,P11-1038,0,0.286219,"dly (Eisenstein, 2013b), fully supervised training is generally not considered appropriate for this task. However, due to the extremely high-dimensional output space — arbitrary sequences of words across the vocabulary — it is a very challenging problem for unsupervised learning. Perhaps it is for these reasons that the most successful systems are pipeline architectures that cobble together a diverse array of techniques and resources, including statistical language models, dependency parsers, string edit distances, off-the-shelf spellcheckers, and curated slang dictionaries (Liu et al., 2011; Han and Baldwin, 2011; Han et al., 2013). We propose a different approach, performing normalization in a maximum-likelihood framework. There are two main sources of information to be exploited: local context, and surface similarity between the observed strings and normalization candidates. We treat the local context using standard language modeling techniques; we treat string similarity with a log-linear model that includes features for both surface similarity and word-word pairs. Because labeled examples of normalized text are not available, this model cannot be trained in the standard supervised fashion. Nor can"
D13-1007,P13-1155,0,0.533896,"ges (Choudhury et al., 2007b). It has become still more salient in the era of widespread social media, particularly Twitter. Han and Baldwin (2011) formally define a normalization task for Twitter, focusing on normalizations between single tokens, and excluding multi-word tokens like lol (laugh out loud). The normalization task has been criticized by Eisenstein (2013b), who argues that it strips away important social meanings. In recent work, normalization has been shown to yield improvements for part-of-speech tagging (Han et al., 2013), parsing (Zhang et al., 2013), and machine translation (Hassan and Menezes, 2013). As we will show in Section 7, accurate automated normalization can also improve our understanding of the nature of social media language. Supervised methods Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al., 2007a) and machine translation (Aw et al., 2006), as well as hybrid combinations of spelling correction and speech recognition (Kobus et al., 2008; Beaufort et al., 2010). This work sought to balance language models (favoring words that fit in context) with transformation models (favoring words that are similar"
D13-1007,C08-1056,0,0.0992402,"work, normalization has been shown to yield improvements for part-of-speech tagging (Han et al., 2013), parsing (Zhang et al., 2013), and machine translation (Hassan and Menezes, 2013). As we will show in Section 7, accurate automated normalization can also improve our understanding of the nature of social media language. Supervised methods Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al., 2007a) and machine translation (Aw et al., 2006), as well as hybrid combinations of spelling correction and speech recognition (Kobus et al., 2008; Beaufort et al., 2010). This work sought to balance language models (favoring words that fit in context) with transformation models (favoring words that are similar to the observed text). Our approach can also be seen as a noisy channel model, but unlike this prior work, no labeled data is required. Unsupervised methods Cook and Stevenson (2009) manually identify several word formation types within a noisy channel framework. They parametrize each formation type with a small num62 ber of scalar values, so that all legal transformations of a given type are equally likely. The scalar parameters"
D13-1007,P11-2013,0,0.75945,"ion system called UN LOL, which achieves the best known results on two normalization datasets, outperforming more complex systems. We use the output of UN LOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation. 1 Introduction Social media language can differ substantially from other written text. Many of the attempts to characterize and overcome this variation have focused on normalization: transforming social media language into text that better matches standard datasets (Sproat et al., 2001; Liu et al., 2011). Because there is little available training data, and because social media language changes rapidly (Eisenstein, 2013b), fully supervised training is generally not considered appropriate for this task. However, due to the extremely high-dimensional output space — arbitrary sequences of words across the vocabulary — it is a very challenging problem for unsupervised learning. Perhaps it is for these reasons that the most successful systems are pipeline architectures that cobble together a diverse array of techniques and resources, including statistical language models, dependency parsers, strin"
D13-1007,P12-1109,0,0.78604,"sought to improve accuracy by bringing more external resources and complex architectures to bear. Han and Baldwin (2011) begin with a set of string similarity metrics, and then apply dependency parsing to identify contextuallysimilar words. Liu et al. (2011) extract noisy training pairs from the search snippets that result from carefully designed queries to Google, and then train a conditional random field (Lafferty et al., 2001) to estimate a character-based translation model. They later extend this work by adding a model of visual priming, an off-the-shelf spell-checker, and local context (Liu et al., 2012a). Hassan and Menezes (2013) use a random walk framework to capture contextual similarity, which they then interpolate with an edit distance metric. Rather than seeking additional external resources or designing more complex metrics of context and similarity, we propose a unified statistical model, which learns feature weights in a maximum-likelihood framework. 3 Approach Our approach is motivated by the following criteria: • Unsupervised. We want to be able to train a model without labeled data. At present, labeled data for Twitter normalization is available only in small quantities. Moreove"
D13-1007,P12-1055,0,0.512411,"sought to improve accuracy by bringing more external resources and complex architectures to bear. Han and Baldwin (2011) begin with a set of string similarity metrics, and then apply dependency parsing to identify contextuallysimilar words. Liu et al. (2011) extract noisy training pairs from the search snippets that result from carefully designed queries to Google, and then train a conditional random field (Lafferty et al., 2001) to estimate a character-based translation model. They later extend this work by adding a model of visual priming, an off-the-shelf spell-checker, and local context (Liu et al., 2012a). Hassan and Menezes (2013) use a random walk framework to capture contextual similarity, which they then interpolate with an edit distance metric. Rather than seeking additional external resources or designing more complex metrics of context and similarity, we propose a unified statistical model, which learns feature weights in a maximum-likelihood framework. 3 Approach Our approach is motivated by the following criteria: • Unsupervised. We want to be able to train a model without labeled data. At present, labeled data for Twitter normalization is available only in small quantities. Moreove"
D13-1007,W10-0513,0,0.0919659,"Missing"
D13-1007,P05-1044,0,0.252279,"to-word transformations without supervision would be impossible without the strong additional cue of local context. For example, in the phrase None of these tokens are standard (except 2, which appears in a nonstandard sense here), so without joint inference, it would not be possible to use context to help normalize suttin. Only by jointly reasoning over the entire message can we obtain the correct normalization. These desiderata point towards a featurized sequence model, which must be trained without labeled examples. While there is prior work on training sequence models without supervision (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010), there is an additional complication not faced by models for tasks such as part-of-speech tagging and named entity recognition: the potential label space of standard words is large, on the order of at least 104 . Naive application of Viterbi decoding — which is a component of training for both Contrastive Estimation (Smith and Eisner, 2005) and the locally-normalized sequence labeling model of Berg-Kirkpatrick et al. (2010) — will be stymied by Viterbi’s quadratic complexity in the dimension of the label space. While various pruning heuristics may be applied, w"
D13-1007,P13-1114,0,0.12172,"ttained popularity in the context of SMS messages (Choudhury et al., 2007b). It has become still more salient in the era of widespread social media, particularly Twitter. Han and Baldwin (2011) formally define a normalization task for Twitter, focusing on normalizations between single tokens, and excluding multi-word tokens like lol (laugh out loud). The normalization task has been criticized by Eisenstein (2013b), who argues that it strips away important social meanings. In recent work, normalization has been shown to yield improvements for part-of-speech tagging (Han et al., 2013), parsing (Zhang et al., 2013), and machine translation (Hassan and Menezes, 2013). As we will show in Section 7, accurate automated normalization can also improve our understanding of the nature of social media language. Supervised methods Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al., 2007a) and machine translation (Aw et al., 2006), as well as hybrid combinations of spelling correction and speech recognition (Kobus et al., 2008; Beaufort et al., 2010). This work sought to balance language models (favoring words that fit in context) with tra"
D15-1037,P12-2054,1,0.91825,"Ramage et al. (2009) propose Labeled-LDA, which improves LDA with document labels. It assumes that there is a one-to-one mapping between topics and labels, and it restricts each document’s topics to be sampled only from those allowed by the documents label set. Therefore, Labeled-LDA can be expressed in our model. We define ( 1, if z ∈ md fm (z, w, d) = (8) −∞, else 3.1 3.2 able 1,900,000 100,000,000 1,946,000 Dataset Prior Knowledge Generation Word Correlation Prior Knowledge Previous work proposes two methods to automatically generate prior word correlation knowledge from external sources. Hu and Boyd-Graber (2012) use WordNet 3.0 to obtain synsets for word types, and then if a synset is also in the vocabulary, they add a must-link correlation between the word type and the synset. Xie et al. (2015) use a different method that takes advantage of an existing pretrained word embedding. Each word embedding is a real-valued vector capturing the word’s semantic meaning based on distributional similarity. If the similarity between the embeddings of two word types in the vocabulary exceeds a threshold, they generate a must-link between the two words. In our experiments, we adopt a hybrid method that combines th"
D15-1037,P11-1026,1,0.890009,"times (once for each topic) because we need the summation of P (z = t) for sampling. Therefore, the critical part of speeding up the sampler is finding a sparse representation of the second term. In the following sections, we show that natural, sparse prior knowledge representations are possible. We first present an efficient sparse representation of word correlation prior knowledge and then one for document-label knowledge. 2.3 We now illustrate how we can encode word correlation knowledge as a set of sparse constraints fm (z, w, d) in our model. In previous work (Andrzejewski et al., 2009; Hu et al., 2011; Xie et al., 2015), word correlation prior knowledge is represented as word must-link constraints and cannotlink constraints. A must-link relation between two words indicates that the two words tend to be related to the same topics, i.e. their topic probabilities are correlated. In contrast, a cannot-link relation between two words indicates that these two words are not topically similar, and they should not both be prominent within the same topic. For example, “quarterback” and “fumble” are both related to American football, so they can share a must-link relation. But “fumble” and “bank” imp"
D15-1037,P14-1110,1,0.706166,"ent increases. SC-LDA exhibits greater speedup with 5 For 20NG dataset, it may overfit the data with 500 topics, but here we use it to demonstrate the scalability. 7 314 MCMC inference (Mimno et al., 2012). In this paper, we only focus on single-processor learning, but existing parallelization techniques (Newman et al., 2009) are applicable to our model. At the intersection lies models that improve the scalability of upstream topic model inference. In addition to our SC-LDA, Hu and Boyd-Graber (2012) speed Gibbs sampling in tree-based topic models using SparseLDA’s factorization strategy, and Hu et al. (2014) extend this approach by parallelizing global parameter updates using variational inference. Our work is more general (also encompassing document-based constraints) and is faster. In contrast to these upstream models, Zhu et al. (2013) and Nguyen et al. (2015) improve inference of downstream models. Adding knowledge and metadata to topic models makes the models richer, more understandable, and more domain-specific. A common distinction is upstream (conditioning on metadata) vs. downstream models (conditioning on variables already present in a topic model to predict metadata) (Mimno et al., 200"
D15-1037,D07-1109,1,0.376818,"Missing"
D15-1037,P09-2074,0,0.056563,"Missing"
D15-1037,D11-1024,0,0.0720074,"porate different numbers of word correlations # Word Correlations C0 C100 C500 2.02 2.14 2.30 0.53 0.56 0.58 0.48 0.50 0.53 0.48 0.49 0.52 C1000 2.50 0.62 0.56 0.56 Table 2: SC-LDA runtime (in seconds) in the 1st, 50th, 100th, and 200th iteration with different numbers of correlations. in SC-LDA. SC-LDA runs faster as sampling proceeds as the sparsity increases, but additional correlations slow the model. 3.5 Topic Coherence Topic models are often evaluated using perplexity on held-out test data, but this evaluation is of6 313 ten at odds with human evaluations (Chang et al., 2009). Following Mimno et al. (2011), we employ Topic Coherence—a metric that is consistent with human judgment—to measure a topic model’s quality. Topic t’s coherence is defined (t) (t) P Pm−1 F (vm ,vl )+ as C(t : V (t) ) = M log , (t) m=2 l=1 F (vl ) where F (v) is the document frequency of word type v, F (v, v 0 ) is the co-document frequency of (t) (t) word type v and v 0 , and V (t) = (v1 , ..., vM ) is a list of the M most probable words in topic t. In our experiments, we choose the ten words with highest probability in the topic to compute topic coherence, i.e., M = 10. Mimno et al. (2011) use  = 1, but R¨oder et al. ("
D15-1037,P15-1075,1,0.83317,"tional Gibbs sampling takes too long to finish. For standard LDA, recently introduced fast sampling methods (Yao et al., 2009; Li et al., 2014; Yuan et al., 2015) enable industrial applications of topic modeling to search engines and online advertising, where capturing the “long tail” of infrequently used topics requires large topic spaces. For example, while typical LDA models in academic papers have up to 103 topics, industrial applications with 105 –106 topics are common (Wang et al., 2014). Moreover, scaling topic models to many topics can also reveal the hierarchical structure of topics (Downey et al., 2015). Thus, there is a need for topic models that can both benefit from rich prior information and that can scale to large datasets. However, existing methods for improving scalability focus on topic models without prior information. To rectify this, we propose a factor graph model that encodes a potential function over the hidden topic variables, encouraging topics consistent with prior knowledge. The factor model representation admits an efficient sampling algorithm that takes advantage of the model’s sparsity. We show that our method achieves comparable performance but runs significantly faster"
D15-1037,N15-1074,0,0.485055,"each topic) because we need the summation of P (z = t) for sampling. Therefore, the critical part of speeding up the sampler is finding a sparse representation of the second term. In the following sections, we show that natural, sparse prior knowledge representations are possible. We first present an efficient sparse representation of word correlation prior knowledge and then one for document-label knowledge. 2.3 We now illustrate how we can encode word correlation knowledge as a set of sparse constraints fm (z, w, d) in our model. In previous work (Andrzejewski et al., 2009; Hu et al., 2011; Xie et al., 2015), word correlation prior knowledge is represented as word must-link constraints and cannotlink constraints. A must-link relation between two words indicates that the two words tend to be related to the same topics, i.e. their topic probabilities are correlated. In contrast, a cannot-link relation between two words indicates that these two words are not topically similar, and they should not both be prominent within the same topic. For example, “quarterback” and “fumble” are both related to American football, so they can share a must-link relation. But “fumble” and “bank” imply two different to"
D15-1037,D09-1026,0,0.586854,"et’s return to the question whether Equation 6 is sparse, allowing efficient computation of Equation 7. Fortunately, nu,t and nv,t , which are the 2: 3: 4: 5: 6: 4 311 compute st , rt , qt with SparseLDA, (see Eq. 3) for t ← 0 to T do update st , rt , qt . ∀u ∈ Mw if nu,t &gt; λ end for p(t) = st + rt + qt sample new topic assignment for w from p(t) 2.4 DATASET NIPS NYT-N EWS 20NG Other Types of Prior Knowledge The factor model framework can also handle other types of prior knowledge, such as document labels, sentence labels, and document link relations. We briefly describe document labels here. Ramage et al. (2009) propose Labeled-LDA, which improves LDA with document labels. It assumes that there is a one-to-one mapping between topics and labels, and it restricts each document’s topics to be sampled only from those allowed by the documents label set. Therefore, Labeled-LDA can be expressed in our model. We define ( 1, if z ∈ md fm (z, w, d) = (8) −∞, else 3.1 3.2 able 1,900,000 100,000,000 1,946,000 Dataset Prior Knowledge Generation Word Correlation Prior Knowledge Previous work proposes two methods to automatically generate prior word correlation knowledge from external sources. Hu and Boyd-Graber (2"
D15-1037,N15-1076,1,\N,Missing
D15-1237,N10-1145,0,0.283244,"re no correct sentences, enabling researchers to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the W IKI QA dataset. 1 Q: How did Seminole war end? A: Ultimately, the Spanish Crown ceded the colony to United States rule. Introduction Answer sentence selection is a crucial subtask of the open-domain question answering (QA) problem, with the goal of extracting answers from a set of pre-selected sentences (Heilman and Smith, 2010; Yao et al., 2013; Severyn and Moschitti, 2013). In order to conduct research on this important problem, Wang et al. (2007) created a dataset, which we refer to by QAS ENT, based on the TREC-QA data. The QAS ENT dataset chose questions in TREC 8-13 QA tracks and selected sentences that share one or more non-stopwords from the questions. Although QAS ENT has since ∗ Work conducted while interning at Microsoft Research. One significant concern with this approach is that the lexical overlap will make sentence selection easier for the QAS ENT dataset and might inflate the performance of existing"
D15-1237,C02-1150,0,0.499864,"ching methods: Word Count and Weighted Word Count. The first method counts the number of non-stopwords in the question that also occur in the answer sentence. The second method re-weights the counts by the IDF values of the question words. We reimplement LCLR (Yih et al., 2013), an answer sentence selection approach that achieves very competitive results on QAS ENT. LCLR sentences that have human annotations. 4 The classifier is trained using a logistic regression model on the UIUC Question Classification Datasets (http: //cogcomp.cs.illinois.edu/Data/QA/QC). The performance is comparable to (Li and Roth, 2002). 2015 Model Word Cnt Wgt Word Cnt LCLR PV CNN PV-Cnt CNN-Cnt QAS ENT W IKI QA MAP MRR MAP MRR 0.5919 0.6095 0.6954 0.5213 0.5590 0.6762 0.6951 0.6662 0.6746 0.7617 0.6023 0.6230 0.7514 0.7633 0.4891 0.5099 0.5993 0.5110 0.6190 0.5976 0.6520 0.4924 0.5132 0.6086 0.5160 0.6281 0.6058 0.6652 Table 4: Baseline results on both QAS ENT and W IKI QA datasets. Questions without correct answers in the candidate sentences are removed in the W IKI QA dataset. The best results are in bold. makes use of rich lexical semantic features, including word/lemma matching, WordNet and vector-space lexical semanti"
D15-1237,D13-1044,0,0.142527,"rs to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the W IKI QA dataset. 1 Q: How did Seminole war end? A: Ultimately, the Spanish Crown ceded the colony to United States rule. Introduction Answer sentence selection is a crucial subtask of the open-domain question answering (QA) problem, with the goal of extracting answers from a set of pre-selected sentences (Heilman and Smith, 2010; Yao et al., 2013; Severyn and Moschitti, 2013). In order to conduct research on this important problem, Wang et al. (2007) created a dataset, which we refer to by QAS ENT, based on the TREC-QA data. The QAS ENT dataset chose questions in TREC 8-13 QA tracks and selected sentences that share one or more non-stopwords from the questions. Although QAS ENT has since ∗ Work conducted while interning at Microsoft Research. One significant concern with this approach is that the lexical overlap will make sentence selection easier for the QAS ENT dataset and might inflate the performance of existing systems in more natural settings. For instance,"
D15-1237,D07-1003,0,0.502781,"eral systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the W IKI QA dataset. 1 Q: How did Seminole war end? A: Ultimately, the Spanish Crown ceded the colony to United States rule. Introduction Answer sentence selection is a crucial subtask of the open-domain question answering (QA) problem, with the goal of extracting answers from a set of pre-selected sentences (Heilman and Smith, 2010; Yao et al., 2013; Severyn and Moschitti, 2013). In order to conduct research on this important problem, Wang et al. (2007) created a dataset, which we refer to by QAS ENT, based on the TREC-QA data. The QAS ENT dataset chose questions in TREC 8-13 QA tracks and selected sentences that share one or more non-stopwords from the questions. Although QAS ENT has since ∗ Work conducted while interning at Microsoft Research. One significant concern with this approach is that the lexical overlap will make sentence selection easier for the QAS ENT dataset and might inflate the performance of existing systems in more natural settings. For instance, Yih et al. (2013) find that simple word matching methods outperform many sop"
D15-1237,N13-1106,0,0.622424,"Missing"
D15-1237,P13-1171,1,0.917497,". In order to conduct research on this important problem, Wang et al. (2007) created a dataset, which we refer to by QAS ENT, based on the TREC-QA data. The QAS ENT dataset chose questions in TREC 8-13 QA tracks and selected sentences that share one or more non-stopwords from the questions. Although QAS ENT has since ∗ Work conducted while interning at Microsoft Research. One significant concern with this approach is that the lexical overlap will make sentence selection easier for the QAS ENT dataset and might inflate the performance of existing systems in more natural settings. For instance, Yih et al. (2013) find that simple word matching methods outperform many sophisticated approaches on the dataset. We explore this possibility in Section 3. A second, more subtle challenge for question answering is that it normally assumes that there is at least one correct answer for each question in the candidate sentences. During the data construction procedures, all the questions without correct answers are manually discarded.1 We address a new challenge of answer triggering, an important component in QA systems, where the goal is to detect whether there exist correct answers in the set of candidate sentenc"
D16-1152,E06-1002,0,0.068515,"disambiguates entities for mentions such as ‘Sox’ (Boston Red Sox vs. Chicago White Sox), ‘Sanders’ (Bernie Sanders vs. Barry Sanders), and ‘Memphis’ (Memphis Grizzlies vs. Memphis, Tennessee), which are mistakenly linked to the other entities or Nil by the mentionentity model. Another example is that the social network information helps the system correctly link ‘Kim’ to Lil’ Kim instead of Kim Kardashian, despite that the latter entity’s wikipedia page is considerably more popular. 6 Related Work Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambigua1459 tion is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities throug"
D16-1152,D07-1074,0,0.0153479,"or mentions such as ‘Sox’ (Boston Red Sox vs. Chicago White Sox), ‘Sanders’ (Bernie Sanders vs. Barry Sanders), and ‘Memphis’ (Memphis Grizzlies vs. Memphis, Tennessee), which are mistakenly linked to the other entities or Nil by the mentionentity model. Another example is that the social network information helps the system correctly link ‘Kim’ to Lil’ Kim instead of Kim Kardashian, despite that the latter entity’s wikipedia page is considerably more popular. 6 Related Work Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambigua1459 tion is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagat"
D16-1152,Q14-1021,1,0.880186,"performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagation. Shen et al. (2013) employ Twitter user account information to improve entity linking, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Fang and Chang (2014) demonstrate that spatial and temporal signals are critical for the task, and they advance the performance by associating entity prior distributions with different timestamps and locations. Our work overcomes the difficulty by leveraging social relations — socially connected individuals are assumed to share similar interests on entities. As the Twitter post information is often sparse for some users, our assumption enables the utilization of more relevant information that helps to improve the task. NLP with social relations Most previous work on incorporating social relations for NLP problems"
D16-1152,N13-1122,1,0.94033,"ce on embedding information networks (Tang et al., 2015), which induces low-dimensional representations for author nodes based on the network structure. By learning the semantic interactions between the author embeddings and the pre-trained Freebase entity embeddings, the entity linking system can incorporate more disambiguating context from the social network. We also consider lowdimensional representations of mentions, another source of related information for entity linking, with the intuition that semantically related mentions can refer to similar entities. Previously proposed approaches (Guo et al., 2013a; Yang and Chang, 2015) are based on hand-crafted features and off-the-shelf machine learning algorithms. Our preliminary study suggests that simply augmenting the traditional surface features with the distributed representations barely improves the performance of these entity linking systems. Therefore, we propose N TEL, a Neural model for Tweet Entity Linking, to leverage the distributed representations of authors, mentions, and entities. N TEL can not only make efficient use of statistical surface features built from a knowledge base, but also learn the interactions between these distribut"
D16-1152,D13-1085,0,0.619753,"ce on embedding information networks (Tang et al., 2015), which induces low-dimensional representations for author nodes based on the network structure. By learning the semantic interactions between the author embeddings and the pre-trained Freebase entity embeddings, the entity linking system can incorporate more disambiguating context from the social network. We also consider lowdimensional representations of mentions, another source of related information for entity linking, with the intuition that semantically related mentions can refer to similar entities. Previously proposed approaches (Guo et al., 2013a; Yang and Chang, 2015) are based on hand-crafted features and off-the-shelf machine learning algorithms. Our preliminary study suggests that simply augmenting the traditional surface features with the distributed representations barely improves the performance of these entity linking systems. Therefore, we propose N TEL, a Neural model for Tweet Entity Linking, to leverage the distributed representations of authors, mentions, and entities. N TEL can not only make efficient use of statistical surface features built from a knowledge base, but also learn the interactions between these distribut"
D16-1152,P15-1073,0,0.0584107,"mpossible to disambiguate between these entities solely based on the individual text message. We propose to overcome the difficulty and improve the entity disambiguation capability of the entity linking system by employing social network structures. The sociological theory of homophily asserts that socially connected individuals are more likely to have similar behaviors or share similar interests (McPherson et al., 2001). This property has been used to improve many natural language processing tasks such as sentiment analysis (Tan et al., 2011; Yang and Eisenstein, 2015), topic classification (Hovy, 2015) and user attribute inference (Li et al., 2015). We assume Twitter users will have similar interests in real world entities to their near neighbors — an assumption of entity homophily — which 1452 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1452–1461, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics is demonstrated in Figure 1. The social relation between users u1 and u2 may lead to more coherent topics in tweets t1 and t2 . Therefore, by successfully linking the less ambiguous mention ‘Red Sox’ in tweet t2 to"
D16-1152,P14-1036,0,0.01724,"age is considerably more popular. 6 Related Work Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambigua1459 tion is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagation. Shen et al. (2013) employ Twitter user account information to improve entity linking, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Fang and Chang (2014) demonstrate that spatial and temporal signals are critical for the task, and they advance the performance by associating entity prior distributions with different timestamps and locations. Our work overcomes the difficul"
D16-1152,N15-1142,0,0.0125348,"mes that Twitter users sharing many neighbors are close to each other in the embedding space. According to the original paper, the second-order proximity yields slightly better performances than the firstorder proximity, which assumes connecting users are close to each other, on a variety of downstream tasks. be written as: Mention embeddings The representation of a mention is the average of embeddings of words it contains. As each mention is typically one to three words, the simple representations often perform surprisingly well (Socher et al., 2013). We adopt the structured skip-gram model (Ling et al., 2015) to learn the word embeddings E(w) on a Twitter corpus with 52 million tweets (Owoputi et al., 2013). The mention vector of the t-th mention candidate can be written as: X 1 (m) (w) vt = (w) vw , (4) |xt | (w) where W(u,e) and W(m,e) are D(u) × D(e) and D(w) × D(e) bilinear transformation matrices. Similar bilinear formulation has been used in the literature of knowledge base completion and inference (Socher et al., 2013; Yang et al., 2014). The parameters of the composition model are Θ2 = {W(u,e) , W(m,e) , E(u) , E(w) , E(e) }. w∈xt (w) where xt is the set of words in the mention. Entity emb"
D16-1152,N13-1039,0,0.0370358,"Missing"
D16-1152,W10-0510,1,0.916876,"Missing"
D16-1152,D11-1141,0,0.0551077,"onger social network ties than directed links (Kwak et al., 2010; Wu et al., 2011). The numbers of social relations for the networks are 1,604, 379 and 342 respectively. 1 We are able to obtain at most 3,200 tweets for each Twitter user, due to the Twitter API limits. Network F OLLOWER M ENTION R ETWEET sim(i ↔ j) sim(i ↔ / j) 0.128 0.121 0.173 0.025 0.025 0.025 Table 2: The average entity-driven similarity results for the networks. Metrics We propose to use the entity-driven similarity between authors to test the hypothesis of entity homophily. For a user ui , we employ a Twitter NER system (Ritter et al., 2011) to detect entity mentions in the timeline, which we use to construct (ent) (ent) a user entity vector ui , so that ui,j = 1 iff user i has mentioned entity j.2 The entity-driven similarity between two users ui and uj is defined as the cosine similarity score between the vectors (ent) (ent) ui and uj . We evaluate the three networks by calculating the average entity-driven similarity of the connected user pairs and that of the disconnected user pairs, which we name as sim(i ↔ j) and sim(i ↔ / j). Results The entity-driven similarity results of these networks are presented in Table 2. As shown,"
D16-1152,W11-2207,0,0.106461,"work overcomes the difficulty by leveraging social relations — socially connected individuals are assumed to share similar interests on entities. As the Twitter post information is often sparse for some users, our assumption enables the utilization of more relevant information that helps to improve the task. NLP with social relations Most previous work on incorporating social relations for NLP problems focuses on Twitter sentiment analysis, where the existence of social relations between users is considered as a clue that the sentiment polarities of messages from the users should be similar. Speriosu et al. (2011) construct a heterogeneous network with tweets, users, and n-grams as nodes, and the sentiment label distributions associated with the nodes are refined by performing label propagation over social relations. Tan et al. (2011) and Hu et al. (2013) leverage social relations for sentiment analysis by exploiting a factor graph model and the graph Laplacian technique respectively, so that the tweets belonging to social connected users share similar label distributions. We work on entity linking in Twitter messages, where the label space is much larger than that of sentiment classification. The soci"
D16-1152,P15-1049,1,0.811948,"ormation networks (Tang et al., 2015), which induces low-dimensional representations for author nodes based on the network structure. By learning the semantic interactions between the author embeddings and the pre-trained Freebase entity embeddings, the entity linking system can incorporate more disambiguating context from the social network. We also consider lowdimensional representations of mentions, another source of related information for entity linking, with the intuition that semantically related mentions can refer to similar entities. Previously proposed approaches (Guo et al., 2013a; Yang and Chang, 2015) are based on hand-crafted features and off-the-shelf machine learning algorithms. Our preliminary study suggests that simply augmenting the traditional surface features with the distributed representations barely improves the performance of these entity linking systems. Therefore, we propose N TEL, a Neural model for Tweet Entity Linking, to leverage the distributed representations of authors, mentions, and entities. N TEL can not only make efficient use of statistical surface features built from a knowledge base, but also learn the interactions between these distributed representations. Our"
D16-1152,P15-1128,1,0.786133,"Missing"
D18-1109,P15-1026,0,0.0314412,"ge processing tasks. In this work, we model convolution filters with RNNs that naturally capture compositionality and long-term dependencies in language. We show that simple CNN architectures equipped with recurrent neural filters (RNFs) achieve results that are on par with the best published ones on the Stanford Sentiment Treebank and two answer sentence selection datasets.1 1 Introduction Convolutional neural networks (CNNs) have been shown to achieve state-of-the-art results on various natural language processing (NLP) tasks, such as sentence classification (Kim, 2014), question answering (Dong et al., 2015), and machine translation (Gehring et al., 2017). In an NLP system, a convolution operation is typically a sliding window function that applies a convolution filter to every possible window of words in a sentence. Hence, the key components of CNNs are a set of convolution filters that compose low-level word features into higher-level representations. Convolution filters are usually realized as linear systems, as their outputs are affine transformations of the inputs followed by some non-linear activation functions. Prior work directly adopts a linear convolution filter to NLP problems by utili"
D18-1109,D15-1237,1,0.932658,"e. Second, they use separate parameters for each value of the time index, which hinders parameter sharing for the same word type (Goodfellow et al., 2016). The assumptions become more problematic if we increase the window size m. We propose to address the limitations by employing RNNs to realize convolution filters, which we term recurrent neural filters (RNFs). RNFs compose the words of the m-gram from left to right using the same recurrent unit: ht = RNN(ht−1 , xt ), CNN architectures Sentence matching We exploit a CNN architecture that is nearly identical to the CNN-Cnt model introduced by Yang et al. (2015). Let v1 and v2 be the vector representations of the two sentences. A bilinear function is applied to v1 and v2 to produce a sentence matching score. The score is combined with two word matching count features and fed into a sigmoid layer. The output of the sigmoid layer is used by binary cross-entropy loss to optimize the model. 3 Experiments We evaluate RNFs on some of the most popular datasets for the sentence classification and sentence matching tasks. After describing the experimental setup, we compare RNFs against both linear filters and conventional RNN models, and report our findings i"
D18-1109,W13-3214,0,0.0151258,"e specifically derived for NLP tasks, in which a different form of non-linearity, language compositionality, often plays a critical role. Several works have employed neural network architectures that contain both CNN and RNN components to tackle NLP problems. Tan et al. (2016) present a deep neural network for answer sentence selection, in which a convolution layer is applied to the output of a BiLSTM layer for extracting sentence representations. Ma and Hovy (2016) propose to compose character representations of a word using a CNN, whose output is then fed into a BiLSTM for sequence tagging. Kalchbrenner and Blunsom (2013) introduce a neural architecture that uses a sentence model based on CNNs and a discourse model based on RNNs. Their system achieves state-of-the-art results on the task of dialogue act classification. Instead of treating an RNN and a CNN as isolated components, our work directly marries RNNs with the convolution operation, which illustrates a new direction in mixing RNNs with CNNs. 6 7 Acknowledgments We thank Chunyang Xiao for helping us to run the analysis experiments. We thank Kazi Shefaet Rahman, Ozan Irsoy, Chen-Tse Tsai, and Lingjia Deng for their valuable comments on earlier versions o"
D18-1109,D14-1181,0,0.0343902,"ten warranted for natural language processing tasks. In this work, we model convolution filters with RNNs that naturally capture compositionality and long-term dependencies in language. We show that simple CNN architectures equipped with recurrent neural filters (RNFs) achieve results that are on par with the best published ones on the Stanford Sentiment Treebank and two answer sentence selection datasets.1 1 Introduction Convolutional neural networks (CNNs) have been shown to achieve state-of-the-art results on various natural language processing (NLP) tasks, such as sentence classification (Kim, 2014), question answering (Dong et al., 2015), and machine translation (Gehring et al., 2017). In an NLP system, a convolution operation is typically a sliding window function that applies a convolution filter to every possible window of words in a sentence. Hence, the key components of CNNs are a set of convolution filters that compose low-level word features into higher-level representations. Convolution filters are usually realized as linear systems, as their outputs are affine transformations of the inputs followed by some non-linear activation functions. Prior work directly adopts a linear con"
D18-1109,P16-1101,0,0.0242885,"ution filter is developed in the context of a computational model of the visual cortex, which is not suitable for NLP problems. In contrast, RNFs are specifically derived for NLP tasks, in which a different form of non-linearity, language compositionality, often plays a critical role. Several works have employed neural network architectures that contain both CNN and RNN components to tackle NLP problems. Tan et al. (2016) present a deep neural network for answer sentence selection, in which a convolution layer is applied to the output of a BiLSTM layer for extracting sentence representations. Ma and Hovy (2016) propose to compose character representations of a word using a CNN, whose output is then fed into a BiLSTM for sequence tagging. Kalchbrenner and Blunsom (2013) introduce a neural architecture that uses a sentence model based on CNNs and a discourse model based on RNNs. Their system achieves state-of-the-art results on the task of dialogue act classification. Instead of treating an RNN and a CNN as isolated components, our work directly marries RNNs with the convolution operation, which illustrates a new direction in mixing RNNs with CNNs. 6 7 Acknowledgments We thank Chunyang Xiao for helpin"
D18-1109,D14-1162,0,0.0843458,"ine-grained classification settings. Two answer sentence selection datasets, QASent (Wang et al., 2007) and WikiQA (Yang et al., 2015), are adopted in our sentence matching experiments. We use MAP and MRR to evaluate the performance of answer sentence selection models. Competitive systems We consider CNN variants with linear filters and RNFs. For RNFs, we 913 adopt two implementations based on GRUs and LSTMs respectively. We also compare against the following RNN variants: GRU, LSTM, GRU with max pooling, and LSTM with max pooling.2 We use the publicly available 300-dimensional GloVe vectors (Pennington et al., 2014) pre-trained with 840 billion tokens to initialize the word embeddings for all the models. The word vectors are fixed during downstream training. Finally, we report best published results for each dataset.3 System Binary 48.0 53.0 53.4 86.1 90.0 90.0 50.5 50.3 51.7 51.6 88.7 89.3 89.7 89.8 53.2 89.9 CNN variants CNN-linear-filter CNN-RNF-GRU CNN-RNF-LSTM RNN variants GRU LSTM GRU-maxpool LSTM-maxpool Parameter tuning For all the experiments, we tune hyperparameters on the development sets and report results obtained with the selected hyperparameters on the test sets. After the preliminary sear"
D18-1109,D13-1170,0,0.00748873,"or Gated Recurrent Unit (GRU) (Cho et al., 2014). We use the last hidden state hi+m−1 as the RNF output feature vector ci . Features learned by RNFs are interdependent of each other, which permits the learning of complementary information about the word sequence. The left-to-right word composing procedure in RNFs preserves word order information and implicitly models long-term dependencies in language. RNFs can be treated as simple dropin replacements of linear filters and potentially adopted in numerous CNN architectures. 3.1 Experimental settings Data We use the Stanford Sentiment Treebank (Socher et al., 2013) in our sentence classification experiments. We report accuracy results for both binary classification and fine-grained classification settings. Two answer sentence selection datasets, QASent (Wang et al., 2007) and WikiQA (Yang et al., 2015), are adopted in our sentence matching experiments. We use MAP and MRR to evaluate the performance of answer sentence selection models. Competitive systems We consider CNN variants with linear filters and RNFs. For RNFs, we 913 adopt two implementations based on GRUs and LSTMs respectively. We also compare against the following RNN variants: GRU, LSTM, GRU"
D18-1109,D07-1003,0,0.0182047,"complementary information about the word sequence. The left-to-right word composing procedure in RNFs preserves word order information and implicitly models long-term dependencies in language. RNFs can be treated as simple dropin replacements of linear filters and potentially adopted in numerous CNN architectures. 3.1 Experimental settings Data We use the Stanford Sentiment Treebank (Socher et al., 2013) in our sentence classification experiments. We report accuracy results for both binary classification and fine-grained classification settings. Two answer sentence selection datasets, QASent (Wang et al., 2007) and WikiQA (Yang et al., 2015), are adopted in our sentence matching experiments. We use MAP and MRR to evaluate the performance of answer sentence selection models. Competitive systems We consider CNN variants with linear filters and RNFs. For RNFs, we 913 adopt two implementations based on GRUs and LSTMs respectively. We also compare against the following RNN variants: GRU, LSTM, GRU with max pooling, and LSTM with max pooling.2 We use the publicly available 300-dimensional GloVe vectors (Pennington et al., 2014) pre-trained with 840 billion tokens to initialize the word embeddings for all"
D19-1413,D18-1547,0,0.0919532,"nt-induction. virtual assistants such as Apple Siri, Amazon Alexa, or Google Assistant. The first step towards building such systems is to determine the target tasks and construct corresponding ontologies to define the constrained set of dialog states and actions (Henderson et al., 2014b; Mrkˇsi´c et al., 2015). Existing work assumes the target tasks are given and excludes dialog intent discovery from the dialog system design pipeline. Because of this, most of the works focus on few simple dialog intents and fail to explore the realistic complexity of user intent space (Williams et al., 2013; Budzianowski et al., 2018). The assumption puts a great limitation on adapting goal-oriented dialog systems to important but complex domains like customer support and healthcare where having a complete view of user intents is impossible. For example, as shown in Fig. 1, it is non-trivial to predict user intents for troubleshooting a newly released product in advance. To address this problem, we propose to employ data-driven approaches to automatically discover user intents in dialogs from human-human conversations. Follow-up analysis can then be performed to identify the most valuable dialog intents and design dialog s"
D19-1413,D16-1164,0,0.10595,"web queries or user questions together using supervised or unsupervised clustering techniques. Kathuria et al. (2010) perform simple k-means clustering on a variety of query traits to understand user intents. Cheung and Li (2012) present an unsupervised method for query intent clustering that produces a pattern consisting of a sequence of semantic concepts and/or lexical items for each intent. Jeon et al. (2005) use machine translation to estimate word translation probabilities and retrieve similar questions from question archives. A variation of k-means algorithm, MiXKmeans, is presented by Deepak (2016) to cluster threads that present on forums and Community Question Answering websites. Haponchyk et al. (2018) propose to cluster questions into intents using a supervised learning method that yields better semantic similarity modeling. Our work focuses on a related but different task that automatically induces user intents for building dialog systems. Two sources of information are naturally available for exploring our deep multi-view clustering approach. Multi-view clustering Multi-view clustering (MVC) aims at grouping similar subjects into the same cluster by combining the available multivi"
D19-1413,N19-1423,0,0.00875303,"resentations learned by the representation learning methods (k-means only requires query-view representations). In the case where a content-view input corresponds to multiple utterances, we take the average of the utterance vectors as the content-view output representation for autoencoders and quick thoughts. AV-K MEANS is a joint representation learning and multiview clustering method. Therefore, we compare with SOTA representation learning methods autoencoders, and quick thoughts (Logeswaran and Lee, 2018). Quick thoughts is a strong representation learning baseline that is adopted in BERT (Devlin et al., 2019). We also include principal component analysis (PCA), a classic representation learning and dimensionality reduction method, since bag-of-words representations are too expensive to work with for clustering 8 We use the scikit-learn k-means implementation and the MVSC implementation available at: https://pypi. org/project/multiview/. analysis. We compare three variants of AV-K MEANS that differ in the pretraining strategies. In addition to the AV-K MEANS systems pretrained with autoencoders and quick thoughts, we also consider a system whose encoder parameters are randomly initialized (no pretr"
D19-1413,D18-1254,0,0.250999,"o expensive to work with for clustering 8 We use the scikit-learn k-means implementation and the MVSC implementation available at: https://pypi. org/project/multiview/. analysis. We compare three variants of AV-K MEANS that differ in the pretraining strategies. In addition to the AV-K MEANS systems pretrained with autoencoders and quick thoughts, we also consider a system whose encoder parameters are randomly initialized (no pretraining). Metrics We compare the competitive approaches on a number of standard evaluation measures for clustering analysis. Following prior work (Kumar et al., 2011; Haponchyk et al., 2018; Xie et al., 2016), we set the number of clusters to the number of ground truth categories and report precision, recall, F1 score, and unsupervised clustering accuracy (ACC). To compute precision or recall, we assign each predicted cluster to the most frequent gold cluster or assign each gold cluster to the most frequent predicted cluster respectively. The F1 score is the harmonic average of the precision and recall. ACC uses a one-to-one assignment between the gold standard clusters and the predicted clusters. The assignment can be efficiently computed by the Hungarian algorithm (Kuhn, 1955)"
D19-1413,H90-1021,0,0.052728,"ches.2 1 Customer 2: hey man I lost and miss my airpods plz help me! Agent 2: Hi there! With iOS 10.3 or later, Find My iPhone can help you locate missing AirPods. Figure 1: Two dialogs with the FindAirPods user intent. The user query utterances of the two dialogs are lexically and syntactically dissimilar, while the rests of the dialogs are similar. Introduction Goal-oriented dialog systems assist users to accomplish well-defined tasks with clear intents within a limited number of dialog turns. They have been adopted in a wide range of applications, including booking flights and restaurants (Hemphill et al., 1990; Williams, 2012), providing tourist information (Kim et al., 2016), aiding in the customer support domain, and powering intelligent 1 We focus on inducing abstract intents like BookFlight and ignore detailed arguments such as departure date and destination. 2 When ready, the data and code will be published at https://github.com/asappresearch/ dialog-intent-induction. virtual assistants such as Apple Siri, Amazon Alexa, or Google Assistant. The first step towards building such systems is to determine the target tasks and construct corresponding ontologies to define the constrained set of dialo"
D19-1413,W14-4337,0,0.183006,"viding tourist information (Kim et al., 2016), aiding in the customer support domain, and powering intelligent 1 We focus on inducing abstract intents like BookFlight and ignore detailed arguments such as departure date and destination. 2 When ready, the data and code will be published at https://github.com/asappresearch/ dialog-intent-induction. virtual assistants such as Apple Siri, Amazon Alexa, or Google Assistant. The first step towards building such systems is to determine the target tasks and construct corresponding ontologies to define the constrained set of dialog states and actions (Henderson et al., 2014b; Mrkˇsi´c et al., 2015). Existing work assumes the target tasks are given and excludes dialog intent discovery from the dialog system design pipeline. Because of this, most of the works focus on few simple dialog intents and fail to explore the realistic complexity of user intent space (Williams et al., 2013; Budzianowski et al., 2018). The assumption puts a great limitation on adapting goal-oriented dialog systems to important but complex domains like customer support and healthcare where having a complete view of user intents is impossible. For example, as shown in Fig. 1, it is non-trivia"
D19-1413,W14-4340,0,0.242057,"viding tourist information (Kim et al., 2016), aiding in the customer support domain, and powering intelligent 1 We focus on inducing abstract intents like BookFlight and ignore detailed arguments such as departure date and destination. 2 When ready, the data and code will be published at https://github.com/asappresearch/ dialog-intent-induction. virtual assistants such as Apple Siri, Amazon Alexa, or Google Assistant. The first step towards building such systems is to determine the target tasks and construct corresponding ontologies to define the constrained set of dialog states and actions (Henderson et al., 2014b; Mrkˇsi´c et al., 2015). Existing work assumes the target tasks are given and excludes dialog intent discovery from the dialog system design pipeline. Because of this, most of the works focus on few simple dialog intents and fail to explore the realistic complexity of user intent space (Williams et al., 2013; Budzianowski et al., 2018). The assumption puts a great limitation on adapting goal-oriented dialog systems to important but complex domains like customer support and healthcare where having a complete view of user intents is impossible. For example, as shown in Fig. 1, it is non-trivia"
D19-1413,D18-1131,0,0.0416827,"Missing"
D19-1413,W12-1812,0,0.0140774,"y man I lost and miss my airpods plz help me! Agent 2: Hi there! With iOS 10.3 or later, Find My iPhone can help you locate missing AirPods. Figure 1: Two dialogs with the FindAirPods user intent. The user query utterances of the two dialogs are lexically and syntactically dissimilar, while the rests of the dialogs are similar. Introduction Goal-oriented dialog systems assist users to accomplish well-defined tasks with clear intents within a limited number of dialog turns. They have been adopted in a wide range of applications, including booking flights and restaurants (Hemphill et al., 1990; Williams, 2012), providing tourist information (Kim et al., 2016), aiding in the customer support domain, and powering intelligent 1 We focus on inducing abstract intents like BookFlight and ignore detailed arguments such as departure date and destination. 2 When ready, the data and code will be published at https://github.com/asappresearch/ dialog-intent-induction. virtual assistants such as Apple Siri, Amazon Alexa, or Google Assistant. The first step towards building such systems is to determine the target tasks and construct corresponding ontologies to define the constrained set of dialog states and acti"
D19-1413,W13-4065,0,0.0250015,"ppresearch/ dialog-intent-induction. virtual assistants such as Apple Siri, Amazon Alexa, or Google Assistant. The first step towards building such systems is to determine the target tasks and construct corresponding ontologies to define the constrained set of dialog states and actions (Henderson et al., 2014b; Mrkˇsi´c et al., 2015). Existing work assumes the target tasks are given and excludes dialog intent discovery from the dialog system design pipeline. Because of this, most of the works focus on few simple dialog intents and fail to explore the realistic complexity of user intent space (Williams et al., 2013; Budzianowski et al., 2018). The assumption puts a great limitation on adapting goal-oriented dialog systems to important but complex domains like customer support and healthcare where having a complete view of user intents is impossible. For example, as shown in Fig. 1, it is non-trivial to predict user intents for troubleshooting a newly released product in advance. To address this problem, we propose to employ data-driven approaches to automatically discover user intents in dialogs from human-human conversations. Follow-up analysis can then be performed to identify the most valuable dialog"
D19-1413,P15-2130,0,0.119557,"Missing"
D19-1413,D14-1162,0,0.0825441,"42.9 40.4 36.7 35.1 27.8 20.1 39.4 27.7 22.8 23.4 31.0 23.1 41.1 32.9 28.2 28.1 22.0 14.6 33.1 25.5 21.4 22.3 37.5 44.4 48.9 33.6 34.6 43.8 35.4 38.9 46.2 29.5 31.6 39.9 52.0 50.6 53.8 51.9 46.1 52.7 51.9 48.2 53.3 44.0 39.7 41.1 Table 2: Evaluation results on the TwACS and AskUbuntu datasets for different systems. MVSC is short for the multi-view spectral clustering algorithm proposed by Kanaan-Izquierdo et al. (2018). The pretrained representations are fixed during k-means and MVSC clustering and they are fine-tuned during AV-K MEANS clustering. The best results are in bold. GloVe vectors (Pennington et al., 2014) pretrained with 840 billion tokens to initialize the word embeddings for all the models. Competitive systems We consider state-of-theart methods for representation learning and/or multi-view clustering as our baseline systems. We formulate the dialog induction task as an unsupervised clustering task and include two popular clustering algorithms k-means and spectral clustering. multi-view spectral clustering (MVSC) (KanaanIzquierdo et al., 2018) is a competitive standard multi-view clustering approach.8 In particular, we carry out clustering using the query-view and content-view representation"
D19-3014,W06-1323,0,0.106194,"f the interaction, and the user’s per-utterance word count (averaged across all utterances). We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions. Text To Speech After NLG, we adjust the TTS of the system to improve the expressiveness of the voice to convey that the system is an engaged and active participant in the conversation. We use a rule-based system to systematically add interjections, specifically Alexa Speechcons, and fillers to approximate human-like cognitive-emotional expression (Tokuhisa and Terashima, 2006). For more on the framework and analysis of the TTS modifications, see (Cohn et al., 2019). 3 Response Depth: Mean Word Count Analysis From January 5, 2019 to March 5, 2019, we collected conversational data for Gunrock. During this time, no other code updates occurred. We analyzed conversations for Gunrock with at least 3 user turns to avoid conversations triggered by accident. Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 star"
D19-3014,W19-5935,1,0.809868,"ed the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions. Text To Speech After NLG, we adjust the TTS of the system to improve the expressiveness of the voice to convey that the system is an engaged and active participant in the conversation. We use a rule-based system to systematically add interjections, specifically Alexa Speechcons, and fillers to approximate human-like cognitive-emotional expression (Tokuhisa and Terashima, 2006). For more on the framework and analysis of the TTS modifications, see (Cohn et al., 2019). 3 Response Depth: Mean Word Count Analysis From January 5, 2019 to March 5, 2019, we collected conversational data for Gunrock. During this time, no other code updates occurred. We analyzed conversations for Gunrock with at least 3 user turns to avoid conversations triggered by accident. Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?”). Users engaged with Gunrock fo"
D19-3014,2021.eacl-main.94,1,0.845122,"Missing"
D19-3014,N18-5020,0,0.253208,"Ashwin Bhandare, Zhou Yu University of California, Davis {dianyu, mdcohn, yimyang, abtchen, wmwen, jpzhang, minzhou, krjesse}@ucdavis.edu {amchau, abhowmick, shriyer, s.giritheja, ssdavidson, asbhandar, joyu}@ucdavis.edu Abstract strategy to interleave task- and non-task functions in chatbots has been proposed (Rudnicky, 2019), no chatbots to our knowledge have employed a fact/opinion interleaving strategy. Finally, we use an extensive persona database to provide coherent profile information, a critical challenge in building social chatbots (Zhang et al., 2018). Compared to previous systems (Fang et al., 2018), Gunrock generates more balanced conversations between human and machine by encouraging and understanding more human inputs (see Table 1 for an example). Gunrock is the winner of the 2018 Amazon Alexa Prize, as evaluated by coherence and engagement from both real users and Amazonselected expert conversationalists. We focus on understanding complex sentences and having in-depth conversations in open domains. In this paper, we introduce some innovative system designs and related validation analysis. Overall, we found that users produce longer sentences to Gunrock, which are directly related to"
D19-3014,P18-1205,0,0.211022,"reenath Iyer, Giritheja Sreenivasulu, Sam Davidson, Ashwin Bhandare, Zhou Yu University of California, Davis {dianyu, mdcohn, yimyang, abtchen, wmwen, jpzhang, minzhou, krjesse}@ucdavis.edu {amchau, abhowmick, shriyer, s.giritheja, ssdavidson, asbhandar, joyu}@ucdavis.edu Abstract strategy to interleave task- and non-task functions in chatbots has been proposed (Rudnicky, 2019), no chatbots to our knowledge have employed a fact/opinion interleaving strategy. Finally, we use an extensive persona database to provide coherent profile information, a critical challenge in building social chatbots (Zhang et al., 2018). Compared to previous systems (Fang et al., 2018), Gunrock generates more balanced conversations between human and machine by encouraging and understanding more human inputs (see Table 1 for an example). Gunrock is the winner of the 2018 Amazon Alexa Prize, as evaluated by coherence and engagement from both real users and Amazonselected expert conversationalists. We focus on understanding complex sentences and having in-depth conversations in open domains. In this paper, we introduce some innovative system designs and related validation analysis. Overall, we found that users produce longer se"
I11-1042,N10-1020,0,0.0237827,"2010) proposed a real-time earthquake detection framework by treating each Twitter user as a sensor. Petrovic et al. (2010) addressed the problem of detecting new events from a stream of Twitter posts and adopted a method based on localitysensitive hashing to make event detection feasible on web-scale corpora. To facilitate fine-grained information extraction on news tweets, Liu et al. (2010) presented a work on semantic role labeling for such texts. Corvey et al. (2010) proposed a work for entity detection and entity class annotation on tweets that were posted during times of mass emergency. Ritter et al. (2010) proposed a topic model to detect conversational threads among tweets. Since a large amount of tweets are posted every day, ranking strategies is extremely important for users to find information quickly. Current ranking strategy on Twitter considers relevance to an input query, information recency (the latest tweets are preferred), and popularity (the retweet times by other users). The recency information, which is useful for real-time web search, has also been explored by Dong et al. (2010) who used fresh URLs present in tweets to rank documents in response to recency sensitive queries. Duan"
I11-1042,N10-1021,0,0.0848092,"Missing"
I11-1042,C10-1034,0,0.253764,"010) proposed a topic model to detect conversational threads among tweets. Since a large amount of tweets are posted every day, ranking strategies is extremely important for users to find information quickly. Current ranking strategy on Twitter considers relevance to an input query, information recency (the latest tweets are preferred), and popularity (the retweet times by other users). The recency information, which is useful for real-time web search, has also been explored by Dong et al. (2010) who used fresh URLs present in tweets to rank documents in response to recency sensitive queries. Duan et al. (2010) proposed a ranking SVM approach to rank tweets with various features. 3 Problem Formulation and Methodology Given a set of queries Q = {q1 , q2 , · · · , qn }, for each query qk , we have a set of short documents Dk = {d1k , d2k , · · · } which are retrieved by our builtin search engine. The document set Dk is partially labeled, i.e., a small portion of documents in Dk 1 2 were annotated with a category set C={1, 2, 3, 4, 5} where 5 means the highest quality and 1 lowest. Therefore, we denote Dk = DkU ∪ DkL , where DkU indicates the unlabeled documents, and DkL the labeled documents. Each doc"
I11-1042,W06-1650,0,0.0386186,"ed several factors on assessing review helpfulness including reviewer characteristics, reviewer history, and review readability and subjectivity. Lu et al. (2010) proposed a linear regression model with various social contexts for review quality prediction. The authors employed author consistency, trust consistency and co-citation consistency hypothesis to predict more consistently. Liu et al. (2008) studied three factors, i.e., reviewer expertise, writing style, and timeliness, and proposed a non-linear regression model with radial basis functions to predict the helpfulness of movie reviews. Kim et al. (2006) used SVM regression with various features to predict review helpfulness. Finding high-quality content and reliable users is also very important for question answering. Agichtein et al. (2008) proposed a classification framework of estimating answer quality. They studied content-based features (e.g. the answer length) and usage-based features derived from question answering communities. Jeon et al. (2006) used nontextual features extracted from the Naver Q&A service to predict the quality of answers. Bian et al. (2009) proposed a mutual reinforcement learning framework to simultaneously predic"
I11-1042,W10-0513,0,0.359402,"lity prediction has been a very important problem in many tasks. In review mining, quality prediction has two lines of research: one line is to detect spam reviews (Jindal and Liu, 2008) or spam reviewers (Lim et al., 2010), which is helpful to exclude misleading information; the other is to identify high-quality reviews, on which we will focus in this survey. Various factors and contexts have been studied to produce reliable and consistent quality prediction. Danescu-Niculescu-Mizil et al. (2009) stud374 ied several factors on helpfulness voting of Amazon product reviews. Ghose and Ipeirotis (2010) studied several factors on assessing review helpfulness including reviewer characteristics, reviewer history, and review readability and subjectivity. Lu et al. (2010) proposed a linear regression model with various social contexts for review quality prediction. The authors employed author consistency, trust consistency and co-citation consistency hypothesis to predict more consistently. Liu et al. (2008) studied three factors, i.e., reviewer expertise, writing style, and timeliness, and proposed a non-linear regression model with radial basis functions to predict the helpfulness of movie rev"
I11-1042,C10-1079,0,\N,Missing
I17-4006,I17-4006,1,0.0512975,"Missing"
I17-4006,J90-1003,0,0.335624,"Missing"
I17-4006,W04-1213,0,0.705905,"Missing"
I17-4006,W16-4906,0,0.606715,"Missing"
I17-4006,W15-4401,0,0.307126,"Missing"
I17-4006,W13-3601,0,0.190158,"Missing"
I17-4006,W14-1701,0,0.133707,"Missing"
I17-4006,C12-1184,0,0.22298,"Missing"
I17-4006,W16-4907,0,0.287363,"Missing"
J14-1004,N10-1026,1,0.94083,"r knowledge into the POS induction model (Grac¸a et al. 2009; Toutanova and Johnson 2007), or on new training techniques like contrastive estimation (Smith and Eisner 2005) for alternative sequence models. Despite the fact that completely connected, standard HMMs perform poorly at the POS induction task (Johnson 2007), we show that they still provide very useful features for a supervised POS tagger. Experiments in information extraction have previously also shown that HMMs provide informative features for this quite different, semantic processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010). This article extends our previous work on learning representations for domain adaptation (Huang and Yates 2009, 2010) by investigating new language representations—the naive Bayes representation and PL-MRF representation (Huang et al. 2011)—by analyzing results in terms of polysemy, sparsity, and domain divergence; by testing on new data sets including a Chinese POS tagging task; and by providing an empirical comparison with Brown clusters as representations. 3. Learning Representations of Distributional Similarity In this section, we will introduce several representation learning models. 3."
J14-1004,P05-1001,0,0.00903134,"resentation from the task of optimizing a hypothesis. To learn a representation, we can train a statistical language model on unlabeled text, and then use parameters or latent states from the statistical language model to create a representation function. Optimizing a hypothesis then follows the standard learning framework, using the representation from the statistical language model. 90 Huang et al. Computational Linguistics The LMRH is similar to the manifold and cluster assumptions behind other semisupervised approaches to machine learning, such as Alternating Structure Optimization (ASO) (Ando and Zhang 2005) and Structural Correspondence Learning (SCL) (Blitzer, McDonald, and Pereira 2006). All three of these techniques use predictors built on unlabeled data as a way to harness the manifold and cluster assumptions. However, the LMRH is distinct from at least ASO and SCL in important ways. Both ASO and SCL create multiple “synthetic” or “pivot” prediction tasks using unlabeled data, and find transformations of the input feature space that perform well on these tasks. The LMRH, on the other hand, is more specific — it asserts that for language problems, if we optimize word representations on a sing"
J14-1004,C04-1080,0,0.0156283,"like most semisupervised techniques, we concentrate on a particularly simple task decomposition: unsupervised learning for new representations, followed by standard supervised learning. In addition to our task decomposition being simple, our learned representations are also task-independent, so we can learn the representation once, and then apply it to any task. One of the best-performing representations that we consider for domain adaptation is based on the HMM (Rabiner 1989). HMMs have of course also been used for supervised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised POS tagging have focused on incorporating prior knowledge into the POS induction model (Grac¸a et al. 2009; Toutanova and Johnson 2007), or on new training techniques like contrastive estimation (Smith and Eisner 2005) for alternative sequence models. Despite the fact that completely connected, standard HMMs perform poorly at the POS induction task (Johnson 2007), we show that they still provide very useful features for a supervised POS tagger. Experiments in information extraction have previously also shown that HMMs pr"
J14-1004,W04-3224,0,0.221836,"eakly supervised learning, that is, learning when domain-specific labeled training data are scarce. A growing body of theoretical and empirical evidence suggests that traditional, manually crafted features for a variety of NLP tasks limit systems’ performance in this weakly supervised learning for two reasons. First, feature sparsity prevents systems from generalizing accurately, because many words and features are not observed in training. Also because word frequencies are Zipf-distributed, this often means that there is little relevant training data for a substantial fraction of parameters (Bikel 2004b), especially in new domains (Huang and Yates 2009). For example, word-type features form the backbone of most POS-tagging systems, but types like “gene” and “pathway” show up frequently in biomedical literature, and rarely in newswire text. Thus, a classifier trained on newswire data and tested on biomedical data will have seen few training examples related to sentences with features “gene” and “pathway” (Blitzer, McDonald, and Pereira 2006; Ben-David et al. 2010). Further, because words are polysemous, word-type features prevent systems from generalizing to situations in which words have di"
J14-1004,P07-1056,0,0.0089954,"ther representations significantly outperform the more commonly used Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings of sequence-labeling tasks. Most previous work on domain adaptation has focused on the case where some labeled data are available in both the source and target domains (Chan and Ng 2006; Daum´e III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daum´e III 2007; Jiang and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze, Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this problem setting have focused on appropriately weighting examples from the source and target domains so that the learning algorithm can balance the greater relevance of the target-domain data with the larger source-domain data set. In some cases, researchers combine this approach with semi-supervised learning to include unlabeled examples from the target domain as well (Daum´e III, Kumar, and Saha 2010). These techniques do not handle open-domain corpora like the Web, where they require expert input to acquire labels for each new sing"
J14-1004,W06-1615,0,0.123652,"Missing"
J14-1004,J92-4003,0,0.705609,"ious Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010); 2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990; Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and Jordan 2003; V¨ayrynen and Honkela 2004, 2005; V¨ayrynen, Honkela, and Lindqvist 2007); 3) using clusters that are induced from distributional similarity (Brown et al. 1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu 2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008; Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering for rep"
J14-1004,W09-3821,0,0.00854013,"ooccurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010); 2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990; Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and Jordan 2003; V¨ayrynen and Honkela 2004, 2005; V¨ayrynen, Honkela, and Lindqvist 2007); 3) using clusters that are induced from distributional similarity (Brown et al. 1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu 2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008; Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering for representations, but where previous work has used bigram and trigram statistics to form clusters, we build sophisticated models that attempt to capture the context of a word, and hence its sim"
J14-1004,P06-1012,0,0.220756,"results. However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication. These texts have very different characteristics from traditional training corpora in NLP. Evidence from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing (Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan, Ward, and Martin 2007), among other NLP tasks (Daum´e III and Marcu 2006; Chelba and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer, Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training. Collecting labeled training data for each new target domain is typically prohibitively expensive. In this article, we investigate representations that can be applied to weakly supervised learning, that is, learning when domain-specific labeled training data are scarce. A growing body of theoretical and empirical evidence suggests that traditional, manually crafted features for a variety of NLP tasks limit systems’ performa"
J14-1004,W04-3237,0,0.012546,"ning data, these traditional features yield state-of-the-art results. However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication. These texts have very different characteristics from traditional training corpora in NLP. Evidence from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing (Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan, Ward, and Martin 2007), among other NLP tasks (Daum´e III and Marcu 2006; Chelba and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer, Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training. Collecting labeled training data for each new target domain is typically prohibitively expensive. In this article, we investigate representations that can be applied to weakly supervised learning, that is, learning when domain-specific labeled training data are scarce. A growing body of theoretical and empirical evidence suggests that traditional, manually crafted feat"
J14-1004,P07-1033,0,0.0391516,"Missing"
J14-1004,W10-2608,0,0.0266538,"Missing"
J14-1004,D09-1003,0,0.017399,"ques are not representation learning, and are complementary to our techniques. Our representation-learning approach to domain adaptation is an instance of semi-supervised learning. Of the vast number of semi-supervised approaches to sequence labeling in NLP, the most relevant ones here include Suzuki and Isozaki’s (2008) combination of HMMs and CRFs that uses over a billion words of unlabeled text to achieve the current best performance on in-domain chunking, and semi-supervised approaches to improving in-domain SRL with large quantities of unlabeled text ¨ (Weston, Ratle, and Collobert 2008; Deschacht and Moens 2009; and Furstenau and Lapata 2009). Ando and Zhang’s (2005) semi-supervised sequence labeling technique has been tested on a domain adaptation task for POS tagging (Blitzer, McDonald, and Pereira 2006); our representation-learning approaches outperform it. Unlike most semisupervised techniques, we concentrate on a particularly simple task decomposition: unsupervised learning for new representations, followed by standard supervised learning. In addition to our task decomposition being simple, our learned representations are also task-independent, so we can learn the representation once, and then"
J14-1004,P07-1088,1,0.808217,"Missing"
J14-1004,D08-1072,0,0.00844888,"l of their chunking and NER tests. We concentrate on probabilistic graphical models with discrete latent states instead. We show that HMMbased and other representations significantly outperform the more commonly used Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings of sequence-labeling tasks. Most previous work on domain adaptation has focused on the case where some labeled data are available in both the source and target domains (Chan and Ng 2006; Daum´e III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daum´e III 2007; Jiang and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze, Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this problem setting have focused on appropriately weighting examples from the source and target domains so that the learning algorithm can balance the greater relevance of the target-domain data with the larger source-domain data set. In some cases, researchers combine this approach with semi-supervised learning to include unlabeled examples from the target domain as well (Daum´e III, Kumar, and"
J14-1004,N09-1068,0,0.0210352,"e concentrate on probabilistic graphical models with discrete latent states instead. We show that HMMbased and other representations significantly outperform the more commonly used Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings of sequence-labeling tasks. Most previous work on domain adaptation has focused on the case where some labeled data are available in both the source and target domains (Chan and Ng 2006; Daum´e III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daum´e III 2007; Jiang and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze, Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this problem setting have focused on appropriately weighting examples from the source and target domains so that the learning algorithm can balance the greater relevance of the target-domain data with the larger source-domain data set. In some cases, researchers combine this approach with semi-supervised learning to include unlabeled examples from the target domain as well (Daum´e III, Kumar, and Saha 2010). These techniq"
J14-1004,E09-1026,0,0.0281669,"arning, and are complementary to our techniques. Our representation-learning approach to domain adaptation is an instance of semi-supervised learning. Of the vast number of semi-supervised approaches to sequence labeling in NLP, the most relevant ones here include Suzuki and Isozaki’s (2008) combination of HMMs and CRFs that uses over a billion words of unlabeled text to achieve the current best performance on in-domain chunking, and semi-supervised approaches to improving in-domain SRL with large quantities of unlabeled text ¨ (Weston, Ratle, and Collobert 2008; Deschacht and Moens 2009; and Furstenau and Lapata 2009). Ando and Zhang’s (2005) semi-supervised sequence labeling technique has been tested on a domain adaptation task for POS tagging (Blitzer, McDonald, and Pereira 2006); our representation-learning approaches outperform it. Unlike most semisupervised techniques, we concentrate on a particularly simple task decomposition: unsupervised learning for new representations, followed by standard supervised learning. In addition to our task decomposition being simple, our learned representations are also task-independent, so we can learn the representation once, and then apply it to any task. One of the"
J14-1004,W01-0521,0,0.0187287,"feature like, “the previous token is the” to help classify a given token as a noun or adjective. For supervised NLP tasks with sufficient domain-specific training data, these traditional features yield state-of-the-art results. However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication. These texts have very different characteristics from traditional training corpora in NLP. Evidence from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing (Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan, Ward, and Martin 2007), among other NLP tasks (Daum´e III and Marcu 2006; Chelba and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer, Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training. Collecting labeled training data for each new target domain is typically prohibitively expensive. In this article, we investigate representations that can be applied to weakly supervised learning, that"
J14-1004,P07-1094,0,0.0203683,"ed techniques, we concentrate on a particularly simple task decomposition: unsupervised learning for new representations, followed by standard supervised learning. In addition to our task decomposition being simple, our learned representations are also task-independent, so we can learn the representation once, and then apply it to any task. One of the best-performing representations that we consider for domain adaptation is based on the HMM (Rabiner 1989). HMMs have of course also been used for supervised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised POS tagging have focused on incorporating prior knowledge into the POS induction model (Grac¸a et al. 2009; Toutanova and Johnson 2007), or on new training techniques like contrastive estimation (Smith and Eisner 2005) for alternative sequence models. Despite the fact that completely connected, standard HMMs perform poorly at the POS induction task (Johnson 2007), we show that they still provide very useful features for a supervised POS tagger. Experiments in information extraction have previously also shown that HMMs provide informative features for"
J14-1004,P90-1034,0,0.486328,"never appears in the WSJ portion of the Penn Treebank (Huang and Yates 2010). Our response to the sparsity and polysemy challenges with traditional NLP representations is to seek new representations that allow systems to generalize to previously unseen examples. That is, we seek representations that permit classifiers to have close to the same accuracy on examples from other domains as they do on the domain of the training data. Our approach depends on the well-known distributional hypothesis, which states that a word’s meaning is identified with the contexts in which it appears (Harris 1954; Hindle 1990). Our goal is to develop probabilistic statistical language models that describe the contexts of individual words accurately. We then construct representations, or mappings from word tokens and types to real-valued vectors, from statistical language models. Because statistical language models are designed to model words’ contexts, the features they produce can be used to combat problems with polysemy. And by careful design of the statistical language models, we can limit 86 Huang et al. Computational Linguistics the number of features that they produce, controlling how sparse those features ar"
J14-1004,P09-1056,1,0.690554,"tagger would traditionally use a feature like, “the previous token is the” to help classify a given token as a noun or adjective. For supervised NLP tasks with sufficient domain-specific training data, these traditional features yield state-of-the-art results. However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication. These texts have very different characteristics from traditional training corpora in NLP. Evidence from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing (Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan, Ward, and Martin 2007), among other NLP tasks (Daum´e III and Marcu 2006; Chelba and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer, Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training. Collecting labeled training data for each new target domain is typically prohibitively expensive. In this article, we investigate representations that can be applied to weakly supe"
J14-1004,W10-2604,1,0.871307,"Missing"
J14-1004,W11-0315,1,0.580237,"ed, standard HMMs perform poorly at the POS induction task (Johnson 2007), we show that they still provide very useful features for a supervised POS tagger. Experiments in information extraction have previously also shown that HMMs provide informative features for this quite different, semantic processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010). This article extends our previous work on learning representations for domain adaptation (Huang and Yates 2009, 2010) by investigating new language representations—the naive Bayes representation and PL-MRF representation (Huang et al. 2011)—by analyzing results in terms of polysemy, sparsity, and domain divergence; by testing on new data sets including a Chinese POS tagging task; and by providing an empirical comparison with Brown clusters as representations. 3. Learning Representations of Distributional Similarity In this section, we will introduce several representation learning models. 3.1 Traditional POS-Tagging Representations As an example of our terminology, we begin by describing a representation used in traditional POS taggers (this representation will later form a baseline for our POS tagging experiments). The instance"
J14-1004,P07-1034,0,0.0259863,"than neural net models on all of their chunking and NER tests. We concentrate on probabilistic graphical models with discrete latent states instead. We show that HMMbased and other representations significantly outperform the more commonly used Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings of sequence-labeling tasks. Most previous work on domain adaptation has focused on the case where some labeled data are available in both the source and target domains (Chan and Ng 2006; Daum´e III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daum´e III 2007; Jiang and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze, Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this problem setting have focused on appropriately weighting examples from the source and target domains so that the learning algorithm can balance the greater relevance of the target-domain data with the larger source-domain data set. In some cases, researchers combine this approach with semi-supervised learning to include unlabeled examples from the target domain"
J14-1004,D07-1031,0,0.0115092,"osition being simple, our learned representations are also task-independent, so we can learn the representation once, and then apply it to any task. One of the best-performing representations that we consider for domain adaptation is based on the HMM (Rabiner 1989). HMMs have of course also been used for supervised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised POS tagging have focused on incorporating prior knowledge into the POS induction model (Grac¸a et al. 2009; Toutanova and Johnson 2007), or on new training techniques like contrastive estimation (Smith and Eisner 2005) for alternative sequence models. Despite the fact that completely connected, standard HMMs perform poorly at the POS induction task (Johnson 2007), we show that they still provide very useful features for a supervised POS tagger. Experiments in information extraction have previously also shown that HMMs provide informative features for this quite different, semantic processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010). This article extends our previous work on learning representation"
J14-1004,P08-1068,0,0.237807,"Missing"
J14-1004,P09-1116,0,0.185604,"t-level lexical cooccurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010); 2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990; Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and Jordan 2003; V¨ayrynen and Honkela 2004, 2005; V¨ayrynen, Honkela, and Lindqvist 2007); 3) using clusters that are induced from distributional similarity (Brown et al. 1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu 2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008; Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering for representations, but where previous work has used bigram and trigram statistics to form clusters, we build sophisticated models that attempt to capture the context of"
J14-1004,J93-2004,0,0.0464831,"Missing"
J14-1004,N10-1004,0,0.00858595,"us token is the” to help classify a given token as a noun or adjective. For supervised NLP tasks with sufficient domain-specific training data, these traditional features yield state-of-the-art results. However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication. These texts have very different characteristics from traditional training corpora in NLP. Evidence from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing (Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan, Ward, and Martin 2007), among other NLP tasks (Daum´e III and Marcu 2006; Chelba and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer, Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training. Collecting labeled training data for each new target domain is typically prohibitively expensive. In this article, we investigate representations that can be applied to weakly supervised learning, that is, learning when domain-spec"
J14-1004,N04-1043,0,0.431145,"Missing"
J14-1004,P93-1024,0,0.834677,"Missing"
J14-1004,N07-1070,0,0.0143387,"Missing"
J14-1004,W09-1119,0,0.167518,"eaning based on document-level lexical cooccurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010); 2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990; Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and Jordan 2003; V¨ayrynen and Honkela 2004, 2005; V¨ayrynen, Honkela, and Lindqvist 2007); 3) using clusters that are induced from distributional similarity (Brown et al. 1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu 2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008; Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering for representations, but where previous work has used bigram and trigram statistics to form clusters, we build sophisticated models that attempt to captur"
J14-1004,A97-1015,0,0.0543968,", “the previous token is the” to help classify a given token as a noun or adjective. For supervised NLP tasks with sufficient domain-specific training data, these traditional features yield state-of-the-art results. However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication. These texts have very different characteristics from traditional training corpora in NLP. Evidence from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing (Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan, Ward, and Martin 2007), among other NLP tasks (Daum´e III and Marcu 2006; Chelba and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer, Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training. Collecting labeled training data for each new target domain is typically prohibitively expensive. In this article, we investigate representations that can be applied to weakly supervised learning, that is, learning"
J14-1004,P07-1096,0,0.0200571,"Missing"
J14-1004,P05-1044,0,0.080707,"t, so we can learn the representation once, and then apply it to any task. One of the best-performing representations that we consider for domain adaptation is based on the HMM (Rabiner 1989). HMMs have of course also been used for supervised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised POS tagging have focused on incorporating prior knowledge into the POS induction model (Grac¸a et al. 2009; Toutanova and Johnson 2007), or on new training techniques like contrastive estimation (Smith and Eisner 2005) for alternative sequence models. Despite the fact that completely connected, standard HMMs perform poorly at the POS induction task (Johnson 2007), we show that they still provide very useful features for a supervised POS tagger. Experiments in information extraction have previously also shown that HMMs provide informative features for this quite different, semantic processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010). This article extends our previous work on learning representations for domain adaptation (Huang and Yates 2009, 2010) by investigating new language"
J14-1004,P08-1076,0,0.018922,"Missing"
J14-1004,D09-1058,0,0.0174586,"006; Turney and Pantel 2010); 2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990; Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and Jordan 2003; V¨ayrynen and Honkela 2004, 2005; V¨ayrynen, Honkela, and Lindqvist 2007); 3) using clusters that are induced from distributional similarity (Brown et al. 1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu 2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008; Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering for representations, but where previous work has used bigram and trigram statistics to form clusters, we build sophisticated models that attempt to capture the context of a word, and hence its similarity to other words, more precisely. Our experimen"
J14-1004,W00-0726,0,0.112069,"Missing"
J14-1004,I05-3005,0,0.0811765,"Missing"
J14-1004,N09-2062,0,0.0574409,"Missing"
J14-1004,P10-1040,0,0.219549,"Missing"
J14-1004,C96-2212,0,0.159741,"s model. We train our models using standard expectation-maximization (Dempster, Laird, and Rubin 1977) with random initialization of the parameters. Because our factorization of the sentence does not take into account the fact that the trigrams overlap, the resulting statistical language model is mass-deficient. Worse still, it is throwing away information from the dependencies among trigrams which might help make better clustering decisions. Nevertheless, this model closely mirrors many of the clustering algorithms used in previous approaches to representation learning for sequence labeling (Ushioda 1996; Miller, Guinness, and Zamanian 2004; Koo, Carreras, 1 Compare with Dhillon, Foster, and Ungar (2011), who use canonical correlation analysis to find a simultaneous reduction of the left and right context vectors, a significantly more complex undertaking. 95 Computational Linguistics Volume 40, Number 1 and Collins 2008; Lin and Wu 2009; Ratinov and Roth 2009), and therefore serves as an important benchmark. Given a naive Bayes statistical language model, we construct an NB-R representation that produces |S |boolean features Fs (xi ) for each token xi and each possible latent state s ∈ S:  F"
J14-1004,N13-1065,1,0.85938,"Missing"
J14-1004,W09-1208,0,0.0149806,"el 2010); 2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990; Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and Jordan 2003; V¨ayrynen and Honkela 2004, 2005; V¨ayrynen, Honkela, and Lindqvist 2007); 3) using clusters that are induced from distributional similarity (Brown et al. 1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu 2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008; Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering for representations, but where previous work has used bigram and trigram statistics to form clusters, we build sophisticated models that attempt to capture the context of a word, and hence its similarity to other words, more precisely. Our experiments show that the ne"
J14-1004,J04-4004,0,\N,Missing
J14-1004,D09-1098,0,\N,Missing
N13-1065,N10-1026,1,0.487713,"andom access to model parameters. Thus, the model parameters must be stored in memory on each node. Because LVMs include a multinomial distribution over words for each latent variable value, the model parameter space increases with the number of latent variable values times the vocabulary size. For large models (i.e., with many latent variable values) and large corpora (with large vocabularies), the memory required for training can exceed the limits of the commodity servers comprising modern computational clusters. Because model accuracy tends to increase with both corpus size and model size (Ahuja and Downey, 2010; Huang and Yates, 2010), training accurate language models requires that we overcome the memory bottleneck. We present a simple technique for mitigating the memory bottleneck in parallel LVM training. Existing parallelization schemes begin by partitioning the training corpus arbitrarily across computational nodes. In this paper, we show how to reduce memory footprint by instead partitioning the corpus to minimize the number of unique words on each node (and thereby minimize the number of parameters the node must store). Because corpus partitioning is a pre-processing step in parallel LVM trai"
N13-1065,P07-1088,1,0.774586,"rated in Figure 1. BJac reduces Vmax by a larger factor over the baseline as more computational nodes are employed. 140,000 100,000 Evaluation of Partitioning Methods We evaluate our partitioning method against the baseline and Z&I, the best performing scalable method from previous work, which uses random document selection and M INIMUM node selection (Zhu and Ibarra, 1999). We evaluate on three corpora (Table 1): the Brown corpus of newswire text (Kucera and Francis, 1967), the Reuters Corpus Volume1 (RCV1) (Lewis et al., 2004), and a larger WebSent corpus of sentences gathered from the Web (Downey et al., 2007). Corpus Brown RCV1 Web-Sent N 57339 804414 2747282 V 56058 288062 214588 Z 1161183 99702278 58666983 Table 1: Characteristics of the three corpora. N = # of documents, V = # of word types, Z = # of tokens. We treat each sentence as a document in the Brown and Web-Sent corpora. Table 2 shows how the maximum word type size Vmax varies for each method and corpus, for T = 50 nodes. BJAC significantly decreases Vmax over the 582 Vmax by baseline Vmax by BJac 120,000 number of word types 3.2 Corpus 80,000 60,000 40,000 20,000 0 10 20 30 40 50 60 70 number of nodes 80 90 100 Figure 1: Effects of par"
N13-1065,P09-1056,1,0.922535,"Missing"
N13-1065,W10-2604,1,0.842447,"rameters. Thus, the model parameters must be stored in memory on each node. Because LVMs include a multinomial distribution over words for each latent variable value, the model parameter space increases with the number of latent variable values times the vocabulary size. For large models (i.e., with many latent variable values) and large corpora (with large vocabularies), the memory required for training can exceed the limits of the commodity servers comprising modern computational clusters. Because model accuracy tends to increase with both corpus size and model size (Ahuja and Downey, 2010; Huang and Yates, 2010), training accurate language models requires that we overcome the memory bottleneck. We present a simple technique for mitigating the memory bottleneck in parallel LVM training. Existing parallelization schemes begin by partitioning the training corpus arbitrarily across computational nodes. In this paper, we show how to reduce memory footprint by instead partitioning the corpus to minimize the number of unique words on each node (and thereby minimize the number of parameters the node must store). Because corpus partitioning is a pre-processing step in parallel LVM training, our 579 Proceeding"
N13-1065,N10-1020,0,0.0958516,"Missing"
N15-1069,P14-2134,0,0.00803711,"with neural word embeddings from Collobert and Weston (2008) and Mnih and Hinton (2009). Word embeddings can also be computed via neural language models (Mikolov et al., 2013b), or from canonical correlation analysis (Dhillon et al., 2011). Xiao and Guo (2013) induce word embeddings across multiple domains, and concatenate these representations into a single feature vector for labeled instances in each domain, following EasyAdapt (Daum´e III, 2007). However, they do not apply this idea to unsupervised domain adaptation, and do not work in the structured feature setting that we consider here. Bamman et al. (2014) learn geographically-specific word embeddings, in an approach that is similar to our multi-domain feature embeddings, but they do not consider the application to domain adaptation. We can also view the distributed representations in FLORS as a sort of word embedding, computed directly from rescaled bigram counts (Schnabel and Sch¨utze, 2014). Feature embeddings are based on a different philosophy than word embeddings. While many NLP features are lexical in nature, the role of a word towards linguistic structure prediction may differ across feature templates. Applying a single word representat"
N15-1069,W06-1615,0,0.982722,"natural language processing is to be successfully employed in highimpact application areas such as social media, patient medical records, and historical texts. Unsupervised domain adaptation is particularly appealing, since it requires no labeled data in the target domain. Some of the most successful approaches to unsupervised domain adaptation are based on representation learning: transforming sparse high-dimensional surface features into dense vector representations, 1 Source code and a demo are available at https:// github.com/yiyang-gt/feat2vec which are often more robust to domain shift (Blitzer et al., 2006; Glorot et al., 2011). However, these methods are computationally expensive to train, and often require special task-specific heuristics to select good “pivot features.” A second, more subtle challenge for unsupervised domain adaptation is that it is normally framed as adapting from a single source domain to a single target domain. For example, we may be given partof-speech labeled text from 19th Century narratives, and we hope to adapt the tagger to work on academic dissertations from the 16th Century. This ignores text from the intervening centuries, as well as text that is related by genre"
N15-1069,P07-1056,0,0.416924,"learn to reconstruct a subset of “pivot features”, as shown in Figure 2(a). The reconstruction function — which is learned from unlabeled data in both domains — is then employed to project each instance into a dense representation, which will hopefully be better suited to cross-domain generalization. The pivot features are chosen to be both predictive of the label and general across domains. Meeting these two criteria requires task-specific heuristics; for example, differ673 ent pivot selection techniques are employed in SCL for syntactic tagging (Blitzer et al., 2006) and sentiment analysis (Blitzer et al., 2007). Furthermore, the pivot features correspond to a small subspace of the feature co-occurrence matrix. In Denoising Autoencoders, each pivot feature corresponds to a dense feature in the transformed representation, but large dense feature vectors impose substantial computational costs at learning time. In SCL, each pivot feature introduces a new classification problem, which makes computation of the cross-domain representation expensive. In either case, we face a tradeoff between the amount of feature co-occurrence information that we can use, and the computational complexity for representation"
N15-1069,P07-1033,0,0.329636,"Missing"
N15-1069,N09-1068,0,0.085658,"and shape features. The tradeoff is that feature embeddings must be recomputed for each set of feature templates, unlike word embeddings, which can simply be downloaded and plugged into any NLP problem. However, computing feature embeddings is easy in practice, since it requires 680 only a light modification to existing well-optimized implementations for computing word embeddings. Multi-domain adaptation The question of adaptation across multiple domains has mainly been addressed in the context of supervised multi-domain learning, with labeled data available in all domains (Daum´e III, 2007). Finkel and Manning (2009) propagate classification parameters across a tree of domains, so that classifiers for sibling domains are more similar; Daum´e III (2009) shows how to induce such trees using a nonparametric Bayesian model. Dredze et al. (2010) combine classifier weights using confidence-weighted learning, which represents the covariance of the weight vectors. Joshi et al. (2013) formulate the problem of multi-attribute multi-domain learning, where all attributes are potential distinctions between domains; Wang et al. (2013) present an approach for automatically partitioning instances into domains according t"
N15-1069,D13-1205,0,0.0116438,"iction problems and the selection of pivot features both involve heuristic decisions, which may vary depending on the task. F EMA avoids the selection of pivot features by directly learning a low-dimensional representation, through which features in each template predict the other templates. An alternative is to link unsupervised learning in the source and target domains with the label distribution in the source domain, through the framework of posterior regularization (Ganchev et al., 2010). This idea is applied to domain adaptation by Huang and Yates (2012), and to cross-lingual learning by Ganchev and Das (2013). This approach requires a forward-backward computation for representation learning, while F EMA representations can be learned without dynamic programming, through negative sampling. Word embeddings Word embeddings can be viewed as special case of representation learning, where the goal is to learn representations for each word, and then to supply these representations in place of lexical features. Early work focused on discrete clusters (Brown et al., 1990), while more recent approaches induce dense vector representations; Turian et al. (2010) compare Brown clusters with neural word embeddin"
N15-1069,N06-2015,0,0.00860493,"f the shared embedding h(0) . The regularization penalty is selected by grid search over {0.001, 0.01, 0.1, 1.0, 10.0}. In general, we find that the hyperparameters that yield good word embeddings tend to yield good feature embeddings too. 4.2 Evaluation 1: Web text Recent work in domain adaptation for natural language processing has focused on the data from the shared task on Syntactic Analysis of Non-Canonical Language (SANCL; Petrov and McDonald, 2012), which contains several web-related corpora (newsgroups, reviews, weblogs, answers, emails) as well as the WSJ portion of OntoNotes corpus (Hovy et al., 2006). Following Schnabel and Sch¨utze (2014), we use sections 02-21 of WSJ for training and section 22 for development, and use 100,000 unlabeled WSJ sentences from 1988 for learning representations. On the web text side, each of the five target domains has an unlabeled training set of 100,000 sentences (except the ANSWERS domain, which has 27,274 unlabeled sentences), along with development and test sets of about 1000 labeled sentences each. In the spirit of truly unsupervised domain adaptation, we do not use any target domain data for parameter tuning. Settings For F EMA, we consider only the si"
N15-1069,D12-1120,0,0.0174955,"source domain training data. The design of auxiliary prediction problems and the selection of pivot features both involve heuristic decisions, which may vary depending on the task. F EMA avoids the selection of pivot features by directly learning a low-dimensional representation, through which features in each template predict the other templates. An alternative is to link unsupervised learning in the source and target domains with the label distribution in the source domain, through the framework of posterior regularization (Ganchev et al., 2010). This idea is applied to domain adaptation by Huang and Yates (2012), and to cross-lingual learning by Ganchev and Das (2013). This approach requires a forward-backward computation for representation learning, while F EMA representations can be learned without dynamic programming, through negative sampling. Word embeddings Word embeddings can be viewed as special case of representation learning, where the goal is to learn representations for each word, and then to supply these representations in place of lexical features. Early work focused on discrete clusters (Brown et al., 1990), while more recent approaches induce dense vector representations; Turian et al"
N15-1069,N13-1080,0,0.0745404,"Missing"
N15-1069,N15-1142,0,0.00734312,"04 65.95 40.09 59.94 48.39 49.97 63.91 F EMA-current F EMA-prev F EMA-next F EMA-all F EMA-prev Table 5: Label consistency of the Q-most similar words in each embedding. F EMA-all is the concatenation of the current, previous, and next-word F EMA embeddings. also used to obtain the most common tag for each word. Table 5 shows that the F EMA embeddings are more consistent with the type-level POS tags than WORD 2 VEC embeddings. This is not surprising, since they are based on feature templates that are specifically designed for capturing syntactic regularities. In simultaneously published work, Ling et al. (2015) present “position-specific” word embeddings, which are an alternative method to induce more syntactically-oriented word embeddings. Table 6 shows the most similar words for three query keywords, in each of four different embeddings. The next-word and previous-word embeddings are most related to syntax, because they help to predict each other and the current-word feature; the current-word embedding brings in aspects of orthography, because it must help to predict the affix features. In morphologically rich languages such as Portuguese, this can help to compute good embeddings for rare inflecte"
N15-1069,W96-0213,0,0.26321,"thods can be extremely competitive on these datasets, at a fraction of the computational cost. Specifically, we apply a support vector machine (SVM) classifier, Component Feature template Lexical (5) wi−2 = X, wi−1 = Y, . . . Affixes (8) X is prefix of wi , |X |≤ 4 X is suffix of wi , |X |≤ 4 Orthography (3) wi contains number, uppercase character, or hyphen Table 1: Basic feature templates for token wi . adding dense features from F EMA (and the alternative representation learning techniques) to a set of basic features. 4.1.1 Basic features We apply sixteen feature templates, motivated by by Ratnaparkhi (1996). Table 1 provides a summary of the templates; there are four templates each for the prefix and suffix features. Feature embeddings are learned for all lexical and affix features, yielding a total of thirteen embeddings per instance. We do not learn embeddings for the binary orthographic features. Santos and Zadrozny (2014) demonstrate the utility of embeddings for affix features. 4.1.2 Competitive systems We consider three competitive unsupervised domain adaptation methods. Structural Correspondence Learning (Blitzer et al., 2006, SCL) creates a binary classification problem for each pivot fe"
N15-1069,Q14-1002,0,0.0238776,"Missing"
N15-1069,P10-1040,0,0.301924,"gs, which are dense representations of individual features. Each embedding is selected to help predict the features that fill out the other templates: for example, an embedding for the current word feature is selected to help predict the previous word feature and successor word feature, and vice versa; see Figure 2(b). The embeddings for each active feature are then concatenated together across templates, giving a dense representation for the entire instance. Our approach is motivated by word embeddings, in which dense representations are learned for individual words based on their neighbors (Turian et al., 2010; Xiao and Guo, 2013), but rather than learning a single embedding for each word, we learn embeddings for each feature. This means that the embedding of, say, ‘toughness’ will differ depending on whether it appears in the current-word template or the previous-word template (see Table 6). This provides additional flexibility for the downstream learning algorithm, and the increase in the dimensionality of the overall dense representation can be offset by learning shorter embeddings for each feature. In Section 4, we show that feature embeddings convincingly outperform word embeddings on two part"
N15-1069,D13-1086,0,0.0379065,"Missing"
N15-1069,P14-2088,1,0.860135,"nsider three competitive unsupervised domain adaptation methods. Structural Correspondence Learning (Blitzer et al., 2006, SCL) creates a binary classification problem for each pivot feature, and uses the weights of the resulting classifiers to project the instances into a dense representation. Marginalized Denoising Autoencoders (Chen et al., 2012, mDA) learn robust representation across domains by reconstructing pivot features from artificially corrupted input instances. We use structured dropout noise, which has achieved state-of-art results on domain adaptation for part-of-speech tagging (Yang and Eisenstein, 2014). We also directly compare with WORD 2 VEC3 word embeddings, and with a “no-adaptation” baseline in which only surface features are used. 4.1.3 Parameter tuning All the hyperparameters are tuned on development data. Following Blitzer et al. (2006), we consider pivot features that appear more than 50 times in 3 https://code.google.com/p/word2vec/ 676 all the domains for SCL and mDA. In SCL, the parameter K selects the number of singular vectors of the projection matrix to consider; we try values between 10 and 100, and also employ feature normalization and rescaling. For embedding-based methods"
N15-1069,J92-4003,0,\N,Missing
N16-1157,W06-1615,0,0.879931,"ges in the meanings of words (Kulkarni et al., 2015). In the example above, the word ‘ryottours’ is not successfully normalized to ‘rioters’; the syntax is comprehensible to contemporary English speakers, but usages such as ‘wild disposed’ and ‘drew unto’ are sufﬁciently unusual as to pose problems for NLP systems trained on contemporary texts. Domain adaptation A more generic machine learning approach is to apply unsupervised domain adaptation techniques, which transform the representations of the training and target texts to be more similar, typically using feature co-occurrence statistics (Blitzer et al., 2006; Ben-David et al., 2010). It is natural to think of historical texts as a distinct domain from contemporary training corpora, and Yang and Eisenstein (2014, 2015) show that the accuracy of historical Portuguese POS tagging can be signiﬁcantly improved by domain adaption. However, we are unaware of prior work that empirically evaluates the efﬁcacy of this approach on Early Modern English texts. Furthermore, historical texts are often associated with multiple metadata attributes (e.g., author, genre, and epoch), each of which may inﬂuence the text’s linguistic properties. Multi-domain adaptatio"
N16-1157,J92-4003,0,0.197326,"storical text but has no labeled data in the target domain (e.g., Muralidharan and Hearst, 2013). This best ﬁts the paradigm of unsupervised domain adaptation, when labeled data from the source domain (e.g., the PTB) is combined with unlabeled data from the target domain. Representational differences between source and target domains can be a major source of errors in domain adaptation (BenDavid et al., 2010), and so several representation learning approaches have been proposed. The most straightforward approach is to replace lexical features with word representations, such as Brown clusters (Brown et al., 1992; Lin et al., 2012) or word embeddings (Turian et al., 2010), such as word2vec (Mikolov et al., 2013). Lexical features can then be replaced or augmented with the resulting word representations. This can assist in domain adaptation by linking out-of-vocabulary words to invocabulary words with similar distributional properties. Word representations are suitable for adapting lexical features, but a more general solution is to adapt the entire feature representation. One such method is Structural Correspondence Learning (Blitzer et al., 2006, SCL). In SCL, we create artiﬁcial binary classiﬁcation"
N16-1157,P07-1033,0,0.218938,"Missing"
N16-1157,N13-1037,1,0.919687,"Missing"
N16-1157,N09-1068,0,0.0819637,"Missing"
N16-1157,gimenez-marquez-2004-svmtool,0,0.117732,"Missing"
N16-1157,P07-1034,0,0.0877654,"Missing"
N16-1157,N13-1080,0,0.0393106,"Missing"
N16-1157,P12-3029,0,0.065559,"Missing"
N16-1157,P14-5010,0,0.00422553,"texts in the period from 1500 until 1710, and it is divided into three 70-year time periods similar to the PPCMBE corpus. The statistics of the corpus by time period is summarized in Table 2. The PPCEME consists of text from the same eighteen genres as the PPCMBE. Penn Treebank Release 3 The Penn Treebank (Marcus et al., 1993) is the de facto standard syntactically annotated corpus for English, 2 All the statistics in this section include punctuation, but exclude extra-linguistic material such as page numbers or token ID numbers. 1320 which is used to train software such as Stanford CoreNLP (Manning et al., 2014). When using this dataset for supervised training, we follow Toutanova et al. (2003) and use WSJ sections 0-18 for training, and sections 19-21 for tuning. When applying unsupervised domain adaptation, we use all WSJ sections, together with texts from the PPCMBE and the PPCEME. Tagsets The Penn Corpora of Historical English (PCHE) use a tagset that differs from the Penn Treebank, mainly in the direction of greater speciﬁcity. Auxiliary verbs ‘do’, ‘have’, and ‘be’ all have their own tags, as do words like ‘one’ and ‘else’, due to their changing syntactic function over time. Overall, there are"
N16-1157,J93-2004,0,0.0562741,"The PPCEME is a collection of text samples from the Helsinki Corpus (Rissanen et al., 1993), as well as two supplements mainly consisting of text material by the same authors and from the same editions as the material in the Helsinki Corpus. The corpus contains nearly two million words from texts in the period from 1500 until 1710, and it is divided into three 70-year time periods similar to the PPCMBE corpus. The statistics of the corpus by time period is summarized in Table 2. The PPCEME consists of text from the same eighteen genres as the PPCMBE. Penn Treebank Release 3 The Penn Treebank (Marcus et al., 1993) is the de facto standard syntactically annotated corpus for English, 2 All the statistics in this section include punctuation, but exclude extra-linguistic material such as page numbers or token ID numbers. 1320 which is used to train software such as Stanford CoreNLP (Manning et al., 2014). When using this dataset for supervised training, we follow Toutanova et al. (2003) and use WSJ sections 0-18 for training, and sections 19-21 for tuning. When applying unsupervised domain adaptation, we use all WSJ sections, together with texts from the PPCMBE and the PPCEME. Tagsets The Penn Corpora of H"
N16-1157,D07-1041,0,0.566422,"the Penn-Helsinki Parsed Corpus of Early Modern English (Kroch et al., 2004, PPCEME), and the Penn Parsed Corpus of Modern British English (Kroch and Taylor, 2000, PPCMBE). The corpora are annotated with part-of-speech tags and syntactic parsing trees in an annotation style similar to that of the Penn Treebank. In this work, we focus on POS tagging the PPCMBE and the PPCEME.1 1 Middle English is outside the scope of this paper, because it is sufﬁciently unintelligible to modern English speakers that texts such as Canterbury Tales are published in translation. In tagging Middle English texts, Moon and Baldridge (2007) apply bitext projection techniques from multilingual learning, rather than domain adaptation. Period # Sentence # Token 1840-1914 1770-1839 1700-1769 17,770 23,462 16,083 322,255 427,424 343,024 Total 57,315 1,092,703 Table 1: Statistics of the Penn Parsed Corpus of Modern British English (PPCMBE), by time period. Period 1640-1710 1570-1639 1500-1569 Total # Sentence # Token 29,181 39,799 31,416 614,315 706,587 640,255 100,396 1,961,157 Table 2: Statistics of the Penn Parsed Corpus of Early Modern English (PPCEME), by time period. The Penn Parsed Corpus of Modern British English The PPCMBE is"
N16-1157,W11-1512,0,0.0709538,"Missing"
N16-1157,W96-0213,0,0.835857,"Missing"
N16-1157,W11-1503,0,0.0426855,"rical texts Historical texts differ from modern texts in spellings, syntax and semantics, posing signiﬁcant challenges for standard NLP systems, which are usually trained with modern news text. Numerous resources have been created for overcoming the difﬁculties, including syntactically annotated corpora (Kroch et al., 2004; Kroch et al., 2010; Galves and Faria, 2010) and spelling normalization tools (Giusti et al., 2007; Baron and Rayson, 2008). Most previous work focuses on normalization, which can signiﬁcantly increase tagging accuracy on historical English (Rayson et al., 2007) and German (Scheible et al., 2011). Similar im1326 provements have been obtained for syntactic parsing (Schneider et al., 2014). Domain adaptation offers an alternative approach which is more generic — for example, it can be applied to any corpus without requiring the design of a set of normalization rules. As shown above, when normalization is possible, it can be combined with domain adaptation to yield better performance than that obtained by either approach alone. 7 Conclusion Syntactic analysis is a key ﬁrst step towards processing historical texts, but it is confounded by changes in spelling and usage over time. We empiri"
N16-1157,Q14-1002,0,0.0293341,"Missing"
N16-1157,N03-1033,0,0.0417227,"Missing"
N16-1157,P10-1040,0,0.0609493,"(e.g., Muralidharan and Hearst, 2013). This best ﬁts the paradigm of unsupervised domain adaptation, when labeled data from the source domain (e.g., the PTB) is combined with unlabeled data from the target domain. Representational differences between source and target domains can be a major source of errors in domain adaptation (BenDavid et al., 2010), and so several representation learning approaches have been proposed. The most straightforward approach is to replace lexical features with word representations, such as Brown clusters (Brown et al., 1992; Lin et al., 2012) or word embeddings (Turian et al., 2010), such as word2vec (Mikolov et al., 2013). Lexical features can then be replaced or augmented with the resulting word representations. This can assist in domain adaptation by linking out-of-vocabulary words to invocabulary words with similar distributional properties. Word representations are suitable for adapting lexical features, but a more general solution is to adapt the entire feature representation. One such method is Structural Correspondence Learning (Blitzer et al., 2006, SCL). In SCL, we create artiﬁcial binary classiﬁcation problems for thousands of cross-domain “pivot” features, an"
N16-1157,P14-2088,1,0.758274,"x is comprehensible to contemporary English speakers, but usages such as ‘wild disposed’ and ‘drew unto’ are sufﬁciently unusual as to pose problems for NLP systems trained on contemporary texts. Domain adaptation A more generic machine learning approach is to apply unsupervised domain adaptation techniques, which transform the representations of the training and target texts to be more similar, typically using feature co-occurrence statistics (Blitzer et al., 2006; Ben-David et al., 2010). It is natural to think of historical texts as a distinct domain from contemporary training corpora, and Yang and Eisenstein (2014, 2015) show that the accuracy of historical Portuguese POS tagging can be signiﬁcantly improved by domain adaption. However, we are unaware of prior work that empirically evaluates the efﬁcacy of this approach on Early Modern English texts. Furthermore, historical texts are often associated with multiple metadata attributes (e.g., author, genre, and epoch), each of which may inﬂuence the text’s linguistic properties. Multi-domain adaptation (Mansour et al., 2009) and multi-attribute domain adaptation (Joshi et al., 2013; Yang and Eisenstein, 2015) can potentially exploit these metadata attrib"
N16-1157,N15-1069,1,0.828081,"domain from contemporary training corpora, and Yang and Eisenstein (2014, 2015) show that the accuracy of historical Portuguese POS tagging can be signiﬁcantly improved by domain adaption. However, we are unaware of prior work that empirically evaluates the efﬁcacy of this approach on Early Modern English texts. Furthermore, historical texts are often associated with multiple metadata attributes (e.g., author, genre, and epoch), each of which may inﬂuence the text’s linguistic properties. Multi-domain adaptation (Mansour et al., 2009) and multi-attribute domain adaptation (Joshi et al., 2013; Yang and Eisenstein, 2015) can potentially exploit these metadata attributes to obtain further improvements. This paper presents the ﬁrst comprehensive empirical comparison of effectiveness of these approaches for part-of-speech tagging on historical texts. We focus on the two historical treebanks of the Penn Corpora of Historical English — the Penn Parsed Corpus of Modern British English (Kroch et al., 2010, PPCMBE) and the Penn-Helsinki Parsed Corpus of Early Modern English (Kroch et al., 2004, PPCEME). These datasets enable a range of analyses, which isolate the key issues in dealing with historical corpora: • In on"
N18-1071,P16-1231,0,0.166856,"dge, the first structured gradient tree boosting (SGTB) model for collective entity disambiguation. Building on the general SGTB framework introduced by Yang and Chang (2015), we develop a globally normalized model for ED that employs a conditional random field (CRF) objective (Lafferty et al., 2001). The model permits the utilization of global features defined between the current entity candidate and the entire decision history for previous entity assignments, which enables the global optimization for all the entity mentions in a document. As discussed in prior work (Smith and Johnson, 2007; Andor et al., 2016), globally normalized models are more expressive than locally normalized models. As in many other global models, our SGTB model suffers from the difficulty of computing the partition function (normalization term) for training and inference. We adopt beam search to address this problem, in which we keep track of multiple hypotheses and sum over the paths in the beam. In particular, we propose Bidirectional Beam Search with Gold path (BiBSG) technique that is specifically designed for SGTB model training. Compared to standard beam search strategies, BiBSG reduces model variance and also enjoys t"
N18-1071,D17-1277,0,0.68285,"refore yields: p(y|x) ∝ p(y1:t−1 |x) · p(yt |y1:t−1 , x) ·p(yt |yt+1:t , x) · p(yt+1:T |x), adopted local and global features, and some efforts to make training and inference faster. (10) 4.1 which decomposes into multiplication of a forward probability and a backward probability. In (Sun et al., 2017), these are retrieved from forward and backward recurrent networks, whereas in our work we use the joint scores (log probabilities shown in Eq. 1) computed for partial sequences from forward and backward beams. We use a mention prior pˆ(y|x) to select entity candidates for a mention x. Following Ganea and Hofmann (2017), the prior is computed by averaging mention prior probabilities built from mention-entity hyperlink statistics from Wikipedia3 and a large Web corpus (Spitkovsky and Chang, 2012). Given a mention, we select the top 30 entity candidates according to pˆ(y|x). We also use a simple heuristic proposed by Ganea and Hofmann (2017) to improve candidate selection for persons: for a mention x, if there are mentions of persons that contain x as a continuous subsequence of words, then we consider the candidate set obtained from the longest mention for the mention x. Algorithm 1: Bidirectional Beam Search"
N18-1071,P16-1059,0,0.0986633,"Missing"
N18-1071,D13-1160,0,0.0676012,"Missing"
N18-1071,N13-1122,0,0.0577723,"Missing"
N18-1071,D13-1184,0,0.0238445,"entities, which is usually performed by maximizing the global topical coherence between entities. As discussed above, directly optimizing the coherence objective is computationally intractable, and several heuristics and approximations have been proposed to address the problem. Hoffart et al. (2011) use an iterative heuristic to remove unpromising mention-entity edges. Yamada et al. (2016) employ a two-stage approach, in which global information is incorporated in the second stage based on local decisions from the first stage. Approximate inference techniques have been widely adopted for ED. Cheng and Roth (2013) use an integer linear program (ILP) solver. Belief propagation (BP) and its variant loopy belief propagation (LBP) have been used by Ganea et al. (2016) and Ganea and Hofmann (2017) respectively. We employ another standard approximate inference algorithm, beam search, in this work. To make beam search a better fit for SGTB training, we propose BiBSG that improves beam search training on stability and effectiveness. 7 Conclusion and future work In this paper, we present a structured gradient tree boosting model for entity disambiguation. Entity coherence modeling is challenging, as exact infer"
N18-1071,Q15-1011,0,0.0373516,"Missing"
N18-1071,P04-1015,0,0.113018,"|yt , y1:t−1 , x) is uniform. Therefore, at any given time, there is no information from the future when incorporating the global structure. In this work, we adopt a Bidirectional Beam Search (BiBS) methodology that incorporates multiple beams to take future information into account (Sun et al., 2017). It makes two simplifying assumptions that better approximate the joint probability above while remaining tractable: (1) future predictions are independent of past predictions given yt ; (2) p(yt ) is uniform. These yield the following approximation: Beam search with gold path The early update (Collins and Roark, 2004) and LaSO (Daum´e III and Marcu, 2005; Xu and Fern, 2007) strategies are widely adopted with beam search for updating model parameters in previous work. Both methods keep track of the location of the gold path in the beam while decoding a training sequence. A gradient update step will be taken if the gold path falls out of the beam at a specific time step t or after the last step T . Adapting the strategies to SGTB training is straightforward. We will compute point-wise functional gradients for all candidate entity sequences after time step T or when the gold sequence falls out the beam. Both"
N18-1071,D11-1072,0,0.661716,"Missing"
N18-1071,D07-1074,0,0.505305,"Missing"
N18-1071,P11-1115,0,0.0970969,"Missing"
N18-1071,spitkovsky-chang-2012-cross,0,0.0322058,"aster. (10) 4.1 which decomposes into multiplication of a forward probability and a backward probability. In (Sun et al., 2017), these are retrieved from forward and backward recurrent networks, whereas in our work we use the joint scores (log probabilities shown in Eq. 1) computed for partial sequences from forward and backward beams. We use a mention prior pˆ(y|x) to select entity candidates for a mention x. Following Ganea and Hofmann (2017), the prior is computed by averaging mention prior probabilities built from mention-entity hyperlink statistics from Wikipedia3 and a large Web corpus (Spitkovsky and Chang, 2012). Given a mention, we select the top 30 entity candidates according to pˆ(y|x). We also use a simple heuristic proposed by Ganea and Hofmann (2017) to improve candidate selection for persons: for a mention x, if there are mentions of persons that contain x as a continuous subsequence of words, then we consider the candidate set obtained from the longest mention for the mention x. Algorithm 1: Bidirectional Beam Search with Gold path (BiBSG) Input : input document x, candidate sequences {y}, joint scoring function S(x, yt1 :t2 ) Output: beam sequence set C C←∅ while not converged do // forward"
N18-1071,W03-0419,0,0.410729,"Missing"
N18-1071,P09-1113,0,0.0511429,"Missing"
N18-1071,K16-1025,0,0.277905,"posed into the summation of a local feature function φL (x, yt ) and a global feature function φG (yt , y1:t−1 ). Local features We consider standard local features that have been used in prior work, including mention priors p(y|x) obtained from different resources; entity popularity features based on Wikipedia page view count statistics;4 named entity recognition (NER) type features given by an in-house NER system trained on the CoNLL 2003 NER data (Tjong Kim Sang and De Meulder, 2003); entity type features based on Freebase type information; and three textual similarity features proposed by Yamada et al. (2016).5 The full inference algorithm, Bidirectional Beam Search with Gold path (BiBSG), is presented in Alg. 1. When performing the forward pass to update the forward beam, forward joint scores, S(x, y1:t ), are computed with respect to current forward beam, and backward joint scores, S(x, yT :t ), are computed with respect to previous backward beam. A similar procedure is used for the backward pass. The search converges very fast, and we use two rounds of bidirectional search as a good approximation. Finally, SGTB-BiBSG compares the conditional probabilities p(y(·) |x) of the best scoring output s"
N18-1071,P15-1049,1,0.846889,"ariety of downstream applications 1 When ready, the code will be published at https:// github.com/bloomberg/sgtb. 777 Proceedings of NAACL-HLT 2018, pages 777–786 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 global gradient tree boosting model that produces coherent entity assignments for all the mentions in a document is still an open question. In this work, we present, to the best of our knowledge, the first structured gradient tree boosting (SGTB) model for collective entity disambiguation. Building on the general SGTB framework introduced by Yang and Chang (2015), we develop a globally normalized model for ED that employs a conditional random field (CRF) objective (Lafferty et al., 2001). The model permits the utilization of global features defined between the current entity candidate and the entire decision history for previous entity assignments, which enables the global optimization for all the entity mentions in a document. As discussed in prior work (Smith and Johnson, 2007; Andor et al., 2016), globally normalized models are more expressive than locally normalized models. As in many other global models, our SGTB model suffers from the difficulty"
N18-1071,N15-1026,0,0.410876,"ain testing) and all other datasets (cross-domain testing). We follow prior work and report in-KB accuracies for AIDA-test and Bag-of-Title (BoT) F1 scores for the other test sets. Two AIDA-CoNLL specific resources have been widely used in previous work. In order to have fair comparisons with these works, we also adopt them only for the AIDA datasets. First, we use a mention prior obtained from aliases to candidate entities released by Hoffart et al. (2011) along with the two priors described in § 4.1. Second, we also experiment with PPRforNED, an entity candidate selection system released by Pershina et al. (2015). It is unclear how candidates were pruned, but the entity candidates generated by this system have high recall and low ambiguity, and they contribute to some of the best results reported for AIDA-test (Yamada et al., 2016; Sil et al., 2018). Experiments In this section, we evaluate SGTB on some of the most popular datasets for ED. After describing the experimental setup, we compare SGTB with previous state-of-the-art (SOTA) ED systems and present our main findings in § 5.3. 5.1 # doc further split into training (AIDA-train), development (AIDA-dev), and test (AIDA-test) sets.7 AQUAINT (Milne a"
N18-1071,P15-1128,0,0.0715153,"Missing"
N18-1071,P11-1138,0,0.168333,"Missing"
N18-1071,J07-4003,0,0.0319143,"to the best of our knowledge, the first structured gradient tree boosting (SGTB) model for collective entity disambiguation. Building on the general SGTB framework introduced by Yang and Chang (2015), we develop a globally normalized model for ED that employs a conditional random field (CRF) objective (Lafferty et al., 2001). The model permits the utilization of global features defined between the current entity candidate and the entire decision history for previous entity assignments, which enables the global optimization for all the entity mentions in a document. As discussed in prior work (Smith and Johnson, 2007; Andor et al., 2016), globally normalized models are more expressive than locally normalized models. As in many other global models, our SGTB model suffers from the difficulty of computing the partition function (normalization term) for training and inference. We adopt beam search to address this problem, in which we keep track of multiple hypotheses and sum over the paths in the beam. In particular, we propose Bidirectional Beam Search with Gold path (BiBSG) technique that is specifically designed for SGTB model training. Compared to standard beam search strategies, BiBSG reduces model varia"
N18-2056,P17-1178,0,0.0262044,"with previous work, but explore the use of external knowledge base (Radford et al., 2015) as constraints. Qn i=1 exp(θ · f (yi−1 , yi , φ(x))) , Z where y are predicted labels and Z is the normalizing constant. 3.1.1 Entity Embeddings We extend the BiLSTM-CRF model by adding entity embedding channel to the embedding layer. As a result, xi is the concatenation of word embedding, character embedding and its entity embedding, xi = [ωi , ci , gi ]. Entity embeddings are derived from a noisy gazetteer created using Wikipedia articles. The gazetteer is derived from the word-entity statistics from (Pan et al., 2017). More specifically, each coordinate of the entity embedding is the probability distribution of a word occurring as the corresponding entity type. 3.1.2 Domain Adaption To explore external datasets, we apply MT BiLSTM-CRF with domain adaptions, as illustrated in Figure 1(b). The fully connection layer are adapted to different datasets. The CRF features are computed separately, i.e. φT (x) = GT · h, φS (x) = GS · h for target and source dataset respectively. The loss function p(y|x; θT ) and p(y|x; θS ) are optimized in alternating order. 3 Approach This section describes the baseline model use"
N18-2056,D14-1162,0,0.0813453,"n general. Table 3 presents the performance impact of knowledge based constrained decoding. It is worth noting that the performance gain in the Chinese language is more limited in comparison with English. The primary reason behind this is that the English Wikipedia site is more comprehensive than its Chinese counterpart. Constrained decoding does not change the NOM performance because only name mentions are included in the knowledge base. Baseline The baseline is a BiLSTM-CRF model with word and character embeddings which simply combines source and target data as training data. GloVe vectors (Pennington et al., 2014) are used as word embeddings. NAM and NOM models are trained separately with individually tuned parameters. 4.3 NOM 0.587 0.626 0.634 0.305 0.351 0.364 Table 2: Effectiveness of Multi-Task Data Selection (MTDS). (Song et al., 2015) entity annotations as source datasets. It is worth noting that annotation guidelines are different from one dataset to another, especially for nominal entity annotations. 4.2 NAM 0.842 0.842 0.842 0.851 0.851 0.851 Results First, we examine the performance impact of entity embedding. As shown in Table 1, entity embedding is very useful for both NAM and NOM predictio"
N18-2056,D15-1058,0,0.259132,"al., 2017). However, we introduce additional channel in the embedding layer(Peng and Dredze, 2016). The idea of multi-task data selection is derived from topics of data selection (Moore and Lewis, 2010) and instance weighting (Jiang and Zhai, 2007) from the transfer learning community. Different from previous work, we propose an adaptive selection approach interleaved with MT BiLSTMCRF model training. Decoding with global constraints has been studied in (Yarowsky, 1993; Krishnan and Manning, 2006). Here we share similar ideas with previous work, but explore the use of external knowledge base (Radford et al., 2015) as constraints. Qn i=1 exp(θ · f (yi−1 , yi , φ(x))) , Z where y are predicted labels and Z is the normalizing constant. 3.1.1 Entity Embeddings We extend the BiLSTM-CRF model by adding entity embedding channel to the embedding layer. As a result, xi is the concatenation of word embedding, character embedding and its entity embedding, xi = [ωi , ci , gi ]. Entity embeddings are derived from a noisy gazetteer created using Wikipedia articles. The gazetteer is derived from the word-entity statistics from (Pan et al., 2017). More specifically, each coordinate of the entity embedding is the proba"
N18-2056,W15-0812,0,0.0237704,"s that the English Wikipedia site is more comprehensive than its Chinese counterpart. Constrained decoding does not change the NOM performance because only name mentions are included in the knowledge base. Baseline The baseline is a BiLSTM-CRF model with word and character embeddings which simply combines source and target data as training data. GloVe vectors (Pennington et al., 2014) are used as word embeddings. NAM and NOM models are trained separately with individually tuned parameters. 4.3 NOM 0.587 0.626 0.634 0.305 0.351 0.364 Table 2: Effectiveness of Multi-Task Data Selection (MTDS). (Song et al., 2015) entity annotations as source datasets. It is worth noting that annotation guidelines are different from one dataset to another, especially for nominal entity annotations. 4.2 NAM 0.842 0.842 0.842 0.851 0.851 0.851 Results First, we examine the performance impact of entity embedding. As shown in Table 1, entity embedding is very useful for both NAM and NOM prediction tasks, and for both languages. It provides an overall performance improvement of 2.2 F1 points. Since the entity embeddings are derived from soft gazetteer features, this experiment confirms again the usefulness of gazetteer even"
N18-2056,P07-1034,0,0.00993192,"sk model with domain adaptions. apple is more likely to be a ORG when it occurs in the same discussion forum with Apple Inc. 2 Related Works p(y|x; θ) = There are many works in literature applying neural networks to ER problems (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2017; Peng and Dredze, 2016). The baseline model of this work is mostly closed to (Yang et al., 2017). However, we introduce additional channel in the embedding layer(Peng and Dredze, 2016). The idea of multi-task data selection is derived from topics of data selection (Moore and Lewis, 2010) and instance weighting (Jiang and Zhai, 2007) from the transfer learning community. Different from previous work, we propose an adaptive selection approach interleaved with MT BiLSTMCRF model training. Decoding with global constraints has been studied in (Yarowsky, 1993; Krishnan and Manning, 2006). Here we share similar ideas with previous work, but explore the use of external knowledge base (Radford et al., 2015) as constraints. Qn i=1 exp(θ · f (yi−1 , yi , φ(x))) , Z where y are predicted labels and Z is the normalizing constant. 3.1.1 Entity Embeddings We extend the BiLSTM-CRF model by adding entity embedding channel to the embeddin"
N18-2056,D17-1018,0,0.0228023,"duction Entity Recognition (ER) is a fundamental task in Natural Language Processing (NLP). The task includes named entity recognition and nominal entity recognition. ER is the building blocks for higher level applications such as natural language understanding, question answering, machine reading comprehension, etc. They are usually treated as sequence labeling problems. Although the topics have been studied extensively for the past several decades, development of neural network and deep learning based methods in recent years (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2017; Kenton Lee and Zettlemoyer, 2017; Xinchi Chen, 2017) significantly improves the previous state-of-the-art. ∗ Multi-Task Data Selection To ensure homogeneity between source and target training data, adaptive training data selection is applied to source data during multi-task learning, to filter out instances with different distribution and misaligned annotation guideline. Data selection is interleaved with model training iteratively, and this training process terminates until convergence. Constrained Decoding using Knowledge Base Knowledge-based constraints are enforced at decoding time. The goal is to capture document level"
N18-2056,P17-1114,0,0.0260452,"chitecture for ER is BiLSTM-CRF (Lample et al., 2016). The architecture has been shown to achieve best performance on many sequence labeling tasks. In addition, the architecture can be easily extended to model different sources of training data. In real world applications, it is important to include external data sources for model training, because using only domain-specific data for training is usually not enough to achieve best performance. For example, in the case of KBP 2016 tracks, both the 1st and the 2nd teams (ranking in the NERC evaluation) use external data source (Liu et al., 2016; Xu et al., 2017) for model training. The challenge here is to transfer knowledge from external data source to target data source. Multi-Task (MT) BiLSTM-CRF architecture (Yang et al., 2017) is designed for this knowledge transfer. In this work, we develop an ER model based on the MT BiLSTM-CRF architecture, with additional entity embeddings and domain adaption. Two novel methods are proposed to further improve the model performance. Entity recognition is a widely benchmarked task in natural language processing due to its massive applications. The state-of-the-art solution applies a neural architecture named B"
N18-2056,H93-1052,0,0.195758,"ple et al., 2016; Ma and Hovy, 2016; Yang et al., 2017; Peng and Dredze, 2016). The baseline model of this work is mostly closed to (Yang et al., 2017). However, we introduce additional channel in the embedding layer(Peng and Dredze, 2016). The idea of multi-task data selection is derived from topics of data selection (Moore and Lewis, 2010) and instance weighting (Jiang and Zhai, 2007) from the transfer learning community. Different from previous work, we propose an adaptive selection approach interleaved with MT BiLSTMCRF model training. Decoding with global constraints has been studied in (Yarowsky, 1993; Krishnan and Manning, 2006). Here we share similar ideas with previous work, but explore the use of external knowledge base (Radford et al., 2015) as constraints. Qn i=1 exp(θ · f (yi−1 , yi , φ(x))) , Z where y are predicted labels and Z is the normalizing constant. 3.1.1 Entity Embeddings We extend the BiLSTM-CRF model by adding entity embedding channel to the embedding layer. As a result, xi is the concatenation of word embedding, character embedding and its entity embedding, xi = [ωi , ci , gi ]. Entity embeddings are derived from a noisy gazetteer created using Wikipedia articles. The g"
P14-2088,W06-1615,0,0.871234,"248194 295154 148061 182208 91582 57477 0 83938 117515 148061 126516 Overall 1480528 625089 34137 84465 130327 115062 115252 0 0 0 0 148519 49194 62387 0 55692 0 60404 0 0 0 0 0 479243 315792 60404 Table 1: Statistics of the Tycho Brahe Corpus CRF tagger We use a conditional random field tagger, choosing CRFsuite because it supports arbitrary real valued features (Okazaki, 2007), with SGD optimization. Following the work of Nogueira Dos Santos et al. (2008) on this dataset, we apply the feature set of Ratnaparkhi (1996). There are 16 feature templates and 372, 902 features in total. Following Blitzer et al. (2006), we consider pivot features that appear more than 50 times in all the domains. This leads to a total of 1572 pivot features in our experiments. Results Table 2 presents results for different domain adaptation tasks. We also compute the transfer raaccuracy tio, which is defined as adaptation baseline accuracy , shown in Figure 1. The generally positive trend of these graphs indicates that adaptation becomes progressively more important as we select test sets that are more temporally remote from the training data. In general, mDA outperforms SCL and PCA, the latter of which shows little improve"
P14-2088,P07-1056,0,0.697883,"red feature spaces that characterize syntactic tasks such as sequence labeling and parsing (Smith, 2011). As we will show, substantial efficiency improvements can be obtained by designing domain 538 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 538–544, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics on older documents. Both structure-aware domain adaptation algorithms perform as well as standard dropout — and better than the wellknown structural correspondence learning (SCL) algorithm (Blitzer et al., 2007) — but structured dropout is more than an order-of-magnitude faster. As a secondary contribution of this paper, we demonstrate the applicability of unsupervised domain adaptation to the syntactic analysis of historical texts. 2 High-dimensional setting Structured prediction tasks often have much more features than simple bag-of-words representation, and performance relies on the rare features. In a naive implementation of the denoising approach, both P and Q will be dense matrices with dimensionality d × d, which would be roughly 1011 elements in our experiments. To solve this problem, Chen et"
P14-2088,P09-1056,0,0.0228206,"consider the domain adaptation problem of training on recent data and testing on older historical text. on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks (Hinton et al., 2012; Wang et al., 2013). On the specific problem of sequence labeling, Xiao and Guo (2013) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model. Dhillon et al. (2011) presented a spectral method to estimate low dimensional context-specific word representations for sequence labeling. Huang and Yates (2009; 2012) used an HMM model to learn latent representations, and then leverage the Posterior Regularization framework to incorporate specific biases. Unlike these methods, our approach uses a standard CRF, but with transformed features. 5 Conclusion and Future Work Denoising autoencoders provide an intuitive solution for domain adaptation: transform the features into a representation that is resistant to the noise that may characterize the domain adaptation process. The original implementation of this idea produced this noise directly (Glorot et al., 2011b); later work showed that dropout noise"
P14-2088,D12-1120,0,0.0961259,"Missing"
P14-2088,P07-1034,0,0.870946,"ject the entire dataset onto a low-dimensional sub-space (while still including the original features). Finally, we compare against Structural Correspondence Learning (SCL; Blitzer et al., 2006), another feature learning algorithm. In all cases, we include the entire dataset to compute the feature projections; we also conducted experiments using only the test and training data for feature projections, with very similar results. Related Work Domain adaptation Most previous work on domain adaptation focused on the supervised setting, in which some labeled data is available in the target domain (Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009). Our work focuses on unsupervised domain adaptation, where no labeled data is available in the target domain. Several representation learning methods have been proposed to solve this problem. In structural correspondence learning (SCL), the induced representation is based on the task of predicting the presence of pivot features. Autoencoders apply a similar idea, but use the denoised instances as the latent representation (Vincent et al., 2008; Glorot et al., 2011b; Chen et al., 2012). Within the context of denoising autoencoders, we have focused P"
P14-2088,P07-1033,0,0.906734,"Missing"
P14-2088,D07-1041,0,0.198289,"Missing"
P14-2088,N13-1037,1,0.800888,"imple and efficient feature projection. Applied to the task of fine-grained part-of-speech tagging on a dataset of historical Portuguese, marginalized structured dropout yields state-of-the-art accuracy while increasing speed by more than an order-ofmagnitude over previous work. 1 Introduction Unsupervised domain adaptation is a fundamental problem for natural language processing, as we hope to apply our systems to datasets unlike those for which we have annotations. This is particularly relevant as labeled datasets become stale in comparison with rapidly evolving social media writing styles (Eisenstein, 2013), and as there is increasing interest in natural language processing for historical texts (Piotrowski, 2012). While a number of different approaches for domain adaptation have been proposed (Pan and Yang, 2010; Søgaard, 2013), they tend to emphasize bag-ofwords features for classification tasks such as sentiment analysis. Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing (Smith, 2011). As we will show, substantial effic"
P14-2088,N09-1068,0,0.174542,"Missing"
P14-2088,N03-1033,0,0.0461648,"et al. (2012) showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising (Wang et al., 2013). While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 105 or more features. In this paper we investigate noising functions that are explicitly designed for structured feature spaces, which are common in NLP. For example, in part-of-speech tagging, Toutanova et al. (2003) define several feature “templates”: the current word, the previous word, the suffix of the current word, and so on. For each feature template, there are thousands of binary features. To exploit this structure, we propose two alternative noising techniques: (1) feature scrambling, which randomly chooses a feature template and randomly selects an alternative value within the template, and (2) structured dropout, which randomly eliminates all but a single feature template. We show how it is possible to marginalize over both types of noise, and find that the solution for structured dropout is sub"
P14-2088,D13-1117,0,0.151336,"ct adaptation methods for learning in structured feature spaces. We build on work from the deep learning community, in which denoising autoencoders are trained to remove synthetic noise from the observed instances (Glorot et al., 2011a). By using the autoencoder to transform the original feature space, one may obtain a representation that is less dependent on any individual feature, and therefore more robust across domains. Chen et al. (2012) showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising (Wang et al., 2013). While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 105 or more features. In this paper we investigate noising functions that are explicitly designed for structured feature spaces, which are common in NLP. For example, in part-of-speech tagging, Toutanova et al. (2003) define several feature “templates”: the current word, the previous word, the suffix of the current word, and so on. For each feature template, there are thousands"
P14-2088,W96-0213,0,\N,Missing
P15-1049,E06-1002,0,0.0203941,"As Guo et al. (2013) shows that most mentions in tweets should be linked to the most popular entities, IE setting actually pays more attention on mention detection sub-problem. In contrast to IE setting, IR setting focuses on entity disambiguation, since we only need to decide whether the tweet is relevant to the query entity. Therefore, we believe that both evaluation policies are needed for tweet entity linking. Balance Precision and Recall Figure 2 shows the results of tuning the bias term for balancing 511 References Early research on entity linking has focused on well written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008). Due to the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to p"
P15-1049,W02-1001,0,0.0978265,"statistical features such as the probability of the surface to be used as anchor text in Wikipedia. We also add additional Entity Type features correspond to the following entity types: Character, Event, Product and Brand. Finally, we include several NER features to indicate each mention candidate belongs to one the following NER types: Twitter user, Twitter hashtag, Person, Location, Organization, Product, Event and Date. Algorithms Table 2 summarizes all the algorithms that are compared in our experiments. First, we consider two linear structured learning algorithms: Structured Perceptron (Collins, 2002) and Linear Structured SVM (SSVM) (Tsochantaridis et al., 2004). For non-linear models, we consider polynomial SSVM, which employs polynomial kernel inside the structured SVM algorithm. We also include LambdaRank (Quoc and Le, 2007), a neuralbased learning to rank algorithm, which is widely used in the information retrieval literature. We further compare with MART, which is designed for performing multiclass classification using log loss without considering the structured information. Finally, we have our proposed log-loss SMART algorithm, as described in Section 3. 9 Note that our baseline sy"
P15-1049,P14-1001,0,0.0180591,"ed by popular entities (e.g. new york in Figure 1). 5 Related Work Linear structured learning methods have been proposed and widely used in the literature. Popular models include Structured Perceptron (Collins, 2002), Conditional Random Field (Lafferty et al., 2001) and Structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCRF (Dietterich et al., 2004) is the only work that explores tree-"
P15-1049,D07-1074,0,0.190164,"Missing"
P15-1049,Q14-1021,1,0.906986,"Γ(t−i )| • Do structured entity linking models perform better than non-structured ones? • How can we best capture the relationships between entities? 4.1 Evaluation Methodology and Data We evaluate each entity linking system using two evaluation policies: Information Extraction (IE) driven evaluation and Information Retrieval (IR) driven evaluation. For both evaluation settings, precision, recall and F1 scores are reported. Our data is constructed from two publicly available sources: Named Entity Extraction & Linking (NEEL) Challenge (Cano et al., 2014) datasets, and the datasets released by Fang and Chang (2014). Note that we gather two datasets from Fang and Chang (2014) and they are used in two different evaluation settings. We refer to these two datasets as TACL-IE and TACL-IR, respectively. We perform some data cleaning and unification on Beyond S- MART: Modeling entity-entity relationships It is important for entity linking systems to take advantage of the entity-to-entity information while making local decisions. For instance, the identification of entity “eli manning” leads to a strong clue for linking “new york giants” to the NFL team. Instead of defining a more complicated structure and lear"
P15-1049,D11-1141,0,0.0471981,"Missing"
P15-1049,N13-1122,1,0.943744,"ion candidate has different own entity sets. 5 Sorting helps the algorithms find non-overlapping candidates. 507 sion by two-stage approach as the solution for modeling entity-entity relationships after we found that SMART achieves high precision and reasonable recall. Specifically, in the first stage, the system identifies all possible entities with basic features, which enables the extraction of entity-entity features. In the second stage, we re-train S- MART on a union of basic features and entity-entity features. We define entity-entity features based on the Jaccard distance introduced by Guo et al. (2013). Let Γ(ei ) denotes the set of Wikipedia pages that contain a hyperlink to an entity ei and Γ(t−i ) denotes the set of pages that contain a hyperlink to any identified entity ej of the tweet t in the first stage excluding ei . The Jaccard distance between ei and t is β(uK , K) =1 X exp(F (x, yk+Q = uk+Q )) β(uk , k) = uk+Q · Q−1 Y exp(F (x, yk+q = Nil)) q=1 · β(uk+Q , k + Q) (5) where k + Q is the index of the next nonoverlapping mention candidate. Note that the third terms of equation (4) or (5) will vanish if there are no corresponding non-overlapping mention candidates. Given the potential"
P15-1049,P11-1115,0,0.0237247,"the raise of social media, many techniques have been proposed or tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011). Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems (Fang and Chang, 2014). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity linking research (Ji et al., 2010; Ji and Grishman, 2011; Cano and others, 2014; Carmel et al., 2014). Building an end-to-end entity linking system involves in solving two interrelated sub-problems: mention detection and entity disambiguation. Earlier research on entity linking has been largely focused on the entity disambiguation problem, including most work on entity linking for wellwritten documents such as news and encyclopedia articles (Cucerzan, 2007) and also few for tweets (Liu et al., 2013). Recently, people have focused on building systems that consider mention detection and entity disambiguation jointly. For example, Cucerzan (2012) dela"
P15-1049,D13-1170,0,0.00140779,"ect is that S- MART can easily eliminate some common errors caused by popular entities (e.g. new york in Figure 1). 5 Related Work Linear structured learning methods have been proposed and widely used in the literature. Popular models include Structured Perceptron (Collins, 2002), Conditional Random Field (Lafferty et al., 2001) and Structured SVM (Taskar et al., 2004; Tsochantaridis et al., 2005). Recently, many structured learning models based on neural networks have been proposed and are widely used in language modeling (Bengio et al., 2006; Mikolov et al., 2010), sentiment classification (Socher et al., 2013), as well as parsing (Socher et al., 2011). Cortes et al. (2014) recently proposed a boosting framework which treats different structured learning algorithms as base learners to ensemble structured prediction results. Tree-based models have been shown to provide more robust and accurate performances than neural networks in some tasks of computer vision (Roe et al., 2005; Babenko et al., 2011) and information retrieval (Li et al., 2007; Wu et al., 2010), suggesting that it is worth to investigate tree-based non-linear models for structured learning problems. To the best of our knowledge, TreeCR"
P15-1049,P13-1128,0,0.0263518,"Missing"
P15-1075,P12-2054,0,0.0629908,"Missing"
P15-1075,D11-1024,0,0.386202,"Missing"
P15-1075,P11-2118,0,0.0717788,"Missing"
P15-1075,D12-1087,0,0.039024,"Missing"
P19-1038,D14-1162,0,0.0820109,"forced alignment step, for each sentence in the conference call transcript, we obtain the sentence text as well as its corresponding audio clip4 . 2 https://seekingalpha.com/ https://earningscast.com/ 4 It is worth noting that some third-party data provider companies provide human-annotated transcript text and audio recording alignment. In that case, text-audio forced alignment step may not be necessary. 3 392 Textual Features We use pre-trained word embeddings and calculate the arithmetic mean of word vector in each sentence as the sentence representation. We choose the embedding GloVe-300 (Pennington et al., 2014) pre-trained on Wikipedia and Gigaword 55 . Therefore, each sentence is represented as a 300-dimension vector. Audio Features We use Praat (Boersma and Van Heuven, 2001) to extract vocal features, such as pitch, intensity, jitter, HNR(Harmonic to Noise Ratio) and etc, from audio recordings. A total of 27 vocal features are extracted by Praat. In summary, for each sentence in an earnings conference call, we generate a 300-dimension text vector and a 27-dimension audio vector to represent verbal and vocal features separately. Data Statistics We build our dataset by acquiring all S&P 500 companie"
P19-1038,P17-1081,0,0.291122,"l learning: Despite our financial domain, our approach is relevant to multimodal learning using text and audio. Recent studies on speech communication have shown that a speaker’s acoustic features, such as voice pitch, amplitude, and intensity, are highly correlated with the speaker’s emotion (Bachorowski, 1999), deception or trustworthiness(Sporer and Schwandt, 2006; Belin et al., 2017), anxiety (Laukka et al., 2008) and confidence or doubt (Jiang and Pell, 2017). Recently, multimodal learning has drawn attentions for different applications, such as sentiment analysis (Zadeh et al., 2016b,a; Poria et al., 2017; Luo et al., 2018), image caption generation (You et al., 2016), suicide risk detection (Scherer et al., 2016), crime drama understanding (Frermann et al., 2018) and human trafficking detection (Tong et al., 2017). To the best of our knowledge, this work presents the first multimodal deep learning model using text and audio features for a financial markets application. Earnings Conference Calls Dataset In this section, we present dataset details. 4.1 Data Acquisition Conference call transcripts have been extensively studied in prior research. However, there is no existing conference call audi"
P19-1038,Q18-1001,0,0.0145399,"own that a speaker’s acoustic features, such as voice pitch, amplitude, and intensity, are highly correlated with the speaker’s emotion (Bachorowski, 1999), deception or trustworthiness(Sporer and Schwandt, 2006; Belin et al., 2017), anxiety (Laukka et al., 2008) and confidence or doubt (Jiang and Pell, 2017). Recently, multimodal learning has drawn attentions for different applications, such as sentiment analysis (Zadeh et al., 2016b,a; Poria et al., 2017; Luo et al., 2018), image caption generation (You et al., 2016), suicide risk detection (Scherer et al., 2016), crime drama understanding (Frermann et al., 2018) and human trafficking detection (Tong et al., 2017). To the best of our knowledge, this work presents the first multimodal deep learning model using text and audio features for a financial markets application. Earnings Conference Calls Dataset In this section, we present dataset details. 4.1 Data Acquisition Conference call transcripts have been extensively studied in prior research. However, there is no existing conference call audio dataset. Therefore, we set up our S&P 500 Earnings Conference Calls dataset by acquiring audio records and text transcripts from the following two sources. Earn"
P19-1038,P17-1157,0,0.52987,"wing two lines of research: financial risk prediction with multimedia data: It is a received wisdom in economics and finance that one can predict a stock’s risk using historical information (Bernard et al., 2007). Various work has studied the problem of financial risk prediction using firm financial reports. A pioneer work (Kogan et al., 2009) shows that simple bagof-words features in firm annual report (Form 10Ks) combined with historical volatility can simply outperform statistical models that is built upon historical volatility only. Other work (Tsai and Wang, 2014; Nopp and Hanbury, 2015; Rekabsaz et al., 2017; Theil et al., 2018; Wang and Hua, 2014) also proposes different document representation methods to predict stock price volatility. To the best of our knowledge, none of existing NLP research on stock volatility prediction considers the usage of vocal features from audio data, especially the interplay between vocal and verbal features. In finance research, only two studies (Mayew and Venkatachalam, 2012; Hobson et al., 2012) have examined the executive voice in earnings calls. However, they extract CEO’s affective state from a blackbox third-party audio processing software, the validity of wh"
P19-1038,N09-1031,0,0.471182,"l text regression and multimodal learning. We then present our earnings conference call dataset and how data is processed in Section 4. In section 5, we introduce our multimodal learning framework that fuses ver391 3 4 Related Work Our work is closely related with the following two lines of research: financial risk prediction with multimedia data: It is a received wisdom in economics and finance that one can predict a stock’s risk using historical information (Bernard et al., 2007). Various work has studied the problem of financial risk prediction using firm financial reports. A pioneer work (Kogan et al., 2009) shows that simple bagof-words features in firm annual report (Form 10Ks) combined with historical volatility can simply outperform statistical models that is built upon historical volatility only. Other work (Tsai and Wang, 2014; Nopp and Hanbury, 2015; Rekabsaz et al., 2017; Theil et al., 2018; Wang and Hua, 2014) also proposes different document representation methods to predict stock price volatility. To the best of our knowledge, none of existing NLP research on stock volatility prediction considers the usage of vocal features from audio data, especially the interplay between vocal and ve"
P19-1199,K16-1002,0,0.700614,"fferent syntactic tree templates. 1 VP NP DT The SBAR NN book VBZ WHNP S ADJP is IN NP VP that PRP VBP you . . JJ good love Figure 1: An example of a constituency tree structure. Introduction Neural language models based on recurrent neural networks (Mikolov et al., 2010) and sequence-tosequence architectures (Sutskever et al., 2014) have revolutionized the NLP world. Deep latent variable modes, in particular, the variational autoencoders (VAE) (Kingma and Welling, 2014; Rezende et al., 2014) integrating inference models with neural language models have been widely adopted on text generation (Bowman et al., 2016; Yang et al., 2017; Kim et al., 2018), where the encoder and the decoder are modeled by long short-term memory ∗ Part of this work was done when the first two authors were at Bloomberg. (LSTM) networks (Chung et al., 2014). For a random vector from the latent space representing an unseen input, the decoder can generate realisticlooking novel data in the context of a text model, making the VAE an attractive generative model. Compared to simple neural language models, the latent representation in a VAE is supposed to give the model more expressive capacity. Although syntactic properties can be"
P19-1199,P17-1177,0,0.121563,"e neural language models, the latent representation in a VAE is supposed to give the model more expressive capacity. Although syntactic properties can be implicitly discovered by such generative models, Shi et al. (2016) show that many deep structural details are still missing in the generated text. As a result of the absence of explicit syntactic information, generative models often produce ungrammatical sentences. To address this problem, recent works attempt to leverage explicit syntactic knowledge to improve the quality of machine translation (Eriguchi et al., 2016; Bastings et al., 2017; Chen et al., 2017) and achieve good results. Motivated by such success, we suggest that deep latent variable models for text generation can also benefit from the incorporation of syntactic knowledge. Instead of solely modeling sentences, we want to utilize augmented data by introducing an auxiliary input, a syntactic tree, to enrich the latent representation and make the generated sentences more grammatical and fluent. Syntactic trees can either be obtained from existing human-labeled 2069 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2069–2078 c Florence, Italy,"
P19-1199,D16-1257,0,0.0227149,"empera. The difference is that SIVAE-c first selects possible syntactic tree templates using the conditional prior network pψ (z y |z x ) then generates paraphrases based on the syntactic template and the latent variable. 4 Related Work Syntax-Aware Neural Text Generation The ability to generate sentences is core to many NLP tasks, such as machine translation (Bahdanau et al., 2015), summarization (Rush et al., 2015), and dialogue generation (Vinyals and Le, 2015). Recent works have shown that neural text generation can benefit from the incorporation of syntactic knowledge (Shen et al., 2018; Choe and Charniak, 2016). Sennrich and Haddow (2016) propose to augment each source word representation with its corresponding part-of-speech tag, lemmatized form and dependency label; Eriguchi et al. (2016) and Bastings et al. (2017) utilize a tree-based encoder and a graph convolutional network encoder respectively to embed the syntactic parse trees as part of the source sentence representations; Chen et al. (2017) model source-side syntactic trees with a bidirectional tree encoder and tree-coverage decoder; Eriguchi et al. (2017) implicitly leverage linguistic prior by treating syntactic parsing as an auxiliary ta"
P19-1199,P16-1078,0,0.171231,"attractive generative model. Compared to simple neural language models, the latent representation in a VAE is supposed to give the model more expressive capacity. Although syntactic properties can be implicitly discovered by such generative models, Shi et al. (2016) show that many deep structural details are still missing in the generated text. As a result of the absence of explicit syntactic information, generative models often produce ungrammatical sentences. To address this problem, recent works attempt to leverage explicit syntactic knowledge to improve the quality of machine translation (Eriguchi et al., 2016; Bastings et al., 2017; Chen et al., 2017) and achieve good results. Motivated by such success, we suggest that deep latent variable models for text generation can also benefit from the incorporation of syntactic knowledge. Instead of solely modeling sentences, we want to utilize augmented data by introducing an auxiliary input, a syntactic tree, to enrich the latent representation and make the generated sentences more grammatical and fluent. Syntactic trees can either be obtained from existing human-labeled 2069 Proceedings of the 57th Annual Meeting of the Association for Computational Ling"
P19-1199,P17-2012,0,0.014507,"ation can benefit from the incorporation of syntactic knowledge (Shen et al., 2018; Choe and Charniak, 2016). Sennrich and Haddow (2016) propose to augment each source word representation with its corresponding part-of-speech tag, lemmatized form and dependency label; Eriguchi et al. (2016) and Bastings et al. (2017) utilize a tree-based encoder and a graph convolutional network encoder respectively to embed the syntactic parse trees as part of the source sentence representations; Chen et al. (2017) model source-side syntactic trees with a bidirectional tree encoder and tree-coverage decoder; Eriguchi et al. (2017) implicitly leverage linguistic prior by treating syntactic parsing as an auxiliary task. However, most of these syntax-aware generation works only focus on neural machine translation. Deep Latent Variable Models Deep latent variable models that combine the complementary strengths of latent variable models and deep learning have drawn much attention recently. Generative adversarial networks (Goodfellow et al., 2014) and variational autoencoders (Kingma and Welling, 2014) are the two families of deep generative models that are widely adopted in applications. As VAEs allow discrete generation fr"
P19-1199,N18-1108,0,0.0200239,"n be utilized to achieve paraphrase generation (Hasan et al., 2016; Mallinson et al., 2017). Recently, Iyyer et al. (2018) proposed to syntactically control the generated paraphrase and Gupta et al. (2018) generate paraphrases in a deep generative architecture. However, all these methods assume the existence of some parallel paraphrase corpora while unsupervised paraphrase generation has been little explored. 5 Experiments We conduct our experiments on two datasets: sentence-level Penn Treebank (Marcus et al., 1993) with human-constituted parse trees and a 90 million word subset of Wikipedia (Gulordava et al., 2018) with parsed trees. When the decoder is too strong, VAE suffers from posterior collapse where the model learns to ignore the latent variable (Bowman et al., 2016). To avoid posterior collapse, KLterm annealing and dropping out words during decoding are employed for training in this work. We also tried an advanced method replacing Gaussian priors with von Mises-Fisher priors (Xu and Durrett, 2018) to prevent KL collapse, but the results 2073 PTB Model PPL Standard NLL KL KN5 LSTM-LM VAE 145 110 112 132 124 125 SIVAE-c SIVAE-i 98(1.6) 90(1.7) 121(53) 119(60) wiki90M PPL Inputless NLL PPL Standar"
P19-1199,C16-1275,0,0.0958148,"ing text generation (Bowman et al., 2016; Yang et al., 2017; Xu and Durrett, 2018; Shen et al., 2019; Wang et al., 2019). The flexibility of VAEs also enables adding conditions during inference to perform controlled language generation (Hu et al., 2017; Zhao et al., 2017). Divergent from these VAE-based text generation models, our work decouples the latent representations corresponding to the sentence and its syntactic tree respectively. Paraphrase Generation Due to the similarity between two tasks, neural machine-translationbased models can often be utilized to achieve paraphrase generation (Hasan et al., 2016; Mallinson et al., 2017). Recently, Iyyer et al. (2018) proposed to syntactically control the generated paraphrase and Gupta et al. (2018) generate paraphrases in a deep generative architecture. However, all these methods assume the existence of some parallel paraphrase corpora while unsupervised paraphrase generation has been little explored. 5 Experiments We conduct our experiments on two datasets: sentence-level Penn Treebank (Marcus et al., 1993) with human-constituted parse trees and a 90 million word subset of Wikipedia (Gulordava et al., 2018) with parsed trees. When the decoder is too"
P19-1199,P13-2121,0,0.0278607,"12(1.0) 16(1.9) 278(2.3) 256(2.4) 161(99) 158(104) 29(2.4) 36(5.1) Table 2: Language modeling results on testing sets of PTB and wiki90M. For two SIVAE models, the syntactic tree sequence reconstruction scores are shown in parenthesis alongside the sentence reconstruction scores. Lower is better for PPL and NLL. The best results are in bold. are about the same. To discover whether the incorporation of syntactic trees is helpful for sentence generation, we compare our two versions of SIVAE with three baselines that do not utilize syntactic information: a 5gram Kneser-Ney language model (KN5) (Heafield et al., 2013), an LSTM language model (LSTMLM) (Sundermeyer et al., 2012), and a standard VAE (Bowman et al., 2016) using an LSTM-based encoder and decoder. Experimental results of language modeling are evaluated by the reconstruction loss using perplexity and the targeted syntactic evaluation proposed in (Marvin and Linzen, 2018). In section 5.3, we show the unsupervised paraphrase generation results. Datasets We use two datasets in this paper. For sentence-level Penn Treebank (PTB), the syntactic trees are labeled by humans (i.e. “gold-standard” trees). For Wikipedia-90M (wiki90M), which does not contain"
P19-1199,N18-1170,0,0.0785647,"Since y and z x are assumed to be independent when computing the joint probability p(x, y), we seek to minimize the mutual information I(y; z x ) during training. The recognition networks and the generation networks of SIVAE-i are similar to those adopted in SIVAE-c, so we omit them for brevity. 3 Unsupervised Paraphrasing Paraphrases are sentences with the same meaning but different syntactic structures. SIVAE allows us to execute syntax transformation, producing the desired paraphrases with variable syntactic tree templates. The syntactically controlled paraphrase generation is inspired by Iyyer et al. (2018); the difference is that our SIVAE-based syntactic paraphrase network is purely unsupervised. Unsupervised paraphrasing can be performed using both SIVAE-c and SIVAE-i. One way to generate paraphrases is to perform syntactically controlled paraphrase generation using SIVAE-i. The latent representations of an input sentence z x and a syntactic tree template z y are fed into SIVAE-i, and the syntax of the generated sentence conforms with the explicitly selected target template. However, linearized syntactic sequences are relatively long (as shown in Table 1) and long templates are more likely to"
P19-1199,P18-1249,0,0.097847,"ests syntactically-controlled sentence generation as it allows to alter the syntactic structure, desirable for related tasks like paraphrase generation. Given a sentence and a syntactic tree template, the model produces a paraphrase of the sentence whose syntax conforms to the template. Our SIVAE-based paraphrasing network is purely unsupervised, which makes it particularly suitable for generating paraphrases in low-resource languages or types of content. The experiments are conducted on two datasets: one has trees labeled by humans and the other has trees parsed by a state-of-the-art parser (Kitaev and Klein, 2018). Other than employing the standard language modeling evaluation metrics like perplexity, we also adopt the targeted syntactic evaluation (Marvin and Linzen, 2018) to verify whether the incorporation of syntactic trees improves the grammar of generated sentences. Experiments demonstrate that the proposed model improves the quality of generated sentences compared to other baseline methods, on both the reconstruction and grammar evaluations. The proposed methods show the ability for unsupervised paraphrase generation under different syntactic tree templates. Our contributions are four-fold: i) W"
P19-1199,E17-1083,0,0.0474292,"(Bowman et al., 2016; Yang et al., 2017; Xu and Durrett, 2018; Shen et al., 2019; Wang et al., 2019). The flexibility of VAEs also enables adding conditions during inference to perform controlled language generation (Hu et al., 2017; Zhao et al., 2017). Divergent from these VAE-based text generation models, our work decouples the latent representations corresponding to the sentence and its syntactic tree respectively. Paraphrase Generation Due to the similarity between two tasks, neural machine-translationbased models can often be utilized to achieve paraphrase generation (Hasan et al., 2016; Mallinson et al., 2017). Recently, Iyyer et al. (2018) proposed to syntactically control the generated paraphrase and Gupta et al. (2018) generate paraphrases in a deep generative architecture. However, all these methods assume the existence of some parallel paraphrase corpora while unsupervised paraphrase generation has been little explored. 5 Experiments We conduct our experiments on two datasets: sentence-level Penn Treebank (Marcus et al., 1993) with human-constituted parse trees and a 90 million word subset of Wikipedia (Gulordava et al., 2018) with parsed trees. When the decoder is too strong, VAE suffers from"
P19-1199,J93-2004,0,0.0704145,"Generation Due to the similarity between two tasks, neural machine-translationbased models can often be utilized to achieve paraphrase generation (Hasan et al., 2016; Mallinson et al., 2017). Recently, Iyyer et al. (2018) proposed to syntactically control the generated paraphrase and Gupta et al. (2018) generate paraphrases in a deep generative architecture. However, all these methods assume the existence of some parallel paraphrase corpora while unsupervised paraphrase generation has been little explored. 5 Experiments We conduct our experiments on two datasets: sentence-level Penn Treebank (Marcus et al., 1993) with human-constituted parse trees and a 90 million word subset of Wikipedia (Gulordava et al., 2018) with parsed trees. When the decoder is too strong, VAE suffers from posterior collapse where the model learns to ignore the latent variable (Bowman et al., 2016). To avoid posterior collapse, KLterm annealing and dropping out words during decoding are employed for training in this work. We also tried an advanced method replacing Gaussian priors with von Mises-Fisher priors (Xu and Durrett, 2018) to prevent KL collapse, but the results 2073 PTB Model PPL Standard NLL KL KN5 LSTM-LM VAE 145 110"
P19-1199,D18-1151,0,0.403612,"entence and a syntactic tree template, the model produces a paraphrase of the sentence whose syntax conforms to the template. Our SIVAE-based paraphrasing network is purely unsupervised, which makes it particularly suitable for generating paraphrases in low-resource languages or types of content. The experiments are conducted on two datasets: one has trees labeled by humans and the other has trees parsed by a state-of-the-art parser (Kitaev and Klein, 2018). Other than employing the standard language modeling evaluation metrics like perplexity, we also adopt the targeted syntactic evaluation (Marvin and Linzen, 2018) to verify whether the incorporation of syntactic trees improves the grammar of generated sentences. Experiments demonstrate that the proposed model improves the quality of generated sentences compared to other baseline methods, on both the reconstruction and grammar evaluations. The proposed methods show the ability for unsupervised paraphrase generation under different syntactic tree templates. Our contributions are four-fold: i) We propose a syntax-infused VAE that integrates syntactic trees with sentences, to grammatically improve the generated sentences. ii) We redesign the ELBO of the jo"
P19-1199,D15-1044,0,0.0444494,"sentence that conforms to y from p(x|y, z x ). We can also use a trained SIVAE-c to generate paraphrases. The paraphrase generation process is similar to sampling from a standard VAE with various tempera. The difference is that SIVAE-c first selects possible syntactic tree templates using the conditional prior network pψ (z y |z x ) then generates paraphrases based on the syntactic template and the latent variable. 4 Related Work Syntax-Aware Neural Text Generation The ability to generate sentences is core to many NLP tasks, such as machine translation (Bahdanau et al., 2015), summarization (Rush et al., 2015), and dialogue generation (Vinyals and Le, 2015). Recent works have shown that neural text generation can benefit from the incorporation of syntactic knowledge (Shen et al., 2018; Choe and Charniak, 2016). Sennrich and Haddow (2016) propose to augment each source word representation with its corresponding part-of-speech tag, lemmatized form and dependency label; Eriguchi et al. (2016) and Bastings et al. (2017) utilize a tree-based encoder and a graph convolutional network encoder respectively to embed the syntactic parse trees as part of the source sentence representations; Chen et al. (2017)"
P19-1199,W16-2209,0,0.0151343,"that SIVAE-c first selects possible syntactic tree templates using the conditional prior network pψ (z y |z x ) then generates paraphrases based on the syntactic template and the latent variable. 4 Related Work Syntax-Aware Neural Text Generation The ability to generate sentences is core to many NLP tasks, such as machine translation (Bahdanau et al., 2015), summarization (Rush et al., 2015), and dialogue generation (Vinyals and Le, 2015). Recent works have shown that neural text generation can benefit from the incorporation of syntactic knowledge (Shen et al., 2018; Choe and Charniak, 2016). Sennrich and Haddow (2016) propose to augment each source word representation with its corresponding part-of-speech tag, lemmatized form and dependency label; Eriguchi et al. (2016) and Bastings et al. (2017) utilize a tree-based encoder and a graph convolutional network encoder respectively to embed the syntactic parse trees as part of the source sentence representations; Chen et al. (2017) model source-side syntactic trees with a bidirectional tree encoder and tree-coverage decoder; Eriguchi et al. (2017) implicitly leverage linguistic prior by treating syntactic parsing as an auxiliary task. However, most of these s"
P19-1199,P19-1200,1,0.842977,"ural machine translation. Deep Latent Variable Models Deep latent variable models that combine the complementary strengths of latent variable models and deep learning have drawn much attention recently. Generative adversarial networks (Goodfellow et al., 2014) and variational autoencoders (Kingma and Welling, 2014) are the two families of deep generative models that are widely adopted in applications. As VAEs allow discrete generation from a continuous space, they have been a popular variant for NLP tasks including text generation (Bowman et al., 2016; Yang et al., 2017; Xu and Durrett, 2018; Shen et al., 2019; Wang et al., 2019). The flexibility of VAEs also enables adding conditions during inference to perform controlled language generation (Hu et al., 2017; Zhao et al., 2017). Divergent from these VAE-based text generation models, our work decouples the latent representations corresponding to the sentence and its syntactic tree respectively. Paraphrase Generation Due to the similarity between two tasks, neural machine-translationbased models can often be utilized to achieve paraphrase generation (Hasan et al., 2016; Mallinson et al., 2017). Recently, Iyyer et al. (2018) proposed to syntactically"
P19-1199,D16-1159,0,0.0260247,"encoder and the decoder are modeled by long short-term memory ∗ Part of this work was done when the first two authors were at Bloomberg. (LSTM) networks (Chung et al., 2014). For a random vector from the latent space representing an unseen input, the decoder can generate realisticlooking novel data in the context of a text model, making the VAE an attractive generative model. Compared to simple neural language models, the latent representation in a VAE is supposed to give the model more expressive capacity. Although syntactic properties can be implicitly discovered by such generative models, Shi et al. (2016) show that many deep structural details are still missing in the generated text. As a result of the absence of explicit syntactic information, generative models often produce ungrammatical sentences. To address this problem, recent works attempt to leverage explicit syntactic knowledge to improve the quality of machine translation (Eriguchi et al., 2016; Bastings et al., 2017; Chen et al., 2017) and achieve good results. Motivated by such success, we suggest that deep latent variable models for text generation can also benefit from the incorporation of syntactic knowledge. Instead of solely mo"
P19-1199,D18-1480,0,0.0945731,"works only focus on neural machine translation. Deep Latent Variable Models Deep latent variable models that combine the complementary strengths of latent variable models and deep learning have drawn much attention recently. Generative adversarial networks (Goodfellow et al., 2014) and variational autoencoders (Kingma and Welling, 2014) are the two families of deep generative models that are widely adopted in applications. As VAEs allow discrete generation from a continuous space, they have been a popular variant for NLP tasks including text generation (Bowman et al., 2016; Yang et al., 2017; Xu and Durrett, 2018; Shen et al., 2019; Wang et al., 2019). The flexibility of VAEs also enables adding conditions during inference to perform controlled language generation (Hu et al., 2017; Zhao et al., 2017). Divergent from these VAE-based text generation models, our work decouples the latent representations corresponding to the sentence and its syntactic tree respectively. Paraphrase Generation Due to the similarity between two tasks, neural machine-translationbased models can often be utilized to achieve paraphrase generation (Hasan et al., 2016; Mallinson et al., 2017). Recently, Iyyer et al. (2018) propos"
P19-1199,P17-1061,0,0.0301402,"drawn much attention recently. Generative adversarial networks (Goodfellow et al., 2014) and variational autoencoders (Kingma and Welling, 2014) are the two families of deep generative models that are widely adopted in applications. As VAEs allow discrete generation from a continuous space, they have been a popular variant for NLP tasks including text generation (Bowman et al., 2016; Yang et al., 2017; Xu and Durrett, 2018; Shen et al., 2019; Wang et al., 2019). The flexibility of VAEs also enables adding conditions during inference to perform controlled language generation (Hu et al., 2017; Zhao et al., 2017). Divergent from these VAE-based text generation models, our work decouples the latent representations corresponding to the sentence and its syntactic tree respectively. Paraphrase Generation Due to the similarity between two tasks, neural machine-translationbased models can often be utilized to achieve paraphrase generation (Hasan et al., 2016; Mallinson et al., 2017). Recently, Iyyer et al. (2018) proposed to syntactically control the generated paraphrase and Gupta et al. (2018) generate paraphrases in a deep generative architecture. However, all these methods assume the existence of some pa"
P19-1587,N04-4028,0,0.109475,"(Domingos, 1999; Margineantu, 2001; Elkan, 2001; Zadrozny et al., 2003) is another body of work where different mis-classification errors have different costs and one attempts to minimize the total cost that a model incurs on the test data. Our approach uses similar ideas – we make the costs of false positive prediction higher than the false-negative costs – and therefore can be viewed as a cost-sensitive model for sequence tagging problems. For sequence tagging problems, inference-time heuristics for tuning the precision-recall trade-off for information extraction models have been proposed. Culotta and McCallum (2004) calculate confidence scores of the extracted phrases from a CRF model: these scores are used for sorting and filtering extractions. Similarly, Carpenter (2007) computes phrase-level conditional probabilities from an HMM model, and try to increase the recall of gene name extraction by lowering the threshold on these probabilities. Given a trained CRF model, Minkov et al. (2006) hyper-tune the weight for the feature which indicates the token is not a named entity. Changing this weight could encourage or discourage the CRF decoding process to extract entities. We compare our model with these inf"
P19-1587,H05-1087,0,0.301127,"omponent for many natural language processing (NLP) pipelines. The most common evaluation metric for information extraction tasks is F1 , which is the harmonic mean between precision and recall: that is, false positives and false negatives are weighted equally. In certain real-world applications (e.g., medicine and finance), extracting wrong information is much worse than extracting nothing: hence, ∗ Work conducted while working at Bloomberg L.P. in such domains, high precision is emphasized. Trade-offs between precision and recall have been well researched for classification (Joachims, 2005; Jansche, 2005; Cortes and Mohri, 2004). However, barring studies on inference-time heuristics, there is limited work on training precision-oriented sequence tagging models. In this paper, we present a method for training precision-driven NER models. By defining custom loss objectives for the structured SVM (SSVM) model, we extend costsensitive learning (Domingos, 1999; Margineantu, 2001) to sequence tagging problems. A difficulty in applying cost-sensitive learning to NER is that the model needs to operate on segmentations of the input sentence and the labels of the segments. Inspired by semi-Markov CRF (S"
P19-1587,N06-2024,0,0.149456,"ed as a cost-sensitive model for sequence tagging problems. For sequence tagging problems, inference-time heuristics for tuning the precision-recall trade-off for information extraction models have been proposed. Culotta and McCallum (2004) calculate confidence scores of the extracted phrases from a CRF model: these scores are used for sorting and filtering extractions. Similarly, Carpenter (2007) computes phrase-level conditional probabilities from an HMM model, and try to increase the recall of gene name extraction by lowering the threshold on these probabilities. Given a trained CRF model, Minkov et al. (2006) hyper-tune the weight for the feature which indicates the token is not a named entity. Changing this weight could encourage or discourage the CRF decoding process to extract entities. We compare our model with these inference-time approaches. 3 Models We adopt the BiLSTM-CNNs architecture (Ma and Hovy, 2016) to extract features from a sequence of words for all models in this paper. 1 Each word is passed through character-level CNN, and the result is concatenated with Glove word 1 Our implementation is based on NCRF++ (Yang and Zhang, 2018). embedding (Pennington et al., 2014) to form the inpu"
P19-1587,D14-1162,0,0.0927237,"a trained CRF model, Minkov et al. (2006) hyper-tune the weight for the feature which indicates the token is not a named entity. Changing this weight could encourage or discourage the CRF decoding process to extract entities. We compare our model with these inference-time approaches. 3 Models We adopt the BiLSTM-CNNs architecture (Ma and Hovy, 2016) to extract features from a sequence of words for all models in this paper. 1 Each word is passed through character-level CNN, and the result is concatenated with Glove word 1 Our implementation is based on NCRF++ (Yang and Zhang, 2018). embedding (Pennington et al., 2014) to form the input of Bi-directional LSTM. To map the word representation obtained from BiLSTM into k (label) dimensions, one layer of feed-forward neural network is applied. At the output layer, instead of using a CRF (Lafferty et al., 2001) to capture the output label dependencies, we use the SSVM objective (Tsochantaridis et al., 2004). While CRFs have consistently given state-of-the-art NER results, their objective function is difficult to directly modify for highprecision extraction. Hence, we select the SSVM formulation as it allows us to directly modify the loss function for high precis"
P19-1587,P18-4013,0,0.0325962,"old on these probabilities. Given a trained CRF model, Minkov et al. (2006) hyper-tune the weight for the feature which indicates the token is not a named entity. Changing this weight could encourage or discourage the CRF decoding process to extract entities. We compare our model with these inference-time approaches. 3 Models We adopt the BiLSTM-CNNs architecture (Ma and Hovy, 2016) to extract features from a sequence of words for all models in this paper. 1 Each word is passed through character-level CNN, and the result is concatenated with Glove word 1 Our implementation is based on NCRF++ (Yang and Zhang, 2018). embedding (Pennington et al., 2014) to form the input of Bi-directional LSTM. To map the word representation obtained from BiLSTM into k (label) dimensions, one layer of feed-forward neural network is applied. At the output layer, instead of using a CRF (Lafferty et al., 2001) to capture the output label dependencies, we use the SSVM objective (Tsochantaridis et al., 2004). While CRFs have consistently given state-of-the-art NER results, their objective function is difficult to directly modify for highprecision extraction. Hence, we select the SSVM formulation as it allows us to directly mod"
P19-1587,P18-2038,0,0.0616246,"es contribute to phrase-level false positives. Therefore, we try a semi-Markov variation of the SSVM following (Sarawagi and Cohen, 2005). The semi-Markov formulation groups consecutive tokens into segments. Whole segments are considered as a single unit and only transitions between segments are modeled. We ignore all intrasegment transition probabilities, effectively collapsing the number of labels to 5 (ORG, PER, LOC, MISC, O instead of the BIO labelling scheme for CoNLL data). The scores of each segment are obtained by summing up the word-level class scores of words present in the segment (Ye and Ling, 2018). We restrict segments to be ≤ 7 tokens long, and we do not use any additional segment level features. During decoding, all possible segmentations of a sentence (≤ 7) will be considered. The architecture of our BiLSTM semiMarkov SSVM model is shown in Figure 1. To tune the semi-Markov SSVM model to high precision for a specific class, a segment will contribute `tgt to the loss if it is predicted as the target class and this segment does not exist in the gold segmentation. Other types of errors in the prediction have a loss of `tgt ˜ . This is similar to the class-specific loss used on the toke"
P19-1587,P16-1101,0,0.0352049,"these scores are used for sorting and filtering extractions. Similarly, Carpenter (2007) computes phrase-level conditional probabilities from an HMM model, and try to increase the recall of gene name extraction by lowering the threshold on these probabilities. Given a trained CRF model, Minkov et al. (2006) hyper-tune the weight for the feature which indicates the token is not a named entity. Changing this weight could encourage or discourage the CRF decoding process to extract entities. We compare our model with these inference-time approaches. 3 Models We adopt the BiLSTM-CNNs architecture (Ma and Hovy, 2016) to extract features from a sequence of words for all models in this paper. 1 Each word is passed through character-level CNN, and the result is concatenated with Glove word 1 Our implementation is based on NCRF++ (Yang and Zhang, 2018). embedding (Pennington et al., 2014) to form the input of Bi-directional LSTM. To map the word representation obtained from BiLSTM into k (label) dimensions, one layer of feed-forward neural network is applied. At the output layer, instead of using a CRF (Lafferty et al., 2001) to capture the output label dependencies, we use the SSVM objective (Tsochantaridis"
Q17-1021,P15-2126,0,0.0241596,"ets, and offered minimal performance improvements. In this paper, we convert social network positions into node embeddings, and use an attentional component to smooth the classification rule across the embedding space. Personalization has been an active research topic in areas such as speech recognition and information retrieval. Standard techniques for these tasks include linear transformation of model parameters (Leggetter and Woodland, 1995) and collaborative filtering (Breese et al., 1998). These methods have recently been adapted to personalized sentiment analysis (Tang et al., 2015a; Al Boni et al., 2015). Supervised personalization typically requires labeled training examples for every individual user. In contrast, by leveraging the social network structure, we can obtain personalization even when labeled data is unavailable for many authors. Sentiment analysis with social relations Previous work on incorporating social relations into sentiment classification has relied on the label consistency assumption, where the existence of social connections between users is taken as a clue that the sentiment polarities of the users’ messages should be similar. Speriosu et al. (2011) construct a heterog"
Q17-1021,P15-1104,0,0.0542016,"th a large amount of isolated author nodes. To improve the quality of the author embeddings, we expand the set of author nodes by adding nodes that do the most to densify the author networks: for the follower network, we add additional individuals that are followed by at least a hundred authors in the original set; for the mention and retweet networks, we add all users that have been mentioned or retweeted by at least twenty authors in the original set. The statistics of the resulting networks are presented in Table 2. 5.2 Experimental Settings We employ the pretrained word embeddings used by Astudillo et al. (2015), which are trained with a corpus of 52 million tweets, and have been shown to perform very well on this task. The embeddings are learned using the structured skip-gram model (Ling et al., 2015), and the embedding dimension is set at 600, following Astudillo et al. (2015). We report the same evaluation metric as the SemEval challenge: the average F1 score of positive and negative classes.8 Competitive systems We consider five competitive Twitter sentiment classification methods. Convolutional neural network (CNN) has been described in § 4.2, and is the basis model of S OCIAL ATTENTION. Mixture"
Q17-1021,P06-2005,0,0.0446254,"Missing"
Q17-1021,W06-1615,0,0.0332027,"1 https://code.google.com/archive/p/ word2vec 303 Related Work Domain adaptation and personalization Domain adaptation is a classic approach to handling the variation inherent in social media data (Eisenstein, 2013). Early approaches to supervised domain adaptation focused on adapting the classifier weights across domains, using enhanced feature spaces (Daum´e III, 2007) or Bayesian priors (Chelba and Acero, 2006; Finkel and Manning, 2009). Recent work focuses on unsupervised domain adaptation, which typically works by transforming the input feature space so as to overcome domain differences (Blitzer et al., 2006). However, in many cases, the data has no natural partitioning into domains. In preliminary work, we constructed social network domains by running community detection algorithms on the author social network (Fortunato, 2010). However, these algorithms proved to be unstable on the sparse networks obtained from social media datasets, and offered minimal performance improvements. In this paper, we convert social network positions into node embeddings, and use an attentional component to smooth the classification rule across the embedding space. Personalization has been an active research topic in"
Q17-1021,D16-1120,0,0.08253,"Missing"
Q17-1021,P07-1033,0,0.0658963,"Missing"
Q17-1021,D10-1124,1,0.876353,"Missing"
Q17-1021,N13-1037,1,0.853875,"perts and concatenation obtain slightly worse F1 scores than the baseline CNN system, but random attention performs significantly better. In contrast to the SemEval datasets, individual users often contribute multiple reviews in the Ciao datasets (the average number of reviews from an author is 10.8; Table 7). As an author tends to express similar opinions toward related products, random attention 11 https://code.google.com/archive/p/ word2vec 303 Related Work Domain adaptation and personalization Domain adaptation is a classic approach to handling the variation inherent in social media data (Eisenstein, 2013). Early approaches to supervised domain adaptation focused on adapting the classifier weights across domains, using enhanced feature spaces (Daum´e III, 2007) or Bayesian priors (Chelba and Acero, 2006; Finkel and Manning, 2009). Recent work focuses on unsupervised domain adaptation, which typically works by transforming the input feature space so as to overcome domain differences (Blitzer et al., 2006). However, in many cases, the data has no natural partitioning into domains. In preliminary work, we constructed social network domains by running community detection algorithms on the author so"
Q17-1021,N09-1068,0,0.0298879,"views in the Ciao datasets (the average number of reviews from an author is 10.8; Table 7). As an author tends to express similar opinions toward related products, random attention 11 https://code.google.com/archive/p/ word2vec 303 Related Work Domain adaptation and personalization Domain adaptation is a classic approach to handling the variation inherent in social media data (Eisenstein, 2013). Early approaches to supervised domain adaptation focused on adapting the classifier weights across domains, using enhanced feature spaces (Daum´e III, 2007) or Bayesian priors (Chelba and Acero, 2006; Finkel and Manning, 2009). Recent work focuses on unsupervised domain adaptation, which typically works by transforming the input feature space so as to overcome domain differences (Blitzer et al., 2006). However, in many cases, the data has no natural partitioning into domains. In preliminary work, we constructed social network domains by running community detection algorithms on the author social network (Fortunato, 2010). However, these algorithms proved to be unstable on the sparse networks obtained from social media datasets, and offered minimal performance improvements. In this paper, we convert social network p"
Q17-1021,S15-2097,0,0.0646927,"Missing"
Q17-1021,S15-2095,0,0.0125221,"ce the pretrained author embeddings with random embedding vectors, drawing uniformly from the interval (−0.25, 0.25). Concatenation concatenates the author embedding with the sentence representation obtained from CNN, and then feeds the new representation to a softmax classifier. Finally, we include S OCIAL ATTENTION, the attention-based neural network method described in § 4. We also compare against the three top-performing systems in the SemEval 2015 Twitter sentiment analysis challenge (Rosenthal et al., 2015): W E BIS (Hagen et al., 2015), UNITN (Severyn and Moschitti, 2015), and LSISLIF (Hamdan et al., 2015). UNITN achieves the best average F1 score on Test 2013–2015 sets among all the submitted systems. Finally, we republish results of N LSE (Astudillo et al., 2015), a non-linear subspace embedding model. Parameter tuning We tune all the hyperparameters on the SemEval 2013 development set. We choose the number of bigram filters for the CNN models from {50, 100, 150}. The size of author embeddings is selected from {50, 100}. For mixture of experts, random attention and S OCIAL ATTENTION , we compare a range of numbers of basis models, {3, 5, 10, 15}. We found that a relatively small number of bas"
Q17-1021,P15-1073,0,0.0485176,"w data. 1 Introduction Words can mean different things to different people. Fortunately, these differences are rarely idiosyncratic, but are often linked to social factors, such as age (Rosenthal and McKeown, 2011), gender (Eckert and McConnell-Ginet, 2003), race (Green, 2002), geography (Trudgill, 1974), and more ineffable characteristics such as political and cultural attitudes (Fischer, 1958; Labov, 1963). In natural language processing (NLP), social media data has brought variation to the fore, spurring the development of new computational techniques for characOne exception is the work of Hovy (2015), who shows that the accuracies of sentiment analysis and topic classification can be improved by the inclusion of coarse-grained author demographics such as age and gender. However, such demographic information is not directly available in most datasets, and it is not yet clear whether predicted age and gender offer any improvements. On the other end of the spectrum are attempts to create personalized language technologies, as are often employed in information retrieval (Shen et al., 2005), recommender systems (Basilico and Hofmann, 2004), and language modeling (Federico, 1996). But personali"
Q17-1021,P07-1034,0,0.0426865,"training instance: models are inactive. This can occur because some basis models may be initialized with parameters that are globally superior. As a result, the “dead” basis models will receive near-zero gradient updates, and therefore can never improve. The true model capacity can thereby be substantially lower than the K assigned experts. Ideally, dead basis models will be avoided because each basis model should focus on a unique region of the social network. To ensure that this happens, we pretrain the basis models using an instance weighting approach from the domain adaptation literature (Jiang and Zhai, 2007). For each basis model k, each author a has an instance weight αa,k . These instance weights are based on the author’s social network node embedding, so that socially proximate authors will have high weights for the same basis models. This is ensured by endowing each basis model with a random vector γk ∼ N (0, σ 2 I), and setting the instance weights as, αa,k = sigmoid(γk> va ). (7) The simple design results in similar instance weights for socially proximate authors. During pretraining, we train the k-th basis model by optimizing the following loss function for every instance: `k = −αa,k T X t"
Q17-1021,P14-1062,0,0.011628,"combining all of these relation types into a unified multi-relational network. It is possible that embeddings in such a network could be estimated using techniques borrowed from multirelational knowledge networks (Bordes et al., 2014; Wang et al., 2014). 4.2 Sentiment Classification with Convolutional Neural Networks We next describe the basis models, p(y |x, Z = k). Because our target task is classification on microtext documents, we model this distribution using convolutional neural networks (CNNs; Lecun et al., 1989), which have been proven to perform well on sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014). CNNs apply layers of convolving filters to n-grams, thereby generating a vector of dense local features. CNNs improve upon traditional bagof-words models because of their ability to capture word ordering information. Let x = [h1 , h2 , · · · , hn ] be the input sentence, where hi is the D(w) dimensional word vector corresponding to the i-th word in the sentence. We use one convolutional layer and one max pooling layer to generate the sentence representation of x. The convolutional layer involves filters that are applied to bigrams to produce feature maps. Formally, given the bigr"
Q17-1021,D14-1181,0,0.00450147,"tion types into a unified multi-relational network. It is possible that embeddings in such a network could be estimated using techniques borrowed from multirelational knowledge networks (Bordes et al., 2014; Wang et al., 2014). 4.2 Sentiment Classification with Convolutional Neural Networks We next describe the basis models, p(y |x, Z = k). Because our target task is classification on microtext documents, we model this distribution using convolutional neural networks (CNNs; Lecun et al., 1989), which have been proven to perform well on sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014). CNNs apply layers of convolving filters to n-grams, thereby generating a vector of dense local features. CNNs improve upon traditional bagof-words models because of their ability to capture word ordering information. Let x = [h1 , h2 , · · · , hn ] be the input sentence, where hi is the D(w) dimensional word vector corresponding to the i-th word in the sentence. We use one convolutional layer and one max pooling layer to generate the sentence representation of x. The convolutional layer involves filters that are applied to bigrams to produce feature maps. Formally, given the bigram word vect"
Q17-1021,N15-1142,0,0.0125705,"follower network, we add additional individuals that are followed by at least a hundred authors in the original set; for the mention and retweet networks, we add all users that have been mentioned or retweeted by at least twenty authors in the original set. The statistics of the resulting networks are presented in Table 2. 5.2 Experimental Settings We employ the pretrained word embeddings used by Astudillo et al. (2015), which are trained with a corpus of 52 million tweets, and have been shown to perform very well on this task. The embeddings are learned using the structured skip-gram model (Ling et al., 2015), and the embedding dimension is set at 600, following Astudillo et al. (2015). We report the same evaluation metric as the SemEval challenge: the average F1 score of positive and negative classes.8 Competitive systems We consider five competitive Twitter sentiment classification methods. Convolutional neural network (CNN) has been described in § 4.2, and is the basis model of S OCIAL ATTENTION. Mixture of experts employs the same CNN model as an expert, but the mixture densi8 Regarding the neutral class: systems are penalized with false positives when neutral tweets are incorrectly classified"
Q17-1021,S13-2052,0,0.103279,"Missing"
Q17-1021,P11-1077,0,0.0796886,"n is divided among several basis models, depending on the author’s position in the social network. This has the effect of smoothing the classification function across the social network, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata. This model significantly improves the accuracies of sentiment analysis on Twitter and on review data. 1 Introduction Words can mean different things to different people. Fortunately, these differences are rarely idiosyncratic, but are often linked to social factors, such as age (Rosenthal and McKeown, 2011), gender (Eckert and McConnell-Ginet, 2003), race (Green, 2002), geography (Trudgill, 1974), and more ineffable characteristics such as political and cultural attitudes (Fischer, 1958; Labov, 1963). In natural language processing (NLP), social media data has brought variation to the fore, spurring the development of new computational techniques for characOne exception is the work of Hovy (2015), who shows that the accuracies of sentiment analysis and topic classification can be improved by the inclusion of coarse-grained author demographics such as age and gender. However, such demographic inf"
Q17-1021,S15-2078,0,0.0342528,"cture of random attention is nearly identical to S OCIAL ATTENTION: the only distinction is that we replace the pretrained author embeddings with random embedding vectors, drawing uniformly from the interval (−0.25, 0.25). Concatenation concatenates the author embedding with the sentence representation obtained from CNN, and then feeds the new representation to a softmax classifier. Finally, we include S OCIAL ATTENTION, the attention-based neural network method described in § 4. We also compare against the three top-performing systems in the SemEval 2015 Twitter sentiment analysis challenge (Rosenthal et al., 2015): W E BIS (Hagen et al., 2015), UNITN (Severyn and Moschitti, 2015), and LSISLIF (Hamdan et al., 2015). UNITN achieves the best average F1 score on Test 2013–2015 sets among all the submitted systems. Finally, we republish results of N LSE (Astudillo et al., 2015), a non-linear subspace embedding model. Parameter tuning We tune all the hyperparameters on the SemEval 2013 development set. We choose the number of bigram filters for the CNN models from {50, 100, 150}. The size of author embeddings is selected from {50, 100}. For mixture of experts, random attention and S OCIAL ATTENTION , we comp"
Q17-1021,S15-2079,0,0.0132732,"TION: the only distinction is that we replace the pretrained author embeddings with random embedding vectors, drawing uniformly from the interval (−0.25, 0.25). Concatenation concatenates the author embedding with the sentence representation obtained from CNN, and then feeds the new representation to a softmax classifier. Finally, we include S OCIAL ATTENTION, the attention-based neural network method described in § 4. We also compare against the three top-performing systems in the SemEval 2015 Twitter sentiment analysis challenge (Rosenthal et al., 2015): W E BIS (Hagen et al., 2015), UNITN (Severyn and Moschitti, 2015), and LSISLIF (Hamdan et al., 2015). UNITN achieves the best average F1 score on Test 2013–2015 sets among all the submitted systems. Finally, we republish results of N LSE (Astudillo et al., 2015), a non-linear subspace embedding model. Parameter tuning We tune all the hyperparameters on the SemEval 2013 development set. We choose the number of bigram filters for the CNN models from {50, 100, 150}. The size of author embeddings is selected from {50, 100}. For mixture of experts, random attention and S OCIAL ATTENTION , we compare a range of numbers of basis models, {3, 5, 10, 15}. We found th"
Q17-1021,W11-2207,0,0.102353,"(Tang et al., 2015a; Al Boni et al., 2015). Supervised personalization typically requires labeled training examples for every individual user. In contrast, by leveraging the social network structure, we can obtain personalization even when labeled data is unavailable for many authors. Sentiment analysis with social relations Previous work on incorporating social relations into sentiment classification has relied on the label consistency assumption, where the existence of social connections between users is taken as a clue that the sentiment polarities of the users’ messages should be similar. Speriosu et al. (2011) construct a heterogeneous network with tweets, users, and n-grams as nodes. Each node is then associated with a sentiment label distribution, and these label distributions are smoothed by label propagation over the graph. Similar approaches are explored by Hu et al. (2013), who employ the graph Laplacian as a source of regularization, and by Tan et al. (2011) who take a factor graph approach. A related idea is to label the sentiment of individuals in a social network towards each other: West et al. (2014) exploit the sociological theory of structural balance to improve the accuracy of dyadic"
Q17-1021,C14-1018,0,0.0309121,"ON outperforms prior work regardless of which network is selected. Twitter’s “following” relation is a relatively low-cost form of social engagement, and it is less public than retweeting or mentioning another user. Thus it is unsurprising that the follower network is least useful for socially-informed personalization. The R ETWEET + network has denser social connections than M ENTION +, which could lead to better author embeddings. 5.4 Analysis We now investigate whether language variation in sentiment meaning has been captured by different basis models. We focus on the same sentiment words (Tang et al., 2014) that we used to test linguistic homophily in our analysis. We are interested to discover sentiment words that are used with the opposite sentiment meanings by some authors. To measure the level of model-specificity for each Basis model More positive More negative 1 2 3 4 5 banging loss fever broken fucking chilling cold ill sick suck ass damn piss bitch shit insane bawling fever weird cry ruin silly bad boring dreadful dear like god yeah wow satisfy trust wealth strong lmao talent honestly voting win clever lmao super lol haha hahaha lovatics wish beliebers arianators kendall Table 5: Top 5 m"
Q17-1021,P15-1098,0,0.365126,"k’, speakers like Taylor Swift may employ either the positive and negative meanings, while speakers like Charles Rangel employ only the negative meaning. In other cases, communities may maintain completely distinct semantics for a word, such as the term ‘pants’ in American and British English. Thanks to Christopher Potts for suggesting this distinction and this example. 2 296 # Positive # Negative # Neutral # Tweet 3,230 477 1,572 982 1,038 1,265 273 601 202 365 4,109 614 1,640 669 987 8,604 1,364 3,813 1,853 2,390 Table 1: Statistics of the SemEval Twitter sentiment datasets. scale networks (Tang et al., 2015b). Applying the algorithm to Figure 1, the authors within each triad would likely be closer to each other than to authors in the opposite triad. We then incorporate these embeddings into an attention-based neural network model, called S OCIAL ATTENTION, which employs multiple basis models to focus on different regions of the social network. We apply S OCIAL ATTENTION to Twitter sentiment classification, gathering social network metadata for Twitter users in the SemEval Twitter sentiment analysis tasks (Nakov et al., 2013). We further adopt the system to Ciao product reviews (Tang et al., 2012"
Q17-1021,W06-1639,0,0.0413497,"omputational Linguistics, vol. 5, pp. 295–307, 2017. Action Editor: Christopher Potts. Submission batch: 10/2016; Revision batch: 12/2016; Published 8/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Dataset Train 2013 Dev 2013 Test 2013 Test 2014 Test 2015 Figure 1: Words such as ‘sick’ can express opposite sentiment polarities depending on the author. We account for this variation by generalizing across the social network. network information is available in a wide range of contexts, from social media (Huberman et al., 2008) to political speech (Thomas et al., 2006) to historical texts (Winterer, 2012). Thus, social network homophily has the potential to provide a more general way to account for linguistic variation in NLP. Figure 1 gives a schematic of the motivation for our approach. The word ‘sick’ typically has a negative sentiment, e.g., ‘I would like to believe he’s sick rather than just mean and evil.’1 However, in some communities the word can have a positive sentiment, e.g., the lyric ‘this sick beat’, recently trademarked by the musician Taylor Swift.2 Given labeled examples of ‘sick’ in use by individuals in a social network, we assume that th"
Q17-1021,Q14-1024,0,0.0271447,"en as a clue that the sentiment polarities of the users’ messages should be similar. Speriosu et al. (2011) construct a heterogeneous network with tweets, users, and n-grams as nodes. Each node is then associated with a sentiment label distribution, and these label distributions are smoothed by label propagation over the graph. Similar approaches are explored by Hu et al. (2013), who employ the graph Laplacian as a source of regularization, and by Tan et al. (2011) who take a factor graph approach. A related idea is to label the sentiment of individuals in a social network towards each other: West et al. (2014) exploit the sociological theory of structural balance to improve the accuracy of dyadic sentiment labels in this setting. All of these efforts are based on the intuition that individual predictions p(y) should be smooth across the network. In contrast, our work is based on the intuition that social neighbors use language similarly, so they should have a similar conditional distribution p(y |x). These intuitions are complementary: if both hold for a specific setting, then label consistency and linguistic consistency could in principle be combined to improve performance. Social relations can al"
W14-3104,P11-1026,0,0.0214044,"odels in practice, users often face one critical problem: topics discovered by the model do not always make sense. A topic may contain thematically unrelated words. Moreover, two thematic related words may appear in different topics. This is mainly because the objective function optimized by LDA may not reflect human judgments of topic quality (Boyd-Graber et al., 2009). Potentially, we can solve these problems by incorporating additional user guidance or domain knowledge in topic modeling. With standard LDA however, it is impossible for users to interact with the model and provide feedback. (Hu et al., 2011) proposed an interactive topic modeling framework that allows users to add word must-links. However, it has several limitations. Since the vocabulary size of a large document collection can be very large, users may need to annotate a large number of word constraints for this method to be effective. Thus, this process can be very tedious. More importantly, it 2 Active Learning With Constrained Topic Modeling In this section, we first summarize our work on constrained topic modeling. Then, we introduce an active topic learning framework that employs constrained topic modeling. In LDA, a document"
W14-3104,D11-1024,0,0.0498247,"ens in d, and φ is model’s topic-word distribution. Since the overall likelihood of the input documents is the objective function LDA aims to maximize, using this criteria, the system will choose a document that is most difficult for which the current model achieves the lowest objective score. 3 Evaluation In this section, we evaluate our active learning framework. Topic models are often evaluated using perplexity on held-out test data. However, recent work (Boyd-Graber et al., 2009; Chuang et al., 2013) has shown that human judgment sometimes is contrary to the perplexity measure. Following (Mimno et al., 2011), we employ Topic Coherence, a metric which was shown to be highly consistent with human judgment, to measure a topic model’s quality. It relies upon word co-occurrence statistics within documents, and does not depend on external resources or human labeling. We followed (Basu et al., 2004) to create a Mix3 sub-dataset from the 20 Newsgroups data2 , which consists of two newsgroups with similar topics (rec.sport.hockey, rec.sport.baseball) and one with a distinctive topic (sci.space). We use this dataset to evaluate the effectiveness of the proposed framework. 3.1 Simulated Experiments We first"
