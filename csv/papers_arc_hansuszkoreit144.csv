W19-5351,Linguistic Evaluation of {G}erman-{E}nglish Machine Translation Using a Test Suite,2019,12,2,4,0.395092,5140,eleftherios avramidis,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"We present the results of the application of a grammatical test suite for German-to-English MT on the systems submitted at WMT19, with a detailed analysis for 107 phenomena organized in 14 categories. The systems still translate wrong one out of four test items in average. Low performance is indicated for idioms, modals, pseudo-clefts, multi-word expressions and verb valency. When compared to last year, there has been a improvement of function words, non verbal agreement and punctuation. More detailed conclusions about particular systems and phenomena are also presented."
W18-6436,Fine-grained evaluation of {G}erman-{E}nglish Machine Translation based on a Test Suite,2018,11,5,4,1,12459,vivien macketanz,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We present an analysis of 16 state-of-the-art MT systems on German-English based on a linguistically-motivated test suite. The test suite has been devised manually by a team of language professionals in order to cover a broad variety of linguistic phenomena that MT often fails to translate properly. It contains 5,000 test sentences covering 106 linguistic phenomena in 14 categories, with an increased focus on verb tenses, aspects and moods. The MT outputs are evaluated in a semi-automatic way through regular expressions that focus only on the part of the sentence that is relevant to each phenomenon. Through our analysis, we are able to compare systems based on their performance on these categories. Additionally, we reveal strengths and weaknesses of particular systems and we identify grammatical phenomena where the overall performance of MT is relatively low."
W18-2107,Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically motivated Test Suite,2018,0,1,4,0.395092,5140,eleftherios avramidis,Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing,0,None
L18-1142,{TQ}-{A}uto{T}est {--} An Automated Test Suite for (Machine) Translation Quality,2018,0,2,4,1,12459,vivien macketanz,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
S17-1026,Generating Pattern-Based Entailment Graphs for Relation Extraction,2017,21,1,3,1,32412,kathrin eichler,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Relation extraction is the task of recognizing and extracting relations between entities or concepts in texts. A common approach is to exploit existing knowledge to learn linguistic patterns expressing the target relation and use these patterns for extracting new relation mentions. Deriving relation patterns automatically usually results in large numbers of candidates, which need to be filtered to derive a subset of patterns that reliably extract correct relation mentions. We address the pattern selection task by exploiting the knowledge represented by entailment graphs, which capture semantic relationships holding among the learned pattern candidates. This is motivated by the fact that a pattern may not express the target relation explicitly, but still be useful for extracting instances for which the relation holds, because its meaning entails the meaning of the target relation. We evaluate the usage of both automatically generated and gold-standard entailment graphs in a relation extraction scenario and present favorable experimental results, exhibiting the benefits of structuring and selecting patterns based on entailment graphs."
cotik-etal-2017-annotation,Annotation of Entities and Relations in {S}panish Radiology Reports,2017,13,4,4,1,20993,viviana cotik,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Radiology reports express the results of a radiology study and contain information about anatomical entities, findings, measures and impressions of the medical doctor. The use of information extraction techniques can help physicians to access this information in order to understand data and to infer further knowledge. Supervised machine learning methods are very popular to address information extraction, but are usually domain and language dependent. To train new classification models, annotated data is required. Moreover, annotated data is also required as an evaluation resource of information extraction algorithms. However, one major drawback of processing clinical data is the low availability of annotated datasets. For this reason we performed a manual annotation of radiology reports written in Spanish. This paper presents the corpus, the annotation schema, the annotation guidelines and further insight of the data."
simova-uszkoreit-2017-word,Word Embeddings as Features for Supervised Coreference Resolution,2017,0,2,2,0,31051,iliana simova,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"A common reason for errors in coreference resolution is the lack of semantic information to help determine the compatibility between mentions referring to the same entity. Distributed representations, which have been shown successful in encoding relatedness between words, could potentially be a good source of such knowledge. Moreover, being obtained in an unsupervised manner, they could help address data sparsity issues in labeled training data at a small cost. In this work we investigate whether and to what extend features derived from word embeddings can be successfully used for supervised coreference resolution. We experiment with several word embedding models, and several different types of embeddingbased features, including embedding cluster and cosine similarity-based features. Our evaluations show improvements in the performance of a supervised state-of-theart coreference system."
thomas-etal-2017-streaming,Streaming Text Analytics for Real-Time Event Recognition,2017,17,0,8,1,13907,philippe thomas,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"A huge body of continuously growing written knowledge is available on the web in the form of social media posts, RSS feeds, and news articles. Real-time information extraction from such high velocity, high volume text streams requires scalable, distributed natural language processing pipelines. We introduce such a system for fine-grained event recognition within the big data framework Flink, and demonstrate its capabilities for extracting and geo-locating mobility- and industry-related events from heterogeneous text sources. Performance analyses conducted on several large datasets show that our system achieves high throughput and maintains low latency, which is crucial when events need to be detected and acted upon in real-time. We also present promising experimental results for the event extraction component of our system, which recognizes a novel set of event types. The demo system is available at \url{http://dfki.de/sd4m-sta-demo/}."
K17-3001,{C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to {U}niversal {D}ependencies,2017,28,32,38,0,5828,daniel zeman,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2017, the task was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe how the data sets were prepared, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems."
E17-3002,Common Round: Application of Language Technologies to Large-Scale Web Debates,2017,10,0,1,1,23887,hans uszkoreit,Proceedings of the Software Demonstrations of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Web debates play an important role in enabling broad participation of constituencies in social, political and economic decision-taking. However, it is challenging to organize, structure, and navigate a vast number of diverse argumentations and comments collected from many participants over a long time period. In this paper we demonstrate Common Round, a next generation platform for large-scale web debates, which provides functions for eliciting the semantic content and structures from the contributions of participants. In particular, Common Round applies language technologies for the extraction of semantic essence from textual input, aggregation of the formulated opinions and arguments. The platform also provides a cross-lingual access to debates using machine translation."
W16-6404,Deeper Machine Translation and Evaluation for {G}erman,2016,-1,-1,5,0.427557,5140,eleftherios avramidis,Proceedings of the 2nd Deep Machine Translation Workshop,0,None
W16-5113,Negation Detection in Clinical Reports Written in {G}erman,2016,22,6,4,1,20993,viviana cotik,Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016),0,"An important subtask in clinical text mining tries to identify whether a clinical finding is expressed as present, absent or unsure in a text. This work presents a system for detecting mentions of clinical findings that are negated or just speculated. The system has been applied to two different types of German clinical texts: clinical notes and discharge summaries. Our approach is built on top of NegEx, a well known algorithm for identifying non-factive mentions of medical findings. In this work, we adjust a previous adaptation of NegEx to German and evaluate the system on our data to detect negation and speculation. The results are compared to a baseline algorithm and are analyzed for both types of clinical documents. Our system achieves an F1-Score above 0.9 on both types of reports."
W16-4210,A fine-grained corpus annotation schema of {G}erman nephrology records,2016,0,5,2,1,13906,roland roller,Proceedings of the Clinical Natural Language Processing Workshop ({C}linical{NLP}),0,"In this work we present a fine-grained annotation schema to detect named entities in German clinical data of chronically ill patients with kidney diseases. The annotation schema is driven by the needs of our clinical partners and the linguistic aspects of German language. In order to generate annotations within a short period, the work also presents a semi-automatic annotation which uses additional sources of knowledge such as UMLS, to pre-annotate concepts in advance. The presented schema will be used to apply novel techniques from natural language processing and machine learning to support doctors treating their patients by improved information access from unstructured German texts."
P16-4007,"Real-Time Discovery and Geospatial Visualization of Mobility and Industry Events from Large-Scale, Heterogeneous Data Streams",2016,6,3,11,1,5578,leonhard hennig,Proceedings of {ACL}-2016 System Demonstrations,0,"Monitoring mobility- and industryrelevant events is important in areas such as personal travel planning and supply chain management, but extracting events pertaining to specific companies, transit routes and locations from heterogeneous, high-volume text streams remains a significant challenge. We present Spree, a scalable system for real-time, automatic event extraction from social media, news and domain-specific RSS feeds. Our system is tailored to a range of mobilityand industry-related events, and processes German texts within a distributed linguistic analysis pipeline implemented in Apache Flink. The pipeline detects and disambiguates highly ambiguous domain-relevant entities, such as street names, and extracts various events with their geo-locations. Event streams are visualized on a dynamic, interactive map for monitoring and analysis."
L16-1383,Relation- and Phrase-level Linking of {F}rame{N}et with Sar-graphs,2016,0,0,5,1,5580,aleksandra gabryszak,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Recent research shows the importance of linking linguistic knowledge resources for the creation of large-scale linguistic data. We describe our approach for combining two English resources, FrameNet and sar-graphs, and illustrate the benefits of the linked data in a relation extraction setting. While FrameNet consists of schematic representations of situations, linked to lexemes and their valency patterns, sar-graphs are knowledge resources that connect semantic relations from factual knowledge graphs to the linguistic phrases used to express instances of these relations. We analyze the conceptual similarities and differences of both resources and propose to link sar-graphs and FrameNet on the levels of relations/frames as well as phrases. The former alignment involves a manual ontology mapping step, which allows us to extend sar-graphs with new phrase patterns from FrameNet. The phrase-level linking, on the other hand, is fully automatic. We investigate the quality of the automatically constructed links and identify two main classes of errors."
L16-1537,{TEG}-{REP}: A corpus of Textual Entailment Graphs based on Relation Extraction Patterns,2016,19,0,3,1,32412,kathrin eichler,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The task of relation extraction is to recognize and extract relations between entities or concepts in texts. Dependency parse trees have become a popular source for discovering extraction patterns, which encode the grammatical relations among the phrases that jointly express relation instances. State-of-the-art weakly supervised approaches to relation extraction typically extract thousands of unique patterns only potentially expressing the target relation. Among these patterns, some are semantically equivalent, but differ in their morphological, lexical-semantic or syntactic form. Some express a relation that entails the target relation. We propose a new approach to structuring extraction patterns by utilizing entailment graphs, hierarchical structures representing entailment relations, and present a novel resource of gold-standard entailment graphs based on a set of patterns automatically acquired using distant supervision. We describe the methodology used for creating the dataset and present statistics of the resource as well as an analysis of inference types underlying the entailment decisions."
K16-1024,Event Linking with Sentential Features from Convolutional Neural Networks,2016,44,7,3,1,12589,sebastian krause,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,0,"Coreference resolution for event mentions enables extraction systems to process document-level information. Current systems in this area base their decisions on rich semantic features from various knowledge bases, thus restricting them to domains where such external sources are available. We propose a model for this task which does not rely on such features but instead utilizes sentential features coming from convolutional neural networks. Two such networks first process coreference candidates and their respective context, thereby generating latent-feature representations which are tuned towards event aspects relevant for a linking decision. These representations are augmented with lexicallevel and pairwise features, and serve as input to a trainable similarity function producing a coreference score. Our model achieves state-of-the-art performance on two datasets, one of which is publicly available. An error analysis points out directions for further research."
D16-1065,{AMR} Parsing with an Incremental Joint Model,2016,15,23,3,0,7847,junsheng zhou,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W15-5702,Towards Deeper {MT} - A Hybrid System for {G}erman,2015,-1,-1,4,0.488163,5140,eleftherios avramidis,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-4405,Semi-automatic Generation of Multiple-Choice Tests from Mentions of Semantic Relations,2015,15,4,5,1,29662,renlong ai,Proceedings of the 2nd Workshop on Natural Language Processing Techniques for Educational Applications,0,"We propose a strategy for the semiautomatic generation of learning material for reading-comprehension tests, guided by semantic relations embedded in expository texts. Our approach combines methods from the areas of information extraction and paraphrasing in order to present a language teacher with a set of candidate multiple-choice questions and answers that can be used for verifying a language learners reading capabilities. We implemented a web-based prototype showing the feasibility of our approach and carried out a pilot user evaluation that resulted in encouraging feedback but also pointed out aspects of the strategy and prototype implementation which need improvements."
W15-4204,Sar-graphs: A Linked Linguistic Knowledge Resource Connecting Facts with Language,2015,21,4,5,1,12589,sebastian krause,Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources and Applications,0,"We present sar-graphs, a knowledge resource that links semantic relations from factual knowledge graphs to the linguistic patterns with which a language can express instances of these relations. Sar-graphs expand upon existing lexicosemantic resources by modeling syntactic and semantic information at the level of relations, and are hence useful for tasks such as knowledge base population and relation extraction. We present a languageindependent method to automatically construct sar-graph instances that is based on distantly supervised relation extraction. We link sar-graphs at the lexical level to BabelNet, WordNet and UBY, and present our ongoing work on pattern- and relationlevel linking to FrameNet. An initial dataset of English sar-graphs for 25 relations is made publicly available, together with a Java-based API."
S15-2056,{DFKI}: Multi-objective Optimization for the Joint Disambiguation of Entities and Nouns {\\&} Deep Verb Sense Disambiguation,2015,15,2,3,0,28972,dirk weissenborn,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,We introduce an approach to word sense disambiguation and entity linking that combines a set of complementary objectives in an extensible multi-objective formalism. During disambiguation the system performs continuous optimization to find optimal probability distributions over candidate senses. Verb senses are disambiguated using a separate neural network model. Our results on noun and verb sense disambiguation as well as entity linking outperform all other submissions on the SemEval 2015 Task 13 for English.
P15-4008,A Web-based Collaborative Evaluation Tool for Automatically Learned Relation Extraction Patterns,2015,10,0,5,1,5578,leonhard hennig,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"Patterns extracted from dependency parses of sentences are a major source of knowledge for most state-of-the-art relation extraction systems, but can be of low quality in distantly supervised settings. We present a linguistic annotation tool that allows human experts to analyze and categorize automatically learned patterns, and to identify common error classes. The annotations can be used to create datasets that enable machine learning approaches to pattern quality estimation. We also present an experimental pattern error analysis for three semantic relations, where we find that between 24% and 61% of the learned dependency patterns are defective due to preprocessing or parsing errors, or due to violations of the distant supervision assumption."
P15-1058,Multi-Objective Optimization for the Joint Disambiguation of Nouns and Named Entities,2015,31,16,4,0,28972,dirk weissenborn,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"In this paper, we present a novel approach to joint word sense disambiguation (WSD) and entity linking (EL) that combines a set of complementary objectives in an extensible multi-objective formalism. During disambiguation the system performs continuous optimization to find optimal probability distributions over candidate senses. The performance of our system on nominal WSD as well as EL improves state-ofthe-art results on several corpora. These improvements demonstrate the importance of combining complementary objectives in a joint model for robust disambiguation."
W14-1001,Analytical Approaches to Combining {MT} Technologies,2014,0,0,1,1,23887,hans uszkoreit,Proceedings of the 3rd Workshop on Hybrid Approaches to Machine Translation ({H}y{T}ra),0,The talk will report on recent and ongoing work dedicated to analytical methods for a systematic combination of observed strengths of translation technologies. The focus will be on different ways of exploiting existing data on MT output and performance measures for system combination and for gaining insights on strengths and weaknesses of existing technologies.
krieger-etal-2014-information,Information Extraction from {G}erman Patient Records via Hybrid Parsing and Relation Extraction Strategies,2014,9,8,3,0,37133,hansulrich krieger,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we report on first attempts and findings to analyzing German patient records, using a hybrid parsing architecture and a combination of two relation extraction strategies. On a practical level, we are interested in the extraction of concepts and relations among those concepts, a necessary cornerstone for building medical information systems. The parsing pipeline consists of a morphological analyzer, a robust chunk parser adapted to Latin phrases used in medical diagnosis, a repair rule stage, and a probabilistic context-free parser that respects the output from the chunker. The relation extraction stage is a combination of two systems: SProUT, a shallow processor which uses hand-written rules to discover relation instances from local text units and DARE which extracts relation instances from complete sentences, using rules that are learned in a bootstrapping process, starting with semantic seeds. Two small experiments have been carried out for the parsing pipeline and the relation extraction stage."
li-etal-2014-annotating,Annotating Relation Mentions in Tabloid Press,2014,7,2,4,1,20464,hong li,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents a new resource for the training and evaluation needed by relation extraction experiments. The corpus consists of annotations of mentions for three semantic relations: marriage, parentâchild, siblings, selected from the domain of biographic facts about persons and their social relationships. The corpus contains more than one hundred news articles from Tabloid Press. In the current corpus, we only consider the relation mentions occurring in the individual sentences. We provide multi-level annotations which specify the marked facts from relation, argument, entity, down to the token level, thus allowing for detailed analysis of linguistic phenomena and their interactions. A generic markup tool Recon developed at the DFKI LT lab has been utilised for the annotation task. The corpus has been annotated by two human experts, supported by additional conflict resolution conducted by a third expert. As shown in the evaluation, the annotation is of high quality as proved by the stated inter-annotator agreements both on sentence level and on relationmention level. The current corpus is already in active use in our research for evaluation of the relation extraction performance of our automatically learned extraction patterns."
avramidis-etal-2014-taraxu,"The tara{X{\\\U}} corpus of human-annotated machine translations""",2014,7,2,7,0.488163,5140,eleftherios avramidis,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Human translators are the key to evaluating machine translation (MT) quality and also to addressing the so far unanswered question when and how to use MT in professional translation workflows. This paper describes the corpus developed as a result of a detailed large scale human evaluation consisting of three tightly connected tasks: ranking, error classification and post-editing."
rehm-etal-2014-strategic,"The Strategic Impact of {META}-{NET} on the Regional, National and International Level",2014,47,2,2,0,60,georg rehm,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This article provides an overview of the dissemination work carried out in META-NET from 2010 until early 2014; we describe its impact on the regional, national and international level, mainly with regard to politics and the situation of funding for LT topics. This paper documents the initiativeÂs work throughout Europe in order to boost progress and innovation in our field."
ai-etal-2014-sprinter,{S}printer: Language Technologies for Interactive and Multimedia Language Learning,2014,13,4,5,0.977012,29662,renlong ai,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Modern language learning courses are no longer exclusively based on books or face-to-face lectures. More and more lessons make use of multimedia and personalized learning methods. Many of these are based on e-learning solutions. Learning via the Internet provides 7/24 services that require sizeable human resources. Therefore we witness a growing economic pressure to employ computer-assisted methods for improving language learning in quality, efficiency and scalability. In this paper, we will address three applications of language technologies for language learning: 1) Methods and strategies for pronunciation training in second language learning, e.g., multimodal feedback via visualization of sound features, speech verification and prosody transplantation; 2) Dialogue-based language learning games; 3) Application of parsing and generation technologies to the automatic generation of paraphrases for the semi-automatic production of learning material."
krause-etal-2014-language,Language Resources and Annotation Tools for Cross-Sentence Relation Extraction,2014,12,0,4,1,12589,sebastian krause,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we present a novel combination of two types of language resources dedicated to the detection of relevant relations (RE) such as events or facts across sentence boundaries. One of the two resources is the sar-graph, which aggregates for each target relation ten thousands of linguistic patterns of semantically associated relations that signal instances of the target relation (Uszkoreit and Xu, 2013). These have been learned from the Web by intra-sentence pattern extraction (Krause et al., 2012) and after semantic filtering and enriching have been automatically combined into a single graph. The other resource is cockrACE, a specially annotated corpus for the training and evaluation of cross-sentence RE. By employing our powerful annotation tool Recon, annotators mark selected entities and relations (including events), coreference relations among these entities and events, and also terms that are semantically related to the relevant relations and events. This paper describes how the two resources are created and how they complement each other."
2014.eamt-1.38,Using a new analytic measure for the annotation and analysis of {MT} errors on real data,2014,7,12,6,0.555556,28494,arle lommel,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,"This work presents the new flexible Multidimensional Quality Metrics (MQM) framework and uses it to analyze the performance of state-of-the-art machine translation systems, focusing on xe2x80x9cnearly acceptablexe2x80x9d translated sentences. A selection of WMT news data and xe2x80x9ccustomerxe2x80x9d data provided by language service providers (LSPs) in four language pairs was annotated using MQM issue types and examined in terms of the types of errors found in it. Despite criticisms of WMT data by the LSPs, an examination of the resulting errors and patterns for both types of data shows that they are strikingly consistent, with more variation between language pairs and system types than between text types. These results validate the use of WMT data in an analytic approach to assessing quality and show that analytic approaches represent a useful addition to more traditional assessment methodologies such as BLEU or METEOR."
2014.eamt-1.41,"Relations between different types of post-editing operations, cognitive effort and temporal effort",2014,-1,-1,5,0.153289,5059,maja popovic,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,None
2013.mtsummit-wptp.2,What can we learn about the selection mechanism for post-editing?,2013,-1,-1,5,0.153289,5059,maja popovic,Proceedings of the 2nd Workshop on Post-editing Technology and Practice,0,None
2013.mtsummit-european.17,{QTL}aunchpad,2013,-1,-1,6,0,40954,stephen doherty,Proceedings of Machine Translation Summit XIV: European projects,0,None
P12-1026,Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate {C}hinese Part-of-Speech Tagging,2012,30,15,2,0,4541,weiwei sun,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated approaches yield a relative error reduction of 18% in total over a state-of-the-art baseline."
kluewer-etal-2012-evaluation,Evaluation of the {K}om{P}arse Conversational Non-Player Characters in a Commercial Virtual World,2012,19,3,4,0,43160,tina kluewer,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The paper describes the evaluation of the KomParse system. KomParse is a dialogue system embedded in a 3-D massive multiplayer online game, allowing conversations between non player characters (NPCs) and game users. In a field test with game users, the system was evaluated with respect to acceptability and usability of the overall system as well as task completion, dialogue control and efficiency of three conversational tasks. Furthermore, subjective feedback has been collected for evaluating the single communication components of the system such as natural language understanding. The results are very satisfying and promising. In general, both the usability and acceptability tests show that the tested NPC is useful and well-accepted by the users. Even if the NPC does not always understand the users well and expresses things unexpected, he could still provide appropriate responses to help users to solve their problems or entertain them."
F12-4001,Quality Translation for a Multilingual Continent - Priorities and Chances for {E}uropean {MT} Research,2012,-1,-1,1,1,23887,hans uszkoreit,"Proceedings of the Joint Conference {JEP}-{TALN}-{RECITAL} 2012, volume 4: Invited Conferences",0,None
W11-2915,Minimally Supervised Domain-Adaptive Parse Reranking for Relation Extraction,2011,34,2,4,1,21300,feiyu xu,Proceedings of the 12th International Conference on Parsing Technologies,0,"The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted head-driven phrase structure grammar (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less generic HPSG treebank. It will be shown how the estimated confidence of RE rules learned from the n best parses can be exploited for parse reranking. The acquired reranking model improves the performance of RE in both training and test phases with the new first parses. The obtained significant boost of recall does not come from an overall gain in parsing performance but from an application-driven selection of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for task-specific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task."
W11-2161,{DFKI} Hybrid Machine Translation System for {WMT} 2011 - On the Integration of {SMT} and {RBMT},2011,14,4,2,0,4017,jia xu,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,We present the DFKI hybrid translation system at the WMT workshop 2011. Three SMT and two RBMT systems are combined at the level of the final translation output. The translation results show that our hybrid system significantly outperformed individual systems by exploring strengths of both rule-based and statistical translations.
R11-1003,Minimally Supervised Rule Learning for the Extraction of Biographic Information from Various Social Domains,2011,22,5,3,1,20464,hong li,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,This paper investigates the application of an existing seed-based minimally supervised learning algorithm to different social domains exhibiting different properties of the available data. A systematic analysis studies the respective data properties of the three domains including the distribution of the semantic arguments and their combinations. The experimental results confirm that data properties have a strong influence on the performance of the learning system. The main results are insights about: (i) the effects of data properties such as redundancy and frequency of argument mentions on coverage and precision (ii) the positive effects of negative examples if used effectively (iii) the different effects of negative examples depending on the domain data properties and (iv) the potential of reusing rules from one domain for improving the relation extraction performance in another domain.
R11-1052,{META}-{DARE}: Monitoring the Minimally Supervised {ML} of Relation Extraction Rules,2011,13,0,3,1,20464,hong li,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"This paper demonstrates a web-based online system, called META-DARE1. META-DARE is built to assist researchers to obtain insights into seed-based minimally supervised machine learning for relation extraction. META-DARE allows researchers and students to conduct experiments with an existing machine learning system called DARE (Xu et al., 2007). Users can run their own learning experiments by constructing initial seed examples and can monitor the learning process in a very detailed way, namely, via interacting with each node in the learning graph and viewing its content. Furthermore, users can study the learned relation extraction rules and their applications. META-DARE is also an analysis tool which gives an overview of the whole learning process: the number of iterations, the input and output behaviors of each iteration, and the general performance of the extracted instances and their distributions. Moreover, META-DARE provides a very convenient user interface for visualization of the learning graph, the learned rules and the system performance profile."
R11-1095,{T}ech{W}atch{T}ool: Innovation and Trend Monitoring,2011,9,6,3,1,20464,hong li,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"In this paper we present an information service system that allows users to search for the key players of requested technology areas and for their collaboration networks. This system utilizes information extraction and wrapper technologies for detecting persons, organizations, publications and patents as well as relationships among them. Furthermore, it applies relation extraction to detect statements on the web that indicate innovation trends. Various visualization methods are provided to let users monitor key players, their networks and technology trends in a comfortable way."
2011.mtsummit-plenaries.2,"Strategic {MT} Research in {E}urope: Themes, Approaches, Results and Plans",2011,-1,-1,1,1,23887,hans uszkoreit,Proceedings of Machine Translation Summit XIII: Plenaries,0,None
W10-1708,Further Experiments with Shallow Hybrid {MT} Systems,2010,3,18,6,0.833333,6017,christian federmann,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"We describe our hybrid machine translation system which has been developed for and used in the WMT10 shared task. We compute translations from a rule-based MT system and combine the resulting translation templates with partial phrases from a state-of-the-art phrase-based, statistical MT engine. Phrase substitution is guided by several decision factors, a continuation of previous work within our group. For the shared task, we have computed translations for six language pairs including English, German, French and Spanish. Our experiments have shown that our shallow substitution approach can effectively improve the translation result from the RBMT system; however it has also become clear that a deeper integration is needed to further improve translation quality."
P10-4007,Talking {NPC}s in a Virtual Game World,2010,6,6,4,0,39621,tina kluwer,Proceedings of the {ACL} 2010 System Demonstrations,0,"This paper describes the KomParse system, a natural-language dialog system in the three-dimensional virtual world Twinity. In order to fulfill the various communication demands between nonplayer characters (NPCs) and users in such an online virtual world, the system realizes a flexible and hybrid approach combining knowledge-intensive domain-specific question answering, task-specific and domain-specific dialog with robust chatbot-like chitchat."
adolphs-etal-2010-question,Question Answering Biographic Information and Social Network Powered by the Semantic Web,2010,7,8,4,1,34902,peter adolphs,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"After several years of development, the vision of the Semantic Web is gradually becoming reality. Large data repositories have been created and offer semantic information in a machine-processable form for various domains. Semantic Web data can be published on the Web, gathered automatically, and reasoned about. All these developments open interesting perspectives for building a new class of domain-specific, broad-coverage information systems that overcome a long-standing bottleneck of AI systems, the notoriously incomplete knowledge base. We present a system that shows how the wealth of information in the Semantic Web can be interfaced with humans once again, using natural language for querying and answering rather than technical formalisms. Whereas current Question Answering systems typically select snippets from Web documents retrieved by a search engine, we utilize Semantic Web data, which allows us to provide natural-language answers that are tailored to the current dialog context. Furthermore, we show how to use natural language processing technologies to acquire new data and enrich existing data in a Semantic Web framework. Our system has acquired a rich biographic data resource by combining existing Semantic Web resources, which are discovered from semi-structured textual data in Web pages, with information extracted from free natural language texts."
jorg-etal-2010-lt,{LT} World: Ontology and Reference Information Portal,2010,9,2,2,0,46258,brigitte jorg,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"LT World (www.lt-world.org) is an ontology-driven web portal aimed at serving the global language technology community. Ontology-driven means, that the system is driven by an ontological schema to manage the research information and knowledge life-cycles: identify relevant concepts of information, structure and formalize them, assign relationships, functions and views, add states and rules, modify them. For modelling such a complex structure, we employ (i) concepts from the research domain, such as person, organisation, project, tool, data, patent, news, event (ii) concepts from the LT domain, such as technology and resource (iii) concepts from closely related domains, such as language, linguistics, and mathematics. Whereas the research entities represent the general context, that is, a research environment as such, the LT entities define the information and knowledge space of the field, enhanced by entities from closely related areas. By managing information holistically â that is, within a research context â its inherent semantics becomes much more transparent. This paper introduces LT World as a reference information portal through ontological eyes: its content, its system, its method for maintaining knowledge-rich items, its ontology as an asset."
fu-etal-2010-determining,Determining the Origin and Structure of Person Names,2010,12,5,3,0,46274,yu fu,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper presents a novel system HENNA (Hybrid Person Name Analyzer) for identifying language origin and analyzing linguistic structures of person names. We conduct ME-based classification methods for the language origin identification and achieve very promising performance. We will show that word-internal character sequences provide surprisingly strong evidence for predicting the language origin of person names. Our approach is context-, language- and domain-independent and can thus be easily adapted to person names in or from other languages. Furthermore, we provide a novel strategy to handle origin ambiguities or multiple origins in a name. HENNA also provides a person name parser for the analysis of linguistic and knowledge structures of person names. All the knowledge about a person name in HENNA is modelled in a person-name ontology, including relationships between language origins, linguistic features and grammars of person names of a specific language and interpretation of name elements. The approaches presented here are useful extensions of the named entity recognition task."
C10-2065,Using Syntactic and Semantic based Relations for Dialogue Act Recognition,2010,24,27,2,0,39621,tina kluwer,Coling 2010: Posters,0,"This paper presents a novel approach to dialogue act recognition employing multilevel information features. In addition to features such as context information and words in the utterances, the recognition task utilizes syntactic and semantic relations acquired by information extraction methods. These features are utilized by a Bayesian network classifier for our dialogue act recognition. The evaluation results show a clear improvement from the accuracy of the baseline (only with word features) with 61.9% to an accuracy of 67.4% achieved by the extended feature set."
C10-2155,Boosting Relation Extraction with Limited Closed-World Knowledge,2010,16,22,2,1,21300,feiyu xu,Coling 2010: Posters,0,"This paper presents a new approach to improving relation extraction based on minimally supervised learning. By adding some limited closed-world knowledge for confidence estimation of learned rules to the usual seed data, the precision of relation extraction can be considerably improved. Starting from an existing baseline system we demonstrate that utilizing limited closed world knowledge can effectively eliminate dangerous or plainly wrong rules during the bootstrapping process. The new method improves the reliability of the confidence estimation and the precision value of the extracted instances. Although recall suffers to a certain degree depending on the domain and the selected settings, the overall performance measured by F-score considerably improves. Finally we validate the adaptability of the best ranking method to a new domain and obtain promising results."
W09-0405,Combining Multi-Engine Translations with {M}oses,2009,14,19,8,0.325203,3161,yu chen,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"We present a simple method for generating translations with the Moses toolkit (Koehn et al., 2007) from existing hypotheses produced by other translation engines. As the structures underlying these translation engines are not known, an evaluation-based strategy is applied to select systems for combination. The experiments show promising improvements in terms of BLEU."
W09-0411,Translation Combination using Factored Word Substitution,2009,8,6,4,1,6017,christian federmann,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"We present a word substitution approach to combine the output of different machine translation systems. Using part of speech information, candidate words are determined among possible translation options, which in turn are estimated through a pre-computed word alignment. Automatic substitution is guided by several decision factors, including part of speech, local context, and language model probabilities. The combination of these factors is defined after careful manual analysis of their respective impact. The approach is tested for the language pair German-English, however the general technique itself is language independent."
W09-0105,Linguistics in Computational Linguistics: Observations and Predictions,2009,0,3,1,1,23887,hans uszkoreit,"Proceedings of the {EACL} 2009 Workshop on the Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?",0,"As my title suggests, this position paper focuses on the relevance of linguistics in NLP instead of asking the inverse question. Although the question about the role of computational linguistics in the study of language may theoretically be much more interesting than the selected topic, I feel that my choice is more appropriate for the purpose and context of this workshop.n n This position paper starts with some retrospective observations clarifying my view on the ambivalent and multi-facetted relationship between linguistics and computational linguistics as it has evolved from both applied and theoretical research on language processing. In four brief points I will then strongly advocate a strengthened relationship from which both sides benefit.n n First, I will observe that recent developments in both deep linguistic processing and statistical NLP suggest a certain plausible division of labor between the two paradigms.n n Second, I want to propose a systematic approach to research on hybrid systems which determines optimal combinations of the paradigms and continuously monitors the division of labor as both paradigm progress. Concrete examples illustrating the proposal are taken from our own research.n n Third, I will argue that a central vision of computational linguistics is still alive, the dream of a formalized reusable linguistic knowledge source embodying the core competence of a language that can be utilized for wide range of applications."
E09-2004,Gossip Galore {--} A Self-Learning Agent for Exchanging Pop Trivia,2009,5,3,4,1,43128,xiwen cheng,Proceedings of the Demonstrations Session at {EACL} 2009,0,"This paper describes a self-learning software agent who collects and learns knowledge from the web and also exchanges her knowledge via dialogues with the users. The agent is built on top of information extraction, web mining, question answering and dialogue system technologies, and users can freely formulate their questions within the gossip domain and obtain the answers in multiple ways: textual response, graph-based visualization of the related concepts and speech output."
W08-2126,Hybrid Learning of Dependency Structures from Heterogeneous Linguistic Resources,2008,9,17,3,0.0967871,3425,yi zhang,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"In this paper we present our syntactic and semantic dependency parsing system participated in both closed and open competitions of the CoNLL 2008 Shared Task. By combining the outcome of two state-of-the-art syntactic dependency parsers, we achieved high accuracy in syntactic dependencies (87.32%). With MRSes from grammar-based HPSG parsers, we achieved significant performance improvement on semantic role labeling (from 71.31% to 71.89%), especially in the out-domain evaluation (from 60.16% to 62.11%)."
schafer-etal-2008-extracting,Extracting and Querying Relations in Scientific Papers on Language Technology,2008,10,3,2,0,42237,ulrich schafer,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We describe methods for extracting interesting factual relations from scientific texts in computational linguistics and language technology taken from the ACL Anthology. We use a hybrid NLP architecture with shallow preprocessing for increased robustness and domain-specific, ontology-based named entity recognition, followed by a deep HPSG parser running the English Resource Grammar (ERG). The extracted relations in the MRS (minimal recursion semantics) format are simplified and generalized using WordNet. The resulting ÂquriplesÂ are stored in a database from where they can be retrieved (again using abstraction methods) by relation-based search. The query interface is embedded in a web browser-based application we call the ScientistÂs Workbench. It supports researchers in editing and online-searching scientific papers."
xu-etal-2008-adaptation,Adaptation of Relation Extraction Rules to New Domains,2008,12,7,2,1,21300,feiyu xu,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents various strategies for improving the extraction performance of less prominent relations with the help of the rules learned for similar relations, for which large volumes of data are available that exhibit suitable data properties. The rules are learned via a minimally supervised machine learning system for relation extraction called DARE. Starting from semantic seeds, DARE extracts linguistic grammar rules associated with semantic roles from parsed news texts. The performance analysis with respect to different experiment domains shows that the data property plays an important role for DARE. Especially the redundancy of the data and the connectivity of instances and pattern rules have a strong influence on recall. However, most real-world data sets do not possess the desirable small-world property. Therefore, we propose three scenarios to overcome the data property problem of some domains by exploiting a similar domain with better data properties. The first two strategies stay with the same corpus but try to extract new similar relations with learned rules. The third strategy adapts the learned rules to a new corpus. All three strategies show that frequently mentioned relations can help in the detection of less frequent relations."
2008.eamt-1.6,Hybrid machine translation architectures within and beyond the {E}uro{M}atrix project,2008,-1,-1,3,0.231687,13817,andreas eisele,Proceedings of the 12th Annual conference of the European Association for Machine Translation,0,None
P07-1074,A Seed-driven Bottom-up Machine Learning Framework for Extracting Relations of Various Complexity,2007,11,73,2,1,21300,feiyu xu,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"A minimally supervised machine learning framework is described for extracting relations of various complexity. Bootstrapping starts from a small set of n-ary relation instances as xe2x80x9cseedsxe2x80x9d, in order to automatically learn pattern rules from parsed data, which then can extract new instances of the relation and its projections. We propose a novel rule representation enabling the composition of n-ary relation rules on top of the rules for projections of the relation. The compositional approach to rule construction is supported by a bottom-up pattern extraction method. In comparison to other automatic approaches, our rules cannot only localize relation arguments but also assign their exact target argument roles. The method is evaluated in two tasks: the extraction of Nobel Prize awards and management succession events. Performance for the new Nobel Prize task is strong. For the management succession task the results compare favorably with those of existing pattern acquisition approaches."
W06-3001,Contextual phenomena and thematic relations in database {QA} dialogues: results from a {W}izard-of-{O}z Experiment,2006,7,23,2,0,48723,nuria bertomeu,Proceedings of the Interactive Question Answering Workshop at {HLT}-{NAACL} 2006,0,"Considering data obtained from a corpus of database QA dialogues, we address the nature of the discourse structure needed to resolve the several kinds of contextual phenomena found in our corpus. We look at the thematic relations holding between questions and the preceding context and discuss to which extent thematic related-ness plays a role in discourse structure."
P06-4010,{C}hinese Named Entity and Relation Identification System,2006,6,2,2,1,45157,tianfang yao,Proceedings of the {COLING}/{ACL} 2006 Interactive Presentation Sessions,0,"In this interactive presentation, a Chinese named entity and relation identification system is demonstrated. The domain-specific system has a three-stage pipeline architecture which includes word segmentation and part-of-speech (POS) tagging, named entity recognition, and named entity relation identitfication. The experimental results have shown that the average F-measure for word segmentation and POS tagging after correcting errors achieves 92.86 and 90.01 separately. Moreover, the overall average F-measure for 6 kinds of name entities and 14 kinds of named entity relations is 83.08% and 70.46% respectively."
uszkoreit-etal-2006-pragmatic,The pragmatic combination of different crosslingual resources,2006,0,0,1,1,23887,hans uszkoreit,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We will describe new cross-lingual strategies for the development multilingual information services on mobile devices. The novelty of our approach is the intelligent modeling of cross-lingual application domains and the combination of textual translation with speech generation. The final system helps users to speak foreign languages and communicate with the local people in relevant situations, such as restaurant, taxi and emergencies. The advantage of our information services is that they are robust enough for the use in real-world situations. They are developed for the Beijing Olympic Games 2008, where most foreigners will have to rely on translation assistance. Their deployment is foreseen as part of the planned ubiquitous mobile information system of the Olympic Games."
W05-0401,A Novel Machine Learning Approach for the Identification of Named Entity Relations,2005,7,2,2,1,45157,tianfang yao,Proceedings of the {ACL} Workshop on Feature Engineering for Machine Learning in Natural Language Processing,0,"In this paper, a novel machine learning approach for the identification of named entity relations (NERs) called positive and negative case-based learning (PNCBL) is proposed. It pursues the improvement of the identification performance for NERs through simultaneously learning two opposite cases and automatically selecting effective multi-level linguistic features for NERs and non-NERs. This approach has been applied to the identification of domain-specific and cross-sentence NERs for Chinese texts. The experimental results have shown that the overall average recall, precision, and F-measure for 14 NERs are 78.50%, 63.92% and 70.46% respectively. In addition, the above F-measure has been enhanced from 63.61% to 70.46% due to adoption of both positive and negative cases."
W05-0108,Language Technology from a {E}uropean Perspective,2005,0,1,1,1,23887,hans uszkoreit,Proceedings of the Second {ACL} Workshop on Effective Tools and Methodologies for Teaching {NLP} and {CL},0,This paper describes the cooperation of four European Universities aiming at attracting more students to European master studies in Language and Communication Technologies. The cooperation has been formally approved within the framework of the new European program Erasmus Mundus as a Specific Support Action in 2004. The consortium also aims at creating a sound basis for a joint master program in the field of language technology and computer science.
2005.mtsummit-swtmt.1,Ontologies for Crosslingual Applications,2005,-1,-1,1,1,23887,hans uszkoreit,Workshop on Semantic Web technologies for machine translation,0,"Human translation is based on linguistic and extralinguistic knowledge. Despite promising pioneering advances, knowledge-based machine translation has remained a tempting vision. The bottleneck has been the engineering of sufficiently comprehensive bodies of relevant knowledge The Semantic Web offers opportunities for the gradual evolution of a global heterogeneous knowledge base. The immediate target has been the modelling of certain knowledge domains by practical ontologies. In the talk we will demonstrate the utilization of ontological knowledge indifferent crosslingual applications reaching from crosslingual document retrieval via crosslingual question answering to complex information services involving several crosslingual functionalities, including machine translation. We will then discuss the ramifications of this development and of the evolution of the World Wide Web for future directions in both statistical and rule-based machine translation."
uszkoreit-2004-strategic,Strategic Directions of National and International Research Funding,2004,0,0,1,1,23887,hans uszkoreit,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Just as in other areas of hi-tech R&D, progress in language technology could not have been achieved without a sustained level of goal-oriented public funding. Especially in the area of language resources and language technology evaluation, much of the existing infrastructure can be attributed to dedicated funding efforts. On this panel representatives of national and international funding agencies will report on their programs and outline their strategic priorities as well as concrete plans for the years to come. Additional topics will be opportunities for international collaboration and the envisaged division of labour between publicly financed and industrial research."
baumann-etal-2004-muli,The {MULI} Project: Annotation and Analysis of Information Structure in {G}erman and {E}nglish,2004,12,11,9,0,51388,stefan baumann,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The goal of the MULI (MUltiLingual Information structure) project is to empirically analyse information structure in German and English newspaper texts. In contrast to other projects in which information structure is annotated and investigated (e.g. in the Prague Dependency Treebank, which mirrors the basic information about the topic-focus articulation of the sentence), we do not annotate theory-biased categories like topic-focus or theme-rheme. Trying to be as theory-independent as possible, we annotate those features which are relevant to information structure and on the basis of which typical patterns, co-occurrences or correlations can be determined. We distinguish between three annotation levels: syntax, discourse and prosody. The data is based on the TIGER Corpus for German and the Penn Treebank for English, since the existing information on part-of-speech and syntactic structure can be re-used for our purposes. The actual annotation of an English example sequence illustrates our choice of categories on each level. Their combination offers the possibility to investigate how information structure is realised and can be interpreted."
buitelaar-etal-2004-evaluation,Evaluation Resources for Concept-based Cross-Lingual Information Retrieval in the Medical Domain,2004,7,9,8,0,6276,paul buitelaar,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The paper describes evaluation resources for concept-based, cross-lingual information retrieval in the medical domain. All resources were constructed in the context of the MuchMore project and are freely available through the project website. Available resources include: a bilingual, parallel document collection of German and English medical scientific abstracts, a set of queries and corresponding relevance assessments, two manually disambiguated test sets for semantic annotation (sense disambiguation), two evaluation lists for German morphological decomposition of medical terms."
P03-2019,Integrating Information Extraction and Automatic Hyperlinking,2003,5,6,6,0,5120,stephan busemann,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,"This paper presents a novel information system integrating advanced information extraction technology and automatic hyper-linking. Extracted entities are mapped into a domain ontology that relates concepts to a selection of hyperlinks. For information extraction, we use SProUT, a generic platform for the development and use of multilingual text processing components. By combining finite-state and unification-based formalisms, the grammar formalism used in SProUT offers both processing efficiency and a high degree of decalrativeness. The ExtraLink demo system show-cases the extraction of relevant concepts from German texts in the tourism domain, offering the direct connection to associated web documents on demand."
P02-1056,An Integrated Archictecture for Shallow and Deep Processing,2002,12,44,9,0,36792,berthold crysmann,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"We present an architecture for the integration of shallow and deep NLP components which is aimed at flexible combination of different language technologies for a range of practical current and future applications. In particular, we describe the integration of a high-level HPSG parsing system with different high-performance shallow components, ranging from named entity recognition to chunk parsing and shallow clause recognition. The NLP components enrich a representation of natural language text with layers of new XML meta-information using a single shared data structure, called the text chart. We describe details of the integration methods, and show how information extraction and language checking applications for realworld German text benefit from a deep grammatical analysis."
bird-etal-2002-open,The Open Language Archives Community,2002,0,5,2,0,8953,steven bird,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
capstick-etal-2002-collate,{COLLATE}: Competence Center in Speech and Language Technology,2002,9,4,2,0,53562,joanne capstick,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper presents the structure and activitities of the recently established Competence Center in Speech and Language Technology in Saarbrucken. The objectives of the Competence Center are to provide a comprehensive information service about speech and language technologies, including live demonstrations of the most important language technology (LT) systems, and to advance the state of the art in the evaluation of LT systems for real-world applications. The Competence Center comprises the following components: 1. the Virtual Information Center xe2x80x9cLanguage Technology Worldxe2x80x9d (www.lt-world.org), the world's most comprehensive information resource about speech and language technology, 2. the Demonstration Center in Saarbrucken, which offers interested parties the possibility to play and experiment with different speech and language technologies, or to attend guided demonstrations, 3. the Evaluation Center, which conducts evaluations of the overall usability of language technology systems and advances knowledge of relevant usability issues and evaluation methods. The work presented in this paper was carried out by the German Research Center for Artificial Intelligence in collaboration with Saarland University in the context of the project COLLATE (COmputational Linguistics and LAnguage TEchnology for Real Life Applications), funded by the German Federal Ministry of Education and Research (www.bmbf.de)."
W01-1003,Crosslingual Language Technologies for Knowledge Creation and Knowledge Sharing (invited talk),2001,0,0,1,1,23887,hans uszkoreit,Proceedings of the {ACL} 2001 Workshop on Human Language Technology and Knowledge Management,0,None
declerck-etal-2000-new,The New Edition of the Natural Language Software Registry (an Initiative of {ACL} hosted at {DFKI}),2000,0,5,3,0,2109,thierry declerck,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"In this paper we present the new version (4th edition) of the Natural Language Software Registry (NLSR), an initiative of the Association for Computational Linguistics (ACL) hosted at DFKI in Saarbrucken. We give a brief overview of the history of this repository for Natural Language Processing (NLP) software, list some related works and go into the details of the design and the implementation of the"
C00-1005,An ontology of systematic relations for a shared grammar of {S}lavic,2000,10,3,2,0,12120,tania avgustinova,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"Sharing portions of grammars across languages greatly reduces the costs of multilingual grammar engineering. Related languages share a much wider range of linguistic information than typically assumed in standard multilingual grammar architectures. Taking grammatical relatedness seriously, we are particularly interested in designing linguistically motivated grammatical resources for Slavic languages to be used in applied and theoretical computational linguistics. In order to gain the perspective of a language-family oriented grammar design, we consider an array of systematic relations that can hold between syntactical units. While the categorisation of primitive linguistic entities tends to be language-specific or even construction-specific, the relations holding between them allow various degrees of abstraction. On the basis of Slavic data, we show how a domain ontology conceptualising morpho-syntactic building blocks can serve as a basis of a shared grammar of Slavic."
A97-2016,Software for Annotating Argument Structure,1997,5,1,4,0,52345,wojciech skut,Fifth Conference on Applied Natural Language Processing: Descriptions of System Demonstrations and Videos,0,"We present a tool developed for annotating corpora with argument structure representations. The presentation focuses on the architecture of the annotation scheme and a number of techniques for increasing the efficiency and accuracy of annotation. Among others, we show how the assignment of grammatical functions can be automatised using standard part-of-speech tagging methods."
A97-1014,An Annotation Scheme for Free Word Order Languages,1997,10,258,4,0,52345,wojciech skut,Fifth Conference on Applied Natural Language Processing,0,"We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particular representational strata."
C94-1072,{DISCO}-An {HPSG}-based {NLP} System and its Application for Appointment Scheduling Project Note,1994,0,2,1,1,23887,hans uszkoreit,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The natural language system DISCO is described. It combines o a powerful and flexible grammar development system; o linguistic competence for German including morphology, syntax and semantics; o new methods for linguistic performance modelling on the basis of high-level competence grammars; o new methods for modelling multi-agent dialogue competence; o an interesting sample application for appointment scheduling and calendar management."
P92-1026,Handling Linear Precedence Constraints by Unification,1992,10,17,3,0,56935,judith engelkamp,30th Annual Meeting of the Association for Computational Linguistics,1,"Linear precedence (LP) rules are widley used for stating word order principles. They have been adopted as constraints by HPSG but no encoding in the formalism has been provided. Since they only order siblings, they are not quite adequate, at least not for German. We propose a notion of LP constraints that applies to linguistically motivated branching domains such as head domains. We show a type-based encoding in an HPSG-style formalism that supports processing. The encoding can be achieved by a compilation step."
P91-1031,Strategies for Adding Control Information to Declarative Grammars,1991,21,22,1,1,23887,hans uszkoreit,29th Annual Meeting of the Association for Computational Linguistics,1,"Strategies are proposed for combining different kinds of constraints in declarative grammars with a detachable layer of control information. The added control information is the basis for parametrized dynamically controlled linguistic deduction, a form of linguistic processing that permits the implementation of plausible linguistic performance models without giving up the declarative formulation of linguistic competence. The information can be used by the linguistic processor for ordering the sequence in which conjuncts and disjuncts are processed, for mixing depth-first and breadth-first search, for cutting off undesired derivations, and for constraint-relaxation."
C86-1045,Categorial Unification Grammars,1986,7,136,1,1,23887,hans uszkoreit,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms. Their efficient and uniform way of encoding linguistic knowledge in well-understood and widely used representations makes them attractive for computational applications and for linguistic research.In this paper, the basic concepts of CUGs and simple examples of their application will be presented. It will be argued that the strategies and potentials of CUGs justify their further exploration in the wider context of research on unification grammars. Approaches to selected linguistic phenomena such as long-distance dependencies, adjuncts, word order, and extraposition are discussed."
P83-1004,Formal Constraints on Metarules,1983,6,12,3,0,12905,stuart shieber,21st Annual Meeting of the Association for Computational Linguistics,1,"Metagrammatical formalisms that combine context-free phrase structure rules and metarules (MPS grammars) allow concise statement of generalizations about the syntax of natural languages. Unconstrained MPS grammars, unfortunately, are not computationally safe. We evaluate several proposals for constraining them, basing our assessment on computational tractability and explanatory adequacy. We show that none of them satisfies both criteria, and suggest new directions for research on alternative metagrammatical formalisms."
P83-1016,A Framework for Processing Partially Free Word Order,1983,6,5,1,1,23887,hans uszkoreit,21st Annual Meeting of the Association for Computational Linguistics,1,"The partially free word order in German belongs to the class of phenomena in natural language that require a close interaction between syntax and pragmatics. Several competing principles, which are based on syntactic and on discourse information, determine the linear order of noun phrases. A solution to problems of this sort is a prerequisite for high-quality language generation. The linguistic framework of Generalized Phrase Structure Grammar offers tools for dealing with word order variation. Some slight modifications to the framework allow for an analysis of the German data that incorporates just the right degree of interaction between syntactic and pragmatic components and that can account for conflicting ordering statements."
