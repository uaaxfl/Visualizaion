2020.emnlp-main.125,Compositional Phrase Alignment and Beyond,2020,-1,-1,2,0.701754,6514,yuki arase,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Phrase alignment is the basis for modelling sentence pair interactions, such as paraphrase and textual entailment recognition. Most phrase alignments are compositional processes such that an alignment of a phrase pair is constructed based on the alignments of their child phrases. Nonetheless, studies have revealed that non-compositional alignments involving long-distance phrase reordering are prevalent in practice. We address the phrase alignment problem by combining an unordered tree mapping algorithm and phrase representation modelling that explicitly embeds the similarity distribution in the sentences onto powerful contextualized representations. Experimental results demonstrate that our method effectively handles compositional and non-compositional global phrase alignments. Our method significantly outperforms that used in a previous study and achieves a performance competitive with that of experienced human annotators."
2020.coling-main.609,{C}haracter{BERT}: Reconciling {ELM}o and {BERT} for Word-Level Open-Vocabulary Representations From Characters,2020,-1,-1,6,0,8589,hicham boukkouri,Proceedings of the 28th International Conference on Computational Linguistics,0,"Due to the compelling improvements brought by BERT, many recent representation models adopted the Transformer architecture as their main building block, consequently inheriting the wordpiece tokenization system despite it not being intrinsically linked to the notion of Transformers. While this system is thought to achieve a good balance between the flexibility of characters and the efficiency of full words, using predefined wordpiece vocabularies from the general domain is not always suitable, especially when building models for specialized domains (e.g., the medical domain). Moreover, adopting a wordpiece tokenization shifts the focus from the word level to the subword level, making the models conceptually more complex and arguably less convenient in practice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations."
D19-1542,Transfer Fine-Tuning: A {BERT} Case Study,2019,28,0,2,0.857143,6514,yuki arase,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"A semantic equivalence assessment is defined as a task that assesses semantic equivalence in a sentence pair by binary judgment (i.e., paraphrase identification) or grading (i.e., semantic textual similarity measurement). It constitutes a set of tasks crucial for research on natural language understanding. Recently, BERT realized a breakthrough in sentence representation learning (Devlin et al., 2019), which is broadly transferable to various NLP tasks. While BERT{'}s performance improves by increasing its model size, the required computational power is an obstacle preventing practical applications from adopting the technology. Herein, we propose to inject phrasal paraphrase relations into BERT in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size. Experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller BERT model while maintaining the model size. The generated model exhibits superior performance compared to a larger BERT model on semantic equivalence assessment tasks. Furthermore, it achieves larger performance gains on tasks with limited training datasets for fine-tuning, which is a property desirable for transfer learning."
L18-1220,{SPADE}: Evaluation Dataset for Monolingual Phrase Alignment,2018,0,0,2,1,6514,yuki arase,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
E17-1093,Distributed Document and Phrase Co-embeddings for Descriptive Clustering,2017,22,2,6,0,25550,motoki sato,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Descriptive document clustering aims to automatically discover groups of semantically related documents and to assign a meaningful label to characterise the content of each cluster. In this paper, we present a descriptive clustering approach that employs a distributed representation model, namely the paragraph vector model, to capture semantic similarities between documents and phrases. The proposed method uses a joint representation of phrases and documents (i.e., a co-embedding) to automatically select a descriptive phrase that best represents each document cluster. We evaluate our method by comparing its performance to an existing state-of-the-art descriptive clustering method that also uses co-embedding but relies on a bag-of-words representation. Results obtained on benchmark datasets demonstrate that the paragraph vector-based method obtains superior performance over the existing approach in both identifying clusters and assigning appropriate descriptive labels to them."
D17-1001,Monolingual Phrase Alignment on Parse Forests,2017,18,4,2,1,6514,yuki arase,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We propose an efficient method to conduct phrase alignment on parse forests for paraphrase detection. Unlike previous studies, our method identifies syntactic paraphrases under linguistically motivated grammar. In addition, it allows phrases to non-compositionally align to handle paraphrases with non-homographic phrase correspondences. A dataset that provides gold parse trees and their phrase alignments is created. The experimental results confirm that the proposed method conducts highly accurate phrase alignment compared to human performance."
P16-2062,A Latent Concept Topic Model for Robust Topic Inference Using Word Embeddings,2016,12,12,2,0,34426,weihua hu,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,None
N15-1143,Estimating Numerical Attributes by Bringing Together Fragmentary Clues,2015,8,7,2,0,4800,hiroya takamura,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This work is an attempt to automatically obtain numerical attributes of physical objects. We propose representing each physical object as a feature vector and representing sizes as linear functions of feature vectors. We train the function in the framework of the combined regression and ranking with many types of fragmentary clues including absolute clues (e.g., A is 30cm long) and relative clues (e.g., A is larger than B)."
E14-4022,Using a Random Forest Classifier to Compile Bilingual Dictionaries of Technical Terms from Comparable Corpora,2014,20,13,3,1,33045,georgios kontonatsios,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"We describe a machine learning approach, a Random Forest (RF) classifier, that is used to automatically compile bilingual dictionaries of technical terms from comparable corpora. We evaluate the RF classifier against a popular term alignment method, namely context vectors, and we report an improvement of the translation accuracy. As an application, we use the automatically extracted dictionary in combination with a trained Statistical Machine Translation (SMT) system to more accurately translate unknown terms. The dictionary extraction method described in this paper is freely available 1 ."
D14-1177,Combining String and Context Similarity for Bilingual Term Alignment from Comparable Corpora,2014,40,11,3,1,33045,georgios kontonatsios,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Automatically compiling bilingual dictionaries of technical terms from comparable corpora is a challenging problem, yet with many potential applications. In this paper, we exploit two independent observations about term translations: (a) terms are often formed by corresponding sub-lexical units across languages and (b) a term and its translation tend to appear in similar lexical context. Based on the first observation, we develop a new character n-gram compositional method, a logistic regression classifier, for learning a string similarity measure of term translations. According to the second observation, we use an existing context-based approach. For evaluation, we investigate the performance of compositional and context-based methods on: (a) similar and unrelated languages, (b) corpora of different degree of comparability and (c) the translation of frequent and rare terms. Finally, we combine the two translation clues, namely string and contextual similarity, in a linear model and we show substantial improvements over the two translation signals."
C14-1150,Common Space Embedding of Primal-Dual Relation Semantic Spaces,2014,26,0,2,0,27871,hidekazu oiwa,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Explicit continuous vector representation such as vector representation of words, phrases, etc. has been proven effective for various NLP tasks. This paper proposes a novel method of constructing such vector representation for both entity-pairs and relation expressions which link them in text. Based on the insight of the duality of relations, the representation is constructed by embedding of two separately constructed semantic spaces, one for entity-pairs and the other for relation expressions, into a common semantic space. By representing the two different types of objects (i.e. entity-pairs and relation expressions) in the same semantic space, we can treat the two tasks, relation mining and relation expression mining (a.k.a. pattern mining), systematically and in a unified manner. The approach is the first attempt to construct a continuous vector representation for expressions whose validity can be explicitly checked by their proximities to known sets of entity-pairs. We also experimentally validate the effectiveness of the common space for relation mining and relation expression mining."
W13-4403,Deep Context-Free Grammar for {C}hinese with Broad-Coverage,2013,10,0,5,1,40670,xiangli wang,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"The accuracy of Chinese parsers trained on Penn Chinese Treebank is evidently lower than that of the English parsers trained on Penn Treebank. It is plausible that the essential reason is the lack of surface syntactic constraints in Chinese. In this paper, we present evidences to show that strict deep syntactic constraints exist in Chinese sentences and such constraints cannot be effectively described with context-free phrase structure rules as in the Penn Chinese Treebank annotation; we show that such constraints may be described precisely by the idea of Sentence Structure Grammar; we introduce how to develop a broad-coverage rule-based grammar for Chinese based on this idea; we evaluated the grammar and the evaluation results show that the coverage of the current grammar is 94.2%."
W13-2512,Using a Random Forest Classifier to recognise translations of biomedical terms across languages,2013,37,1,4,1,33045,georgios kontonatsios,Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,0,"We present a novel method to recognise semantic equivalents of biomedical terms in language pairs. We hypothesise that biomedical term are formed by semantically similar textual units across languages. Based on this hypothesis, we employ a Random Forest (RF) classifier that is able to automatically mine higher order associations between textual units of the source and target language when trained on a corpus of both positive and negative examples. We apply our method on two language pairs: one that uses the same character set and another with a different script, English-French and EnglishChinese, respectively. We show that English-French pairs of terms are highly transliterated in contrast to the EnglishChinese pairs. Nonetheless, our method performs robustly on both cases. We evaluate RF against a state-of-the-art alignment method, GIZA, and we report a statistically significant improvement. Finally, we compare RF against Support Vector Machines and analyse our results."
W13-2009,Overview of the Pathway Curation ({PC}) task of {B}io{NLP} Shared Task 2013,2013,47,30,9,1,35319,tomoko ohta,Proceedings of the {B}io{NLP} Shared Task 2013 Workshop,0,"We present the Pathway Curation (PC) task, a main event extraction task of the BioNLP shared task (ST) 2013. The PC task concerns the automatic extraction of biomolecular reactions from text. The task setting, representation and semantics are defined with respect to pathway model standards and ontologies (SBML, BioPAX, SBO) and documents selected by relevance to specific model reactions. Two BioNLP ST 2013 participants successfully completed the PC task. The highest achieved Fscore, 52.8%, indicates that event extraction is a promising approach to supporting pathway curation efforts. The PC task continues as an open challenge with data, resources and tools available from http://2013.bionlp-st.org/"
W12-4304,Open-domain Anatomical Entity Mention Detection,2012,31,31,3,1,35319,tomoko ohta,Proceedings of the Workshop on Detecting Structure in Scholarly Discourse,0,"Anatomical entities such as kidney, muscle and blood are central to much of biomedical scientific discourse, and the detection of mentions of anatomical entities is thus necessary for the automatic analysis of the structure of domain texts. Although a number of resources and methods addressing aspects of the task have been introduced, there have so far been no annotated corpora for training and evaluating systems for broad-coverage, open-domain anatomical entity mention detection. We introduce the AnEM corpus, a domain- and species-independent resource manually annotated for anatomical entity mentions using a fine-grained classification system. The corpus texts are selected randomly from citation abstracts and full-text papers with the aim of making the corpus representative of the entire available biomedical scientific literature. We demonstrate the use of the corpus through an evaluation of the broad-coverage MetaMap tagger and a CRF-based system trained on the corpus data, considering also a combination of these two methods. The combined system demonstrates a promising level of performance, approaching 80% F-score for mention detection for a relaxed matching criterion. The corpus and other introduced resources are available under open licences from http://www.nactem.ac.uk/anatomy/."
W12-3806,Bridging the Gap Between Scope-based and Event-based Negation/Speculation Annotations: A Bridge Not Too Far,2012,25,3,5,1,4231,pontus stenetorp,Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,0,"We study two approaches to the marking of extra-propositional aspects of statements in text: the task-independent cue-and-scope representation considered in the CoNLL-2010 Shared Task, and the tagged-event representation applied in several recent event extraction tasks. Building on shared task resources and the analyses from state-of-the-art systems representing the two broad lines of research, we identify specific points of mismatch between the two perspectives and propose ways of addressing them. We demonstrate the feasibility of our approach by constructing a method that uses cue-and-scope analyses together with a small set of features motivated by data analysis to predict event negation and speculation. Evaluation on BioNLP Shared Task 2011 data indicates the method to outperform the negation/speculation components of state-of-the-art event extraction systems.n n The system and resources introduced in this work are publicly available for research purposes at: https://github.com/ninjin/eepura"
P12-3022,{A}kamon: An Open Source Toolkit for Tree/Forest-Based Statistical Machine Translation,2012,34,2,3,0.909091,6319,xianchao wu,Proceedings of the {ACL} 2012 System Demonstrations,0,"We describe Akamon, an open source toolkit for tree and forest-based statistical machine translation (Liu et al., 2006; Mi et al., 2008; Mi and Huang, 2008). Akamon implements all of the algorithms required for tree/forest-to-string decoding using tree-to-string translation rules: multiple-thread forest-based decoding, n-gram language model integration, beam- and cube-pruning, k-best hypotheses extraction, and minimum error rate training. In terms of tree-to-string translation rule extraction, the toolkit implements the traditional maximum likelihood algorithm using PCFG trees (Galley et al., 2004) and HPSG trees/forests (Wu et al., 2010)."
P12-1110,"Incremental Joint Approach to Word Segmentation, {POS} Tagging, and Dependency Parsing in {C}hinese",2012,18,54,4,1,42725,jun hatori,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models."
wang-etal-2012-biomedical,Biomedical {C}hinese-{E}nglish {CLIR} Using an Extended {CM}e{SH} Resource to Expand Queries,2012,25,4,3,0,42943,xinkai wang,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Cross-lingual information retrieval (CLIR) involving the Chinese language has been thoroughly studied in the general language domain, but rarely in the biomedical domain, due to the lack of suitable linguistic resources and parsing tools. In this paper, we describe a Chinese-English CLIR system for biomedical literature, which exploits a bilingual ontology, the ``eCMeSH Tree''''''''. This is an extension of the Chinese Medical Subject Headings (CMeSH) Tree, based on Medical Subject Headings (MeSH). Using the 2006 and 2007 TREC Genomics track data, we have evaluated the performance of the eCMeSH Tree in expanding queries. We have compared our results to those obtained using two other approaches, i.e. pseudo-relevance feedback (PRF) and document translation (DT). Subsequently, we evaluate the performance of different combinations of these three retrieval methods. Our results show that our method of expanding queries using the eCMeSH Tree can outperform the PRF method. Furthermore, combining this method with PRF and DT helps to smooth the differences in query expansion, and consequently results in the best performance amongst all experiments reported. All experiments compare the use of two different retrieval models, i.e. Okapi BM25 and a query likelihood language model. In general, the former performs slightly better."
E12-2021,brat: a Web-based Tool for {NLP}-Assisted Text Annotation,2012,20,383,6,1,4231,pontus stenetorp,Proceedings of the Demonstrations at the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15% decrease in total annotation time. BRAT is available under an open-source license from: http://brat.nlplab.org"
E12-1044,Coordination Structure Analysis using Dual Decomposition,2012,13,10,3,0,43593,atsushi hanamoto,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Coordination disambiguation remains a difficult sub-problem in parsing despite the frequency and importance of coordination structures. We propose a method for disambiguating coordination structures. In this method, dual decomposition is used as a framework to take advantage of both HPSG parsing and coordinate structure analysis with alignment-based local features. We evaluate the performance of the proposed method on the Genia corpus and the Wall Street Journal portion of the Penn Treebank. Results show it increases the percentage of sentences in which coordination structures are detected correctly, compared with each of the two algorithms alone."
W11-2907,Analysis of the Difficulties in {C}hinese Deep Parsing,2011,33,9,5,1,44132,kun yu,Proceedings of the 12th International Conference on Parsing Technologies,0,"This paper discusses the difficulties in Chinese deep parsing, by comparing the accuracy of a Chinese HPSG parser to the accuracy of an English HPSG parser and the commonly used Chinese syntactic parsers. Analysis reveals that deep parsing for Chinese is more challenging than for English, due to the shortage of syntactic constraints of Chinese verbs, the widespread pro-drop, and the large distribution of ambiguous constructions. Moreover, the inherent ambiguities caused by verbal co-ordination and relative clauses make semantic analysis of Chinese more difficult than the syntactic analysis of Chinese."
W11-1801,Overview of {B}io{NLP} Shared Task 2011,2011,30,181,6,1,15849,jindong kim,Proceedings of {B}io{NLP} Shared Task 2011 Workshop,0,"The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects."
W11-1803,Overview of the Epigenetics and Post-translational Modifications ({EPI}) task of {B}io{NLP} Shared Task 2011,2011,39,32,3,1,35319,tomoko ohta,Proceedings of {B}io{NLP} Shared Task 2011 Workshop,0,"This paper presents the preparation, resources, results and analysis of the Epigenetics and Post-translational Modifications (EPI) task, a main task of the BioNLP Shared Task 2011. The task concerns the extraction of detailed representations of 14 protein and DNA modification events, the catalysis of these reactions, and the identification of instances of negated or speculatively stated event instances. Seven teams submitted final results to the EPI task in the shared task, with the highest-performing system achieving 53% F-score in the full task and 69% F-score in the extraction of a simplified set of core event arguments."
W11-1804,Overview of the Infectious Diseases ({ID}) task of {B}io{NLP} Shared Task 2011,2011,33,34,8,1,2607,sampo pyysalo,Proceedings of {B}io{NLP} Shared Task 2011 Workshop,0,"This paper presents the preparation, resources, results and analysis of the Infectious Diseases (ID) information extraction task, a main task of the BioNLP Shared Task 2011. The ID task represents an application and extension of the BioNLP'09 shared task event extraction approach to full papers on infectious diseases. Seven teams submitted final results to the task, with the highest-performing system achieving 56% F-score in the full task, comparable to state-of-the-art performance in the established BioNLP'09 task. The results indicate that event extraction methods generalize well to new domains and full-text publications and are applicable to the extraction of events relevant to the molecular mechanisms of infectious diseases."
W11-1811,Overview of {B}io{NLP} 2011 Protein Coreference Shared Task,2011,7,14,3,1,1934,ngan nguyen,Proceedings of {B}io{NLP} Shared Task 2011 Workshop,0,None
W11-1812,Overview of the Entity Relations ({REL}) supporting task of {B}io{NLP} Shared Task 2011,2011,29,23,3,1,2607,sampo pyysalo,Proceedings of {B}io{NLP} Shared Task 2011 Workshop,0,"This paper presents the Entity Relations (REL) task, a supporting task of the BioNLP Shared Task 2011. The task concerns the extraction of two types of part-of relations between a gene/protein and an associated entity. Four teams submitted final results for the REL task, with the highest-performing system achieving 57.7% F-score. While experiments suggest use of the data can help improve event extraction performance, the task data has so far received only limited use in support of event extraction. The REL task continues as an open challenge, with all resources available from the shared task website."
W11-1816,{B}io{NLP} Shared Task 2011: Supporting Resources,2011,37,41,6,1,4231,pontus stenetorp,Proceedings of {B}io{NLP} Shared Task 2011 Workshop,0,"This paper describes the supporting resources provided for the BioNLP Shared Task 2011. These resources were constructed with the goal to alleviate some of the burden of system development from the participants and allow them to focus on the novel aspects of constructing their event extraction systems. With the availability of these resources we also seek to enable the evaluation of the applicability of specific tools and representations towards improving the performance of event extraction systems. Additionally we supplied evaluation software and services and constructed a visualisation tool, stav, which visualises event extraction results and annotations. These resources helped the participants make sure that their final submissions and research efforts were on track during the development stages and evaluate their progress throughout the duration of the shared task. The visualisation software was also employed to show the differences between the gold annotations and those of the submitted results, allowing the participants to better understand the performance of their system. The resources, evaluation tools and visualisation tool are provided freely for research purposes and can be found at http://sites.google.com/site/bionlpst/"
W11-1203,Learning the Optimal Use of Dependency-parsing Information for Finding Translations with Comparable Corpora,2011,15,4,3,0.769231,37700,daniel andrade,Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,0,"Using comparable corpora to find new word translations is a promising approach for extending bilingual dictionaries (semi-) automatically. The basic idea is based on the assumption that similar words have similar contexts across languages. The context of a word is often summarized by using the bag-of-words in the sentence, or by using the words which are in a certain dependency position, e.g. the predecessors and successors. These different context positions are then combined into one context vector and compared across languages. However, previous research makes the (implicit) assumption that these different context positions should be weighted as equally important. Furthermore, only the same context positions are compared with each other, for example the successor position in Spanish is compared with the successor position in English. However, this is not necessarily always appropriate for languages like Japanese and English. To overcome these limitations, we suggest to perform a linear transformation of the context vectors, which is defined by a matrix. We define the optimal transformation matrix by using a Bayesian probabilistic model, and show that it is feasible to find an approximate solution using Markov chain Monte Carlo methods. Our experiments demonstrate that our proposed method constantly improves translation accuracy."
W11-0407,A Collaborative Annotation between Human Annotators and a Statistical Parser,2011,11,0,5,0,44409,shunya iwasawa,Proceedings of the 5th Linguistic Annotation Workshop,0,"We describe a new interactive annotation scheme between a human annotator who carries out simplified annotations on CFG trees, and a statistical parser that converts the human annotations automatically into a richly annotated HPSG treebank. In order to check the proposed scheme's effectiveness, we performed automatic pseudo-annotations that emulate the system's idealized behavior and measured the performance of the parser trained on those annotations. In addition, we implemented a prototype system and conducted manual annotation experiments on a small test set."
W11-0208,Automatic Acquisition of Huge Training Data for Bio-Medical Named Entity Recognition,2011,18,18,4,0,44431,yu usami,Proceedings of {B}io{NLP} 2011 Workshop,0,"Named Entity Recognition (NER) is an important first step for BioNLP tasks, e.g., gene normalization and event extraction. Employing supervised machine learning techniques for achieving high performance recent NER systems require a manually annotated corpus in which every mention of the desired semantic types in a text is annotated. However, great amounts of human effort is necessary to build and maintain an annotated corpus. This study explores a method to build a high-performance NER without a manually annotated corpus, but using a comprehensible lexical database that stores numerous expressions of semantic types and with huge amount of unannotated texts. We underscore the effectiveness of our approach by comparing the performance of NERs trained on an automatically acquired training data and on a manually annotated corpus."
W11-0214,From Pathways to Biomolecular Events: Opportunities and Challenges,2011,23,19,3,1,35319,tomoko ohta,Proceedings of {B}io{NLP} 2011 Workshop,0,"The construction of pathways is a major focus of present-day biology. Typical pathways involve large numbers of entities of various types whose associations are represented as reactions involving arbitrary numbers of reactants, outputs and modifiers. Until recently, few information extraction approaches were capable of resolving the level of detail in text required to support the annotation of such pathway representations. We argue that event representations of the type popularized by the BioNLP Shared Task are potentially applicable for pathway annotation support. As a step toward realizing this possibility, we study the mapping from a formal pathway representation to the event representation in order to identify remaining challenges in event extraction for pathway annotation support. Following initial analysis, we present a detailed study of protein association and dissociation reactions, proposing a new event class and representation for the latter and, as a step toward its automatic extraction, introduce a manually annotated resource incorporating the type among a total of nearly 1300 annotated event instances. As a further practical contribution, we introduce the first pathway-to-event conversion software for SBML/CellDesigner pathways and discuss the opportunities arising from the ability to convert the substantial existing pathway resources to events."
W11-0215,Towards Exhaustive Event Extraction for Protein Modifications,2011,0,8,4,1,2607,sampo pyysalo,Proceedings of {B}io{NLP} 2011 Workshop,0,None
W11-0218,{S}im{S}em: Fast Approximate String Matching in Relation to Semantic Category Disambiguation,2011,26,6,3,1,4231,pontus stenetorp,Proceedings of {B}io{NLP} 2011 Workshop,0,"In this study we investigate the merits of fast approximate string matching to address challenges relating to spelling variants and to utilise large-scale lexical resources for semantic class disambiguation. We integrate string matching results into machine learning-based disambiguation through the use of a novel set of features that represent the distance of a given textual span to the closest match in each of a collection of lexical resources. We collect lexical resources for a multitude of semantic categories from a variety of biomedical domain sources. The combined resources, containing more than twenty million lexical items, are queried using a recently proposed fast and efficient approximate string matching algorithm that allows us to query large resources without severely impacting system performance. We evaluate our results on six corpora representing a variety of disambiguation tasks. While the integration of approximate string matching features is shown to substantially improve performance on one corpus, results are modest or negative for others. We suggest possible explanations and future research directions. Our lexical resources and implementation are made freely available for research purposes at: http://github.com/ninjin/simsem"
P11-1003,Effective Use of Function Words for Rule Generalization in Forest-Based Translation,2011,26,7,3,1,6319,xianchao wu,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-to-Japanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system."
I11-1084,Exploring Difficulties in Parsing Imperatives and Questions,2011,19,2,4,1,38278,tadayoshi hara,Proceedings of 5th International Joint Conference on Natural Language Processing,0,This paper analyzes the effect of the structural variation of sentences on parsing performance. We examine the performance of both shallow and deep parsers for two sentence constructions: imperatives and questions. We first prepare an annotated corpus for each of these sentence constructions by extracting sentences from a fiction domain that cover various types of imperatives and questions. The target parsers are then adapted to each of the obtained corpora as well as the existing query-focused corpus. Analysis of the experimental results reveals that the current mainstream parsing technologies and adaptation techniques cannot cope with different sentence constructions even with much in-domain data.
I11-1136,Incremental Joint {POS} Tagging and Dependency Parsing in {C}hinese,2011,21,60,4,1,42725,jun hatori,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We address the problem of joint part-of-speech (POS) tagging and dependency parsing in Chinese. In Chinese, some POS tags are often hard to disambiguate without considering longrange syntactic information. Also, the traditional pipeline approach to POS tagging and dependency parsing may suffer from the problem of error propagation. In this paper, we propose the first incremental approach to the task of joint POS tagging and dependency parsing, which is built upon a shift-reduce parsing framework with dynamic programming. Although the incremental approach encounters difficulties with underspecified POS tags of look-ahead words, we overcome this issue by introducing so-called delayed features. Our joint approach achieved substantial improvements over the pipeline and baseline systems in both POS tagging and dependency parsing task, achieving the new state-of-the-art performance on this joint task."
2011.mtsummit-plenaries.3,Challenges of Patent {MT} {--} Term and Structure Translation,2011,-1,-1,1,1,20171,junichi tsujii,Proceedings of Machine Translation Summit XIII: Plenaries,0,None
2011.iwslt-keynotes.3,Resource-rich research on natural language processing and understanding,2011,0,0,1,1,20171,junichi tsujii,Proceedings of the 8th International Workshop on Spoken Language Translation: Keynotes,0,None
Y10-1055,A Modular Architecture for the Wide-Coverage Translation of Natural Language Texts into Predicate Logic Formulas,2010,16,1,4,0,5928,yusuke miyao,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"We present a new method for translating unrestricted natural language texts into predicate logic formulas. This relies on the semantic evaluation procedure of Scope Control Theory (SCT), a variant of Dynamic Semantic formalisms. The key benefit is that parsed syntactic structures are shown to form sufficient input for semantic evaluation, eliminating the need to build distinct semantic expressions to feed semantic evaluation. To have parsed syntactic structures for SCT to evaluate we apply an existing wide-coverage syntactic parser by converting the parser output into a form SCT can receive. This modularity has led to the rapid attainment of a broad coverage on real text. An experiment revealed our system achieved 82.7% coverage on real-world sentences, generating representations that make explicit the scopes of quantifiers (e.g., xe2x88x83x), operators (e.g., negation), connectives (e.g., conjunction) and embedding predicates (e.g., thinks), while also capturing the inter and intra sentential dependencies and cross-sentential anaphoric dependencies that connect predicates."
W10-1903,Event Extraction for Post-Translational Modifications,2010,20,10,5,1,35319,tomoko ohta,Proceedings of the 2010 Workshop on Biomedical Natural Language Processing,0,"We consider the task of automatically extracting post-translational modification events from biomedical scientific publications. Building on the success of event extraction for phosphorylation events in the BioNLP'09 shared task, we extend the event annotation approach to four major new post-transitional modification event types. We present a new targeted corpus of 157 PubMed abstracts annotated for over 1000 proteins and 400 post-translational modification events identifying the modified proteins and sites. Experiments with a state-of-the-art event extraction system show that the events can be extracted with 52% precision and 36% recall (42% F-score), suggesting remaining challenges in the extraction of the events. The annotated corpus is freely available in the BioNLP'09 shared task format at the GE-NIA project homepage."
W10-1904,Scaling up Biomedical Event Extraction to the Entire {P}ub{M}ed,2010,20,27,4,0,28457,jari bjorne,Proceedings of the 2010 Workshop on Biomedical Natural Language Processing,0,"We present the first full-scale event extraction experiment covering the titles and abstracts of all PubMed citations. Extraction is performed using a pipeline composed of state-of-the-art methods: the BANNER named entity recognizer, the McClosky-Charniak domain-adapted parser, and the Turku Event Extraction System. We analyze the statistical properties of the resulting dataset and present evaluations of the core event extraction as well as negation and speculation detection components of the system. Further, we study in detail the set of extracted events relevant to the apoptosis pathway to gain insight into the biological relevance of the result. The dataset, consisting of 19.2 million occurrences of 4.5 million unique events, is freely available for use in research at http://bionlp.utu.fi/."
W10-1905,A Comparative Study of Syntactic Parsers for Event Extraction,2010,17,31,4,1,3222,makoto miwa,Proceedings of the 2010 Workshop on Biomedical Natural Language Processing,0,"The extraction of biomolecular events from text is an important task for a number of domain applications such as pathway construction. Several syntactic parsers have been used in Biomedical Natural Language Processing (BioNLP) applications, and the BioNLP 2009 Shared Task results suggest that incorporation of syntactic analysis is important to achieving state-of-the-art performance. Direct comparison of parsers is complicated by to differences in the such as the division between phrase structure- and dependency-based analyses and the variety of output formats, structures and representations applied. In this paper, we present a task-oriented comparison of five parsers, measuring their contribution to biomolecular event extraction using a state-of-the-art event extraction system. The results show that the parsers with domain models using dependency formats provide very similar performance, and that an ensemble of different parsers in different formats can improve the event extraction system."
W10-1919,Towards Event Extraction from Full Texts on Infectious Diseases,2010,17,11,7,1,2607,sampo pyysalo,Proceedings of the 2010 Workshop on Biomedical Natural Language Processing,0,"Event extraction approaches based on expressive structured representations of extracted information have been a significant focus of research in recent biomedical natural language processing studies. However, event extraction efforts have so far been limited to publication abstracts, with most studies further considering only the specific transcription factor-related subdo-main of molecular biology of the GENIA corpus. To establish the broader relevance of the event extraction approach and proposed methods, it is necessary to expand on these constraints. In this study, we propose an adaptation of the event extraction approach to a subdomain related to infectious diseases and present analysis and initial experiments on the feasibility of event extraction from domain full text publications."
W10-1816,The Deep Re-Annotation in a {C}hinese Scientific Treebank,2010,13,0,5,1,44132,kun yu,Proceedings of the Fourth Linguistic Annotation Workshop,0,"In this paper, we introduce our recent work on re-annotating the deep information, which includes both the grammatical functional tags and the traces, in a Chinese scientific treebank. The issues with regard to re-annotation and its corresponding solutions are discussed. Furthermore, the process of the re-annotation work is described."
P10-1034,Fine-Grained Tree-to-String Translation Rule Extraction,2010,28,12,3,1,6319,xianchao wu,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this paper, we propose to use deep syntactic information for obtaining fine-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on large-scale bidirectional Japanese-English translations testified the effectiveness of our approach."
N10-1090,A Simple Approach for {HPSG} Supertagging Using Dependency Information,2010,16,5,3,1,45809,yaozhong zhang,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In a supertagging task, sequence labeling models are commonly used. But their limited ability to model long-distance information presents a bottleneck to make further improvements. In this paper, we modeled this long-distance information in dependency formalism and integrated it into the process of HPSG supertagging. The experiments showed that the dependency information is very informative for supertag disambiguation. We also evaluated the improved supertagger in the HPSG parser."
kano-etal-2010-u,{U}-Compare: An Integrated Language Resource Evaluation Platform Including a Comprehensive {UIMA} Resource Library,2010,8,10,5,1,8106,yoshinobu kano,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Language resources, including corpus and tools, are normally required to be combined in order to achieve a userÂs specific task. However, resources tend to be developed independently in different, incompatible formats. In this paper we describe about U-Compare, which consists of the U-Compare component repository and the U-Compare platform. We have been building a highly interoperable resource library, providing the world largest ready-to-use UIMA component repository including wide variety of corpus readers and state-of-the-art language tools. These resources can be deployed as local services or web services, even possible to be hosted in clustered machines to increase the performance, while users do not need to be aware of such differences. In addition to the resource library, an integrated language processing platform is provided, allowing workflow creation, comparison, evaluation and visualization, using the resources in the library or any UIMA component, without any programming via graphical user interfaces, while a command line launcher is also available without GUIs. The evaluation itself is processed in a UIMA component, users can create and plug their own evaluation metrics in addition to the predefined metrics. U-Compare has been successfully used in many projects including BioCreative, Conll and the BioNLP shared task."
hanaoka-etal-2010-japanese,A {J}apanese Particle Corpus Built by Example-Based Annotation,2010,5,2,3,0,41495,hiroki hanaoka,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper is a report on an on-going project of creating a new corpus focusing on Japanese particles. The corpus will provide deeper syntactic/semantic information than the existing resources. The initial target particle is ``to'' which occurs 22,006 times in 38,400 sentences of the existing corpus: the Kyoto Text Corpus. In this annotation task, an ``example-based'' methodology is adopted for the corpus annotation, which is different from the traditional annotation style. This approach provides the annotators with an example sentence rather than a linguistic category label. By avoiding linguistic technical terms, it is expected that any native speakers, with no special knowledge on linguistic analysis, can be an annotator without long training, and hence it can reduce the annotation cost. So far, 10,475 occurrences have been already annotated, with an inter-annotator agreement of 0.66 calculated by Cohen's kappa. The initial disagreement analyses and future directions are discussed in the paper."
C10-2098,Imbalanced Classification Using Dictionary-based Prototypes and Hierarchical Decision Rules for Entity Sense Disambiguation,2010,17,2,3,0,26225,tingting mu,Coling 2010: Posters,0,"Entity sense disambiguation becomes difficult with few or even zero training instances available, which is known as imbalanced learning problem in machine learning. To overcome the problem, we create a new set of reliable training instances from dictionary, called dictionary-based prototypes. A hierarchical classification system with a tree-like structure is designed to learn from both the prototypes and training instances, and three different types of classifiers are employed. In addition, supervised dimensionality reduction is conducted in a similarity-based space. Experimental results show our system outperforms three baseline systems by at least 8.3% as measured by macro F1 score."
C10-2162,Semi-automatically Developing {C}hinese {HPSG} Grammar from the {P}enn {C}hinese Treebank for Deep Parsing,2010,29,16,5,1,44132,kun yu,Coling 2010: Posters,0,"In this paper, we introduce our recent work on Chinese HPSG grammar development through treebank conversion. By manually defining grammatical constraints and annotation rules, we convert the bracketing trees in the Penn Chinese Treebank (CTB) to be an HPSG treebank. Then, a large-scale lexicon is automatically extracted from the HPSG treebank. Experimental results on the CTB 6.0 show that a HPSG lexicon was successfully extracted with 97.24% accuracy; furthermore, the obtained lexicon achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text, which are comparable to the state-of-the-art works for English."
C10-1003,Robust Measurement and Comparison of Context Similarity for Finding Translation Pairs,2010,19,16,3,0.769231,37700,daniel andrade,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"In cross-language information retrieval it is often important to align words that are similar in meaning in two corpora written in different languages. Previous research shows that using context similarity to align words is helpful when no dictionary entry is available. We suggest a new method which selects a subset of words (pivot words) associated with a query and then matches these words across languages. To detect word associations, we demonstrate that a new Bayesian method for estimating Point-wise Mutual Information provides improved accuracy. In the second step, matching is done in a novel way that calculates the chance of an accidental overlap of pivot words using the hypergeometric distribution. We implemented a wide variety of previously suggested methods. Testing in two conditions, a small comparable corpora pair and a large but unrelated corpora pair, both written in disparate languages, we show that our approach consistently outperforms the other systems."
C10-1088,Evaluating Dependency Representations for Event Extraction,2010,19,28,4,1,3222,makoto miwa,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"The detailed analyses of sentence structure provided by parsers have been applied to address several information extraction tasks. In a recent bio-molecular event extraction task, state-of-the-art performance was achieved by systems building specifically on dependency representations of parser output. While intrinsic evaluations have shown significant advances in both general and domain-specific parsing, the question of how these translate into practical advantage is seldom considered. In this paper, we analyze how event extraction performance is affected by parser and dependency representation, further considering the relation between intrinsic evaluation and performance at the extraction task. We find that good intrinsic evaluation results do not always imply good extraction performance, and that the types and structures of different dependency representations have specific advantages and disadvantages for the event extraction task."
C10-1089,Entity-Focused Sentence Simplification for Relation Extraction,2010,19,36,4,1,3222,makoto miwa,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Relations between entities in text have been widely researched in the natural language processing and information-extraction communities. The region connecting a pair of entities (in a parsed sentence) is often used to construct kernels or feature vectors that can recognize and extract interesting relations. Such regions are useful, but they can also incorporate unnecessary distracting information. In this paper, we propose a rule-based method to remove the information that is unnecessary for relation extraction. Protein-protein interaction (PPI) is used as an example relation extraction problem. A dozen simple rules are defined on output from a deep parser. Each rule specifically examines the entities in one target interaction pair. These simple rules were tested using several PPI corpora. The PPI extraction performance was improved on all the PPI corpora."
C10-1096,Simple and Efficient Algorithm for Approximate Dictionary Matching,2010,24,42,2,1,4956,naoaki okazaki,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"This paper presents a simple and efficient algorithm for approximate dictionary matching designed for similarity measures such as cosine, Dice, Jaccard, and overlap coefficients. We propose this algorithm, called CPMerge, for the xcfx84-overlap join of inverted lists. First we show that this task is solvable exactly by a xcfx84-overlap join. Given inverted lists retrieved for a query, the algorithm collects fewer candidate strings and prunes unlikely candidates to efficiently find strings that satisfy the constraint of the xcfx84-overlap join. We conducted experiments of approximate dictionary matching on three large-scale datasets that include person names, biomedical names, and general English words. The algorithm exhibited scalable performance on the datasets. For example, it retrieved strings in 1.1 ms from the string collection of Google Web1T unigrams (with cosine similarity and threshold 0.7)."
C10-1144,Forest-guided Supertagger Training,2010,18,3,3,1,45809,yaozhong zhang,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Supertagging is an important technique for deep syntactic analysis. A supertagger is usually trained independently of the parser using a sequence labeling method. This presents an inconsistent training objective between the supertagger and the parser. In this paper, we propose a forest-guided supertagger training method to alleviate this problem by incorporating global grammar constraints into the supertagging process using a CFG-filter. It also provides an approach to make the supertagger and the parser more tightly integrated. The experiment shows that using the forest-guided trained supertagger, the parser got an absolute 0.68% improvement from baseline in F-score for predicate-argument relation recognition accuracy and achieved a competitive result of 89.31% with a faster parsing speed, compared to a state-of-the-art HPSG parser."
Y09-2040,{G}uide{L}ink: A Corpus Annotation System that Integrates the Management of Annotation Guidelines,2009,9,1,4,0,46715,kenta oouchida,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"This paper presents an annotation framework wherein the management process of the annotation guidelines is integrated into the annotation process. Such an integration allows systematic management and reference of guidelines during annotation. For the evaluation of the proposed annotation system, we compare the conventional and proposed annotation frameworks, experiments using automatic guideline suggestion, and describe a unique feature of the integrated framework."
Y09-2048,Design of {C}hinese {HPSG} Framework for Data-Driven Parsing,2009,15,7,6,1,40670,xiangli wang,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"Data-driven parsing has been a main method for analyzing natural languages. We aim at exploring a data-driven Chinese parser, by basing it on Head-driven Phrase Structure Grammar (HPSG). Unlike for English, there is still no available Chinese HPSG framework. As the first step of our work, we design a Chinese HPSG framework, which can be used as the basis for a practical parser. In this paper, 1) we present a Chinese syntactic structure system and 2) we design a primary Chinese HPSG framework."
W09-3814,Evaluating Contribution of Deep Syntactic Information to Shallow Semantic Analysis,2009,5,2,2,0,29831,sumire uematsu,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"This paper presents shallow semantic parsing based only on HPSG parses. An HPSG-FrameNet map was constructed from a semantically annotated corpus, and semantic parsing was performed by mapping HPSG dependencies to FrameNet relations. The semantic parsing was evaluated in a Senseval-3 task; the results suggested that there is a high contribution of syntactic information to semantic analysis."
W09-3828,Effective Analysis of Causes and Inter-dependencies of Parsing Errors,2009,12,2,3,1,38278,tadayoshi hara,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"In this paper, we propose two methods for analyzing errors in parsing. One is to classify errors into categories which grammar developers can easily associate with defects in grammar or a parsing model and thus its improvement. The other is to discover inter-dependencies among errors, and thus grammar developers can focus on errors which are crucial for improving the performance of a parsing model.n n The first method uses patterns of errors to associate them with categories of causes for those errors, such as errors in scope determination of coordination, PP-attachment, identification of antecedent of relative clauses, etc. On the other hand, the second method, which is based on reparsing with one of observed errors corrected, assesses inter-dependencies among errors by examining which other errors were to be corrected as a result if a specific error was corrected.n n Experiments show that these two methods are complementary and by being combined, they can provide useful clues as to how to improve a given grammar."
W09-3832,{HPSG} Supertagging: A Sequence Labeling View,2009,12,8,3,1,45809,yaozhong zhang,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"Supertagging is a widely used speed-up technique for deep parsing. In another aspect, supertagging has been exploited in other NLP tasks than parsing for utilizing the rich syntactic information given by the supertags. However, the performance of supertagger is still a bottleneck for such applications. In this paper, we investigated the relationship between supertagging and parsing, not just to speed up the deep parser; We started from a sequence labeling view of HPSG supertagging, examining how well a supertagger can do when separated from parsing. Comparison of two types of supertagging model, point-wise model and sequential model, showed that the former model works competitively well despite its simplicity, which indicates the true dependency among supertag assignments is far more complex than the crude first-order approximation made in the sequential model. We then analyzed the limitation of separated supertagging by using a CFG-filter. The results showed that big gains could be acquired by resorting to a light-weight parser."
W09-1504,Integrated {NLP} Evaluation System for Pluggable Evaluation Metrics with Extensive Interoperable Toolkit,2009,17,7,4,1,8106,yoshinobu kano,"Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing ({SETQA}-{NLP} 2009)",0,"To understand the key characteristics of NLP tools, evaluation and comparison against different tools is important. And as NLP applications tend to consist of multiple semi-independent sub-components, it is not always enough to just evaluate complete systems, a fine grained evaluation of underlying components is also often worthwhile. Standardization of NLP components and resources is not only significant for reusability, but also in that it allows the comparison of individual components in terms of reliability and robustness in a wider range of target domains. But as many evaluation metrics exist in even a single domain, any system seeking to aid inter-domain evaluation needs not just predefined metrics, but must also support pluggable user-defined metrics. Such a system would of course need to be based on an open standard to allow a large number of components to be compared, and would ideally include visualization of the differences between components. We have developed a pluggable evaluation system based on the UIMA framework, which provides visualization useful in error analysis. It is a single integrated system which includes a large ready-to-use, fully interoperable library of NLP tools."
W09-1401,Overview of {B}io{NLP}{'}09 Shared Task on Event Extraction,2009,21,451,5,1,15849,jindong kim,Proceedings of the {B}io{NLP} 2009 Workshop Companion Volume for Shared Task,0,"The paper presents the design and implementation of the BioNLP'09 Shared Task, and reports the final results with analysis. The shared task consists of three sub-tasks, each of which addresses bio-molecular event extraction at a different level of specificity. The data was developed based on the GENIA event corpus. The shared task was run over 12 weeks, drawing initial interest from 42 teams. Of these teams, 24 submitted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges."
W09-1406,A {M}arkov {L}ogic Approach to Bio-Molecular Event Extraction,2009,11,78,4,0,3873,sebastian riedel,Proceedings of the {B}io{NLP} 2009 Workshop Companion Volume for Shared Task,0,"In this paper we describe our entry to the BioNLP 2009 Shared Task regarding biomolecular event extraction. Our work can be described by three design decisions: (1) instead of building a pipeline using local classifier technology, we design and learn a joint probabilistic model over events in a sentence; (2) instead of developing specific inference and learning algorithms for our joint model, we apply Markov Logic, a general purpose Statistical Relation Learning language, for this task; (3) we represent events as relational structures over the tokens of a sentence, as opposed to structures that explicitly mention abstract event entities. Our results are competitive: we achieve the 4th best scores for task 1 (in close range to the 3rd place) and the best results for task 2 with a 13 percent point margin."
W09-1414,From Protein-Protein Interaction to Molecular Event Extraction,2009,3,12,4,1,32381,rune saetre,Proceedings of the {B}io{NLP} 2009 Workshop Companion Volume for Shared Task,0,"This document describes the methods and results for our participation in the BioNLP'09 Shared Task #1 on Event Extraction. It also contains some error analysis and a brief discussion of the results. Previous shared tasks in the BioNLP community have focused on extracting gene and protein names, and on finding (direct) protein-protein interactions (PPI). This year's task was slightly different, since the protein names were already manually annotated in the text. The new challenge was to extract biological events involving these given gene and gene products. We modified a publicly available system (AkanePPI) to apply it to this new, but similar, protein interaction task. AkanePPI has previously achieved state-of-the-art performance on all existing public PPI corpora, and only small changes were needed to achieve competitive results on this event extraction task. Our official result was an F-score of 36.9%, which was ranked as number six among submissions from 24 different groups. We later balanced the recall/precision by including more predictions than just the most confident one in ambiguous cases, and this raised the F-score on the test-set to 42.6%. The new Akane program can be used freely for academic purposes."
W09-1301,Static Relations: a Piece in the Biomedical Information Extraction Puzzle,2009,20,34,4,1,2607,sampo pyysalo,Proceedings of the {B}io{NLP} 2009 Workshop,0,"We propose a static relation extraction task to complement biomedical information extraction approaches. We argue that static relations such as part-whole are implicitly involved in many common extraction settings, define a task setting making them explicit, and discuss their integration into previously proposed tasks and extraction methods. We further identify a specific static relation extraction task motivated by the BioNLP'09 shared task on event extraction, introduce an annotated corpus for the task, and demonstrate the feasibility of the task by experiments showing that the defined relations can be reliably extracted. The task setting and corpus can serve to support several forms of domain information extraction."
W09-1313,Incorporating {GENETAG}-style annotation to {GENIA} corpus,2009,4,36,5,1,35319,tomoko ohta,Proceedings of the {B}io{NLP} 2009 Workshop,0,"Proteins and genes are the most important entities in molecular biology, and their automated recognition in text is the most widely studied task in biomedical information extraction (IE). Several corpora containing annotation for these entities have been introduced, GENIA (Kim et al., 2003; Kim et al., 2008) and GENETAG (Tanabe et al., 2005) being the most prominent and widely applied. While both aim to address protein/gene annotation, their annotation principles differ notably. One key difference is that GENETAG annotates the conceptual entity, gene, which is often associated with a function, while GENIA concentrates on the physical forms of gene, i.e. protein, DNA and RNA. The difference has caused serious problems relating to the compatibility and comparability of the annotations. In this work, we present an extension of GENIA annotation which integrates GENETAG-style gene annotation. The new version of the GENIA corpus is the first to bring together these two types of entity annotation."
W09-1321,Bridging the Gap between Domain-Oriented and Linguistically-Oriented Semantics,2009,13,6,3,0,29831,sumire uematsu,Proceedings of the {B}io{NLP} 2009 Workshop,0,"This paper compares domain-oriented and linguistically-oriented semantics, based on the GENIA event corpus and FrameNet. While the domain-oriented semantic structures are direct targets of Text Mining (TM), their extraction from text is not straghtforward due to the diversity of linguistic expressions. The extraction of linguistically-oriented semactics is more straghtforward, and has been studied independentely of specific domains. In order to find a use of the domain-independent research achievements for TM, we aim at linking classes of the two types of semantics. The classes were connected by analyzing linguistically-oriented semantics of the expressions that mention one biological class. With the obtained relationship between the classes, we discuss a link between TM and linguistically-oriented semantics."
P09-2008,A Novel Word Segmentation Approach for Written Languages with Word Boundary Markers,2009,3,0,5,1,44432,hancheol cho,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Most NLP applications work under the assumption that a user input is error-free; thus, word segmentation (WS) for written languages that use word boundary markers (WBMs), such as spaces, has been regarded as a trivial issue. However, noisy real-world texts, such as blogs, e-mails, and SMS, may contain spacing errors that require correction before further processing may take place. For the Korean language, many researchers have adopted a traditional WS approach, which eliminates all spaces in the user input and re-inserts proper word boundaries. Unfortunately, such an approach often exacerbates the word spacing quality for user input, which has few or no spacing errors; such is the case, because a perfect WS model does not exist. In this paper, we propose a novel WS method that takes into consideration the initial word spacing information of the user input. Our method generates a better output than the original user input, even if the user input has few spacing errors. Moreover, the proposed method significantly outperforms a state-of-the-art Korean WS model when the user input initially contains less than 10% spacing errors, and performs comparably for cases containing more spacing errors. We believe that the proposed method will be a very practical pre-processing module."
P09-1003,A Comparative Study on Generalization of Semantic Roles in {F}rame{N}et,2009,20,20,3,0,9338,yuichiroh matsubayashi,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"A number of studies have presented machine-learning approaches to semantic role labeling with availability of corpora such as FrameNet and PropBank. These corpora define the semantic roles of predicates for each frame independently. Thus, it is crucial for the machine-learning approach to generalize semantic roles across different frames, and to increase the size of training instances. This paper explores several criteria for generalizing semantic roles in FrameNet: role hierarchy, human-understandable descriptors of roles, semantic types of filler phrases, and mappings from FrameNet roles to thematic roles of VerbNet. We also propose feature functions that naturally combine and weight these criteria, based on the training data. The experimental result of the role classification shows 19.16% and 7.42% improvements in error reduction rate and macro-averaged F1 score, respectively. We also provide in-depth analyses of the proposed criteria."
P09-1054,Stochastic Gradient Descent Training for {L}1-regularized Log-linear Models with Cumulative Penalty,2009,26,143,2,1,338,yoshimasa tsuruoka,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Stochastic gradient descent (SGD) uses approximate gradients estimated from subsets of the training data and updates the parameters in an online fashion. This learning framework is attractive because it often requires much less training time in practice than batch training algorithms. However, L1-regularization, which is becoming popular in natural language processing because of its ability to produce compact models, cannot be efficiently applied in SGD training, due to the large dimensions of feature vectors and the fluctuations of approximate gradients. We present a simple method to solve these problems by penalizing the weights according to cumulative values for L1 penalty. We evaluate the effectiveness of our method in three applications: text chunking, named entity recognition, and part-of-speech tagging. Experimental results demonstrate that our method can produce compact and accurate models much more quickly than a state-of-the-art quasi-Newton method for L1-regularized loglinear models."
P09-1102,Robust Approach to Abbreviating Terms: A Discriminative Latent Variable Model with Global Information,2009,20,13,3,1,3749,xu sun,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"The present paper describes a robust approach for abbreviating terms. First, in order to incorporate non-local information into abbreviation generation tasks, we present both implicit and explicit solutions: the latent variable model, or alternatively, the label encoding approach with global information. Although the two approaches compete with one another, we demonstrate that these approaches are also complementary. By combining these two approaches, experiments revealed that the proposed abbreviation generator achieved the best results for both the Chinese and English languages. Moreover, we directly apply our generator to perform a very different task from tradition, the abbreviation recognition. Experiments revealed that the proposed model worked robustly, and outperformed five out of six state-of-the-art abbreviation recognizers."
N09-2025,Learning Combination Features with {L}1 Regularization,2009,9,11,2,1,44082,daisuke okanohara,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"When linear classifiers cannot successfully classify data, we often add combination features, which are products of several original features. The searching for effective combination features, namely feature engineering, requires domain-specific knowledge and hard work. We present herein an efficient algorithm for learning an L1 regularized logistic regression model with combination features. We propose to use the grafting algorithm with efficient computation of gradients. This enables us to find optimal weights efficiently without enumerating all combination features. By using L1 regularization, the result we obtain is very compact and achieves very efficient inference. In experiments with NLP tasks, we show that the proposed method can extract effective combination features, and achieve high performance with very few features."
N09-2031,Extracting Bilingual Dictionary from Comparable Corpora with Dependency Heterogeneity,2009,15,41,2,1,44132,kun yu,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,This paper proposes an approach for bilingual dictionary extraction from comparable corpora. The proposed approach is based on the observation that a word and its translation share similar dependency relations. Experimental results using 250 randomly selected translation pairs prove that the proposed approach significantly outperforms the traditional context-based approach that uses bag-of-words around translation candidates.
N09-1007,A Discriminative Latent Variable {C}hinese Segmenter with Hybrid Word/Character Information,2009,21,39,5,1,3749,xu sun,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Conventional approaches to Chinese word segmentation treat the problem as a character-based tagging task. Recently, semi-Markov models have been applied to the problem, incorporating features based on complete words. In this paper, we propose an alternative, a latent variable model, which uses hybrid information based on both word sequences and character sequences. We argue that the use of latent variables can help capture long range dependencies and improve the recall on segmenting long words, e.g., named-entities. Experimental results show that this is indeed the case. With this improvement, evaluations on the data of the second SIGHAN CWS bakeoff show that our system is competitive with the best ones in the literature."
N09-1048,Semi-Supervised Lexicon Mining from Parenthetical Expressions in Monolingual Web Pages,2009,21,7,3,1,6319,xianchao wu,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper presents a semi-supervised learning framework for mining Chinese-English lexicons from large amount of Chinese Web pages. The issue is motivated by the observation that many Chinese neologisms are accompanied by their English translations in the form of parenthesis. We classify parenthetical translations into bilingual abbreviations, transliterations, and translations. A frequency-based term recognition approach is applied for extracting bilingual abbreviations. A self-training algorithm is proposed for mining transliteration and translation lexicons. In which, we employ available lexicons in terms of morpheme levels, i.e., phoneme correspondences in transliteration and grapheme (e.g., suffix, stem, and prefix) correspondences in translation. The experimental results verified the effectiveness of our approaches."
J09-4003,{O}bituaries: Hozumi {T}anaka,2009,-1,-1,3,0,1468,timothy baldwin,Computational Linguistics,0,None
E09-1088,Sequential Labeling with Latent Variables: An Exact Inference Algorithm and its Efficient Approximation,2009,17,15,2,1,3749,xu sun,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"Latent conditional models have become popular recently in both natural language processing and vision processing communities. However, establishing an effective and efficient inference method on latent conditional models remains a question. In this paper, we describe the latent-dynamic inference (LDI), which is able to produce the optimal label sequence on latent conditional models by using efficient search strategy and dynamic programming. Furthermore, we describe a straightforward solution on approximating the LDI, and show that the approximated LDI performs as well as the exact LDI, while the speed is much faster. Our experiments demonstrate that the proposed inference algorithm outperforms existing inference methods on a variety of natural language processing tasks."
E09-1090,Fast Full Parsing by Linear-Chain Conditional Random Fields,2009,25,37,2,1,338,yoshimasa tsuruoka,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,This paper presents a chunking-based discriminative approach to full parsing. We convert the task of full parsing into a series of chunking tasks and apply a conditional random field (CRF) model to each level of chunking. The probability of an entire parse tree is computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depth-first search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser.
D09-1013,A Rich Feature Vector for Protein-Protein Interaction Extraction from Multiple Corpora,2009,28,64,4,1,3222,makoto miwa,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Because of the importance of protein-protein interaction (PPI) extraction from text, many corpora have been proposed with slightly differing definitions of proteins and PPI. Since no single corpus is large enough to saturate a machine learning system, it is necessary to learn from multiple different corpora. In this paper, we propose a solution to this challenge. We designed a rich feature vector, and we applied a support vector machine modified for corpus weighting (SVM-CW) to complete the task of multiple corpora PPI extraction. The rich feature vector, made from multiple useful kernels, is used to express the important information for PPI extraction, and the system with our feature vector was shown to be both faster and more accurate than the original kernel-based system, even when using just a single corpus. SVM-CW learns from one corpus, while using other corpora for support. SVM-CW is simple, but it is more effective than other methods that have been successfully applied to other NLP tasks earlier. With the feature vector and SVM-CW, our system achieved the best performance among all state-of-the-art PPI extraction systems reported so far."
D09-1121,Descriptive and Empirical Approaches to Capturing Underlying Dependencies among Parsing Errors,2009,9,5,3,1,38278,tadayoshi hara,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we provide descriptive and empirical approaches to effectively extracting underlying dependencies among parsing errors. In the descriptive approach, we define some combinations of error patterns and extract them from given errors. In the empirical approach, on the other hand, we re-parse a sentence with a target error corrected and observe errors corrected together. Experiments on an HPSG parser show that each of these approaches can clarify the dependencies among individual errors from each point of view. Moreover, the comparison between the results of the two approaches shows that combining these approaches can achieve a more detailed error analysis."
D09-1138,Supervised Learning of a Probabilistic Lexicon of Verb Semantic Classes,2009,26,3,2,1,5928,yusuke miyao,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"The work presented in this paper explores a supervised method for learning a probabilistic model of a lexicon of VerbNet classes. We intend for the probabilistic model to provide a probability distribution of verb-class associations, over known and unknown verbs, including polysemous words. In our approach, training instances are obtained from an existing lexicon and/or from an annotated corpus, while the features, which represent syntactic frames, semantic similarity, and selectional preferences, are extracted from unannotated corpora. Our model is evaluated in type-level verb classification tasks: we measure the prediction accuracy of VerbNet classes for unknown verbs, and also measure the dissimilarity between the learned and observed probability distributions. We empirically compare several settings for model learning, while we vary the use of features, source corpora for feature extraction, and disam-biguated corpora. In the task of verb classification into all VerbNet classes, our best model achieved a 10.69% error reduction in the classification accuracy, over the previously proposed model."
D09-1157,Classifying Relations for Biomedical Named Entity Disambiguation,2009,36,4,2,0.714286,46478,xinglong wang,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Named entity disambiguation concerns linking a potentially ambiguous mention of named entity in text to an unambiguous identifier in a standard database. One approach to this task is supervised classification. However, the availability of training data is often limited, and the available data sets tend to be imbalanced and, in some cases, heterogeneous. We propose a new method that distinguishes a named entity by finding the informative keywords in its surrounding context, and then trains a model to predict whether each keyword indicates the semantic class of the entity. While maintaining a comparable performance to supervised classification, this method avoids using expensive manually annotated data for each new domain, and thus achieves better portability."
2009.mtsummit-posters.26,Bilingual Dictionary Extraction from {W}ikipedia,2009,-1,-1,2,1,44132,kun yu,Proceedings of Machine Translation Summit XII: Posters,0,None
2009.iwslt-evaluation.15,The {UOT} system,2009,-1,-1,5,1,6319,xianchao wu,Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"We present the UOT Machine Translation System that was used in the IWSLT-09 evaluation campaign. This year, we participated in the BTEC track for Chinese-to-English translation. Our system is based on a string-to-tree framework. To integrate deep syntactic information, we propose the use of parse trees and semantic dependencies on English sentences described respectively by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets."
W08-1305,Parser Evaluation Across Frameworks without Format Conversion,2008,8,2,4,0,47752,wai tam,Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,0,"In the area of parser evaluation, formats like GR and SD which are based on dependencies, the simplest representation of syntactic information, are proposed as framework-independent metrics for parser evaluation. The assumption behind these proposals is that the simplicity of dependencies would make conversion from syntactic structures and semantic representations used in other formalisms to GR/SD a easy job. But (Miyao et al., 2007) reports that even conversion between these two formats is not easy at all. Not to mention that the 80% success rate of conversion is not meaningful for parsers that boast 90% accuracy. In this paper, we make an attempt at evaluation across frameworks without format conversion. This is achieved by generating a list of names of phenomena with each parse. These names of phenomena are matched against the phenomena given in the gold standard. The number of matches found is used for evaluating the parser that produces the parses. The evaluation method is more effective than evaluation methods which involve format conversion because the generation of names of phenomena from the output of a parser loaded is done by a recognizer that has a 100% success rate of recognizing a phenomenon illustrated by a sentence. The success rate is made possible by the reuse of native codes: codes used for writing the parser and rules of the grammar loaded into the parser."
W08-0605,Accelerating the Annotation of Sparse Named Entities by Dynamic Sentence Selection,2008,21,18,2,1,338,yoshimasa tsuruoka,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"This paper presents an active learning-like framework for reducing the human effort for making named entity annotations in a corpus. In this framework, the annotation work is performed as an iterative and interactive process between the human annotator and a probabilistic named entity tagger. At each iteration, sentences that are most likely to contain named entities of the target category are selected by the probabilistic tagger and presented to the annotator. This iterative annotation process is repeated until the estimated coverage reaches the desired level. Unlike active learning approaches, our framework produces a named entity corpus that is free from the sampling bias introduced by the active strategy. We evaluated our framework by simulating the annotation process using two named entity corpora and show that our approach could drastically reduce the number of sentences to be annotated when applied to sparse named entities."
W08-0613,Prediction of Protein Sub-cellular Localization using Information from Texts and Sequences.,2008,3,0,9,0,40991,hongwoo chun,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,This paper presents a novel prediction approach for protein sub-cellular localization. We have incorporated text and sequence-based approaches.
W08-0627,Raising the Compatibility of Heterogeneous Annotations: A Case Study on,2008,2,0,5,0,3524,yue wang,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"While there are several corpora which claim to have annotations for protein references, the heterogeneity between the annotations is recognized as an obstacle to develop expensive resources in a synergistic way. Here we present a series of experimental results which show the differences of protein mention annotations made to two corpora, GENIA and AImed."
W08-0504,Evaluating the Effects of Treebank Size in a Practical Application for Parsing,2008,14,2,4,1,6910,kenji sagae,"Software Engineering, Testing, and Quality Assurance for Natural Language Processing",0,"Natural language processing modules such as part-of-speech taggers, named-entity recognizers and syntactic parsers are commonly evaluated in isolation, under the assumption that artificial evaluation metrics for individual parts are predictive of practical performance of more complex language technology systems that perform practical tasks. Although this is an important issue in the design and engineering of systems that use natural language input, it is often unclear how the accuracy of an end-user application is affected by parameters that affect individual NLP modules. We explore this issue in the context of a specific task by examining the relationship between the accuracy of a syntactic parser and the overall performance of an information extraction system for biomedical text that includes the parser as one of its components. We present an empirical investigation of the relationship between factors that affect the accuracy of syntactic analysis, and how the difference in parse accuracy affects the overall system."
P08-1006,Task-oriented Evaluation of Syntactic Parsers and Their Representations,2008,41,71,5,1,5928,yusuke miyao,Proceedings of ACL-08: HLT,1,"This paper presents a comparative evaluation of several state-of-the-art English parsers based on different frameworks. Our approach is to measure the impact of each parser when it is used as a component of an information extraction system that performs protein-protein interaction (PPI) identification in biomedical papers. We evaluate eight parsers (based on dependency parsing, phrase structure parsing, or deep parsing) using five different parse representations. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data."
nguyen-etal-2008-challenges,Challenges in Pronoun Resolution System for Biomedical Text,2008,9,8,3,1,1934,ngan nguyen,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents our findings on the feasibility of doing pronoun resolution for biomedical texts, in comparison with conducting pronoun resolution for the newswire domain. In our experiments, we built a simple machine learning-based pronoun resolution system, and evaluated the system on three different corpora: MUC, ACE, and GENIA. Comparative statistics not only reveal the noticeable issues in constructing an effective pronoun resolution system for a new domain, but also provides a comprehensive view of those corpora often used for this task."
tateisi-etal-2008-genia,{GENIA}-{GR}: a Grammatical Relation Corpus for Parser Evaluation in the Biomedical Domain,2008,21,5,4,1,35318,yuka tateisi,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We report the construction of a corpus for parser evaluation in the biomedical domain. A 50-abstract subset (492 sentences) of the GENIA corpus (Kim et al., 2003) is annotated with labeled head-dependent relations using the grammatical relations (GR) evaluation scheme (Carroll et al., 1998) ,which has been used for parser evaluation in the newswire domain."
tsunakawa-etal-2008-building-bilingual,Building Bilingual Lexicons using Lexical Translation Probabilities via Pivot Languages,2008,15,13,3,0,37902,takashi tsunakawa,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper proposes a method of increasing the size of a bilingual lexicon obtained from two other bilingual lexicons via a pivot language. When we apply this approach, there are two main challenges, ÂambiguityÂ and ÂmismatchÂ of terms; we target the latter problem by improving the utilization ratio of the bilingual lexicons. Given two bilingual lexicons between language pairs Lf-Lp and Lp-Le, we compute lexical translation probabilities of word pairs by using a statistical word-alignment model, and term decomposition/composition techniques. We compare three approaches to generate the bilingual lexicon: Âexact mergingÂ, Âword-based mergingÂ, and our proposed Âalignment-based mergingÂ. In our method, we combine lexical translation probabilities and a simple language model for estimating the probabilities of translation pairs. The experimental results show that our method could drastically improve the number of translation terms compared to the two methods mentioned above. Additionally, we evaluated and discussed the quality of the translation outputs."
saetre-etal-2008-connecting,Connecting Text Mining and Pathways using the {P}ath{T}ext Resource,2008,10,0,10,1,32381,rune saetre,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Many systems have been developed in the past few years to assist researchers in the discovery of knowledge published as English text, for example in the PubMed database. At the same time, higher level collective knowledge is often published using a graphical notation representing all the entities in a pathway and their interactions. We believe that these pathway visualizations could serve as an effective user interface for knowledge discovery if they can be linked to the text in publications. Since the graphical elements in a Pathway are of a very different nature than their corresponding descriptions in English text, we developed a prototype system called PathText. The goal of PathText is to serve as a bridge between these two different representations. In this paper, we first describe the overall architecture and the interfaces of the PathText system, and then provide some details about the core Text Mining components."
J08-1002,Feature Forest Models for Probabilistic {HPSG} Parsing,2008,82,171,2,1,5928,yusuke miyao,Computational Linguistics,0,"Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures. This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into sub-structures under the assumption of statistical independence among sub-structures. For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules. These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures.n n This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures. The feature forest model provides a method for probabilistic modeling without the independence assumption when probabilistic events are represented with feature forests. Feature forests are generic data structures that represent ambiguous trees in a packed forest structure. Feature forest models are maximum entropy models defined over feature forests. A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests. Thus probabilistic modeling of any data structures is possible when they are represented by feature forests.n n This article also describes methods for representing HPSG syntactic structures and predicate-argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed."
I08-2122,Towards Data and Goal Oriented Analysis: Tool Inter-operability and Combinatorial Comparison,2008,8,2,9,1,8106,yoshinobu kano,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"Recently, NLP researches have advanced using F-scores, precisions, and recalls with gold standard data as evaluation measures. However, such evaluations cannot capture the different behaviors of varying NLP tools or the different behaviors of a NLP tool that depends on the data and domain in which it works. Because an increasing number of tools are available nowadays, it has become increasingly important to grasp these behavioral differences, in order to select a suitable set of tools, which forms a complex workflow for a specific purpose. In order to observe such differences, we need to integrate available combinations of tools into a workflow and to compare the combinatorial results. Although generic frameworks like UIMA (Unstructured Information Management Architecture) provide interoperability to solve this problem, the solution they provide is only partial. In order for truly interoperable toolkits to become a reality, we also need sharable and comparable type systems with an automatic combinatorial comparison generator, which would allow systematic comparisons of available tools. In this paper, we describe such an environment, which we developed based on UIMA, and we show its feasibility through an example of a protein-protein interaction (PPI) extraction system."
I08-2127,A Discriminative Approach to {J}apanese Abbreviation Extraction,2008,15,7,3,1,4956,naoaki okazaki,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"This paper addresses the difficulties in recognizing Japanese abbreviations through the use of previous approaches, examining actual usages of parenthetical expressions in newspaper articles. In order to bridge the gap between Japanese abbreviations and their full forms, we present a discriminative approach to abbreviation recognition. More specifically, we formalize the abbreviation recognition task as a binary classification problem in which a classifier determines a positive (abbreviation) or negative (nonabbreviation) class, given a candidate of abbreviation definition. The proposed method achieved 95.7% accuracy, 90.0% precision, and 87.6% recall on the evaluation corpus containing 7,887 (1,430 abbreviations and 6,457 non-abbreviation) instances of parenthetical expressions."
I08-1060,Bilingual Synonym Identification with Spelling Variations,2008,21,3,2,0,37902,takashi tsunakawa,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"This paper proposes a method for identifying synonymous relations in a bilingual lexicon, which is a set of translation-equivalent term pairs. We train a classifier for identifying those synonymous relations by using spelling variations as main clues. We compared two approaches: the direct identification of bilingual synonym pairs, and the merger of two monolingual synonyms. We showed that our approach achieves a high pair-wise precision and recall, and outperforms the baseline method."
D08-1047,A Discriminative Candidate Generator for String Transformations,2008,26,27,4,1,4956,naoaki okazaki,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"String transformation, which maps a source string s into its desirable form t*, is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1-regularized logistic regression model. We also propose a procedure to generate negative instances that affect the decision boundary of the model. The advantage of this approach is that candidate strings can be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations."
C08-2011,Word Sense Disambiguation for All Words using Tree-Structured Conditional Random Fields,2008,13,2,3,1,42725,jun hatori,Coling 2008: Companion volume: Posters,0,"We propose a supervised word sense disambiguation (WSD) method using tree-structured conditional random fields (TCRFs). By applying TCRFs to a sentence described as a dependency tree structure, we conduct WSD as a labeling problem on tree structures. To incorporate dependencies between word senses, we introduce a set of features on tree edges, in combination with coarse-grained tagsets, and show that these contribute to an improvement in WSD accuracy. We also show that the tree-structured model outperforms the linear-chain model. Experiments on the SENSEVAL-3 data set show that our TCRF model performs comparably with state-of-the-art WSD systems."
C08-2016,Exact Inference for Multi-label Classification using Sparse Graphical Models,2008,12,1,2,1,5928,yusuke miyao,Coling 2008: Companion volume: Posters,0,"This paper describes a parameter estimation method for multi-label classification that does not rely on approximate inference. It is known that multi-label classification involving label correlation features is intractable, because the graphical model for this problem is a complete graph. Our solution is to exploit the sparsity of features, and express a model structure for each object by using a sparse graph. We can thereby apply the junction tree algorithm, allowing for efficient exact inference on sparse graphs. Experiments on three data sets for text categorization demonstrated that our method increases the accuracy for text categorization with a reasonable cost."
C08-2032,Building a Bilingual Lexicon Using Phrase-based Statistical Machine Translation via a Pivot Language,2008,12,7,3,0,37902,takashi tsunakawa,Coling 2008: Companion volume: Posters,0,"This paper proposes a novel method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT). Given two bilingual lexicons between language pairs Lf xe2x80x93Lp and Lpxe2x80x93Le, we assume these lexicons as parallel corpora. Then, we merge the extracted two phrase tables into one phrase table between Lf and Le. Finally, we construct a phrase-based SMT system for translating the terms in the lexicon Lf xe2x80x93Lp into terms of Le and, obtain a new lexicon Lf xe2x80x93Le. In our experiments with Chinese-English and JapaneseEnglish lexicons, our system could cover 72.8% of Chinese terms and drastically improve the utilization ratio."
C08-1069,Comparative Parser Performance Analysis across Grammar Frameworks through Automatic Tree Conversion using Synchronous Grammars,2008,17,15,2,1,32672,takuya matsuzaki,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a methodology for the comparative performance analysis of the parsers developed for different grammar frameworks. For such a comparison, we need a common representation format of the parsing results since the representation of the parsing results depends on the grammar frameworks; hence they are not directly comparable to each other. We first convert the parsing result to a shallow CFG analysis by using an automatic tree converter based on synchronous grammars. The use of such a shallow representation as a common format has the advantage of reduced noise introduced by the conversion in comparison with the noise produced by the conversion to deeper representations. We compared an HPSG parser with several CFG parsers in our experiment and found that meaningful differences among the parsers' performance can still be observed by such a shallow representation."
C08-1083,A Discriminative Alignment Model for Abbreviation Recognition,2008,23,13,3,1,4956,naoaki okazaki,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a discriminative alignment model for extracting abbreviations and their full forms appearing in actual text. The task of abbreviation recognition is formalized as a sequential alignment problem, which finds the optimal alignment (origins of abbreviation letters) between two strings (abbreviation and full form). We design a large amount of finegrained features that directly express the events where letters produce or do not produce abbreviations. We obtain the optimal combination of features on an aligned abbreviation corpus by using the maximum entropy framework. The experimental results show the usefulness of the alignment model and corpus for improving abbreviation recognition."
C08-1095,Shift-Reduce Dependency {DAG} Parsing,2008,18,61,2,1,6910,kenji sagae,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Most data-driven dependency parsing approaches assume that sentence structure is represented as trees. Although trees have several desirable properties from both computational and linguistic perspectives, the structure of linguistic phenomena that goes beyond shallow syntax often cannot be fully captured by tree representations. We present a parsing approach that is nearly as simple as current data-driven transition-based dependency parsing frameworks, but outputs directed acyclic graphs (DAGs). We demonstrate the benefits of DAG parsing in two experiments where its advantages over dependency tree parsing can be clearly observed: predicate-argument analysis of English and syntactic analysis of Danish with a representation that includes long-distance dependencies and anaphoric reference links."
C08-1106,Modeling Latent-Dynamic in Shallow Parsing: A Latent Conditional Model with Improved Inference,2008,30,48,5,1,3749,xu sun,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Shallow parsing is one of many NLP tasks that can be reduced to a sequence labeling problem. In this paper we show that the latent-dynamics (i.e., hidden substructure of shallow phrases) constitutes a problem in shallow parsing, and we show that modeling this intermediate structure is useful. By analyzing the automatically learned hidden states, we show how the latent conditional model explicitly learn latent-dynamics. We propose in this paper the Best Label Path (BLP) inference algorithm, which is able to produce the most probable label sequence on latent conditional models. It outperforms two existing inference algorithms. With the BLP inference, the LDCRF model significantly outperforms CRF models on word features, and achieves comparable performance of the most successful shallow parsers on the CoNLL data when further using part-of-speech features."
2008.amta-papers.19,Improving {E}nglish-to-{C}hinese Translation for Technical Terms using Morphological Information,2008,-1,-1,4,1,6319,xianchao wu,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"The continuous emergence of new technical terms and the difficulty of keeping up with neologism in parallel corpora deteriorate the performance of statistical machine translation (SMT) systems. This paper explores the use of morphological information to improve English-to-Chinese translation for technical terms. To reduce the morpheme-level translation ambiguity, we group the morphemes into morpheme phrases and propose the use of domain information for translation candidate selection. In order to find correspondences of morpheme phrases between the source and target languages, we propose an algorithm to mine morpheme phrase translation pairs from a bilingual lexicon. We also build a cascaded translation model that dynamically shifts translation units from phrase level to word and morpheme phrase levels. The experimental results show the significant improvements over the current phrase-based SMT systems."
W07-2202,Evaluating Impact of Re-training a Lexical Disambiguation Model on Domain Adaptation of an {HPSG} Parser,2007,28,39,3,1,38278,tadayoshi hara,Proceedings of the Tenth International Conference on Parsing Technologies,0,"This paper describes an effective approach to adapting an HPSG parser trained on the Penn Treebank to a biomedical domain. In this approach, we train probabilities of lexical entry assignments to words in a target domain and then incorporate them into the original parser. Experimental results show that this method can obtain higher parsing accuracy than previous work on domain adaptation for parsing the same data. Moreover, the results show that the combination of the proposed method and the existing method achieves parsing accuracy that is as high as that of an HPSG parser retrained from scratch, but with much lower training cost. We also evaluated our method in the Brown corpus to show the portability of our approach in another domain."
W07-2208,A log-linear model with an n-gram reference distribution for accurate {HPSG} parsing,2007,32,22,4,1,3213,takashi ninomiya,Proceedings of the Tenth International Conference on Parsing Technologies,0,"This paper describes a log-linear model with an n-gram reference distribution for accurate probabilistic HPSG parsing. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and POS n-gram as defined in the CCG/HPSG/CDG supertagging. Recently, supertagging becomes well known to drastically improve the parsing accuracy and speed, but supertagging techniques were heuristically introduced, and hence the probabilistic models for parse trees were not well defined. We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG. This is the first model which properly incorporates the supertagging probabilities into parse tree's probabilistic model."
W07-1033,Reranking for Biomedical Named-Entity Recognition,2007,19,30,2,1,47023,kazuhiro yoshida,"Biological, translational, and clinical language processing",0,"This paper investigates improvement of automatic biomedical named-entity recognition by applying a reranking method to the COLING 2004 JNLPBA shared task of bioentity recognition. Our system has a common reranking architecture that consists of a pipeline of two statistical classifiers which are based on log-linear models. The architecture enables the reranker to take advantage of features which are globally dependent on the label sequences, and features from the labels of other sentences than the target sentence. The experimental results show that our system achieves the labeling accuracies that are comparable to the best performance reported for the same task, thanks to the 1.55 points of F-score improvement by the reranker."
P07-1010,A discriminative language model with pseudo-negative samples,2007,14,44,2,1,44082,daisuke okanohara,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"In this paper, we propose a novel discriminative language model, which can be applied quite generally. Compared to the well known N-gram language models, discriminative language models can achieve more accurate discrimination because they can employ overlapping features and nonlocal information. However, discriminative language models have been used only for re-ranking in specific applications because negative examples are not available. We propose sampling pseudo-negative examples taken from probabilistic language models. However, this approach requires prohibitive computational cost if we are dealing with quite a few features and training samples. We tackle the problem by estimating the latent information in sentences using a semiMarkov class model, and then extracting features from them. We also use an online margin-based algorithm with efficient kernel computation. Experimental results show that pseudo-negative examples can be treated as real negative examples and our model can classify these sentences correctly."
P07-1079,{HPSG} Parsing with Shallow Dependency Constraints,2007,23,38,3,1,6910,kenji sagae,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing to increase deep parsing accuracy, specifically by combining dependency and HPSG parsing. We show that by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for highaccuracy dependency parsing, while actually performing deep syntactic analysis. Our framework results in a 1.4% absolute improvement over a state-of-the-art approach for wide coverage HPSG parsing."
D07-1111,Dependency Parsing and Domain Adaptation with {LR} Models and Parser Ensembles,2007,29,212,2,1,6910,kenji sagae,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabilistic generalized LR dependency parsing. Parser actions are determined by a classifier, based on features that represent the current state of the parser. We apply this parsing framework to both tracks of the CoNLL 2007 shared task, in each case taking advantage of multiple models trained with different learners. In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme. In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-ofdomain training set, in a scheme similar to one iteration of co-training."
2007.mtsummit-papers.35,Development of a {J}apanese-{C}hinese machine translation system,2007,-1,-1,3,0,15925,hitoshi isahara,Proceedings of Machine Translation Summit XI: Papers,0,None
W06-3327,Subdomain adaptation of a {POS} tagger with a small corpus,2006,11,6,3,1,35318,yuka tateisi,Proceedings of the {HLT}-{NAACL} {B}io{NLP} Workshop on Linking Natural Language and Biology,0,"For the domain of biomedical research abstracts, two large corpora, namely GENIA (Kim et al 2003) and Penn BioIE (Kulik et al 2004) are available. Both are basically in human domain and the performance of systems trained on these corpora when they are applied to abstracts dealing with other species is unknown. In machine-learning-based systems, re-training the model with addition of corpora in the target domain has achieved promising results (e.g. Tsuruoka et al 2005, Lease et al 2005). In this paper, we compare two methods for adaptation of POS taggers trained for GENIA and Penn BioIE corpora to Drosophila melanogaster (fruit fly) domain."
W06-1619,Extremely Lexicalized Models for Accurate and Fast {HPSG} Parsing,2006,30,34,5,1,3213,takashi ninomiya,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"This paper describes an extremely lexicalized probabilistic model for fast and accurate HPSG parsing. In this model, the probabilities of parse trees are defined with only the probabilities of selecting lexical entries. The proposed model is very simple, and experiments revealed that the implemented parser runs around four times faster than the previous model and that the proposed model has a high accuracy comparable to that of the previous model for probabilistic HPSG, which is defined over phrase structures. We also developed a hybrid of our probabilistic model and the conventional phrase-structure-based model. The hybrid model is not only significantly faster but also significantly more accurate by two points of precision and recall compared to the previous model."
W06-1634,Automatic Construction of Predicate-argument Structure Patterns for Biomedical Information Extraction,2006,19,30,5,1,49766,akane yakushiji,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a method of automatically constructing information extraction patterns on predicate-argument structures (PASs) obtained by full parsing from a smaller training corpus. Because PASs represent generalized structures for syntactical variants, patterns on PASs are expected to be more generalized than those on surface words. In addition, patterns are divided into components to improve recall and we introduce a Support Vector Machine to learn a prediction model using pattern matching results. In this paper, we present experimental results and analyze them on how well protein-protein interactions were extracted from MEDLINE abstracts. The results demonstrated that our method improved accuracy compared to a machine learning approach using surface word/part-of-speech patterns."
P06-4005,An Intelligent Search Engine and {GUI}-based Efficient {MEDLINE} Search Tool Based on Deep Syntactic Parsing,2006,14,31,12,1,35319,tomoko ohta,Proceedings of the {COLING}/{ACL} 2006 Interactive Presentation Sessions,0,"We present a practical HPSG parser for English, an intelligent search engine to retrieve MEDLINE abstracts that represent biomedical events and an efficient MEDLINE search tool helping users to find information about biomedical entities such as genes, proteins, and the interactions between them."
P06-2091,Translating {HPSG}-Style Outputs of a Robust Parser into Typed Dynamic Logic,2006,13,2,4,0,49947,manabu sato,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic representations of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus. With its higher-order representations of contexts, TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner. The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics. The present implementation achieves high coverage, approximately 90%, for the real text of the Penn Treebank corpus."
P06-2109,Trimming {CFG} Parse Trees for Sentence Compression Using Machine Learning Approaches,2006,13,18,4,0,49953,yuya unno,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Sentence compression is a task of creating a short grammatical sentence by removing extraneous words or phrases from an original sentence while preserving its meaning. Existing methods learn statistics on trimming context-free grammar (CFG) rules. However, these methods sometimes eliminate the original meaning by incorrectly removing important parts of sentences, because trimming probabilities only depend on parents' and daughters' non-terminals in applied CFG rules. We apply a maximum entropy model to the above method. Our method can easily include various features, for example, other parts of a parse tree or words the sentences contain. We evaluated the method using manually compressed sentences and human judgments. We found that our method produced more grammatical and informative compressed sentences than other methods."
P06-1059,Improving the Scalability of Semi-{M}arkov Conditional Random Fields for Named Entity Recognition,2006,16,66,4,1,44082,daisuke okanohara,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents techniques to apply semi-CRFs to Named Entity Recognition tasks with a tractable computational cost. Our framework can handle an NER task that has long named entities and many labels which increase the computational cost. To reduce the computational cost, we propose two techniques: the first is the use of feature forests, which enables us to pack feature-equivalent states, and the second is the introduction of a filtering process which significantly reduces the number of candidate states. This framework allows us to use a rich set of features extracted from the chunk-based representation that can capture informative characteristics of entities. We also introduce a simple trick to transfer information about distant entities by embedding label information into non-entity labels. Experimental results show that our model achieves an F-score of 71.48% on the JNLPBA 2004 shared task without using any external resources or post-processing techniques."
P06-1128,Semantic Retrieval for the Accurate Identification of Relational Concepts in Massive Textbases,2006,18,83,7,1,5928,yusuke miyao,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts. Prior to retrieval, all sentences are annotated with predicate argument structures and ontological identifiers by applying a deep parser and a term recognizer. During the run time, user requests are converted into queries of region algebra on these annotations. Structural matching with pre-computed semantic annotations establishes the accurate and efficient retrieval of relational concepts. This framework was applied to a text retrieval system for MEDLINE. Experiments on the retrieval of biomedical correlations revealed that the cost is sufficiently small for real-time applications and that the retrieval precision is significantly improved."
ohta-etal-2006-linguistic,Linguistic and Biological Annotations of Biological Interaction Events,2006,19,2,5,1,35319,tomoko ohta,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,This paper discusses an augmentation of a corpus ofresearch abstracts in biomedical domain (the GENIA corpus) with two kinds of annotations: tree annotation and event annotation. The tree annotation identifies the linguistic structure that encodes the relations among entities. The event annotation reveals the semantic structure of the biological interaction events encoded in the text. With these annotations we aim to provide a link between the clue and the target of biological event information extraction.
W05-1510,Probabilistic Models for Disambiguation of an {HPSG}-Based Chart Generator,2005,22,48,3,0,50782,hiroko nakanishi,Proceedings of the Ninth International Workshop on Parsing Technology,0,"We describe probabilistic models for a chart generator based on HPSG. Within the research field of parsing with lexicalized grammars such as HPSG, recent developments have achieved efficient estimation of probabilistic models and high-speed parsing guided by probabilistic models. The focus of this paper is to show that two essential techniques -- model estimation on packed parse forests and beam search during parsing -- are successfully exported to the task of natural language generation. Additionally, we report empirical evaluation of the performance of several disambiguation models and how the performance changes according to the feature set used in the models and the size of training data."
W05-1511,"Efficacy of Beam Thresholding, Unification Filtering and Hybrid Parsing in Probabilistic {HPSG} Parsing",2005,49,20,4,1,3213,takashi ninomiya,Proceedings of the Ninth International Workshop on Parsing Technology,0,"We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The contributions of the large constituent inhibition and global thresholding were not significant, while the quick check and chunk parser greatly contributed to total parsing performance. The precision, recall and average parsing time for the Penn treebank (Section 23) were 87.85%, 86.85%, and 360 ms, respectively."
W05-1514,Chunk Parsing Revisited,2005,18,31,2,1,338,yoshimasa tsuruoka,Proceedings of the Ninth International Workshop on Parsing Technology,0,"Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use. In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple sliding-window method and maximum entropy classifiers for phrase recognition in each level of chunking. Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/sentence). We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers, and show that the search method can further improve the parsing accuracy."
W05-1304,A Machine Learning Approach to Acronym Generation,2005,8,15,3,1,338,yoshimasa tsuruoka,"Proceedings of the {ACL}-{ISMB} Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics",0,"This paper presents a machine learning approach to acronym generation. We formalize the generation process as a sequence labeling problem on the letters in the definition (expanded form) so that a variety of Markov modeling approaches can be applied to this task. To construct the data for training and testing, we extracted acronym-definition pairs from MEDLINE abstracts and manually annotated each pair with positional information about the letters in the acronym. We have built an MEMM-based tagger using this training data set and evaluated the performance of acronym generation. Experimental results show that our machine learning method gives significantly better performance than that achieved by the standard heuristic rule for acronym generation and enables us to obtain multiple candidate acronyms together with their likelihoods represented in probability values."
P05-1010,Probabilistic {CFG} with Latent Annotations,2005,16,236,3,1,32672,takuya matsuzaki,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine-grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences xe2x89xa4 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection."
P05-1011,Probabilistic Disambiguation Models for Wide-Coverage {HPSG} Parsing,2005,26,104,2,1,5928,yusuke miyao,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"This paper reports the development of log-linear models for the disambiguation in wide-coverage HPSG parsing. The estimation of log-linear models requires high computational cost, especially with wide-coverage grammars. Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Tree-bank. A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences."
I05-2038,Syntax Annotation for the {GENIA} Corpus,2005,10,108,4,1,35318,yuka tateisi,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"Linguistically annotated corpus based on texts in biomedical domain has been constructed to tune natural language processing (NLP) tools for biotextmining. As the focus of information extraction is shifting from nominal information such as named entity to verbal information such as function and interaction of substances, application of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sentences is in demand. A subset of the GENIA corpus consisting of 500 MEDLINE abstracts has been annotated for syntactic structure in an XMLbased format based on Penn Treebank II (PTB) scheme. Inter-annotator agreement test indicated that the writing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation, and that annotation can be stably done by linguists without much knowledge of biology with appropriate guidelines regarding to linguistic phenomena particular to scientific texts."
I05-1018,Adapting a Probabilistic Disambiguation Model of an {HPSG} Parser to a New Domain,2005,12,50,3,1,38278,tadayoshi hara,Second International Joint Conference on Natural Language Processing: Full Papers,0,"This paper describes a method of adapting a domain-independent HPSG parser to a biomedical domain. Without modifying the grammar and the probabilistic model of the original HPSG parser, we develop a log-linear model with additional features on a treebank of the biomedical domain. Since the treebank of the target domain is limited, we need to exploit an original disambiguation model that was trained on a larger treebank. Our model incorporates the original model as a reference probabilistic distribution. The experimental results for our model trained with a small amount of a treebank demonstrated an improvement in parsing accuracy."
I05-1028,Assigning Polarity Scores to Reviews Using Machine Learning Techniques,2005,24,21,2,1,44082,daisuke okanohara,Second International Joint Conference on Natural Language Processing: Full Papers,0,We propose a novel type of document classification task that quantifies how much a given document (review) appreciates the target object using not binary polarity (good or bad) but a continuous measure called sentiment polarity score (sp-score). An sp-score gives a very concise summary of a review and provides more information than binary classification. The difficulty of this task lies in the quantification of polarity. In this paper we use support vector regression (SVR) to tackle the problem. Experiments on book reviews with five-point scales show that SVR outperforms a multi-class classification method using support vector machines and the results are close to human performance.
H05-1059,Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data,2005,17,231,2,1,338,yoshimasa tsuruoka,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking. The algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time. We also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance to full bidirectional inference with significantly lower computational cost. Experimental results of part-of-speech tagging and text chunking show that the proposed bidirectional inference methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines."
W04-3314,Generalizing Subcategorization Frames Acquired from Corpora Using Lexicalized Grammars,2004,20,0,2,1,4617,naoki yoshinaga,Proceedings of the 7th International Workshop on Tree Adjoining Grammar and Related Formalisms,0,"This paper presents a method of improving the quality of subcategorization frames (SCFs) acquired from corpora in order to augment a lexicon of a lexicalized grammar. We first estimate a confidence value that a word can have each SCF, and create an SCF confidence-value vector for each word. Since the SCF confidence vectors obtained from the lexicon of the target grammar involve co-occurrence tendency among SCFs for words, we can improve the quality of the acquired SCFs by clustering vectors obtained from the acquired SCF lexicon and the lexicon of the target grammar. We apply our method to SCFs acquired from corpora by using a subset of the SCF lexicon of the XTAG English grammar. A comparison between the resulting SCF lexicon and the rest of the lexicon of the XTAG English grammar reveals that we can achieve higher precision and recall compared to naive frequency cut-off."
W04-3323,Context-free Approximation of {LTAG} towards {CFG} Filtering,2004,19,3,3,0,46715,kenta oouchida,Proceedings of the 7th International Workshop on Tree Adjoining Grammar and Related Formalisms,0,"We present a method to approximate a LTAG grammar by a CFG. A key process in the approximation method is finite enumeration of partial parse results that can be generated during parsing. We applied our method to the XTAG English grammar and LTAG grammars which are extracted from the Penn Treebank, and investigated characteristics of the obtained CFGs. We perform CFG filtering for LTAG by the obtained CFG. In the experiments, we describe that the obtained CFG is useful for CFG filtering for LTAG parser."
P04-3017,Finding Anchor Verbs for Biomedical {IE} Using Predicate-Argument Structures,2004,7,4,4,1,49766,akane yakushiji,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"For biomedical information extraction, most systems use syntactic patterns on verbs (anchor verbs) and their arguments. Anchor verbs can be selected by focusing on their arguments. We propose to use predicate-argument structures (PASs), which are outputs of a full parser, to obtain verbs and their arguments. In this paper, we evaluated PAS method by comparing it to a method using part of speech (POSs) pattern matching. POS patterns produced larger results with incorrect arguments, and the results will cause adverse effects on a phase selecting appropriate verbs."
tsujii-2004-thesaurus,"Thesaurus or Logical Ontology, Which do we Need for Mining Text?",2004,7,5,1,1,20171,junichi tsujii,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"It has been claimed that domain ontology is necessary not only for effective and efficient information sharing but also for information extraction and text mining. In particular, the need of common ontology for information sharing among different research communities has been recognized in bio-medical fields, and several domain ontologies are now being built (GO 2003). This is because of the sheer complexity of the semantic space and a huge number of concepts or terms used in these specific domains. We have been engaged in annotation of Medline abstracts in molecular biology (GENIA corpus: http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/) for 5 years and, in the process, we also developed our own ontology (GENIA ontology). However, there seem to be confusions on what kinds of ontology we really need and how it can be used for effective information management systems for bio-medical research. We argue in this paper that there are several different views of ontology and that, while logical ontology a la OWL would be useful, it may be neither practical nor possible to build a single large logical ontology of the domain. We would also like to claim that urgent issues we have to resolve are more concerned with construction of thesauri than logical ontology, i.e. lexical resources that treat convoluted nature of a mapping from linguistic forms to concepts. Although we need ontological consideration to construct thesauri, consistency and coherency of the whole system that logical ontology usually requires should not be the main concern."
tateisi-tsujii-2004-part,Part-of-Speech Annotation of Biology Research Abstracts,2004,6,41,2,1,35318,yuka tateisi,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"A part-of-speech (POS) tagged corpus was built on research abstracts in biomedical domain with the Penn Treebank scheme. As consistent annotation was difficult without domain-specific knowledge we made use of the existing term annotation of the GENIA corpus. A list of frequent terms annotated in the GENIA corpus was compiled and the POS of each constituent of those terms were determined with assistance from domain specialists. The POS of the terms in the list are pre-assigned, then a tagger assigns POS to remaining words preserving the pre-assigned POS, whose results are corrected by human annotators. We also modified the PTB scheme slightly. An inter-annotator agreement tested on new 50 abstracts was 98.5%. A POS tagger trained with the annotated abstracts was tested against a gold-standard set made from the interannotator agreement. The untrained tagger had the accuracy of 83.0%. Trained with 2000 annotated abstracts the accuracy rose to 98.2%. The 2000 annotated abstracts are publicly available."
C04-1204,Deep Linguistic Analysis for the Accurate Identification of Predicate-Argument Relations,2004,31,18,2,1,5928,yusuke miyao,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations. We could directly compare the output of HPSG parsing with PropBank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations."
2004.iwslt-evaluation.1,Overview of the {IWSLT} evaluation campaign,2004,25,56,6,0,51985,yasuhiro akiba,Proceedings of the First International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper gives an overview of the evaluation campaign results of the IWSLT04 1 workshop, which is organized by the C-STAR 2 consortium to investigate novel speech translation technologies and their evaluation. The objectives of this workshop is to provide a framework for the applicability validation of existing machine translation evaluation methodologies to evaluate speech translation technologies. The workshop also strives to find new directions in how to improve current methods."
W03-2416,Stretching {TEI}: Converting the {G}enia Corpus,2003,0,0,5,0,52611,tomaz erjavec,Proceedings of 4th International Workshop on Linguistically Interpreted Corpora ({LINC}-03) at {EACL} 2003,0,None
W03-1306,Boosting Precision and Recall of Dictionary-Based Protein Name Recognition,2003,17,78,2,1,338,yoshimasa tsuruoka,Proceedings of the {ACL} 2003 Workshop on Natural Language Processing in Biomedicine,0,"Dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches. However, dictionary based approaches have two serious problems: (1) a large number of false recognitions mainly caused by short names. (2) low recall due to spelling variation. In this paper, we tackle the former problem by using a machine learning method to filter out false positives. We also present an approximate string searching method to alleviate the latter problem. Experimental results using the GE-NIA corpus show that the filtering using a naive Bayes classifier greatly improves precision with slight loss of recall, resulting in a much better F-score."
W03-1313,Encoding Biomedical Resources in {TEI}: The Case of the {GENIA} Corpus,2003,3,7,5,0,52611,tomaz erjavec,Proceedings of the {ACL} 2003 Workshop on Natural Language Processing in Biomedicine,0,"It is well known that standardising the annotation of language resources significantly raises their potential, as it enables re-use and spurs the development of common technologies. Despite the fact that increasingly complex linguistic information is being added to biomedical texts, no standard solutions have so far been proposed for their encoding. This paper describes a standardised XML tagset (DTD) for annotated biomedical corpora and other resources, which is based on the Text Encoding Initiative Guidelines P4, a general and parameterisable standard for encoding language resources. We ground the discussion in the encoding of the GENIA corpus, which currently contains 2,000 abstracts taken from the MEDLINE database, and has almost 100,000 hand-annotated terms marked for semantic class from the accompanying ontology. The paper introduces GENIA and TEI and implements a TEI parametrisation and conversion for the GENIA corpus. A number of aspects of biomedical language are discussed, such as complex tokenisation, prevalence of contractions and complex terms, and the linkage and encoding of ontologies."
W03-1018,Evaluation and Extension of Maximum Entropy Models with Inequality Constraints,2003,13,81,2,0.952381,41587,junichi kazama,Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,0,"A maximum entropy (ME) model is usually estimated so that it conforms to equality constraints on feature expectations. However, the equality constraint is inappropriate for sparse and therefore unreliable features. This study explores an ME model with box-type inequality constraints, where the equality can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension."
W03-0401,A model of syntactic disambiguation based on lexicalized grammars,2003,22,7,2,1,5928,yusuke miyao,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"This paper presents a new approach to syntactic disambiguation based on lexicalized grammars. While existing disambiguation models decompose the probability of parsing results into that of primitive dependencies of two words, our model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars."
W03-0416,An efficient clustering algorithm for class-based language models,2003,11,11,3,1,32672,takuya matsuzaki,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"This paper defines a general form for class-based probabilistic language models and proposes an efficient algorithm for clustering based on this. Our evaluation experiments revealed that our method decreased computation time drastically, while retaining accuracy."
W03-0417,Training a Naive {B}ayes Classifier via the {EM} Algorithm with a Class Distribution Constraint,2003,21,28,2,1,338,yoshimasa tsuruoka,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"Combining a naive Bayes classifier with the EM algorithm is one of the promising approaches for making use of unlabeled data for disambiguation tasks when using local context features including word sense disambiguation and spelling correction. However, the use of unlabeled data via the basic EM algorithm often causes disastrous performance degradation instead of improving classification performance, resulting in poor classification performance on average. In this study, we introduce a class distribution constraint into the iteration process of the EM algorithm. This constraint keeps the class distribution of unlabeled data consistent with the class distribution estimated from labeled data, preventing the EM algorithm from converging into an undesirable state. Experimental results from using 26 confusion sets and a large amount of unlabeled data show that our proposed method for using unlabeled data considerably improves classification performance when the amount of labeled data is small."
P03-2033,A Debug Tool for Practical Grammar Development,2003,10,3,5,1,49766,akane yakushiji,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,"We have developed willex, a tool that helps grammar developers to work efficiently by using annotated corpora and recording parsing errors. Willex has two major new functions. First, it decreases ambiguity of the parsing results by comparing them to an annotated corpus and removing wrong partial results both automatically and manually. Second, willex accumulates parsing errors as data for the developers to clarify the defects of the grammar statistically. We applied willex to a large-scale HPSG-style grammar as an example."
P03-2036,Comparison between {CFG} Filtering Techniques for {LTAG} and {HPSG},2003,22,1,3,1,4617,naoki yoshinaga,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,An empirical comparison of CFG filtering techniques for LTAG and HPSG is presented. We demonstrate that an approximation of HPSG produces a more effective CFG filter than that of LTAG. We also investigate the reason for that difference.
P03-1038,Self-Organizing {M}arkov Models and Their Application to Part-of-Speech Tagging,2003,12,7,3,1,15849,jindong kim,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,This paper presents a method to develop a class of variable memory Markov models that have higher memory capacity than traditional (uniform memory) Markov models. The structure of the variable memory models is induced from a manually annotated corpus through a decision tree learning algorithm. A series of comparative experiments show the resulting models outperform uniform memory Markov models in a part-of-speech tagging task.
N03-2020,A Robust Retrieval Engine for Proximal and Structural Search,2003,5,6,5,0.952381,49887,katsuya masuda,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,"In the text retrieval area including XML and Region Algebra, many researchers pursued models for specifying what kinds of information should appear in specified structural positions and linear positions (Chinenyanga and Kushmerick, 2001; Wolff et al., 1999; Theobald and Weilkum, 2000; Clarke et al., 1995). The models attracted many researchers because they are considered to be basic frameworks for retrieving or extracting complex information like events. However, unlike IR by keyword-based search, their models are not robust, that is, they support only exact matching of queries, while we would like to know to what degree the contents in specified structural positions are relevant to those in the query even when the structure does not exactly match the query."
E03-1047,Lexicalized Grammar Acquisition,2003,8,7,3,1,5928,yusuke miyao,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,This paper presents a formalization of automatic grammar acquisition that is based on lexicalized grammar formalisms (e.g. LTAG and HPSG). We state the conditions for the consistent acquisition of a unique lexicalized grammar from an annotated corpus.
W02-2227,A Formal Proof of Strong Equivalence for a Grammar Conversion from {LTAG} to {HPSG}-style,2002,14,0,3,1,4617,naoki yoshinaga,Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+6),0,"This paper presents a sketch of a formal proof of strong equivalence,1 where both grammars generate equivalent parse results, between any LTAG (Lexicalized Tree Adjoining Grammar: Schabes, Abeille and Joshi (1988)) G and an HPSG (Head-Driven Phrase Structure Grammar: Pollard and Sag (1994))-style grammar converted from G by a grammar conversion (Yoshinaga and Miyao, 2001). Our proof theoretically justifies some applications of the grammar conversion that exploit the nature of strong equivalence (Yoshinaga et al., 2001b; Yoshinaga et al., 2001a), applications which contribute much to the developments of the two formalisms. In the past decades, LTAG and HPSG have received considerable attention as approaches to the formalization of natural languages in the field of computational linguistics. Discussion of the correspondences between the two formalisms has accompanied their development; that is, their linguistic relationships and differences have been investigated (Abeille, 1993; Kasper, 1998), as has conversion between two grammars in the two formalisms (Kasper et al., 1995; Tateisi et al., 1998; Becker and Lopez, 2000). These ongoing efforts have contributed greatly to the development of the two formalisms. Following this direction, in our earlier work (Yoshinaga and Miyao, 2001), we provided a method for converting grammars from LTAG to HPSG-style, which is the notion that we defined according to the computational device that underlies HPSG. We used the grammar conversion to obtain an HPSG-style grammar from LTAG (The XTAG Research Group, 2001), and then empirically showed strong equivalence between the LTAG and the obtained HPSG-style grammar for the sentences in the ATIS corpus (Marcus, Santorini and Marcinkiewicz, 1994). We exploited the nature of strong equivalence between the LTAG and the HPSG-style grammars to provide some applications such as sharing of existing resources between the two grammar formalisms (Yoshinaga et al., 2001b), a comparison of performance between parsers based on the two different formalisms (Yoshinaga et al., 2001a), and linguistic correspondence between the HPSG-style grammar and HPSG. As the most important result for the LTAG community, through the experiments of parsing within the above sentences, we showed that the empirical time complexity of an LTAG parser (Sarkar, 2000) is higher than that of an HPSG parser (Torisawa et al., 2000). This result is contrary to the general expectations from the viewpoint of the theoretical bound of worst time complexity, which is worth exploring further. However, the lack of the formal proof of strong equivalence restricts scope of the applications of our grammar conversion to grammars which are empirically attested the strong equivalence, and this prevents the applications from maximizing their true potential. In this paper we give a formal proof of strong equivalence between any LTAG G and an HPSG-style grammar converted from G by our grammar conversion in order to remove such restrictions on the applications."
W02-2232,Clustering for obtaining syntactic classes of words from automatically extracted {LTAG} grammars,2002,5,2,3,1,38278,tadayoshi hara,Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+6),0,"We propose a method for obtaining syntactic classes of words from a lexicalized tree adjoining grammar (LTAG: Schabes, Abeille and Joshi (1988)) automatically extracted from a corpus. Since elementary trees in LTAG grammars represent syntactic roles of a word, we can obtain syntactic classes by clustering words having the similar elementary trees. With our method, automatically extracted LTAG grammars will be arranged according to the syntactic classes of words, and the grammars can be improved from various points of view. For example, we can improve the coverage of the grammars by supplementing to a word the elementary trees of the syntactic class of the word. An LTAG grammar consists of elementary trees, which determine the position where the word can be put in a sentence, that is, an elementary tree corresponds to a certain syntactic role. Hence, a syntactic class of a word is represented as a set of elementary trees assigned to the word. Since the words of the same syntactic class are expected to have similar elementary trees, we can obtain syntactic classes by clustering words having similar sets of elementary trees. We applied our clustering algorithm to an LTAG grammar automatically extracted from sections 02xe2x80x9321 of the Penn Treebank (Marcus, Santorini and Marcinkiewicz (1994)), and investigated the obtained clusters with changing the number of clusters. We successfully obtained some of the clusters that correspond to certain syntactic classes. On the other hand, we could not obtain some clusters, such as the one for ditransitive verbs, and obtained the clusters that we could not associate clearly with syntactic classes. This is because our method was strongly affected by the difference in the number of words in each part-of-speech class. We concluded that, although our clustering method needs to be improved for practical use, it is effective to automatically obtain syntactic classes of words. The XTAG English grammar (The XTAG Research Group (1995)) is a handmade LTAG grammar which is arranged according to syntactic classes of words, xe2x80x9ctree families.xe2x80x9d Each tree family corresponds to a certain subcategorization frame, and determines elementary trees to be assigned to a word. Thanks to the tree families, the XTAG grammar is independent of a corpus. However, it needs considerable human effort to manually construct such a grammar. Automatically extracted LTAG grammars are superior to manually developed grammars in the sense that it takes much less costs to construct the grammars. Chiang (2000) and Xia (1999) gave the methods of automatically extracting LTAG grammars from a bracketed corpus. They first decided a trunk path of the tree structure of a bracketed sentence, and the relationship (substitution or adjunction) between the trunk and branches. The methods then cut off the branches of them according to the relationship. Because the sentences used for extraction are in real-world texts, extracted grammars are practical for natural language processing. However, automatically extracted grammars are not systematically arranged according to syntactic classes their anchors belong to, like the XTAG grammar. Because of this, automatically extracted grammars tend to be strongly dependent on the corpus. This limitation can be a critical disadvantage of such extracted grammars when the grammars are used for various applications. Then, we want to arrange an extracted grammar according to the syntactic classes of words, without loosing the benefit for the cost. Chen and Vijay-Shanker (2000) proposed the solution to the issue. To improve the coverage of an extracted LTAG grammar, they classified the extracted elementary trees according to the tree families in the XTAG English grammar. First, the method searches for a tree family that contains an elementary tree template of extracted elementary tree et. Next, the method collects other possible tree templates in the tree family and makes elementary trees with the anchor of et and the tree templates. By using tree families, the method can add only proper elementary trees that correspond to the syntactic class of anchors. Chen and Vijay-Shanker (2000) applied this method to an extracted LTAG grammar, and showed the improvement of the coverage of the grammar. Although their method showed the effectiveness of arranging a grammar according to syntactic classes, the method depends on"
W02-0301,Tuning support vector machines for biomedical named entity recognition,2002,22,207,4,0.952381,41587,junichi kazama,Proceedings of the {ACL}-02 Workshop on Natural Language Processing in the Biomedical Domain,0,"We explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition. To make the SVM training with the available largest corpus - the GENIA corpus - tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information. In addition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning. Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy. The proposed new features also contribute to improve the accuracy. We compare our SVM-based recognition system with a system using Maximum Entropy tagging method."
C02-2024,An Indexing Scheme for Typed Feature Structures,2002,9,6,3,1,3213,takashi ninomiya,{COLING} 2002: The 17th International Conference on Computational Linguistics: Project Notes,0,"This paper describes an indexing substrate for typed feature structures (ISTFS), which is an efficient retrieval engine for typed feature structures. Given a set of typed feature structures, the ISTFS efficiently retrieves its subset whose elements are unifiable or in a subsumption relation with a query feature structure. The efficiency of the ISTFS is achieved by calculating a unifiability checking table prior to retrieval and finding the best index paths dynamically."
C02-1083,A Methodology for Terminology-based Knowledge Acquisition and Integration,2002,10,14,4,0.517352,41496,hideki mima,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"In this paper we propose an integrated knowledge management system in which terminology-based knowledge acquisition, knowledge integration, and XML-based knowledge retrieval are combined using tag information and ontology management tools. The main objective of the system is to facilitate knowledge acquisition through query answering against XML-based documents in the domain of molecular biology. Our system integrates automatic term recognition, term variation management, context-based automatic term clustering, ontology-based inference, and intelligent tag information retrieval. Tag-based retrieval is implemented through interval operations, which prove to be a powerful means for textual mining and knowledge acquisition. The aim is to provide efficient access to heterogeneous biological textual data and databases, enabling users to integrate a wide range of textual and non-textual resources effortlessly."
C02-1100,Lenient Default Unification for Robust Processing within Unification Based Grammar Formalisms,2002,13,4,3,1,3213,takashi ninomiya,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper describes new default unification, lenient default unification. It works efficiently, and gives more informative results because it maximizes the amount of information in the result, while other default unification maximizes it in the default. We also describe robust processing within the framework of HPSG. We extract grammar rules from the results of robust parsing using lenient default unification. The results of a series of experiments show that parsing with the extracted rules works robustly, and the coverage of a manually-developed HPSG grammar for Penn Treebank was greatly increased with a little overgeneration."
W01-1510,Resource Sharing Amongst {HPSG} and {LTAG} Communities by a Method of Grammar Conversion between {FB}-{LTAG} and {HPSG},2001,23,0,4,1,4617,naoki yoshinaga,Proceedings of the {ACL} 2001 Workshop on Sharing Tools and Resources,0,"This paper describes the RenTAL system, which enables sharing resources in LTAG and HPSG formalisms by a method of grammar conversion from an FB-LTAG grammar to a strongly equivalent HPSG-style grammar. The system is applied to the latest version of the XTAG English grammar. Experimental results show that the obtained HPSG-style grammar successfully worked with an HPSG parser, and achieved a drastic speed-up against an LTAG parser. This system enables to share not only grammars and lexicons but also parsing techniques."
W00-1704,Building an Annotated Corpus in the Molecular-Biology Domain,2000,15,23,5,1,35318,yuka tateisi,Proceedings of the {COLING}-2000 Workshop on Semantic Annotation and Intelligent Content,0,"Corpus annotation is now a key topic for all areas of natural language processing (NLP) and information extraction (IE) which employ supervised learning. With the explosion of results in molecular-biology there is an increased need for IE to extract knowledge to support database building and to search intelligently for information in online journal collections. To support this we are building a corpus of annotated abstracts taken from National Library of Medicine's MEDLINE database. In this paper we report on this new corpus, its ontological basis, and our experience in designing the annotation scheme. Experimental results are shown for inter-annotator agreement and comments are made on methodological considerations."
W00-0904,Comparison between Tagged Corpora for the Named Entity Task,2000,19,25,3,0,51944,chikashi nobata,The Workshop on Comparing Corpora,0,"We present two measures for comparing corpora based on information theory statistics such as gain ratio as well as simple term-class frequency counts. We tested the predictions made by these measures about corpus difficulty in two domains --- news and molecular biology --- using the result of two well-used paradigms for NE, decision trees and HMMs and found that gain ratio was the more reliable predictor."
P00-1002,"Invited Talk: Generic {NLP} Technologies: Language, Knowledge and Information Extraction",2000,0,0,1,1,20171,junichi tsujii,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,None
P00-1034,Part-of-Speech Tagging Based on Hidden {M}arkov Model Assuming Joint Independence,2000,13,10,2,0,53188,sangzoo lee,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we present part-of-speech taggers based on hidden Markov models, which adopt a less strict Markov assumption to consider rich contexts. In models whose parameters are very specific like lexicalized ones, sparse-data problem is very serious and also conditional probabilities tend to be estimated unreliably. To overcome data-sparseness, a simplified version of the well-known back-off smoothing method is used. To mitigate unreliable estimation problem, our models assume joint independence instead of conditional independence because joint probabilities have the same degree of estimation reliability. In experiments for the Brown corpus, models with rich contexts achieve relatively high accuracy and some models assuming joint independence show better results than the corresponding HMMs."
P00-1044,Difficulty Indices for the Named Entity Task in {J}apanese,2000,2,3,3,0,51944,chikashi nobata,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"We propose indices to measure the difficulty of the named entity (NE) task by looking at test corpora, based on expressions inside and outside the NEs. These indices are intended to estimate the difficulty of each task without actually using an NE system and to be unbiased towards a specific system. The values of the indices are compared with the systems' performance in Japanese documents. We also discuss the difference between NE classes with the indices and show useful clues which will make it easier to recognize NEs."
P00-1048,"Hidden {M}arkov Model-Based {K}orean Part-of-Speech Tagging Considering High Agglutinativity, Word-Spacing, and Lexical Correlativity",2000,7,11,2,0,53188,sangzoo lee,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we present hidden Markov models for Korean part-of-speech tagging, which consider Korean characteristics such as high agglutinativity, word-spacing, and high lexical correlativity. In order ot consider rich information in contexts, the models adopt a less strict Markov assumption. In the models, sparse-data problem is very serious and their parameters tend to be estimated unreliably because they have a large number of parameters. To overcome sparse-data problem, our model uses a simplified version of the well-known back-off smoothing method. To mitigate unreliable estimation problem, our models assume joint independence instead of conditional independence because joint probabilities have the same degree of estimation reliability. Experimental results show that models with rich contexts perform even better than standard HMMs and that joint independent assumption is effective in some models."
C00-1030,Extracting the Names of Genes and Gene Products with a Hidden {M}arkov Model,2000,17,217,3,1,219,nigel collier,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,We report the results of a study into the use of a linear interpolating hidden Markov model (HMM) for the task of extracting technical terminology from MEDLINE abstracts and texts in the molecular-biology domain. This is the first stage in a system that will extract event information for automatically updating biology databases. We trained the HMM entirely with bigrams based on lexical and character features in a relatively small corpus of 100 MEDLINE abstracts that were marked-up by domain experts with term classes such as proteins and DNA. Using cross-validation methods we achieved an F-score of 0.73 and we examine the contribution made by each part of the interpolation model to overcoming data sparseness.
C00-1047,A Method of Measuring Term Representativeness - Baseline Method Using Co-occurrence Distribution,2000,16,24,3,0,54680,tom hisamitsu,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"This paper introduces a scheme, which we call the baseline method, to define a measure of term representativeness and measures defined by using the scheme. The representativeness of a term is measured by a normalized characteristic value defined for a set of all documents that contain the term. Normalization is done by comparing the original characteristic value with the characteristic value defined for a randomly chosen document set of the same size. The latter value is estimated by a baseline function obtained by random sampling and logarithmic linear approximation. We found that the distance between the word distribution in a document set and the word distribution in a whole corpus is an effective characteristic value to use for the baseline method. Measures defined by the baseline method have several advantages including that they can be used to compare the representativeness of two terms with very different frequencies, and that they have well-defined threshold values of being representative. In addition, the baseline function for a corpus is robust against differences in corpora; that is, it can be used for normalization in a different corpus that has a different size or is in a different domain."
C00-1060,A Hybrid {J}apanese Parser with Hand-crafted Grammar and Statistics,2000,13,25,4,0,1260,hiroshi kanayama,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"This paper describes a hybrid parsing method for Japanese which uses both a hand-crafted grammar and a statistical technique. The key feature of our system is that in order to estimate likelihood for a parse tree, the system uses information taken from alternative partial parse trees generated by the grammar. This utilization of alternative trees enables us to construct a new statistical model called Triplet/Quadruplet Model. We show that this model can capture a certain tendency in Japanese syntactic structures and this point contributes to improvement of parsing accuracy on a shallow level. We report that, with an underspecified HPSG-based grammar and a maximum entropy estimation, our parser achieved high accuracy: 88.6% accuracy in dependency analysis of the EDR annotated corpus, and that it outperformed other purely statistical parsing methods on the same corpus. This result suggests that proper treatment of hand-crafted grammars can contribute to parsing accuracy on a shallow level."
C00-1070,Lexicalized Hidden {M}arkov Models for Part-of-Speech Tagging,2000,11,28,2,0,53188,sangzoo lee,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"Since most previous works for HMM-based tagging consider only part-of-speech information in contexts, their models cannot utilize lexical information which is crucial for resolving some morphological ambiguity. In this paper we introduce uniformly lexicalized HMMs for part-of-speech tagging in both English and Korean. The lexicalized models use a simplified back-off smoothing technique to overcome data sparseness. In experiments, lexicalized models achieve higher accuracy than non-lexicalized models and the back-off smoothing method mitigates data sparseness better than simple smoothing methods."
E99-1043,The {GENIA} project: corpus-based knowledge acquisition and information extraction from genome research papers,1999,8,57,10,1,219,nigel collier,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present an outline of the genome information acquisition (GENIA) project for automatically extracting biochemical information from journal papers and abstracts. GENIA will be available over the Internet and is designed to aid in information extraction, retrieval and visualisation and to help reduce information overload on researchers. The vast repository of papers available online in databases such as MEDLINE is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the Internet."
1999.mtsummit-1.26,Machine translation for the next century,1999,-1,-1,1,1,20171,junichi tsujii,Proceedings of Machine Translation Summit VII,0,"The panel intends to pick up some of the issues discussed in the Summit and discuss them further in the final session from broader perspectives. Since the Summit has not even started yet, I will just enumerate in this paper a list of possible perspectives on MT that I hope are relevant to our discussion."
1999.mtsummit-1.74,Transfer in experience-guided machine translation,1999,-1,-1,2,0,55040,gang zhao,Proceedings of Machine Translation Summit VII,0,"Experience-Guided Machine Translation (EGMT) seeks to represent the translators' knowledge of translation as experiences and translates by analogy. The transfer in EGMT finds the experiences most similar to a new text and its parts, segments it into units of translation and translates them by analogy to the experiences and then assembles them into a whole. A research prototype of analogical transfer from Chinese to English is built to prove the viability of the approach in the exploration of new architecture of machine translation. The paper discusses how the experiences are represented and selected with respect to a new text. It describes how units of translation are defined, partial translation is derived and composed into a whole."
W98-0127,Packing of feature structures for optimizing the {HPSG}-style grammar translated from {TAG},1998,0,1,4,1,5928,yusuke miyao,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,None
W98-0141,Translating the {XTAG} {E}nglish grammar to {HPSG},1998,2,22,4,1,35318,yuka tateisi,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,None
P98-2132,{L}i{LF}e{S} - Towards a Practical {HPSG} Parser,1998,12,23,4,1,53249,takaki makino,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper presents the LiLFeS system, an efficient feature-structure description language for HPSG. The core engine of LiLFeS is an Abstract Machine for Attribute-Value Logics, proposed by Carpenter and Qu. Basic design policies, the current status, and performance evaluation of the LiLFeS system are described. The paper discusses two implementations of the LiLFeS. The first one is based on an emulator of the abstract machine, while the second one uses a native-code compiler and therefore is much more efficient than the first one."
P98-2144,{HPSG}-Style Underspecified {J}apanese Grammar with Wide Coverage,1998,5,21,3,1,39370,yutaka mitsuishi,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper describes a wide-coverage Japanese grammar based on HPSG. The aim of this work is to see the coverage and accuracy attainable using an underspecified grammar. Underspecification, allowed in a typed feature structure formalism, enables us to write down a wide-coverage grammar concisely. The grammar we have implemented consists of only 6 ID schemata, 68 lexical entries (assigned to functional words), and 63 lexical entry templates (assigned to parts of speech (POSs) ). Furthermore, word-specific constraints such as subcategorization of verbs are not fixed in the grammar. However, this grammar can generate parse trees for 87% of the 10000 sentences in the Japanese EDR corpus. The dependency accuracy is 78% when a parser uses the heuristic that every bunsetsu is attached to the nearest possible one."
P98-2159,An Efficient Parallel Substrate for Typed Feature Structures on Shared Memory Parallel Machines,1998,10,0,3,1,3213,takashi ninomiya,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper describes an efficient parallel system for processing Typed Feature Structures (TFSs) on shared-memory parallel machines. We call the system Parallel Substrate for TFS (PSTFS). PSTFS is designed for parallel computing environments where a large number of agents are working and communicating with each other. Such agents use PSTFS as their low-level module for solving constraints on TFSs and sending/receiving TFSs to/from other agents in an efficient manner. From a programmers' point of view, PSTFS provides a simple and unified mechanism for building high-level parallel NLP systems. The performance and the flexibility of our PSTFS are shown through the experiments on two different types of parallel HPSG parsers. The speed-up was more than 10 times on both parsers."
C98-2128,{L}i{LF}e{S}- Towards a Practical {HPSG} Parser,1998,12,23,4,1,53249,takaki makino,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper presents the LiLFeS system, an efficient feature-structure description language for HPSG. The core engine of LiLFeS is an Abstract Machine for Attribute-Value Logics, proposed by Carpenter and Qu. Basic design policies, the current status, and performance evaluation of the LiLFeS system are described. The paper discusses two implementations of the LiLFeS. The first one is based on an emulator of the abstract machine, while the second one uses a native-code compiler and therefore is much more efficient than the first one."
C98-2139,{HPSG}-Style Underspecified {J}apanese Grammar with Wide Coverage,1998,5,21,3,1,39370,yutaka mitsuishi,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper describes a wide-coverage Japanese grammar based on HPSG. The aim of this work is to see the coverage and accuracy attainable using an underspecified grammar. Underspecification, allowed in a typed feature structure formalism, enables us to write down a wide-coverage grammar concisely. The grammar we have implemented consists of only 6 ID schemata, 68 lexical entries (assigned to functional words), and 63 lexical entry templates (assigned to parts of speech (POSs) ). Furthermore, word-specific constraints such as subcategorization of verbs are not fixed in the grammar. However, this grammar can generate parse trees for 87% of the 10000 sentences in the Japanese EDR corpus. The dependency accuracy is 78% when a parser uses the heuristic that every bunsetsu is attached to the nearest possible one."
C98-2154,An Efficient Parallel Substrate for Typed Feature Structures on Shared Memory Parallel Machines,1998,10,0,3,1,3213,takashi ninomiya,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper describes an efficient parallel system for processing Typed Feature Structures (TFSs) on shared-memory parallel machines. We call the system Parallel Substrate for TFS (PSTFS). PSTFS is designed for parallel computing environments where a large number of agents are working and communicating with each other. Such agents use PSTFS as their low-level module for solving constraints on TFSs and sending/receiving TFSs to/from other agents in an efficient manner. From a programmers' point of view, PSTFS provides a simple and unified mechanism for building high-level parallel NLP systems. The performance and the flexibility of our PSTFS are shown through the experiments on two different types of parallel HPSG parsers. The speed-up was more than 10 times on both parsers."
C96-2160,Computing Phrasal-signs in {HPSG} prior to Parsing,1996,5,14,2,1,12932,kentaro torisawa,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"This paper describes techniques to compile lexical entries in HPSG (Pollard and Sag, 1987; Pollard and Sag, 1993)-style grammar into a set of finite state automata. The states in automata are possible signs derived from lexical entries and contain information raised from the lexical entries. The automata are augmented with feature structures used by a partial unification routine and delayed/frozen definite clause programs."
1995.iwpt-1.29,An {HPSG}-based Parser for Automatic Knowledge Acquisition,1995,-1,-1,2,1,12932,kentaro torisawa,Proceedings of the Fourth International Workshop on Parsing Technologies,0,
P94-1042,A Computational View of the Cognitive Semantics of Spatial Prepositions,1994,15,14,2,0,56284,patrick olivier,32nd Annual Meeting of the Association for Computational Linguistics,1,"This paper outlines the linguistic semantic commitments underlying an application which automatically constructs depictions of verbal spatial descriptions. Our approach draws on the ideational view of linguistic semantics developed by Ronald Langacker in his theory of Cognitive Grammar, and the conceptual representation of physical objects from the two-level semantics of Bierwisch and Lang. In particular the dimensions of the process of conventional imagery are used as a metric for the design of our own conceptual representation."
C94-2122,Automatic Recognition of Verbal Polysemy,1994,5,11,2,0,9286,fumiyo fukumoto,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Polysemy is one of the major causes of difficulties in semantic clustering of words in a corpus. In this paper, we first give a definition of polysemy from the viewpoint of clustering and then, based on this definition, we propose a clustering method which recognises verbal polysemies from a textual corpus. The results of experiments demonstrate the effectiveness of the proposed method."
C94-2134,Hypothesis Selection in Grammar Acquisition,1994,8,3,2,1,56450,masaki kiyono,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper presents some techniques for selecting linguistically adequate hypotheses of new grammatical knowledge to be used as resources of grammatical knowledge acquisition. In our framework of linguistic knowledge acquisition, a rule-based hypothesis generator is invoked in case of parsing failures and all the possible hypotheses of new grammar rules or lexical entries are generated from partial parsing results. Although each hypothesis could recover the defects of the existing grammar, the greater part of hypotheses are linguistically unnatural. The techniques we propose here prevent such unnatural hypotheses from being generated without discarding plausible ones and make the following corpus-based acquisition process more efficient and more reliable."
C94-2192,Breaking Down Rhetorical Relations for the purpose of Analysing Discourse Structures,1994,6,3,2,0,49658,junichi fukumoto,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In Rhetorical Structure Theory (RST) the definitions of some relations are rather vague because they are given on a pragmatic basis. This paper presents another way of seeing the relations which leads to a more precise specification of the relations. The relations are associated with constraints on the semantic relationships between the propositional contents of two clauses, their Modality and Tense/Aspect."
A94-1012,Combination of Symbolic and Statistical Approaches for Grammatical Knowledge Acquisition,1994,8,3,2,1,56450,masaki kiyono,Fourth Conference on Applied Natural Language Processing,0,"The framework we adopted for customizing linguistic knowledge to individual application domains is an integration of symbolic and statistical approaches. In order to acquire domain specific knowledge, we have previously proposed a rule-based mechanism to hypothesize missing knowledge from partial parsing results of unsuccessfully parsed sentences. In this paper, we focus on the statistical process which selects plausible knowledge from a set of hypotheses generated from the whole corpus. In particular, we introduce two statistical measures of hypotheses, Local Plausibility, and Global Plausibility, and describe how these measures are determined iteratively. The proposed method will be incorporated into the tool kit for linguistic knowledge acquisition which we are now developing."
E93-1027,Linguistic Knowledge Acquisition from Parsing Failures,1993,7,5,2,1,56450,masaki kiyono,Sixth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"A semi-automatic procedure of linguistic knowledge acquisition is proposed, which combines corpus-based techniques with the conventional rule-based approach. The rule-based component generates all the possible hypotheses of defects which the existing linguistic knowledge might contain, when it fails to parse a sentence. The rule-based component does not try to identify the defects, but generates a set of hypotheses and the corpus-based component chooses the plausible ones among them. The procedure will be used for adapting or re-using existing linguistic resources for new application domains."
1993.tmi-1.11,Treatment of Tense and Aspect in Translation from {I}talian to {G}reek {---} An Example of Treatment of Implicit Information in Knowledge-based Transfer {MT} {---},1993,-1,-1,2,0,56870,margherita antona,Proceedings of the Fifth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
1993.tmi-1.12,{---} An Example of Treatment of Implicit Information in Knowledge-based Transfer {MT} {---},1993,-1,-1,2,0,56870,margherita antona,Proceedings of the Fifth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
1993.mtsummit-1.20,After Linguistics-based {MT},1993,-1,-1,1,1,20171,junichi tsujii,Proceedings of Machine Translation Summit IV,0,None
H92-1051,Interaction between Structural Changes in Machine Translation,1992,5,5,3,0,31477,satoshi kinoshita,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"This paper discusses complex structural changes during transfer within a non-destructive transfer framework. Though the description of each individual structural change is not difficult, special provision must be made when they are combined, because interaction between them sometimes causes unexpected problems. Transfer of coordinate structures is also discussed as this sometimes necessitates a structural change and interacts with other structural changes in a problematic way."
C92-2085,Linguistic Knowledge Generator,1992,8,18,4,0.288777,1087,satoshi sekine,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The difficulties in current NLP applications are seldom due to the lack of appropriate frameworks for encoding our linguistic or extra-linguistic knowledge, hut rather to the fact that we do not know in advance what actual znstances of knowledge should be, even though we know in advance what types of knowledge are required. It normally takes a long time and requires painful trial and error processes to adapt knowledge, for example, in existing MT systems in order to translate documents of a new text-type and of a new subject domain. Semantic classification schemes for words, for example, usually reflect ontologies of subject domains so that we cannot expect a single classification scheme to be effective across different domains. To treat different suhlanguages requires different word classification schemes. We have to construct appropriate schemes for given sublanguages from scratch [1]. It has also been reported that not only knowledge concerned with extra-linguistic domains but also syntactic knowledge, such as subcategorization frames of verbs (which is usually conceived as a par t of general language knowledge), often varies from one sublanguage to another [2]. Though re-usability of linguistic knowledge is currently and intensively prescribed [3], our contention is that the adaptation of existing knowledge requires processes beyond mere re-use. Tha t is,"
C92-2102,Interaction between Structural Changes in Machine Translation,1992,5,5,3,0,31477,satoshi kinoshita,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper discusses complex structural changes during transfer within a non-destructive transfer framework. Though the description of each individual structural change is not difficult, special provision must be made when they are combined, because interaction between them sometimes causes unexpected problems. Transfer of coordinate structures is also discussed as this sometimes necessitates a structural change and interacts with other structural changes in a problematic way."
A92-1014,Automatic Learning for Semantic Collocation,1992,9,59,4,0.288777,1087,satoshi sekine,Third Conference on Applied Natural Language Processing,0,"The real difficulty in development of practical NLP systems comes from the fact that we do not have effective means for gathering knowledge. In this paper, we propose an algorithm which acquires automatically knowledge of semantic collocations among words from sample corpora.The algorithm proposed in this paper tries to discover semantic collocations which will be useful for disambiguating structurally ambiguous sentences, by a statistical approach. The algorithm requires a corpus and minimum linguistic knowledge (parts-of-speech of words, simple inflection rules, and a small number of general syntactic rules).We conducted two experiments of applying the algorithm to diferent corpora to extract different types of semantic collocations. Though there are some unsolved problems, the results showed the effectiveness of the proposed algorithm."
C90-3048,Machine Translation without a source text,1990,8,33,2,0,45023,harold somers,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"This paper concerns an approach to Machine Translation which differs from the typical 'standard' approaches crucially in that it does not rely on the prior existence of a source text as a basis of the translation. Our approach can be characterised as an 'intelligent secretary with knowledge of the foreign language', which helps monolingual users to formulate the desired target-language text in the context of a (key-board) dialogue translation systems."
1990.tc-1.1,Machine Translation and Machine-Aided Translation - What{'}s going on,1990,0,1,1,1,20171,junichi tsujii,Proceedings of Translating and the Computer 12: Applying technology to the translation process,0,"PURPOSE:To efficiently translate sentences by obtaining a preferential solution and one or more different solution based on the difference of the modified relation of clauses, displaying them to a user, when the different solution is selected, replacing the preferential solution with the different solution, and correcting it. CONSTITUTION:An inputted sentence is divided into words while a translation dictionary 40 and a translation grammar 50 are being referred to, the clauses are collected, and the contents of a clause table 11 are prepared. In the preferential column of 'the clause modified by the clause concerned' in the table 11, number is written to the clause judged to be the modified destination of the clause. The different solution of the modification is searched for while the modifying characteristic and modified attribute of the table 11 and the grammar 50 are being referred to. The table 11 obtained by the analysis up to present a time is outputted to an intermediate result preserving table 70. Next, the meaning relation between the two clauses in the modified relation is analyzed by referring to the grammar 50. By referring the table 11 and the grammar 50, an English sentence is generated, and the translated English is outputted to a text memory 60."
C88-2095,Reasons why {I} do not care grammar formalism,1988,0,1,1,1,20171,junichi tsujii,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Computational linguistics (CL) has borrowed a lot of ideas from Theoretical Linguistics (TZ). We could not have developed even a simple parser without the research results in TL. It is obviously nonsense to claim that we, computational linguists, do not care research results in TL. llowever, the researchers in TL, it seems to me, are very fond of fighlinq~ especially, those who are called Synlaclicians. They always fight with e~h other by asserting that their grammar formalisms are superior to the others'. They are oversensitive and tend to distinguish people into two groups, the ally and the enemy."
C88-2141,How to Get Preferred Readings in Natural Language Analysis,1988,4,3,1,1,20171,junichi tsujii,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper describes a parsing program called KGWp which is designed for integrating various sorts of knowledge to get most preferred structural descriptions of sentences. The system accepts not only a set of rules specifying constraints which any descriptions of sentences should satisfy, but also preferential rules which are utilized in selecting most preferred descriptions among possible ones. During the parsing process, the preferetial rules are utilized to select feasible parsing paths. Furthermore, KGWp is complete in the sense that it can generate all possible structural descriptions of sentences it required. The descriptions are generated in a preferential order."
C88-2142,Dialogue Translation vs. Text Translation,1988,3,3,1,1,20171,junichi tsujii,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The authors discuss the differences of envi~onmeats whexe dialogue translation and textual translation systems might be used. The differences are summarized as clear definition of information and active participations of speakers and hearers in dialogue t~anslation. A new approach to MT, interpretation based approach, is proposed to take the advantages of dialogue translation environments. The approach introduces a layer o/ understanding to MT and can produce less structure bound translations than conventional approaches."
1987.mtsummit-1.23,What is {`}{PIVOT}{'}?,1987,-1,-1,1,1,20171,junichi tsujii,Proceedings of Machine Translation Summit I,0,None
1987.mtsummit-1.31,The Current Stage of the Mu-Project,1987,-1,-1,1,1,20171,junichi tsujii,Proceedings of Machine Translation Summit I,0,None
C86-1021,The Transfer Phase of the {M}u Machine Translation System,1986,2,19,2,0,58176,hakoto nagao,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,None
C86-1029,Solutions for Problems of {MT} Parser - Methods Used in {M}u-Machine Translation Project -,1986,6,2,2,0.869565,51028,junichi nakamura,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,None
C86-1155,Future Directions of Machine Translation,1986,37,14,1,1,20171,junichi tsujii,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,None
J85-2001,The {J}apanese Government Project for Machine Translation,1985,4,62,2,0.75,51221,makoto nagao,Computational Linguistics,0,"The project is funded by a grant from the Agency of Science and Technology through the Special Coordination Funds for the Promotion of Science and Technology, and was started in fiscal 1982. The formal title of the project is Research on Fast Information Services between Japanese and English for Scientific and Engineering Literature. The purpose is to demonstrate the feasibility of machine translation of abstracts of scientific and engineering papers between the two languages, and as a result, to establish a fast information exchange system for these papers. The project term was initially scheduled as three years from the fiscal year of 1982 with a budget of about seven hundred million yen, but, due to the present financial pressures on the government, the term has been extended to four years, up to 1986. The project is conducted by the close cooperation between four organizations. At Kyoto University, we have the responsibility of developing the software system for the core part of the machine translation process (grammar writing system and execution system); grammar systems for analysis, transfer and synthesis; detailed specification of what information is written in the word dictionaries (all the parts of speech in the analysis, transfer, and generation dictionaries), and the working manuals for constructing these dictionaries. The Electrotechnical Laboratories (ETL) are responsible for the machine translation text input and output, morphological analysis and synthesis, and the construction of the verb and adjective dictionaries based on the working manuals prepared at Kyoto. The Japan Information Center for Science and Technology (JICST) is in charge of the noun dictionary and the compiling of special technical terms in scientific and technical fields. The Research Information Processing System (RIPS) under the Agency of Engineer. # . mg Technology is responsible for completing the machine translation system, including the man-machine interfaces to the system developed at Kyoto, which allow preand post-editing, access to grammar rules, and dictionary maintenance. The project is not primarily concerned with the development of a final practical system; that will be developed by private industry using the results of this project. Technical know-how is already being transferred gradually to private enterprise through the participation in the project of people from industry. Software and linguistic data are also being transferred in part. Finally, complete technical transfer will be done under the proper conditions. The Japanese source texts being used are abstracts of scientific and technical papers published in the monthly JICST journal d Current Bibliography of Science and Technology. At present, the project is only processing texts in the electronics, electrical engineering, and computer science fields. English source texts will be abstracts from INSPEC in these f ields. . The sentence structures used in abstracts tend .to be complex compared to ordinary sentences, with long nominal compounds, noun-phrase conjunctions, mathematical and physical formulas, long embedded sentences, and so on. The analysis and translation of this type of sentence structure is far more difficult than ordinary sentence patterns. However, we have not included a pre-editing stage because we wanted to find the ultimate limitations on handling this type of complex sentence structure. Our system is based on the following concepts: 1. The use of all available linguistic information, both surface and syntactic. The writing of as detailed as possible syntactic rules. The development of a grammar writing system that can accept any future level of sophisticated linguistic theory. 2. The introduction of semantic information wherever necessary to enable the syntactic analysis to be as accurate as possible. The importance of semantic information not over-estimated; a well-balanced usage of both syntax and semantics. Heavily seman-"
P84-1057,Analysis Grammar of {J}apanese in the {M}u-project - A Procedural Approach to Analysis Grammar,1984,9,9,1,1,20171,junichi tsujii,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"Analysis grammar of Japanese in the Mu-project is presented. It is emphasized that rules expressing constraints on single linguistic structures and rules for selecting the most preferable readings are completely different in nature, and that rules for selecting preferale readings should be utilized in analysis grammars of practical MT systems. It is also claimed that procedural control is essential in integrating such rules into a unified grammar. Some sample rules are given to make the points of discussion clear and concrete."
P84-1069,Grammar Writing System ({GRADE}) of {M}u-Machine Translation Project and its Characteristics,1984,4,19,2,0.869565,51028,junichi nakamura,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"A powerful grammar writing system has been developed. This grammar writing system is called GRADE (GRAmmar DEscriber). GRADE allows a grammar writer to write grammars including analysis, transfer, and generation using the same expression. GRADE has powerful grammar writing facility. GRADE allows a grammar writer to control the process of a machine translation. GRADE also has a function to use grammatical rules written in a word dictionary. GRADE has been used for more than a year as the software of the machine translation project from Japanese into English, which is supported by the Japanese Government and called Mu-project."
P84-1086,Dealing With Incompleteness of Linguistic Knowledge in Language Translation {--} Transfer and Generation Stage of {M}u Machine Translation Project,1984,4,11,3,0.333333,51221,makoto nagao,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"Therefore the linguistic contents of machine translation system always fluctuate, and make gradual progress. The system should be designed to allow such constant change and improvements. This paper explains the details of the transfer and generation stages of Japanese-to-English system of the machine translation project by the Japanese Government, with the emphasis on the ideas to deal with the incompleteness of linguistic knowledge for machine translation."
C82-1039,An {E}nglish {J}apanese Machine Translation System of the Titles of Scientific and Engineering Papers,1982,0,14,2,0,51221,makoto nagao,{C}oling 1982: Proceedings of the {N}inth {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The title sentences of scientific and engineering papers are analyzed by simple parsing strategies, and only eighteen fundamental sentential structures are obtained from ten thousand titles. Title sentences of physics and mathematics of some databases in English are translated into Japanese with their keywords, author names, journal names and so on by using these fundamental structures. The translation accuracy for the specific areas of physics and mathematics from INSPEC database was about 93%."
C82-1062,The Transfer Phase In an {E}nglish-{J}apanese Translation System,1982,1,7,1,1,20171,junichi tsujii,{C}oling 1982: Proceedings of the {N}inth {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
J76-1003,{PLATON}--A New Programming Language for Natural Language Analysis,1976,-1,-1,2,0,51221,makoto nagao,American Journal of Computational Linguistics,0,None
J76-1008,Analysis of {J}apanese Sentences,1976,-1,-1,2,0,51221,makoto nagao,American Journal of Computational Linguistics,0,None
