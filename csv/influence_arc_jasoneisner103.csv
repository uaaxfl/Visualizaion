2020.acl-main.415,2020.lrec-1.520,0,0.0419634,"on requires access to large amounts of cross-linguistic data. Previous cross-linguistic phonetic studies have been limited to a small number of languages with available data (Disner, 1983; Cho and Ladefoged, 1999), or have relied on previously reported measures from many studies (Whalen and Levitt, 1995; Becker-Kristal, 2010; Gordon and Roettger, 2017; Chodroff et al., 2019). Existing multilingual speech corpora have similar restrictions, with data too limited for many tasks (Engstrand and Cunningham-Andersson, 1988; Ladefoged and Maddieson, 2007) or approximately 20 to 30 recorded languages (Ardila et al., 2020; Harper, 2011; Schultz, 2002). The recently developed CMU Wilderness corpus (Black, 2019) constitutes an exception to this rule with over 600 languages. This makes it the largest and most typologically diverse speech corpus to date. In addition to its coverage, the CMU Wilderness corpus is unique in two additional aspects: cleanly recorded, read speech exists for all languages in the corpus, and the same content (modulo translation) exists across all languages. However, this massively multilingual speech corpus is challenging to work with directly. Copyright, computational restrictions, and s"
2020.acl-main.415,2020.lrec-1.521,0,0.334554,"e mean mel cepstral distortion score (see §3.1.3) converges. Baum-Welch does not change the predicted phoneme labels, but obtains a language-specific, reading-specific, contextual (triphone) acoustic model for each phoneme type in the language. We then use Viterbi alignment to identify an audio segment for each phoneme token. 3.1.2 High-Resource Languages A subset of the languages in our corpus are supported by existing pronunciation resources. Two such resources are Epitran (Mortensen et al., 2018), a G2P tool based on language-specific rules, available in both IPA and X-SAMPA, and WikiPron (Lee et al., 2020), a collection of crowd-sourced pronunciations scraped from Wiktionary. These are mapped from IPA to X-SAMPA for label consistency across our corpus. Epitran covers 29 of our languages (39 readings), while WikiPron’s ‘phonemic’ annotations7 provide partial coverage of 13 additional languages (18 readings). We use Epitran for languages with regular orthographies where it provides high-quality support, and WikiPron for other languages covered by WikiPron annotations. While Unitran and Epitran provide a single pronunciation for a word from the orthography, WikiPron may include multiple pronunciat"
2020.acl-main.415,qian-etal-2010-python,0,0.477559,"for each utterance. A pronunciation is predicted from the text alone using some graphemeto-phoneme (G2P) method. Each word’s predicted pronunciation is a sequence of categorical labels, which are ‘phoneme-level’ in the sense that they are usually intended to distinguish the words of the language. We then align this predicted sequence of ‘phonemes’ to the corresponding audio. 3.1.1 All Languages Most of our languages have neither existing pronunciation lexicons nor G2P resources. To provide coverage for all languages, we generate pronunciations using the simple ‘universal’ G2P system Unitran (Qian et al., 2010, as extended by Black, 2019), which deterministically expands each grapheme to a fixed sequence of phones in the Extended Speech Assessment Methods Phonetic Alphabet (XSAMPA) (Wells, 1995/2000). This naive process is error-prone for languages with opaque orthographies, as we show in §3.1.3 below and discuss further in §3.4 (Caveat B). Even so, it provides a starting point for exploring low-resource languages: after some manual inspection, a linguist may be 6 able to correct the labels in a given language by a combination of manual and automatic methods. For each reading, to align the pronunci"
2020.tacl-1.36,P96-1009,0,0.132698,"uces the same result, or an alternative interpretation that is also contextually appropriate. Count 8 Related work 3 The view of dialogue as an interactive process of shared plan synthesis dates back to Grosz and Sidner’s earliest work on discourse structure (1986; 1988). That work represents the state of a dialogue as a predicate recognizing whether a desired piece of information has been communicated or change in world state effected. Goals can be refined via questions and corrections from both users and agents. The only systems to attempt full versions of this shared-plans framework (e.g., Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2"
2020.tacl-1.36,D18-1547,0,0.165132,"65 1,052 3,315 Dataflow inline .729 .696 .665 .606 .642 .533 .574 .465 .697 .631 .565 .474 Dataflow inline refer inline both TRADE Table 2: SMCalFlow results. Agent action accuracy is significantly higher than a baseline without metacomputation, especially on turns that involve a reference (Ref. Turns) or revision (Rev. Turns) to earlier turns in the dialogue (p &lt; 10−6 , McNemar’s test). Joint Goal Dialogue Prefix .467 .447 .467 .454 .220 .202 .205 .168 3.07 2.97 2.90 2.73 Table 3: MultiWOZ 2.1 test set results. TRADE (Wu et al., 2019) results are from the public implementation. “Joint Goal” (Budzianowski et al., 2018) is average dialogue state exact-match, “Dialogue” is average dialogue-level exact-match, and “Prefix” is the average number of turns before an incorrect prediction. Within each column, the best result is boldfaced, along with all results that are not significantly worse (p &lt; 0.05, paired permutation test). Moreover, all of “Dataflow,” “inline refer,” and “inline both” have higher dialogue accuracy than TRADE (p &lt; 0.005). els that train on inlined metacomputation. These experiments make it possible to evaluate the importance of explicit dataflow manipulation compared to a standard contextual s"
2020.tacl-1.36,D19-1459,0,0.0502204,"bling side-byside comparisons and experiments with alternative representations. We provide full conversion scripts for MultiWOZ. 567 9 teractive dialogues. It is assumed that any user intent can be represented with a flat structure consisting of a categorical dialogue act and a mapping between a fixed set of slots and string-valued fillers. Existing fine-grained dialogue act schemes (Stolcke et al., 2000) can distinguish among a range of communicative intents not modeled by our approach, and slot-filling representations have historically been easier to predict (Zue et al., 1994) and annotate (Byrne et al., 2019). But while recent variants support interaction between related slots (Budzianowski et al., 2018) and fixed-depth hierarchies of slots (Gupta et al., 2018), modern slot-filling approaches remain limited in their support for semantic compositionality. By contrast, our approach supports user requests corresponding to general compositional programs. Conclusions We have presented a representational framework for task-oriented dialogue modeling based on dataflow graphs, in which dialogue agents predict a sequence of compositional updates to a graphical state representation. This approach makes it p"
2020.tacl-1.36,P17-1167,0,0.0181575,"bout underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors on the SMCalFlow dataset. The largest categories are underprediction (omitting steps from agent programs), entity linking (errors in extraction of entities from user utterances, fencing (classifying a user request as outof-scope), and ambiguity (user utterances with multiple possible inter"
2020.tacl-1.36,P17-4012,0,0.0122901,"propriate type constraint, provided that the reference resolution heuristic would retrieve the correct string from earlier in the dataflow. This covers references like the same day. Otherwise, our re-annotation retains the literal string value. Data statistics are shown in Table 1. To the best of our knowledge, SMCalFlow is the largest annotated task-oriented dialogue dataset to date. Compared to MultiWOZ, it features a larger user vocabulary, a more complex space of statemanipulation primitives, and a long tail of agent programs built from numerous function calls and deep composition. 7 NMT (Klein et al., 2017) pointer-generator network (See et al., 2017), a sequence-to-sequence model that can copy tokens from the source sequence while decoding. Our goal is to demonstrate that dataflow-based representations benefit standard neural model architectures. Dataflowspecific modeling might improve on this baseline, and we leave this as a challenge for future work. For each user turn i, we linearize the target program into a sequence of tokens zi . This must be predicted from the dialogue context— namely the concatenated source sequence xi−c zi−c · · · xi−1 zi−1 xi (for SMCalFlow) or xi−c yi−c · · · xi−1 yi"
2020.tacl-1.36,J94-4002,0,0.175555,"tate representations. While a complete description of dataflow-based language generation is beyond the scope of this paper, we briefly describe the components of the generation system relevant to the understanding system presented here. 3 Reference resolution In a dialogue, entities that have been introduced once may be referred to again. In dataflow dialogues, the entities available for reference are given by the nodes in the dataflow graph. Entities are salient to conversation participants to different degrees, and their relative salience determines the ways in which they may be referenced (Lappin and Leass, 1994). For example, it generally refers to the most salient non-human entity, while more specific expressions like the Friday meeting are needed to refer to accessible but less salient entities. Not all references to entities are overt: if the agent says “You have a meeting tomorrow” and the user responds “What time?”, the agent must predict the implicit reference to a salient event. 559 Dataflow pointers We have seen that refer is used to find referents for referring expressions. In general, these referents may be existing dataflow nodes or new subgraphs for newly mentioned entities. We now give m"
2020.tacl-1.36,J86-3001,0,0.780827,"Missing"
2020.tacl-1.36,W18-6322,0,0.0167485,"ser utterances with multiple possible interpretations). See §7 for discussion. Error analysis Beyond the quantitative results shown in Tables 2–3, we manually analyzed 100 SMCalFlow turns where our model mispredicted. Table 4 breaks down the errors by type. Three categories involve straightforward parsing errors. In underprediction errors, the model fails to predict some computation (e.g., a search constraint or property extractor) specified in the user request. This behavior is not specific to our system: under-length predictions are also welldocumented in neural machine translation systems (Murray and Chiang, 2018). In entity linking errors, the model correctly identifies the presence of an entity mention in the input utterance, but uses it incorrectly in the input plan. Sometimes the entity that appears in the plan is hallucinated, appearing nowhere in the utterance; sometimes the entity is cast to a wrong type (e.g., locations interpreted as event names) used in the wrong field or extracted with wrong boundaries. In fencing errors, the model interprets an out-of-scope user utterance as an interpretable command, or vice-versa versions of the full dataset, and inlined and non-inlined versions of our mod"
2020.tacl-1.36,D18-1300,0,0.0282209,"It is assumed that any user intent can be represented with a flat structure consisting of a categorical dialogue act and a mapping between a fixed set of slots and string-valued fillers. Existing fine-grained dialogue act schemes (Stolcke et al., 2000) can distinguish among a range of communicative intents not modeled by our approach, and slot-filling representations have historically been easier to predict (Zue et al., 1994) and annotate (Byrne et al., 2019). But while recent variants support interaction between related slots (Budzianowski et al., 2018) and fixed-depth hierarchies of slots (Gupta et al., 2018), modern slot-filling approaches remain limited in their support for semantic compositionality. By contrast, our approach supports user requests corresponding to general compositional programs. Conclusions We have presented a representational framework for task-oriented dialogue modeling based on dataflow graphs, in which dialogue agents predict a sequence of compositional updates to a graphical state representation. This approach makes it possible to represent and learn from complex, natural dialogues. Future work might focus on improving prediction by introducing learned implementations of r"
2020.tacl-1.36,H90-1021,0,0.582952,"ted or change in world state effected. Goals can be refined via questions and corrections from both users and agents. The only systems to attempt full versions of this shared-plans framework (e.g., Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach"
2020.tacl-1.36,D14-1162,0,0.0839852,"with a separator token that indicates the speaker (user or agent). Our formulation of context for MultiWOZ is standard (e.g., Wu et al., 2019). We take the source and target vocabularies to consist of all words that occur in (respectively) the source and target sequences in training data, as just defined. The model is trained using the Adam optimizer (Kingma and Ba, 2015) with the maximum likelihood objective. We use 0.001 as the learning rate. Training ends when there have been two different epochs that increased the development loss. We use Glove800B-300d (cased) and Glove6B300d (uncased) (Pennington et al., 2014) to initialize the vocabulary embeddings for the SMCalFlow and MultiWoZ experiments, respectively. The context window size c, hidden layer size d, number of hidden layers l, and dropout rates r are selected based on the agent action accuracy (for SMCalFlow) or dialogue-level exact match (for MultiWoZ) on the development set from {2, 4, 10}, {256, 300, 320, 384}, {1, 2, 3}, {0.3, 0.5, 0.7} respectively. Approximate 1-best decoding uses a beam of size 5. Quantitative evaluation Table 2 shows results for the SMCalFlow dataset. We report program accuracy: specifically, exact-match accuracy of the"
2020.tacl-1.36,P17-1062,0,0.0717515,"Missing"
2020.tacl-1.36,P19-1078,0,0.0158945,"work. For each user turn i, we linearize the target program into a sequence of tokens zi . This must be predicted from the dialogue context— namely the concatenated source sequence xi−c zi−c · · · xi−1 zi−1 xi (for SMCalFlow) or xi−c yi−c · · · xi−1 yi−1 xi (for MultiWOZ 2.1). Here c is a context window size, xj is the user utterance at user turn j, yj is the agent’s naturallanguage response, and zj is the linearized agent program. Each sequence xj , yj , or zj begins with a separator token that indicates the speaker (user or agent). Our formulation of context for MultiWOZ is standard (e.g., Wu et al., 2019). We take the source and target vocabularies to consist of all words that occur in (respectively) the source and target sequences in training data, as just defined. The model is trained using the Adam optimizer (Kingma and Ba, 2015) with the maximum likelihood objective. We use 0.001 as the learning rate. Training ends when there have been two different epochs that increased the development loss. We use Glove800B-300d (cased) and Glove6B300d (uncased) (Pennington et al., 2014) to initialize the vocabulary embeddings for the SMCalFlow and MultiWoZ experiments, respectively. The context window s"
2020.tacl-1.36,P17-1099,0,0.0599554,"Missing"
2020.tacl-1.36,J00-3003,0,0.766051,"Missing"
2020.tacl-1.36,N18-1203,0,0.0172756,"goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors on the SMCalFlow dataset. The largest categories are underprediction (omitting steps from agent programs), entity linking (errors in extraction of entities from user utterances, fencing (classifying a user request as outof-scope), and ambiguity (user utterances with multiple possible interpretations). See §7"
2020.tacl-1.36,P19-1443,0,0.056047,"state effected. Goals can be refined via questions and corrections from both users and agents. The only systems to attempt full versions of this shared-plans framework (e.g., Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse wi"
2020.tacl-1.36,Q14-1042,0,0.0241672,", Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors o"
2020.tacl-1.36,E17-1042,0,0.105146,"Missing"
2020.tacl-1.36,P09-1110,0,0.038465,"derstanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors on the SMCalFlow dataset. The largest categories are underprediction (omitting steps from agent programs), entity linking (errors in extraction of entities from user utterances, fencing (classifying a user request as outof-scope), and ambiguity (user utterances with mul"
2020.tacl-1.36,W16-3601,0,0.0677409,"Missing"
2020.tacl-1.36,H94-1037,0,0.573739,"odel’s test set predictions, enabling side-byside comparisons and experiments with alternative representations. We provide full conversion scripts for MultiWOZ. 567 9 teractive dialogues. It is assumed that any user intent can be represented with a flat structure consisting of a categorical dialogue act and a mapping between a fixed set of slots and string-valued fillers. Existing fine-grained dialogue act schemes (Stolcke et al., 2000) can distinguish among a range of communicative intents not modeled by our approach, and slot-filling representations have historically been easier to predict (Zue et al., 1994) and annotate (Byrne et al., 2019). But while recent variants support interaction between related slots (Budzianowski et al., 2018) and fixed-depth hierarchies of slots (Gupta et al., 2018), modern slot-filling approaches remain limited in their support for semantic compositionality. By contrast, our approach supports user requests corresponding to general compositional programs. Conclusions We have presented a representational framework for task-oriented dialogue modeling based on dataflow graphs, in which dialogue agents predict a sequence of compositional updates to a graphical state repres"
2021.findings-emnlp.322,W11-2903,0,0.0860434,"Missing"
2021.findings-emnlp.322,P96-1025,0,0.201479,"Missing"
2021.findings-emnlp.322,P14-2102,1,0.671127,"• Linear index-grammar parsing: O n7 in Vijay Shanker and Weir (1989), sped up to O n6 by Vijay-Shanker and Weir (1993). • Lexicalized tree adjoining grammar parsing:  O n8 in Vijay-Shankar and Joshi (1985), sped  up to O n7 by Eisner and Satta (2000). Algorithmic research in natural language processing (NLP) has focused—in large part—on developing dynamic programming solutions to combinatorial problems that arise in the field (Huang, 2009). Such algorithms have been introduced over the years for countless linguistic formalisms, such as finite-state transduction (Mohri, 1997; Eisner, 2002; Cotterell et al., 2014), context-free parsing (Stolcke, 1995; Goodman, 1999), dependency parsing (Eisner, 1996; Koo and Collins, 2010; Ma and Zhao, 2012) and mildly context-sensitive parsing (Vijay-Shanker and Weir, 1989, 1990; Kuhlmann et al., 2018). In recent years, the same algorithms have often been used for deep structured prediction, 1 Many of these examples were brought to our attention using a neural scoring function that decomposes in the works of Eisner and Blatz (2007) and Gildea (2011); over the structure (Durrett and Klein, 2015; Rastogi further discussion can be found therein. 3812 Findings of the Asso"
2021.findings-emnlp.322,N09-1026,0,0.040575,"ived using our methodology assume that relations are dense. Often relations are statically known to be sparse. Many low-level details affect actual execution time, but do not matter for asymptotic complexity. For example, memory layouts (e.g., row-order or column-order layout of a dense array in memory), sparse vs. dense representations of relations (e.g., hash tables vs. arrays), and indexes on relations (including sorted order) can have a dramatic effect on the running time in practice. However, they will not manifest in the degree analysis (e.g., Bilmes et al. (1997); Dunlop et al. (2011); DeNero et al. (2009); Lopez (2007)). Such choices are out of the control of our specific search space, but they may interact with the program in ways that are not represented in the degree. An obvious alternative cost function would be the empirical execution time of executing the transformed program on a workload of representative inputs (e.g., running a transformed parser on actual sentences from the Penn Treebank (Marcus et al., 1993)). But as we noted earlier, such a cost function might be impractically expensive. For example, evaluating the degree of a degree-1000 program is linear in the size of the program"
2021.findings-emnlp.322,C12-2077,0,0.012458,"tree adjoining grammar parsing:  O n8 in Vijay-Shankar and Joshi (1985), sped  up to O n7 by Eisner and Satta (2000). Algorithmic research in natural language processing (NLP) has focused—in large part—on developing dynamic programming solutions to combinatorial problems that arise in the field (Huang, 2009). Such algorithms have been introduced over the years for countless linguistic formalisms, such as finite-state transduction (Mohri, 1997; Eisner, 2002; Cotterell et al., 2014), context-free parsing (Stolcke, 1995; Goodman, 1999), dependency parsing (Eisner, 1996; Koo and Collins, 2010; Ma and Zhao, 2012) and mildly context-sensitive parsing (Vijay-Shanker and Weir, 1989, 1990; Kuhlmann et al., 2018). In recent years, the same algorithms have often been used for deep structured prediction, 1 Many of these examples were brought to our attention using a neural scoring function that decomposes in the works of Eisner and Blatz (2007) and Gildea (2011); over the structure (Durrett and Klein, 2015; Rastogi further discussion can be found therein. 3812 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3812–3830 November 7–11, 2021. ©2021 Association for Computational Lingui"
2021.findings-emnlp.322,J93-2004,0,0.074266,"orted order) can have a dramatic effect on the running time in practice. However, they will not manifest in the degree analysis (e.g., Bilmes et al. (1997); Dunlop et al. (2011); DeNero et al. (2009); Lopez (2007)). Such choices are out of the control of our specific search space, but they may interact with the program in ways that are not represented in the degree. An obvious alternative cost function would be the empirical execution time of executing the transformed program on a workload of representative inputs (e.g., running a transformed parser on actual sentences from the Penn Treebank (Marcus et al., 1993)). But as we noted earlier, such a cost function might be impractically expensive. For example, evaluating the degree of a degree-1000 program is linear in the size of the program, whereas evalu 1000 ating the wallclock time is O η . Optimizing the program degree is a crucial design choice as it enables a more exhaustive search in practice. Additionally, it sidesteps the need to optimize for a specific workload. However, in future work, we would like to investigate hybrid search algorithms (e.g., Song et al. (2019)) that do attempt to minimize empirical execution time, but replace some of the"
2021.findings-emnlp.322,P17-1076,0,0.0118469,"mations to improve its running time as much as possible. To this end, we describe a set of program transformations, a simple metric for assessing the efficiency of a transformed program, and a heuristic search procedure to improve this metric. We show that in practice, automated search—like the mental search performed by human programmers—can find substantial improvements to the initial program. Empirically, we show that many speed-ups described in the NLP literature could have been discovered automatically by our system. 1 Introduction et al., 2016; Lee et al., 2016; Dozat and Manning, 2017; Stern et al., 2017; Kim et al., 2017; Hong and Huang, 2018; Wu et al., 2018; Wu and Cotterell, 2019; Qi et al., 2020; Rush, 2020). When a dynamic programming algorithm for a new problem is first introduced in the literature, its runtime may not be optimal—faster versions are often published over time. Indeed, the process of introducing a first algorithm and subsequently finding improvements is common throughout computer science. In the case of dynamic programming, there are program transformations that may be exploited to derive algorithms with a faster runtime (Eisner and Blatz, 2007). These transformations ma"
2021.findings-emnlp.322,J95-2002,0,0.670937,"Shanker and Weir (1989), sped up to O n6 by Vijay-Shanker and Weir (1993). • Lexicalized tree adjoining grammar parsing:  O n8 in Vijay-Shankar and Joshi (1985), sped  up to O n7 by Eisner and Satta (2000). Algorithmic research in natural language processing (NLP) has focused—in large part—on developing dynamic programming solutions to combinatorial problems that arise in the field (Huang, 2009). Such algorithms have been introduced over the years for countless linguistic formalisms, such as finite-state transduction (Mohri, 1997; Eisner, 2002; Cotterell et al., 2014), context-free parsing (Stolcke, 1995; Goodman, 1999), dependency parsing (Eisner, 1996; Koo and Collins, 2010; Ma and Zhao, 2012) and mildly context-sensitive parsing (Vijay-Shanker and Weir, 1989, 1990; Kuhlmann et al., 2018). In recent years, the same algorithms have often been used for deep structured prediction, 1 Many of these examples were brought to our attention using a neural scoring function that decomposes in the works of Eisner and Blatz (2007) and Gildea (2011); over the structure (Durrett and Klein, 2015; Rastogi further discussion can be found therein. 3812 Findings of the Association for Computational Linguistics"
2021.findings-emnlp.322,P85-1011,0,0.399947,"Missing"
2021.findings-emnlp.322,W89-0218,0,0.0916546,"and Joshi (1985), sped  up to O n7 by Eisner and Satta (2000). Algorithmic research in natural language processing (NLP) has focused—in large part—on developing dynamic programming solutions to combinatorial problems that arise in the field (Huang, 2009). Such algorithms have been introduced over the years for countless linguistic formalisms, such as finite-state transduction (Mohri, 1997; Eisner, 2002; Cotterell et al., 2014), context-free parsing (Stolcke, 1995; Goodman, 1999), dependency parsing (Eisner, 1996; Koo and Collins, 2010; Ma and Zhao, 2012) and mildly context-sensitive parsing (Vijay-Shanker and Weir, 1989, 1990; Kuhlmann et al., 2018). In recent years, the same algorithms have often been used for deep structured prediction, 1 Many of these examples were brought to our attention using a neural scoring function that decomposes in the works of Eisner and Blatz (2007) and Gildea (2011); over the structure (Durrett and Klein, 2015; Rastogi further discussion can be found therein. 3812 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3812–3830 November 7–11, 2021. ©2021 Association for Computational Linguistics fold(1, [1, 2]) elim(4) start β(X,I,K) += γ(X,Y,Z) * β(Y,I,J)"
2021.findings-emnlp.322,P90-1001,0,0.419344,"Missing"
2021.findings-emnlp.322,J93-4002,0,0.57893,"thmist. Consider the following instances1 of published dynamic programs whose runtime bounds were later improved using specific applications of the program transformations mentioned above. • Projective dependency parsing: Collins (1996)  5 gave an  O n algorithm that was sped up to O n4 by Eisner and Satta (1999). • Split-head-factored dependency  parsing: imple5 ; with some efmented na¨ıvely runs in O n  fort, an O n3 algorithm can be derived (Eisner, 1996; Johnson, 2007; Eisner and Blatz, 2007).  • Linear index-grammar parsing: O n7 in Vijay Shanker and Weir (1989), sped up to O n6 by Vijay-Shanker and Weir (1993). • Lexicalized tree adjoining grammar parsing:  O n8 in Vijay-Shankar and Joshi (1985), sped  up to O n7 by Eisner and Satta (2000). Algorithmic research in natural language processing (NLP) has focused—in large part—on developing dynamic programming solutions to combinatorial problems that arise in the field (Huang, 2009). Such algorithms have been introduced over the years for countless linguistic formalisms, such as finite-state transduction (Mohri, 1997; Eisner, 2002; Cotterell et al., 2014), context-free parsing (Stolcke, 1995; Goodman, 1999), dependency parsing (Eisner, 1996; Koo and"
2021.naacl-main.405,P08-2007,0,0.0642436,"utoregressive models, regardless of how much computation and training data are used to learn the model parameters. That is, for an NP-hard problem, scoring a string x under a standard autoregressive model ?(x) cannot be used to verify a witness. Nor can finding a witness be solved by prompting such a model with a description of a problem instance and sampling a continuation x of that string. Such problems are abundant in NLP: for example, surface realization under Optimality Theory (Idsardi, 2006), decoding text from an AMR parse (Cai and Knight, 2013), phrase alignment between two sentences (DeNero and Klein, 2008), and in general inference for propositional logic (Cook, 1971), which underlies the NP-hardness of general natural language inference, as in Figure 1. In other words, our results imply that standard autoregressive models do not have the right structure to capture important linguistic regularities: e.g., that observed sequences were in fact constructed to be phonologically optimal, expressive of a semantic form, or logically coherent! Our work is also relevant to autoregressive models of fixed-dimensional vectors, such as NADE (Uria et al., 2016). These can be extended to arbitrary ?dimensiona"
2021.naacl-main.405,N18-1205,0,0.139141,"achine ? ? (similar to ?˜ ? in §2.3) in time ? (poly(?)) • ?˜ is normalizable (so ? exists) • ? ? maps xˆ ? ↦→ ?(? |xˆ ) for all ? ∈ ? ∪ {$} and all prefixes xˆ ∈ ? ∗ with |xˆ |≤ ? and ? ( xˆ ) > 0 7An autoregressive model architecture generally defines ?(x) as an efficiently computable (§2.2) product of local conditional probabilities. However, the parametrization usually ensures only P that ? ∈? ?? (? |xˆ ) = 1 for all prefixes xˆ . Some parameter settings may give rise to inconsistent distributions where ? , P x∈? ∗ ?? (x) < 1 because the generative process terminates with probability < 1 (Chen et al., 2018). In this case, the factors ?? (? |xˆ ) defined by the autoregressive model are not actually the conditional probabilities of the weighted language (as defined by §2.1). It is true that training ? with a likelihood objective does encourage finding a weighted language whose generative process always terminates (hence ? = 1), since this is the behavior observed in the training corpus (Chi and Geman, 1998; Chen et al., 2018; Welleck et al., 2020). Our definitions of ELN(CP) models require the actual conditional probabilities to be efficiently computable. Autoregressive models that do not sum to 1"
2021.naacl-main.405,N16-1076,1,0.81063,"wever, light marginalizations of ELNCP distributions are more powerful still,16 and can have any language ∈ NP or even NP/poly (§2.4) as support: Theorem 8. The following statements are equivalent for any nonempty ? ⊆ ? ∗ : (a) ? ∈ NP/poly. (b) ? is the support of a light marginalization of an ELNCP distribution. (c) ? is the support of a light marginalization of an ECCP weighted language. Theorems 7 and 8 make use of unrestricted latentvariable autoregressive models. There exist more practical restricted families of such models that admit tractable computation of ?(x) (Lafferty et al., 2001; Rastogi et al., 2016; Wu et al., 2018; Buys and Blunsom, 2018). Such models are EC (and indeed, typically ELN) — but this limits their expressivity, by Theorem 1. Both Lin et al. (2019) and Buys and Blunsom (2018) observed that such models yield worse empirical results than models that do not have tractable exact inference methods. The tractability requirement is dropped in “self-talk” (blixt, 2020; Gontier et al., 2020; Shwartz et al., 2020), where a neural autoregressive language model generates an analysis of the prefix xˆ via latent intermediate symbols before predicting the next output symbol.17 We remark th"
2021.naacl-main.405,D08-1006,0,0.0624093,"es We now discuss alternative families of sequence distributions that trade away efficiency or compactness in exchange for greater capacity, as shown in Table 1. 12Dropping the normalization requirement on the approximated P local probabilities (so that possibly ? ∈? ?(? |xˆ ) ≠ 1) does not help. Otherwise, again, Sat could be solved in polynomial time (with the help of polysize advice strings) by using ?(1 |? 0 ) to determine in the proof of Theorem 1 whether ?(1 |? 0 ) > 0. 4.1 Energy-based models (EBMs) Energy-based models (LeCun et al., 2006) of discrete sequences (Rosenfeld et al., 2001; Sandbank, 2008; Huang et al., 2018) traditionally refer to the EC models of §2.2. Only the unnormalized probabilities ?˜? (x) are required to be efficiently computable. Lemmas 1 and 2 showed that this model family contains all ELN languages and can achieve any support in P. Theorem 1 shows that it also contains languages that are not ELN or even ELNCP: intuitively, the reason is that the sums ? ( xˆ ) needed to compute the local normalizing constants (see §2.1) can be intractable. If we generalize energy-based sequence models to include all ECCP models — that is, we allow nonuniform computation with compact"
2021.naacl-main.405,2020.emnlp-main.448,0,0.0337683,"parameter settings may give rise to inconsistent distributions where ? , P x∈? ∗ ?? (x) < 1 because the generative process terminates with probability < 1 (Chen et al., 2018). In this case, the factors ?? (? |xˆ ) defined by the autoregressive model are not actually the conditional probabilities of the weighted language (as defined by §2.1). It is true that training ? with a likelihood objective does encourage finding a weighted language whose generative process always terminates (hence ? = 1), since this is the behavior observed in the training corpus (Chi and Geman, 1998; Chen et al., 2018; Welleck et al., 2020). Our definitions of ELN(CP) models require the actual conditional probabilities to be efficiently computable. Autoregressive models that do not sum to 1, whose normalized probabilities can be uncomputable, are not ruled out by our theorems that concern ELN(CP). • ? ? runs on those inputs xˆ ? in time ? (poly(?)) If there is ? q that efficiently locally normalizes a weighted language ?˜ with compact parameters ?q , we say ?˜ is efficiently locally normalizable with compact parameters, or ELNCP. Note that this is a property of the weighted language itself. In this case, it is obvious that ?˜ is"
2021.naacl-main.405,D18-1473,0,0.0184289,"zations of ELNCP distributions are more powerful still,16 and can have any language ∈ NP or even NP/poly (§2.4) as support: Theorem 8. The following statements are equivalent for any nonempty ? ⊆ ? ∗ : (a) ? ∈ NP/poly. (b) ? is the support of a light marginalization of an ELNCP distribution. (c) ? is the support of a light marginalization of an ECCP weighted language. Theorems 7 and 8 make use of unrestricted latentvariable autoregressive models. There exist more practical restricted families of such models that admit tractable computation of ?(x) (Lafferty et al., 2001; Rastogi et al., 2016; Wu et al., 2018; Buys and Blunsom, 2018). Such models are EC (and indeed, typically ELN) — but this limits their expressivity, by Theorem 1. Both Lin et al. (2019) and Buys and Blunsom (2018) observed that such models yield worse empirical results than models that do not have tractable exact inference methods. The tractability requirement is dropped in “self-talk” (blixt, 2020; Gontier et al., 2020; Shwartz et al., 2020), where a neural autoregressive language model generates an analysis of the prefix xˆ via latent intermediate symbols before predicting the next output symbol.17 We remark that for autoregres"
2021.naacl-main.410,P14-1023,0,0.0451188,"rly as good as informed initialization. and paraphrasing based methods to automatically augment the prompt sets. Finding out what young children know is difficult because they can be very sensitive to the form of the question (Donaldson, 1978). Opinion polling is also sensitive to question design (Broughton, 1995). We observe that when we are querying an LM rather than a human, we have the opportunity to tune prompts using gradient descent—the workhorse of modern NLP—so that they better elicit the desired type of knowledge. A neural LM sees the prompt as a sequence of continuous word vectors (Baroni et al., 2014). We tune in this continuous space, relaxing the constraint that the vectors be the embeddings of actual English words. Allowing “soft prompts” consisting of “soft words” is not only convenient for optimization, but is also more expressive. Soft prompts can emphasize particular words (by lengthening their vectors) or particular dimensions of those words. They can also adjust words that are misleading, ambiguous, or overly specific. Consider the following prompt for the relation date-of-death: x performed until his death in y. This prompt may work for the male singer Cab Calloway, but if we wan"
2021.naacl-main.410,N19-1423,0,0.249535,"ing of “soft words” is not only convenient for optimization, but is also more expressive. Soft prompts can emphasize particular words (by lengthening their vectors) or particular dimensions of those words. They can also adjust words that are misleading, ambiguous, or overly specific. Consider the following prompt for the relation date-of-death: x performed until his death in y. This prompt may work for the male singer Cab Calloway, but if we want it to also work for the Pretrained language models, such as ELMo (Pe- female painter Mary Cassatt, it might help to soften ters et al., 2018), BERT (Devlin et al., 2019), and “performed” and “his” so that they do not insist on BART (Lewis et al., 2020a), have proved to pro- the wrong occupation and gender, and perhaps to vide useful representations for other NLP tasks. Re- soften “until” into a weaker connective (as Cassatt cently, Petroni et al. (2019) and Jiang et al. (2020) was in fact too blind to paint in her final years). Another way to bridge between these cases is to demonstrated that language models (LMs) also conhave one prompt using “performed” and another tain factual and commonsense knowledge that can using “painted.” In general, there may be man"
2021.naacl-main.410,2020.tacl-1.28,0,0.160911,"t for the relation date-of-death: x performed until his death in y. This prompt may work for the male singer Cab Calloway, but if we want it to also work for the Pretrained language models, such as ELMo (Pe- female painter Mary Cassatt, it might help to soften ters et al., 2018), BERT (Devlin et al., 2019), and “performed” and “his” so that they do not insist on BART (Lewis et al., 2020a), have proved to pro- the wrong occupation and gender, and perhaps to vide useful representations for other NLP tasks. Re- soften “until” into a weaker connective (as Cassatt cently, Petroni et al. (2019) and Jiang et al. (2020) was in fact too blind to paint in her final years). Another way to bridge between these cases is to demonstrated that language models (LMs) also conhave one prompt using “performed” and another tain factual and commonsense knowledge that can using “painted.” In general, there may be many varbe elicited with a prompt. For example, to query ied lexical patterns that signal a particular relation, the date-of-birth of Mozart, we can use the and having more patterns will get better coverage prompt “Mozart ,” where we have Mozart was born in (Hearst, 1992; Riloff and Jones, 1999). We therefilled th"
2021.naacl-main.410,N16-1030,0,0.0164097,"mpts themselves. “Probing” systems that ask what language models know about particular sentences (e.g., Eichler et al., 2019) usually use feedforward networks rather than further natural-language prompts. Yet Shin et al. (2020) show how to use naturallanguage prompts to ask about particular sentences. Our method could potentially be applied to those prompts, or to “few-shot learning” prompts that include input-output examples (Brown et al., 2020). Factual knowledge is traditionally extracted from large corpora using a pipeline of NLP tools (Surdeanu and Ji, 2014), including entity extraction (Lample et al., 2016), entity linking (Rao et al., 2013) and relation extraction (Sorokin and Gurevych, 2017). However, recent work has shown that simply training a system to complete sentences—language 3 Method modeling—causes it to implicitly acquire nonlinguistic abilities from its training corpora (Rogers Our experiments will specifically aim at extracting et al., 2020), including factual knowledge (Petroni relational knowledge from language models. We et al., 2019; Jiang et al., 2020), common sense are given a fixed pretrained LM, a specific binary (Bisk et al., 2019), reasoning (Talmor et al., 2020; relation"
2021.naacl-main.410,2020.acl-main.703,0,0.448295,"sive. Soft prompts can emphasize particular words (by lengthening their vectors) or particular dimensions of those words. They can also adjust words that are misleading, ambiguous, or overly specific. Consider the following prompt for the relation date-of-death: x performed until his death in y. This prompt may work for the male singer Cab Calloway, but if we want it to also work for the Pretrained language models, such as ELMo (Pe- female painter Mary Cassatt, it might help to soften ters et al., 2018), BERT (Devlin et al., 2019), and “performed” and “his” so that they do not insist on BART (Lewis et al., 2020a), have proved to pro- the wrong occupation and gender, and perhaps to vide useful representations for other NLP tasks. Re- soften “until” into a weaker connective (as Cassatt cently, Petroni et al. (2019) and Jiang et al. (2020) was in fact too blind to paint in her final years). Another way to bridge between these cases is to demonstrated that language models (LMs) also conhave one prompt using “performed” and another tain factual and commonsense knowledge that can using “painted.” In general, there may be many varbe elicited with a prompt. For example, to query ied lexical patterns that si"
2021.naacl-main.410,N18-1202,0,0.0859004,"Missing"
2021.naacl-main.410,D19-1250,0,0.404036,"nsider the following prompt for the relation date-of-death: x performed until his death in y. This prompt may work for the male singer Cab Calloway, but if we want it to also work for the Pretrained language models, such as ELMo (Pe- female painter Mary Cassatt, it might help to soften ters et al., 2018), BERT (Devlin et al., 2019), and “performed” and “his” so that they do not insist on BART (Lewis et al., 2020a), have proved to pro- the wrong occupation and gender, and perhaps to vide useful representations for other NLP tasks. Re- soften “until” into a weaker connective (as Cassatt cently, Petroni et al. (2019) and Jiang et al. (2020) was in fact too blind to paint in her final years). Another way to bridge between these cases is to demonstrated that language models (LMs) also conhave one prompt using “performed” and another tain factual and commonsense knowledge that can using “painted.” In general, there may be many varbe elicited with a prompt. For example, to query ied lexical patterns that signal a particular relation, the date-of-birth of Mozart, we can use the and having more patterns will get better coverage prompt “Mozart ,” where we have Mozart was born in (Hearst, 1992; Riloff and Jones,"
2021.naacl-main.410,D17-1188,0,0.0200953,"cular sentences (e.g., Eichler et al., 2019) usually use feedforward networks rather than further natural-language prompts. Yet Shin et al. (2020) show how to use naturallanguage prompts to ask about particular sentences. Our method could potentially be applied to those prompts, or to “few-shot learning” prompts that include input-output examples (Brown et al., 2020). Factual knowledge is traditionally extracted from large corpora using a pipeline of NLP tools (Surdeanu and Ji, 2014), including entity extraction (Lample et al., 2016), entity linking (Rao et al., 2013) and relation extraction (Sorokin and Gurevych, 2017). However, recent work has shown that simply training a system to complete sentences—language 3 Method modeling—causes it to implicitly acquire nonlinguistic abilities from its training corpora (Rogers Our experiments will specifically aim at extracting et al., 2020), including factual knowledge (Petroni relational knowledge from language models. We et al., 2019; Jiang et al., 2020), common sense are given a fixed pretrained LM, a specific binary (Bisk et al., 2019), reasoning (Talmor et al., 2020; relation r such as date-of-death, and a trainBrown et al., 2020), summarization (Radford et al.,"
C00-1038,P97-1040,1,0.926432,"nally awkward. We propose replacing these unbounded constraints, as well as non- nite-state Generalized Alignment constraints, with a new class of nite-state directional constraints. We give linguistic applications, results on generative power, and algorithms to compile grammars into transducers. 1 Introduction Optimality Theory is a grammar framework that directly expresses constraints on phonological forms. Roughly, the grammar prefers forms that violate each constraint as little as possible. Most constraints used in practice describe disfavored local con gurations in the phonological form (Eisner, 1997a). It is therefore possible for a given form to o end a single constraint at several locations in the form. (For example, a constraint against syllable codas will be offended by every syllable that has a coda.) When comparing forms, then, how do we aggregate a form&apos;s multiple local o enses into an overall violation level? A constraint could answer this question in at least three ways, the third being our proposal:  Unbounded evaluation (Prince and Smolensky, 1993). A form&apos;s violation level is given by the number of o enses. Forms with fewer o enses are preferred.  Bounded evaluation (Frank"
C00-1038,C94-2163,0,0.210244,"er a oating tone on a phrase, no hierarchy of nite-state unbounded constraints could even de ne the same optimal candidates as a GA constraint. Thus GA cannot be simulated in Ellison&apos;s (1994) nite-state framework (x3.2). For this reason, as well as the awkwardness and non-locality of evaluating GA o enses, we propose to replace GA with directional constraints. Directional constraints appear to more directly capture the observed phenomena. We do note that another, trickier possibility is to eliminate GA in favor of ordinary unbounded constraints that are indi erent to the location of o enses. (Ellison, 1994) noted that GA constraints that evaluated the placement of only one element (e.g., primary stress) could be replaced by simpler NoIntervening constraints. (Eisner, 1997b) gives a GA-free treatment of the metrical stress typology of (Hayes, 1995). 3 2.4 Generative power It has recently been proposed that for computational reasons, OT should eliminate not only GA but all unbounded constraints (Frank and Satta, 1998; Karttunen, 1998). As with GA, we o er the less extreme approach of replacing them with directional constraints instead. Recall that a phonological grammar, as usually conceived, is a"
C00-1038,J98-2006,0,0.344534,", 1997a). It is therefore possible for a given form to o end a single constraint at several locations in the form. (For example, a constraint against syllable codas will be offended by every syllable that has a coda.) When comparing forms, then, how do we aggregate a form&apos;s multiple local o enses into an overall violation level? A constraint could answer this question in at least three ways, the third being our proposal:  Unbounded evaluation (Prince and Smolensky, 1993). A form&apos;s violation level is given by the number of o enses. Forms with fewer o enses are preferred.  Bounded evaluation (Frank and Satta, 1998; Karttunen, 1998). A form&apos;s violation level is min(k; number of o enses) for some k. This is like unbounded evaluation except that the constraint does not distinguish among forms with  k o enses.  Directional evaluation. A form&apos;s violation level considers the location of offenses, not their total number. Under left1  I am grateful to the 3 anonymous referees for feedback. 1 Note that k = 1 gives inary&quot; constraints that can be described simply as languages. Any k-bounded constraint can easily be simulated by k binary constraints. to-right evaluation, the constraint prefers forms whose o e"
C00-1038,J94-3001,0,0.182644,"(Eisner, 1997b) gives a GA-free treatment of the metrical stress typology of (Hayes, 1995). 3 2.4 Generative power It has recently been proposed that for computational reasons, OT should eliminate not only GA but all unbounded constraints (Frank and Satta, 1998; Karttunen, 1998). As with GA, we o er the less extreme approach of replacing them with directional constraints instead. Recall that a phonological grammar, as usually conceived, is a description of permissible (UR, SR) pairs. It has long been believed that naturally occurring phonological grammars are regular relations (Johnson, 1972; Kaplan and Kay, 1994). This means that they can be implemented as nite-state transducers (FSTs) that accept exactly the grammatical pairs. FSTs are immensely useful in perform4 3 4 This is indeed too powerful: centering is unattested. UR = underlying representation, SR = surface repn. ing many relevant tasks rapidly: generation (obtaining all possible SRs for a UR), comprehension (conversely), characterizing the set of forms on which two grammars (perhaps from di erent descriptive frameworks) would di er, etc. Moreover, FSTs can be applied in parallel to regular sets of forms. For example, one can obtain a weighte"
C00-1038,W98-1301,0,0.63716,"re possible for a given form to o end a single constraint at several locations in the form. (For example, a constraint against syllable codas will be offended by every syllable that has a coda.) When comparing forms, then, how do we aggregate a form&apos;s multiple local o enses into an overall violation level? A constraint could answer this question in at least three ways, the third being our proposal:  Unbounded evaluation (Prince and Smolensky, 1993). A form&apos;s violation level is given by the number of o enses. Forms with fewer o enses are preferred.  Bounded evaluation (Frank and Satta, 1998; Karttunen, 1998). A form&apos;s violation level is min(k; number of o enses) for some k. This is like unbounded evaluation except that the constraint does not distinguish among forms with  k o enses.  Directional evaluation. A form&apos;s violation level considers the location of offenses, not their total number. Under left1  I am grateful to the 3 anonymous referees for feedback. 1 Note that k = 1 gives inary&quot; constraints that can be described simply as languages. Any k-bounded constraint can easily be simulated by k binary constraints. to-right evaluation, the constraint prefers forms whose o enses are as late a"
C00-1038,J97-2003,0,0.0560784,"tructions for directional-style constraints (see x2.5) only nd the optimal output for a single input, or at best a nite lexicon. 3.4.3 Dir. Best Paths: A special case x3.2 restricted our FSTs such that for every arc label : , j j  1. In this section we construct Ti from T^i under the stronger assumption that j j = 1, i.e., T^i is -free on the input side. If Q is the stateset of T^i , then let the stateset of Ti be f[q; R; S ] : R  S  Q; q 2 S Rg. This has size jQj  3jQj . However, most of these states are typically unreachable from the start state. Lazy on-they&quot; construction techniques (Mohri, 1997) can be used to avoid allocating states or arcs until they are discovered during exploration from the start state. For  2  ; q 2 Q, de ne V (; q) as the minimum cost (a jj-tuple of weights) of any -reading path from T^i &apos;s start state q to q. The start state of Ti is [q ; ;; fq g]. The intent is that Ti have a path from its start state to [q; R; S ] that transduces  : Æ i  T^i has a q to q,  : Æ path of cost V (; q);  R = fq0 2 Q : V (; q0 ) < V (; q)g; and  S = fq0 2 Q : V (; q0 )  V (; q)g. So as Ti reads , it follows&quot; T^i &apos;s cheapest reading paths to q, while calculating"
C00-1038,W98-0904,0,0.0135697,"y grow quite large for long inputs like phonological phrases. state must keep track of the o ense count. Intersecting many such large constraints can produce very large FSTs|while still failing to capture simple generalizations, e.g., that all codas are dispreferred. In x3, we will show that directional constraints are more powerful than bounded constraints, as they can express such generalizations|yet they keep us within the world of regular relations and FSTs. 2.5 Related Work Walther (1999), working with intersective constraints, de nes a similar notion of Bounded Local Optimization (BLO). Trommer (1998; 1999) applies a variant of Walther&apos;s idea to OT. The motivation in both cases is linguistic. We sketch how our idea di ers via 3 examples: UR uuuuu uu uuu uuuuu candidate X vvvbb vv vbb vvvbb candidate Y vvbaa vvvvbaa vzbaa Consider b, a left-to-right constraint that is offended by each instance of b. On our proposal, candidate X wins in each column, because Y always o ends b rst, at position 3 in the UR. But under BLO, this o ense is not fatal. Y can survive b by inserting epenthetic material (column 2: Y wins by postponing b relative to the SR), or by changing v to z (column 3: Y ties X"
C10-2075,P05-1074,0,0.0655539,"pecific loss function, it is likely to perform better than maximum likelihood training. 4.2 Experimental Results We have applied the confusion-based discriminative language model (CDLM) to the IWSLT 2005 Chinese-to-English text translation task5 (Eck and Hori, 2005). We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al., 2009). 5.1 Data Partitions for Training & Testing Four kinds of data are used for CDLM training: 5 Set1 Set2 Set3 Set4 Paraphrasing Models Our method is also related to methods for training paraphrasing models (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Madnani et al., 2007). Specifically, the form of our confusion grammar is similar to that of the paraphrase model they use, and the ways of extracting the grammar/model are also similar as both employ a second language (e.g., Chinese in our case) as a pivot. However, while a “translation” rule in a paraphrase model is expected to contain a pair of phrases that are good alternatives for each other, a confusion rule in our CG is based on an MT system processing unseen test data and contains pairs of phrases that are typically bad (and only rarely good) alternatives"
C10-2075,N06-1003,0,0.0291331,"y to perform better than maximum likelihood training. 4.2 Experimental Results We have applied the confusion-based discriminative language model (CDLM) to the IWSLT 2005 Chinese-to-English text translation task5 (Eck and Hori, 2005). We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al., 2009). 5.1 Data Partitions for Training & Testing Four kinds of data are used for CDLM training: 5 Set1 Set2 Set3 Set4 Paraphrasing Models Our method is also related to methods for training paraphrasing models (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Madnani et al., 2007). Specifically, the form of our confusion grammar is similar to that of the paraphrase model they use, and the ways of extracting the grammar/model are also similar as both employ a second language (e.g., Chinese in our case) as a pivot. However, while a “translation” rule in a paraphrase model is expected to contain a pair of phrases that are good alternatives for each other, a confusion rule in our CG is based on an MT system processing unseen test data and contains pairs of phrases that are typically bad (and only rarely good) alternatives for each other. The motivati"
C10-2075,N09-1025,0,0.0284366,"few non-identity rules. See (Li, 2010) for further discussion. 659 of N (y). In other recent work (Li et al., 2010), we have taken the round-trip view more seriously, by imputing likely source sentences x and translating them back to separate, weighted confusion forests N (y), without any same-segmentation constraint. 3.3 Confusion-based Discriminative Training With the training sentences yi and their simulated confusion sets N (yi ) — represented as hypergraphs D(yi )) — we can perform the discriminative training using any of a number of procedures such as MERT (Och, 2003) or MIRA as used by Chiang et al. (2009). In our paper, we use hypergraph-based minimum risk (Li and Eisner, 2009), X θ∗ = arg min Riskθ (yi ) (5) θ = arg min θ i XX L(Y(d), yi )pθ (d |D(yi )), i d∈D(yi ) where L(y 0 , yi ) is the loss (e.g negated BLEU) incurred by producing y 0 when the true answer is yi , Y(d) is the English yield of a derivation d, and pθ (d |D(yi )) is defined as, ef (d)·θ , f (d)·θ d∈D(yi ) e pθ (d |D(yi )) = P (6) where f (d) is a feature vector over d. We will specify the features in Section 5, but in general they should be defined such that the training will be efficient and the actual MT decoding can use t"
C10-2075,J07-2003,0,0.518374,"008) for MT. Note that identifying the best candidate requires supervised training data— bilingual text in case of MT—which is expensive in many domains (e.g. weblog or newsgroup) and for most language pairs (e.g. Urdu-English). We propose a novel discriminative LM in this paper: a globally normalized log-linear LM that can be trained in an efficient and unsupervised manner, using only monolingual (English) text. The main idea is to exploit (translation) uncertainties inherent in an MT system to derive an English-to-English confusion grammar (CG), illustrated in this paper for a Hiero system (Chiang, 2007). From the bilingual synchronous context-free grammar (SCFG) used in Hiero, we extract a monolingual SCFG, with rules of the kind, X → hstrong tea, powerful teai or 656 Coling 2010: Poster Volume, pages 656–664, Beijing, August 2010 X → hin X1 , in the X1 i. Thus our CG is also an SCFG that generates pairs of English sentences that differ from each other in ways that alternative English hypothesis considered during translation would differ from each other. This CG is then used to “translate” each sentence in the LM training corpus into what we call its confusion set — a set of other “sentences"
C10-2075,2005.iwslt-1.1,0,0.0566575,"y the CG, is automatically learnt (e.g., from the bilingual grammar) and specific to the MT system being used. Therefore, our neighborhood function is more likely to be informative and adaptive to the task. Secondly, when tuning θ, CE uses the maximum likelihood training, but we use the minimum 660 risk training of (5). Since our training uses a taskspecific loss function, it is likely to perform better than maximum likelihood training. 4.2 Experimental Results We have applied the confusion-based discriminative language model (CDLM) to the IWSLT 2005 Chinese-to-English text translation task5 (Eck and Hori, 2005). We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al., 2009). 5.1 Data Partitions for Training & Testing Four kinds of data are used for CDLM training: 5 Set1 Set2 Set3 Set4 Paraphrasing Models Our method is also related to methods for training paraphrasing models (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Madnani et al., 2007). Specifically, the form of our confusion grammar is similar to that of the paraphrase model they use, and the ways of extracting the grammar/model are also similar as both employ a"
C10-2075,D09-1005,1,0.846575,"). In other recent work (Li et al., 2010), we have taken the round-trip view more seriously, by imputing likely source sentences x and translating them back to separate, weighted confusion forests N (y), without any same-segmentation constraint. 3.3 Confusion-based Discriminative Training With the training sentences yi and their simulated confusion sets N (yi ) — represented as hypergraphs D(yi )) — we can perform the discriminative training using any of a number of procedures such as MERT (Och, 2003) or MIRA as used by Chiang et al. (2009). In our paper, we use hypergraph-based minimum risk (Li and Eisner, 2009), X θ∗ = arg min Riskθ (yi ) (5) θ = arg min θ i XX L(Y(d), yi )pθ (d |D(yi )), i d∈D(yi ) where L(y 0 , yi ) is the loss (e.g negated BLEU) incurred by producing y 0 when the true answer is yi , Y(d) is the English yield of a derivation d, and pθ (d |D(yi )) is defined as, ef (d)·θ , f (d)·θ d∈D(yi ) e pθ (d |D(yi )) = P (6) where f (d) is a feature vector over d. We will specify the features in Section 5, but in general they should be defined such that the training will be efficient and the actual MT decoding can use them conveniently. The objective of (5) is differentiable and thus we can o"
C10-2075,2008.amta-papers.12,1,0.88869,"criptive models of text. However, in a task like Chinese-to-English MT, the de facto role of the LM is to discriminate among the alternative English translations being contemplated by the MT system for a particular Chinese input sentence. We call the set of such alternative translations a confusion set. Since a confusion set is typically a minuscule subset of the set of all possible word sequences, it is arguably better to train the LM parameters so as to make the best candidate in the confusion set more likely than its competitors, as done by Roark et al. (2004) for speech recognition and by Li and Khudanpur (2008) for MT. Note that identifying the best candidate requires supervised training data— bilingual text in case of MT—which is expensive in many domains (e.g. weblog or newsgroup) and for most language pairs (e.g. Urdu-English). We propose a novel discriminative LM in this paper: a globally normalized log-linear LM that can be trained in an efficient and unsupervised manner, using only monolingual (English) text. The main idea is to exploit (translation) uncertainties inherent in an MT system to derive an English-to-English confusion grammar (CG), illustrated in this paper for a Hiero system (Chia"
C10-2075,W09-0424,1,0.919782,"Missing"
C10-2075,W07-0716,0,0.0153995,"mum likelihood training. 4.2 Experimental Results We have applied the confusion-based discriminative language model (CDLM) to the IWSLT 2005 Chinese-to-English text translation task5 (Eck and Hori, 2005). We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al., 2009). 5.1 Data Partitions for Training & Testing Four kinds of data are used for CDLM training: 5 Set1 Set2 Set3 Set4 Paraphrasing Models Our method is also related to methods for training paraphrasing models (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Madnani et al., 2007). Specifically, the form of our confusion grammar is similar to that of the paraphrase model they use, and the ways of extracting the grammar/model are also similar as both employ a second language (e.g., Chinese in our case) as a pivot. However, while a “translation” rule in a paraphrase model is expected to contain a pair of phrases that are good alternatives for each other, a confusion rule in our CG is based on an MT system processing unseen test data and contains pairs of phrases that are typically bad (and only rarely good) alternatives for each other. The motivation and goal are also di"
C10-2075,P03-1021,0,0.106266,"small test set and hence has few non-identity rules. See (Li, 2010) for further discussion. 659 of N (y). In other recent work (Li et al., 2010), we have taken the round-trip view more seriously, by imputing likely source sentences x and translating them back to separate, weighted confusion forests N (y), without any same-segmentation constraint. 3.3 Confusion-based Discriminative Training With the training sentences yi and their simulated confusion sets N (yi ) — represented as hypergraphs D(yi )) — we can perform the discriminative training using any of a number of procedures such as MERT (Och, 2003) or MIRA as used by Chiang et al. (2009). In our paper, we use hypergraph-based minimum risk (Li and Eisner, 2009), X θ∗ = arg min Riskθ (yi ) (5) θ = arg min θ i XX L(Y(d), yi )pθ (d |D(yi )), i d∈D(yi ) where L(y 0 , yi ) is the loss (e.g negated BLEU) incurred by producing y 0 when the true answer is yi , Y(d) is the English yield of a derivation d, and pθ (d |D(yi )) is defined as, ef (d)·θ , f (d)·θ d∈D(yi ) e pθ (d |D(yi )) = P (6) where f (d) is a feature vector over d. We will specify the features in Section 5, but in general they should be defined such that the training will be effici"
C10-2075,N09-1024,0,0.0361715,"e to English). Specifically, we can add the discriminative model into an MT pipeline as a feature, and tune its weight relative to other models in the MT system, including the baseline n-gram LM. 4 Related and Similar Work The detailed relation between the proposed procedure and other language modeling techniques has been discussed in Sections 1 and 2. Here, we review two other methods that are related to our method in a broader context. 4.1 Unsupervised Training of Global Log-linear Models Our method is similar to the contrastive estimation (CE) of Smith and Eisner (2005) and its successors (Poon et al., 2009). In particular, our confusion grammar is like a neighborhood function in CE. Also, our goal is to improve both efficiency and accuracy, just as CE does. However, there are two important differences. First, the neighborhood function in CE is manually created based on human insights about the particular task, while our neighborhood function, generated by the CG, is automatically learnt (e.g., from the bilingual grammar) and specific to the MT system being used. Therefore, our neighborhood function is more likely to be informative and adaptive to the task. Secondly, when tuning θ, CE uses the ma"
C10-2075,W04-3219,0,0.0334591,"raining uses a taskspecific loss function, it is likely to perform better than maximum likelihood training. 4.2 Experimental Results We have applied the confusion-based discriminative language model (CDLM) to the IWSLT 2005 Chinese-to-English text translation task5 (Eck and Hori, 2005). We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al., 2009). 5.1 Data Partitions for Training & Testing Four kinds of data are used for CDLM training: 5 Set1 Set2 Set3 Set4 Paraphrasing Models Our method is also related to methods for training paraphrasing models (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Madnani et al., 2007). Specifically, the form of our confusion grammar is similar to that of the paraphrase model they use, and the ways of extracting the grammar/model are also similar as both employ a second language (e.g., Chinese in our case) as a pivot. However, while a “translation” rule in a paraphrase model is expected to contain a pair of phrases that are good alternatives for each other, a confusion rule in our CG is based on an MT system processing unseen test data and contains pairs of phrases that are typically bad ("
C10-2075,P04-1007,0,0.29948,"as the whole-sentence model are generative or descriptive models of text. However, in a task like Chinese-to-English MT, the de facto role of the LM is to discriminate among the alternative English translations being contemplated by the MT system for a particular Chinese input sentence. We call the set of such alternative translations a confusion set. Since a confusion set is typically a minuscule subset of the set of all possible word sequences, it is arguably better to train the LM parameters so as to make the best candidate in the confusion set more likely than its competitors, as done by Roark et al. (2004) for speech recognition and by Li and Khudanpur (2008) for MT. Note that identifying the best candidate requires supervised training data— bilingual text in case of MT—which is expensive in many domains (e.g. weblog or newsgroup) and for most language pairs (e.g. Urdu-English). We propose a novel discriminative LM in this paper: a globally normalized log-linear LM that can be trained in an efficient and unsupervised manner, using only monolingual (English) text. The main idea is to exploit (translation) uncertainties inherent in an MT system to derive an English-to-English confusion grammar (C"
C10-2075,P05-1044,1,0.818509,"actual MT decoding (e.g., translating Chinese to English). Specifically, we can add the discriminative model into an MT pipeline as a feature, and tune its weight relative to other models in the MT system, including the baseline n-gram LM. 4 Related and Similar Work The detailed relation between the proposed procedure and other language modeling techniques has been discussed in Sections 1 and 2. Here, we review two other methods that are related to our method in a broader context. 4.1 Unsupervised Training of Global Log-linear Models Our method is similar to the contrastive estimation (CE) of Smith and Eisner (2005) and its successors (Poon et al., 2009). In particular, our confusion grammar is like a neighborhood function in CE. Also, our goal is to improve both efficiency and accuracy, just as CE does. However, there are two important differences. First, the neighborhood function in CE is manually created based on human insights about the particular task, while our neighborhood function, generated by the CG, is automatically learnt (e.g., from the bilingual grammar) and specific to the MT system being used. Therefore, our neighborhood function is more likely to be informative and adaptive to the task."
C10-2075,D11-1085,1,\N,Missing
C12-1154,D08-1031,0,0.861502,"ata is available. Experiments show that our system outperforms recent state-of-the-art coreference systems including Raghunathan et al.’s system as well as a competitive baseline that uses a pairwise classifier. KEYWORDS: coreference resolution, discourse processing, supervised clustering, greedy approaches. KEYWORDS IN L2 : resolución de correferencia. Proceedings of COLING 2012: Technical Papers, pages 2519–2534, COLING 2012, Mumbai, December 2012. 2519 1 Introduction Coreference resolution has traditionally benefited from machine learning approaches (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2009). Surprisingly, however, recent work has shown that simple rule-based coreference systems can compete with the state-of-the-art machine-learning-based systems if provided with rich lexical, syntactic, semantic and discourse information (Haghighi and Klein, 2010; Raghunathan et al., 2010). In fact, the rule-based system of Raghunathan et al. (2010) exhibited the top score in the recent CoNLL evaluation (Pradhan et al., 2011). This system’s innovation is to build the coreference clusters incrementally, starting with the most precise rules (dubbed sieves by Raghunathan et"
C12-1154,P06-1005,0,0.355203,"Missing"
C12-1154,P04-1015,0,0.283884,"We cannot practically enumerate every possible state even for a single training example, because there are exponentially many clusterings; and of course test examples will give rise to entirely new states. In early experiments we attempted using standard reinforcement learning algorithms such as Q-learning (Watkins and Dayan, 1992) and policy gradient (Sutton et al., 2000), but the learned weights did not lead to accurate predictions. We believe that this is due to the large variation of the values of our actions. Thus, we settled on a variant of the structured perceptron with early updates (Collins and Roark, 2004) with beam width 1. Our algorithm was inspired by the successful use of a similar algorithm by Goldberg and Elhadad (2010) for training an easy-first dependency parser. The nature of our task necessitates certain differences from both the structured perceptron and the “easy-first” algorithm of Goldberg and Elhadad. Those differences are described bellow. Algorithm 2 is our training algorithm. Overall, the algorithm monitors the agent’s behavior, and performs perceptron-style updates when the current weights would result in a mistake. Learning starts from a vector of initial weights w (0) (init"
C12-1154,N07-1011,0,0.129675,"sieves. Coreference resolution is inherently a global task: for example, discovering that Smith and she corefer makes it more probable that Smith corefers with Jane Smith (i.e., a female name) rather than Jason Smith. Furthermore, grouping Jane Smith, she and Jason Smith in the same cluster should be rejected by the clustering algorithm because it results in poor gender and proper name agreement. Yet incorporating such structured information in coreference has proven challenging. There have been several successful attempts to incorporate structured information through joint inference such as Culotta et al. (2007) and Poon and Domingos (2008), but they do not explicitly learn parameters tuned to the inference algorithm used at test time (and the latter relies on a complicated and expensive inference procedures). The difficulty of inferring globally consistent clusterings lies in the fact that there are exponentially many clusterings and producing an “optimal” clustering according to most measures of global coherence is NP-hard. In this context, the approach of Raghunathan et al. (2010) can be seen as a rule-based greedy search for a globally consistent clustering. We propose a coreference resolution ap"
C12-1154,N10-1115,0,0.0186598,"ly many clusterings; and of course test examples will give rise to entirely new states. In early experiments we attempted using standard reinforcement learning algorithms such as Q-learning (Watkins and Dayan, 1992) and policy gradient (Sutton et al., 2000), but the learned weights did not lead to accurate predictions. We believe that this is due to the large variation of the values of our actions. Thus, we settled on a variant of the structured perceptron with early updates (Collins and Roark, 2004) with beam width 1. Our algorithm was inspired by the successful use of a similar algorithm by Goldberg and Elhadad (2010) for training an easy-first dependency parser. The nature of our task necessitates certain differences from both the structured perceptron and the “easy-first” algorithm of Goldberg and Elhadad. Those differences are described bellow. Algorithm 2 is our training algorithm. Overall, the algorithm monitors the agent’s behavior, and performs perceptron-style updates when the current weights would result in a mistake. Learning starts from a vector of initial weights w (0) (initialization is discussed below) and iterates over all training documents. For each document d training begins at the start"
C12-1154,N10-1061,0,0.368714,"ering, greedy approaches. KEYWORDS IN L2 : resolución de correferencia. Proceedings of COLING 2012: Technical Papers, pages 2519–2534, COLING 2012, Mumbai, December 2012. 2519 1 Introduction Coreference resolution has traditionally benefited from machine learning approaches (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2009). Surprisingly, however, recent work has shown that simple rule-based coreference systems can compete with the state-of-the-art machine-learning-based systems if provided with rich lexical, syntactic, semantic and discourse information (Haghighi and Klein, 2010; Raghunathan et al., 2010). In fact, the rule-based system of Raghunathan et al. (2010) exhibited the top score in the recent CoNLL evaluation (Pradhan et al., 2011). This system’s innovation is to build the coreference clusters incrementally, starting with the most precise rules (dubbed sieves by Raghunathan et al. (2010)) and use the available coreference information to guide the less precise sieves. Coreference resolution is inherently a global task: for example, discovering that Smith and she corefer makes it more probable that Smith corefers with Jane Smith (i.e., a female name) rather t"
C12-1154,hasler-etal-2006-nps,0,0.0556023,"Missing"
C12-1154,N12-1015,0,0.0203651,"s noted above, our algorithm is a slight modification of the structured perceptron with early updates (Collins and Roark, 2004) for beam width 1. The difference is that upon making a mistake on a training example (in our case a training example is a document), the structured perceptron updates the weights and moves onto the next example. In contrast, we update the weights and continue working on the current example. Our subsequent updates on the example may thus have to update from one incorrect clustering to another—unlike the structured perceptron or violation-fixing perceptrons in general (Huang et al., 2012) (but like DAgger (Ross et al., 2011)). The difference is motivated by the fact that our data is highly nonseparable—it is extremely rare to get all of the coreference decisions right in a document. Thus, the algorithm needs to operate in states in which some errors were previously made. Additionally, there is a significant overhead in initializing the necessary data structures for each document. Thus, a straightforward implementation of the structured perceptron with early updates is inefficient. Goldberg and Elhadad (2010) use an update strategy similar to the structured perceptron, but keep"
C12-1154,P02-1014,0,0.846997,"for which training data is available. Experiments show that our system outperforms recent state-of-the-art coreference systems including Raghunathan et al.’s system as well as a competitive baseline that uses a pairwise classifier. KEYWORDS: coreference resolution, discourse processing, supervised clustering, greedy approaches. KEYWORDS IN L2 : resolución de correferencia. Proceedings of COLING 2012: Technical Papers, pages 2519–2534, COLING 2012, Mumbai, December 2012. 2519 1 Introduction Coreference resolution has traditionally benefited from machine learning approaches (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2009). Surprisingly, however, recent work has shown that simple rule-based coreference systems can compete with the state-of-the-art machine-learning-based systems if provided with rich lexical, syntactic, semantic and discourse information (Haghighi and Klein, 2010; Raghunathan et al., 2010). In fact, the rule-based system of Raghunathan et al. (2010) exhibited the top score in the recent CoNLL evaluation (Pradhan et al., 2011). This system’s innovation is to build the coreference clusters incrementally, starting with the most precise rules (dubbed"
C12-1154,D08-1068,0,0.228913,"ution is inherently a global task: for example, discovering that Smith and she corefer makes it more probable that Smith corefers with Jane Smith (i.e., a female name) rather than Jason Smith. Furthermore, grouping Jane Smith, she and Jason Smith in the same cluster should be rejected by the clustering algorithm because it results in poor gender and proper name agreement. Yet incorporating such structured information in coreference has proven challenging. There have been several successful attempts to incorporate structured information through joint inference such as Culotta et al. (2007) and Poon and Domingos (2008), but they do not explicitly learn parameters tuned to the inference algorithm used at test time (and the latter relies on a complicated and expensive inference procedures). The difficulty of inferring globally consistent clusterings lies in the fact that there are exponentially many clusterings and producing an “optimal” clustering according to most measures of global coherence is NP-hard. In this context, the approach of Raghunathan et al. (2010) can be seen as a rule-based greedy search for a globally consistent clustering. We propose a coreference resolution approach that like Raghunathan"
C12-1154,W11-1901,0,0.0672176,". 2519 1 Introduction Coreference resolution has traditionally benefited from machine learning approaches (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2009). Surprisingly, however, recent work has shown that simple rule-based coreference systems can compete with the state-of-the-art machine-learning-based systems if provided with rich lexical, syntactic, semantic and discourse information (Haghighi and Klein, 2010; Raghunathan et al., 2010). In fact, the rule-based system of Raghunathan et al. (2010) exhibited the top score in the recent CoNLL evaluation (Pradhan et al., 2011). This system’s innovation is to build the coreference clusters incrementally, starting with the most precise rules (dubbed sieves by Raghunathan et al. (2010)) and use the available coreference information to guide the less precise sieves. Coreference resolution is inherently a global task: for example, discovering that Smith and she corefer makes it more probable that Smith corefers with Jane Smith (i.e., a female name) rather than Jason Smith. Furthermore, grouping Jane Smith, she and Jason Smith in the same cluster should be rejected by the clustering algorithm because it results in poor g"
C12-1154,D10-1048,0,0.592793,"KEYWORDS IN L2 : resolución de correferencia. Proceedings of COLING 2012: Technical Papers, pages 2519–2534, COLING 2012, Mumbai, December 2012. 2519 1 Introduction Coreference resolution has traditionally benefited from machine learning approaches (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2009). Surprisingly, however, recent work has shown that simple rule-based coreference systems can compete with the state-of-the-art machine-learning-based systems if provided with rich lexical, syntactic, semantic and discourse information (Haghighi and Klein, 2010; Raghunathan et al., 2010). In fact, the rule-based system of Raghunathan et al. (2010) exhibited the top score in the recent CoNLL evaluation (Pradhan et al., 2011). This system’s innovation is to build the coreference clusters incrementally, starting with the most precise rules (dubbed sieves by Raghunathan et al. (2010)) and use the available coreference information to guide the less precise sieves. Coreference resolution is inherently a global task: for example, discovering that Smith and she corefer makes it more probable that Smith corefers with Jane Smith (i.e., a female name) rather than Jason Smith. Furthermor"
C12-1154,J01-4004,0,0.957519,"dapt to any dataset for which training data is available. Experiments show that our system outperforms recent state-of-the-art coreference systems including Raghunathan et al.’s system as well as a competitive baseline that uses a pairwise classifier. KEYWORDS: coreference resolution, discourse processing, supervised clustering, greedy approaches. KEYWORDS IN L2 : resolución de correferencia. Proceedings of COLING 2012: Technical Papers, pages 2519–2534, COLING 2012, Mumbai, December 2012. 2519 1 Introduction Coreference resolution has traditionally benefited from machine learning approaches (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2009). Surprisingly, however, recent work has shown that simple rule-based coreference systems can compete with the state-of-the-art machine-learning-based systems if provided with rich lexical, syntactic, semantic and discourse information (Haghighi and Klein, 2010; Raghunathan et al., 2010). In fact, the rule-based system of Raghunathan et al. (2010) exhibited the top score in the recent CoNLL evaluation (Pradhan et al., 2011). This system’s innovation is to build the coreference clusters incrementally, starting with the most p"
C12-1154,P10-2029,1,0.932026,"Missing"
C12-1154,N12-1013,1,0.871811,"Missing"
C12-1154,P09-1074,1,0.662094,"ents show that our system outperforms recent state-of-the-art coreference systems including Raghunathan et al.’s system as well as a competitive baseline that uses a pairwise classifier. KEYWORDS: coreference resolution, discourse processing, supervised clustering, greedy approaches. KEYWORDS IN L2 : resolución de correferencia. Proceedings of COLING 2012: Technical Papers, pages 2519–2534, COLING 2012, Mumbai, December 2012. 2519 1 Introduction Coreference resolution has traditionally benefited from machine learning approaches (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2009). Surprisingly, however, recent work has shown that simple rule-based coreference systems can compete with the state-of-the-art machine-learning-based systems if provided with rich lexical, syntactic, semantic and discourse information (Haghighi and Klein, 2010; Raghunathan et al., 2010). In fact, the rule-based system of Raghunathan et al. (2010) exhibited the top score in the recent CoNLL evaluation (Pradhan et al., 2011). This system’s innovation is to build the coreference clusters incrementally, starting with the most precise rules (dubbed sieves by Raghunathan et al. (2010)) and use the"
C12-1154,P03-1023,0,0.0662654,"Missing"
C12-1154,C04-1075,0,0.112302,"dness incrementally as the algorithm progresses. The model of Raghunathan et al. (2010) uses an approach based on multiple sieves of decreasing precision and increasing recall. It begins by creating a coreference chain for each mention in the document. Each sieve consists of deterministic tests that are applied to pairs of chains in the current clustering. When the tests succeed, the coreference chains in the pair are joined together. The sieves are manually designed and tuned manually on development data. Another approach has considered a set of hand crafted rules — the multi-agent method of Zhou and Su (2004). Like Raghunathan et al., it relies on multiple agents that filter potential antecedent candidates. However, the system of Raghunathan et al. differs in that coreference 1 Based on our personal experience deploying coreference resolution systems. 2521 decisions are not made sequentially but in order of the precision of the corresponding sieves, with later (less precise) sieves relying on the information from earlier sieves. 3 Easy-first Coreference The intuition behind the multi-pass sieve coreference system is that some decisions are easier than others. For instance, the first sieve joins to"
C96-1058,C92-2066,0,\N,Missing
C96-1058,H92-1026,0,\N,Missing
C96-1058,A88-1019,0,\N,Missing
C96-1058,P96-1025,0,\N,Missing
C96-1058,P95-1037,0,\N,Missing
C96-1058,P93-1005,0,\N,Missing
D07-1070,P06-1121,0,0.0299677,"hand-labeled treebank in the target domain, together with a large unannotated collection of indomain sentences. Additional resources such as parsers for other domains or languages can be integrated naturally. Dependency parsing is important as a key component in leading systems for information extraction (Weischedel, 2004)1 and question answering (Peng et al., 2005). These systems rely on edges or paths in dependency parse trees to define their extraction patterns and classification features. Parsing is also key to the latest advances in machine translation, which translate syntactic phrases (Galley et al., 2006; Marcu et al., 2006; Cowan et al., 2006). 2 Our Approach Our approach rests on three observations: • Recent “feature-based” parsing models are an excellent fit for bootstrapping, because the parse is often overdetermined by many redundant features. • The feature-based framework is flexible enough to incorporate other sources of guidance during training or testing—such as the knowledge contained in a parser for another language or domain. • Maximizing a combination of likelihood on labeled data and confidence on unlabeled data is a principled approach to bootstrapping. 2.1 Feature-Based Parsin"
D07-1070,W06-1615,0,0.0476143,"ave a treebank for a different domain or genre of the target language. One could simply include these trees in the initial supervised training, and hope that bootstrapping corrects any learned weights that are inappropriate to the target domain, as discussed above. In fact, McClosky et al. (2006) found a similar technique to be effective—though only in a model with a large feature space (“PCFG + reranking”), as we would predict. However, another approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data (Blitzer et al., 2006). Bootstrapping now teaches us where to trust the out-of-domain parser. If our basic model has 100 features, we could add features 101 through 200, where for example ˜ ˜ f123 (e) = f23 · log Pr(e) and Pr(e) is the posterior edge probability according to the out-of-domain parser. Learning that this feature has a high weight means learning to trust the out-of-domain parser’s decision on edges where in-domain feature 23 fires. Even more sensibly, we could add features such as P ˜ ˜ ˜ ˜ f201 (e) = 10 i=1 fi (e) · θi , where f and θ are the feature and weight vectors for the out-of-domain parser. L"
D07-1070,J04-3004,0,0.0541045,"ach iteration retrains the model on the examples where it is currently most confident. This kind of “confidence thresholding” has been popular in previous bootstrapping work (as cited in §2.2). It attempts to maintain high accuracy while gradually expanding coverage. The assumption is that throughout the training procedure, the parser’s confidence is a trustworthy guide to its correctness. Different bootstrapping procedures use different learners, smoothing methods, confidence measures, and procedures for “forgetting” the labelings from previous iterations. In his analysis of Yarowsky (1995), Abney (2004) formulates several variants of bootstrapping. These are shown to increase either the likelihood of the training data, or a lower bound on that likelihood. In particular, Abney defines a function K that is an upper bound on the negative log-likelihood, and shows his bootstrapping algorithms locally minimize K. We now present a generalization of Abney’s K function and relate it to another semi-supervised learning technique, entropy regularization (Brand, 1999; Grandvalet and Bengio, 2005; Jiao et al., 2006). Our experiments will tune the feature weight vector, θ, to minimize our function. We wi"
D07-1070,W06-2920,0,0.057846,"experiments on small corpora, using the features from (McDonald et al., 2005). After discussing experimental setup (§3.1), we look at the correlation of confidence with accuracy and with oracle likelihood, and at the fine-grained behaviour of models’ dependency edge posteriors (§3.2). We then compare our confidence-maximizing bootstrapping to EM, which has been widely used in semi-supervised learning (§3.4). Section 3.3 presents overall bootstrapping accuracy. 3.1 Experimental Design We bootstrapped non-projective parsers for languages assembled for the CoNLL dependency parsing competitions (Buchholz and Marsi, 2006). We selected German, Spanish, and Czech (Brants et al., 2002; Civit Torruella and Mart´ı Anton´ın, 2002; B¨ohmov´a et al., 2003). After removing sentences more than 60 words long, we randomly divided each corpus into small seed sets of 100 and 1000 trees; development and test sets of 200 trees each; and an unlabeled training set from the rest. These treebanks contain strict dependency trees, in the sense that their only nodes are the words and a distinguished root node. In the Czech dataset, more than one word can attach to the root; also, the trees in German, Spanish, and Czech may be nonpro"
D07-1070,W00-1306,0,0.0302444,"sing, the analogy to the inside algorithm is the O(n3 ) “matrix-tree algorithm,” which is dominated asymptotically by a matrix determinant (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007). The gradient of a determinant may be computed by matrix inversion, so evaluating the gradient again has the same O(n3 ) complexity as evaluating the function. The second term of (4) is the Shannon entropy of the posterior distribution over parses. Computing this for projective parsing takes O(n3 ) time, using a dynamic programming algorithm that is closely related to the inside algorithm (Hwa, 2000).6 For nonprojective parsing, unfortunately, the runtime rises to O(n4 ), since it requires determinants of n distinct matrices (each incorporating a log factor in a different column; we omit the details). The gradient evaluation in both cases is again about as expensive as the function evaluation. A convenient speedup is to replace Shannon entropy with R´enyi entropy. The family of R´enyi entropy measures is parameterized by α: Rα (p) = X 1 log p(y)α 1−α y ! (6) In our setting, where p = pθ,i , the events y are the possible parses yi,k of xi . Observe that under our P P P definition of p, y p"
D07-1070,W05-1508,0,0.0287936,"ill tune the feature weight vector, θ, to minimize our function. We will do so simply by applying a generic function minimization method (stochastic gradient descent), rather than by crafting a new Yarowsky-style or Abney-style iterative procedure for our specific function. Suppose we have examples xi and corresponding possible labelings yi,k . We are trying to learn a parametric model pθ (yi,k |xi ). If p˜(yi,k |xi ) is a “labeling distribution” that reflects our uncertainty about the true labels, then our expected negative loglikelihood of the model is def K = − XX i = XX = X i k Similarly, Jansche (2005) imputes “missing” trees by using comparable corpora. 670 p˜(yi,k |xi ) log p˜(yi,k |xi ) pθ (yi,k |xi )˜ p(yi,k |xi ) D(˜ pi kpθ,i ) + H(˜ pi ) (3) i def 3 p˜(yi,k |xi ) log pθ (yi,k |xi ) k def where p˜i (·) = p˜(· |xi ) and pθ,i (·) = pθ (· |xi ). Note that K is a function not only of θ but also of the labeling distribution p˜; a learner might be allowed to manipulate either in order to decrease K. The summands of K in equation (3) can be divided into two cases, according to whether xi is labeled or not. For the labeled examples {xi : i ∈ L}, the labeling distribution p˜i is a point distrib"
D07-1070,P06-1027,0,0.268878,"s for “forgetting” the labelings from previous iterations. In his analysis of Yarowsky (1995), Abney (2004) formulates several variants of bootstrapping. These are shown to increase either the likelihood of the training data, or a lower bound on that likelihood. In particular, Abney defines a function K that is an upper bound on the negative log-likelihood, and shows his bootstrapping algorithms locally minimize K. We now present a generalization of Abney’s K function and relate it to another semi-supervised learning technique, entropy regularization (Brand, 1999; Grandvalet and Bengio, 2005; Jiao et al., 2006). Our experiments will tune the feature weight vector, θ, to minimize our function. We will do so simply by applying a generic function minimization method (stochastic gradient descent), rather than by crafting a new Yarowsky-style or Abney-style iterative procedure for our specific function. Suppose we have examples xi and corresponding possible labelings yi,k . We are trying to learn a parametric model pθ (yi,k |xi ). If p˜(yi,k |xi ) is a “labeling distribution” that reflects our uncertainty about the true labels, then our expected negative loglikelihood of the model is def K = − XX i = XX"
D07-1070,N07-1032,1,0.825026,"o a theoretically attractive generalization. It can be shown that limα→1 Rα (p) 5 The numerator of pθ,i (yi∗ ) (see definition above) is trivial since yi∗ is a single known parse. But the denominator Zi is a normalizing constant that sums over all parses; it is found by a dependency-parsing variant of the inside algorithm, following (Eisner, 1996). 6 See also (Mann and McCallum, 2007) for similar results on conditional random fields. 672 is in fact the Shannon entropy H(p) and that limα→∞ Rα (p) = − log maxy p(y), i.e. the negative log probability of the modal or “Viterbi” label (Arndt, 2001; Karakos et al., 2007). The α = 2 case, widely used as a measure of purity in decision tree learning, is often called the “Gini index.” Finally, when α = 0, we get the log of the number of labels, which equals the H(uniform distribution) that Abney used in equation (3). 3 Evaluation For this paper, we performed some initial bootstrapping experiments on small corpora, using the features from (McDonald et al., 2005). After discussing experimental setup (§3.1), we look at the correlation of confidence with accuracy and with oracle likelihood, and at the fine-grained behaviour of models’ dependency edge posteriors (§3."
D07-1070,P04-1061,0,0.206471,"Missing"
D07-1070,P06-1103,0,0.0183451,"not be able to confidently parse without an English translation. Parses of comparable English sentences. World knowledge can be useful in parsing. Suppose you see a French sentence that contains mangeons and pommes, and you know that manger=eat and pomme=apple. You might reasonably guess that pommes is the direct object of mangeons, because you know that apple is a plausible direct object for eat. We can discover this last bit of world knowledge from comparable English text. Translation dictionaries can themselves be induced from comparable corpora (Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006), or extracted from bitext or digitized versions of human-readable dictionaries if these are available. The above inference pattern can be captured by features similar to those in equation (1). For example, one can define a feature j by poss def prep pobj (2) fi (c −→ p) = log Pr (p0 ←− of ←− c0 |p0 translates p, c0 translates c) where each event in the event space is a pair (c0 , p0 ) of same-sentence tokens in comparable English text, all pairs being equally likely. Thus, to estimate Pr(· |·), the denominator counts same-sentence token pairs (c0 , p0 ) in the comparable English corpus that t"
D07-1070,W06-1628,0,0.059052,"Missing"
D07-1070,D07-1015,0,0.108661,"∈yi,k f (e) · θ)/Zi . With exponentially many parses of xi , does our objective function (4) now have prohibitive computational complexity? The complexity is actually similar to that of the inside algorithm for parsing. In fact, the first term of (4) for projective parsing is found by running the O(n3 ) inside algorithm on supervised data,5 and its gradient is found by the corresponding O(n3 ) outside algorithm. For nonprojective parsing, the analogy to the inside algorithm is the O(n3 ) “matrix-tree algorithm,” which is dominated asymptotically by a matrix determinant (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007). The gradient of a determinant may be computed by matrix inversion, so evaluating the gradient again has the same O(n3 ) complexity as evaluating the function. The second term of (4) is the Shannon entropy of the posterior distribution over parses. Computing this for projective parsing takes O(n3 ) time, using a dynamic programming algorithm that is closely related to the inside algorithm (Hwa, 2000).6 For nonprojective parsing, unfortunately, the runtime rises to O(n4 ), since it requires determinants of n distinct matrices (each incorporating a log factor in a dif"
D07-1070,N07-2028,0,0.0318561,"the inside algorithm, and we can find the numerator by running the inside algorithm again with θ scaled by α. Thus with R´enyi entropy, all computations and their gradients are O(n3 )—even in the nonprojective case. R´enyi entropy is also a theoretically attractive generalization. It can be shown that limα→1 Rα (p) 5 The numerator of pθ,i (yi∗ ) (see definition above) is trivial since yi∗ is a single known parse. But the denominator Zi is a normalizing constant that sums over all parses; it is found by a dependency-parsing variant of the inside algorithm, following (Eisner, 1996). 6 See also (Mann and McCallum, 2007) for similar results on conditional random fields. 672 is in fact the Shannon entropy H(p) and that limα→∞ Rα (p) = − log maxy p(y), i.e. the negative log probability of the modal or “Viterbi” label (Arndt, 2001; Karakos et al., 2007). The α = 2 case, widely used as a measure of purity in decision tree learning, is often called the “Gini index.” Finally, when α = 0, we get the log of the number of labels, which equals the H(uniform distribution) that Abney used in equation (3). 3 Evaluation For this paper, we performed some initial bootstrapping experiments on small corpora, using the features"
D07-1070,W06-1606,0,0.0177912,"in the target domain, together with a large unannotated collection of indomain sentences. Additional resources such as parsers for other domains or languages can be integrated naturally. Dependency parsing is important as a key component in leading systems for information extraction (Weischedel, 2004)1 and question answering (Peng et al., 2005). These systems rely on edges or paths in dependency parse trees to define their extraction patterns and classification features. Parsing is also key to the latest advances in machine translation, which translate syntactic phrases (Galley et al., 2006; Marcu et al., 2006; Cowan et al., 2006). 2 Our Approach Our approach rests on three observations: • Recent “feature-based” parsing models are an excellent fit for bootstrapping, because the parse is often overdetermined by many redundant features. • The feature-based framework is flexible enough to incorporate other sources of guidance during training or testing—such as the knowledge contained in a parser for another language or domain. • Maximizing a combination of likelihood on labeled data and confidence on unlabeled data is a principled approach to bootstrapping. 2.1 Feature-Based Parsing McDonald et al. (2"
D07-1070,C96-1058,1,0.864752,"schedel (p.c.) reports that this system’s performance degrades considerably when only phrase chunking is available rather than full parsing. 667 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 667–677, Prague, June 2007. 2007 Association for Computational Linguistics Given an n-word input sentence, the parser begins by scoring each of the O(n2 ) possible edges, and then seeks the highest-scoring legal dependency tree formed by any n − 1 of these edges, using an O(n3 ) dynamic programming algorithm (Eisner, 1996) for projective trees. For non-projective parsing, O(n3 ), or with some trickery O(n2 ), greedy algorithms exist (Chu and Liu, 1965; Edmonds, 1967; Gabow et al., 1986). The feature function f may pay attention to many properties of the directed edge e. Of course, features may consider the parent and child words connected by e, and their parts of speech.2 But some features used by McDonald et al. (2005) also consider the parts of speech of words adjacent to the parent and child, or between the parent and child, as well as the number of words between the parent and child. In general, these featu"
D07-1070,P06-1043,0,0.0290869,"domain (plus a large unsupervised corpus). We now consider other, more dubious, knowledge sources that might supplement or replace this small treebank. In each case, we can use these knowledge sources to derive features that may—or may not— prove trustworthy during bootstrapping. Parses from a different domain. One might have a treebank for a different domain or genre of the target language. One could simply include these trees in the initial supervised training, and hope that bootstrapping corrects any learned weights that are inappropriate to the target domain, as discussed above. In fact, McClosky et al. (2006) found a similar technique to be effective—though only in a model with a large feature space (“PCFG + reranking”), as we would predict. However, another approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data (Blitzer et al., 2006). Bootstrapping now teaches us where to trust the out-of-domain parser. If our basic model has 100 features, we could add features 101 through 200, where for example ˜ ˜ f123 (e) = f23 · log Pr(e) and Pr(e) is the posterior edge probability according to the out-of-domain pars"
D07-1070,E06-1011,0,0.0488428,"its ML baseline. Since these models were only trained and tested on sentences of 10 words or fewer, accuracy is much higher than the full results in Table 2. 4 We hypothesize that qualitatively better bootstrapping results will require much larger unlabeled data sets. In scaling up bootstrapping to larger unlabeled training sets, we must carefully weight tradeoffs between expanding coverage and introducing noise from out-of-domain data. We could also better exploit the data we have with richer models of syntax. In supervised dependency parsing, secondorder edge features provide improvements (McDonald and Pereira, 2006; Riedel and Clarke, 2006); moreover, the feature-based approach is not limited to dependency parsing. Similar techniques could score parses in other formalisms, such as CFG or TAG. In this case, f extracts features from each of the derivation tree’s rewrite rules (CFG) or elementary trees (TAG). In lexicalized formalisms, f will still be able to score lexical dependencies that are implicitly represented in the parse. Finally, we want to investigate whether larger training sets will provide traction for sparser cross-lingual and crossdomain features. 5 to sentences of ten words or fewer and to"
D07-1070,W07-2216,0,0.0484585,"i . With exponentially many parses of xi , does our objective function (4) now have prohibitive computational complexity? The complexity is actually similar to that of the inside algorithm for parsing. In fact, the first term of (4) for projective parsing is found by running the O(n3 ) inside algorithm on supervised data,5 and its gradient is found by the corresponding O(n3 ) outside algorithm. For nonprojective parsing, the analogy to the inside algorithm is the O(n3 ) “matrix-tree algorithm,” which is dominated asymptotically by a matrix determinant (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007). The gradient of a determinant may be computed by matrix inversion, so evaluating the gradient again has the same O(n3 ) complexity as evaluating the function. The second term of (4) is the Shannon entropy of the posterior distribution over parses. Computing this for projective parsing takes O(n3 ) time, using a dynamic programming algorithm that is closely related to the inside algorithm (Hwa, 2000).6 For nonprojective parsing, unfortunately, the runtime rises to O(n4 ), since it requires determinants of n distinct matrices (each incorporating a log factor in a different column; we omit the"
D07-1070,P05-1012,0,0.538306,"Marcu et al., 2006; Cowan et al., 2006). 2 Our Approach Our approach rests on three observations: • Recent “feature-based” parsing models are an excellent fit for bootstrapping, because the parse is often overdetermined by many redundant features. • The feature-based framework is flexible enough to incorporate other sources of guidance during training or testing—such as the knowledge contained in a parser for another language or domain. • Maximizing a combination of likelihood on labeled data and confidence on unlabeled data is a principled approach to bootstrapping. 2.1 Feature-Based Parsing McDonald et al. (2005) introduced a simple, flexible framework for scoring dependency parses. Each directed edge e in the dependency tree is described with a high-dimensional feature vector f (e). The edge’s score is the dot product f (e) · θ, where θ is a learned weight vector. The overall score of a dependency tree is the sum of the scores of all edges in the tree. 1 Ralph Weischedel (p.c.) reports that this system’s performance degrades considerably when only phrase chunking is available rather than full parsing. 667 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and"
D07-1070,J94-2001,0,0.0583027,"words or fewer and to part-ofspeech sequences alone, without any lexical information. Since the DMV models projective trees, we ran experiments on three CoNLL corpora that had augmented their primary non-projective parses with alternate projective annotations: Bulgarian (Simov et al., 2005), German, and Spanish. We performed supervised maximum likelihood and conditional likelihood estimation on a seed set of 100 sentences for each language. These models respectively initialized EM and confidence training on unlabeled data. We see (Table 4) that EM degrades the performance of its ML baseline. Merialdo (1994) saw a similar degradation over small (and large) seed sets in HMM POS tagging. We tried fixing and not fixing the feature expectations on the seed set during EM and show the former, better numbers. Confidence maximization improved over both its own conditional likelihood initializer and also over ML. We selected optimal smoothing parameters for all models and optimal α (equation (6)) and γ (equation (4)) for the confidence model on labeled held-out data. 675 Future Work Conclusions Feature-rich dependency models promise to help bootstrapping by providing many redundant features for the learne"
D07-1070,H05-1039,0,0.0285249,"slations. Yet an adequate labeled training corpus—a large treebank of manually constructed parse trees of typical sentences—is rarely available and would be prohibitively expensive to develop. We show how it is possible to train instead from a small hand-labeled treebank in the target domain, together with a large unannotated collection of indomain sentences. Additional resources such as parsers for other domains or languages can be integrated naturally. Dependency parsing is important as a key component in leading systems for information extraction (Weischedel, 2004)1 and question answering (Peng et al., 2005). These systems rely on edges or paths in dependency parse trees to define their extraction patterns and classification features. Parsing is also key to the latest advances in machine translation, which translate syntactic phrases (Galley et al., 2006; Marcu et al., 2006; Cowan et al., 2006). 2 Our Approach Our approach rests on three observations: • Recent “feature-based” parsing models are an excellent fit for bootstrapping, because the parse is often overdetermined by many redundant features. • The feature-based framework is flexible enough to incorporate other sources of guidance during tr"
D07-1070,W06-1616,0,0.0124744,"e models were only trained and tested on sentences of 10 words or fewer, accuracy is much higher than the full results in Table 2. 4 We hypothesize that qualitatively better bootstrapping results will require much larger unlabeled data sets. In scaling up bootstrapping to larger unlabeled training sets, we must carefully weight tradeoffs between expanding coverage and introducing noise from out-of-domain data. We could also better exploit the data we have with richer models of syntax. In supervised dependency parsing, secondorder edge features provide improvements (McDonald and Pereira, 2006; Riedel and Clarke, 2006); moreover, the feature-based approach is not limited to dependency parsing. Similar techniques could score parses in other formalisms, such as CFG or TAG. In this case, f extracts features from each of the derivation tree’s rewrite rules (CFG) or elementary trees (TAG). In lexicalized formalisms, f will still be able to score lexical dependencies that are implicitly represented in the parse. Finally, we want to investigate whether larger training sets will provide traction for sparser cross-lingual and crossdomain features. 5 to sentences of ten words or fewer and to part-ofspeech sequences a"
D07-1070,W02-2026,0,0.0216572,"e some unsupervised sentences that we would not be able to confidently parse without an English translation. Parses of comparable English sentences. World knowledge can be useful in parsing. Suppose you see a French sentence that contains mangeons and pommes, and you know that manger=eat and pomme=apple. You might reasonably guess that pommes is the direct object of mangeons, because you know that apple is a plausible direct object for eat. We can discover this last bit of world knowledge from comparable English text. Translation dictionaries can themselves be induced from comparable corpora (Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006), or extracted from bitext or digitized versions of human-readable dictionaries if these are available. The above inference pattern can be captured by features similar to those in equation (1). For example, one can define a feature j by poss def prep pobj (2) fi (c −→ p) = log Pr (p0 ←− of ←− c0 |p0 translates p, c0 translates c) where each event in the event space is a pair (c0 , p0 ) of same-sentence tokens in comparable English text, all pairs being equally likely. Thus, to estimate Pr(· |·), the denominator counts same-sentence token pairs (c0 , p"
D07-1070,W06-3104,1,0.841037,"ulting noisy treebank. They used only 1-best translations, 1-best alignments, dependency paths of length 1, and no labeled data in Spanish or Chinese. Hwa et al. (2005) used a manually written postprocessor to correct some of the many incorrect projections. By contrast, our framework uses the projected dependencies only as one source of features. They may be overridden by other features in particular cases, and will be given a high weight only if they tend to agree with other features during bootstrapping. A similar soft projection of dependencies was used in supervised machine translation by Smith and Eisner (2006), who used a source sentence’s dependency paths to bias the generation of its translation. Note that these bilingual features will only fire on those supervised or unsupervised sentences for which we have an English translation. In particular, they will usually be unavailable on the test set. However, we hope that they will seed and facilitate the bootstrapping process, by helping us confidently parse some unsupervised sentences that we would not be able to confidently parse without an English translation. Parses of comparable English sentences. World knowledge can be useful in parsing. Suppos"
D07-1070,D07-1014,1,0.85215,"f pθ,i (yi,k ) = exp( e∈yi,k f (e) · θ)/Zi . With exponentially many parses of xi , does our objective function (4) now have prohibitive computational complexity? The complexity is actually similar to that of the inside algorithm for parsing. In fact, the first term of (4) for projective parsing is found by running the O(n3 ) inside algorithm on supervised data,5 and its gradient is found by the corresponding O(n3 ) outside algorithm. For nonprojective parsing, the analogy to the inside algorithm is the O(n3 ) “matrix-tree algorithm,” which is dominated asymptotically by a matrix determinant (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007). The gradient of a determinant may be computed by matrix inversion, so evaluating the gradient again has the same O(n3 ) complexity as evaluating the function. The second term of (4) is the Shannon entropy of the posterior distribution over parses. Computing this for projective parsing takes O(n3 ) time, using a dynamic programming algorithm that is closely related to the inside algorithm (Hwa, 2000).6 For nonprojective parsing, unfortunately, the runtime rises to O(n4 ), since it requires determinants of n distinct matrices (each incorporating a l"
D07-1070,E03-1008,0,0.0283579,"the more robust it will be against errors even when θ is incorrectly trained. (Some feature weights may be too strong or have the wrong sign, because of overfitting or mistaken parses during bootstrapping.) In the former case, strong features lend their strength to weak ones. In the latter case, a conflict among strong features weakens the ones that depart from the consensus, or discounts the example sentence if there is no consensus. Previous work on parser bootstrapping has not been able to exploit this redundancy among features, because it has used PCFG-like models with far fewer features (Steedman et al., 2003). 2.3 Adaptation and Projection via Features The previous section assumed that we had a small supervised treebank in the target language and domain (plus a large unsupervised corpus). We now consider other, more dubious, knowledge sources that might supplement or replace this small treebank. In each case, we can use these knowledge sources to derive features that may—or may not— prove trustworthy during bootstrapping. Parses from a different domain. One might have a treebank for a different domain or genre of the target language. One could simply include these trees in the initial supervised t"
D07-1070,P00-1027,0,0.0153504,"eans accepting all other features on that edge. It also means rejecting various other edges, because of the global constraints that a legal parse tree must give each word only one parent and must be free of cycles and, in 2 Note that since we are not trying to predict parts of speech, we treat the output of one or more automatic taggers as yet more inputs to edge feature functions. 668 the projective case, crossings. Our observation is that this situation is ideal for so-called “bootstrapping,” “co-training,” or “minimally supervised” learning methods (Yarowsky, 1995; Blum and Mitchell, 1998; Yarowsky and Wicentowski, 2000). Such methods should thrive when the right answer is overdetermined owing to redundant features and/or global constraints. Concretely, suppose we start by training a supervised parser on only 100 examples, using some regularization method to prevent overfitting to this set. While many features might truly be relevant to the task, only a few appear often enough in this small training set to acquire significantly positive or negative weights. Even this lightly trained parser may be quite sure of itself on some test sentences in a large unannotated corpus, when one parse scores far higher than a"
D07-1070,P95-1026,0,0.291664,"f one another. Selecting a good feature means accepting all other features on that edge. It also means rejecting various other edges, because of the global constraints that a legal parse tree must give each word only one parent and must be free of cycles and, in 2 Note that since we are not trying to predict parts of speech, we treat the output of one or more automatic taggers as yet more inputs to edge feature functions. 668 the projective case, crossings. Our observation is that this situation is ideal for so-called “bootstrapping,” “co-training,” or “minimally supervised” learning methods (Yarowsky, 1995; Blum and Mitchell, 1998; Yarowsky and Wicentowski, 2000). Such methods should thrive when the right answer is overdetermined owing to redundant features and/or global constraints. Concretely, suppose we start by training a supervised parser on only 100 examples, using some regularization method to prevent overfitting to this set. While many features might truly be relevant to the task, only a few appear often enough in this small training set to acquire significantly positive or negative weights. Even this lightly trained parser may be quite sure of itself on some test sentences in a large u"
D08-1004,P99-1009,0,0.0240709,"ct substring rationales for a sentiment classification task (Pang and Lee, 2004) and use them to obtain significant accuracy improvements for each annotator. Our new generative approach exploits the rationales more effectively than our previous “masking SVM” approach. It is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks. approach remains “data-driven” if the annotators repeatedly refine their system against a corpus of labeled or unlabeled examples. This achieves high performance in some domains, such as NP chunking (Brill and Ngai, 1999), but requires more analytical skill from the annotators. One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. 1.2 Feature selection by humans Many recent papers aim to reduce the amount of annotated data needed to train the parameters of a statistical model. Well-known paradigms include active learning, semi-supervised learning, and either domain adaptation or cross-lingual transfer from existing annotated data. A rather different paradigm is to change the actual task that is given to annotators, giving them a greater hand in sha"
D08-1004,N06-1041,0,0.0413246,"one can ask annotators to examine or propose some candidate features. This is possible even for the very large feature sets that are typically used in NLP. In document classification, Raghavan et al. (2006) show that feature selection by an oracle could be helpful, and that humans are both rapid and reasonably good at distinguishing highly useful n-gram features from randomly chosen ones, even when viewing these n-grams out of context. Druck et al. (2008) show annotators some features f from a fixed feature set, and ask them to choose a class label y such that p(y |f ) is as high as possible. Haghighi and Klein (2006) do the reverse: for each class label y, they ask the annotators to propose a few “prototypical” features f such that p(y |f ) is as high as possible. 1.1 1.3 1 Background Hand-crafted rules Feature selection in context An obvious option is to have the annotators directly express their knowledge by hand-crafting rules. This The above methods consider features out of context. An annotator might have an easier time examining ∗ This work was supported by National Science Foundation grant No. 0347822 and the JHU WSE/APL Partnership Fund. Special thanks to Christine Piatko for many useful discussio"
D08-1004,P00-1016,0,0.0734273,"ain significant accuracy improvements for each annotator. Our new generative approach exploits the rationales more effectively than our previous “masking SVM” approach. It is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks. approach remains “data-driven” if the annotators repeatedly refine their system against a corpus of labeled or unlabeled examples. This achieves high performance in some domains, such as NP chunking (Brill and Ngai, 1999), but requires more analytical skill from the annotators. One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. 1.2 Feature selection by humans Many recent papers aim to reduce the amount of annotated data needed to train the parameters of a statistical model. Well-known paradigms include active learning, semi-supervised learning, and either domain adaptation or cross-lingual transfer from existing annotated data. A rather different paradigm is to change the actual task that is given to annotators, giving them a greater hand in shaping the learned classifier. After all, human annotators themselves are more than just black-box classi"
D08-1004,P04-1035,0,0.424703,"on Eisner Dept. of Computer Science, Johns Hopkins University Baltimore, MD 21218, USA {ozaidan,jason}@cs.jhu.edu Abstract A human annotator can provide hints to a machine learner by highlighting contextual “rationales” for each of his or her annotations (Zaidan et al., 2007). How can one exploit this side information to better learn the desired parameters θ? We present a generative model of how a given annotator, knowing the true θ, stochastically chooses rationales. Thus, observing the rationales helps us infer the true θ. We collect substring rationales for a sentiment classification task (Pang and Lee, 2004) and use them to obtain significant accuracy improvements for each annotator. Our new generative approach exploits the rationales more effectively than our previous “masking SVM” approach. It is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks. approach remains “data-driven” if the annotators repeatedly refine their system against a corpus of labeled or unlabeled examples. This achieves high performance in some domains, such as NP chunking (Brill and Ngai, 1999), but requires more analytical skill from the annotators. O"
D08-1004,W02-1011,0,0.0463332,"sizes, due to limited data from A3–A8. The larger A0 dataset will still allow us to evaluate our method on a range of training set sizes. 5 5.1 Detailed Models Modeling class annotations with pθ We define the basic classifier pθ in equation (1) to be a standard conditional log-linear model: def pθ (y |x) = exp(θ~ · f~(x, y)) def u(x, y) = Zθ (x) Zθ (x) (2) where f~(·) extracts a feature vector from a classified document, θ~ are the corresponding weights of those def P features, and Zθ (x) = y u(x, y) is a normalizer. We use the same set of binary features as in previous work on this dataset (Pang et al., 2002; Pang and Lee, 2004; Zaidan et al., 2007). Specifically, let V = {v1 , ..., v17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus. Define fh (x, y) to be y if vh appears at least once in x, and 0 otherwise. Thus θ ∈ R17744 , and positive weights in θ favor class label y = +1 and equally discourage y = −1, while negative weights do the opposite. This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales. Future work should consider more complex features and how they are signaled by rationales, as dis"
D08-1004,N07-1033,1,0.68947,"Missing"
D08-1004,J03-4003,0,\N,Missing
D08-1016,P05-1022,0,0.0856779,"Missing"
D08-1016,J07-2003,0,0.00538807,"as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008). We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links Mij (not constrained by T REE/PT REE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; Buch-Kromann, 2006). Other parsing-like problems that could be attacked with BP appear in syntax-based machine translation. Decoding is very expensive with a synchronous grammar composed with an n-gram language model (Chiang, 2007)—but our footnote 10 suggests that BP might incorporate a language model rapidly. String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG (Wu, 1997)—but Duchi et al. (2007) show how to incorporate bipartite matching into max-product BP. Finally, we can take advantage of improvements to BP proposed in the context of other applications. For example, instead of updating all messages in parallel at every iteration, it is empirically faster to serialize updates using a priority queue (Elidan et al., 2006; Sutton and McCallum, 2007).31 31 These m"
D08-1016,P99-1059,1,0.280355,"projective parsing, we might prefer (but not require) that a parent and child be on the same side of the grandparent. S IB. Shorthand for the family of O(n3 ) binary factors {PAIRij,ik }, which judge whether two children of the same parent are compatible. E.g., a given verb may not like to have two noun children both to its left.6 The children do not need to be adjacent. C HILD S EQ. A family of O(n) global factors. C HILD S EQi scores i’s sequence of children; hence it consults all variables of the form Lij . The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999). If 5 has children 2, 7, 9 under A, then C HILD S EQi is a product of subfactors of the form PAIR5#,57 , PAIR 57,59 , PAIR 59,5# (right child sequence) and PAIR 5#,52 , PAIR 52,5# (left child sequence). N O C ROSS. A family of O(n2 ) global constraints. If the parent-to-j link crosses the parent-to-` link, then N O C ROSSj` fires with a value that depends only on j and `. (If j and ` do not each have exactly one parent, N O C ROSSj` fires with value 0; i.e., it incorporates E XACTLY 1j and E XACTLY 1` .)7 TAGi is a unary factor that evaluates whether Ti ’s value is consistent with W (especial"
D08-1016,H05-1036,1,0.772834,"gation was written in C++. The BP parser averaged 1.8 seconds per sentence for non-projective parsing and 1.5 seconds per sentence for projective parsing (1.2 and 0.9 seconds/sentence for ≤ 40 words), using our standard setup, which included five iterations of BP and the final MBR tree decoding pass. In our tables, we boldface the best result in each column along with any results that are not significantly worse (paired permutation test, p &lt; .05). 27 This dominates runtime, and probably slows down all our parsers by a factor of 4–11 owing to known inefficiencies in the Dyna prototype we used (Eisner et al., 2005). 0.3 (a) 0.2 Input length (b) 0.1 40 words 50 words 60 words 70 words 0.0 Error relative to exact MBR 0.4 2 iterations of BP 3 iterations of BP 5 iterations of BP 10 iterations of BP MBR DP 0 20 40 60 Parsing time in seconds Figure 4: Runtime vs. search error after different numbers of BP iterations. This shows the simpler model of Fig. 2, where DP is still relatively fast. 8.4 Faster higher-order projective parsing We built a first-order projective parser—one that uses only factors PT REE and L INK—and then compared the cost of incorporating second-order factors, G RAND and C HILD S EQ, by B"
D08-1016,C96-1058,1,0.911972,"ees having edge i → j. We perform these combinatorial sums by calling a first-order parsing algorithm, with edge weights q¯ij . Thus, as outlined in §2, a first-order parser is called each time we propagate through the global T REE or PT REE constraint, using edge weights that include the first-order L INK factors but also multiply in any current messages from higher-order factors. The parsing algorithm simultaneously computes the partition function b(), and all O(n2 ) marginal beliefs b(Lij = true). For PT REE (projective), it is the inside-outside version of a dynamic programming algorithm (Eisner, 1996). For T REE (nonprojective), Koo et al. (2007) and Smith and Smith (2007) show how to employ the matrix-tree theorem. In both cases, the total time is O(n3 ).19 N O C ROSSj` must sum over assignments to O(n) neighboring variables {Lij } and {Lk` }. The nonzero summands are assignments where j and ` def each Q have exactly Q one parent. At step 1, π = At step 2, the i qLij (false) · k qLk` (false). marginal belief b(Lij = true) sums over the n nonzero P assignments containing i → j. It is π · q¯Lij · ¯Lk` · PAIRij,k` , where PAIRij,k` is xj` if i → j kq crosses k → ` and is 1 otherwise. xj` is"
D08-1016,P96-1024,0,0.334433,"eration, via serial forward and backward sweeps through the subfactors. Handling the subfactors in parallel, (3)–(4), would need O(n) iterations. 23 Ignoring the treatment of boundary symbols “#” (see §3.4). 22 In our experiments, we actually take the edge weights to be not the messages q¯Lij from the links, def but the full beliefs ¯bL at the links (where ¯bL = ij ij log bLij (true)/bLij (false)). These are passed into a fast algorithm for maximum spanning tree (Tarjan, 1977) or maximum projective spanning tree (Eisner, 1996). This procedure is equivalent to minimum Bayes risk (MBR) parsing (Goodman, 1996) with a dependency accuracy loss function. Notice that the above decoding approaches do not enforce any hard constraints other than T REE in the final output. In addition, they only recover values of the Lij variables. They marginalize over other variables such as tags and link roles. This solves the problem of “nuisance” variables (which merely fragment probability mass among refinements of a parse). On the other hand, it may be undesirable for variables whose values we desire to recover.24 7 Training Our training method also uses beliefs computed by BP, but at the factors. We choose the weig"
D08-1016,J99-4004,0,0.0286991,"lso uses beliefs computed by BP, but at the factors. We choose the weight vector θ by maximizing the log-probability of training data 24 An alternative is to attempt to find the most probable (“MAP”) assignment to all variables—using the max-product algorithm (footnote 13) or one of its recent variants. The estimated marginal beliefs become “max marginals,” which assess the 1-best assignment consistent with each value of the variable. We can indeed build max-product propagators for our global constraints. PT REE still propagates in O(n3 ) time: simply change the first-order parser’s semiring (Goodman, 1999) to use max instead of sum. T REE requires O(n4 ) time: it seems that the O(n2 ) max marginals must be computed separately, each requiring a separate call to an O(n2 ) maximum spanning tree algorithm (Tarjan, 1977). If max-product BP converges, we may simply output each variable’s favorite value (according to its belief), if unique. However, max-product BP tends to be unstable on loopy graphs, and we may not wish to wait for full convergence in any case. A more robust technique for extracting an assignment is to mimic Viterbi decoding, and “follow backpointers” of the max-product computation a"
D08-1016,P08-1067,0,0.0828718,"proximation technique from machine learning, namely, loopy belief propagation (BP). In this paper, we show that BP can be used to train and decode complex parsing models. Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem.1 2 Overview and Related Work We wish to make a dependency parse’s score depend on higher-order features, which consider ar∗ This work was supported by the Human Language Technology Center of Excellence. 1 As do constraint relaxation (Tromble and Eisner, 2006) and forest reranking (Huang, 2008). In contrast, generic NP-hard solution techniques like Integer Linear Programming (Riedel and Clarke, 2006) know nothing about optimal substructure. bitrary interactions among two or more edges in the parse (and perhaps also other latent variables such as part-of-speech tags or edge labels). Such features can help accuracy—as we show. Alas, they raise the polynomial runtime of projective parsing, and render non-projective parsing NP-hard. Hence we seek approximations. We will show how BP’s “message-passing” discipline offers a principled way for higher-order features to incrementally adjust t"
D08-1016,D07-1015,0,0.0469871,"mbinatorial sums by calling a first-order parsing algorithm, with edge weights q¯ij . Thus, as outlined in §2, a first-order parser is called each time we propagate through the global T REE or PT REE constraint, using edge weights that include the first-order L INK factors but also multiply in any current messages from higher-order factors. The parsing algorithm simultaneously computes the partition function b(), and all O(n2 ) marginal beliefs b(Lij = true). For PT REE (projective), it is the inside-outside version of a dynamic programming algorithm (Eisner, 1996). For T REE (nonprojective), Koo et al. (2007) and Smith and Smith (2007) show how to employ the matrix-tree theorem. In both cases, the total time is O(n3 ).19 N O C ROSSj` must sum over assignments to O(n) neighboring variables {Lij } and {Lk` }. The nonzero summands are assignments where j and ` def each Q have exactly Q one parent. At step 1, π = At step 2, the i qLij (false) · k qLk` (false). marginal belief b(Lij = true) sums over the n nonzero P assignments containing i → j. It is π · q¯Lij · ¯Lk` · PAIRij,k` , where PAIRij,k` is xj` if i → j kq crosses k → ` and is 1 otherwise. xj` is some factor value defined by equation (2) to p"
D08-1016,D08-1017,0,0.127616,"elf. BP’s behavior in our setup can be understood intuitively as follows. Inasmuch as the first-order parser finds that edge e is probable, the higher-order features will kick in and discourage other edges e0 to the extent that they prefer not to coexist with e.2 Thus, the next call to the first-order parser assigns lower probabilities to parses that contain these e0 . (The method is approximate because a first-order parser must equally penalize all parses containing e0 , even those that do not in fact contain e.) This behavior is somewhat similar to parser stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a first-order parser derives some of its input features from the full 1-best output of another parser. In our method, a first-order parser derives such input features from its own previous full output (but probabilistic output rather than just 1best). This circular process is iterated to convergence. Our method also permits the parse to interact cheaply with other variables. Thus first-order parsing, part-of-speech tagging, and other tasks on a common input could mutually influence one another. Our method and its numerical details emerge naturally as an instance of the well-studied"
D08-1016,P05-1010,0,0.0182072,"ns and Future Work Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008). We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links Mij (not constrained by T REE/PT REE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; Buch-Kromann, 2006). Other parsing-like problems that could be attacked with BP appear in syntax-based machine translation. Decoding is very expensive with a synchronous grammar composed with an n-gram language model (Chiang, 2007)—but our footnote 10 suggests that BP might incorporate a language model rapidly. String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG (Wu, 1997)—but Duc"
D08-1016,E06-1011,0,0.341435,"E factor to train conditional log-linear parsing models of two highly non-projective languages, Danish and Dutch, as well as slightly non-projective English (§8.1). In all three languages, the first-order non-projective parser greatly overpredicts the number of crossing links. We thus added N O C ROSS factors, as well as G RAND and C HILD S EQ as before. All of these significantly improve the first-order baseline, though not necessarily cumulatively (Table 2). Finally, Table 2 compares loopy BP to a previously proposed “hill-climbing” method for approximate inference in non-projective parsing McDonald and Pereira (2006). Hill-climbing decodes our richest non-projective model by finding the best projective parse under that model—using slow, higherorder DP—and then greedily modifies words’ parents until the parse score (1) stops improving. Decoding N OT 2 AT M OST 1 E XACTLY 1 + N O 2C YCLE T REE PT REE Danish 81.8 (76.7) 85.4 (82.2) 85.7 (85.0) 85.0 (85.2) 85.5 (85.5) 85.8 Dutch 83.3 (75.0) 87.3 (86.3) 87.0 (86.7) 86.2 (86.7) 87.3 (87.3) 83.9 English 87.5 (66.4) 88.5 (84.6) 88.6 (86.0) 88.5 (86.2) 88.6 (88.6) 88.8 Table 3: After training a non-projective first-order model with T REE, decoding it with weaker c"
D08-1016,P05-1012,0,0.464359,"exp θh fh (A, W, m) (2) h∈features(Fm ) where θ is a learned, finite collection of weights and f is a corresponding collection of feature functions, some of which are used by Fm . (Note that fh is permitted to consult the observed input W . It also sees which factor Fm it is scoring, to support reuse of a single feature function fh and its weight θh by unboundedly many factors in a model.) L INK. A family of unary soft factors that judge the links in a parse A individually. L INKij fires iff Lij = true, and then its value depends on (i, j), W , and θ. Our experiments use the same features as McDonald et al. (2005). A first-order (or “edge-factored”) parsing model (McDonald et al., 2005) contains only L INK factors, along with a global T REE or PT REE factor. Though there are O(n2 ) link factors (one per Lij ), only n of them fire on any particular parse, since the global factor ensures that exactly n are true. We’ll consider various higher-order soft factors: PAIR . The binary factor PAIRij,k` fires with some value iff Lij and Lk` are both true. Thus, it penalizes or rewards a pair of links for being simultaneously present. This is a soft version of NAND. G RAND. Shorthand for the family of O(n3 ) bina"
D08-1016,P08-1108,0,0.0239291,"ering the interactions itself. BP’s behavior in our setup can be understood intuitively as follows. Inasmuch as the first-order parser finds that edge e is probable, the higher-order features will kick in and discourage other edges e0 to the extent that they prefer not to coexist with e.2 Thus, the next call to the first-order parser assigns lower probabilities to parses that contain these e0 . (The method is approximate because a first-order parser must equally penalize all parses containing e0 , even those that do not in fact contain e.) This behavior is somewhat similar to parser stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a first-order parser derives some of its input features from the full 1-best output of another parser. In our method, a first-order parser derives such input features from its own previous full output (but probabilistic output rather than just 1best). This circular process is iterated to convergence. Our method also permits the parse to interact cheaply with other variables. Thus first-order parsing, part-of-speech tagging, and other tasks on a common input could mutually influence one another. Our method and its numerical details emerge naturally as an instan"
D08-1016,W06-1616,0,0.362548,"r, we show that BP can be used to train and decode complex parsing models. Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem.1 2 Overview and Related Work We wish to make a dependency parse’s score depend on higher-order features, which consider ar∗ This work was supported by the Human Language Technology Center of Excellence. 1 As do constraint relaxation (Tromble and Eisner, 2006) and forest reranking (Huang, 2008). In contrast, generic NP-hard solution techniques like Integer Linear Programming (Riedel and Clarke, 2006) know nothing about optimal substructure. bitrary interactions among two or more edges in the parse (and perhaps also other latent variables such as part-of-speech tags or edge labels). Such features can help accuracy—as we show. Alas, they raise the polynomial runtime of projective parsing, and render non-projective parsing NP-hard. Hence we seek approximations. We will show how BP’s “message-passing” discipline offers a principled way for higher-order features to incrementally adjust the numerical edge weights that are fed to a fast first-order parser. Thus the first-order parser is influenc"
D08-1016,1993.iwpt-1.22,0,0.132554,"e parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008). We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links Mij (not constrained by T REE/PT REE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; Buch-Kromann, 2006). Other parsing-like problems that could be attacked with BP appear in syntax-based machine translation. Decoding is very expensive with a synchronous grammar composed with an n-gram language model (Chiang, 2007)—but our footnote 10 suggests that BP might incorporate a language model rapidly. String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG (Wu, 1997)—but Duchi et al. (2007) show how to incorporate bipartite matching into max-product BP. Finally, we can take advantage of improvements to BP proposed in the context"
D08-1016,D07-1014,1,0.244897,"lling a first-order parsing algorithm, with edge weights q¯ij . Thus, as outlined in §2, a first-order parser is called each time we propagate through the global T REE or PT REE constraint, using edge weights that include the first-order L INK factors but also multiply in any current messages from higher-order factors. The parsing algorithm simultaneously computes the partition function b(), and all O(n2 ) marginal beliefs b(Lij = true). For PT REE (projective), it is the inside-outside version of a dynamic programming algorithm (Eisner, 1996). For T REE (nonprojective), Koo et al. (2007) and Smith and Smith (2007) show how to employ the matrix-tree theorem. In both cases, the total time is O(n3 ).19 N O C ROSSj` must sum over assignments to O(n) neighboring variables {Lij } and {Lk` }. The nonzero summands are assignments where j and ` def each Q have exactly Q one parent. At step 1, π = At step 2, the i qLij (false) · k qLk` (false). marginal belief b(Lij = true) sums over the n nonzero P assignments containing i → j. It is π · q¯Lij · ¯Lk` · PAIRij,k` , where PAIRij,k` is xj` if i → j kq crosses k → ` and is 1 otherwise. xj` is some factor value defined by equation (2) to penalize or reward the cross"
D08-1016,N06-1054,1,0.753759,"roximations. We propose borrowing a different approximation technique from machine learning, namely, loopy belief propagation (BP). In this paper, we show that BP can be used to train and decode complex parsing models. Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem.1 2 Overview and Related Work We wish to make a dependency parse’s score depend on higher-order features, which consider ar∗ This work was supported by the Human Language Technology Center of Excellence. 1 As do constraint relaxation (Tromble and Eisner, 2006) and forest reranking (Huang, 2008). In contrast, generic NP-hard solution techniques like Integer Linear Programming (Riedel and Clarke, 2006) know nothing about optimal substructure. bitrary interactions among two or more edges in the parse (and perhaps also other latent variables such as part-of-speech tags or edge labels). Such features can help accuracy—as we show. Alas, they raise the polynomial runtime of projective parsing, and render non-projective parsing NP-hard. Hence we seek approximations. We will show how BP’s “message-passing” discipline offers a principled way for higher-order"
D08-1016,J97-3002,0,0.0486573,"Missing"
D08-1016,W07-2216,0,\N,Missing
D08-1016,D07-1096,0,\N,Missing
D08-1113,P07-1013,0,0.0310607,"Missing"
D08-1113,P97-1040,1,0.722483,"Missing"
D08-1113,P02-1001,1,0.750053,"P pθ (y |x) = P P (1) y0 A∈Axy0 exp i θi fi (A) Given a parameter vector θ, we compute equation (1) using a finite-state machine. We define a WFSA, Uθ , such that Uθ [A] yields the unnormalized P def probability uθ (A) = exp i θi fi (A) for any A ∈ Σ∗ . (See section 5 for the construction.) To obtain P the numerator of equation (1), with its A∈Axy , we sum over all paths in Uθ that are compatible with x and y. That is, we build x ◦ πx−1 ◦ Uθ ◦ πy ◦ y and sum over all paths. For the denominator we build the larger machine x ◦ πx−1 ◦ Uθ and again compute the pathsum. We use standard algorithms (Eisner, 2002) to compute the pathsums as well as their gradients with respect to θ for optimization (section 4.1). Below, we will restrict our notion of valid alignment strings in Σ∗ . Uθ is constructed not to accept invalid ones, thus assigning them probability 0. Note that the possible output strings y 0 in the denominator in equation (1) may have arbitrary length, leading to an infinite summation over alignment strings. Thus, for some values of θ, the sum in the denominator diverges and the probability distribution is undefined. There exist principled ways to avoid such θ during training. However, in ou"
D08-1113,P08-1109,0,0.0299814,"Missing"
D08-1113,D07-1025,0,0.0316532,"ince many alignments are possible, we sum over all these possibilities, evaluating each alignment separately.1 At each window position, we accumulate logprobability based on the material that appears within the current window. The window is a few characters wide, and successive window positions overlap. This stands in contrast to a competing approach (Sherif and Kondrak, 2007; Zhao et al., 2007) that is inspired by phrase-based machine translation (Koehn et al., 2007), which segments the input string into substrings that are transduced independently, ignoring context.2 1 At the other extreme, Freitag and Khadivi (2007) use no alignment; each feature takes its own view of how (x, y) relate. 2 We feel that this independence is inappropriate. By analogy, it would be a poor idea for a language model to score a string highly if it could be segmented into independently frequent n-grams. Rather, language models use overlapping ngrams (indeed, it is the language model that rescues phrasebased MT from producing disjointed translations). We believe phrase-based MT avoids overlapping phrases in the channel model only because these would complicate the modeling of reordering (though see, e.g., Schwenk et al. (2007) and"
D08-1113,N07-1047,0,0.0786138,"Missing"
D08-1113,P07-2045,0,0.00557339,"tural generalization. Given an input x, we evaluate a candidate output y by moving a sliding window over the aligned (x, y) pair. More precisely, since many alignments are possible, we sum over all these possibilities, evaluating each alignment separately.1 At each window position, we accumulate logprobability based on the material that appears within the current window. The window is a few characters wide, and successive window positions overlap. This stands in contrast to a competing approach (Sherif and Kondrak, 2007; Zhao et al., 2007) that is inspired by phrase-based machine translation (Koehn et al., 2007), which segments the input string into substrings that are transduced independently, ignoring context.2 1 At the other extreme, Freitag and Khadivi (2007) use no alignment; each feature takes its own view of how (x, y) relate. 2 We feel that this independence is inappropriate. By analogy, it would be a poor idea for a language model to score a string highly if it could be segmented into independently frequent n-grams. Rather, language models use overlapping ngrams (indeed, it is the language model that rescues phrasebased MT from producing disjointed translations). We believe phrase-based MT a"
D08-1113,P05-1010,0,0.0261387,"re form pair. See sections 4.2 and 4.3 for examples of what classes were learned in our experiments. The latent string `2 splits the string pair into numbered regions. In a valid alignment string, the region numbers must increase throughout `2 , although numbers may be skipped to permit omitted regions. To guide the model to make a useful division into regions, we also require that identity characters such as (b, b) fall in even regions while change characters such as (e, o) (substitutions, deletions, or inser6 The latent class is comparable to the latent variable on the tree root symbol S in Matsuzaki et al. (2005). tions) fall in odd regions.7 Region numbers must not increase within a sequence of consecutive changes or consecutive identities.8 In Fig. 1, the start of region 1 is triggered by e:o, the start of region 2 by the identity k:k, region 3 by :e. Allowing region numbers to be skipped makes it possible to consistently assign similar labels to similar regions across different training examples. Table 2, for example, shows pairs that contain a vowel change in the middle, some of which contain an additional insertion of ge in the begining (verbinden / verbunden, reibt / gerieben). We expect the mo"
D08-1113,J97-2003,0,0.0878005,"t better than stochastic gradient descent. To decode a test example x, we wish to find yˆ = argmaxy∈Σ∗y pθ (y |x). Constructively, yˆ is the highest-probability string in the WFSA T [x], where T = πx−1 ◦Uθ ◦πy is the trained transducer that maps x nondeterministically to y. Alas, it is NP-hard to find the highest-probability string in a WFSA, even an acyclic one (Casacuberta and Higuera, 2000). The problem is that the probability of each string y is a sum over many paths in T [x] that reflect different alignments of y to x. Although it is straightforward to use a determinization construction (Mohri, 1997)13 to collapse these down to a single path per y (so that yˆ is easily read off the single best path), determinization can increase the WFSA’s size exponentially. We approximate by pruning T [x] back to its 1000-best paths before we determinize.14 Since the alignments, classes and regions are not observed in C, we do not enjoy the convex objective function of fully-supervised log-linear models. Training equation (2) therefore converges only to some local maximum that depends on the starting point in parameter space. To find a good starting point we employ staged training, a technique in which"
D08-1113,D07-1094,0,0.0325132,"Missing"
D08-1113,D07-1045,0,0.0286273,"Missing"
D08-1113,P07-1119,0,0.0108224,"os, 1998) would consider each character in isolation. To consider more context, we pursue a very natural generalization. Given an input x, we evaluate a candidate output y by moving a sliding window over the aligned (x, y) pair. More precisely, since many alignments are possible, we sum over all these possibilities, evaluating each alignment separately.1 At each window position, we accumulate logprobability based on the material that appears within the current window. The window is a few characters wide, and successive window positions overlap. This stands in contrast to a competing approach (Sherif and Kondrak, 2007; Zhao et al., 2007) that is inspired by phrase-based machine translation (Koehn et al., 2007), which segments the input string into substrings that are transduced independently, ignoring context.2 1 At the other extreme, Freitag and Khadivi (2007) use no alignment; each feature takes its own view of how (x, y) relate. 2 We feel that this independence is inappropriate. By analogy, it would be a poor idea for a language model to score a string highly if it could be segmented into independently frequent n-grams. Rather, language models use overlapping ngrams (indeed, it is the language model tha"
D08-1113,P08-1059,0,0.0196713,"lation and other tasks.9 Here we describe two sets of experiments: an inflectional morphology task in which models are trained to transduce verbs from one form into another (section 4.2), and a lemmatization task (section 4.3), in which any inflected verb is to be reduced to its root form. 4.1 Training and decoding We train θ to maximize the regularized10 conditional log-likelihood11 X log pθ (y ∗ |x) + ||θ||2 /2σ 2 , (2) (x,y ∗ )∈C where C is a supervised training corpus. To maximize (2) during training, we apply the gradientbased optimization method L-BFGS (Liu and Nocedal, 1989).12 9 E.g., Toutanova et al. (2008) improve MT performance by selecting correct morphological forms from a knowledge source. We instead focus on generalizing from observed forms and generating new forms (but see with rootlist in Table 3). 10 The variance σ 2 of the L2 prior is chosen by optimizing on development data. We are also interested in trying an L1 prior. 11 Alternatives would include faster error-driven methods (perceptron, MIRA) and slower max-margin Markov networks. 12 This worked a bit better than stochastic gradient descent. To decode a test example x, we wish to find yˆ = argmaxy∈Σ∗y pθ (y |x). Constructively, yˆ"
D08-1113,N07-1046,0,0.0202563,"ach character in isolation. To consider more context, we pursue a very natural generalization. Given an input x, we evaluate a candidate output y by moving a sliding window over the aligned (x, y) pair. More precisely, since many alignments are possible, we sum over all these possibilities, evaluating each alignment separately.1 At each window position, we accumulate logprobability based on the material that appears within the current window. The window is a few characters wide, and successive window positions overlap. This stands in contrast to a competing approach (Sherif and Kondrak, 2007; Zhao et al., 2007) that is inspired by phrase-based machine translation (Koehn et al., 2007), which segments the input string into substrings that are transduced independently, ignoring context.2 1 At the other extreme, Freitag and Khadivi (2007) use no alignment; each feature takes its own view of how (x, y) relate. 2 We feel that this independence is inappropriate. By analogy, it would be a poor idea for a language model to score a string highly if it could be segmented into independently frequent n-grams. Rather, language models use overlapping ngrams (indeed, it is the language model that rescues phrasebase"
D08-1113,P06-1096,0,\N,Missing
D09-1005,2008.amta-papers.12,1,0.723533,"sistently better (statistically significant) than MERT, despite approximating BLEU.17 Did DA help? For both n-best and hypergraph, MR+DA did obtain a better BLEU score than plain MR on the dev set.18 This shows that DA helps with the local minimum problem, as hoped. However, DA’s improvement on the dev set did not transfer to the test set. 7.4.3 Training scheme MERT (Nbest, small) MR (Nbest, small) MR+DA (Nbest, small) MR (hypergraph, small) MR+DA (hypergraph, small) MR (hypergraph, large) system. In this stage, we add 21k additional unigram and bigram target-side language model features (cf. Li and Khudanpur (2008)). For example, a specific bigram “the cat” can be a feature. Note that the total score by the baseline system is also a feature in the second-stage model. With these features and the 40k hypergraphs, we run the MR training to obtain the optimal weights. During test time, a similar procedure is followed. For a given test sentence, the baseline system first generates a hypergraph, and then the hypergraph is reranked by the second-stage model. The last row in Table 5 reports the BLEU scores. Clearly, adding more features improves (statistically significant) the case with only five features. We p"
D09-1005,W09-0424,1,0.110499,"Missing"
D09-1005,P09-1067,1,0.776741,"ergraph is already annotated with n-gram (n ≥ 4) language model states, this loss function is additively def decomposable. Using re = Le where Le is the loss for a hyperedge e, we compute the expected loss, P p(d)L(Y(d), y ∗ ) r R(p) = d∈D = (9) Z Z = log Z − d∈D def providedPthat we define re = log pe (so that r(d) = e∈d re = log p(d)). Of course, we can compute hZ, ri as explained in Section 3.2. Cross-Entropy and KL Divergence We may be interested in computing the cross-entropy or KL divergence between two distributions p and q. For example, in variational decoding for machine translation (Li et al., 2009b), p is a distribution represented by a hypergraph, while q, represented by a finite state automaton, is an approximation to p. The cross entropy between p and q is defined as X H(p, q) = − (p(d)/Zp ) log(q(d)/Zq ) (6) 6.2 Second-Order Expectation Semirings With second-order expectation semirings, we can compute from a hypergraph the expectation and variance of hypothesis length; the feature expectation vector and covariance matrix; the Hessian (matrix of second derivatives) of Z; and the gradients of entropy and expected loss. The computations should be clear from earlier discussion. Below w"
D09-1005,P06-1077,0,0.0186889,"chine translation toolkit, Joshua, enabling minimum-risk training for a benefit of up to 1.0 BLEU point. 1 Introduction A hypergraph or “packed forest” (Gallo et al., 1993; Klein and Manning, 2004; Huang and Chiang, 2005) is a compact data structure that uses structure-sharing to represent exponentially many trees in polynomial space. A weighted hypergraph also defines a probability or other weight for each tree, and can be used to represent the hypothesis space considered (for a given input) by a monolingual parser or a tree-based translation system, e.g., tree to string (Quirk et al., 2005; Liu et al., 2006), string to tree (Galley et al., 2006), tree to tree (Eisner, 2003), or string to string with latent tree structures (Chiang, 2007). ∗ This research was partially supported by the Defense Advanced Research Projects Agency’s GALE program via Contract No HR0011-06-2-0001. We are grateful to Sanjeev Khudanpur for early guidance and regular discussions. 40 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 40–51, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP goal item annealing (Rose, 1998), minimum risk (Smith and Eisner, 2006), active and semi-superv"
D09-1005,E09-1061,0,0.134067,"Computer Science and Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD 21218, USA zhifei.work@gmail.com, jason@cs.jhu.edu Abstract Given a hypergraph, we are often interested in computing some quantities over it using dynamic programming algorithms. For example, we may want to run the Viterbi algorithm to find the most probable derivation tree in the hypergraph, or the k most probable trees. Semiring-weighted logic programming is a general framework to specify these algorithms (Pereira and Warren, 1983; Shieber et al., 1994; Goodman, 1999; Eisner et al., 2005; Lopez, 2009). Goodman (1999) describes many useful semirings (e.g., Viterbi, inside, and Viterbin-best). While most of these semirings are used in “testing” (i.e., decoding), we are mainly interested in the semirings that are useful for “training” (i.e., parameter estimation). The expectation semiring (Eisner, 2002), originally proposed for finite-state machines, is one such “training” semiring, and can be used to compute feature expectations for the Estep of the EM algorithm, or gradients of the likelihood function for gradient descent. In this paper, we apply the expectation semiring (Eisner, 2002) to a"
D09-1005,P03-1021,0,0.0496256,"by X def score(d) = Φ(d) · θ = Φi (d) θi (12) i where Φ(d) ∈ Rm is a vector of features of d. We then define the unnormalized distribution p(d) as p(d) = exp(γ · score(d)) (13) where the scale factor γ adjusts how sharply the distribution favors the highest-scoring hypotheses. 7.2 Gradient Descent Optimization e∈d e∈d We can then set pe = exp(γ · scoree ), and ∇pe = γpe Φ(e), and use the algorithms described in Section 6 to compute H(p) and R(p) and their gradients with respect to θ and γ.16 Minimum-Risk Training Adjusting θ or γ changes the distribution p. Minimum error rate training (MERT) (Och, 2003) tries to tune θ to minimize the BLEU loss of a decoder that chooses the most probable output according to p. (γ has no effect.) MERT’s specialized linesearch addresses the problem that this objective function is piecewise constant, but it does not scale to a large number of parameters. Smith and Eisner (2006) instead propose a differentiable objective that can be optimized by gradient descent: the Bayes risk R(p) of (7). This is the expected loss if one were (hypothetically) to use a randomized decoder, which chooses a hypothesis d in proportion to its probability p(d). If entropy H(p) is lar"
D09-1005,2001.mtsummit-papers.68,0,0.0139015,") First-Order Expectation Semiring ER,R In Section 3, we show how to compute the expected hypothesis length or expected feature counts, using the algorithm of Figure 2 with a first-order expectation semiring ER,R . In general, given hyperedge weights hpe , pe re i, the algorithm computes hZ, ri and thus r/Z, the expectation of def P r(d) = e∈d re . We now show how to compute a few other quantities by choosing re appropriately. d∈D where Y(d) is the target yield of d and L(y, y ∗ ) is the loss of the hypothesis y with respect to the reference y ∗ . The popular machine translation metric, BLEU (Papineni et al., 2001), is not additively decomposable, and thus we are not able to compute the expected loss for it. Tromble et al. (2008) develop the following loss function, of which a linear approximation to BLEU is a special case, X L(y, y ∗ ) = −(θ0 |y |+ θw #w (y)δw (y∗)) (8) Entropy on a Hypergraph The entropy of the distribution of derivations in a hypergraph14 is X H(p) = − (p(d)/Z) log(p(d)/Z) (5) w∈N d∈D 1 X p(d) log p(d) Z d∈D 1 X r = log Z − p(d)r(d) = log Z − Z Z where w is an n-gram type, N is a set of n-gram types with n ∈ [1, 4], #w (y) is the number of occurrence of the n-gram w in y, δw (y ∗ ) i"
D09-1005,D09-1147,0,0.0942219,"loss of a decoder that chooses the most probable output according to p. (γ has no effect.) MERT’s specialized linesearch addresses the problem that this objective function is piecewise constant, but it does not scale to a large number of parameters. Smith and Eisner (2006) instead propose a differentiable objective that can be optimized by gradient descent: the Bayes risk R(p) of (7). This is the expected loss if one were (hypothetically) to use a randomized decoder, which chooses a hypothesis d in proportion to its probability p(d). If entropy H(p) is large (e.g., small γ), the Bayes risk 15 Pauls et al. (2009) concurrently developed a method to maximize the expected n-gram counts on a hypergraph using gradient descent. Their objective is similar to the minimum risk objective (though without annealing), and their gradient descent optimization involves in algorithms in computing expected feature/n-gram counts as well as expected products of features and n-gram counts, which can be viewed as instances of our general algorithms with first- and second-order semirings. They focused on tuning only a small number (i.e. nine) of features as in a regular MERT setting, while our experiments involve both a sma"
D09-1005,P83-1021,0,0.167612,"nimum-Risk Training on Translation Forests∗ Zhifei Li and Jason Eisner Department of Computer Science and Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD 21218, USA zhifei.work@gmail.com, jason@cs.jhu.edu Abstract Given a hypergraph, we are often interested in computing some quantities over it using dynamic programming algorithms. For example, we may want to run the Viterbi algorithm to find the most probable derivation tree in the hypergraph, or the k most probable trees. Semiring-weighted logic programming is a general framework to specify these algorithms (Pereira and Warren, 1983; Shieber et al., 1994; Goodman, 1999; Eisner et al., 2005; Lopez, 2009). Goodman (1999) describes many useful semirings (e.g., Viterbi, inside, and Viterbin-best). While most of these semirings are used in “testing” (i.e., decoding), we are mainly interested in the semirings that are useful for “training” (i.e., parameter estimation). The expectation semiring (Eisner, 2002), originally proposed for finite-state machines, is one such “training” semiring, and can be used to compute feature expectations for the Estep of the EM algorithm, or gradients of the likelihood function for gradient desce"
D09-1005,P05-1034,0,0.0313796,"in an open-source machine translation toolkit, Joshua, enabling minimum-risk training for a benefit of up to 1.0 BLEU point. 1 Introduction A hypergraph or “packed forest” (Gallo et al., 1993; Klein and Manning, 2004; Huang and Chiang, 2005) is a compact data structure that uses structure-sharing to represent exponentially many trees in polynomial space. A weighted hypergraph also defines a probability or other weight for each tree, and can be used to represent the hypothesis space considered (for a given input) by a monolingual parser or a tree-based translation system, e.g., tree to string (Quirk et al., 2005; Liu et al., 2006), string to tree (Galley et al., 2006), tree to tree (Eisner, 2003), or string to string with latent tree structures (Chiang, 2007). ∗ This research was partially supported by the Defense Advanced Research Projects Agency’s GALE program via Contract No HR0011-06-2-0001. We are grateful to Sanjeev Khudanpur for early guidance and regular discussions. 40 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 40–51, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP goal item annealing (Rose, 1998), minimum risk (Smith and Eisner, 2006), act"
D09-1005,P06-2101,1,0.770581,"o string (Quirk et al., 2005; Liu et al., 2006), string to tree (Galley et al., 2006), tree to tree (Eisner, 2003), or string to string with latent tree structures (Chiang, 2007). ∗ This research was partially supported by the Defense Advanced Research Projects Agency’s GALE program via Contract No HR0011-06-2-0001. We are grateful to Sanjeev Khudanpur for early guidance and regular discussions. 40 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 40–51, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP goal item annealing (Rose, 1998), minimum risk (Smith and Eisner, 2006), active and semi-supervised learning (Grandvalet and Bengio, 2004; Jiao et al., 2006). In these settings, we must compute the gradient of entropy or risk. The semirings can also be used for second-order gradient optimization algorithms. We implement the expectation and variance semirings in Joshua (Li et al., 2009a), and demonstrate their practical benefit by using minimumrisk training to improve Hiero (Chiang, 2007). 2 S→""X0 , X0 # X→""X0 的 X1 , X1 on X0 # X→""X0 的 X1 , X0 ’s X1 # X→""X0 的 X1 , X1 of X0 # X→""X0 的 X1 , X0 X1 # X 0,2 the mat NA X 3,4 a cat NA X→""垫子 上, the mat# Semiring Parsing on"
D09-1005,D08-1065,0,0.553343,"d feature counts, using the algorithm of Figure 2 with a first-order expectation semiring ER,R . In general, given hyperedge weights hpe , pe re i, the algorithm computes hZ, ri and thus r/Z, the expectation of def P r(d) = e∈d re . We now show how to compute a few other quantities by choosing re appropriately. d∈D where Y(d) is the target yield of d and L(y, y ∗ ) is the loss of the hypothesis y with respect to the reference y ∗ . The popular machine translation metric, BLEU (Papineni et al., 2001), is not additively decomposable, and thus we are not able to compute the expected loss for it. Tromble et al. (2008) develop the following loss function, of which a linear approximation to BLEU is a special case, X L(y, y ∗ ) = −(θ0 |y |+ θw #w (y)δw (y∗)) (8) Entropy on a Hypergraph The entropy of the distribution of derivations in a hypergraph14 is X H(p) = − (p(d)/Z) log(p(d)/Z) (5) w∈N d∈D 1 X p(d) log p(d) Z d∈D 1 X r = log Z − p(d)r(d) = log Z − Z Z where w is an n-gram type, N is a set of n-gram types with n ∈ [1, 4], #w (y) is the number of occurrence of the n-gram w in y, δw (y ∗ ) is an indicator to check if y ∗ contains at least one occurrence of w, and θn is the weight indicating the relative im"
D09-1005,N09-1025,0,0.0190503,"re. Note that the total score by the baseline system is also a feature in the second-stage model. With these features and the 40k hypergraphs, we run the MR training to obtain the optimal weights. During test time, a similar procedure is followed. For a given test sentence, the baseline system first generates a hypergraph, and then the hypergraph is reranked by the second-stage model. The last row in Table 5 reports the BLEU scores. Clearly, adding more features improves (statistically significant) the case with only five features. We plan to incorporate more informative features described by Chiang et al. (2009).19 8 Conclusions We presented first-order expectation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings. This enables efficient computation of many interesting quantities over the exponentially many derivations encoded in a hypergraph: second derivatives (Hessians), expectations of products (covariances), and expectations such as risk and entropy along with their derivatives. To our knowledge, algorithms for these problems have not been presented before. Our approach is theoretically elegant, like other"
D09-1005,J07-2003,0,0.52133,"“packed forest” (Gallo et al., 1993; Klein and Manning, 2004; Huang and Chiang, 2005) is a compact data structure that uses structure-sharing to represent exponentially many trees in polynomial space. A weighted hypergraph also defines a probability or other weight for each tree, and can be used to represent the hypothesis space considered (for a given input) by a monolingual parser or a tree-based translation system, e.g., tree to string (Quirk et al., 2005; Liu et al., 2006), string to tree (Galley et al., 2006), tree to tree (Eisner, 2003), or string to string with latent tree structures (Chiang, 2007). ∗ This research was partially supported by the Defense Advanced Research Projects Agency’s GALE program via Contract No HR0011-06-2-0001. We are grateful to Sanjeev Khudanpur for early guidance and regular discussions. 40 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 40–51, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP goal item annealing (Rose, 1998), minimum risk (Smith and Eisner, 2006), active and semi-supervised learning (Grandvalet and Bengio, 2004; Jiao et al., 2006). In these settings, we must compute the gradient of entropy or risk."
D09-1005,2005.iwslt-1.1,0,0.0173429,"general algorithms with first- and second-order semirings. They focused on tuning only a small number (i.e. nine) of features as in a regular MERT setting, while our experiments involve both a small and a large number of features. 16 It is easy to verify that the gradient of a function f (e.g. entropy or risk) with respect to γ can be written as a weighted sum of gradients with respect to the feature weights θi , i.e. ∂f 1X ∂f = θi × (16) ∂γ γ i ∂θi 49 7.4 7.4.1 Experimental Results Experimental Setup We built a translation model on a corpus for IWSLT 2005 Chinese-to-English translation task (Eck and Hori, 2005), which consists of 40k pairs of sentences. We used a 5-gram language model with modified Kneser-Ney smoothing, trained on the bitext’s English using SRILM (Stolcke, 2002). 7.4.2 NEW ! NEW ! NEW ! dev 42.6 40.8 41.6 41.3 41.9 42.3 test 47.7 47.7 47.8 48.4 48.3 48.7 Table 5: BLEU scores on the Dev and test sets under different training scenarios. In the “small” model, five features (i.e., one for the language model, three for the translation model, and one for word penalty) are tuned. In the “large” model, 21k additional unigram and bigram features are used. Tuning a Small Number of Features We"
D09-1005,H05-1036,1,0.643214,"Eisner Department of Computer Science and Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD 21218, USA zhifei.work@gmail.com, jason@cs.jhu.edu Abstract Given a hypergraph, we are often interested in computing some quantities over it using dynamic programming algorithms. For example, we may want to run the Viterbi algorithm to find the most probable derivation tree in the hypergraph, or the k most probable trees. Semiring-weighted logic programming is a general framework to specify these algorithms (Pereira and Warren, 1983; Shieber et al., 1994; Goodman, 1999; Eisner et al., 2005; Lopez, 2009). Goodman (1999) describes many useful semirings (e.g., Viterbi, inside, and Viterbin-best). While most of these semirings are used in “testing” (i.e., decoding), we are mainly interested in the semirings that are useful for “training” (i.e., parameter estimation). The expectation semiring (Eisner, 2002), originally proposed for finite-state machines, is one such “training” semiring, and can be used to compute feature expectations for the Estep of the EM algorithm, or gradients of the likelihood function for gradient descent. In this paper, we apply the expectation semiring (Eisn"
D09-1005,P02-1001,1,0.380807,"ant to run the Viterbi algorithm to find the most probable derivation tree in the hypergraph, or the k most probable trees. Semiring-weighted logic programming is a general framework to specify these algorithms (Pereira and Warren, 1983; Shieber et al., 1994; Goodman, 1999; Eisner et al., 2005; Lopez, 2009). Goodman (1999) describes many useful semirings (e.g., Viterbi, inside, and Viterbin-best). While most of these semirings are used in “testing” (i.e., decoding), we are mainly interested in the semirings that are useful for “training” (i.e., parameter estimation). The expectation semiring (Eisner, 2002), originally proposed for finite-state machines, is one such “training” semiring, and can be used to compute feature expectations for the Estep of the EM algorithm, or gradients of the likelihood function for gradient descent. In this paper, we apply the expectation semiring (Eisner, 2002) to a hypergraph (or packed forest) rather than just a lattice. We then propose a novel second-order expectation semiring, nicknamed the “variance semiring.” The original first-order expectation semiring allows us to efficiently compute a vector of firstorder statistics (expectations; first derivatives) on th"
D09-1005,P03-2041,1,0.569711,"benefit of up to 1.0 BLEU point. 1 Introduction A hypergraph or “packed forest” (Gallo et al., 1993; Klein and Manning, 2004; Huang and Chiang, 2005) is a compact data structure that uses structure-sharing to represent exponentially many trees in polynomial space. A weighted hypergraph also defines a probability or other weight for each tree, and can be used to represent the hypothesis space considered (for a given input) by a monolingual parser or a tree-based translation system, e.g., tree to string (Quirk et al., 2005; Liu et al., 2006), string to tree (Galley et al., 2006), tree to tree (Eisner, 2003), or string to string with latent tree structures (Chiang, 2007). ∗ This research was partially supported by the Defense Advanced Research Projects Agency’s GALE program via Contract No HR0011-06-2-0001. We are grateful to Sanjeev Khudanpur for early guidance and regular discussions. 40 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 40–51, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP goal item annealing (Rose, 1998), minimum risk (Smith and Eisner, 2006), active and semi-supervised learning (Grandvalet and Bengio, 2004; Jiao et al., 2006). In"
D09-1005,P06-1121,0,0.0462454,"enabling minimum-risk training for a benefit of up to 1.0 BLEU point. 1 Introduction A hypergraph or “packed forest” (Gallo et al., 1993; Klein and Manning, 2004; Huang and Chiang, 2005) is a compact data structure that uses structure-sharing to represent exponentially many trees in polynomial space. A weighted hypergraph also defines a probability or other weight for each tree, and can be used to represent the hypothesis space considered (for a given input) by a monolingual parser or a tree-based translation system, e.g., tree to string (Quirk et al., 2005; Liu et al., 2006), string to tree (Galley et al., 2006), tree to tree (Eisner, 2003), or string to string with latent tree structures (Chiang, 2007). ∗ This research was partially supported by the Defense Advanced Research Projects Agency’s GALE program via Contract No HR0011-06-2-0001. We are grateful to Sanjeev Khudanpur for early guidance and regular discussions. 40 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 40–51, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP goal item annealing (Rose, 1998), minimum risk (Smith and Eisner, 2006), active and semi-supervised learning (Grandvalet and Bengio,"
D09-1005,E09-1037,0,0.0265265,"ctation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings. This enables efficient computation of many interesting quantities over the exponentially many derivations encoded in a hypergraph: second derivatives (Hessians), expectations of products (covariances), and expectations such as risk and entropy along with their derivatives. To our knowledge, algorithms for these problems have not been presented before. Our approach is theoretically elegant, like other work in this vein (Goodman, 1999; Lopez, 2009; Gimpel and Smith, 2009). We used it practically to enable a new form of minimum-risk training that improved Chinese-English MT by 1.0 BLEU point. Our implementation will be released within the open-source MT toolkit Joshua (Li et al., 2009a). Tuning a Large Number of Features MR (with or without DA) is scalable to tune a large number of features, while MERT is not. To achieve competitive performance, we adopt a forest reranking approach (Li and Khudanpur, 2009; Huang, 2008). Specifically, our training has two stages. In the first stage, we train a baseline system as usual. We also find the optimal feature weights fo"
D09-1005,J99-4004,0,0.487791,"ei Li and Jason Eisner Department of Computer Science and Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD 21218, USA zhifei.work@gmail.com, jason@cs.jhu.edu Abstract Given a hypergraph, we are often interested in computing some quantities over it using dynamic programming algorithms. For example, we may want to run the Viterbi algorithm to find the most probable derivation tree in the hypergraph, or the k most probable trees. Semiring-weighted logic programming is a general framework to specify these algorithms (Pereira and Warren, 1983; Shieber et al., 1994; Goodman, 1999; Eisner et al., 2005; Lopez, 2009). Goodman (1999) describes many useful semirings (e.g., Viterbi, inside, and Viterbin-best). While most of these semirings are used in “testing” (i.e., decoding), we are mainly interested in the semirings that are useful for “training” (i.e., parameter estimation). The expectation semiring (Eisner, 2002), originally proposed for finite-state machines, is one such “training” semiring, and can be used to compute feature expectations for the Estep of the EM algorithm, or gradients of the likelihood function for gradient descent. In this paper, we apply the expec"
D09-1005,W05-1506,0,0.0115549,"econd-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy). This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk. We use these semirings in an open-source machine translation toolkit, Joshua, enabling minimum-risk training for a benefit of up to 1.0 BLEU point. 1 Introduction A hypergraph or “packed forest” (Gallo et al., 1993; Klein and Manning, 2004; Huang and Chiang, 2005) is a compact data structure that uses structure-sharing to represent exponentially many trees in polynomial space. A weighted hypergraph also defines a probability or other weight for each tree, and can be used to represent the hypothesis space considered (for a given input) by a monolingual parser or a tree-based translation system, e.g., tree to string (Quirk et al., 2005; Liu et al., 2006), string to tree (Galley et al., 2006), tree to tree (Eisner, 2003), or string to string with latent tree structures (Chiang, 2007). ∗ This research was partially supported by the Defense Advanced Researc"
D09-1005,P08-1067,0,0.0213868,"lems have not been presented before. Our approach is theoretically elegant, like other work in this vein (Goodman, 1999; Lopez, 2009; Gimpel and Smith, 2009). We used it practically to enable a new form of minimum-risk training that improved Chinese-English MT by 1.0 BLEU point. Our implementation will be released within the open-source MT toolkit Joshua (Li et al., 2009a). Tuning a Large Number of Features MR (with or without DA) is scalable to tune a large number of features, while MERT is not. To achieve competitive performance, we adopt a forest reranking approach (Li and Khudanpur, 2009; Huang, 2008). Specifically, our training has two stages. In the first stage, we train a baseline system as usual. We also find the optimal feature weights for the five features mentioned before, using the method of MR+DA operating on a hypergraph. In the second stage, we generate a hypergraph for each sentence in the training data (which consists of about 40k sentence pairs), using the baseline 17 Pauls et al. (2009) concurrently observed a similar pattern (i.e., MR performs worse than MERT on the dev set, but performs better on a test set). 18 We also verified that MR+DA found a better objective value (i"
D09-1005,P06-1027,0,0.0229023,"Missing"
D09-1005,H93-1021,0,0.160443,"rly straightforwardly. Pearlmutter and Siskind (2007) give the relevant generalizations of dual numbers. Log-linear Models as a Special Case Replacing ∇pe with pe re is unnecessary if ∇pe already equals pe re . That is the case in log-linear models, def where pe = exp(re · θ) for some feature vector re associated with e. So there, ∇Z already equals r¯—yielding a key useful property of log-linear P P 13 e∈d (∇p P Proof: r(d) = P e∈d re = Qe )/pe = e∈d ∇ log pe = ∇ e∈d log pe = ∇ log e∈d pe = ∇ log p(d) = (∇p(d))/p(d). 47 models, that ∇ log Z = (∇Z)/Z = r¯/Z, the vector of feature expectations (Lau et al., 1993). 6 where the first term Zq can be computed using the inside algorithm with hyperedge weights qe , and the numerator and denominator of the second term using an expectation semiring with hydef peredge weights hpe , pe re i with re = log qe . The KL divergence to p from q can be computed as KL(p k q) = H(p, q) − H(p). Practical Applications Given a hypergraph HG whose hyperedges e are annotated with values pe . Recall from Section 3.1 that this defines a probability distribution over all derivations d in the hypergraph, namely p(d)/Z def Q where p(d) = e∈d pe . 6.1 Expected Loss (Risk) Given a"
D09-1005,P02-1040,0,\N,Missing
D09-1011,P09-1067,1,0.594231,"he gradient of that likelihood function with respect to θ.14 6 6.1 Comparison With Other Approaches Multi-tape WFSMs In principle, one could use a 100-tape WFSM to jointly model the 100 distinct forms of a typical Polish verb. In other words, the WFSM would de~ = scribe the distribution of a random variable V hV1 , . . . , V100 i, where each Vi is a string. One would train the parameters of the WFSM on a ~ , each sample being a fully or partially sample of V observed paradigm for some Polish verb. The resulting distribution could be used to infer missing forms for these or other verbs. 12 See Li et al. (2009, footnote 9) for a sketch of the construction, which finds locally normalized edge weights. Or if Q is large but parameterized by some compact parameter vector φ, so we are only allowed to control its edge weights via φ, then Li and Eisner (2009, section 6) explain how to minimize KL(P k Q) by gradient descent. In both cases Q must be deterministic. We remark that if a factor F were specified by a synchronous grammar rather than a WFSM, then its outgoing messages would be weighted context-free languages. Exact intersection of these is undecidable, but they too can be approximated variationall"
D09-1011,J97-2003,0,0.0146653,"ction 3.2), there is an upper bound on the weights it assigns to strings. That guarantees that all the messages and beliefs computed by (2)–(4) will be proper FSMs, provided that all the factors are proper WFSMs. 9 If it is unary, (3) trivially reduces to µF →U = F . 10 The usual implementation of projection does not change the topology of the WFST, but only deletes the U 0 part of its arc labels. Thus, multiple paths that accept the same value of U remain distinct according to the distinct values of U 0 that they were paired with before projection. 11 If there is no deterministic equivalent (Mohri, 1997). • Each message is a WFSA. • Messages are typically initialized to a one-state WFSA that accepts all strings in Σ∗ , each with 6 Preferably when the beliefs converge to some fixed point (a local minimum of the Bethe free energy). However, convergence is not guaranteed. 7 More generally, on all possible observed variables. 104 set. They are distributions over the infinite set Σ∗ . A WFSA represents this in finite space, but more complex distributions require bigger WFSAs, with more distinct states and arc weights. Facing the same problem for distributions over the infinite set R, Sudderth et a"
D09-1011,N09-1008,0,0.0596871,"Missing"
D09-1011,E09-1020,0,0.0316005,"Missing"
D09-1011,D08-1016,1,0.756635,"er its arguments are equal. Under this model, p() &lt; 1 iff there exists a string x 6=  that can be transduced to itself by the unweighted transducer F2 . This question can be used to encode any instance of Post’s Correspondence Problem, so is undecidable. 4 Notice that the simplest approximation to cure undecidability would be to impose an arbitrary maximum on string length, so that the random variables have a finite domain, just as in most discrete graphical models. 5 Notable exceptions are Sutton et al. (2004) for chunking and tagging, Sutton and McCallum (2004) for information extraction, Smith and Eisner (2008) for dependency parsing, and Cromier`es and Kurohashi (2009) for alignment. 103 weight 1.8 • Taking a pointwise product of messages to V in equation (2) corresponds to WFSA intersection. • If F in equation (3) is binary,9 then there is only one U 0 . Then the outgoing message µF →U , a WFSA, is computed as domain(F ◦ µU 0 →F ). The factor F can then collect such incoming messages from neighboring variables and send its own message on to another neighbor U . Such a message µF →U suggests good values for U , in the form of an (unnormalized) distribution over U ’s values u, computed by µF →U (u)"
D09-1011,P05-1003,0,0.022954,"f the gradient can be misleading. Second, semisupervised training (which we will attempt below) is always difficult and prone to local optima. As in EM, a small number of supervised examples for some variable may be drowned out by many noisily reconstructed examples. Faster and potentially more stable approaches include the piecewise training methods of Sutton and McCallum (2008), which train the factors independently or in small groups. In the semisupervised case, each factor can be trained on only the supervised forms available for it. It might be useful to reweight the trained factors (cf. Smith et al. (2005)), or train the factors consecutively (cf. Fahlman and Lebiere (1990)), in a way that minimizes the loss of BP decoding on held-out data. Training the Model Parameters Any standard training method for MRFs will transfer naturally to our setting. In all cases we draw on Eisner (2002), who showed how to train the parameters θ of a single WFST, F , to (locally) maximize the joint or conditional probability of fully or partially observed training data. This involves computing the gradient of that likelihood function with respect to θ.14 6 6.1 Comparison With Other Approaches Multi-tape WFSMs In pr"
D09-1011,D08-1113,1,0.845174,"approximate joint inference using belief propagation.22 We extract our output from the final beliefs: for each unseen variable V , we preModel factors and parameters Our current MRF uses only binary factors. Each factor is a WFST that is trained to relate 2 of the 10 variables (morphological forms). Each WFST can score an aligned pair using a log-linear model that counts features in a sliding 3-character window. To score an unaligned pair, it sums over all possible alignments. Specifically, our WFST topology and parameterization follow the state-of-theart approach to supervised morphology in Dreyer et al. (2008), although we dropped some of their features to speed up these early experiments.19 We seemed to hurt in our current training setup. We followed Dreyer et al. (2008) in slightly pruning the space of possible alignments. We compensated by replacing their WFST, F , with the union F ∪ 10−12 (0.999Σ × Σ)∗ . This ensured that the factor could still map any string to any other string (though perhaps with very low weight), guaranteeing that the intersection at the end of section 4.3 would be non-empty. 20 The second term is omitted if V is the lemma. We do not train the model to predict the lemma sin"
D09-1011,P02-1001,1,0.620795,"m x before consuming the next character from y. 2 Weighted acceptors and transducers are the cases k = 1 and k = 2, which are said to define rational languages and rational relations. 102 3.3 µV →F Parameters Our probability model has trainable parameters: a vector of feature weights θ ∈ R. Each arc in each WFSM has a real-valued weight that depends on θ. Thus, tuning θ during training will change the arc weights, hence the path weights, the factor functions, and the whole probability distribution p(A). Designing the probability model includes specifying the topology and weights of each WFSM. Eisner (2002) explains how to specify and train such parameterized WFSMs. Typically, the weight of an arc is a simple sum like θ12 + θ55 + θ72 , where θ12 is included on all arcs that share feature 12. However, more interesting parameterizations arise if the WFSM is constructed by operations such as transducer composition, or from a weighted regular expression. 3.4 V µF →U F U Figure 2: Illustration of messages being passed from variable to factor and factor to variable. Each message is represented by a finite-state acceptor. veloped many such methods, to deal with the computational intractability (if not"
D09-1011,P97-1017,0,0.0417604,"models of several strings. To our knowledge, they have all chosen acyclic, directed graphical models. The acyclicity meant that exact inference was at least possible for them, if not necessarily efficient. The factors in these past models have been WFSTs (though typically simpler than the ones we will use). Many papers have used cascades of probabilistic finite-state transducers. Such a cascade may be regarded as a directed graphical model with a linear-chain structure. Pereira and Riley (1997) built a speech recognizer in this way, relating acoustic to phonetic to lexical strings. Similarly, Knight and Graehl (1997) presented a generative cascade using 4 variables and 5 factors: def p(w, e, j, k, o) = p(w)·p(e |w)·p(j |e)·p(k |j) ·p(o |k) where e is an English word sequence, w its pronunciation, j a Japanese version of the pronunciation, k a katakana rendering of the Japanese pronunciation, and o an OCR-corrupted version of the katakana. Knight and Graehl used finite-state operations to perform inference at test time, observing o and recovering the most likely w, while marginalizing out e, j, and k. Bouchard-Cˆot´e et al. (2009) reconstructed an7 Experiments To study our approach, we conducted initial ex"
D09-1011,D09-1005,1,0.0971915,"the WFSM would de~ = scribe the distribution of a random variable V hV1 , . . . , V100 i, where each Vi is a string. One would train the parameters of the WFSM on a ~ , each sample being a fully or partially sample of V observed paradigm for some Polish verb. The resulting distribution could be used to infer missing forms for these or other verbs. 12 See Li et al. (2009, footnote 9) for a sketch of the construction, which finds locally normalized edge weights. Or if Q is large but parameterized by some compact parameter vector φ, so we are only allowed to control its edge weights via φ, then Li and Eisner (2009, section 6) explain how to minimize KL(P k Q) by gradient descent. In both cases Q must be deterministic. We remark that if a factor F were specified by a synchronous grammar rather than a WFSM, then its outgoing messages would be weighted context-free languages. Exact intersection of these is undecidable, but they too can be approximated variationally by WFSAs, with the same methods. 13 We are also considering other ways of adaptively choosing the topology of WFSA approximations at runtime, particularly in conjunction with expectation propagation. 14 The likelihood is usually non-convex; eve"
D09-1011,P07-2045,0,\N,Missing
D09-1086,P05-1012,0,0.402187,"e.1 Given the two sentences w, w0 , our probability distribution over possible graphs considers local features of the parses, the alignment, and both jointly. Thus, we learn what local syntactic configurations tend to occur in each language and how they correspond across languages. As a result, we might learn that parses are “mostly synchronous,” but that there are some systematic cross-linguistic 1 i + X wj gj (t, t0 , a, w, w0 ) j The features f look only at target words and dependencies. In the conditional models of §3 and §6, these features are those of an edge-factored dependency parser (McDonald et al., 2005). In the generative models of §5, f has the form of a dependency model with valence (Klein and Manning, 2004). All models, for instance, have a feature template that considers the parts of speech of a potential parent-child relation. In order to benefit from the source language, we also need to include bilingual features g. When scoring a candidate target dependency link from word x → y, these features consider the relationship of their corresponding source words x0 and y 0 . (The correspondences are determined by the alignment a.) For instance, the source tree t0 may contain the link x0 → y 0"
D09-1086,D08-1092,0,0.119978,"oordination show up in human-produced dependency treebanks. (The other possibility is a reverse Mel’ˇcuk scheme.) These treebanks also differ on other conventions. Figure 2: With the English tree and alignment provided by a parser and aligner at test time, the Chinese parser finds the correct dependencies (see §6). A monolingual parser’s incorrect edges are shown with dashed lines. This model should enable us to convert the original large source corpus to target style, giving us additional training data in the target style. 1.2 recover t), as well as full joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008). In this paper, we condition on the 1-best source tree t0 . As for the alignment a, our models either condition on the 1-best alignment or integrate the alignment out. Our models are thus of the form p(t |w, w0 , t0 , a) or, in the generative case, p(w, t, a |w0 , t0 ). We intend to consider other formulations in future work. So far, this is very similar to the monolingual parser adaptation scenario, but there are a few key differences. Since the source and target sentences in the bitext are in different languages, there is no longer a trivial alignment between the words of the source and tar"
D09-1086,W06-3104,1,0.752311,"t, a, t0 |w, w0 ). Such a joint model captures how t, a, t0 mutually constrain each other, so that even partial knowledge of some of these three variables can help us to recover the others when training or decoding on bilingual text. This idea underlies a number of recent papers on syntax-based alignment (using t and t0 to better recover a), grammar induction from bitext (using a to better recover t and t0 ), parser projection (using t0 and a to better 823 divergences and some instances of sloppy (nonparallel or inexact) translation. Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). All the models in this paper are conditioned on the source tree t0 . Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to w0 and thus have the form p(t |w, w0 , t0 , a); the unsupervised, generative projection models in §5 have the form p(w, t, a |w0 , t0 ). The score s of a given tuple of trees, words, and ali"
D09-1086,P09-1053,0,0.0520215,"cover the others when training or decoding on bilingual text. This idea underlies a number of recent papers on syntax-based alignment (using t and t0 to better recover a), grammar induction from bitext (using a to better recover t and t0 ), parser projection (using t0 and a to better 823 divergences and some instances of sloppy (nonparallel or inexact) translation. Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). All the models in this paper are conditioned on the source tree t0 . Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to w0 and thus have the form p(t |w, w0 , t0 , a); the unsupervised, generative projection models in §5 have the form p(w, t, a |w0 , t0 ). The score s of a given tuple of trees, words, and alignment can thus be written as a dot product of weights w with features f and g: X s(t, t0 , a, w, w0 ) = wi fi (t, w) phrase that confused the undertrained monolingual pa"
D09-1086,P07-1033,0,0.0319557,"Missing"
D09-1086,P06-1072,1,0.684427,"t, a, t0 |w, w0 ). Such a joint model captures how t, a, t0 mutually constrain each other, so that even partial knowledge of some of these three variables can help us to recover the others when training or decoding on bilingual text. This idea underlies a number of recent papers on syntax-based alignment (using t and t0 to better recover a), grammar induction from bitext (using a to better recover t and t0 ), parser projection (using t0 and a to better 823 divergences and some instances of sloppy (nonparallel or inexact) translation. Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). All the models in this paper are conditioned on the source tree t0 . Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to w0 and thus have the form p(t |w, w0 , t0 , a); the unsupervised, generative projection models in §5 have the form p(w, t, a |w0 , t0 ). The score s of a given tuple of trees, words, and ali"
D09-1086,D07-1070,1,0.836522,"ser corrects additional errors in the second half of this sentence, which has been removed to improve legibility.) The parser is able to recover the long-distance dependency from the first Chinese word (China) to the last (begun), while skipping over the intervening noun Parser Projection For many target languages, however, we do not have the luxury of a large parsed “source corpus” in the language, even one in a different style or domain as above. Thus, we may seek other forms of data to augment our small target corpus. One option would be to leverage unannotated text (McClosky et al., 2006; Smith and Eisner, 2007). But we can also try to transfer syntactic information from a parsed source corpus in another language. This is an extreme case of out-of-domain data. This leads to the second task of this paper: learning a statistical model to transform a syntactic analysis of a sentence in one language into an analysis of its translation. Tree transformations are often modeled with synchronous grammars. Suppose we are given a sentence w0 in the “source” language and its translation w into the “target” language. Their syntactic parses t0 and t are presumably not independent, but will tend to have some parall"
D09-1086,D08-1016,1,0.0785439,"Missing"
D09-1086,P09-1042,0,0.530933,"Missing"
D09-1086,W04-3207,1,0.637558,"chemes for annotating coordination show up in human-produced dependency treebanks. (The other possibility is a reverse Mel’ˇcuk scheme.) These treebanks also differ on other conventions. Figure 2: With the English tree and alignment provided by a parser and aligner at test time, the Chinese parser finds the correct dependencies (see §6). A monolingual parser’s incorrect edges are shown with dashed lines. This model should enable us to convert the original large source corpus to target style, giving us additional training data in the target style. 1.2 recover t), as well as full joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008). In this paper, we condition on the 1-best source tree t0 . As for the alignment a, our models either condition on the 1-best alignment or integrate the alignment out. Our models are thus of the form p(t |w, w0 , t0 , a) or, in the generative case, p(w, t, a |w0 , t0 ). We intend to consider other formulations in future work. So far, this is very similar to the monolingual parser adaptation scenario, but there are a few key differences. Since the source and target sentences in the bitext are in different languages, there is no longer a trivial alignment between the w"
D09-1086,D09-1023,0,0.0343813,"ilingual text. This idea underlies a number of recent papers on syntax-based alignment (using t and t0 to better recover a), grammar induction from bitext (using a to better recover t and t0 ), parser projection (using t0 and a to better 823 divergences and some instances of sloppy (nonparallel or inexact) translation. Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). All the models in this paper are conditioned on the source tree t0 . Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to w0 and thus have the form p(t |w, w0 , t0 , a); the unsupervised, generative projection models in §5 have the form p(w, t, a |w0 , t0 ). The score s of a given tuple of trees, words, and alignment can thus be written as a dot product of weights w with features f and g: X s(t, t0 , a, w, w0 ) = wi fi (t, w) phrase that confused the undertrained monolingual parser. Although, due to the auxiliary verb, “China”"
D09-1086,P09-1009,0,0.106485,"Missing"
D09-1086,D09-1127,0,0.0627136,"Missing"
D09-1086,D07-1003,0,0.0329947,"of these three variables can help us to recover the others when training or decoding on bilingual text. This idea underlies a number of recent papers on syntax-based alignment (using t and t0 to better recover a), grammar induction from bitext (using a to better recover t and t0 ), parser projection (using t0 and a to better 823 divergences and some instances of sloppy (nonparallel or inexact) translation. Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). All the models in this paper are conditioned on the source tree t0 . Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to w0 and thus have the form p(t |w, w0 , t0 , a); the unsupervised, generative projection models in §5 have the form p(w, t, a |w0 , t0 ). The score s of a given tuple of trees, words, and alignment can thus be written as a dot product of weights w with features f and g: X s(t, t0 , a, w, w0 ) = wi fi (t, w) phrase t"
D09-1086,P02-1017,0,0.0603758,"ntain posterior probabilities or null links, nor do they distinguish which links are in the IBM Model intersection, we treated all links as equally likely when learning the QG. Target language words unaligned to any source language words were the only nodes allowed to align to NULL in QG derivations. We parsed the English side of the bitext with the projective dependency parser described by McDonald et al. (2005) trained on the Penn Treebank §§2–20. Much previous work on unsupervised grammar induction has used gold-standard partof-speech tags (Smith and Eisner, 2006b; Klein and Manning, 2004; Klein and Manning, 2002). While there are no gold-standard tags for the Europarl bitext, we did train a conditional Markov Details of EM Training As in previous work on grammar induction, we learn the DMV from part-of-speech-tagged targetlanguage text. We use expectation maximization (EM) to maximize the likelihood of the data. Since the likelihood function is nonconvex in the unsupervised case, our choice of initial parameters can have a significant effect on the outcome. Although we could also try many random starting points, the initializer in Klein and Manning (2004) performs quite well. The base dependency parse"
D09-1086,P04-1061,0,0.878939,"res of the parses, the alignment, and both jointly. Thus, we learn what local syntactic configurations tend to occur in each language and how they correspond across languages. As a result, we might learn that parses are “mostly synchronous,” but that there are some systematic cross-linguistic 1 i + X wj gj (t, t0 , a, w, w0 ) j The features f look only at target words and dependencies. In the conditional models of §3 and §6, these features are those of an edge-factored dependency parser (McDonald et al., 2005). In the generative models of §5, f has the form of a dependency model with valence (Klein and Manning, 2004). All models, for instance, have a feature template that considers the parts of speech of a potential parent-child relation. In order to benefit from the source language, we also need to include bilingual features g. When scoring a candidate target dependency link from word x → y, these features consider the relationship of their corresponding source words x0 and y 0 . (The correspondences are determined by the alignment a.) For instance, the source tree t0 may contain the link x0 → y 0 , which would cause a feature for monotonic projection to fire for the x → y edge. If, on the other hand, y"
D09-1086,P04-1060,0,0.214054,"Missing"
D09-1086,D08-1017,0,0.0908269,"Missing"
D09-1086,P06-1043,0,0.0449812,"nes. (The bilingual parser corrects additional errors in the second half of this sentence, which has been removed to improve legibility.) The parser is able to recover the long-distance dependency from the first Chinese word (China) to the last (begun), while skipping over the intervening noun Parser Projection For many target languages, however, we do not have the luxury of a large parsed “source corpus” in the language, even one in a different style or domain as above. Thus, we may seek other forms of data to augment our small target corpus. One option would be to leverage unannotated text (McClosky et al., 2006; Smith and Eisner, 2007). But we can also try to transfer syntactic information from a parsed source corpus in another language. This is an extreme case of out-of-domain data. This leads to the second task of this paper: learning a statistical model to transform a syntactic analysis of a sentence in one language into an analysis of its translation. Tree transformations are often modeled with synchronous grammars. Suppose we are given a sentence w0 in the “source” language and its translation w into the “target” language. Their syntactic parses t0 and t are presumably not independent, but will"
D09-1086,D07-1112,0,\N,Missing
D09-1105,2005.iwslt-1.8,0,0.0842658,"Missing"
D09-1105,P07-2045,0,0.00586889,"1 do 13: ∆[i, j, k] ← ∆[i, j, k − 1] + ∆[i + 1, j, k] − ∆[i + 1, j, k − 1] + B[πk , πi+1 ] − B[πi+1 , πk ] 14: β[i, k] ← max(β[i, k], β[i, j] + β[j, k] + max(0, ∆[i, j, k])) 15: end for 16: end for 17: end for 18: return β[0, n] 19: end procedure Figure 3: Pseudocode for computing the score of the best permutation in the neighborhood of π under the Linear Ordering Problem specified by the matrix B. Computing the best neighbor is a simple matter of keeping back pointers to the choices of max and ordering them as implied. alignments using the “grow-diag-final-and” procedure provided with Moses (Koehn et al., 2007). For each of these German sentences, we derived the English-like reordering of it, which we call German0 , by the following procedure. Each German token was assigned an integer key, namely the position of the leftmost of the English tokens to which it was aligned, or 0 if it was not aligned to any English tokens. We then did a stable sort of the German tokens based on these keys, meaning that if two German tokens had the same key, their order was preserved. This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. They kept"
D09-1105,2005.mtsummit-papers.11,0,0.00311099,"s permuted into target English word order—from which to train the parameters of our model. Unfortunately, the alignments between German and English sentences are only infrequently one-to-one. Furthermore, human-aligned parallel sentences are hard to come by, and never in the quantity we would like. Instead, we make do with automaticallygenerated word alignments, and we heuristically derive an English-like word order for the German sentence based on the alignment. We used GIZA++ (Och and Ney, 2003) to align approximately 751,000 sentences from the German-English portion of the Europarl corpus (Koehn, 2005), in both the German-to-English and English-to-German directions. We combined the 1010 1: procedure L OCAL S EARCH S TEP(B, π, n) 2: for i ← 0 to n − 1 do 3: β[i, i + 1] ← 0 4: for k ← i + 1 to n do 5: ∆[i, i, k] ← ∆[i, k, k] ← 0 6: end for 7: end for 8: for w ← 2 to n do 9: for i ← 0 to n − w do 10: k ←i+w 11: β[i, k] ← −∞ 12: for j ← i + 1 to k − 1 do 13: ∆[i, j, k] ← ∆[i, j, k − 1] + ∆[i + 1, j, k] − ∆[i + 1, j, k − 1] + B[πk , πi+1 ] − B[πi+1 , πk ] 14: β[i, k] ← max(β[i, k], β[i, j] + β[j, k] + max(0, ∆[i, j, k])) 15: end for 16: end for 17: end for 18: return β[0, n] 19: end procedure Fi"
D09-1105,P06-1067,0,0.502212,"gnments using the “grow-diag-final-and” procedure provided with Moses (Koehn et al., 2007). For each of these German sentences, we derived the English-like reordering of it, which we call German0 , by the following procedure. Each German token was assigned an integer key, namely the position of the leftmost of the English tokens to which it was aligned, or 0 if it was not aligned to any English tokens. We then did a stable sort of the German tokens based on these keys, meaning that if two German tokens had the same key, their order was preserved. This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. They kept unaligned words with the closest preceding aligned word.4 Having found the German0 corresponding to each German sentence, we randomly divided the sentences into 2,000 each for development and evaluation, and the remaining approximately 747,000 for training. We used the averaged perceptron algorithm (Freund and Schapire, 1998; Collins, 2002) to train the parameters of the model. We ran the algorithm multiple times over the training sentences, 4 We tried two other methods for deriving English word order from word alignments. The first a"
D09-1105,N06-1004,0,0.0115136,"roposes several other ways of integrating our reordering model into machine translation. It also experiments with numerous other parameter estimation procedures, including some that use the probabilistic interpretation of our model from (3). It presents numerous additional neighborhoods for search in the Linear Ordering Problem. We mentioned several possible extensions to the model, such as going beyond the scoring model of equation (2), or considering syntax-based features. Another extension would try to reorder not words but phrases, following (Xiong et al., 2006), or segment choice models (Kuhn et al., 2006), which assume a single segmentation of the words into phrases. We would have to define the pairwise preference matrix B over phrases rather than 1014 words (Eisner and Tromble, 2006). This would have the disadvantage of complicating the feature space, but might be a better fit for integration with a phrase-based decoder. Finally, we gave a novel algorithm for approximately solving the Linear Ordering Problem, interestingly combining dynamic programming with local search. Another novel contribution is that we showed how to parameterize a function that constructs a specific Linear Ordering Prob"
D09-1105,H05-1021,0,0.0575098,"Missing"
D09-1105,P05-1066,0,0.911061,"Missing"
D09-1105,lavie-etal-2004-significance,0,0.0143204,"Missing"
D09-1105,W02-1001,0,0.00558855,"a stable sort of the German tokens based on these keys, meaning that if two German tokens had the same key, their order was preserved. This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. They kept unaligned words with the closest preceding aligned word.4 Having found the German0 corresponding to each German sentence, we randomly divided the sentences into 2,000 each for development and evaluation, and the remaining approximately 747,000 for training. We used the averaged perceptron algorithm (Freund and Schapire, 1998; Collins, 2002) to train the parameters of the model. We ran the algorithm multiple times over the training sentences, 4 We tried two other methods for deriving English word order from word alignments. The first alternative was to align only in one direction, from English to German, with null alignments disallowed, so that every German word was aligned to a single English word. The second alternative used BerkeleyAligner (Liang et al., 2006; DeNero and Klein, 2007), which shares information between the two alignment directions to improve alignment quality. Neither alternative produced improvements in our ult"
D09-1105,W06-1609,0,0.0515509,"Missing"
D09-1105,P07-1003,0,0.00986156,"or development and evaluation, and the remaining approximately 747,000 for training. We used the averaged perceptron algorithm (Freund and Schapire, 1998; Collins, 2002) to train the parameters of the model. We ran the algorithm multiple times over the training sentences, 4 We tried two other methods for deriving English word order from word alignments. The first alternative was to align only in one direction, from English to German, with null alignments disallowed, so that every German word was aligned to a single English word. The second alternative used BerkeleyAligner (Liang et al., 2006; DeNero and Klein, 2007), which shares information between the two alignment directions to improve alignment quality. Neither alternative produced improvements in our ultimate translation quality. measuring the quality of the learned parameters by reordering the held-out development set after each iteration. We stopped when the BLEU score on the development set failed to improve for two consecutive iterations, which occurred after fourteen passes over the data. Each perceptron update should compare the true German0 to the German0 that would be predicted by the model (2). As the latter is NP-hard to find, we instead s"
D09-1105,J03-1002,0,0.00486014,"rder Ideally, we would have a large corpus of desirable reorderings of source sentences—in our case, German sentences permuted into target English word order—from which to train the parameters of our model. Unfortunately, the alignments between German and English sentences are only infrequently one-to-one. Furthermore, human-aligned parallel sentences are hard to come by, and never in the quantity we would like. Instead, we make do with automaticallygenerated word alignments, and we heuristically derive an English-like word order for the German sentence based on the alignment. We used GIZA++ (Och and Ney, 2003) to align approximately 751,000 sentences from the German-English portion of the Europarl corpus (Koehn, 2005), in both the German-to-English and English-to-German directions. We combined the 1010 1: procedure L OCAL S EARCH S TEP(B, π, n) 2: for i ← 0 to n − 1 do 3: β[i, i + 1] ← 0 4: for k ← i + 1 to n do 5: ∆[i, i, k] ← ∆[i, k, k] ← 0 6: end for 7: end for 8: for w ← 2 to n do 9: for i ← 0 to n − w do 10: k ←i+w 11: β[i, k] ← −∞ 12: for j ← i + 1 to k − 1 do 13: ∆[i, j, k] ← ∆[i, j, k − 1] + ∆[i + 1, j, k] − ∆[i + 1, j, k − 1] + B[πk , πi+1 ] − B[πi+1 , πk ] 14: β[i, k] ← max(β[i, k], β[i,"
D09-1105,P03-1021,0,0.0787304,"2005). We used the model learned in Section 5 to generate a German0 ordering of the training, development, and test sets. The training sentences are the same that the model was trained on, and the development set is the same that was used as the stopping criterion for the perceptron. The test set was unused in training. We used the resulting German0 as the input to the Moses training pipeline. That is, Moses recomputed alignments of the German0 training data to the English sentences using GIZA++, then constructed a phrase table. Moses used the development data for minimum error-rate training (Och, 2003) of its small number of parameters. Finally, Moses translated the test sentences, and we measured performance against the English reference sentences. This is the standard Moses pipeline, except German has been replaced by German0 . Table 3 shows the results of translation, both starting with unreordered German, and starting with German0 , reordered using the learned Linear Ordering Problems. Note that Moses may itself re1012 System baseline (a) (b) (a)+(b) Input German German German0 German0 Moses Reord. p1 Distance 59.6 Lexical 60.0 Distance 60.4 Lexical 59.9 p2 31.4 32.0 32.7 32.4 p3 18.8 1"
D09-1105,P02-1040,0,0.100423,"Missing"
D09-1105,2006.amta-papers.25,0,0.061331,"Missing"
D09-1105,N04-4026,0,0.584118,"Missing"
D09-1105,J97-3002,0,0.624033,"ermutation in a certain exponentially large neighborhood N (π) of the current permutation π. 2 One can introduce randomness to obtain MCMC sampling or simulated annealing algorithms. Our algorithms extend naturally to allow this (cf. Tromble (2009)). S → S0,n Si,k → Si,j Sj,k Si−1,i → πi Figure 1: A grammar for a large neighborhood of permutations, given one permutation π of length n. The Si,k rules are instantiated for each 0 ≤ i < j < k ≤ n, and the Si−1,i rules for each 0 < i ≤ n. We say that two permutations are neighbors iff they can be aligned by an Inversion Transduction Grammar (ITG) (Wu, 1997), which is a familiar reordering device in machine translation. Equivalently, π 0 ∈ N (π) iff π can be transformed into π 0 by swapping various adjacent substrings of π, as long as these swaps are properly nested. Zens and Ney (2003) used a normal form to show that the size of the ITG neighborhood N (π) is a large Schr¨oder number, which grows exponentially in n. Asymptotically, the ratio between the size of the neighborhood √ for n + 1 and the size for n approaches 3 + 2 2 ≈ 5.8. We show that equation (2) can be optimized within N (π) in O(n3 ) time, using dynamic programming. The algorithm i"
D09-1105,C04-1073,0,0.68787,"Missing"
D09-1105,P06-1066,0,0.101075,") covers this integration in more detail, and proposes several other ways of integrating our reordering model into machine translation. It also experiments with numerous other parameter estimation procedures, including some that use the probabilistic interpretation of our model from (3). It presents numerous additional neighborhoods for search in the Linear Ordering Problem. We mentioned several possible extensions to the model, such as going beyond the scoring model of equation (2), or considering syntax-based features. Another extension would try to reorder not words but phrases, following (Xiong et al., 2006), or segment choice models (Kuhn et al., 2006), which assume a single segmentation of the words into phrases. We would have to define the pairwise preference matrix B over phrases rather than 1014 words (Eisner and Tromble, 2006). This would have the disadvantage of complicating the feature space, but might be a better fit for integration with a phrase-based decoder. Finally, we gave a novel algorithm for approximately solving the Linear Ordering Problem, interestingly combining dynamic programming with local search. Another novel contribution is that we showed how to parameterize a function t"
D09-1105,P03-1019,0,0.0755763,"this (cf. Tromble (2009)). S → S0,n Si,k → Si,j Sj,k Si−1,i → πi Figure 1: A grammar for a large neighborhood of permutations, given one permutation π of length n. The Si,k rules are instantiated for each 0 ≤ i < j < k ≤ n, and the Si−1,i rules for each 0 < i ≤ n. We say that two permutations are neighbors iff they can be aligned by an Inversion Transduction Grammar (ITG) (Wu, 1997), which is a familiar reordering device in machine translation. Equivalently, π 0 ∈ N (π) iff π can be transformed into π 0 by swapping various adjacent substrings of π, as long as these swaps are properly nested. Zens and Ney (2003) used a normal form to show that the size of the ITG neighborhood N (π) is a large Schr¨oder number, which grows exponentially in n. Asymptotically, the ratio between the size of the neighborhood √ for n + 1 and the size for n approaches 3 + 2 2 ≈ 5.8. We show that equation (2) can be optimized within N (π) in O(n3 ) time, using dynamic programming. The algorithm is based on CKY parsing. However, a novelty is that the grammar weights must themselves be computed by O(n3 ) dynamic programming. Our grammar is shown in Figure 1. Parsing the “input sentence” π with this grammar simply constructs al"
D09-1105,P95-1033,0,\N,Missing
D09-1105,N06-1014,0,\N,Missing
D11-1057,W02-0606,0,0.013905,"ically related words; he calls these sets paradigms but notes that they are not substructured entities, in contrast to the paradigms we model in this paper. His models are restricted to concatenative and regular morphology. 625 Morphology discovery approaches that handle nonconcatenative and irregular phenomena are more closely related to our work; they are rarer. Yarowsky and Wicentowski (2000) identify inflection-root pairs in large corpora without supervision. Using similarity as well as distributional clues, they identify even irregular pairs like take/took. Schone and Jurafsky (2001) and Baroni et al. (2002) extract whole conflation sets, like “abuse, abused, abuses, abusive, abusively, . . . ,” which may also be irregular. We advance this work by not only extracting pairs or sets of related observed words, but whole structured inflectional paradigms, in which we can also predict forms that have never been observed. On the other hand, our present model does not yet use contextual information; we regard this as a future opportunity (see Appendix G). Naradowsky and Goldwater (2009) add simple spelling rules to the Bayesian model of (Goldwater et al., 2006a), enabling it to handle some systematicall"
D11-1057,W06-3209,0,0.673633,"forms that “should” be common. To fix this, we shall incorporate an unlabeled or POS-tagged corpus (token data) into learning. We therefore need a model for generating tokens— a probabilistic lexicon that specifies which inflections of which lexemes are common, and how they are spelled. We do not know our language’s probabilistic lexicon, but we assume it was generated as follows: 1. Choose parameters θ~ of the MRF. This defines p(π): which paradigms are likely a priori. 2. Choose a distribution over the abstract lexemes. 5 This view is adopted by some morphological theorists (Albright, 2002; Chan, 2006), although see Appendix E.2 for a caution about syncretism. Note that when the lemma is unobserved, the other forms do still influence one another indirectly. 618 Details are given later. Briefly, step 1 samples θ~ from a Gaussian prior. Step 2 samples a distribution from a Dirichlet process. This chooses a countable number of lexemes to have positive probability in the language, and decides which ones are most common. Step 3 samples a distribution from a Dirichlet. For the lexeme think, this might choose to make 1stperson singular more common than for typical verbs. Step 4 just samples I"
D11-1057,W98-1239,0,0.0236268,"d-and-paradigm model seamlessly handles nonconcatenative and concatenative morphology alike, whereas most previous work in morphological knowledge discovery has modeled concatenative morphology only, assuming that the orthographic form of a word can be split neatly into stem and affixes—a simplifying asssumption that is convenient but often not entirely appropriate (Kay, 1987) (how should one segment English stopping, hoping, or knives?). In concatenative work, Harris (1955) finds morpheme boundaries and segments words accordingly, an approach that was later refined by Hafer and Weiss (1974), Déjean (1998), and many others. The unsupervised segmentation task is tackled in the annual Morpho Challenge (Kurimo et al., 2010), where ParaMor (Monson et al., 2007) and Morfessor (Creutz and Lagus, 2005) are influential contenders. The Bayesian methods that Goldwater et al. (2006b, et seq.) use to segment between words might also be applied to segment within words, but have no notion of paradigms. Goldsmith (2001) finds what he calls signatures—sets of affixes that are used with a given set of stems, for example (NULL, -er, -ing, -s). Chan (2006) learns sets of morphologically related words; he calls th"
D11-1057,D09-1011,1,0.918503,"finite-state transducers. 2.2 Modeling Morphological Paradigms A paradigm such as Table 1 describes how some abstract lexeme (b&r ak) is expressed in German.4 We evaluate whole paradigms as linguistic objects, following word-and-paradigm or realizational morphology (Matthews, 1972; Stump, 2001). That is, we presume that some language-specific distribution p(π) defines whether a paradigm π is a grammatical—and a priori likely—way for a lexeme to express itself in the language. Learning p(π) helps us reconstruct paradigms, as described at the end of section 1.2. Let π = (x1 , x2 , . . .). In Dreyer and Eisner (2009), we showed how to model p(π) as a renormalized product of many pairwise distributions prs (xr , xs ), each having the log-linear form of section 2.1: Y X −→ ~ frs (xr , xs , ars )) p(π) ∝ prs (xr , xs ) ∝ exp( θ· r,s We begin by sketching the main ideas of our model, first reviewing components that we introduced in earlier papers. Sections 5–7 will give more formal details. Full details and more discussion can be found in the first author’s dissertation (Dreyer, 2011). 2.1 Modeling Morphological Alternations We begin with a family of joint distributions p(x, y) ~ For example, over string pair"
D11-1057,D08-1113,1,0.800579,"eneral patterns in the language. We learn a prior distribution over inflectional paradigms by learning (e.g.) how a verb’s suffix or stem vowel tends to change when it is pluralized. We also learn (e.g.) whether singular or plural forms are more common. Our basic strategy is Monte Carlo EM, so these parameters tell us how to guess the paradigms (Monte Carlo E step), then these reconstructed paradigms tell us how to reestimate the parameters (M step), and so on iteratively. We use a few supervised paradigms to initialize the parameters and help reestimate them. 2 y via p(y |x), and vice-versa. Dreyer et al. (2008) define such a family via a log-linear model with latent alignments, X X p(x, y) = p(x, y, a) ∝ exp(θ~ · f~(x, y, a)) Overview of the Model Here a ranges over monotonic 1-to-1 character alignments between x and y. ∝ means “proportional to” (p is normalized to sum to 1). f~ extracts a vector of local features from the aligned pair by examining trigram windows. Thus θ~ can reward or penalize specific features—e.g., insertions, deletions, or substitutions in specific contexts, as well as trigram features of x and y separately.3 Inference and training are done by dynamic programming on finite-stat"
D11-1057,J01-2001,0,0.653345,"English stopping, hoping, or knives?). In concatenative work, Harris (1955) finds morpheme boundaries and segments words accordingly, an approach that was later refined by Hafer and Weiss (1974), Déjean (1998), and many others. The unsupervised segmentation task is tackled in the annual Morpho Challenge (Kurimo et al., 2010), where ParaMor (Monson et al., 2007) and Morfessor (Creutz and Lagus, 2005) are influential contenders. The Bayesian methods that Goldwater et al. (2006b, et seq.) use to segment between words might also be applied to segment within words, but have no notion of paradigms. Goldsmith (2001) finds what he calls signatures—sets of affixes that are used with a given set of stems, for example (NULL, -er, -ing, -s). Chan (2006) learns sets of morphologically related words; he calls these sets paradigms but notes that they are not substructured entities, in contrast to the paradigms we model in this paper. His models are restricted to concatenative and regular morphology. 625 Morphology discovery approaches that handle nonconcatenative and irregular phenomena are more closely related to our work; they are rarer. Yarowsky and Wicentowski (2000) identify inflection-root pairs in large c"
D11-1057,P06-1085,0,0.0288877,"atly into stem and affixes—a simplifying asssumption that is convenient but often not entirely appropriate (Kay, 1987) (how should one segment English stopping, hoping, or knives?). In concatenative work, Harris (1955) finds morpheme boundaries and segments words accordingly, an approach that was later refined by Hafer and Weiss (1974), Déjean (1998), and many others. The unsupervised segmentation task is tackled in the annual Morpho Challenge (Kurimo et al., 2010), where ParaMor (Monson et al., 2007) and Morfessor (Creutz and Lagus, 2005) are influential contenders. The Bayesian methods that Goldwater et al. (2006b, et seq.) use to segment between words might also be applied to segment within words, but have no notion of paradigms. Goldsmith (2001) finds what he calls signatures—sets of affixes that are used with a given set of stems, for example (NULL, -er, -ing, -s). Chan (2006) learns sets of morphologically related words; he calls these sets paradigms but notes that they are not substructured entities, in contrast to the paradigms we model in this paper. His models are restricted to concatenative and regular morphology. 625 Morphology discovery approaches that handle nonconcatenative and irregular"
D11-1057,J00-4006,0,0.0107619,"Missing"
D11-1057,E87-1002,0,0.577526,"paradigms, the added corpus plays an important role in correcting its mistakes, especially for the more frequent, irregular verb forms. For examples of specific errors that the models make, see Appendix E.3. 9 Related Work Our word-and-paradigm model seamlessly handles nonconcatenative and concatenative morphology alike, whereas most previous work in morphological knowledge discovery has modeled concatenative morphology only, assuming that the orthographic form of a word can be split neatly into stem and affixes—a simplifying asssumption that is convenient but often not entirely appropriate (Kay, 1987) (how should one segment English stopping, hoping, or knives?). In concatenative work, Harris (1955) finds morpheme boundaries and segments words accordingly, an approach that was later refined by Hafer and Weiss (1974), Déjean (1998), and many others. The unsupervised segmentation task is tackled in the annual Morpho Challenge (Kurimo et al., 2010), where ParaMor (Monson et al., 2007) and Morfessor (Creutz and Lagus, 2005) are influential contenders. The Bayesian methods that Goldwater et al. (2006b, et seq.) use to segment between words might also be applied to segment within words, but have"
D11-1057,W10-2211,0,0.0550102,"Missing"
D11-1057,W07-1315,0,0.0141097,"discovery has modeled concatenative morphology only, assuming that the orthographic form of a word can be split neatly into stem and affixes—a simplifying asssumption that is convenient but often not entirely appropriate (Kay, 1987) (how should one segment English stopping, hoping, or knives?). In concatenative work, Harris (1955) finds morpheme boundaries and segments words accordingly, an approach that was later refined by Hafer and Weiss (1974), Déjean (1998), and many others. The unsupervised segmentation task is tackled in the annual Morpho Challenge (Kurimo et al., 2010), where ParaMor (Monson et al., 2007) and Morfessor (Creutz and Lagus, 2005) are influential contenders. The Bayesian methods that Goldwater et al. (2006b, et seq.) use to segment between words might also be applied to segment within words, but have no notion of paradigms. Goldsmith (2001) finds what he calls signatures—sets of affixes that are used with a given set of stems, for example (NULL, -er, -ing, -s). Chan (2006) learns sets of morphologically related words; he calls these sets paradigms but notes that they are not substructured entities, in contrast to the paradigms we model in this paper. His models are restricted to c"
D11-1057,N01-1024,0,0.0442829,"(2006) learns sets of morphologically related words; he calls these sets paradigms but notes that they are not substructured entities, in contrast to the paradigms we model in this paper. His models are restricted to concatenative and regular morphology. 625 Morphology discovery approaches that handle nonconcatenative and irregular phenomena are more closely related to our work; they are rarer. Yarowsky and Wicentowski (2000) identify inflection-root pairs in large corpora without supervision. Using similarity as well as distributional clues, they identify even irregular pairs like take/took. Schone and Jurafsky (2001) and Baroni et al. (2002) extract whole conflation sets, like “abuse, abused, abuses, abusive, abusively, . . . ,” which may also be irregular. We advance this work by not only extracting pairs or sets of related observed words, but whole structured inflectional paradigms, in which we can also predict forms that have never been observed. On the other hand, our present model does not yet use contextual information; we regard this as a future opportunity (see Appendix G). Naradowsky and Goldwater (2009) add simple spelling rules to the Bayesian model of (Goldwater et al., 2006a), enabling it to"
D11-1057,H05-1060,0,0.0213568,"Missing"
D11-1057,P06-1124,0,0.10664,"Missing"
D11-1057,P02-1019,0,0.00974395,"only in the factors from headings 6.3 and 6.6, we can just integrate it out of their product, to get a collapsed sub-model that generates p(~` |~t, α ~ ) directly: ! n ! Z Z Y Y · · · dG p(Gt |αt ) p(`i |Gti ) GADJ GVERB = p(~` |~t, α ~) = 6.9 Collapsing the Assignment Again, a full assignment’s probability is the product of all the above factors (see drawing in Appendix B). 11 To account for typographical errors in the corpus, the spellout process could easily be made nondeterministic, with the observed word wi derived from the correct spelling πti ,`i (si ) by a noisy channel model (e.g., (Toutanova and Moore, 2002)) represented as a WFST. This would make it possible to analyze brkoen as a misspelling of a common or contextually likely word, rather than treating it as an unpronounceable, irregularly inflected neologism, which is presumably less likely. 622 n Y i=1 p(`i |`1 , . . . `i−1 ~t, α ~) where it turns out that the factor that generates `i is proportional to |{j < i : `j = `i and tj = ti } |if that integer is positive, else proportional to αti G(`i ). Metaphorically, each tag t is a Chinese restaurant whose tables are labeled with lexemes. The tokens are hungry customers. Each customer i = 1, 2, ."
D11-1057,P00-1027,0,0.865449,"o segment within words, but have no notion of paradigms. Goldsmith (2001) finds what he calls signatures—sets of affixes that are used with a given set of stems, for example (NULL, -er, -ing, -s). Chan (2006) learns sets of morphologically related words; he calls these sets paradigms but notes that they are not substructured entities, in contrast to the paradigms we model in this paper. His models are restricted to concatenative and regular morphology. 625 Morphology discovery approaches that handle nonconcatenative and irregular phenomena are more closely related to our work; they are rarer. Yarowsky and Wicentowski (2000) identify inflection-root pairs in large corpora without supervision. Using similarity as well as distributional clues, they identify even irregular pairs like take/took. Schone and Jurafsky (2001) and Baroni et al. (2002) extract whole conflation sets, like “abuse, abused, abuses, abusive, abusively, . . . ,” which may also be irregular. We advance this work by not only extracting pairs or sets of related observed words, but whole structured inflectional paradigms, in which we can also predict forms that have never been observed. On the other hand, our present model does not yet use contextua"
D11-1085,P08-1024,0,0.0401701,"ur approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which"
D11-1085,N09-1025,0,0.0625912,"T systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ (x) need not be probabilistic. For example, θ may be the parameters of a sco"
D11-1085,J07-2003,0,0.581901,"ness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ (y|x) by marginalizing out d, X pθ (y |x) = pθ (d |x) (11) d∈D(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly the expectation maximization (EM) algorithm, a widely used generative approach. So it is ins"
D11-1085,W02-1001,0,0.20038,"N 1 2 ing − M j=1 log pφ (xj |yj ) + 2σ 2 kφk2 on some bilingual data, with the regularization coefficient σ 2 tuned on held out data. It may be tolerable for pφ to impute mediocre translations xij . All that is necessary is that the (forward) translations generated from the imputed xij “simulate” the competing hypotheses that we would see when translating the correct Chinese input xi . 3.3 The Forward Translation System δθ and The Loss Function L(δθ (xi ), yi ) The minimum empirical risk objective of (2) is quite general and various popular supervised training methods (Lafferty et al., 2001; Collins, 2002; Och, 2003; Crammer et al., 2006; Smith and Eisner, 8 In a translation task from x to y, one usually does not make use of in-domain monolingual data x. But we can exploit x to train a language model pφ (x) for the reverse translation system, which will make the imputed xij look like true Chinese inputs. 2006) can be formalized in this framework by choosing different functions for δθ and L(δθ (xi ), yi ). The generality of (2) extends to our minimum imputed risk objective of (4). Below, we specify the δθ and L(δθ (xi ), yi ) we considered in our investigation. 3.3.1 Deterministic Decoding A si"
D11-1085,P08-1115,0,0.0269094,"for the reverse and forward translations. In other words, this does round-trip imputation (i.e., from y to x, and then to y 0 ) at the rule level. This is essentially the approach taken by Li et al. (2010). 3.5 The Log-Linear Model pθ We have not yet specified the form of pθ . Following much work in MT, we begin with a linear model X score(x, y) = θ · f (x, y) = θk fk (x, y) (9) k where f (x, y) is a feature vector indexed by k. Our deterministic test-time translation system δθ simply 12 Note that the forward translation of a WFSA is tractable by using a lattice-based decoder such as that by Dyer et al. (2008). 924 outputs the highest-scoring y for fixed x. At training time, our randomized decoder (Section 3.3.2) uses the Boltzmann distribution (here a log-linear model) pθ (y |x) = eγ·score(x,y) eγ·score(x,y) = P γ·score(x,y0 ) (10) Z(x) y0 e The scaling factor γ controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often n"
D11-1085,2005.iwslt-1.1,0,0.0473602,"|y) and pθ (y |x). By sticking to conditional models, we can efficiently use more sophisticated model features, and we can incorporate the loss function when we train θ, which should improve both efficiency and accuracy at test time. 5 Experimental Results We report results on Chinese-to-English translation tasks using Joshua (Li et al., 2009a), an open-source implementation of Hiero (Chiang, 2007). 5.1 5.1.1 Baseline Systems IWSLT Task We train both reverse and forward baseline systems. The translation models are built using the corpus for the IWSLT 2005 Chinese to English translation task (Eck and Hori, 2005), which comprises 40,000 pairs of transcribed utterances in the travel domain. We use a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on the English (resp. Chinese) side of the bitext. We use a standard training pipeline and pruning settings recommended by (Chiang, 2007). 5.1.2 NIST Task For the NIST task, the TM is trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method implemented in Joshua. We also used a 5-gram languag"
D11-1085,P06-1121,0,0.0422989,"γ controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ (y|x) by marginalizing out d, X pθ (y |x) = pθ (d |x) (11) d∈D(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly the expectation maximization (EM) algorithm, a widely used generative approac"
D11-1085,W05-1506,0,0.0238305,"isner, 2009) to efficiently compute the gradients needed for optimizing (8). 3.4 Approximating pφ (x |yi ) As mentioned at the end of Section 3.1, it is computationally infeasible to forward-translate each of the imputed reverse translations xij . We propose four approximations that are computationally feasible. Each may be regarded as a different approximation of pφ (x |yi ) in equations (4) or (8). k-best. For each yi , add to the imputed training set only the k most probable translations {xi1 , . . . xik } according to pφ (x |yi ). (These can be extracted from Xi using standard algorithms (Huang and Chiang, 2005).) Rescale their probabilities to sum to 1. Sampling. For each yi , add to the training set k independent samples {xi1 , . . . xik } from the distribution pφ (x |yi ), each with weight 1/k. (These can be sampled from Xi using standard algorithms (Johnson et al., 2007).) This method is known in the literature as multiple imputation (Rubin, 1987). Lattice. 11 Under certain special cases it is be possible to compute the expected loss in (3) exactly via dynamic programming. Although Xi does contain exponentially many translations, it may use a “packed” representation in which these translations sh"
D11-1085,N07-1018,0,0.0210271,"imations that are computationally feasible. Each may be regarded as a different approximation of pφ (x |yi ) in equations (4) or (8). k-best. For each yi , add to the imputed training set only the k most probable translations {xi1 , . . . xik } according to pφ (x |yi ). (These can be extracted from Xi using standard algorithms (Huang and Chiang, 2005).) Rescale their probabilities to sum to 1. Sampling. For each yi , add to the training set k independent samples {xi1 , . . . xik } from the distribution pφ (x |yi ), each with weight 1/k. (These can be sampled from Xi using standard algorithms (Johnson et al., 2007).) This method is known in the literature as multiple imputation (Rubin, 1987). Lattice. 11 Under certain special cases it is be possible to compute the expected loss in (3) exactly via dynamic programming. Although Xi does contain exponentially many translations, it may use a “packed” representation in which these translations share structure. This representation may furthermore enable sharing work in forward-translation, so as to efficiently translate the entire set Xi and obtain a distribution over translations y. Finally, the expected loss under that distribution, as required by equation ("
D11-1085,N03-1017,0,0.00578651,"ore(x,y) eγ·score(x,y) = P γ·score(x,y0 ) (10) Z(x) y0 e The scaling factor γ controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ (y|x) by marginalizing out d, X pθ (y |x) = pθ (d |x) (11) d∈D(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly th"
D11-1085,D09-1005,1,0.926783,"criminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ (x) need not be proba"
D11-1085,W09-0424,1,0.90745,"Missing"
D11-1085,P09-1067,1,0.797285,"wed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ (x) need not be probabilistic. For example, θ may be the parameters of a scoring function used by δ, along with pruning and decoding heuristics, for extracti"
D11-1085,C10-2075,1,0.85854,"ed by the reverse Hiero system; each translation phrase (or rule) corresponding to a hyperedge. To exploit structure-sharing, we can use a forward translation system that decomposes according to that existing parse of xi . We can do that by considering only forward translations that respect the hypergraph structure of Xi . The simplest way to do this is to require complete isomorphism of the SCFG trees used for the reverse and forward translations. In other words, this does round-trip imputation (i.e., from y to x, and then to y 0 ) at the rule level. This is essentially the approach taken by Li et al. (2010). 3.5 The Log-Linear Model pθ We have not yet specified the form of pθ . Following much work in MT, we begin with a linear model X score(x, y) = θ · f (x, y) = θk fk (x, y) (9) k where f (x, y) is a feature vector indexed by k. Our deterministic test-time translation system δθ simply 12 Note that the forward translation of a WFSA is tractable by using a lattice-based decoder such as that by Dyer et al. (2008). 924 outputs the highest-scoring y for fixed x. At training time, our randomized decoder (Section 3.3.2) uses the Boltzmann distribution (here a log-linear model) pθ (y |x) = eγ·score(x,y"
D11-1085,P06-1096,0,0.256824,"Missing"
D11-1085,D08-1076,0,0.0341042,"pθ (y), as is the norm. Following the theoretical development below, the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. On"
D11-1085,N06-1045,0,0.0232079,"he forward translations. Still, even with the worse imputation (in the case of “NLM”), our forward translations improve as we add more monolingual data. 5.5.2 Imputation with Different k-best Sizes In all the experiments so far, we used the reverse translation system to impute only a single Chinese translation for each English monolingual sentence. This is the 1-best approximation of section 3.4. Table 5 shows (in the fully unsupervised case) that the performance does not change much as k increases.16 This may be because that the 5-best sentences are likely to be quite similar to one another (May and Knight, 2006). Imputing a longer k-best list, a sample, or a lattice for xi (see section 3.4) might achieve more diversity in the training inputs, which might make the system more robust. 6 Conclusions In this paper, we present an unsupervised discriminative training method that works with missing inputs. The key idea in our method is to use a reverse model to impute the missing input from the observed output. The training will then forward translate the imputed input, and choose the parameters of the forward model such that the imputed risk (i.e., 16 In the present experiments, however, we simply weighted"
D11-1085,P03-1021,0,0.401482,"guage model pθ (y), as is the norm. Following the theoretical development below, the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003)"
D11-1085,2001.mtsummit-papers.68,0,0.0244807,"English y = δθ (x) need not be probabilistic. For example, θ may be the parameters of a scoring function used by δ, along with pruning and decoding heuristics, for extracting a high-scoring translation of x. The goal of discriminative training is to minimize the expected loss of δθ (·), under a given taskspecific loss function L(y 0 , y) that measures how 2 Note that the extra monolingual data is used only for tuning the model weights, but not for inducing new phrases or rules. 921 bad it would be to output y 0 when the correct output is y. For an MT system that is judged by the BLEU metric (Papineni et al., 2001), for instance, L(y 0 , y) may be the negated BLEU score of y 0 w.r.t. y. To be precise, the goal3 is to find θ with low Bayes risk, θ∗ = argmin θ X p(x, y) L(δθ (x), y) (1) x,y where p(x, y) is the joint distribution of the inputoutput pairs.4 The true p(x, y) is, of course, not known and, in practice, one typically minimizes empirical risk by replacing p(x, y) above with the empirical distribution p˜(x, y) given by a supervised training set {(xi , yi ), i = 1, . . . , N }. Therefore, θ∗ = argmin θ = argmin θ X p˜(x, y) L(δθ (x), y) x,y N 1 X L(δθ (xi ), yi ). N (2) i=1 The search for θ∗ typi"
D11-1085,P06-2101,1,0.955514,"ing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ"
D11-1085,D08-1065,0,0.0319576,"a theoretical contribution, and we do not empirically evaluate it since its implementation requires extensive engineering effort that is beyond the main scope of this paper. ambiguous weighted finite-state automaton Xi , (b) the forward translation system δθ is structured in a certain way as a weighted synchronous context-free grammar, and (c) the loss function decomposes in a certain way. We omit the details of the construction as beyond the scope of this paper. In our experimental setting described below, (b) is true (using Joshua), and (c) is true (since we use a loss function presented by Tromble et al. (2008) that is an approximation to BLEU and is decomposable). While (a) is not true in our setting because Xi is a hypergraph (which is ambiguous), Li et al. (2009b) show how to approximate a hypergraph representation of pφ (x |yi ) by an unambiguous WFSA. One could then apply the construction to this WFSA12 , obtaining an approximation to (3). Rule-level Composition. Intuitively, the reason why the structure-sharing in the hypergraph Xi (generated by the reverse system) cannot be exploited during forward translating is that when the forward Hiero system translates a string xi ∈ Xi , it must parse i"
D11-1085,D07-1080,0,0.0393648,"development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ (x). The function δθ , which translates Chinese x to English y = δθ (x) need not be probabilistic. For example, θ may be t"
D11-1085,P02-1040,0,\N,Missing
D12-1032,N03-1003,0,0.199016,"st name into the final phylogeny, thereby locating variants of the test name. We find that our method can effectively find name variants in a corpus of web strings used to refer to persons in Wikipedia, improving over standard untrained distances such as Jaro-Winkler and Levenshtein distance. 1 Introduction Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al., 2007), and paraphrasing (Barzilay and Lee, 2003). Stochastic transducers such as probabilistic finite-state transducers are often used to capture such relationships. They model a conditional distribution p(y |x), and are ordinarily trained on input-output pairs of strings (Dreyer et al., 2008). In this paper, we are interested in learning from an unorganized collection of strings, some of which might have been derived from others by transformative linguistic processes such as abbreviation, morphological derivation, historical sound or spelling change, loanword formation, translation, transliteration, editing, or transcription error. We assu"
D12-1032,D07-1074,0,0.0461471,"iscourse” in which our authors operate.10 In this case, we would need (expensive) new algorithms to reconstruct the strings w. However, this model could infer a more realistic phylogeny by positing unobserved ancestral or intermediate forms that relate the observed tokens, as in transformation models (Eisner, 2002; Andrews and Eisner, 2011). 7 7.1 Experimental Evaluation Data preparation Scraping Wikipedia. Wikipedia documents many variant names for entities. As a result, it has frequently been used as a source for mining name variations, both within and across languages (Parton et al., 2008; Cucerzan, 2007). We used Wikipedia to create a list of name aliases for different entities. Specifically, we mined English Wikipedia11 for all redirects: page names that lead directly to another page. Redirects are created by Wikipedia users for resolving common name variants to the correct page. For example, the pages titled Barack Obama Junior and Barack Hussein Obama automatically redirect to the page titled Barack Obama. This redirection implies that the first two are name variants of the third. Collecting all such links within English Wikipedia yields a large number of aliases for each page. However, ma"
D12-1032,D11-1057,1,0.91438,"ional EM learning algorithm alternately reestimates this phylogeny and the transducer parameters. The final learned transducer can quickly link any test name into the final phylogeny, thereby locating variants of the test name. We find that our method can effectively find name variants in a corpus of web strings used to refer to persons in Wikipedia, improving over standard untrained distances such as Jaro-Winkler and Levenshtein distance. 1 Introduction Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al., 2007), and paraphrasing (Barzilay and Lee, 2003). Stochastic transducers such as probabilistic finite-state transducers are often used to capture such relationships. They model a conditional distribution p(y |x), and are ordinarily trained on input-output pairs of strings (Dreyer et al., 2008). In this paper, we are interested in learning from an unorganized collection of strings, some of which might have been derived from others by transformative linguistic processes such as abbreviation, morp"
D12-1032,D08-1113,1,0.922811,"ch as Jaro-Winkler and Levenshtein distance. 1 Introduction Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al., 2007), and paraphrasing (Barzilay and Lee, 2003). Stochastic transducers such as probabilistic finite-state transducers are often used to capture such relationships. They model a conditional distribution p(y |x), and are ordinarily trained on input-output pairs of strings (Dreyer et al., 2008). In this paper, we are interested in learning from an unorganized collection of strings, some of which might have been derived from others by transformative linguistic processes such as abbreviation, morphological derivation, historical sound or spelling change, loanword formation, translation, transliteration, editing, or transcription error. We assume that each string was derived from at most one parent, but may give rise to any number of children. The difficulty is that most or all of these parentchild relationships are unobserved. We must reconstruct this evolutionary phylogeny. At the sa"
D12-1032,W11-2202,0,0.1157,"Missing"
D12-1032,W02-1009,1,0.865922,"ion over sequences of entities e is exchangeable. However, our distribution over sequences of named entities y = (e, w) is non-exchangeable. It assigns different probabilities to different orderings of the same tokens. This is because our model posits that later authors are influenced by earlier authors, copying entity names from them with mutation. So ordering is important. The mutation process is not symmetric—for example, Figure 1 reflects a tendency to shorten rather than lengthen names. Non-exchangeability is one way that our present model differs from (parametric) transformation models (Eisner, 2002) and (non-parametric) transformation processes (Andrews and Eisner, 2011). These too are defined using mutation of strings or other types. From a transformation process, one can draw a distribution over types, from which the tokens are then sampled IID. This results in an exchangeable sequence of tokens, just as in the Dirichlet process. We avoid transformation models here for three reasons. (1) Inference is more expensive. (2) A transformation process seems less realistic as a model of authorship. It constructs a distribution over derivational paths, similar to the paths in Figure 1. It effec"
D12-1032,P08-1088,0,0.0285121,"der. Given learned parameters, we can ask the model whether a name Dr. J. J. Jingelheimer in the collection is more likely to have been generated from scratch, or derived from some previous name. 1.1 Related Work Several previous papers have also considered learning transducers or other models of word pairs when the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreyer and Eisner (2011) org"
D12-1032,P06-1103,0,0.0219438,"ollection, in an unknown order. Given learned parameters, we can ask the model whether a name Dr. J. J. Jingelheimer in the collection is more likely to have been generated from scratch, or derived from some previous name. 1.1 Related Work Several previous papers have also considered learning transducers or other models of word pairs when the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreye"
D12-1032,D07-1015,0,0.0523126,"edge from vertex x to vertex y, according to (10), is X cxy = pθ (T ) (11) T ∈T♦ (G):(x→y)∈T The probability cxy is a “pseudocount” for the expected number of mutations from x to y. This is at most 1 under our assumptions. Calculating cxy requires summing over all spanning trees of G, of which there are nn−2 for a fully connected graph with n vertices. Fortunately, Tutte (1984) shows how to compute this sum by the following method, which extends Kirchhoff’s classical matrix-tree theorem to weighted directed graphs. This result has previously been employed in nonprojective dependency parsing (Koo et al., 2007; Smith and Smith, 2007). Let L ∈ Rn×n denote the Laplacian of G, namely  P 0 x0 δ(y |x ) if x = y L= (12) −δ(y |x) if x 6= y Tutte’s theorem relates the determinant of the Laplacian to the spanning trees in graph G. In particular, the cofactor L0,0 equals the total weight of all directed spanning trees rooted at node 0. This yields the partition function Z(G) (assuming node 0 is ♦). ˆ be the matrix L with the 0th row and 0th Let L column removed. Then the edge marginals of interest are related to the log partition function by cxy = ˆ ∂ log Z(G) ∂ log |L| = ∂ δ(y |x) ∂ δ(y |x) which has the c"
D12-1032,P11-1044,0,0.0237375,"e model whether a name Dr. J. J. Jingelheimer in the collection is more likely to have been generated from scratch, or derived from some previous name. 1.1 Related Work Several previous papers have also considered learning transducers or other models of word pairs when the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreyer and Eisner (2011) organize words into morphological paradigms of"
D12-1032,W02-2026,0,0.0117651,"is could include learning common nicknames; explicitly modeling abbreviation processes such as initials; conditioning on name components such as title and middle name; and transliterating across languages.14 In other domains, one could model bibliographic entry propagation, derivational morphology, or historical sound change (again using language tags). Another future direction would be to incorporate the context of tokens in order to help reconstruct which tokens are coreferent. Combining contextual similarity with string similarity has previously proved very useful for identifying cognates (Schafer and Yarowsky, 2002; Schafer, 2006b; Bergsma and Van Durme, 2011). In our setting it would help to distinguish people with identical names, as well as determining whether two people with similar names are really the same. 14 8 Conclusions and Future Work We have presented a new unsupervised method for learning string-to-string transducers. It learns from a collection of related strings whose relationships are unknown. The key idea is that some strings are mutations of common strings that occurred earlier. We compute a distribution over the unknown phylogeThese last two points suggest that the mutation model shou"
D12-1032,2006.amta-papers.23,0,0.316217,"e names in the collection, in an unknown order. Given learned parameters, we can ask the model whether a name Dr. J. J. Jingelheimer in the collection is more likely to have been generated from scratch, or derived from some previous name. 1.1 Related Work Several previous papers have also considered learning transducers or other models of word pairs when the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ances"
D12-1032,D07-1014,0,0.0537622,"x to vertex y, according to (10), is X cxy = pθ (T ) (11) T ∈T♦ (G):(x→y)∈T The probability cxy is a “pseudocount” for the expected number of mutations from x to y. This is at most 1 under our assumptions. Calculating cxy requires summing over all spanning trees of G, of which there are nn−2 for a fully connected graph with n vertices. Fortunately, Tutte (1984) shows how to compute this sum by the following method, which extends Kirchhoff’s classical matrix-tree theorem to weighted directed graphs. This result has previously been employed in nonprojective dependency parsing (Koo et al., 2007; Smith and Smith, 2007). Let L ∈ Rn×n denote the Laplacian of G, namely  P 0 x0 δ(y |x ) if x = y L= (12) −δ(y |x) if x 6= y Tutte’s theorem relates the determinant of the Laplacian to the spanning trees in graph G. In particular, the cofactor L0,0 equals the total weight of all directed spanning trees rooted at node 0. This yields the partition function Z(G) (assuming node 0 is ♦). ˆ be the matrix L with the 0th row and 0th Let L column removed. Then the edge marginals of interest are related to the log partition function by cxy = ˆ ∂ log Z(G) ∂ log |L| = ∂ δ(y |x) ∂ δ(y |x) which has the closed-form solution ( ˆ"
D12-1032,P10-1107,0,0.0219979,"meters, we can ask the model whether a name Dr. J. J. Jingelheimer in the collection is more likely to have been generated from scratch, or derived from some previous name. 1.1 Related Work Several previous papers have also considered learning transducers or other models of word pairs when the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreyer and Eisner (2011) organize words into morp"
D12-1032,P10-1105,0,\N,Missing
D12-1032,J98-4003,0,\N,Missing
D13-1152,C10-1007,0,0.0558249,"simple heuristics such as a global relative pruning threshold, we learn a featurized decisionmaking policy of a more complex form. Since each decision can affect later stages, or later decisions in the same stage, we model this problem as a sequential decision-making process and solve it by Dataset Aggregation (DAgger) (Ross et al., 2011), a recent iterative imitation learning technique for structured prediction. Previous work has made much progress on the complementary problem: speeding up the decoding stage by pruning the search space of tree structures. In Roark and Hollingshead (2008) and Bergsma and Cherry (2010), pruning decisions are made locally as a preprocessing step. In the recent vine pruning approach (Rush and Petrov, 2012), significant speedup is gained by leveraging structured information via a coarse-to-fine projective parsing cas1455 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics cade (Charniak et al., 2006). These approaches do not directly tackle the feature selection problem. Although pruned edges do not require further feature computat"
D13-1152,W06-2920,0,0.0529003,"how many locked children it currently has. It turns out that given all the parsing features, the margin is the most discriminative meta-feature. When it is present, other metafeatures we added do not help much, Thus we do not include them in our experiments due to overhead. 6 6.1 Experiment Setup We generate dependency structures from the PTB constituency trees using the head rules of Yamada and Matsumoto (2003). Following convention, we use sections 02–21 for training, section 22 for development and section 23 for testing. We also report results on six languages from the CoNLL-X shared task (Buchholz and Marsi, 2006) as suggested in (Rush and Petrov, 2012), which cover a variety of language families. We follow the standard training/test split specified in the CoNLL-X data and tune parameters by cross validation when training the classifiers (policies). The PTB test data is tagged by a Stanford part-of-speech (POS) tagger (Toutanova et al., 2003) trained on sections 02–21. We use the provided gold POS tags for the CoNLL test data. All results are evaluated by the unlabeled attachment score (UAS). For fair comparison with previous work, punctuation is included when computing parsing accuracy of all CoNLL-X"
D13-1152,D07-1101,0,0.0472707,"nd one 1456 of its modifiers. Finding the best tree requires first computing θ·φ(E) for each of the n2 possible edges. Since scoring the edges independently in this way restricts the parser to a local view of the dependency structure, higher-order models can achieve better accuracy. For example, in the second-order model of McDonald and Pereira (2006), each local subgraph E is a triple that includes the head and two modifiers of the head, which are adjacent to each other. Other methods that use triples include grandparent-parent-child triples (Koo and Collins, 2010), or non-adjacent siblings (Carreras, 2007). Third-order models (Koo and Collins, 2010) use quadruples, employing grand-sibling and tri-sibling information. The usual inference problem is to find the highest scoring tree for the input sentence. Note that in a valid tree, each token 1, . . . , n must be attached to exactly one parent (either another token or the root 0). We can further require the tree to be projective, meaning that edges are not allowed to cross each other. It is well known that dynamic programming can be used to find the best projective dependency tree in O(n3 ) time, much as in CKY, for firstorder models and some hig"
D13-1152,N06-1022,0,0.10832,"Missing"
D13-1152,C96-1058,1,0.389269,"ls (Koo and Collins, 2010) use quadruples, employing grand-sibling and tri-sibling information. The usual inference problem is to find the highest scoring tree for the input sentence. Note that in a valid tree, each token 1, . . . , n must be attached to exactly one parent (either another token or the root 0). We can further require the tree to be projective, meaning that edges are not allowed to cross each other. It is well known that dynamic programming can be used to find the best projective dependency tree in O(n3 ) time, much as in CKY, for firstorder models and some higher-order models (Eisner, 1996; McDonald and Pereira, 2006).1 When the projectivity restriction is lifted, McDonald et al. (2005b) pointed out that the best tree can be found in O(n2 ) time using a minimum directed spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977), though only for first-order models.2 We will make use of this fast non-projective algorithm as a subroutine in early stages of our system. 3 Dynamic Feature Selection Unlike typical feature selection methods that fix a subset of selected features and use it throughout testing, in dynamic feature selection we choose features adaptively for"
D13-1152,P10-1001,0,0.259435,"n] and m ∈ [1, n] (with h 6= m) are a head token and one 1456 of its modifiers. Finding the best tree requires first computing θ·φ(E) for each of the n2 possible edges. Since scoring the edges independently in this way restricts the parser to a local view of the dependency structure, higher-order models can achieve better accuracy. For example, in the second-order model of McDonald and Pereira (2006), each local subgraph E is a triple that includes the head and two modifiers of the head, which are adjacent to each other. Other methods that use triples include grandparent-parent-child triples (Koo and Collins, 2010), or non-adjacent siblings (Carreras, 2007). Third-order models (Koo and Collins, 2010) use quadruples, employing grand-sibling and tri-sibling information. The usual inference problem is to find the highest scoring tree for the input sentence. Note that in a valid tree, each token 1, . . . , n must be attached to exactly one parent (either another token or the root 0). We can further require the tree to be projective, meaning that edges are not allowed to cross each other. It is well known that dynamic programming can be used to find the best projective dependency tree in O(n3 ) time, much as"
D13-1152,J93-2004,0,0.0463942,"Missing"
D13-1152,D08-1017,0,0.0165487,"tic feature selection methods for dependency parsing, and show that even a few feature templates can give decent accuracy (Section 3.2). We then propose a novel way to dynamically select features for each edge while keeping the overhead of decision making low (Section 4). Our present experiments use the Maximum Spanning Tree (MST) parsing algorithm (McDonald et al., 2005a; McDonald and Pereira, 2006). However, our approach applies to other graph-based dependency parsers as well—including non-projective parsing, higher-order parsing, or approximations to higher-order parsing that use stacking (Martins et al., 2008), belief propagation (Smith and Eisner, 2008), or structured boosting (Wang et al., 2007). 2 Graph-based Dependency Parsing In graph-based dependency parsing of an n-word input sentence, we must construct a tree y whose vertices 0, 1, . . . n correspond to the root node (namely 0) and the ordered words of the sentence. Each directed edge of this tree points from a head (parent) to one of its modifiers (child). Following a common approach to structured prediction problems, the score of a tree y is defined as P a sum of local Pscores. That is, sθ (y) = θ · E∈y φ(E) = E∈y θ · φ(E), where E ranges"
D13-1152,E06-1011,0,0.52191,"s to a smaller feature set for time efficiency. We aim to do feature selection and edge pruning dynamically, balancing speed and accuracy by using only as many features as needed. In this paper, we first explore standard static feature selection methods for dependency parsing, and show that even a few feature templates can give decent accuracy (Section 3.2). We then propose a novel way to dynamically select features for each edge while keeping the overhead of decision making low (Section 4). Our present experiments use the Maximum Spanning Tree (MST) parsing algorithm (McDonald et al., 2005a; McDonald and Pereira, 2006). However, our approach applies to other graph-based dependency parsers as well—including non-projective parsing, higher-order parsing, or approximations to higher-order parsing that use stacking (Martins et al., 2008), belief propagation (Smith and Eisner, 2008), or structured boosting (Wang et al., 2007). 2 Graph-based Dependency Parsing In graph-based dependency parsing of an n-word input sentence, we must construct a tree y whose vertices 0, 1, . . . n correspond to the root node (namely 0) and the ordered words of the sentence. Each directed edge of this tree points from a head (parent) t"
D13-1152,P05-1012,0,0.874911,"all possible edges (or other small substructures) using a learned function; in the decoding stage, we use combinatorial optimization to find the dependency tree with the highest total score. Generally linear edge-scoring functions are used for speed. But they use a large set of features, derived from feature templates that consider different conjunctions of the edge’s attributes. As a result, parsing time is dominated by the scoring stage— computing edge attributes, using them to instantiate feature templates, and looking up the weights of the resulting features in a hash table. For example, McDonald et al. (2005a) used on average about 120 first-order feature templates on each edge, built from attributes such as the edge direction and length, the We therefore ask the question: can we use fewer features to score the edges, while maintaining the effect that the true dependency tree still gets a higher score? Motivated by recent progress on dynamic feature selection (Benbouzid et al., 2012; He et al., 2012), we propose to add features one group at a time to the dependency graph, and to use these features together with interactions among edges (as determined by intermediate parsing results) to make hard"
D13-1152,H05-1066,0,0.123804,"Missing"
D13-1152,C08-1094,0,0.0311852,". However, in place of relatively simple heuristics such as a global relative pruning threshold, we learn a featurized decisionmaking policy of a more complex form. Since each decision can affect later stages, or later decisions in the same stage, we model this problem as a sequential decision-making process and solve it by Dataset Aggregation (DAgger) (Ross et al., 2011), a recent iterative imitation learning technique for structured prediction. Previous work has made much progress on the complementary problem: speeding up the decoding stage by pruning the search space of tree structures. In Roark and Hollingshead (2008) and Bergsma and Cherry (2010), pruning decisions are made locally as a preprocessing step. In the recent vine pruning approach (Rush and Petrov, 2012), significant speedup is gained by leveraging structured information via a coarse-to-fine projective parsing cas1455 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics cade (Charniak et al., 2006). These approaches do not directly tackle the feature selection problem. Although pruned edges do not re"
D13-1152,N12-1054,0,0.576196,"form. Since each decision can affect later stages, or later decisions in the same stage, we model this problem as a sequential decision-making process and solve it by Dataset Aggregation (DAgger) (Ross et al., 2011), a recent iterative imitation learning technique for structured prediction. Previous work has made much progress on the complementary problem: speeding up the decoding stage by pruning the search space of tree structures. In Roark and Hollingshead (2008) and Bergsma and Cherry (2010), pruning decisions are made locally as a preprocessing step. In the recent vine pruning approach (Rush and Petrov, 2012), significant speedup is gained by leveraging structured information via a coarse-to-fine projective parsing cas1455 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics cade (Charniak et al., 2006). These approaches do not directly tackle the feature selection problem. Although pruned edges do not require further feature computation, the pruning step must itself compute similar high-dimensional features just to decide which edges to prune. For this"
D13-1152,D08-1016,1,0.482264,"y parsing, and show that even a few feature templates can give decent accuracy (Section 3.2). We then propose a novel way to dynamically select features for each edge while keeping the overhead of decision making low (Section 4). Our present experiments use the Maximum Spanning Tree (MST) parsing algorithm (McDonald et al., 2005a; McDonald and Pereira, 2006). However, our approach applies to other graph-based dependency parsers as well—including non-projective parsing, higher-order parsing, or approximations to higher-order parsing that use stacking (Martins et al., 2008), belief propagation (Smith and Eisner, 2008), or structured boosting (Wang et al., 2007). 2 Graph-based Dependency Parsing In graph-based dependency parsing of an n-word input sentence, we must construct a tree y whose vertices 0, 1, . . . n correspond to the root node (namely 0) and the ordered words of the sentence. Each directed edge of this tree points from a head (parent) to one of its modifiers (child). Following a common approach to structured prediction problems, the score of a tree y is defined as P a sum of local Pscores. That is, sθ (y) = θ · E∈y φ(E) = E∈y θ · φ(E), where E ranges over small connected subgraphs of y that can"
D13-1152,N03-1033,0,0.0754605,"the PTB constituency trees using the head rules of Yamada and Matsumoto (2003). Following convention, we use sections 02–21 for training, section 22 for development and section 23 for testing. We also report results on six languages from the CoNLL-X shared task (Buchholz and Marsi, 2006) as suggested in (Rush and Petrov, 2012), which cover a variety of language families. We follow the standard training/test split specified in the CoNLL-X data and tune parameters by cross validation when training the classifiers (policies). The PTB test data is tagged by a Stanford part-of-speech (POS) tagger (Toutanova et al., 2003) trained on sections 02–21. We use the provided gold POS tags for the CoNLL test data. All results are evaluated by the unlabeled attachment score (UAS). For fair comparison with previous work, punctuation is included when computing parsing accuracy of all CoNLL-X languages but not English (PTB). For policy training, we train a linear SVM classifier using Liblinear (Fan et al., 2008). For all languages, we run DAgger for 20 iterations and seLanguage Method DYN FS Bulgarian V INE P DYN FS Chinese V INE P DYN FS English V INE P DYN FS German V INE P DYN FS Japanese V INE P DYN FS Portuguese V IN"
D13-1152,W03-3023,0,0.0530048,"f the next feature group to be added We also tried more complex meta-features, for example, mean and variance of the scores of competing edges, and structured features such as whether the head of e is locked and how many locked children it currently has. It turns out that given all the parsing features, the margin is the most discriminative meta-feature. When it is present, other metafeatures we added do not help much, Thus we do not include them in our experiments due to overhead. 6 6.1 Experiment Setup We generate dependency structures from the PTB constituency trees using the head rules of Yamada and Matsumoto (2003). Following convention, we use sections 02–21 for training, section 22 for development and section 23 for testing. We also report results on six languages from the CoNLL-X shared task (Buchholz and Marsi, 2006) as suggested in (Rush and Petrov, 2012), which cover a variety of language families. We follow the standard training/test split specified in the CoNLL-X data and tune parameters by cross validation when training the classifiers (policies). The PTB test data is tagged by a Stanford part-of-speech (POS) tagger (Toutanova et al., 2003) trained on sections 02–21. We use the provided gold PO"
D15-1108,P03-1006,0,0.0606744,"2). To improve the method, recall that subproblem k considers only variables X k . It is indifferent to the value of Xi if Xi ∈ / X k , so we just leave xki undefined in the subproblem’s solution. We treat that as automatically satisfying the equality constraint; thus we do not need any Lagrange multiplier λki to force equality. Our final solution x ignores undefined values, and sets xi to the value agreed on by the subproblems that did consider Xi .7 4.2 Gki (xk ) = λki · γ(xki ) def (4) These unary factors penalize strings according to the Lagrange multipliers. They can be encoded as WFSAs (Allauzen et al., 2003; Cotterell and Eisner, 2015, Appendices B.1–B.5), allowing us to solve the subproblem by max-product BP as usual. The topology of the WFSA for Gki depends only on Wi , while its weights come from λki . 4.3 Projected Subgradient Method We aim to adjust the collection λ of Lagrange multipliers to minimize the upper bound (3). Following Komodakis et al. (2007), we solve this convex dual problem using a projected subgradient method. We initialize λ = 0 and compute (3) by solving the K subproblems. Then we take a step to adjust λ, and repeat in hopes of eventually satisfying the equality condition"
D15-1108,D07-1093,0,0.086226,"Missing"
D15-1108,D10-1125,0,0.0361387,"inite-state methods to decode a composition of several finite-state noisy channels (Pereira and Riley, 1997; Knight and Graehl, 1998) can be regarded as BP on a graphical model over strings that has a linear-chain topology. 919 4 Dual Decomposition rεzɪgn eɪʃən rεzɪgn#eɪʃən rizajn z rizajn#z dæmn eɪʃən dæmn#eɪʃən r,εzɪgn’eɪʃn riz’ajnz d,æmn’eɪʃn Subproblem 1 Subproblem 2 Subproblem 3 dæmn z Dual decomposition is a general technique for solving constrained optimization problems. It has been widely used for MAP inference in graphical models (Komodakis et al., 2007; Komodakis and Paragios, 2009; Koo et al., 2010; Martins et al., 2011; Sontag et al., 2011; Rush and Collins, 2014). However, previous work has focused on variables Xi whose values are in R or a small finite set; we will consider the infinite set Σ∗ . dæmn#z d’æmz Subproblem 4 Figure 2: To apply dual decomposition, we choose to decompose 1 into one subproblem per surface word. Dashed lines connect two or more variables from different subproblems that correspond to the same variable in the original graph. The method of Lagrange multipliers is used to force these variables to have identical values. An additional unary factor attached to each"
D15-1108,N15-1094,1,0.87548,"work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.2 2.3 def score(x) = X F (x). (1) F ∈F The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable. Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical rea"
D15-1108,P14-2102,1,0.822861,"ability of copying the next character of the underlying word as it is transduced to the surface word. The remaining probability mass 1−θ is apportioned equally among insertion, substitution and deletion operations.11 This models phonology as “noisy concatenation”—the minimum necessary to account for the fact that surface words cannot quite be obtained as simple concatenations of their shared underlying morphemes. Model 2 is a replication of the much more complicated parametric model of Cotterell et al. (2015), which can handle linguistic phonology. Here the factor Sθ is a contextual edit FST (Cotterell et al., 2014). The probabilities of competing edits in a given context are determined by a loglinear model with weight vector θ and features that are meant to pick up on phonological phenomena. E XERCISE Small datasets of Catalan, English, Maori, and Tangale, drawn from phonology textbooks. Each dataset contains 55 to 106 surface words, formed from a collection of 16 to 55 morphemes. CELEX Larger datasets of German, English, and Dutch, drawn from the CELEX database (Baayen et al., 1995). Each dataset contains 1000 surface words, formed from 341 to 381 underlying morphemes. 5.2 Evaluation Scheme We compared"
D15-1108,Q15-1031,1,0.10354,"noisy forms; contemporary or historical forms; underlying or surface forms; source or target language forms. Such relationships arise in domains such as morphology, phonology, historical linguistics, translation between related languages, and social media text analysis. In this paper, we assume a given graphical model, whose factors evaluate the relationships among observed and unobserved strings.1 We present a dual decomposition algorithm for MAP inference, which returns a certifiably optimal solution when it converges. We demonstrate our method on a graphical model for phonology proposed by Cotterell et al. (2015). We show that the method generally converges and that it achieves better results than alternatives. The rest of the paper is arranged as follows: We will review graphical models over strings in section 2, and briefly introduce our sample problem in section 3. Section 4 develops dual decomposition inference for graphical models over strings. Then our experimental setup and results are presented in sections 5 and 6, with some discussion. We investigate dual decomposition for joint MAP inference of many strings. Given an arbitrary graphical model, we decompose it into small acyclic sub-models, w"
D15-1108,D09-1011,1,0.883132,"Thus, a degree d-factor scores some length-d subtuple of x. The score of the whole joint assignment simply sums over all factors: We seek the x of maximum score that is consistent with our partial observation of x. This is a generic constraint satisfaction problem with soft constraints. While our algorithm does not depend on a probabilistic interpretation of the factor graph,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution def p(x) = (1/Z) exp score(x). models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over Σ∗ where Σ is some fixed, finite alphabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to"
D15-1108,D11-1057,1,0.881701,"cores some length-d subtuple of x. The score of the whole joint assignment simply sums over all factors: We seek the x of maximum score that is consistent with our partial observation of x. This is a generic constraint satisfaction problem with soft constraints. While our algorithm does not depend on a probabilistic interpretation of the factor graph,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution def p(x) = (1/Z) exp score(x). models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over Σ∗ where Σ is some fixed, finite alphabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to approximately sample from"
D15-1108,N12-1024,1,0.896936,"g Count Features But what do we do if the variables are strings? The Lagrangian term λki ·xki in (3) is now ill-typed. We replace it with λki · γ(xki ), where γ(·) extracts a real-valued feature vector from a string, and λki is a vector of Lagrange multipliers. This corresponds to changing the constraint in (2). Instead of requiring x1i = · · · = xK i for each 1 i, we are now requiring γ(xi ) = · · · = γ(xK i ), i.e., these strings must agree in their features. We want each possible string to have a unique feature vector, so that matching features forces the actual strings to match. We follow Paul and Eisner (2012) and use a substring count feature for each w ∈ Σ∗ . In other words, γ(x) is an infinitely long vector, which maps each w to the number of times that w appears in x as a substring.8 Computing λki · γ(xki ) in (3) remains possible because in practice, λki will have only finitely many nonzeros. This is so because our feature vector γ(x) has only finitely many nonzeros for any string x, and the subgradient algorithm in section 4.3 below always updates λki by adding multiples of such γ(x) vectors. We will use a further trick below to prevent rapid growth of this finite set of nonzeros. Each variab"
D15-1108,P10-1105,0,0.0677345,"has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.2 2.3 def score(x) = X F (x). (1) F ∈F The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable. Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical reasons, suppose momentarily that all factors have degree ≤ 2 (this loses no power). Then BP manipulates only vectors and matrices—whose dimensionality depends on the number of possible values of the vari2 E.g., it could be used for exactly computing the separation oracle when training a structural SVM"
D15-1108,D11-1032,0,0.0133363,"oximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.2 2.3 def score(x) = X F (x). (1) F ∈F The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable. Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical reasons, suppose momentarily that all factors have degree ≤ 2 (this loses no power). Then BP manipulates only vectors and matrices—whose dimensionality depends on the number of possible values of the vari2 E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al.,"
D15-1108,W10-2902,0,0.0304375,"test11 That is, probability mass of (1 − θ)/3 is divided equally among the |Σ |possible insertions; another (1 − θ)/3 is divided equally among the |Σ|−1 possible substitutions; and the final (1 − θ)/3 is allocated to deletion. 10 The model also has a three-way factor, connecting layers 1 and 2 of Figure 1. This represents deterministic concatenation (appropriate for these languages) and has no parameters. 923 ing. This gives us three approximations to EM, based on DD, SP and MP. Note that DD specifically gives the Viterbi approximation to EM— which sometimes gets better results than true EM (Spitkovsky et al., 2010). For MP (but not SP), we extract only the 1-best predictions for the E step, since we study MP as an approximation to DD. As initialization, our first E step uses the trained version of Model 1 for the same inference method. 5.5 (a) Tangale (b) Catalan (c) Maori (d) English Inference Details We run SP and MP for 20 iterations (usually the predictions converge within 10 iterations). We run DD to convergence (usually &lt; 600 iterations). DD iterations are much faster since each variable considers d strings, not d distributions over strings. Hence DD does not intersect distributions, and many part"
D15-1108,J98-4003,0,\N,Missing
D16-1206,P03-1006,0,0.0582063,"longest suffix of s = yt−k . . . yt that appears in W—and thus w(hyt ) = w(s) ∈ W and provides sufficient information to compute f (x, t, s).2 For a given x of length n and given parameters θ, the log-normalizer log Zθ (x)—which will be needed to compute the log-probability in eq. (1) below—can be found in time O(|W |n) by dynamic programming. Concise pseudocode is in Alg. 1. In effect, this 2 Our DFA construction is essentially that of Cotterell and Eisner (2015, Appendix B.5). However, Appendix B of that paper also gives a construction that obtains an even smaller DFA by using failure arcs (Allauzen et al., 2003), which remove the requirement that W be closed under last-character substitution. This would yield a further speedup to our Alg. 1 (replacing it with the efficient backward algorithm in footnote 16 of that paper) and similarly to our Alg. 2 (by differentiating the new Alg. 1). runs the forward algorithm on the lattice of taggings given by length-n paths through the DFA. For finding the parameters θ that minimize eq. (1) below, we want the gradient ∇θ log Zθ (x). By applying algorithmic differentiation to Alg. 1, we obtain Alg. 2, which uses back-propagation to compute the gradient (asymptotic"
D16-1206,N15-1094,1,0.920904,"with state set H and alphabet Y . If this DFA is used to read any tag sequence y ∈ Y ∗ , then the arc that reads yt comes from a state h such that hyt is the longest suffix of s = yt−k . . . yt that appears in W—and thus w(hyt ) = w(s) ∈ W and provides sufficient information to compute f (x, t, s).2 For a given x of length n and given parameters θ, the log-normalizer log Zθ (x)—which will be needed to compute the log-probability in eq. (1) below—can be found in time O(|W |n) by dynamic programming. Concise pseudocode is in Alg. 1. In effect, this 2 Our DFA construction is essentially that of Cotterell and Eisner (2015, Appendix B.5). However, Appendix B of that paper also gives a construction that obtains an even smaller DFA by using failure arcs (Allauzen et al., 2003), which remove the requirement that W be closed under last-character substitution. This would yield a further speedup to our Alg. 1 (replacing it with the efficient backward algorithm in footnote 16 of that paper) and similarly to our Alg. 2 (by differentiating the new Alg. 1). runs the forward algorithm on the lattice of taggings given by length-n paths through the DFA. For finding the parameters θ that minimize eq. (1) below, we want the g"
D16-1206,W16-5901,1,0.806645,"omain experts or “one size fits all” strategies (e.g., k-CRF). Our goal is to choose θ—and thus W—so that inference is accurate and fast. Our approach is to modify the usual L2 regularized log-likelihood training criterion with a carefully defined runtime penalty scaled by a parameter γ to balance competing objectives: likelihood on the data {(x(i) , y (i) )}m i=1 vs. efficiency (small W). m X − log pθ (y (i) |x(i) ) + λ||θ||22 + γR(θ) (1) | {z } |{z } |{z } i=1 loss generalization runtime Recall that the runtime of inference on a given sentence is proportional to the size of W, the closure 3 Eisner (2016) explains the connection between algorithmic differentiation and the forward-backward algorithm. 4 Extensions to richer sets of higher-order features are possible, such as conjunctions with properties of the words at position t. 1975 ε N NN G&quot; V NV VN VV GV Figure 2: A visual depiction of the tree-structured group lasso penalty. Each node represents a tag string feature. The group indexed by a node’s tag string is defined as the set of features that are proper descendants of the node. For example, the lavender box indicates the largest group Gε and the aubergine box indicates a smaller group G"
D16-1206,D13-1152,1,0.904177,"Missing"
D16-1206,D11-1139,0,0.0487638,"Missing"
D16-1206,D13-1024,0,0.0201484,"ing the online proximal gradient algorithm SPOM (Martins et al., 2011b) and Adagrad (Duchi et al., 2011) with η = 0.01 and 15 inner epochs. We limited to 3 active set iterations, and as a result, our final W contained at most tag trigrams. 4 Related Work Our paper can be seen as transferring methods of Cotterell and Eisner (2015) to the CRF setting. They too used tree-structured group lasso and active set to select variable-order n-gram features W for globally-normalized sequence models (in their case, to rapidly and accurately approximate beliefs during message-passing inference). Similarly, Nelakanti et al. (2013) used tree-structured group lasso to regularize a variable-order language model (though their focus was training speed). Here we apply these techniques to conditional models for tagging. Our work directly builds on the variable-order CRF of Cuong et al. (2014), with a speedup in Alg. 2, but our approach also learns the VoCRF structure. Our method is also related to the generative variable-order tagger of Sch¨utze and Singer (1994). Our static feature selection chooses a single model that permits fast exact marginal inference, similar to learning a low-treewidth graphical model (Bach and 5 Each"
D16-1206,petrov-etal-2012-universal,0,0.0521156,"Missing"
D16-1206,P94-1025,0,0.581918,"Missing"
D16-1206,P15-1015,0,0.015011,"is also related to the generative variable-order tagger of Sch¨utze and Singer (1994). Our static feature selection chooses a single model that permits fast exact marginal inference, similar to learning a low-treewidth graphical model (Bach and 5 Each gradient computation in this inner optimization takes time O(|Wactive |n), which is especially fast at early iterations. 1976 Jordan, 2001; Elidan and Gould, 2008). This contrasts with recent papers that learn to do approximate 1-best inference using a sequence of models, whether by dynamic feature selection within a greedy inference algorithm (Strubell et al., 2015), or by gradually increasing the feature set of a 1-best global inference algorithm and pruning its hypothesis space after each increase (Weiss and Taskar, 2010; He et al., 2013). Schmidt (2010) explores the use of group lasso penalties and the active set method for learning the structure of a graphical model, but does not consider learning repeated structures (in our setting, W defines a structure that is reused at each position). Steinhardt and Liang (2015) jointly modeled the amount of context to use in a variable-order model that dynamically determines how much context to use in a beam sea"
D16-1206,N03-1033,0,0.0744387,"tures over the output structure are limited. For example, an order-k CRF (or “k-CRF” for short, with k &gt; 1 being “higher-order”) allows expressive features over a window of k+1 adjacent tags (as well as the input), and then inference takes time O(n·|Y |k+1 ), where Y is the set of tags and n is the length of the input. How large does k need to be? Typically k = 2 works well, with big gains from 0 → 1 and modest ∗ 95 Equal contribution gains from 1 → 2 (Fig. 1). Small k may be sufficient when there is enough training data to allow the model to attend to many fine-grained features of the input (Toutanova et al., 2003; Liang et al., 2008). For example, when predicting POS tags in morphologicallyrich languages, certain words are easily tagged based on their spelling without considering the context (k = 0). In fact, such languages tend to have a more free word order, making tag context less useful. We investigate a hybrid approach that gives the accuracy of higher-order models while reducing runtime. We build on variable-order CRFs (Ye et al., 2009) (VoCRF), which support features on tag subsequences of mixed orders. Since only modest gains are obtained from moving to higher-order models, we posit that only"
D18-1163,Q16-1022,0,0.263649,"Missing"
D18-1163,Q16-1031,0,0.0975913,"s summary results; the number in each column header gives the number of points summarized. For each column, we boldface the better result between the “Synthetic” and “Original”, or both if they are not significantly different (paired permutation test, p &lt; 0.01). We also show the oracle permutation result in row “Oracle”. Synthetic Treebank: 50.36 90 80 70 60 50 40 30 cu el eu fa fi_ftb ga 20 10 10 20 30 40 50 60 he hr hu id ja_ktc la 70 Original Treebank: 48.61 pl ro sl sv ta 80 90 Figure 3: UAS on 337 language pairs from the training languages to the test languages. Zhang and Barzilay, 2015; Ammar et al., 2016), and improving the lexical representations via multilingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016) and synthetic data generation (§6.2). 6.2 Synthetic data generation Our novel proposal ties into the recent interest in data augmentation in supervised machine learning. In unsupervised parsing, the most widely adopted synthetic data method has been annotation projection, which generates synthetic analyses of target-language sentences by “projecting” the analysis from a source-language translation. Of course, this requires bilingual corpora as an additional re"
D18-1163,J07-2003,0,0.068823,"ted bigram counts cp under pθ (or equivalently pˆθ ), for use above. This averages expected counts from each unordered tree x ∈ B. Algorithm 1 in the supplement gives pseudocode. The insight is that rather than sampling a single realization of x (as B 0 does), we can use dynamic programming to sum efficiently over all of its exponentially many realizations. This gives an exact answer. It algorithmically resembles tree-to-string machine translation, which likewise considers the possible reorderings of a source tree and incorporates a language model by similarly tracking their surface N -grams (Chiang, 2007, §5.3.2). For each node a of the tree x, let the POS string ya be the realization of the subtree rooted at a. Let ca (st) be the expected count of bigram st in ya , whose distribution is governed by equation (1). We allow s = BOS or t = EOS as defined in §2.4.2. The ca function can be represented as a sparse map from POS bigrams to reals. We compute ca at each node a of x in a bottom-up order. The final step computes croot , giving the expected bigram counts in x’s realization y (that is, cp in §2.4). We find ca as follows. Let n = na and recall from §2.2 that π(a) is an ordering of a1 , . ."
D18-1163,K15-1012,0,0.0703765,"ch column, we boldface the better result between the “Synthetic” and “Original”, or both if they are not significantly different (paired permutation test, p &lt; 0.01). We also show the oracle permutation result in row “Oracle”. Synthetic Treebank: 50.36 90 80 70 60 50 40 30 cu el eu fa fi_ftb ga 20 10 10 20 30 40 50 60 he hr hu id ja_ktc la 70 Original Treebank: 48.61 pl ro sl sv ta 80 90 Figure 3: UAS on 337 language pairs from the training languages to the test languages. Zhang and Barzilay, 2015; Ammar et al., 2016), and improving the lexical representations via multilingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016) and synthetic data generation (§6.2). 6.2 Synthetic data generation Our novel proposal ties into the recent interest in data augmentation in supervised machine learning. In unsupervised parsing, the most widely adopted synthetic data method has been annotation projection, which generates synthetic analyses of target-language sentences by “projecting” the analysis from a source-language translation. Of course, this requires bilingual corpora as an additional resource. Annotation projection was proposed by Yarowsky et al. (2001), gained promising results o"
D18-1163,N09-2057,0,0.0355921,"particular order on the tree’s word tokens. More precisely, a language specifies a distribution p(string |unordered tree) over a tree’s possible realizations. As an engineering matter, we now make the strong assumption that the unordered dependency trees are similar across languages. That is, we suppose that different languages use similar underlying syntactic/semantic graphs, but differ in how they realize this graph structure on the surface. 1 Modeling this process was the topic of the recent Surface Realization Shared Task (Mille et al., 2018). Most relevant is work on tree linearization (Filippova and Strube, 2009; Futrell and Gibson, 2015; Puzikov and Gurevych, 2018). Thus, given a gold POS corpus u of the unknown target language, we may hope to explain its distribution of surface POS bigrams as the result of applying some target-language surface realization model to the distribution of cross-linguistically “typical” unordered trees. To obtain samples of the latter distribution, we use the treebanks of one or more other languages. The present paper evaluates our method when only a single source treebank is used. In the future, we could try tuning a mixture of all available source treebanks. 2.1 Realiz"
D18-1163,D15-1231,0,0.080899,"e’s word tokens. More precisely, a language specifies a distribution p(string |unordered tree) over a tree’s possible realizations. As an engineering matter, we now make the strong assumption that the unordered dependency trees are similar across languages. That is, we suppose that different languages use similar underlying syntactic/semantic graphs, but differ in how they realize this graph structure on the surface. 1 Modeling this process was the topic of the recent Surface Realization Shared Task (Mille et al., 2018). Most relevant is work on tree linearization (Filippova and Strube, 2009; Futrell and Gibson, 2015; Puzikov and Gurevych, 2018). Thus, given a gold POS corpus u of the unknown target language, we may hope to explain its distribution of surface POS bigrams as the result of applying some target-language surface realization model to the distribution of cross-linguistically “typical” unordered trees. To obtain samples of the latter distribution, we use the treebanks of one or more other languages. The present paper evaluates our method when only a single source treebank is used. In the future, we could try tuning a mixture of all available source treebanks. 2.1 Realization is systematic We pre"
D18-1163,P09-1042,0,0.134878,"generation Our novel proposal ties into the recent interest in data augmentation in supervised machine learning. In unsupervised parsing, the most widely adopted synthetic data method has been annotation projection, which generates synthetic analyses of target-language sentences by “projecting” the analysis from a source-language translation. Of course, this requires bilingual corpora as an additional resource. Annotation projection was proposed by Yarowsky et al. (2001), gained promising results on sequence labelling tasks, and was later developed for unsupervised parsing (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014; Ma and Xia, 2014; Tiedemann et al., 2014). Recent work in this vein has mainly focused on improving the synthetic data, including reweighting the training trees (Agi´c et al., 2016) or pruning those that cannot be aligned well (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016). On the other hand, Wang and Eisner (2016) proposed to permute source language treebanks using word order realization models trained on other source languages. They generated on the order of 50,000 synthetic languages by “mixing and matching” a few dozen source languages. T"
D18-1163,P10-2036,0,0.0208298,"algorithm, and then use it to parse. Some such approaches try to improve the grammar model. For example, Klein and Manning (2004)’s dependency model with valence was the first to beat a trivial baseline; later improvements considered higher-order effects and punctuation (Headden III et al., 2009; Spitkovsky et al., 2012). Other approaches try to avoid search error, using strategies like convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), informed initialization (Klein and Manning, 2004; Mareˇcek and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013). The alternative of cross-lingual transfer has recently flourished thanks to the development of consistent cross-lingual datasets of POS-tagged (Petrov et al., 2012) and dependency-parsed (McDonald et al., 2013) sentences. McDonald et al. (2011) showed a significant improvement over grammar induction by simply using the delexicalized parser trained on other language(s). Subsequent improvements have come from re-weighting source languages (Søgaard, 2011b; Rosa and ˇ Zabokrtsk´ y, 2015a,b; Wa"
D18-1163,N12-1069,0,0.0434915,"Manning, 2004; Headden III et al., 2009; Naseem et al., 2010) estimate a generative grammar to explain the sentences, for example by the ExpectationMaximization (EM) algorithm, and then use it to parse. Some such approaches try to improve the grammar model. For example, Klein and Manning (2004)’s dependency model with valence was the first to beat a trivial baseline; later improvements considered higher-order effects and punctuation (Headden III et al., 2009; Spitkovsky et al., 2012). Other approaches try to avoid search error, using strategies like convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), informed initialization (Klein and Manning, 2004; Mareˇcek and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013). The alternative of cross-lingual transfer has recently flourished thanks to the development of consistent cross-lingual datasets of POS-tagged (Petrov et al., 2012) and dependency-parsed (McDonald et al., 2013) sentences. McDonald et al. (2011) showed a significant improvement over grammar induction by simply using the dele"
D18-1163,P13-1044,1,0.88619,"aches try to improve the grammar model. For example, Klein and Manning (2004)’s dependency model with valence was the first to beat a trivial baseline; later improvements considered higher-order effects and punctuation (Headden III et al., 2009; Spitkovsky et al., 2012). Other approaches try to avoid search error, using strategies like convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), informed initialization (Klein and Manning, 2004; Mareˇcek and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013). The alternative of cross-lingual transfer has recently flourished thanks to the development of consistent cross-lingual datasets of POS-tagged (Petrov et al., 2012) and dependency-parsed (McDonald et al., 2013) sentences. McDonald et al. (2011) showed a significant improvement over grammar induction by simply using the delexicalized parser trained on other language(s). Subsequent improvements have come from re-weighting source languages (Søgaard, 2011b; Rosa and ˇ Zabokrtsk´ y, 2015a,b; Wang and Eisner, 2016), adapting the model to the targ"
D18-1163,Q16-1025,0,0.0208206,"bination that is close to the target language, we “combine” the source treebank with a POS corpus of the target language, which guides our customized permutation of the source. Beyond unsupervised parsing, synthetic data has been used for several other tasks. In NLP, it has been used for complex tasks such as questionanswering (QA) (Serban et al., 2016) and machine reading comprehension (Weston et al., 2016; Hermann et al., 2015; Rajpurkar et al., 2016), where highly expressive neural models are used and not enough real data is available to train them. In the playground of supervised parsing, Gulordava and Merlo (2016) conduct a controlled study on the parsibility of languages by generating treebanks with short dependency length and low variability of word order. 7 Conclusion & Future Work We have shown how cross-lingual transfer parsing can be improved by permuting the source treebank to better resemble the target language on the surface (in its distribution of gold POS bigrams). The code is available at https://github. com/wddabc/ordersynthetic. Our work is grounded in the notion that by trying to explain the POS bigram counts in a target corpus, we can discover a stochastic realization policy for the tar"
D18-1163,N09-1012,0,0.0222141,"Missing"
D18-1163,Q16-1023,0,0.0679279,"escent (up to a local optimum). This optimization procedure yields trees for a new artificial language that resembles the target language. We show that delexicalized parsers for the target language can be successfully trained using such “made to order” artificial languages. 1 Introduction Dependency parsing is a core task in natural language processing (NLP). Given a sentence, a dependency parser produces a dependency tree, which specifies the typed head-modifier relations between pairs of words. While supervised dependency parsing has been successful (McDonald and Pereira, 2006; Nivre, 2008; Kiperwasser and Goldberg, 2016), unsupervised parsing can hardly produce useful parses (Mareˇcek, 2016). So it is extremely helpful to have some treebank of supervised parses for training purposes. 1.1 Past work: Cross-lingual transfer Unfortunately, manually constructing a treebank for a new target language is expensive (B¨ohmov´a et al., 2003). As an alternative, cross-lingual transfer parsing (McDonald et al., 2011) is sometimes possible, thanks to the recent development of multi-lingual treebanks (McDonald et al., 2013; Nivre et al., 2015; Nivre et al., 2017). The idea is to parse the sentences of the target language wi"
D18-1163,P04-1061,0,0.0158479,"yperparameters (Appendix C) and report on single-source transfer to the 17 held-out treebanks. The development results hold up in Figure 3. Using the synthetic languages yields 50.36 UAS on average—1.75 points over the baseline, which is significant (paired permutation test, p &lt; 0.01). In the supplementary material (Appendix E), we include some auxiliary experiments on multisource transfer. 6 6.1 Related Work Unsupervised parsing Unsupervised parsing has remained challenging for decades (Mareˇcek, 2016). Classical grammar induction approaches (Lari and Young, 1990; Carroll and Charniak, 1992; Klein and Manning, 2004; Headden III et al., 2009; Naseem et al., 2010) estimate a generative grammar to explain the sentences, for example by the ExpectationMaximization (EM) algorithm, and then use it to parse. Some such approaches try to improve the grammar model. For example, Klein and Manning (2004)’s dependency model with valence was the first to beat a trivial baseline; later improvements considered higher-order effects and punctuation (Headden III et al., 2009; Spitkovsky et al., 2012). Other approaches try to avoid search error, using strategies like convexified objectives (Wang et al., 2008; Gimpel and Smi"
D18-1163,N16-1121,0,0.503151,"language translation. Of course, this requires bilingual corpora as an additional resource. Annotation projection was proposed by Yarowsky et al. (2001), gained promising results on sequence labelling tasks, and was later developed for unsupervised parsing (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014; Ma and Xia, 2014; Tiedemann et al., 2014). Recent work in this vein has mainly focused on improving the synthetic data, including reweighting the training trees (Agi´c et al., 2016) or pruning those that cannot be aligned well (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016). On the other hand, Wang and Eisner (2016) proposed to permute source language treebanks using word order realization models trained on other source languages. They generated on the order of 50,000 synthetic languages by “mixing and matching” a few dozen source languages. Their idea was that with a large set of synthetic languages, they could use them as supervised examples to train an unsupervised structure discovery system that could analyze any new language. Systems built with this dataset were competitive in single-source parser transfer (Wang and Eisner, 2016), typology 1332 prediction ("
D18-1163,P99-1004,0,0.0189923,"his contribution to at most log α , for some small α ∈ (0, 1), we define KLα (p ||q) by a variant of equation (3) in which q(t |s) has been redef placed by q˜(t |s) = αp(t |s) + (1 − α)q(t |s).5 3 A more familiar definition of cq would use the total count in u. Our definition, which yields the same bigram probabilities, is analogous to our definition of cp . This cp is needed for KL(p ||q) in (3), and cq symmetrically for KL(q ||p). 4 Ideally one should tune λ to minimize the language model perplexity on held-out data (e.g., by cross-validation). 5 This is inspired by the α-skew divergence of Lee (1999, 1327 Our final divergence metric D(ˆ pθ , qˆ) defines D as a linear combination of exclusive and inclusive KLα divergences, which respectively emphasize pθ ’s precision and recall at matching q’s bigrams: KLα1 (p ||q) KLα2 (q ||p) +β· Ey∼p [ |y |] Ey∼q [ |y |] (4) where β, α1 , α2 are tuned by cross-validation to maximize the downstream parsing performance. The division by average sentence length converts KL from nats per sentence to nats per word,6 so that the KL values have comparable scale even if B has much longer or shorter sentences than u. D(p, q) = (1−β)· 3 3.1 Also, let a0 and an+1"
D18-1163,P14-1126,0,0.200562,"data augmentation in supervised machine learning. In unsupervised parsing, the most widely adopted synthetic data method has been annotation projection, which generates synthetic analyses of target-language sentences by “projecting” the analysis from a source-language translation. Of course, this requires bilingual corpora as an additional resource. Annotation projection was proposed by Yarowsky et al. (2001), gained promising results on sequence labelling tasks, and was later developed for unsupervised parsing (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014; Ma and Xia, 2014; Tiedemann et al., 2014). Recent work in this vein has mainly focused on improving the synthetic data, including reweighting the training trees (Agi´c et al., 2016) or pruning those that cannot be aligned well (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016). On the other hand, Wang and Eisner (2016) proposed to permute source language treebanks using word order realization models trained on other source languages. They generated on the order of 50,000 synthetic languages by “mixing and matching” a few dozen source languages. Their idea was that with a large set of synthetic languages,"
D18-1163,P13-1028,0,0.0583922,"Missing"
D18-1163,D11-1006,0,0.859779,"Missing"
D18-1163,W18-3601,0,0.0181248,"y to realize its unordered trees as surface strings:1 it imposes a particular order on the tree’s word tokens. More precisely, a language specifies a distribution p(string |unordered tree) over a tree’s possible realizations. As an engineering matter, we now make the strong assumption that the unordered dependency trees are similar across languages. That is, we suppose that different languages use similar underlying syntactic/semantic graphs, but differ in how they realize this graph structure on the surface. 1 Modeling this process was the topic of the recent Surface Realization Shared Task (Mille et al., 2018). Most relevant is work on tree linearization (Filippova and Strube, 2009; Futrell and Gibson, 2015; Puzikov and Gurevych, 2018). Thus, given a gold POS corpus u of the unknown target language, we may hope to explain its distribution of surface POS bigrams as the result of applying some target-language surface realization model to the distribution of cross-linguistically “typical” unordered trees. To obtain samples of the latter distribution, we use the treebanks of one or more other languages. The present paper evaluates our method when only a single source treebank is used. In the future, we"
D18-1163,P12-1066,0,0.432165,"rnative of cross-lingual transfer has recently flourished thanks to the development of consistent cross-lingual datasets of POS-tagged (Petrov et al., 2012) and dependency-parsed (McDonald et al., 2013) sentences. McDonald et al. (2011) showed a significant improvement over grammar induction by simply using the delexicalized parser trained on other language(s). Subsequent improvements have come from re-weighting source languages (Søgaard, 2011b; Rosa and ˇ Zabokrtsk´ y, 2015a,b; Wang and Eisner, 2016), adapting the model to the target language using WALS (Dryer and Haspelmath, 2013) features (Naseem et al., 2012; T¨ackstr¨om et al., 2013; 1331 Original Synthetic Oracle All (376) 51.92 52.92 59.45 in-family (46) 63.90 62.85 66.14 cross-family (330) 50.24 51.53 58.51 Figure 2: Unlabeled attachment scores (UAS) from 376 pairs of development treebanks. Each column represents a target treebank, and each polyline within that column shows transfer from variants of a different source treebank. The three points on the polyline (from left to right) represent the target UAS for parsers trained on three sources: the original source treebank, the “made to order” permutation that attempts to match surface statisti"
D18-1163,D10-1120,0,0.0199319,"urce transfer to the 17 held-out treebanks. The development results hold up in Figure 3. Using the synthetic languages yields 50.36 UAS on average—1.75 points over the baseline, which is significant (paired permutation test, p &lt; 0.01). In the supplementary material (Appendix E), we include some auxiliary experiments on multisource transfer. 6 6.1 Related Work Unsupervised parsing Unsupervised parsing has remained challenging for decades (Mareˇcek, 2016). Classical grammar induction approaches (Lari and Young, 1990; Carroll and Charniak, 1992; Klein and Manning, 2004; Headden III et al., 2009; Naseem et al., 2010) estimate a generative grammar to explain the sentences, for example by the ExpectationMaximization (EM) algorithm, and then use it to parse. Some such approaches try to improve the grammar model. For example, Klein and Manning (2004)’s dependency model with valence was the first to beat a trivial baseline; later improvements considered higher-order effects and punctuation (Headden III et al., 2009; Spitkovsky et al., 2012). Other approaches try to avoid search error, using strategies like convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), informed initialization (Klein and Ma"
D18-1163,J08-4003,0,0.0522697,"by gradient descent (up to a local optimum). This optimization procedure yields trees for a new artificial language that resembles the target language. We show that delexicalized parsers for the target language can be successfully trained using such “made to order” artificial languages. 1 Introduction Dependency parsing is a core task in natural language processing (NLP). Given a sentence, a dependency parser produces a dependency tree, which specifies the typed head-modifier relations between pairs of words. While supervised dependency parsing has been successful (McDonald and Pereira, 2006; Nivre, 2008; Kiperwasser and Goldberg, 2016), unsupervised parsing can hardly produce useful parses (Mareˇcek, 2016). So it is extremely helpful to have some treebank of supervised parses for training purposes. 1.1 Past work: Cross-lingual transfer Unfortunately, manually constructing a treebank for a new target language is expensive (B¨ohmov´a et al., 2003). As an alternative, cross-lingual transfer parsing (McDonald et al., 2011) is sometimes possible, thanks to the recent development of multi-lingual treebanks (McDonald et al., 2013; Nivre et al., 2015; Nivre et al., 2017). The idea is to parse the se"
D18-1163,petrov-etal-2012-universal,0,0.0610394,"Missing"
D18-1163,W18-3602,0,0.0234151,"isely, a language specifies a distribution p(string |unordered tree) over a tree’s possible realizations. As an engineering matter, we now make the strong assumption that the unordered dependency trees are similar across languages. That is, we suppose that different languages use similar underlying syntactic/semantic graphs, but differ in how they realize this graph structure on the surface. 1 Modeling this process was the topic of the recent Surface Realization Shared Task (Mille et al., 2018). Most relevant is work on tree linearization (Filippova and Strube, 2009; Futrell and Gibson, 2015; Puzikov and Gurevych, 2018). Thus, given a gold POS corpus u of the unknown target language, we may hope to explain its distribution of surface POS bigrams as the result of applying some target-language surface realization model to the distribution of cross-linguistically “typical” unordered trees. To obtain samples of the latter distribution, we use the treebanks of one or more other languages. The present paper evaluates our method when only a single source treebank is used. In the future, we could try tuning a mixture of all available source treebanks. 2.1 Realization is systematic We presume that the target language"
D18-1163,W15-2209,0,0.183195,"Missing"
D18-1163,P16-1056,0,0.0230688,"Missing"
D18-1163,D09-1086,1,0.866796,"proposal ties into the recent interest in data augmentation in supervised machine learning. In unsupervised parsing, the most widely adopted synthetic data method has been annotation projection, which generates synthetic analyses of target-language sentences by “projecting” the analysis from a source-language translation. Of course, this requires bilingual corpora as an additional resource. Annotation projection was proposed by Yarowsky et al. (2001), gained promising results on sequence labelling tasks, and was later developed for unsupervised parsing (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014; Ma and Xia, 2014; Tiedemann et al., 2014). Recent work in this vein has mainly focused on improving the synthetic data, including reweighting the training trees (Agi´c et al., 2016) or pruning those that cannot be aligned well (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016). On the other hand, Wang and Eisner (2016) proposed to permute source language treebanks using word order realization models trained on other source languages. They generated on the order of 50,000 synthetic languages by “mixing and matching” a few dozen source languages. Their idea was that with"
D18-1163,D16-1264,0,0.0105182,"ur work in this paper differs in that our synthetic treebanks are “made to order.” Rather than combine aspects of different treebanks and hope to get at least one combination that is close to the target language, we “combine” the source treebank with a POS corpus of the target language, which guides our customized permutation of the source. Beyond unsupervised parsing, synthetic data has been used for several other tasks. In NLP, it has been used for complex tasks such as questionanswering (QA) (Serban et al., 2016) and machine reading comprehension (Weston et al., 2016; Hermann et al., 2015; Rajpurkar et al., 2016), where highly expressive neural models are used and not enough real data is available to train them. In the playground of supervised parsing, Gulordava and Merlo (2016) conduct a controlled study on the parsibility of languages by generating treebanks with short dependency length and low variability of word order. 7 Conclusion & Future Work We have shown how cross-lingual transfer parsing can be improved by permuting the source treebank to better resemble the target language on the surface (in its distribution of gold POS bigrams). The code is available at https://github. com/wddabc/ordersynt"
D18-1163,P06-1072,1,0.667862,"Missing"
D18-1163,D15-1039,0,0.369226,"ting” the analysis from a source-language translation. Of course, this requires bilingual corpora as an additional resource. Annotation projection was proposed by Yarowsky et al. (2001), gained promising results on sequence labelling tasks, and was later developed for unsupervised parsing (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014; Ma and Xia, 2014; Tiedemann et al., 2014). Recent work in this vein has mainly focused on improving the synthetic data, including reweighting the training trees (Agi´c et al., 2016) or pruning those that cannot be aligned well (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016). On the other hand, Wang and Eisner (2016) proposed to permute source language treebanks using word order realization models trained on other source languages. They generated on the order of 50,000 synthetic languages by “mixing and matching” a few dozen source languages. Their idea was that with a large set of synthetic languages, they could use them as supervised examples to train an unsupervised structure discovery system that could analyze any new language. Systems built with this dataset were competitive in single-source parser transfer (Wang and Eisner, 2016"
D18-1163,P11-2120,0,0.593104,"l., 2015; Nivre et al., 2017). The idea is to parse the sentences of the target language with a supervised parser trained on the treebanks of one or more source languages. Although the parser cannot be expected to know the words of the target language, it can make do with parts of 1.2 This paper: Tailored synthetic data We will focus on delexicalized dependency parsing, which maps an input POS tag sequence to a dependency tree. We evaluate single-source transfer—train a parser on a single source language, and evaluate it on the target language. This is the setup of Zeman and Resnik (2008) and Søgaard (2011a). Our novel ingredient is that rather than seek a close source language that already exists, we create one. How? Given a dependency treebank of a possibly distant source language, we stochastically permute the children of each node, according to some distribution that makes the permuted language close to the target language. And how do we find this distribution? We adopt the tree-permutation model of Wang and Eisner (2016). We design a dynamic programming algorithm which, for any given distribution p in Wang and Eisner’s family, can compute the expected counts of all POS bigrams in the permu"
D18-1163,Q17-1020,0,0.671807,"Missing"
D18-1163,P15-2040,0,0.477798,"Missing"
D18-1163,D12-1063,0,0.0410006,"nging for decades (Mareˇcek, 2016). Classical grammar induction approaches (Lari and Young, 1990; Carroll and Charniak, 1992; Klein and Manning, 2004; Headden III et al., 2009; Naseem et al., 2010) estimate a generative grammar to explain the sentences, for example by the ExpectationMaximization (EM) algorithm, and then use it to parse. Some such approaches try to improve the grammar model. For example, Klein and Manning (2004)’s dependency model with valence was the first to beat a trivial baseline; later improvements considered higher-order effects and punctuation (Headden III et al., 2009; Spitkovsky et al., 2012). Other approaches try to avoid search error, using strategies like convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), informed initialization (Klein and Manning, 2004; Mareˇcek and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013). The alternative of cross-lingual transfer has recently flourished thanks to the development of consistent cross-lingual datasets of POS-tagged (Petrov et al., 2012) and dependency-parsed (McD"
D18-1163,D13-1204,0,0.0244964,"Klein and Manning (2004)’s dependency model with valence was the first to beat a trivial baseline; later improvements considered higher-order effects and punctuation (Headden III et al., 2009; Spitkovsky et al., 2012). Other approaches try to avoid search error, using strategies like convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), informed initialization (Klein and Manning, 2004; Mareˇcek and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013). The alternative of cross-lingual transfer has recently flourished thanks to the development of consistent cross-lingual datasets of POS-tagged (Petrov et al., 2012) and dependency-parsed (McDonald et al., 2013) sentences. McDonald et al. (2011) showed a significant improvement over grammar induction by simply using the delexicalized parser trained on other language(s). Subsequent improvements have come from re-weighting source languages (Søgaard, 2011b; Rosa and ˇ Zabokrtsk´ y, 2015a,b; Wang and Eisner, 2016), adapting the model to the target language using WALS (Dryer and Haspelmath, 2013)"
D18-1163,N13-1126,0,0.794923,"Missing"
D18-1163,D15-1213,0,0.506598,"ble in the lower left gives summary results; the number in each column header gives the number of points summarized. For each column, we boldface the better result between the “Synthetic” and “Original”, or both if they are not significantly different (paired permutation test, p &lt; 0.01). We also show the oracle permutation result in row “Oracle”. Synthetic Treebank: 50.36 90 80 70 60 50 40 30 cu el eu fa fi_ftb ga 20 10 10 20 30 40 50 60 he hr hu id ja_ktc la 70 Original Treebank: 48.61 pl ro sl sv ta 80 90 Figure 3: UAS on 337 language pairs from the training languages to the test languages. Zhang and Barzilay, 2015; Ammar et al., 2016), and improving the lexical representations via multilingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016) and synthetic data generation (§6.2). 6.2 Synthetic data generation Our novel proposal ties into the recent interest in data augmentation in supervised machine learning. In unsupervised parsing, the most widely adopted synthetic data method has been annotation projection, which generates synthetic analyses of target-language sentences by “projecting” the analysis from a source-language translation. Of course, this requires bilingual corpor"
D18-1163,C14-1175,0,0.183925,"ecent interest in data augmentation in supervised machine learning. In unsupervised parsing, the most widely adopted synthetic data method has been annotation projection, which generates synthetic analyses of target-language sentences by “projecting” the analysis from a source-language translation. Of course, this requires bilingual corpora as an additional resource. Annotation projection was proposed by Yarowsky et al. (2001), gained promising results on sequence labelling tasks, and was later developed for unsupervised parsing (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014; Ma and Xia, 2014; Tiedemann et al., 2014). Recent work in this vein has mainly focused on improving the synthetic data, including reweighting the training trees (Agi´c et al., 2016) or pruning those that cannot be aligned well (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016). On the other hand, Wang and Eisner (2016) proposed to permute source language treebanks using word order realization models trained on other source languages. They generated on the order of 50,000 synthetic languages by “mixing and matching” a few dozen source languages. Their idea was that with a large set of sy"
D18-1163,W14-1614,0,0.434195,"Missing"
D18-1163,Q16-1035,1,0.25999,"and Barzilay, 2015) or crosslingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016). A more serious challenge is that the parser may not know how to handle the word order of the target language, unless the source treebank comes from a closely related language (e.g., using German to parse Luxembourgish). Training the parser on trees from multiple source languages may mitigate this issue (McDonald et al., 2011) because the parser is more likely to have seen target part-of-speech sequences somewhere in the training data. Some authors ˇ (Rosa and Zabokrtsk´ y, 2015a,b; Wang and Eisner, 2016) have shown additional improvements by preferring source languages that are “close” to the target language, where the closeness is measured by distance between POS language models trained on the source and target corpora. To approximately parse an unfamiliar language, it helps to have a treebank of a similar language. But what if the closest available treebank still has the wrong word order? We show how to (stochastically) permute the constituents of an existing dependency treebank so that its surface part-of-speech statistics approximately match those of the target language. The parameters of"
D18-1163,Q17-1011,1,0.742036,"alizing some parameters too close to ±∞ for the optimizer to change them meaningfully. 1329 5.1 ar bg cs da de en es et fi fr 80 Data and setup 70 5.2 Exploratory analysis We have assumed that a smaller divergence between source and target treebanks results in better transfer parsing accuracy. Figure 1 shows that these quantities are indeed correlated, both for the original source treebanks and for their “made to order” permuted versions. 12 We have 19*20=380 pairs in total, minus the four excluded pairs (grc, grc proiel), (grc proiel, grc), (la proiel, la itt) and (la itt, la proiel). Unlike Wang and Eisner (2017), we exclude duplicated languages in development and testing. 13 Specifically, there are 3 duplicated sets: {grc, grc proiel}, {la, la proiel, la itt}, and {fi, fi ftb}. Whenever one treebank is used as the target language, we exclude the other treebanks in the same set. 15 According to the family (and sub-family) information at http://universaldependencies.org. 60 50 40 30 ar bg cs da de en es et fi fr 80 70 60 UAS UAS As our main dataset, we use Universal Dependencies version 1.2 (Nivre et al., 2015)—a set of 37 dependency treebanks for 33 languages, with a unified POS-tag set and relation l"
D18-1163,P08-1061,0,0.109798,"ak, 1992; Klein and Manning, 2004; Headden III et al., 2009; Naseem et al., 2010) estimate a generative grammar to explain the sentences, for example by the ExpectationMaximization (EM) algorithm, and then use it to parse. Some such approaches try to improve the grammar model. For example, Klein and Manning (2004)’s dependency model with valence was the first to beat a trivial baseline; later improvements considered higher-order effects and punctuation (Headden III et al., 2009; Spitkovsky et al., 2012). Other approaches try to avoid search error, using strategies like convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), informed initialization (Klein and Manning, 2004; Mareˇcek and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013). The alternative of cross-lingual transfer has recently flourished thanks to the development of consistent cross-lingual datasets of POS-tagged (Petrov et al., 2012) and dependency-parsed (McDonald et al., 2013) sentences. McDonald et al. (2011) showed a significant improvement over grammar induction"
D18-1163,H01-1035,0,0.217506,"a multilingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016) and synthetic data generation (§6.2). 6.2 Synthetic data generation Our novel proposal ties into the recent interest in data augmentation in supervised machine learning. In unsupervised parsing, the most widely adopted synthetic data method has been annotation projection, which generates synthetic analyses of target-language sentences by “projecting” the analysis from a source-language translation. Of course, this requires bilingual corpora as an additional resource. Annotation projection was proposed by Yarowsky et al. (2001), gained promising results on sequence labelling tasks, and was later developed for unsupervised parsing (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014; Ma and Xia, 2014; Tiedemann et al., 2014). Recent work in this vein has mainly focused on improving the synthetic data, including reweighting the training trees (Agi´c et al., 2016) or pruning those that cannot be aligned well (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016). On the other hand, Wang and Eisner (2016) proposed to permute source language treebanks using word order realization models train"
D18-1163,I08-3008,0,0.411066,"ald et al., 2013; Nivre et al., 2015; Nivre et al., 2017). The idea is to parse the sentences of the target language with a supervised parser trained on the treebanks of one or more source languages. Although the parser cannot be expected to know the words of the target language, it can make do with parts of 1.2 This paper: Tailored synthetic data We will focus on delexicalized dependency parsing, which maps an input POS tag sequence to a dependency tree. We evaluate single-source transfer—train a parser on a single source language, and evaluate it on the target language. This is the setup of Zeman and Resnik (2008) and Søgaard (2011a). Our novel ingredient is that rather than seek a close source language that already exists, we create one. How? Given a dependency treebank of a possibly distant source language, we stochastically permute the children of each node, according to some distribution that makes the permuted language close to the target language. And how do we find this distribution? We adopt the tree-permutation model of Wang and Eisner (2016). We design a dynamic programming algorithm which, for any given distribution p in Wang and Eisner’s family, can compute the expected counts of all POS bi"
D19-1276,K18-2005,0,0.0306752,"ces with length  30. For each sentence, x is obtained by running the standard pre-trained ELMo on the UD token sequence (although UD’s tokenization may not perfectly match that of ELMo’s training data), and y is the labeled UD dependency parse without any part-of-speech (POS) tags. Thus, our tags t are tuned to predict only the dependency relations in UD, and not the gold POS tags a also in UD. Pretrained Word Embeddings For English, we used the pre-trained English ELMo model from the AllenNLP library (Gardner et al., 2017). For the 2748 other 8 languages, we used the pre-trained models from Che et al. (2018). Recall that ELMo has two layers of bidirectional LSTM (layer 1 and 2) built upon a context-independent character CNN (layer 0). We use either layer 1 or 2 as the input (xi ) to our token encoder p✓ . Layer 0 is the input ( xˆi ) to our type encoder s⇠ . Each encoder network (§§3.2– 3.3) has a single hidden layer with a tanh transfer function, which has 2d hidden units (typically 128 or 512) for continuous encodings and 512 hidden units for discrete encodings. Optimization We optimize with Adam (Kingma and Ba, 2014), a variant of stochastic gradient descent. We alternate between improving the"
D19-1276,P81-1022,0,0.704599,"Missing"
D19-1276,P03-1054,0,0.06139,"tion outperforms all three baselines in 8 of 9 languages, and is not significantly worse in the 9th language (Hindi). In short, our VIB joint training generalizes better to test data. This is because the training objective (2) includes terms that focus on the parsing task and also regularize the representations. In the discrete case, the VIB representation outperforms gold POS tags (at the same level of granularity) in 6 of 9 languages, and of the other 3, it is not significantly worse in 2. This suggests that our learned discrete tag set could be an improved alternative to gold POS tags (cf. Klein and Manning, 2003) when a discrete tag set is needed for speed. 8 Related Work Much recent NLP literature examines syntactic information encoded by deep models (Linzen et al., 2016) and more specifically, by powerful unsupervised word embeddings. Hewitt and Manning (2019) learn a linear projection from the embedding space to predict the distance between two words in a parse tree. Peters et al. (2018b) and Goldberg (2019) assess the ability of BERT and ELMo directly on syntactic NLP tasks. Tenney et al. (2019) extract information from the contextual embeddings by self-attention pooling within a span of word embe"
D19-1276,D07-1015,0,0.0347305,"Missing"
D19-1276,Q16-1037,0,0.0328628,"er to test data. This is because the training objective (2) includes terms that focus on the parsing task and also regularize the representations. In the discrete case, the VIB representation outperforms gold POS tags (at the same level of granularity) in 6 of 9 languages, and of the other 3, it is not significantly worse in 2. This suggests that our learned discrete tag set could be an improved alternative to gold POS tags (cf. Klein and Manning, 2003) when a discrete tag set is needed for speed. 8 Related Work Much recent NLP literature examines syntactic information encoded by deep models (Linzen et al., 2016) and more specifically, by powerful unsupervised word embeddings. Hewitt and Manning (2019) learn a linear projection from the embedding space to predict the distance between two words in a parse tree. Peters et al. (2018b) and Goldberg (2019) assess the ability of BERT and ELMo directly on syntactic NLP tasks. Tenney et al. (2019) extract information from the contextual embeddings by self-attention pooling within a span of word embeddings. The IB framework was first used in NLP to cluster distributionally similar words (Pereira et al., 1993). In cognitive science, it has been used to argue th"
D19-1276,W07-2216,0,0.043293,"ture We use the deep biaffine dependency parser (Dozat and Manning, 2016) as our variational distribution q (y |t), which functions as the decoder. This parser uses a Bi-LSTM to extract features from compressed tags or vectors and assign scores to each tree edge, setting q (y |t) proportional to the exp of the total score of all edges in y. During IB training, the code5 computes only an approximation to q (y|t) for the gold tree y (although in principle, it could have computed the exact normalizing constant in polytime with Tutte’s matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007)). When we test the parser, the code does exactly find argmaxy q (y |t) via the directed spanning tree algorithm of Edmonds (1966). 4 Training and Inference With the approximations in §3, our final minimization objective is this upper bound on (2): E x,y h E [ log q (y|t)] + KL(p✓ (t|x)||r (t)) t⇠p✓ (t |x) n X + i=1 KL(p✓ (ti |x) ||s⇠ (ti |xˆi )) i (3) We apply stochastic gradient descent to optimize this objective. To get a stochastic estimate of the objective, we first sample some (x, y) from the treebank. We then have many expectations over t ⇠ p✓ (t |x), including the KL terms. We could es"
D19-1276,P93-1024,0,0.869073,"amines syntactic information encoded by deep models (Linzen et al., 2016) and more specifically, by powerful unsupervised word embeddings. Hewitt and Manning (2019) learn a linear projection from the embedding space to predict the distance between two words in a parse tree. Peters et al. (2018b) and Goldberg (2019) assess the ability of BERT and ELMo directly on syntactic NLP tasks. Tenney et al. (2019) extract information from the contextual embeddings by self-attention pooling within a span of word embeddings. The IB framework was first used in NLP to cluster distributionally similar words (Pereira et al., 1993). In cognitive science, it has been used to argue that color-naming systems across languages are nearly optimal (Zaslavsky et al., 2018). In machine learning, IB provides an information-theoretic perspective to explain the performance of deep neural networks (Tishby and Zaslavsky, 2015). The VIB method makes use of variational upper and lower bounds on mutual information. An alternative lower bound was proposed by Poole et al. (2019), who found it to work better empirically. 9 Conclusion and Future Work In this paper, we have proposed two ways to syntactically compress ELMo word token embeddin"
D19-1276,N18-1202,0,0.222873,"f the information bottleneck, with bottleneck variable T. A jagged arrow indicates a stochastic mapping, i.e. the jagged arrow points from the parameters of a distribution to a sample drawn from that distribution. Introduction Word embedding systems like BERT and ELMo use spelling and context to obtain contextual embeddings of word tokens. These systems are trained on large corpora in a task-independent way. The resulting embeddings have proved to then be useful for both syntactic and semantic tasks, with different layers of ELMo or BERT being somewhat specialized to different kinds of tasks (Peters et al., 2018b; Goldberg, 2019). State-of-the-art performance on many NLP tasks can be obtained by fine-tuning, i.e., back-propagating task loss all the way back into the embedding function (Peters et al., 2018a; Devlin et al., 2018). In this paper, we explore what task-specific information appears in the embeddings before finetuning takes place. We focus on the task of dependency parsing, but our method can be easily extended to other syntactic or semantic tasks. Our method compresses the embeddings by extracting just their syntactic properties—specifically, the information needed to reconstruct parse tre"
D19-1276,D18-1179,0,0.158156,"f the information bottleneck, with bottleneck variable T. A jagged arrow indicates a stochastic mapping, i.e. the jagged arrow points from the parameters of a distribution to a sample drawn from that distribution. Introduction Word embedding systems like BERT and ELMo use spelling and context to obtain contextual embeddings of word tokens. These systems are trained on large corpora in a task-independent way. The resulting embeddings have proved to then be useful for both syntactic and semantic tasks, with different layers of ELMo or BERT being somewhat specialized to different kinds of tasks (Peters et al., 2018b; Goldberg, 2019). State-of-the-art performance on many NLP tasks can be obtained by fine-tuning, i.e., back-propagating task loss all the way back into the embedding function (Peters et al., 2018a; Devlin et al., 2018). In this paper, we explore what task-specific information appears in the embeddings before finetuning takes place. We focus on the task of dependency parsing, but our method can be easily extended to other syntactic or semantic tasks. Our method compresses the embeddings by extracting just their syntactic properties—specifically, the information needed to reconstruct parse tre"
D19-1276,D07-1014,0,0.0343334,"s treebank parse y. 2747 Decoder Architecture We use the deep biaffine dependency parser (Dozat and Manning, 2016) as our variational distribution q (y |t), which functions as the decoder. This parser uses a Bi-LSTM to extract features from compressed tags or vectors and assign scores to each tree edge, setting q (y |t) proportional to the exp of the total score of all edges in y. During IB training, the code5 computes only an approximation to q (y|t) for the gold tree y (although in principle, it could have computed the exact normalizing constant in polytime with Tutte’s matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007)). When we test the parser, the code does exactly find argmaxy q (y |t) via the directed spanning tree algorithm of Edmonds (1966). 4 Training and Inference With the approximations in §3, our final minimization objective is this upper bound on (2): E x,y h E [ log q (y|t)] + KL(p✓ (t|x)||r (t)) t⇠p✓ (t |x) n X + i=1 KL(p✓ (ti |x) ||s⇠ (ti |xˆi )) i (3) We apply stochastic gradient descent to optimize this objective. To get a stochastic estimate of the objective, we first sample some (x, y) from the treebank. We then have many expectations over t ⇠ p"
D19-1679,Q17-1010,0,0.0557769,"in this paper, we simply took = 1. Spelling-Aware Extension ˜ · w+ E ˜ X n ˜ n ·w E ˜n 1 1· w ˜n (6) ˜ is the full-word embedding matrix and w where E ˜ is a one-hot vector associated with the word type ˜ n is a character n-gram embedding matrix w, E and w ˜ n is a multi-hot vector associated with all the character n-grams for the word type w. For each n, the summand gives the average embedding of all n-grams in w (where 1· w ˜ n counts these n-grams). We set n to range from 3 to 4 (see Appendix B). This formulation is similar to previous sub-word based embedding models (Wieting et al., 2016; Bojanowski et al., 2017). Similarly, the embedding of an L2 word w is parameterized as ˜ · w+ F ˜ X n ˜ n ·w F ˜n 1 1· w ˜n (7) ˜ n to µE ˜ n (where Crucially, we initialize F µ &gt; 0) so that L2 words can inherit part of their initial embedding from similarly spelled L1 words: ˜ 4 Afri := µE ˜ 4 Afri .2 But we allow F ˜ n to diverge F over time in case an n-gram functions differently in the two languages. In the same way, we initialize ˜ to the corresponding row of µ · E, ˜ each row of F if any, and otherwise to 0. Our experiments set µ = 0.2 (see Appendix B). We refer to this spelling-aware extension to GSM as sGSM."
D19-1679,P82-1020,0,0.747387,"Missing"
D19-1679,K16-1013,1,0.852077,"such as word order and fixed phrases, in this paper we only consider simple lexical substitutions. Our hope is that such a system can augment traditional foreign language instruction. As an example, consider a native speaker of English (learning German) presented with the following sentence: Der Nile is a Fluss in Afrika. With a little effort, one would hope the student could infer the meaning of the German words because there is sufficient contextual information and spelling information for the cognate Afrika. In our previous papers on foreign language teaching (Renduchintala et al., 2016b; Knowles et al., 2016; Renduchintala et al., 2017), we focused on fitting detailed models of students’ learning when the instructional stimuli (macaronic or otherwise) were chosen by a simple random or heuristic teaching policy. In the present paper, we flip the emphasis to choosing good instructional stimuli—machine teaching. This still requires a model of student learning. We employ a reasonable model that is not trained on any human students at all, but only on text that a generic student is presumed to have read. Thus, our model is not personalized, although it may be specialized to the domain of L1 text that"
D19-1679,P14-1053,0,0.0284583,"a recent workshop (Renduchintala et al., 2019): it experimented with three variants of the generic student model, using an artificial L2 language. In this paper, we extend the best of those models to consider an L2 word’s spelling (along with its context) when guessing its embeddings. We therefore conduct our experiments on real L2 languages (Spanish and German). 2 Related Work Our motivation is similar to that of commercially available prior systems such as Swych (2015) and OneThirdStories (2018) that also incorporate incidental learning within foreign language instruction. Other prior work (Labutov and Lipson, 2014; Renduchintala et al., 2016b) relied on building a model of the student’s incidental learning capabilities, using supervised data that was painfully collected by asking students to react to the actions of an initially untrained machine teacher. Our method, by contrast, constructs a generic student model from unannotated L1 text alone. This makes it possible for us to quickly create macaronic documents in any domain covered by that text corpus. 3 Method Our machine teacher can be viewed as a search algorithm that tries to find the (approximately) best macaronic configuration for the next sente"
D19-1679,L18-1008,0,0.0397456,"Missing"
D19-1679,E17-1096,0,0.0160009,"andidate macaronic configuration: Der Nile ist ein Fluss in Africa.1 Understanding may arise from inference on this sentence as well as whatever the student has learned about these words from previous sentences. The teacher makes this assessment by presenting this sentence to a generic student model (§§3.1–3.3). It uses a L2 embedding scoring scheme (§3.4) to guide a greedy search for the best macaronic configuration (§3.5). 3.1 Generic Student Model Our model of a “generic student” (GSM) is equipped with a cloze language model that uses a bidirectional LSTM to predict L1 words in L1 context (Mousa and Schuller, 2017; Hochreiter and Schmidhuber, 1997). Given a sentence x = [x1 ,...,xt ,...,xT ], the cloze model defines p(xt |hf t ,hb t ) 8t 2 {1,...,T }, where: hf t = LSTMf ([x1 ,...,xt 1 ];✓ f ) 2 RD (1) hb t = LSTMb ([xT ,...,xt+1 ];✓ b ) 2 RD (2) p(· |hf ,hb ) = softmax(E h([hf ;hb ]; ✓ h )) (3) are hidden states of forward and backward LSTM encoders parameterized by ✓ f and ✓ b respectively. The model assumes a fixed L1 vocabulary of size V , and the vectors xt above are embeddings of these word types, which correspond to the rows of an embedding matrix E 2 RV ⇥D . The cloze distribution at each posit"
D19-1679,E17-2025,0,0.0333356,".,xt+1 ];✓ b ) 2 RD (2) p(· |hf ,hb ) = softmax(E h([hf ;hb ]; ✓ h )) (3) are hidden states of forward and backward LSTM encoders parameterized by ✓ f and ✓ b respectively. The model assumes a fixed L1 vocabulary of size V , and the vectors xt above are embeddings of these word types, which correspond to the rows of an embedding matrix E 2 RV ⇥D . The cloze distribution at each position t in the sentence is obtained using where h(·;✓ h ) is a projection function that reduces the dimension of the concatenated hidden states from 2D to D. We “tie” the input embeddings and output embeddings as in Press and Wolf (2017). We train the parameters ✓ = [✓ f ; ✓ b ; ✓ h ; E] using P Adam (Kingma and Ba, 2014) to maximize x L(x), where the summation is over sentences x in a large L1 training corpus, and X L(x) = log p(xt |hf t ,hb t ) (4) t We set the dimensionality of word embeddings and LSTM hidden units to 300. We use the WikiText-103 corpus (Merity et al., 2016) as the L1 training corpus. We apply dropout (p = 0.2) between the word embeddings and LSTM layers, and between the LSTM and projection layers (Srivastava et al., 2014). We assume that the resulting model represents the entirety of the student’s L1 know"
D19-1679,P16-4023,1,0.909362,"rrounding context and spelling (Krashen, 1989). An initial “rough” understanding of a novel word might suffice for the reader to continue reading, with subsequent exposures refining their understanding of the novel word. Our goal is to design a machine teacher that uses a human reader’s incidental learning ability to teach foreign language (L2) vocabulary. The machine teacher’s modus operandi is to replace L1 words with their L2 glosses, which results in a macaronic document that mixes two languages in an effort to ease the human reader into understanding the L2. While some of our prior work (Renduchintala et al., 2016b,a) considered incorporating other features of the L2 such as word order and fixed phrases, in this paper we only consider simple lexical substitutions. Our hope is that such a system can augment traditional foreign language instruction. As an example, consider a native speaker of English (learning German) presented with the following sentence: Der Nile is a Fluss in Afrika. With a little effort, one would hope the student could infer the meaning of the German words because there is sufficient contextual information and spelling information for the cognate Afrika. In our previous papers on fo"
D19-1679,P16-1175,1,0.874373,"rrounding context and spelling (Krashen, 1989). An initial “rough” understanding of a novel word might suffice for the reader to continue reading, with subsequent exposures refining their understanding of the novel word. Our goal is to design a machine teacher that uses a human reader’s incidental learning ability to teach foreign language (L2) vocabulary. The machine teacher’s modus operandi is to replace L1 words with their L2 glosses, which results in a macaronic document that mixes two languages in an effort to ease the human reader into understanding the L2. While some of our prior work (Renduchintala et al., 2016b,a) considered incorporating other features of the L2 such as word order and fixed phrases, in this paper we only consider simple lexical substitutions. Our hope is that such a system can augment traditional foreign language instruction. As an example, consider a native speaker of English (learning German) presented with the following sentence: Der Nile is a Fluss in Afrika. With a little effort, one would hope the student could infer the meaning of the German words because there is sufficient contextual information and spelling information for the cognate Afrika. In our previous papers on fo"
D19-1679,K17-1025,1,0.862213,"d fixed phrases, in this paper we only consider simple lexical substitutions. Our hope is that such a system can augment traditional foreign language instruction. As an example, consider a native speaker of English (learning German) presented with the following sentence: Der Nile is a Fluss in Afrika. With a little effort, one would hope the student could infer the meaning of the German words because there is sufficient contextual information and spelling information for the cognate Afrika. In our previous papers on foreign language teaching (Renduchintala et al., 2016b; Knowles et al., 2016; Renduchintala et al., 2017), we focused on fitting detailed models of students’ learning when the instructional stimuli (macaronic or otherwise) were chosen by a simple random or heuristic teaching policy. In the present paper, we flip the emphasis to choosing good instructional stimuli—machine teaching. This still requires a model of student learning. We employ a reasonable model that is not trained on any human students at all, but only on text that a generic student is presumed to have read. Thus, our model is not personalized, although it may be specialized to the domain of L1 text that it was initially trained on."
D19-1679,W19-4439,1,0.537773,"Nile Nile Nil ist is ist a a ein river Fluss river in in in Africa Africa Africa Table 1: An example English (L1) sentence with German (L2) glosses. Using the glosses, many possible macaronic configurations are possible. Note that the gloss sequence is not a fluent L2 sentence. macaronic sentences shown to the student. Our teacher does not yet attempt to monitor the human student’s actual learning. Still, we show that it is useful to a beginner student and far less frustrating than a random (or heuristic based) alternative. A “pilot” version of the present paper appeared at a recent workshop (Renduchintala et al., 2019): it experimented with three variants of the generic student model, using an artificial L2 language. In this paper, we extend the best of those models to consider an L2 word’s spelling (along with its context) when guessing its embeddings. We therefore conduct our experiments on real L2 languages (Spanish and German). 2 Related Work Our motivation is similar to that of commercially available prior systems such as Swych (2015) and OneThirdStories (2018) that also incorporate incidental learning within foreign language instruction. Other prior work (Labutov and Lipson, 2014; Renduchintala et al."
D19-1679,P16-1174,0,0.0470856,"Missing"
D19-1679,D16-1157,0,0.0274375,"riments. In practice, in this paper, we simply took = 1. Spelling-Aware Extension ˜ · w+ E ˜ X n ˜ n ·w E ˜n 1 1· w ˜n (6) ˜ is the full-word embedding matrix and w where E ˜ is a one-hot vector associated with the word type ˜ n is a character n-gram embedding matrix w, E and w ˜ n is a multi-hot vector associated with all the character n-grams for the word type w. For each n, the summand gives the average embedding of all n-grams in w (where 1· w ˜ n counts these n-grams). We set n to range from 3 to 4 (see Appendix B). This formulation is similar to previous sub-word based embedding models (Wieting et al., 2016; Bojanowski et al., 2017). Similarly, the embedding of an L2 word w is parameterized as ˜ · w+ F ˜ X n ˜ n ·w F ˜n 1 1· w ˜n (7) ˜ n to µE ˜ n (where Crucially, we initialize F µ &gt; 0) so that L2 words can inherit part of their initial embedding from similarly spelled L1 words: ˜ 4 Afri := µE ˜ 4 Afri .2 But we allow F ˜ n to diverge F over time in case an n-gram functions differently in the two languages. In the same way, we initialize ˜ to the corresponding row of µ · E, ˜ each row of F if any, and otherwise to 0. Our experiments set µ = 0.2 (see Appendix B). We refer to this spelling-aware"
E17-2028,P15-5005,0,0.0361252,"Missing"
E17-2028,D14-1165,0,0.0106915,"o a baseline that simply embeds lemmas rather than words (equivalent to fixing rk = 1). 6 Related Work Tensor factorization has already found uses in a few corners of NLP research. Van de Cruys et al. (2013) applied tensor factorization to model the compositionality of subject-verb-object triples. Similarly, Hashimoto and Tsuruoka (2015) use an implicit tensor factorization method to learn embeddings for transitive verb phrases. Tensor factorization also appears in semantic-based NLP tasks. Lei et al. (2015) explicitly factorize a tensor based on feature vectors for predicting semantic roles. Chang et al. (2014) use tensor factorization to create knowledge base embeddings optimized for relation extraction. See Bouchard et al. (2015) for a large bibliography. Other researchers have likewise attempted to escape the bag-of-words assumption in word embeddings, e.g., Yatbaz et al. (2012) incorporates morphological and orthographic features into continuous vectors; Cotterell and Sch¨utze (2015) consider a multi-task set-up to force morphological information into embeddings; Cotterell and Sch¨utze (2017) jointly morphologically segment and embed words; Levy and Goldberg (2014a) derive contexts based on depe"
E17-2028,N15-1140,1,0.893374,"Missing"
E17-2028,P16-1156,1,0.0989234,"Missing"
E17-2028,W16-2506,0,0.00454851,"33 28.54 +7.21 de 353 S IM L 44.90 28.39 50.39 68.08 40.09 53.97 +23.18 +11.7 +3.58 RG -65 Z222 29.75 31.11 +1.36 RG -65 353 70.60 64.50 71.71 63.72 +1.11 -0.78 en MEN MTURK 64.33 58.77 66.66 62.64 +2.33 +3.87 S IM L 41.62 49.70 +8.08 S IM V 30.48 29.96 +0.52 RW 40.78 42.40 +1.62 Table 2: Word similarity results comparing the compositional morphology tensor with the standard skip-gram model. Numbers indicate Spearman’s correlation coefficient ρ between human similarity judgements and the cosine distances of vectors. For each language, we compare on several sets of human judgments as listed by Faruqui et al. (2016, Table 2). tribute signal to the embedding of run. We expect these lemma embeddings to be predictive of human judgments of lemma similarity. We evaluate using standard datasets on four languages (French, Italian, German and English). Given a list of pairs of words (always lemmata), multiple native speakers judged (on a scale of 1– 10) how “similar” those words are conceptually. Our model produces a similarity judgment for each pair using the cosine similarity of their lemma embeddings wj . Table 2 shows how well this learned judgment correlates with the average human judgment. Our model does"
E17-2028,N13-1092,1,0.120323,"Missing"
E17-2028,W15-4001,0,0.0184305,"oduces a similarity judgment for each pair using the cosine similarity of their lemma embeddings wj . Table 2 shows how well this learned judgment correlates with the average human judgment. Our model does achieve higher correlation than skip-gram word embeddings. Note we did not compare to a baseline that simply embeds lemmas rather than words (equivalent to fixing rk = 1). 6 Related Work Tensor factorization has already found uses in a few corners of NLP research. Van de Cruys et al. (2013) applied tensor factorization to model the compositionality of subject-verb-object triples. Similarly, Hashimoto and Tsuruoka (2015) use an implicit tensor factorization method to learn embeddings for transitive verb phrases. Tensor factorization also appears in semantic-based NLP tasks. Lei et al. (2015) explicitly factorize a tensor based on feature vectors for predicting semantic roles. Chang et al. (2014) use tensor factorization to create knowledge base embeddings optimized for relation extraction. See Bouchard et al. (2015) for a large bibliography. Other researchers have likewise attempted to escape the bag-of-words assumption in word embeddings, e.g., Yatbaz et al. (2012) incorporates morphological and orthographic"
E17-2028,C92-2082,0,0.0789371,"onsider a multi-task set-up to force morphological information into embeddings; Cotterell and Sch¨utze (2017) jointly morphologically segment and embed words; Levy and Goldberg (2014a) derive contexts based on dependency relations; PPDB (Ganitkevitch et al., 2013) employs a mixed bag of words, parts of speech, and syntax; Rastogi et al. (2015) represent word contexts, morphology, semantic frame relations, syntactic dependency relations, and multilingual bitext counts each as separate matrices, combined via GCCA; and, finally, Schwartz et al. (2016) derived embeddings based on Hearst patterns (Hearst, 1992). Ling et al. (2015) learn position-specific word embeddings (§4.1), but do not factor them as wj rk to share parameters (we did not compare empirically to this). As demonstrated in the experiments, our tensor factorization method enables us to include other syntactic properties besides word order, e.g. morphology. Poliak et al. (2017) also create positional word embeddings. Our research direction is orthogonal to these efforts in that we provide a general purpose procedure for all sorts of higher-order coocurrence. 7 Conclusion We have presented an interpretation of the skipgram model as expo"
E17-2028,N15-1121,0,0.0411841,"Missing"
E17-2028,P14-2050,0,0.0916527,"is passed through some “inverse link” function to obtain the expected feature values under the distribution, which in turn determines j = i XX j i exp (ci · wj ) Xij log P i0 exp (ci0 · wj ) (4) This is the log-likelihood (plus a constant) if we assume that for each word j, the context vector xj was drawn from a multinomial with natural parameter vector C > wj and count parameter P Nj = i Xij . This is the same model as in Figure 1a, but with a different conditional distribution for xj , and with xj taking an additional observed parent Nj (which is the token count of word j). 2.1 Related work Levy and Goldberg (2014b) also interpreted skipgram as matrix factorization. They argued that skipgram estimation by negative sampling implicitly factorizes a shifted matrix of positive empirical pointwise mutual information values. We instead regard the skip-gram objective itself as demanding EPCA-style factorization of the count matrix X: i.e., X arose stochastically from some unknown matrix of log-linear parameters (column j of X generated from parameter column j), and we seek a rank-d estimate C > W of that matrix. pLSI (Hofmann, 1999) similarly factors an unknown matrix of multinomial probabilities, which is mu"
E17-2028,Q15-1016,0,0.0144649,"l case—it is equal to (wj ; 1) (1; rk ), which uses twice as many dimensions to embed each object. 5 Experiments We build HOSG on top of the HYPERWORDS package. All models (both skip-gram and higher-order skip-gram) are trained for 10 epochs and use 5 negative samples. All models for §5.1 are trained on the Sept. 2016 dump of the full Wikipedia. All models for §5.2 were trained on the lemmatized and POS-tagged WaCky corpora (Baroni et al., 2009) for French, Italian, German and English (Joubarne and Inkpen, 2011; Leviant and Reichart, 2015). To ensure controlled and fair experiments, we follow Levy et al. (2015) for all preprocessing. 5.1 Experiment 1: Positional Tensor We postulate that the positional tensor should encode richer notions of syntax than standard bag6 If one wanted to extend the model to decompose the context words i as well, we see at least four approaches. 7 Cotterell et al. (2016) made two further moves that could be applied to extend the present paper. First, they allowed a word to consist of any number of (unordered) morphemes— not necessarily two—whose embeddings were combined (by summation) to get the word embedding. Second, this sum also included word-specific random noise, all"
E17-2028,N15-1142,0,0.0122328,"-task set-up to force morphological information into embeddings; Cotterell and Sch¨utze (2017) jointly morphologically segment and embed words; Levy and Goldberg (2014a) derive contexts based on dependency relations; PPDB (Ganitkevitch et al., 2013) employs a mixed bag of words, parts of speech, and syntax; Rastogi et al. (2015) represent word contexts, morphology, semantic frame relations, syntactic dependency relations, and multilingual bitext counts each as separate matrices, combined via GCCA; and, finally, Schwartz et al. (2016) derived embeddings based on Hearst patterns (Hearst, 1992). Ling et al. (2015) learn position-specific word embeddings (§4.1), but do not factor them as wj rk to share parameters (we did not compare empirically to this). As demonstrated in the experiments, our tensor factorization method enables us to include other syntactic properties besides word order, e.g. morphology. Poliak et al. (2017) also create positional word embeddings. Our research direction is orthogonal to these efforts in that we provide a general purpose procedure for all sorts of higher-order coocurrence. 7 Conclusion We have presented an interpretation of the skipgram model as exponential family princ"
E17-2028,D14-1162,0,0.0855584,"syntax) or morphological information (to share parameters across related words). We experiment on 40 languages and show that our model improves upon skip-gram. 1 2 Introduction Over the past years NLP has witnessed a veritable frenzy on the topic of word embeddings: lowdimensional representations of distributional information. The embeddings, trained on extremely large text corpora such as Wikipedia and Common Crawl, are claimed to encode semantic knowledge extracted from large text corpora. Numerous methods have been proposed—the most popular being skip-gram (Mikolov et al., 2013) and GloVe (Pennington et al., 2014)—for learning these low-dimensional embeddings from a bag of contexts associated with each word type. Natural language text, however, contains richer structure than simple context-word pairs. In this work, we embed n-tuples rather than pairs, allowing us to escape the bag-of-words assumption and encode richer linguistic structures. As a first step, we offer a novel interpretation of the skip-gram model (Mikolov et al., 2013). We show how skip-gram can be viewed as an application of exponential-family principal components analysis (EPCA) (Collins et al., 2001) to an integer matrix of coocurrenc"
E17-2028,petrov-etal-2012-universal,0,0.0290821,"Missing"
E17-2028,E17-2081,1,0.885781,"Missing"
E17-2028,N15-1058,1,0.772914,"Missing"
E17-2028,N16-1060,0,0.0202674,"Missing"
E17-2028,W16-2520,0,0.0416803,"Missing"
E17-2028,N13-1134,0,0.0390591,"Missing"
E17-2028,D12-1086,0,0.0694959,"Missing"
E17-2028,L16-1262,0,\N,Missing
H05-1036,J98-2004,0,0.0122284,"-cycles in FSMs (Eisner, 2002). 13 One can declare the conditions under which items of a particular type (constit or goal) should be treated as having converged. Then asking for the value of goal will run the agenda algorithm not until the agenda is empty, but only until chart[goal] has converged by this criterion. 4.5 Prioritization The order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed. We implement the agenda as a priority queue whose priority function may be specified by the user.15 Charniak et al. (1998) and Caraballo and Charniak (1998) showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective. Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. For A* in our general framework, the priority of item a should be an estimate of the value of the best proof of goal that uses a. (This non-standard formulation is carefully chosen.16 ) If so, goal is guaranteed to converge the very first time it is selected from the priority-queue agenda. Prioritizing “good” items first can also be useful in other circumstances. The inside-outside tra"
H05-1036,W98-1115,0,0.0186678,"cycles (Stolcke, 1995), or -cycles in FSMs (Eisner, 2002). 13 One can declare the conditions under which items of a particular type (constit or goal) should be treated as having converged. Then asking for the value of goal will run the agenda algorithm not until the agenda is empty, but only until chart[goal] has converged by this criterion. 4.5 Prioritization The order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed. We implement the agenda as a priority queue whose priority function may be specified by the user.15 Charniak et al. (1998) and Caraballo and Charniak (1998) showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective. Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. For A* in our general framework, the priority of item a should be an estimate of the value of the best proof of goal that uses a. (This non-standard formulation is carefully chosen.16 ) If so, goal is guaranteed to converge the very first time it is selected from the priority-queue agenda. Prioritizing “good” items first can also be useful in other cir"
H05-1036,P81-1022,0,0.662932,"Missing"
H05-1036,W05-1504,1,0.258721,"the introduction of declarations that control which items use the agenda or are memoized in the chart. This can be used to support lazy or “on-the-fly” computation (Mohri et al., 1998) and asymptotic space-saving tricks (Binder et al., 1997). 7 7.1 Usefulness of the Implementation Applications The current Dyna compiler has proved indispensable in our own recent projects, in the sense that we would not have attempted many of them without it. In some cases, we were experimenting with genuinely new algorithms not supported by any existing tool, as in our work on dependency-lengthlimited parsing (Eisner and Smith, 2005b) and loosely syntax-based machine translation (Eisner and D. Smith, 2005). (Dyna would have been equally helpful in the first author’s earlier work on new algorithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu,"
H05-1036,P02-1001,1,0.76106,"ther NP-hard constraint satisfaction problems by using cyclic rules with negation over finitely many boolean-valued items (Niemel¨a, 1998). Here the agenda algorithm can end up flipping values forever between false and true; a more general solver would have to be called in order to find a stable model of a SAT problem’s equations. 14 Still assuming the number of items is finite, one could in principle materialize the system of equations and call a dedicated numerical solver. In some special cases only a linear solver is needed: e.g., for unary rule cycles (Stolcke, 1995), or -cycles in FSMs (Eisner, 2002). 13 One can declare the conditions under which items of a particular type (constit or goal) should be treated as having converged. Then asking for the value of goal will run the agenda algorithm not until the agenda is empty, but only until chart[goal] has converged by this criterion. 4.5 Prioritization The order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed. We implement the agenda as a priority queue whose priority function may be specified by the user.15 Charniak et al. (1998) and Caraballo and Charniak (1998) sh"
H05-1036,J99-4004,0,0.748874,"ductive algorithms. Our implemented system encapsulates these implementation techniques behind a clean interface—a small high-level specification language, Dyna, which compiles into C++ classes. This system should help the HLT community to experiment more easily with new models and algorithms. 1.1 Dynamic programming as deduction The “parsing as deduction” framework (Pereira and Warren, 1983) is now over 20 years old. It provides an elegant notation for specifying a variety of parsing algorithms (Shieber et al., 1995), including algorithms for probabilistic or other semiring-weighted parsing (Goodman, 1999). In the parsing community, new algorithms are often stated simply as a set of deductive inference rules (Sikkel, 1997; Eisner and Satta, 1999). It is also straightforward to specify other NLP algorithms this way. Syntactic MT models, language models, and stack decoders can be easily described using deductive rules. So can operations on finitestate and infinite-state machines. ∗ We thank Joshua Goodman, David McAllester, and Paul Ruhlen for useful early discussions; pioneer users Markus Dreyer, David Smith, and Roy Tromble for their feedback and input; John Blatz for discussion of program tran"
H05-1036,P02-1017,0,0.0116196,"orithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu, 1997; Melamed, 2003), lexicalized parsers (Eisner and Satta, 1999),22 partially supervised training a` la (Pereira and Schabes, 1992),23 and grammar induction (Klein and Manning, 2002). These replications were easy to write and extend, and to train via §5.2. 7.2 Experiments We compared the current Dyna compiler to handbuilt systems on a variety of parsing tasks. These problems were chosen not for their novelty or interesting structure, but for the availability of existing well-tuned implementations. Best parse. We compared a Dyna CFG parser to the Java parser of Klein and Manning (2003b),24 22 Markus Dreyer’s reimplementation of the complex Collins (1999) parser uses under 30 lines of Dyna. 23 For example, lines 2–3 of Fig. 1 can be extended with whenever permitted(X,I,K)."
H05-1036,N03-1016,0,0.0597224,"ed as having converged. Then asking for the value of goal will run the agenda algorithm not until the agenda is empty, but only until chart[goal] has converged by this criterion. 4.5 Prioritization The order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed. We implement the agenda as a priority queue whose priority function may be specified by the user.15 Charniak et al. (1998) and Caraballo and Charniak (1998) showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective. Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. For A* in our general framework, the priority of item a should be an estimate of the value of the best proof of goal that uses a. (This non-standard formulation is carefully chosen.16 ) If so, goal is guaranteed to converge the very first time it is selected from the priority-queue agenda. Prioritizing “good” items first can also be useful in other circumstances. The inside-outside training algorithm requires one to find all parses, but finding the high-probability parses first allows one to ignore the rest by “early"
H05-1036,P03-1054,0,0.136066,"ed as having converged. Then asking for the value of goal will run the agenda algorithm not until the agenda is empty, but only until chart[goal] has converged by this criterion. 4.5 Prioritization The order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed. We implement the agenda as a priority queue whose priority function may be specified by the user.15 Charniak et al. (1998) and Caraballo and Charniak (1998) showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective. Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. For A* in our general framework, the priority of item a should be an estimate of the value of the best proof of goal that uses a. (This non-standard formulation is carefully chosen.16 ) If so, goal is guaranteed to converge the very first time it is selected from the priority-queue agenda. Prioritizing “good” items first can also be useful in other circumstances. The inside-outside training algorithm requires one to find all parses, but finding the high-probability parses first allows one to ignore the rest by “early"
H05-1036,N03-1021,0,0.0103129,"loosely syntax-based machine translation (Eisner and D. Smith, 2005). (Dyna would have been equally helpful in the first author’s earlier work on new algorithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu, 1997; Melamed, 2003), lexicalized parsers (Eisner and Satta, 1999),22 partially supervised training a` la (Pereira and Schabes, 1992),23 and grammar induction (Klein and Manning, 2002). These replications were easy to write and extend, and to train via §5.2. 7.2 Experiments We compared the current Dyna compiler to handbuilt systems on a variety of parsing tasks. These problems were chosen not for their novelty or interesting structure, but for the availability of existing well-tuned implementations. Best parse. We compared a Dyna CFG parser to the Java parser of Klein and Manning (2003b),24 22 Markus Dreyer’s rei"
H05-1036,P96-1033,0,0.170489,"mputing goal is equivalent to an aggregation over many separate parse trees. That is not the case for heterogeneous programs. 283 pressions. This allows specification of a wider class of algorithms from NLP and elsewhere (e.g., minimum expected loss decoding, smoothing formulas, neural networks, game tree analysis, and constraint programming). Although §4 and §5 have space to present only techniques for the semiring case, these can be generalized. Our approach may be most closely related to deductive databases, which even in their heyday were apparently ignored by the CL community (except for Minnen, 1996). Deductive database systems permit inference rules that can derive new database facts from old ones.5 They are essentially declarative logic programming languages (with restrictions or extensions) that are—or could be—implemented using efficient database techniques. Some implemented deductive databases such as CORAL (Ramakrishnan et al., 1994) and LOLA (Zukowski and Freitag, 1997) support aggregation (as in Dyna’s +=, log+=, max=, . . . ), although only “stratified” forms of it that exclude unary CFG rule cycles.6 Ross and Sagiv (1992) (and in a more restricted way, Kifer and Subrahmanian, 19"
H05-1036,J03-1006,0,0.576175,"e with parameter optimization code. Second, we fully generalize the agenda-based strategy of Shieber et al. (1995) to the weighted case—in particular supporting a prioritized agenda. That allows probabilities to guide the search for the best parse(s), a crucial technique in state-of-theart context-free parsers.3 We also give a “reverse” agenda algorithm to compute gradients or outside probabilities for parameter estimation. Third, regarding weights, the Dyna language is designed to express systems of arbitrary, heterogeneous equations over item values. In previous work such as (Goodman, 1999; Nederhof, 2003), one only specifies the inference rules as unweighted Horn clauses, and then weights are added automatically in a standard way: all values have the same type W, and all rules transform to equations of the form c ⊕= a1 ⊗ a2 ⊗ · · · ⊗ ak , where ⊕ and ⊗ give W the structure of a semiring.4 In Dyna one writes these equations explicitly in place of Horn clauses (Fig. 1). Accordingly, heterogeneous Dyna programs, to be supported soon by our compiler, will allow items of different types to have values of different types, computed by different aggregation operations over arbitrary right-hand-side ex"
H05-1036,P92-1017,0,0.101118,"helpful in the first author’s earlier work on new algorithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu, 1997; Melamed, 2003), lexicalized parsers (Eisner and Satta, 1999),22 partially supervised training a` la (Pereira and Schabes, 1992),23 and grammar induction (Klein and Manning, 2002). These replications were easy to write and extend, and to train via §5.2. 7.2 Experiments We compared the current Dyna compiler to handbuilt systems on a variety of parsing tasks. These problems were chosen not for their novelty or interesting structure, but for the availability of existing well-tuned implementations. Best parse. We compared a Dyna CFG parser to the Java parser of Klein and Manning (2003b),24 22 Markus Dreyer’s reimplementation of the complex Collins (1999) parser uses under 30 lines of Dyna. 23 For example, lines 2–3 of Fig."
H05-1036,P83-1021,0,0.727088,"e that is efficient enough for real NLP research, though still several times slower than hand-crafted code. 1 Introduction In this paper, we generalize some modern probabilistic parsing techniques to a broader class of weighted deductive algorithms. Our implemented system encapsulates these implementation techniques behind a clean interface—a small high-level specification language, Dyna, which compiles into C++ classes. This system should help the HLT community to experiment more easily with new models and algorithms. 1.1 Dynamic programming as deduction The “parsing as deduction” framework (Pereira and Warren, 1983) is now over 20 years old. It provides an elegant notation for specifying a variety of parsing algorithms (Shieber et al., 1995), including algorithms for probabilistic or other semiring-weighted parsing (Goodman, 1999). In the parsing community, new algorithms are often stated simply as a set of deductive inference rules (Sikkel, 1997; Eisner and Satta, 1999). It is also straightforward to specify other NLP algorithms this way. Syntactic MT models, language models, and stack decoders can be easily described using deductive rules. So can operations on finitestate and infinite-state machines. ∗"
H05-1036,P04-1062,1,0.883504,"es in the objective function as the axiom weights change, (3) can prune by skipping useless updates a that scarcely affected goal (e.g., 5.3 Parameter estimation To support parameter training using these gradients, our implementation of Dyna includes a training module, DynaMITE. DynaMITE supports the EM algorithm (and many variants), supervised and unsupervised training of log-linear (“maximum entropy”) models using quasi-Newton methods, and smoothing-parameter tuning on development data. As an object-oriented C++ library, it also facilitates rapid implementation of new estimation techniques (Smith and Eisner, 2004; Smith and Eisner, 2005). 6 Program Transformations Another interest of Dyna is that its high-level specifications can be manipulated by mechanical sourceto-source program transformations. This makes it possible to derive new algorithms from old ones. §5.1 already sketched the gradient transformation for finding ∇goal. We note a few other examples. Bounding transformations generate a new program that computes upper or lower bounds on goal, via generic bounding techniques (Prieditis, 1993; Culberson and Schaeffer, 1998). The A* heuristics explored by Klein and Manning (2003a) can be seen as re"
H05-1036,P05-1044,1,0.54338,"tion as the axiom weights change, (3) can prune by skipping useless updates a that scarcely affected goal (e.g., 5.3 Parameter estimation To support parameter training using these gradients, our implementation of Dyna includes a training module, DynaMITE. DynaMITE supports the EM algorithm (and many variants), supervised and unsupervised training of log-linear (“maximum entropy”) models using quasi-Newton methods, and smoothing-parameter tuning on development data. As an object-oriented C++ library, it also facilitates rapid implementation of new estimation techniques (Smith and Eisner, 2004; Smith and Eisner, 2005). 6 Program Transformations Another interest of Dyna is that its high-level specifications can be manipulated by mechanical sourceto-source program transformations. This makes it possible to derive new algorithms from old ones. §5.1 already sketched the gradient transformation for finding ∇goal. We note a few other examples. Bounding transformations generate a new program that computes upper or lower bounds on goal, via generic bounding techniques (Prieditis, 1993; Culberson and Schaeffer, 1998). The A* heuristics explored by Klein and Manning (2003a) can be seen as resulting from bounding tra"
H05-1036,W04-3207,1,0.526925,"nt projects, in the sense that we would not have attempted many of them without it. In some cases, we were experimenting with genuinely new algorithms not supported by any existing tool, as in our work on dependency-lengthlimited parsing (Eisner and Smith, 2005b) and loosely syntax-based machine translation (Eisner and D. Smith, 2005). (Dyna would have been equally helpful in the first author’s earlier work on new algorithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu, 1997; Melamed, 2003), lexicalized parsers (Eisner and Satta, 1999),22 partially supervised training a` la (Pereira and Schabes, 1992),23 and grammar induction (Klein and Manning, 2002). These replications were easy to write and extend, and to train via §5.2. 7.2 Experiments We compared the current Dyna compiler to handbuilt systems on a variety of parsing ta"
H05-1036,H05-1060,1,0.608751,"se that we would not have attempted many of them without it. In some cases, we were experimenting with genuinely new algorithms not supported by any existing tool, as in our work on dependency-lengthlimited parsing (Eisner and Smith, 2005b) and loosely syntax-based machine translation (Eisner and D. Smith, 2005). (Dyna would have been equally helpful in the first author’s earlier work on new algorithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu, 1997; Melamed, 2003), lexicalized parsers (Eisner and Satta, 1999),22 partially supervised training a` la (Pereira and Schabes, 1992),23 and grammar induction (Klein and Manning, 2002). These replications were easy to write and extend, and to train via §5.2. 7.2 Experiments We compared the current Dyna compiler to handbuilt systems on a variety of parsing tasks. These problems w"
H05-1036,J95-2002,0,0.0781184,"(say) hW, ⊕, ⊗i = hR≥0 , min, +i, where values are negated log probabilities. Positive-weight cycles will not affect the min. (Negative-weight cycles, however, would correctly cause the computation to diverge; these do not arise with probabilities.) If one is using hW, ⊕, ⊗i = hR≥0 , +, ∗i to compute the total weight of all proofs or parses, as in the inside algorithm, then Dyna must solve a system of nonlinear equations. The agenda algorithm does this by iterative approximation (propagating updates around any cycles in the proof graph until numerical convergence), essentially as suggested by Stolcke (1995) for the case of Earley’s algorithm.14 Again, the computation may diverge. 12 This holds for all Datalog programs, for instance. This argument does not hold if Dyna is used to express programs outside the semiring. In particular, one can write instances of SAT and other NP-hard constraint satisfaction problems by using cyclic rules with negation over finitely many boolean-valued items (Niemel¨a, 1998). Here the agenda algorithm can end up flipping values forever between false and true; a more general solver would have to be called in order to find a stable model of a SAT problem’s equations. 1"
H05-1036,J97-3002,0,0.0265683,"2005b) and loosely syntax-based machine translation (Eisner and D. Smith, 2005). (Dyna would have been equally helpful in the first author’s earlier work on new algorithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu, 1997; Melamed, 2003), lexicalized parsers (Eisner and Satta, 1999),22 partially supervised training a` la (Pereira and Schabes, 1992),23 and grammar induction (Klein and Manning, 2002). These replications were easy to write and extend, and to train via §5.2. 7.2 Experiments We compared the current Dyna compiler to handbuilt systems on a variety of parsing tasks. These problems were chosen not for their novelty or interesting structure, but for the availability of existing well-tuned implementations. Best parse. We compared a Dyna CFG parser to the Java parser of Klein and Manning (2003b),24 22 Mar"
H05-1036,W06-3104,1,\N,Missing
H05-1036,J03-4003,0,\N,Missing
H05-1036,P99-1059,1,\N,Missing
H05-1050,P05-1001,0,0.0175515,"to learn which clues to trust, and their relative weights. Our method is unsupervised in the conventional sense, as it obtains a classifier for drug with no supervision about drug. However, to learn what good classifiers generally look like2 for this task, we first use 1 A word token or document can be characterized by a 20-bit vector, corresponding to its classifications by 20 different binary classifiers. These vectors are detailed abstract representations of the words or documents. They can be clustered, or all their bits can be included as potentially relevant features in another task. 2 Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner. Both their work and ours try to transfer knowledge to a target problem from many artificial supervised “auxiliary problems,” which are generated from unlabeled data (e.g., our pseudoword disambiguation problems). However, in their “structural learning,” the target problem is supervised (if inadequately), and the auxiliary problems (supervised instances of a different task) are a source of useful hidden features for the classifier. In our “strapping,” the target"
H05-1050,J90-2002,0,0.200675,"ider features of Cs , the growth trajectory of Cs , or the relation between Cs and other classifiers. For concretness, we consider the Yarowsky method for word-sense disambiguation (WSD). How can we tell if a seed s = (x, y) was fertile, without using even a small validation set to judge Cs ? There are several types of 5 Alignment methods in machine translation rely even more heavily on this property. While they begin with a small translation lexicon, they are sufficiently robust to the choice of this initial seed (lexicon) that it suffices to construct a single seed by crude automatic means (Brown et al., 1990; Melamed, 1997). Human supervision (or strapping) is unnecessary. 6 This is particularly likely if one favors function words (in particular determiners and pronouns), which are strong indicators of gender. Cucerzan and Yarowsky used only content words because they could be extracted from bilingual dictionaries. 398 clues to fertility, which may be combined into a metaclassifier that identifies fertile seeds. Judge the result of classification with Cs : Even without a validation set, the result of running Cs on the training corpus can be validated in various ways, using independent plausibilit"
H05-1050,W99-0613,0,0.0821132,". Query expansion in IR searches for more documents “similar to” a designated relevant document. This problem too might be regarded as searching for a natural class—a small subset of documents that share some property of the original document—and approached using iterative bootstrapping. The seed would specify the original document plus one or two additional words or documents initially associated with the “relevant” and/or “irrelevant” classes. Strapping would guess various different seeds that extended the original document, then try to determine which seeds found a cohesive “relevant set.” Collins and Singer (1999) bootstrapped a system for classifying phrases in context. Again, they considered only one instance of this task: classifying English proper names as persons, organizations, or locations. Their seed consisted of 7 simple rules (“that New York, California, and U.S. are locations; that any name containing Incorporated is an organization; and that I.B.M. and Microsoft are organizations”). Strapping such a classifier would automatically discover named-entity classes in a different language, or other phrase classes in English. Cucerzan and Yarowsky (1999) built a similar system that identified prop"
H05-1050,W99-0612,0,0.0230285,"e which seeds found a cohesive “relevant set.” Collins and Singer (1999) bootstrapped a system for classifying phrases in context. Again, they considered only one instance of this task: classifying English proper names as persons, organizations, or locations. Their seed consisted of 7 simple rules (“that New York, California, and U.S. are locations; that any name containing Incorporated is an organization; and that I.B.M. and Microsoft are organizations”). Strapping such a classifier would automatically discover named-entity classes in a different language, or other phrase classes in English. Cucerzan and Yarowsky (1999) built a similar system that identified proper names as well as classifying them. Their seed consisted of a list of 40 to 300 names. Large seeds were not necessary for precision but did help recall. Cucerzan and Yarowsky (2003) classified masculine vs. feminine nouns. They experimented with several task instances, namely different Indo-European languages. In each instance, their seed consisted of up to 30 feminine and 30 masculine words (e.g., girl, princess, father). Many more papers along these lines could be listed. A rather different task is grammar induction, where a task instance is a co"
H05-1050,N03-1006,0,0.15681,"ns, organizations, or locations. Their seed consisted of 7 simple rules (“that New York, California, and U.S. are locations; that any name containing Incorporated is an organization; and that I.B.M. and Microsoft are organizations”). Strapping such a classifier would automatically discover named-entity classes in a different language, or other phrase classes in English. Cucerzan and Yarowsky (1999) built a similar system that identified proper names as well as classifying them. Their seed consisted of a list of 40 to 300 names. Large seeds were not necessary for precision but did help recall. Cucerzan and Yarowsky (2003) classified masculine vs. feminine nouns. They experimented with several task instances, namely different Indo-European languages. In each instance, their seed consisted of up to 30 feminine and 30 masculine words (e.g., girl, princess, father). Many more papers along these lines could be listed. A rather different task is grammar induction, where a task instance is a corpus of text in some language, and the learned classifier is a parser. Following Chomsky (1981), we suggest that it may be possible to seed a grammar induction method with a small number of facts about the word order of the lan"
H05-1050,H92-1045,0,0.730579,"s fertile seeds. Judge the result of classification with Cs : Even without a validation set, the result of running Cs on the training corpus can be validated in various ways, using independent plausibility criteria that were not considered by the bootstrapping learner. • Is the classification reasonably balanced? (If virtually all examples of the target word are labeled with the same sense, then Cs has not found a sense distinction.) • When a document contains multiple tokens of the target word, are all examples labeled with the same sense? This property tends to hold for correct classifiers (Gale et al., 1992a), at least for homonyms. • True word senses usually correlate with document or passage topic. Thus, choose a measure of similarity between documents (e.g., the cosine measure in TF/IDF space). Does the target word tend to have the same sense in a document and in its nearby neighbors? • True word senses may also improve performance on some task. Is the perplexity of a language model much reduced by knowing whether sense x or sense y (according to Cs ) appeared in the current context? (This relates to the previous point.) Likewise, given a small bilingual text that has been automatically (and"
H05-1050,1992.tmi-1.9,0,0.527658,"s fertile seeds. Judge the result of classification with Cs : Even without a validation set, the result of running Cs on the training corpus can be validated in various ways, using independent plausibility criteria that were not considered by the bootstrapping learner. • Is the classification reasonably balanced? (If virtually all examples of the target word are labeled with the same sense, then Cs has not found a sense distinction.) • When a document contains multiple tokens of the target word, are all examples labeled with the same sense? This property tends to hold for correct classifiers (Gale et al., 1992a), at least for homonyms. • True word senses usually correlate with document or passage topic. Thus, choose a measure of similarity between documents (e.g., the cosine measure in TF/IDF space). Does the target word tend to have the same sense in a document and in its nearby neighbors? • True word senses may also improve performance on some task. Is the perplexity of a language model much reduced by knowing whether sense x or sense y (according to Cs ) appeared in the current context? (This relates to the previous point.) Likewise, given a small bilingual text that has been automatically (and"
H05-1050,P97-1063,0,0.0200688,", the growth trajectory of Cs , or the relation between Cs and other classifiers. For concretness, we consider the Yarowsky method for word-sense disambiguation (WSD). How can we tell if a seed s = (x, y) was fertile, without using even a small validation set to judge Cs ? There are several types of 5 Alignment methods in machine translation rely even more heavily on this property. While they begin with a small translation lexicon, they are sufficiently robust to the choice of this initial seed (lexicon) that it suffices to construct a single seed by crude automatic means (Brown et al., 1990; Melamed, 1997). Human supervision (or strapping) is unnecessary. 6 This is particularly likely if one favors function words (in particular determiners and pronouns), which are strong indicators of gender. Cucerzan and Yarowsky used only content words because they could be extracted from bilingual dictionaries. 398 clues to fertility, which may be combined into a metaclassifier that identifies fertile seeds. Judge the result of classification with Cs : Even without a validation set, the result of running Cs on the training corpus can be validated in various ways, using independent plausibility criteria that"
H05-1050,N03-2023,0,0.0288826,"acing all occurences of two words or phrases with their conflation. For example, banana and wine are replaced everywhere by bananawine. The original, unconflated text serves as a supervised answer key for the artificial task of disambiguating banana-wine. Traditionally, pseudowords are used as cheap test data to evaluate a disambiguation system. Our idea is to use them as cheap development data to tune a system. In our case, they tune a few free parameters of h(s), which says what a good classifier for this task looks like. Pseudowords should be plausible instances of the task (Gaustad, 2001; Nakov and Hearst, 2003): so it is deliberate that banana and wine share syntactic and semantic features, as senses of real ambiguous words often do. Cheap “pseudo-supervised” data are also available in some other strapping settings. For grammar induction, one could construct an artificial probabilistic grammar at random, and generate text from it. The task of recovering the grammar from the text then has a known answer. 4 Experiments 4.1 Unsupervised Training/Test Data Our experiments focused on the original Yarowsky algorithm. We attempted to strap word-sense classifiers, using English data only, for English words"
H05-1050,W03-0404,0,0.0598686,"d this bad luck by sprouting a different set of examples. 2.2 A Few Other Applications of Bootstrapping Inspired by Yarowsky, Blum and Mitchell (1998) built a classifier for the task of web page classification.4 They considered only one instance of this task, namely distinguishing course home pages from other web pages at a computer science department. Their seed consisted of 3 positive and 9 negative examples. Strapping a web page classifier would mean identifying seeds that lead to other “natural classes” of web pages. Strapping may be useful for unsupervised text categorization in general. Riloff et al. (2003) learned lists of subjective nouns in English, seeding their method with 20 high-frequency, strongly subjective words. This seed set was chosen manually from an automatically generated list of 850 can4 More precisely, they bootstrapped two Naive Bayes classifiers—one that looked at page content and the other that looked at links to the page. This “co-training” approach has become popular. It was also used by the Cucerzan and Yarowsky papers below, which looked at “internal” and “external” features of a phrase. didate words. Strapping their method would identify subjective nouns in other langua"
H05-1050,J98-1004,0,0.210136,"Missing"
H05-1050,P95-1026,0,0.849513,"ore, MD 21218 USA {eisner,damianos}@jhu.edu Abstract “Bootstrapping” methods for learning require a small amount of supervision to seed the learning process. We show that it is sometimes possible to eliminate this last bit of supervision, by trying many candidate seeds and selecting the one with the most plausible outcome. We discuss such “strapping” methods in general, and exhibit a particular method for strapping wordsense classifiers for ambiguous words. Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand (Yarowsky, 1995), which in turn is known to rival supervised methods. 1 Introduction Some of NLP’s most interesting problems have to do with unsupervised learning. Human language learners are able to discover word senses, grammatical genders, morphological systems, grammars, discourse registers, and so forth. One would like to build systems that discover the same linguistic patterns in raw text. For that matter, one would also like to discover patterns in bilingual text (for translation), in document collections (for categorization and retrieval), and in other data that fall outside the scope of humans’ langu"
J00-2014,J95-4011,0,0.0300948,"ensky, and McCarthy in 1993. OT reclaims traditional grammar's ability to express surface generalizations (""syllables have onsets,"" ""no nasal+voiceless obstruent clusters""). Empirically, some surface generalizations are robust within a language, or--perhaps for functionalist reasons-widespread across languages. Derivational theories were forced to posit diverse rules that rescued these robust generalizations from other phonological processes. An OT grammar avoids such ""conspiracies"" by stating the generalizations directly, as in TwoLevel Morphology (Koskenniemi 1983) or Declarative Phonology (Bird 1995). In OT, the processes that try but fail to disrupt a robust generalization are described not as rules (cf. Paradis 1988), but as lower-ranked generalizations. Such a generalization may fail in contexts where it is overruled by a higher-ranked requirement of the language (or of the underlying form). As Kager emphasizes, this interaction of violable constraints can yield complex surface patterns. OT therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface-level constraints that partially mask one another.' Whether this is always possible un"
J00-2014,P97-1040,1,0.922123,"generalization are described not as rules (cf. Paradis 1988), but as lower-ranked generalizations. Such a generalization may fail in contexts where it is overruled by a higher-ranked requirement of the language (or of the underlying form). As Kager emphasizes, this interaction of violable constraints can yield complex surface patterns. OT therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface-level constraints that partially mask one another.' Whether this is always possible under an appropriate definition of ""simple constraints"" (e.g., Eisner 1997b) is of course an empirical question. 2. Relevance Before looking at Kager's textbook in detail, it is worth pausing to ask what broader implications Optimality Theory might have for computational linguistics. If you are an academic phonologist, you already know OT by now. If you are not, should you take the time to learn? So far, OT has served CL mainly as a source of interesting new problems--both theoretical and (assuming a lucrative market for phonology workbench utilities) prac1 This style of analysis is shared by Autolexical Grammar (Sadock 1985), which has focused more on (morpho)synta"
J00-2014,P97-1032,0,0.0253084,"tuation may also be suited to OT analysis. 2 Each constraint/featureis weightedso highlythat it can overwhelmthe total of all lower-ranked constraints, and even the lowest-rankedconstraintis weightedvery highly.Recallthat the incompatibilityof some feature combinations(i.e., nonorthogonalityof features) is alwayswhat makes it nontrivial to normalizeor sample a Gibbs distribution,just as it makes it nontrivialto find optimal forms in OT. 287 Computational Linguistics Volume 26, Number 2 Second, weights are an annoyance when writing grammars by hand. In some cases rankings may work well enough. Samuelsson and Voutilainen (1997) report excellent part-of-speech tagging results using a handcrafted approach that is close to OT. 3 More speculatively, imagine an OT grammar for stylistic revision of parsed sentences. The tension between preserving the original author's text (faithfulness to the underlying form) and making it readable in various ways (well-formedness) is right up OT's alley. The same applies to document layout: I have often wished I could write OT-style TeX macros~ Third, even in statistical corpus-based NLP, estimating a full Gibbs distribution is not always feasible. Even if strict ranking is not quite ac"
K16-1013,Q14-1040,0,0.0517026,"e same novel words out of context. This allows us to study how cognateness and context interact, in a wellcontrolled setting. Cognates and very common words may be easy to translate without context, 1 Both languages mark for number and German occasionally marks for case. 126 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 126–135, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tended into larger models of sentence understanding. Vajjala and Meurers (2012) classify the difficulty level of longer L2 texts. Beinborn et al. (2014b) provide an overview of ways that readability measures and user background may be modeled specifically in the context of L2 learners, including through the use of cognateness features. They include a 17-word pilot study of German L1 speakers’ ability to guess the meanings of Czech cognates with no context, and hypothesize that observing the words in an understandable context would improve guessability (which we confirm in the English-German case in this work). variety of features—that is, we try to identify cues that our learners might plausibly use. 2 Motivation and Related Work In Renduchi"
K16-1013,P07-1033,0,0.0172056,"Missing"
K16-1013,P13-2121,1,0.844967,"Missing"
K16-1013,P14-1053,0,0.106382,"o do this, we must be able to predict when learners will be able to understand a novel L2 vocabulary item. In a previous study (Renduchintala et al., 2016b), we used a small set of simple features to build user-specific models of lexical understanding in macaronic sentences. The present paper evaluates a larger set of features under a more tightly controlled experimental setup. In particular, in the present paper, our model does not have to predict which context words the learner understands, because there is only one L2 word per trial: any context words are always in L1. A similar project by Labutov and Lipson (2014) likewise considers the effect of context on guessing the L2 word. However, it does not consider the effect of the L2 word’s spelling, which we show is also important. Our experimental setup, particularly the cloze task, is closely related to research in the L2 education and computer-assisted language learning (CALL) domains. Educators often use cloze tasks to evaluate learner vocabulary (though these generally use L2 context). Beinborn et al. (2014a) look at automatically predicting the difficulty of C-tests (a cloze-like task where blanks are introduced at the character level, rather than at"
K16-1013,D14-1162,0,0.0896482,"y on training data. As an outside resource for training language models and other resources consulted by our features, we used Simple English Wikipedia (Wikimedia Foundation, 2016). It contains 767,826 sentences, covers a similar set of topics to the NachrichtenLeicht.de data, and uses simple sentence structure. The sentence lengths are also comparable, with a mean of 17.6 tokens and a median of 16 tokens. This makes it well-matched for our task. We also use pre-trained vector representations of words; for these we chose to use the 300-dimensional GloVe vectors trained on a 6Btoken dataset by Pennington et al. (2014). Guessability and Guess Quality We train a log-linear model to predict the words that our subjects guess on training data, and we will check its success at this on test data. However, from an engineering perspective, we do not actually need to predict the user’s specific good or bad answers, but only whether they are good or bad. A language-learning interface should display an L2 word only when the user has a good chance of guessing its L1 translation. Thus we also assess our features and model on the easier task of predicting the guessability of a task instance x—that is, the average empiric"
K16-1013,P16-4023,1,0.611684,"word. We seek to evaluate this quantitatively and qualitatively in “extreme” cases where the context is either completely comprehensible or absent, and where the cognateness information is either present or absent. In doing so, we are able to see how learners react differently to novel words in different contexts. Our controlled experiments can serve as a proxy for incidental learning in other settings: encountering novel words in isolation (e.g. vocabulary lists), while reading in a familiar language, or while using a language-learning interface such as our own mixed-language reading system (Renduchintala et al., 2016a). We train a log-linear model to predict the translations that our novice learners will guess, given what we show them and their L1 knowledge. Within this setup, we evaluate the usefulness of a In this work, we explore how learners can infer second-language noun meanings in the context of their native language. Motivated by an interest in building interactive tools for language learning, we collect data on three word-guessing tasks, analyze their difficulty, and explore the types of errors that novice learners make. We train a log-linear model for predicting our subjects’ guesses of word mea"
K16-1013,P16-1175,1,0.850903,"word. We seek to evaluate this quantitatively and qualitatively in “extreme” cases where the context is either completely comprehensible or absent, and where the cognateness information is either present or absent. In doing so, we are able to see how learners react differently to novel words in different contexts. Our controlled experiments can serve as a proxy for incidental learning in other settings: encountering novel words in isolation (e.g. vocabulary lists), while reading in a familiar language, or while using a language-learning interface such as our own mixed-language reading system (Renduchintala et al., 2016a). We train a log-linear model to predict the translations that our novice learners will guess, given what we show them and their L1 knowledge. Within this setup, we evaluate the usefulness of a In this work, we explore how learners can infer second-language noun meanings in the context of their native language. Motivated by an interest in building interactive tools for language learning, we collect data on three word-guessing tasks, analyze their difficulty, and explore the types of errors that novice learners make. We train a log-linear model for predicting our subjects’ guesses of word mea"
K16-1013,W12-2019,0,0.0289655,"n L2 context each subject understood. We also present novice learners with the same novel words out of context. This allows us to study how cognateness and context interact, in a wellcontrolled setting. Cognates and very common words may be easy to translate without context, 1 Both languages mark for number and German occasionally marks for case. 126 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 126–135, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tended into larger models of sentence understanding. Vajjala and Meurers (2012) classify the difficulty level of longer L2 texts. Beinborn et al. (2014b) provide an overview of ways that readability measures and user background may be modeled specifically in the context of L2 learners, including through the use of cognateness features. They include a 17-word pilot study of German L1 speakers’ ability to guess the meanings of Czech cognates with no context, and hypothesize that observing the words in an understandable context would improve guessability (which we confirm in the English-German case in this work). variety of features—that is, we try to identify cues that our"
K16-1013,J90-1003,0,\N,Missing
K17-1025,D14-1179,0,0.0390085,"Missing"
K17-1025,P16-1174,0,0.130452,"e vector, and the state update rule is also interpretable—it is a type of error-correcting learning rule. In addition, the student’s state is able to predict the student’s actual response and not merely whether the response was correct. We expect that having an interpretable feature vector has better inductive bias (see experiment in section 7.1), and that it may be useful to plan future actions by smart flash card systems. Moreover, in this work we test different plausible state update rules and see how they fit actual student responses, in orer to gain insight about learning. Most recently, Settles and Meeder (2016)’s halflife regression assumes that a student’s retention of a particular skill exponentially decays with time and learns a parameter that models the rate of decay (“half-life regression”). Like Gonz´alez-Brenes et al. (2014) and Settles and Meeder (2016), our model leverages a feature-rich formulation to predict the probability of a learner correctly remembering a skill, but can also capture complex spacing/retention patterns using a neural gating mechanism. Another distinction between our work and half-life regression is that we focus on knowledge tracing within a single session, while half-"
K17-2001,E14-1060,1,0.855884,"Missing"
K17-2001,P17-1136,0,0.0562959,"Missing"
K17-2001,N15-1107,1,0.861491,"Missing"
K17-2001,chrupala-etal-2008-learning,0,0.152497,"Missing"
K17-2001,W16-2004,0,0.0654088,"Missing"
K17-2001,P14-2102,1,0.910119,"Missing"
K17-2001,Q15-1031,1,0.919531,"Missing"
K17-2001,P16-1156,1,0.87299,"Missing"
K17-2001,K17-2002,0,0.132442,"Missing"
K17-2001,E17-2120,1,0.859022,"Missing"
K17-2001,E17-1049,1,0.894341,"Missing"
K17-2001,N07-1048,0,0.21796,"Missing"
K17-2001,P17-1182,1,0.876831,"Missing"
K17-2001,D09-1011,1,0.888474,"Missing"
K17-2001,D08-1113,1,0.874358,"Missing"
K17-2001,P16-2090,0,0.46072,"Missing"
K17-2001,N13-1138,0,0.147728,"Missing"
K17-2001,P08-1115,0,0.089787,"Missing"
K17-2001,L16-1498,1,0.792213,"Missing"
K17-2001,N16-1077,1,0.812854,"Missing"
K17-2001,W10-2211,0,0.123726,"Missing"
K17-2001,W16-2006,0,0.0673676,"Missing"
K17-2001,P82-1020,0,0.75423,"Missing"
K17-2001,P08-1103,0,0.0541683,"Missing"
K17-2001,D15-1272,1,0.92636,"Missing"
K17-2001,D14-1095,0,0.0938069,"Missing"
K17-2001,K17-2011,0,0.0350181,"Missing"
K17-2001,N15-1093,0,0.19098,"Missing"
K17-2001,K17-2008,0,0.055504,"Missing"
K17-2001,K17-2010,0,0.158704,"Missing"
K17-2001,W16-2007,0,\N,Missing
K17-2001,L16-1497,1,\N,Missing
K17-2001,K17-2012,0,\N,Missing
K17-2001,K17-2003,0,\N,Missing
K17-2001,P17-1029,0,\N,Missing
K18-3001,K18-3001,1,0.103672,"Missing"
K18-3001,P16-2090,1,0.838493,"Missing"
K18-3001,K17-2003,1,0.836665,"Missing"
K18-3001,K17-2010,1,0.734971,"Missing"
K18-3001,W18-6011,1,0.913422,"and a target UniMorph sentence is shown in Figure 3. Since the selection of languages in task 2 is small and we do not attempt to correct annotation errors in the UD source materials, conversion between UD and UniMorph morphosyntactic descriptions is generally straightforward.11 However, UD descriptions are more fine-grained than their UniMorph equivalents. For example, UD denotes lexical features such as noun gender which are inherent features of a lexeme possessed by all of its word forms. Such inherent features are missing from UniMorph which exclusively annotates inflectional morphology (McCarthy et al., 2018). Therefore, UD fea$ → sta$ ti$ → dista$ koti$ → kodista$ i$ → ista$ oti$ → odista$ Such rules are then extracted from each example inflection in the training data. At generation time, the longest matching left hand side of a rule is identified and applied to the citation form. For example, if the Finnish noun luoti ‘bullet’ were to be inflected in the elative (N;IN+ABL;SG) using only the extracted rules given above, the transformation oti$ → odista$ would be triggered, producing the output luodista. In case there are multiple candidate rules of equally long left hand sides that all match, tie"
K18-3001,K18-3012,0,0.30177,"Missing"
K18-3001,K18-3015,0,0.276525,"Missing"
K18-3001,P15-2111,1,\N,Missing
K18-3001,K17-2002,1,\N,Missing
K18-3001,K17-3001,0,\N,Missing
K18-3001,W17-4110,1,\N,Missing
K18-3001,N18-2087,1,\N,Missing
K18-3001,P18-1245,1,\N,Missing
K18-3001,L18-1293,1,\N,Missing
K18-3001,K18-3004,0,\N,Missing
K18-3001,K18-3010,0,\N,Missing
K18-3001,K18-3013,0,\N,Missing
K18-3001,K18-3003,0,\N,Missing
K18-3001,K18-3016,0,\N,Missing
K18-3001,K18-3005,0,\N,Missing
K18-3001,W16-2006,0,\N,Missing
K18-3001,K17-2008,1,\N,Missing
K18-3001,K17-2005,0,\N,Missing
K18-3001,K18-3008,0,\N,Missing
K18-3001,K18-3007,0,\N,Missing
L18-1293,E14-1060,1,0.914644,"Missing"
L18-1293,P16-1156,1,0.911216,"Missing"
L18-1293,N07-1048,0,0.136877,"Missing"
L18-1293,N13-1138,0,0.235978,"Missing"
L18-1293,P08-1115,0,0.0456449,"Missing"
L18-1293,N16-1077,1,0.880807,"Missing"
L18-1293,L16-1498,1,0.945257,"The Universal Morphology (UniMorph) project, centered at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University is a collaborative effort to improve how NLP systems handle complex morphology across the world’s languages. The project releases annotated morphological data using a universal tagset, the UniMorph schema. Each inflected form is associated with a lemma, which typically carries its underlying lexical meaning, and a bundle of morphological features from our schema. Additional supporting data and tools are also released on a per-language basis when available. Kirov et al. (2016) introduced version 1.0 of the UniMorph morphological database, created by extracting and normalizing the inflectional paradigms included in Wiktionary (www.wiktionary.org), a large, broadly multi-lingual crowd-sourced collection of lexical data. This paper describes UniMorph 2.0. It details improvements in Wiktionary extraction and annotation, as well as normalization of non-Wiktionary resources, leading to a much higher quality morphological database. The new dataset spans 52 languages representing a range of language families. As in UniMorph 1.0, we provide paradigms from highlyinflected op"
L18-1293,D14-1095,0,0.116143,"Missing"
L18-1293,N15-1093,0,0.150689,"Missing"
L18-1293,Q15-1026,0,0.0836395,"Missing"
L18-1293,P15-2111,1,0.852807,"rsing and normalization of Wiktionary. Wiktionary is a broadly multilingual resource with many crowd-sourced morphological paradigms in the form of custom HTML tables. Figure 1 illustrates the challenge associated with extracting this data. Wiktionary is designed for human, rather than machine readability, and authors have extensive freedom in formatting data. This leads to wildly differing table layouts across languages which need to be converted to a consistent tabular format. The extraction process developed for UniMorph 1.0 relied heavily on statistical, visual, and positional heuristics (Sylak-Glassman et al., 2015b) to: 1. Determine which entries in an HTML table are inflected forms and which are grammatical descriptors. 2. Link each inflected form with its appropriate descriptors. 3. Convert each set of linked descriptors into a universal feature annotation schema, described in detail in Sylak-Glassman (2016).1 This led to a large dataset of 952,530 unique noun, verb, and adjective lemmas across 350 languages. Unfortunately, 1 pdf unimorph.github.io/doc/unimorph-schema. Figure 1: Paradigm extraction and normalization. the UniMorph 1.0 dataset was very error-prone due to the inability of our heuristics"
L18-1293,zeman-2008-reusable,0,0.0233658,"Each group represents a different type of paradigm (e.g., regular verb). For each group, a sample table was selected, and an annotator replaced each inflected form in the table with the appropriate UniMorph features. All annotation was compliant with the UniMorph Schema, which was designed to represent the full range of semantic distinctions that can be captured by inflectional morphology in any language (SylakGlassman et al., 2015a). The schema is similar in form and spirit to other tagset universalization efforts, such as the Universal Dependencies Project (Choi et al., 2015) and Interset (Zeman, 2008), but is designed specifically for typological completeness for inflectional morphology, including a focus on the morphology of especially low-resource languages. It includes over 200 base features distributed among 23 dimensions of meaning (i.e., morphological categories), including both common dimensions like tense and aspect as well as rarer dimensions like evidentiality and switch-reference. Despite the high coverage of the UniMorph tagset, for UniMorph 2.0, annotators were allowed to employ additional ‘language specific’ LGSPEC(1, 2, 3, etc.) features to mark any missing distinctions, or"
L18-1293,W16-2002,1,\N,Missing
M95-1015,A88-1019,0,0.107606,"sed on the above criteria. Also, the tokenizer is responsible for maintaining the mapping between these tw o tokenizations so that the output of tools which use different tokenization schemes can be combined . Part-of-Speech Tagging Several components of the MUC coreference system, such as the noun phrase detector, require part-ofspeech (POS) tags for all of the words in an article . We combined the output of the following three POS tagger s using a simple voting scheme : Eric Brill&apos;s Rule Based Tagger version 1 .14 [2], the XTAG tagger, which is an implementation of Ken Church&apos;s PARTS tagger [4] and Adwait Ratnaparkhi&apos;s Maximum Entropy Tagger [11] . Each of these taggers uses the Penn Treebank tagset [8] . These three taggers, which were trained on the Penn Treebank Wall Street Journal corpus, tag pre-tokenized text . The tag actually used by the MUC system is determined by a majority voting scheme, in which a tag is chose n as the ""winner"" if at least two of the taggers postulate it . In the rare event that all three taggers disagree, the system uses the tag assigned by the maximum entropy tagger . In most cases, the majority voting scheme eliminates error s that are esoteric to a s"
M95-1015,W95-0103,0,0.0276076,"Missing"
M95-1015,C92-3145,0,0.0311847,"ome transformations to the set learned from the treebank . These transformations generalized o n learned ones . For instance, rules were learned which involved days of the week, but due to sparsity of training data , they were learned only for a subset of the seven days of the week . We manually added the missing cases . We did no t independently measure the performance of their tool using this modified rule set, but may do so in the future . 181 Knowledge Source s We experimented with various knowledge sources during system development, including WordNet [9], th e XTAG morphological analyzer [6], Roget&apos;s publicly available 1911 thesaurus, the Collins dictionary, a version of the American Heritage dictionary for which the University of Pennsylvania has a site license and the Gazetteer . Onl y WordNet, the XTAG morphological analyzer and the Gazetteer were used in the final system . We extracted a geographic name database from a publicly available version of the Gazetteer which we downloaded from the Center for Lexical Research. This database contains names of continents, islands, island groups , countries, provinces, cities and airports . This information is used when performing type"
M95-1015,J94-4002,0,0.196275,"ues . The difference in weighting between the two is currently based on intuition, though corpus methods migh t yield a more exact estimate of how much weight to give the female reading based on how often such words are actually used to refer to women . Pleonastic It Detectio n It is often used anaphorically in Wall Street Journal Text . Nonetheless, identifying instances of pleonastic it, which do not corefer, is still significant . The system identifies these instances of it by scanning tagged text and applying partly syntactic and partly lexical tests . Most of these tests are described in [7], but some additional test s were added to increase coverage . The fifteen rules used to detect pleonastic it are shown below in table 4 . Part of speech tags follow words and a slash, and are specified using the Penn Treebank tagset . Disjunctions are indicated using a vertical bar, (I), and optional elements are surrounded by brackets, ([]) . S abbreviates sentence ; NP mean s noun phrase ; and VP stands for verb phrase . We abbreviate CA for comparative adjectives, such as larger or smaller; SA for superlatives, such as greatest or largest ; MA for modal adjectives, such as necessary or unc"
M95-1015,J93-2004,0,0.0288585,"andard tasks . The first is the alteration of headline word capitalization . The Wall Street Journal adheres to standard conventions fo r capitalization of words in headlines, but since capitalization is an important cue for coreference resolution, w e attempted to eliminate capitalization which resulted solely from these conventions . Headline words which were capitalized in the body of the text anywhere other than sentence-initial position remained capitalized, as did thos e which were frequently capitalized other than in sentence-initial position in the Treebank Wall Street Journal corpu s [8] . All other uppercase words were converted to lowercase . The second non-standard task addressed by the tokenizer is the extraction of date information . The datelin e field is parsed to determine when each article was written . This information is later used to posit coreference between words or phrases such as today, tomorrow, this week, this year, and dates, such as November 20, 1995 . The third non-standard component determines whether &apos;s or &apos; is a genitive marker or part of a company name . When it is actually part of a company name, it does not indicate possession of the following noun"
M95-1015,W95-0107,0,0.0211053,"iting parsers and preprocessing utilities which allowed various pre existing tools to communicate with one another and produce output which could be used by other tools further in th e processing pipeline . Thus, we were freed to spend time developing the task-specific components of the system an d performing data analysis . Although no time was spent developing tools particularly for the MUC task prior t o January, many hours went into developing some of the off-the-shelf components we used, such as Eric Brills partof-speech tagger [2] and Lance Ramshaw and Mitch Marcus&apos; Noun Phrase Detector [10] . We estimate the tota l number of hours spent on the project itself to be roughly 1800, distributed among the eight graduate students wh o worked on the project. The vast majority of these hours were contributed between the end of July and competitio n week in early October . Table 1 shows the performance of our system when simple formatting errors, which hurt performance o n two of the 30 test files, were corrected . Table 2 contains our official system performance figures . Table 3 contain s system performance when optional elements were treated as if required . This set of scores is prese"
M95-1015,A88-1000,0,\N,Missing
N03-1009,P02-1001,1,0.908179,"are provided for combining weights from the different arcs of the automaton. The triple (K, ⊕, ⊗) is called a weight semiring and will be explained below. K-valued functions that can be computed by finite-state automata are called rational functions. How does minimization generalize to arbitrary weight semirings? The question is of practical as well as theoretical interest. Some NLP automata use the real semiring (R, +, ×), or its log equivalent, to compute unnormalized probabilities or other scores outside the range [0, 1] (Lafferty et al., 2001; Cortes et al., 2002). Expectation semirings (Eisner, 2002) are used to handle bookkeeping when training the parameters of a probabilistic transducer. A byproduct of this paper is a minimization algorithm that works fully with those semirings, a new result permitting more efficient automaton processing in those situations. Surprisingly, we will see that minimization is not even well-defined for all weight semirings! We will then (nearly) characterize the semirings where it is welldefined, and give a recipe for constructing minimization algorithms similar to Mohri’s in such semirings. Finally, we follow this recipe to obtain a specific, simple and prac"
N03-1009,J97-2003,0,0.20152,"stages of the construction, since (for example) smaller automata can be intersected faster. Recently the computational linguistics community has turned its attention to weighted automata that compute interesting functions of their input strings. A traditional automaton only returns an boolean from the set K = {true, f alse}, which indicates whether it has accepted the input. But a probabilistic automaton returns a probability in K = [0, 1], or equivalently, a negated logprobability in K = [0, ∞]. A transducer returns an output string from K = ∆∗ (for some alphabet ∆). Celebrated algorithms by Mohri (1997; 2000) have recently made it possible to minimize deterministic automata whose weights (outputs) are log-probabilities or strings. These cases are of central interest in language and speech processing. However, automata with other kinds of weights can also be defined. The general formulation of weighted automata (Berstel and Reutenauer, 1988) permits any weight set K, if appropriate operations ⊕ and ⊗ are provided for combining weights from the different arcs of the automaton. The triple (K, ⊕, ⊗) is called a weight semiring and will be explained below. K-valued functions that can be computed"
N06-1054,W04-2412,0,0.0728471,"Missing"
N06-1054,P05-1045,0,0.133702,"oft constraints. On the CoNLL-2004 semantic role labeling task, we report a speedup of at least 16x over a previous method that used integer linear programming. 1 Introduction Many tasks in natural language processing involve sequence labeling. If one models long-distance or global properties of labeled sequences, it can become intractable to find (“decode”) the best labeling of an unlabeled sequence. Nonetheless, such global properties can improve the accuracy of a model, so recent NLP papers have considered practical techniques for decoding with them. Such techniques include Gibbs sampling (Finkel et al., 2005), a general-purpose Monte Carlo method, and integer linear programming (ILP), (Roth and Yih, 2005), a general-purpose exact framework for NP-complete problems. Under generative models such as hidden Markov models, the probability of a labeled sequence depends only on its local properties. The situation improves with discriminatively trained models, such as conditional random fields (Lafferty et al., 2001), which do efficiently allow features that are functions of the entire observation sequence. However, these features can still only look locally at the label sequence. That is a significant sh"
N06-1054,W04-2416,0,0.0205233,"c role labeling The semantic role labeling task (Carreras and M`arques, 2004) involves choosing instantiations of verb arguments from a sentence for a given verb. The verb and its arguments form a proposition. We use data from the CoNLL-2004 shared task—the PropBank (Palmer et al., 2005) annotations of the Penn Treebank (Marcus et al., 1993), with sections 15–18 as the training set and section 20 as the development set. Unless otherwise specified, all measurements are made on the development set. We follow Roth and Yih (2005) exactly, in order to compare system runtimes. They, in turn, follow Hacioglu et al. (2004) and others in labeling only the heads of syntactic chunks rather than all words. We label only the core arguments (A0–A5), treating ? (a) A0 A0 0 1 A0 A1 A2 A3 ? A5 A4 A3 A2 A1 A0 O O 0 ? 1 O (verb position) 2 • A RGUMENT CANDIDATES (Fig. 5) encodes a set of position spans each of which must receive only a single label type. These spans were proposed using a high-recall heuristic (Xue and Palmer, 2004). 2 A3 A2 A1 A0 O • K NOWN VERB POSITION (Fig. 4(b)) simply encodes the position of the verb in question, which must be labeled O. 0 • D ISALLOW ARGUMENTS (Fig. 4(c)) specifies argument types th"
N06-1054,P04-1065,0,0.0262106,"ch type in a given sentence. We separate this into six individual constraints, one for each core argument type. Thus, we have constraints called N O DUPLI CATE A0, N O DUPLICATE A1, etc. Each of these is represented as a three-state FSA. • AT LEAST ONE ARGUMENT (Fig. 1) simply requires that the label sequence is not O∗ . This is a two-state automaton as described in §2. The last three constraints require information about the example, and the automata must be constructed on a per-example basis: 426 5.2 Experiments We implemented our hard constraint relaxation algorithm, using the FSA toolkit (Kanthak and Ney, 2004) for finite-state operations. FSA is an opensource C++ library providing a useful set of algorithms on weighted finite-state acceptors and transducers. For each example we decoded, we chose a random order in which to apply the constraints. Lattices are generated from what amounts to a unigram model—the voted perceptron classifier of Roth and Yih. The features used are a subset of those commonly applied to the task. Our system produces output identical to that of Roth and Yih. Table 1 shows F-measure on the core arguments. Table 2 shows a runtime comparison. The ILP runtime was provided by the"
N06-1054,C90-2040,0,0.0432374,"Our hope is that, although intractable in the worst case, the constraint relaxation algorithm will operate efficiently in practice. The success of traditional sequence models on NLP tasks suggests that, for natural language, much of the correct analysis can be recovered from local features and constraints alone. We suspect that, as a result, global constraints will often be easy to satisfy. Pseudocode for the algorithm appears in Figure 2. Note that line 2 does not specify how to choose C from among multiple violated constraints. This is discussed in §7. Our algorithm resembles the method of Koskenniemi (1990) and later work. The difference is that there lattices are unweighted and may not contain a path that satisfies all constraints, so that the order of constraint intersection matters. 5 Semantic role labeling The semantic role labeling task (Carreras and M`arques, 2004) involves choosing instantiations of verb arguments from a sentence for a given verb. The verb and its arguments form a proposition. We use data from the CoNLL-2004 shared task—the PropBank (Palmer et al., 2005) annotations of the Penn Treebank (Marcus et al., 1993), with sections 15–18 as the training set and section 20 as the d"
N06-1054,J93-2004,0,0.0266719,"ts. This is discussed in §7. Our algorithm resembles the method of Koskenniemi (1990) and later work. The difference is that there lattices are unweighted and may not contain a path that satisfies all constraints, so that the order of constraint intersection matters. 5 Semantic role labeling The semantic role labeling task (Carreras and M`arques, 2004) involves choosing instantiations of verb arguments from a sentence for a given verb. The verb and its arguments form a proposition. We use data from the CoNLL-2004 shared task—the PropBank (Palmer et al., 2005) annotations of the Penn Treebank (Marcus et al., 1993), with sections 15–18 as the training set and section 20 as the development set. Unless otherwise specified, all measurements are made on the development set. We follow Roth and Yih (2005) exactly, in order to compare system runtimes. They, in turn, follow Hacioglu et al. (2004) and others in labeling only the heads of syntactic chunks rather than all words. We label only the core arguments (A0–A5), treating ? (a) A0 A0 0 1 A0 A1 A2 A3 ? A5 A4 A3 A2 A1 A0 O O 0 ? 1 O (verb position) 2 • A RGUMENT CANDIDATES (Fig. 5) encodes a set of position spans each of which must receive only a single label"
N06-1054,J05-1004,0,0.0144635,"how to choose C from among multiple violated constraints. This is discussed in §7. Our algorithm resembles the method of Koskenniemi (1990) and later work. The difference is that there lattices are unweighted and may not contain a path that satisfies all constraints, so that the order of constraint intersection matters. 5 Semantic role labeling The semantic role labeling task (Carreras and M`arques, 2004) involves choosing instantiations of verb arguments from a sentence for a given verb. The verb and its arguments form a proposition. We use data from the CoNLL-2004 shared task—the PropBank (Palmer et al., 2005) annotations of the Penn Treebank (Marcus et al., 1993), with sections 15–18 as the training set and section 20 as the development set. Unless otherwise specified, all measurements are made on the development set. We follow Roth and Yih (2005) exactly, in order to compare system runtimes. They, in turn, follow Hacioglu et al. (2004) and others in labeling only the heads of syntactic chunks rather than all words. We label only the core arguments (A0–A5), treating ? (a) A0 A0 0 1 A0 A1 A2 A3 ? A5 A4 A3 A2 A1 A0 O O 0 ? 1 O (verb position) 2 • A RGUMENT CANDIDATES (Fig. 5) encodes a set of positi"
N06-1054,N04-1042,0,0.0129754,"ties. The situation improves with discriminatively trained models, such as conditional random fields (Lafferty et al., 2001), which do efficiently allow features that are functions of the entire observation sequence. However, these features can still only look locally at the label sequence. That is a significant shortcoming, because in many domains, hard or soft global constraints on the label sequence are motivated by common sense: • For named entity recognition, a phrase that appears multiple times should tend to get the same label each time (Finkel et al., 2005). • In bibliography entries (Peng and McCallum, 2004), a given field (author, title, etc.) should be filled by at most one substring of the input, and there are strong preferences on the cooccurrence and order of certain fields. • In seminar announcements, a given field (speaker, start time, etc.) should appear with at most one value in each announcement, although the field and value may be repeated (Finkel et al., 2005). • For semantic role labeling, each argument should be instantiated only once for a given verb. There are several other constraints that we will describe later (Roth and Yih, 2005). A popular approximate technique is to hypothes"
N06-1054,W04-3212,0,0.0643851,"Missing"
N07-1032,H05-1050,1,0.929858,"then be necessary in order to perform well on new test collections. In this paper, we show how to perform such a tuning in the context of unsupervised document clustering, by (i) introducing a degree of freedom, α, into two leading informationtheoretic clustering algorithms, through the use of generalized mutual information quantities; and (ii) selecting the value of α based on clusterings of similar, but supervised document collections (crossinstance tuning). One option is to perform a tuning that directly minimizes the error on the supervised data sets; another option is to use “strapping” (Eisner and Karakos, 2005), which builds a classifier that learns to distinguish good from bad clusterings, and then selects the α with the best predicted clustering on the test set. Experiments from the “20 Newsgroups” corpus show that, although both techniques improve the performance of the baseline algorithms, “strapping” is clearly a better choice for cross-instance tuning. ∗ This work was partially supported by the DARPA GALE program (Contract No HR0011-06-2-0001) and by the JHU WSE/APL Partnership¯ Fund. Introduction The problem of combining labeled and unlabeled examples in a learning task (semi-supervised learn"
N07-1032,P95-1026,0,0.116775,"h the best predicted clustering on the test set. Experiments from the “20 Newsgroups” corpus show that, although both techniques improve the performance of the baseline algorithms, “strapping” is clearly a better choice for cross-instance tuning. ∗ This work was partially supported by the DARPA GALE program (Contract No HR0011-06-2-0001) and by the JHU WSE/APL Partnership¯ Fund. Introduction The problem of combining labeled and unlabeled examples in a learning task (semi-supervised learning) has been studied in the literature under various guises. A variety of algorithms (e.g., bootstrapping (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), alternating structure optimization (Ando and Zhang, 2005), etc.) have been developed in order to improve the performance of supervised algorithms, by automatically extracting knowledge from lots of unlabeled examples. Of special interest is the work of Ando and Zhang (2005), where the goal is to build many supervised auxiliary tasks from the unsupervised data, by creating artificial labels; this procedure helps learn a transformation of the input space that captures the relatedness of the auxiliary problems to the task at hand. In essence, Ando and Zhan"
N07-1033,P04-1034,0,0.0351681,"Missing"
N07-1033,P04-1035,0,0.184996,"ing the margin constraint if necessary. The parameter C > 0 controls the cost of taking such slack, and should generally be lower for noisier or less linearly separable datasets. We add the contrast constraints → (∀i, j) w ~ · (− xi − − v→ ij ) · yi ≥ µ(1 − ξij ), X X 1 kwk ~ 2 + C( ξi ) + Ccontrast ( ξij ) 2 Rationale Annotation for Movie Reviews In order to demonstrate that annotator rationales help machine learning, we needed annotated data that included rationales for the annotations. We chose a dataset that would be enjoyable to reannotate: the movie review dataset of (Pang et al., 2002; Pang and Lee, 2004).3 The dataset consists of 1000 positive and 1000 negative movie reviews obtained from the Internet Movie Database (IMDb) review archive, all written before 2002 by a total of 312 authors, with a cap of 20 reviews per author per 2 Taking Ccontrast to be constant means that all rationales are equally valuable. One might instead choose, for example, to reduce Ccontrast for examples xi that have many rationales, to prevent xi ’s contrast examples vij from together dominating the optimization. However, in this paper we assume that an xi with more rationales really does provide more evidence about"
N07-1033,W02-1011,0,0.0166184,"xi to miss satisfying the margin constraint if necessary. The parameter C > 0 controls the cost of taking such slack, and should generally be lower for noisier or less linearly separable datasets. We add the contrast constraints → (∀i, j) w ~ · (− xi − − v→ ij ) · yi ≥ µ(1 − ξij ), X X 1 kwk ~ 2 + C( ξi ) + Ccontrast ( ξij ) 2 Rationale Annotation for Movie Reviews In order to demonstrate that annotator rationales help machine learning, we needed annotated data that included rationales for the annotations. We chose a dataset that would be enjoyable to reannotate: the movie review dataset of (Pang et al., 2002; Pang and Lee, 2004).3 The dataset consists of 1000 positive and 1000 negative movie reviews obtained from the Internet Movie Database (IMDb) review archive, all written before 2002 by a total of 312 authors, with a cap of 20 reviews per author per 2 Taking Ccontrast to be constant means that all rationales are equally valuable. One might instead choose, for example, to reduce Ccontrast for examples xi that have many rationales, to prevent xi ’s contrast examples vij from together dominating the optimization. However, in this paper we assume that an xi with more rationales really does provide"
N12-1013,P11-1040,0,0.0536695,"Missing"
N12-1013,H05-1045,0,0.0115505,"danger that this will end up hurting performance through approximations. In this paper we illustrate how to address this problem, even for extremely interconnected models in which every pair of output variables is connected. Introduction Conditional Random Fields (CRFs) (Lafferty et al., 2001) are often used to model dependencies among linguistic variables. CRF-based models have improved the state of the art in a number of natural language processing (NLP) tasks ranging from partof-speech tagging to information extraction and sentiment analysis (Lafferty et al., 2001; Peng and McCallum, 2006; Choi et al., 2005). Robust and theoretically sound training procedures have been developed for CRFs when the model can be used with exact inference and decoding.1 However, some NLP problems seem to 1 “Inference” typically refers to computing posterior marginal or max-marginal probability distributions of output random variables, given some evidence. “Decoding” derives a single structured output from the results of inference. Wainwright (2006) showed that if approximate inference will be used at test time, it may be beneficial to use a learning procedure that does not converge to the true model but to one that p"
N12-1013,D09-1011,1,0.910162,"Missing"
N12-1013,P05-1045,0,0.00904471,"results of Thomas et al. (2006). Their test-time Min-Cut algorithm is exact in this case: binary variables and a two-way classification. 4.2 Information Extraction from Semi-Structured Text We utilize the CMU seminar announcement corpus of Freitag (2000) consisting of emails with seminar announcements. The task is to extract four fields that describe each seminar: speaker, location, start time and end time. The corpus annotates the document with all mentions of these four fields. Sequential CRFs have been used successfully for semi-structured information extraction (Sutton and McCallum, 2005; Finkel et al., 2005). However, they cannot model non-local dependencies in the data. For example, in the seminar announcements corpus, if “Sutner” is mentioned once in an email in a context that identifies him as a speaker, it is 125 likely that other occurrences of “Sutner” in the same email should be marked as speaker. Hence Finkel et al. (2005) and Sutton and McCallum (2005) propose adding non-local edges to a sequential CRF to represent soft consistency constraints. The model, called a “skip-chain CRF” and shown in Figure 2, contains a factor linking each pair of capitalized words with the same lexical form."
N12-1013,N10-1112,0,0.0141145,"dient in this way, and training the CRF with some gradient-based optimization method, has been shown to work relatively well in practice (Vishwanathan et al., 2006; Sutton and McCallum, 2005). The above method takes into account neither the loss function that will be used for evaluation, nor the approximate algorithms that have been selected for inference and decoding at test time. Other structure learning methods do consider loss, though it is not obvious how to make them consider approximations. Those include maximum margin (Taskar et al., 2003; Finley and Joachims, 2008) and softmaxmargin (Gimpel and Smith, 2010). The idea of margin-based methods is to choose weights θ~ so that the correct alternative yi∗ always gets a better score than each possible alternative yi ∈ Y. The loss is incorporated in these methods by requiring the margin (θ~ · f~(xi , yi∗ ) − θ~ · f~(xi , yi )) ≥ `(yi , yi∗ ), with penalized slack in these constraints. The softmaxmargin method uses a different criterion—it resembles MLE P but modifies the denominator of (1) to Zx = y0 ∈Y exp(θ~ · f~(x, y0 ) + `(y0 , y∗ )). In our experiments we compare against MLE training (which is common) and softmax-margin, which incorporates loss and"
N12-1013,P11-1115,0,0.0313528,"Missing"
N12-1013,D09-1005,1,0.873447,"nction that will be used during testing. Thus, we want θ to minimize the expected loss under the true data distribution P : argmin Exy∼P [`(δθ (x), y)] (4) θ where δθ is the decision rule (parameterized by θ), which decodes the results of inference under pθ . In practice, we do not know the true data distribution, but we can do empirical risk minimization (ERM), instead averaging the loss over our sample of (xi , yi ) pairs. ERM for structured prediction was first introduced in the speech community (Bahl et al., 1988) and later used in NLP (Och, 2003; Kakade et al., 2002; Suzuki et al., 2006; Li and Eisner, 2009, etc.). Previous applications of risk minimization assume exact inference, having defined the hypothesis space by a precomputed n-best list, lattice, or packed forest over which exact inference is possible. The ERMA approach (Stoyanov et al., 2011) works with approximate inference and computes exact gradients of the output loss (or a differentiable surrogate) in the context of the approximate inference and decoding algorithms. To determine the gradient of `(δθ (xi ), yi ) with respect to θ, the method relies on automatic differentiation in the reverse mode (Griewank and Corliss, 1991), a gene"
N12-1013,P03-1021,0,0.0972134,"ate inference and decoding algorithms and the loss function that will be used during testing. Thus, we want θ to minimize the expected loss under the true data distribution P : argmin Exy∼P [`(δθ (x), y)] (4) θ where δθ is the decision rule (parameterized by θ), which decodes the results of inference under pθ . In practice, we do not know the true data distribution, but we can do empirical risk minimization (ERM), instead averaging the loss over our sample of (xi , yi ) pairs. ERM for structured prediction was first introduced in the speech community (Bahl et al., 1988) and later used in NLP (Och, 2003; Kakade et al., 2002; Suzuki et al., 2006; Li and Eisner, 2009, etc.). Previous applications of risk minimization assume exact inference, having defined the hypothesis space by a precomputed n-best list, lattice, or packed forest over which exact inference is possible. The ERMA approach (Stoyanov et al., 2011) works with approximate inference and computes exact gradients of the output loss (or a differentiable surrogate) in the context of the approximate inference and decoding algorithms. To determine the gradient of `(δθ (xi ), yi ) with respect to θ, the method relies on automatic different"
N12-1013,N03-1028,0,0.0890851,"oximation) but substitute the approximate posterior marginals of p as computed by BP. For example, if the loss of y is the number of incorrectly recovered output variables, MBR says to separately pick the most probable value for each output variable, according to its (approximate) marginal. y0 This is the minimum Bayes risk (MBR) principle from statistical decision theory: choose y to minimize the expected loss (i.e., the risk) according to the CRF’s posterior beliefs given x. In the NLP literature, CRFs are often decoded by choosing y to be the maximum posterior probability assignment (e.g., Sha and Pereira (2003), Sutton et al. (2007)). This is the MBR procedure for the 0-1 loss function that simply tests whether y = y∗ . For other loss functions, however, the corresponding MBR procedure is preferable. For some loss functions it is tractable given the posterior marginals of p, while in other cases approximations are needed. 122 i The gradient of each summand log pθ (yi∗ |xi ) can be computed by performing inference in two settings, one with xi , yi∗ observed and one with only the conditioning events xi observed. The gradient emerges as the difference between the feature expectations in the two cases."
N12-1013,P06-2101,1,0.855194,"specifically showed how to train a system that will use sum-product BP for inference at test time (unlike margin-based methods). This may be advantageous for some tasks because it marginalizes over latent variables. However, it is popular and sometimes faster to do 1-best decoding, so we also include experiments where the test-time system returns a 1best value of y (or an approximation to this if the CRF is loopy), based on max-product BP inference. Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). In the annealed case we evaluate (4) and its gradient under sum-product BP, except that we perform inference under p(θ/T ) instead of pθ . We gradually reduce the temperature T ∈ R from 1 to 0 as training proceeds, which turns sum-product inference into max-product by moving all the probability mass toward the highest-scoring assignment. 4 Modeling Natural Language with CRFs This section describes three NLP problems that can be naturally modeled with approximate CRFs. The first problem, modeling congressional votes, has not been previously modeled with a CRF. We show that by switching to the"
N12-1013,D08-1016,1,0.874256,"Missing"
N12-1013,P06-1028,0,0.131599,"ithms and the loss function that will be used during testing. Thus, we want θ to minimize the expected loss under the true data distribution P : argmin Exy∼P [`(δθ (x), y)] (4) θ where δθ is the decision rule (parameterized by θ), which decodes the results of inference under pθ . In practice, we do not know the true data distribution, but we can do empirical risk minimization (ERM), instead averaging the loss over our sample of (xi , yi ) pairs. ERM for structured prediction was first introduced in the speech community (Bahl et al., 1988) and later used in NLP (Och, 2003; Kakade et al., 2002; Suzuki et al., 2006; Li and Eisner, 2009, etc.). Previous applications of risk minimization assume exact inference, having defined the hypothesis space by a precomputed n-best list, lattice, or packed forest over which exact inference is possible. The ERMA approach (Stoyanov et al., 2011) works with approximate inference and computes exact gradients of the output loss (or a differentiable surrogate) in the context of the approximate inference and decoding algorithms. To determine the gradient of `(δθ (xi ), yi ) with respect to θ, the method relies on automatic differentiation in the reverse mode (Griewank and C"
N12-1013,W06-1639,0,0.256794,"essional votes, has not been previously modeled with a CRF. We show that by switching to the principled CRF framework we can learn models that are much more accurate when evaluated on test data, though using the same (or less expressive) features as previous work. The other two problems, information extraction from semistructured text and collective multi-label classification, have been modeled with loopy CRFs before. For all three models, we show that ERMA training results in better test set performance.3 4.1 Modeling Congressional Votes The Congressional Vote (ConVote) corpus was created by Thomas et al. (2006) to study whether votes of U.S. congressional representatives can be predicted from the speeches they gave when debating a bill. The corpus consists of transcripts of congressional floor debates split into speech segments. Each speech segment is labeled with the representative who is speaking and the recorded vote of that representative on the bill. We aim to predict a high percentage of the recorded votes correctly. Speakers often reference one another (e.g., “I thank the gentleman from Utah”), to indicate agreement or disagreement. The ConVote corpus manually annotates each phrase such as “t"
N12-1014,P03-1006,0,0.166474,") of those paths as in ordinary EM (see above). Running the forward algorithm P on the resulting lattice would compute E c(w) t p(w, t), P whose log is log E p(w, t) rather than our Pc(w) t desired Ec(w) log t p(w, t). Instead, we use c in section 3.2 to construct a variational family cqφ . We then show in sections 3.3–3.5 how to compute and locally maximize the variational lower bound (5). 3.1 Modeling a corpus with n-gram counts n-gram backoff language models have been used for decades in automatic speech recognition and statistical machine translation. We follow the usual FSA construction (Allauzen et al., 2003). The state of a 5gram FSA model c(w) must remember the previous 4-gram. For example, it would include an arc from state defg (the previous 4-gram) to state efgh with label h and weight c(h |defg). Then, with appropriate handling of boundary conditions, a sentence w = . . . defghi . . . is accepted along a single path of weight c(w) = · · · c(h |defg) · c(i |efgh) · · · . Arcs (a) w Time flies like an arrow w:N (b) p(w,t) w:V Start V Stop N w:V w:DT like : Prep flies : V Time : V (c) w o p(w,t) Start Time : N V N flies : N V flies : V flies : N like : V like : Prep N w:V Prep V an : DT an : DT"
N12-1014,D07-1090,0,0.0261061,"ginal corpus. If one lacks the resources to harvest the web, the Google n-gram corpus was derived from over a trillion words of English web text. Privacy or copyright issues may prevent access, but one may still be able to work with n-gram statistics: Michel et al. (2010) used such statistics from 5 million scanned books. Several systems use n-gram counts (Bergsma et al., 2009; Lin et al., 2009) or other web statistics (Lapata and Keller, 2005) as features within a classifier. A large language model from ngram counts yields an effective prior over hypotheses in tasks like machine translation (Brants et al., 2007). We similarly construct an n-gram model, but treat it as the primary training data whose structure is to be explained by the generative HMM. Thus our criterion does not explain the n-grams in isolation, but rather tries to explain the likely full sentences w that the model reconstructed from overlapping ngrams. This is something like shotgun sequencing, in which likely DNA strings are reconstructed from overlapping short reads (Staden, 1979); however, we train an HMM on the resulting distribution rather than merely trying to find its mode. Finally, unsupervised HMM training discovers latent s"
N12-1014,U09-1010,0,0.012962,"on). φ is the collection of all multinomial parameters. If nq = np , then our variational gap can be made 0 as in ordinary non-variational EM (see section 3.5). In our experiments, however, we save memory by choosing nq = 1. Thus, our variational gap is tight to the extent that a word’s POS tag under the model pθ is conditionally independent of previous tags and the rest of the sentence, given an n-word window.4 This is the assumption made by local classification models (Punyakanok et al., 2005; Toutanova and Johnson, 2007). Note that it is milder than the “one tagging per n-gram” hypothesis (Dawborn and Curran, 2009; Lin et al., 2009), which claims that each 5-gram (and therefore each sentence!) is unambiguous as to its full tagging. In contrast, we allow that a tag may be ambiguous even given an n-word window; we merely suppose that there is no further disambiguating information accessible to pθ .5 We can encode the resulting cq(w, t) as an FST. With nq = 1, the states of cq are isomorphic to the states of c. However, an arc in c from defg with label h and weight 0.2 is replaced in cq by several arcs—one per tag t—with label h : t and weight 0.2 · qφ (t |defgh).6 We remark that an encoding of 3 A condit"
N12-1014,P02-1001,1,0.804321,"and φ. We exploit our representation of cqφ as an FSM over the (+, ×) semiring. The path weights represent a probability distribution over the paths. In general, it is efficient to compute the expected value of a random FSM path, for any definition of value that decomposes additively over the path’s arcs. The approach is to apply the forward algorithm to a version of cqφ where we now regard each arc as weighted by an ordered pair of real numbers. The (+, ×) operations for combining weights (section 3) are replaced with the operations of an “expectation semiring” whose elements are such pairs (Eisner, 2002). Suppose we want to find Ecqφ (w,t) log qφ (t |w). To reduce this to an expected value problem, we must assign a value to each arc of cqφ such that the c is Figure 1a, splitting its states with nq = 2 would yield a cq with a topology like Figure 1c, but with each arc having an independent variational parameter. 7 One could increase the number of arcs and hence variational parameters by splitting the states of cq to remember more history. In particular, one could increase the width nq of the tag window, or one could increase the width of the word window by splitting states of c (without changi"
N12-1014,P08-1085,0,0.0158928,"gure 1. All transitions are allowed, but not all emissions. If a word is listed in a provided “dictionary” with its possible tags, then other tags are given 0 probability of emitting that word. The EM algorithm uses the corpus to learn transition and emission probabilities that explain the data under this constraint. The constraint ensures that the learned states have something to do with true POS tags. Merialdo (1994) spawned a long line of work on this task. Ideas have included Bayesian learning methods (MacKay, 1997; Goldwater and Griffiths, 2007; Johnson, 2007), better initial parameters (Goldberg et al., 2008), and learning how to constrain the possible parts of speech for a word (Ravi and Knight, 2008), as well as non-HMM sequence models (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007). Most of this work has used the Penn Treebank (Marcus et al., 1993) as a dataset. While this million-word Wall Street Journal (WSJ) corpus is one of the largest that is manually annotated with parts of speech, unsupervised learning methods could take advantage of vast amounts of unannotated text. In practice, runtime concerns have sometimes led researchers to use small subsets of the P"
N12-1014,P07-1094,0,0.114591,"coordinate ascent by updating one φ block at a time (and then recomputing ra values for all blocks, which is slow), one can take an approximate step by updating all blocks in parallel. We find that replacing the E-step with a single parallel step still tends to improve the objective, and that this approximate variational EM is faster than gradient ascent with comparable results.21 4 4.1 Experiments Constrained unsupervised HMM learning We follow the unsupervised POS tagging setup of Merialdo (1994) and many others (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Johnson, 2007). Given a corpus of sentences, one seeks the maximum-likelihood or MAP parameters of a bigram HMM (np = 2). The observed sentences, for qφ (t |hc w, hq ) to the probability that t begins with t if we randomly draw a suffix w ∼ c(· |hc w) and randomly tag ww with t ∼ pθ (· |ww, hq ). This is equivalent to using pθ with the backward algorithm to conditionally tag each possible suffix. 19 The first component of α ˆ za βˆza is αza βza = αza · 1. 20 cqφ then ∂ r¯/∂φa is the second component PIf a is an arc of ˆza /∂φa )βˆza . Then ∂LA /∂φa works out to of z∈Za α ˆ za (∂ k P z∈Za ca"
N12-1014,N06-1041,0,0.115677,"a + z∈Za αza log θza )/αa ).20 Rather than doing block coordinate ascent by updating one φ block at a time (and then recomputing ra values for all blocks, which is slow), one can take an approximate step by updating all blocks in parallel. We find that replacing the E-step with a single parallel step still tends to improve the objective, and that this approximate variational EM is faster than gradient ascent with comparable results.21 4 4.1 Experiments Constrained unsupervised HMM learning We follow the unsupervised POS tagging setup of Merialdo (1994) and many others (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Johnson, 2007). Given a corpus of sentences, one seeks the maximum-likelihood or MAP parameters of a bigram HMM (np = 2). The observed sentences, for qφ (t |hc w, hq ) to the probability that t begins with t if we randomly draw a suffix w ∼ c(· |hc w) and randomly tag ww with t ∼ pθ (· |ww, hq ). This is equivalent to using pθ with the backward algorithm to conditionally tag each possible suffix. 19 The first component of α ˆ za βˆza is αza βza = αza · 1. 20 cqφ then ∂ r¯/∂φa is the second component PIf a is an arc of ˆza /∂φa )βˆza"
N12-1014,D07-1031,0,0.187912,"i (where n is as large as possible such that this n-gram is in our training collection). φ is the collection of all multinomial parameters. If nq = np , then our variational gap can be made 0 as in ordinary non-variational EM (see section 3.5). In our experiments, however, we save memory by choosing nq = 1. Thus, our variational gap is tight to the extent that a word’s POS tag under the model pθ is conditionally independent of previous tags and the rest of the sentence, given an n-word window.4 This is the assumption made by local classification models (Punyakanok et al., 2005; Toutanova and Johnson, 2007). Note that it is milder than the “one tagging per n-gram” hypothesis (Dawborn and Curran, 2009; Lin et al., 2009), which claims that each 5-gram (and therefore each sentence!) is unambiguous as to its full tagging. In contrast, we allow that a tag may be ambiguous even given an n-word window; we merely suppose that there is no further disambiguating information accessible to pθ .5 We can encode the resulting cq(w, t) as an FST. With nq = 1, the states of cq are isomorphic to the states of c. However, an arc in c from defg with label h and weight 0.2 is replaced in cq by several arcs—one per t"
N12-1014,D09-1005,1,0.943694,"of the form [hc , hq ], where hc is a state of c (e.g., an (n − 1)-word history) and hq is an (nq − 1)-tag history. We saw in the previous section that an arc a leaving this state, and labeled with w : t where w is a word and t is a tag, will def have a weight of the form ka = c(w |hc )φa where def def φa = qφ (t |hc w, hq ). We now let the value va = log φa .9 Then, Q just as the weight of a path accepting (w, t) is a ka = cqφ (w, t), the value of that path P is a va = log qφ (t |w), as desired. To compute the expected value r¯ over all paths, we follow a generalized forward-backward recipe (Li and Eisner, 2009, section 4.2). First, run the forward and backward algorithms over cqφ .10 Now the expected P value is a sum over all arcs of cqφ , namely r¯ = a αa ka va βa , where αa denotes the forward probability of arc a’s source state and βa denotes the backward probability of arc a’s target state. Now, in fact, the expectation we need to compute is not Ecqφ (w,t) log qφ (t |w) but rather equation (5). So the value va of arc a should not actually be def log φa but rather log θa − log φa where θa = pθ (t | 8 The total value is then the sum of the logs, i.e., the log of the product. This works because qφ"
N12-1014,J93-2004,0,0.0412397,"plain the data under this constraint. The constraint ensures that the learned states have something to do with true POS tags. Merialdo (1994) spawned a long line of work on this task. Ideas have included Bayesian learning methods (MacKay, 1997; Goldwater and Griffiths, 2007; Johnson, 2007), better initial parameters (Goldberg et al., 2008), and learning how to constrain the possible parts of speech for a word (Ravi and Knight, 2008), as well as non-HMM sequence models (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007). Most of this work has used the Penn Treebank (Marcus et al., 1993) as a dataset. While this million-word Wall Street Journal (WSJ) corpus is one of the largest that is manually annotated with parts of speech, unsupervised learning methods could take advantage of vast amounts of unannotated text. In practice, runtime concerns have sometimes led researchers to use small subsets of the Penn Treebank (Goldwater and Griffiths, 2007; Smith and Eisner, 2005; Haghighi and Klein, 2006). Our goal is to point the way to using even larger datasets. The reason for all this past research is that (Merialdo, 1994) was a negative result: while EM is guaranteed to improve the"
N12-1014,J94-2001,0,0.709183,"to scale up to the Google n-gram corpus (Brants and Franz, 2006) and learn a more detailed, explanatory joint model of tags, syntactic dependencies, and topics. Our intuition here is that web-scale data may be needed to learn the large number of lexically and contextually specific parameters. ∗ Work was supported in part by NSF grant No. 0347822. 1.1 Formulation Let w (“words”) denote an observation sequence, and let t (“tags”) denote a hidden HMM state sequence that may explain w. This terminology is taken from the literature on inducing part-of-speech (POS) taggers using a first-order HMM (Merialdo, 1994), which we use as our experimental setting. Maximum a posteriori (MAP) training of an HMM pθ seeks parameters θ to maximize X X N· c(w) log pθ (w, t) + log Pr prior (θ) (1) w t where c is an empirical distribution that assigns probability 1/N to each of the N sentences in a training corpus. Our technical challenge is to generalize this MAP criterion to other, structured distributions c that compactly approximate the corpus. Specifically, we address the case where c is given by any probabilistic FSA, such as a backoff language model—that is, a variable-order Markov model estimated from corpus s"
N12-1014,J00-1003,0,0.0602744,"pecified by weighted automata and grammars. We regard parameter estimation from such a distribution c (rather than from a sample) as a natural question. Previous work on modeling c with a distribution from another family was motivated by approximating a grammar or 131 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 131–141, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics model rather than generalizing from a dataset, and hence removed latent variables while adding parameters (Nederhof, 2000; Mohri and Nederhof, 2001; Liang et al., 2008), whereas we do the reverse. Second, in practice, one may want to incorporate massive amounts of (possibly out-of-domain) data in order to get better coverage of phenomena. Massive datasets usually require a simple model (given a time budget). We propose that it may be possible to use a lot of data and a good model by reducing the accuracy of the data representation instead. While training will become more complicated, it can still result in an overall speedup, because a frequent 5gram collapses into a single parameter of the estimated distributio"
N12-1014,P05-1044,1,0.935572,"s proportional to exp((ra + z∈Za αza log θza )/αa ).20 Rather than doing block coordinate ascent by updating one φ block at a time (and then recomputing ra values for all blocks, which is slow), one can take an approximate step by updating all blocks in parallel. We find that replacing the E-step with a single parallel step still tends to improve the objective, and that this approximate variational EM is faster than gradient ascent with comparable results.21 4 4.1 Experiments Constrained unsupervised HMM learning We follow the unsupervised POS tagging setup of Merialdo (1994) and many others (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Johnson, 2007). Given a corpus of sentences, one seeks the maximum-likelihood or MAP parameters of a bigram HMM (np = 2). The observed sentences, for qφ (t |hc w, hq ) to the probability that t begins with t if we randomly draw a suffix w ∼ c(· |hc w) and randomly tag ww with t ∼ pθ (· |ww, hq ). This is equivalent to using pθ with the backward algorithm to conditionally tag each possible suffix. 19 The first component of α ˆ za βˆza is αza βza = αza · 1. 20 cqφ then ∂ r¯/∂φa is the second component PIf a i"
N12-1014,P00-1073,0,0.051943,"is corpus is not much bigger than the 5-gram language model built from it (at our current pruning level), and so the overhead of the more complex n-gram EM method is a net disadvantage. However, when moving to larger corpora, the iterations of n-gram EM become as fast as standard EM and then faster. We expect this trend to continue as one moves to much larger datasets, as the compression ratio of the pruned language model relative to the original corpus will only improve. The Google n-gram corpus is based on 50× more data than our largest but could be handled in RAM. 22 Entropy-based pruning (Stolcke, 2000) may be a better selection method when one is in a position to choose. However, count cutoffs were already used in the creation of the Google n-gram corpus, and more complex methods of pruning may not be practical for very large datasets. 139 84 82 Accuracy We investigate how much performance degrades when we approximate the corpus and train approximately with nq = 1. We examine two measures: likelihood on a held-out corpus and accuracy in POS tagging. We train on corpora of three different sizes: • WSJ-big (910k words → 441k n-grams @ cutoff 3), • Giga-20 (20M words → 2.9M n-grams @ cutoff 10"
N12-1014,P09-1057,0,\N,Missing
N12-1024,P03-1006,0,0.0917783,"ing the n-grams should occur. In a long string, we might want to encourage or discourage an n-gram in a certain “region” of the string. Our features can only encourage or discourage it everywhere in the string, which may lead to slow convergence. Nevertheless, in our particular experimental settings, we find that this works better than other topologies we have considered. Sparse N-Gram Encoding A full n-gram language model requires ≈ |Σ|n arcs to encode as a WFSA. This could be quite expensive. Fortunately, large n-gram models can be compacted by using failure arcs (φ-arcs) to encode backoff (Allauzen et al., 2003). These arcs act as -transitions that can be taken only when no other transition is available. They allow us to encode the sparse subset of ngrams that have nonzero Lagrangians. We encode G such that all features whose λ value is 0 will back off to the next largest n-gram having nonzero weight. 236 This form of G still accepts Σ∗ and has the same weights as a dense representation, but could require substantially fewer states. 4.2 Incrementally Expanding G As mentioned above, we may need to alter G as we go along. Intuitively, we may want to start with features that are cheap to encode, to mov"
N12-1024,N03-1003,0,0.0538035,"s for a fixed set of n-grams. timality. Even in instances where approximate algorithms perform well, it could be useful to have a true optimality guarantee. For example, our algorithm can be used to produce reference solutions, which are important to have for research purposes. Under a sum-of-pairs Levenshtein objective, the exact multi-sequence alignment can be directly obtained from the Steiner consensus string and vice versa (Gusfield, 1997). This implies that our exact algorithm could be also used to find exact multisequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al., 2006) that is almost always solved with approximate methods. We have noted that some constraints are more useful than others. Position-specific information is hard to agree on and leads to slow convergence, while pure n-gram constraints do not work as well for long strings where the position may be important. One avenue we are investigating is the use of a non-deterministic G, which would allow us to encode latent variables (Dreyer et al., 2008), such as loosely defined “regions” within a string, and to allow for the encoding of alignments between the"
N12-1024,D09-1011,1,0.89767,"Missing"
N12-1024,D08-1113,1,0.903211,"Missing"
N12-1024,knight-al-onaizan-1998-translation,0,0.0230236,"ion Many tasks in natural language processing involve functions that assign scores—such as logprobabilities—to candidate strings or sequences. Often such a function can be represented compactly as a weighted finite state automaton (WFSA). Finding the best-scoring string according to a WFSA is straightforward using standard best-path algorithms. It is common to construct a scoring WFSA by combining two or more simpler WFSAs, taking advantage of the closure properties of WFSAs. For example, consider noisy channel approaches to speech recognition (Pereira and Riley, 1997) or machine translation (Knight and Al-Onaizan, 1998). Given an input f , the score of a possible English transcription or translation e is the sum of its language model score log p(e) and its channel model score log p(f |e). If each of these functions of e is represented as a WFSA, then their sum is represented as the intersection of those two WFSAs. WFSA intersection corresponds to constraint conjunction, and hence is often a mathematically natural way to specify a solution to a problem involving ∗ The authors are grateful to Damianos Karakos for providing tools and data for the ASR experiments. This work was supported in part by an NSF Gradua"
N12-1024,D10-1125,0,0.0738401,"hip. multiple soft constraints on a desired string. Unfortunately, the intersection may be computationally inefficient in practice. The intersection of K WFSAs having n1 , n2 , . . . , nK states may have n1 ·n2 · · · nK states in the worst case.1 In this paper, we propose a more efficient method for finding the best path in an intersection without actually computing the full intersection. Our approach is based on dual decomposition, a combinatorial optimization technique that was recently introduced to the vision (Komodakis et al., 2007) and language processing communities (Rush et al., 2010; Koo et al., 2010). Our idea is to interrogate the several WFSAs separately, repeatedly visiting each WFSA to seek a high-scoring path in each WFSA that agrees with the current paths found in the other WSFAs. This iterative negotiation is reminiscent of message-passing algorithms (Sontag et al., 2008), while the queries to the WFSAs are reminiscent of loss-augmented inference (Taskar et al., 2005). We remark that a general solution whose asymptotic worst-case runtime beat that of naive intersection would have important implications for complexity theory (Karakostas et al., 2003). Our approach is not such a solu"
N12-1024,P11-1008,0,0.0157752,"be solved independently. If we can somehow combine the solutions from the subproblems into a “valid” solution to the global problem, then we can avoid optimizing the joint problem directly. A valid solution is one in which the individual solutions of each subproblem all agree on the variables which are shared in the joint problem. For example, if we are combining a parser with a part-of-speech tagger, the tag assignments from both models must agree in the final solution (Rush et al., 2010); if we are intersecting a translation model with a language model, then it is the words that must agree (Rush and Collins, 2011). More formally, suppose we want to find a global solution that is jointly PKoptimized among K subproblems: argminx k=1 fk (x). Suppose that x ranges over vectors. Introducing an auxiliary variable xk for each subproblem fk allows us to equivalently formulate this as the following constrained optimization problem: min {x1 ,...,xK } K X fk (xk ) + λk · xk (2) k=1 where the Lagrange multiplier vectors λk can be used to penalize solutions that do not satisfy the agreement constraints (∀k) xk = x. Our goal is to maximize this lower bound and hope that the result does satisfy the constraints. The g"
N12-1024,D10-1001,0,0.418378,"te Research Fellowship. multiple soft constraints on a desired string. Unfortunately, the intersection may be computationally inefficient in practice. The intersection of K WFSAs having n1 , n2 , . . . , nK states may have n1 ·n2 · · · nK states in the worst case.1 In this paper, we propose a more efficient method for finding the best path in an intersection without actually computing the full intersection. Our approach is based on dual decomposition, a combinatorial optimization technique that was recently introduced to the vision (Komodakis et al., 2007) and language processing communities (Rush et al., 2010; Koo et al., 2010). Our idea is to interrogate the several WFSAs separately, repeatedly visiting each WFSA to seek a high-scoring path in each WFSA that agrees with the current paths found in the other WSFAs. This iterative negotiation is reminiscent of message-passing algorithms (Sontag et al., 2008), while the queries to the WFSAs are reminiscent of loss-augmented inference (Taskar et al., 2005). We remark that a general solution whose asymptotic worst-case runtime beat that of naive intersection would have important implications for complexity theory (Karakostas et al., 2003). Our approach"
N15-1094,P03-1006,0,0.882486,"tate acceptor: that is, any v ∈ Σ∗ has exactly one accepting path in A. For each arc or final state a in A, we can define a feature function “How 2 Provided that we include special n-grams that match at the boundaries of v. See Appendix B.2 for details. many times is a used when A accepts v?” Thus, f (v) is again a vector of non-negative counts. Section 6 gives algorithms for this general setting. We implement the previous section as a special case, constructing A so that its arcs essentially correspond to the substrings in W. This encodes a variable-order Markov model as an FSA similarly to (Allauzen et al., 2003); see Appendix B.4 for details. In this general setting, E NCODE(θ) just returns a weighted version of A where each arc or final state a has weight exp θa in the (+, ×) semiring. Thus, this WFSA accepts each v with weight exp(θ · f (v)). 3.5 Adaptive featurization How do we choose W (or A)? Expanding W will allow better approximations to p—but at greater computational cost. We would like W to include just the substrings needed to approximate a given p well. For instance, if p is concentrated on a few highprobability strings, then a good W might contain those full strings (with positive weights"
N15-1094,D07-1093,0,0.526368,"nite-state distributions by variable-order n-gram models. On phonological inference problems, we obtain substantial speedup over previous related algorithms with no significant loss in accuracy. 1 Introduction Graphical models are well-suited to reasoning about linguistic structure in the presence of uncertainty. Such models typically use discrete random variables, where each variable ranges over a finite set of values such as words or tags. But a variable can also be allowed to range over an infinite space of discrete structures—in particular, the set of all strings, a case first explored by Bouchard-Côté et al. (2007). This setting arises because human languages make use of many word forms. These strings are systematically related in their spellings due to linguistic processes such as morphology, phonology, abbreviation, copying error and historical change. To analyze or predict novel strings, we can model the joint distribution of many related strings at once. Under a graphical model, the joint probability of an assignment tuple is modeled as a product of potentials on sub-tuples, each of which is usually modeled in turn by a weighted finite-state machine. In general, we wish to infer the values of unknow"
N15-1094,P14-2102,1,0.824667,"Missing"
N15-1094,P06-1039,0,0.0876408,"Missing"
N15-1094,D09-1011,1,0.89159,"ain graphical models are equivalent to cascades of finite-state transducers, which have long been used to model stepwise derivational processes such as speech production (Pereira and Riley, 1997) and transliteration (Knight and Graehl, 1998). Treeshaped graphical models have been used to model the evolution and speciation of word forms, in order to reconstruct ancient languages (Bouchard-Côté et al., 2007; Bouchard-Côté et al., 2008) and discover cognates in related languages (Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms (Cotterell et al., 2015). All of these graphical models, except Dreyer’s, happen to be directed ones. And all of these papers, except Bouchard-Côté’s, use deterministic inference methods—based on BP. 2.1 Graphical models over strings A directed or undirected graphical model describes a joint probability distribution over a set of random variables. To perform inference given a setting of the model parameters and observations of some variables, it is convenient to construct a factor graph (Kschischang et al., 2001). A fa"
N15-1094,D11-1057,1,0.937489,"equivalent to cascades of finite-state transducers, which have long been used to model stepwise derivational processes such as speech production (Pereira and Riley, 1997) and transliteration (Knight and Graehl, 1998). Treeshaped graphical models have been used to model the evolution and speciation of word forms, in order to reconstruct ancient languages (Bouchard-Côté et al., 2007; Bouchard-Côté et al., 2008) and discover cognates in related languages (Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms (Cotterell et al., 2015). All of these graphical models, except Dreyer’s, happen to be directed ones. And all of these papers, except Bouchard-Côté’s, use deterministic inference methods—based on BP. 2.1 Graphical models over strings A directed or undirected graphical model describes a joint probability distribution over a set of random variables. To perform inference given a setting of the model parameters and observations of some variables, it is convenient to construct a factor graph (Kschischang et al., 2001). A factor graph is a finite bip"
N15-1094,P02-1001,1,0.690842,"to p) divided by the expected count of ab. Such expected substring counts can be found by the method of Allauzen et al. (2003). For general A, we can use the method sketched by Li et al. (2009, footnote 9): intersect the WFSA for p with the unweighted FSA A, and then run the forwardbackward algorithm to determine the posterior count of each arc in the result. This tells us the expected total number of traversals of each arc in A, if we have kept track of which arcs in the intersection of p with A were derived from which arcs in A. That bookkeeping can be handled with an expectation semiring (Eisner, 2002), or simply with backpointers. Gradient ascent. For any given θ, we can use the WFSAs p and E NCODE(θ) to exactly compute Ev∼p [log qθ (v)] = −H(p, qθ ) (Cortes et al., 2006). We can tune θ to globally maximize this objective. The technique is to intersect p with E NCODE(θ), after lifting their weights into the expectation semiring via the mappings k 7→ hk, 0i and k 7→ h0, log ki respectively. Summing over all paths of this intersection via the forward algorithm yields hZ, ri where Z is the normalizing constant for p. We also sum over paths of E NCODE(θ) to get the normalizing constant Zθ . No"
N15-1094,P10-1105,0,0.674977,"ssociation for Computational Linguistics 2 Background Graphical models over strings are in fairly broad use. Linear-chain graphical models are equivalent to cascades of finite-state transducers, which have long been used to model stepwise derivational processes such as speech production (Pereira and Riley, 1997) and transliteration (Knight and Graehl, 1998). Treeshaped graphical models have been used to model the evolution and speciation of word forms, in order to reconstruct ancient languages (Bouchard-Côté et al., 2007; Bouchard-Côté et al., 2008) and discover cognates in related languages (Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms (Cotterell et al., 2015). All of these graphical models, except Dreyer’s, happen to be directed ones. And all of these papers, except Bouchard-Côté’s, use deterministic inference methods—based on BP. 2.1 Graphical models over strings A directed or undirected graphical model describes a joint probability distribution over a set of random variables. To perform inference given a setting of the model par"
N15-1094,D11-1032,0,0.533468,"tional Linguistics 2 Background Graphical models over strings are in fairly broad use. Linear-chain graphical models are equivalent to cascades of finite-state transducers, which have long been used to model stepwise derivational processes such as speech production (Pereira and Riley, 1997) and transliteration (Knight and Graehl, 1998). Treeshaped graphical models have been used to model the evolution and speciation of word forms, in order to reconstruct ancient languages (Bouchard-Côté et al., 2007; Bouchard-Côté et al., 2008) and discover cognates in related languages (Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms (Cotterell et al., 2015). All of these graphical models, except Dreyer’s, happen to be directed ones. And all of these papers, except Bouchard-Côté’s, use deterministic inference methods—based on BP. 2.1 Graphical models over strings A directed or undirected graphical model describes a joint probability distribution over a set of random variables. To perform inference given a setting of the model parameters and observation"
N15-1094,D12-1105,0,0.0201737,"cally, they minimized D(µF →V τ ||qθ τ ), where τ was a simple fixed function (a 0-gram model) included so that they were working with distributions (see section 4.3). This is very similar to our (7). Indeed, Hall and Klein (2010) found their procedure “reminiscent of EP,” hinting that τ was a surrogate for a real µV →F term. Dreyer and Eisner (2009) had also suggested EP as future work. EP has been applied only twice before in the NLP community. Daumé III and Marcu (2006) used EP for query summarization (following Minka and Lafferty (2003)’s application to an LDA model with fixed topics) and Hall and Klein (2012) used EP for rich parsing. However, these papers inferred a single structured variable connected to all factors (as in the traditional presentation of EP—see Appendix A), rather than inferring many structured variables connected in a sparse graphical model. We regard EP as a generalization of loopy BP for just this setting: graphical models with large or unbounded variable domains. Of course, we are not the first to use such a scheme; e.g., Qi (2005, chapter 2) applies EP to linear-chain models with both continuous and discrete hidden states. We believe that EP should also be broadly useful in"
N15-1094,D09-1005,1,0.879236,"es et al., 2006). We can tune θ to globally maximize this objective. The technique is to intersect p with E NCODE(θ), after lifting their weights into the expectation semiring via the mappings k 7→ hk, 0i and k 7→ h0, log ki respectively. Summing over all paths of this intersection via the forward algorithm yields hZ, ri where Z is the normalizing constant for p. We also sum over paths of E NCODE(θ) to get the normalizing constant Zθ . Now the desired objective is r/Z − log Zθ . Its gradient with respect to θ can be found by back-propagation, or equivalently by the forward-backward algorithm (Li and Eisner, 2009). An overlarge gradient step can leave the feasible space (footnote 1) by driving ZθV to ∞ and thus driving (2) to ∞ (Dreyer, 2011, section 2.8.2). In this case, we try again with reduced stepsize. 6 This method always yields a probabilistic FSA, i.e., the arc weights are locally normalized probabilities. This does not sacrifice any expressiveness; see Appendix B.7 for discussion. 6.1 Optimizing θ with a penalty Now consider the penalized objective (3). Ideally, Ω(θ) would count the number of nonzero weights in θ—or better, the number of arcs in E NCODE(θ). But it is not known how to efficient"
N15-1094,P09-1067,1,0.927324,"features To obtain our family Q, we must design f . Our strategy is to choose a set of “interesting” substrings W. For each w ∈ W, define a feature function “How many times does w appear as a substring of v?” Thus, f (v) is simply a vector of counts (nonnegative integers), indexed by the substrings in W. A natural choice of W is the set of all n-grams for fixed n. In this case, Q turns out to be equivalent to the family of n-gram language models.2 Already in previous work (“variational decoding”), we used (2) with this family to approximate WFSAs or weighted hypergraphs that arose at runtime (Li et al., 2009). Yet a fixed n is not ideal. If W is the set of bigrams, one might do well to add the trigram the— perhaps because the is “really” a bigram (counting the digraph th as a single consonant), or because the bigram model fails to capture how common the is under p. Adding the to W ensures that qθ will now match p’s expected count for this trigram. Doing this should not require adding all |Σ|3 trigrams. By including strings of mixed lengths in W we get variable-order Markov models (Ron et al., 1996). 3.4 Arbitrary FSA-based features More generally, let A be any unambiguous and complete finite-state"
N15-1094,D11-1139,0,0.202669,"by distributions from a fixed family—e.g., by trigram models. Each message update is found by minimizing a certain KL-divergence (Minka, 2001a). Second, we generalize to variable-order models. To do this, we augment EP’s minimization problem with a novel penalty term that keeps the number of n-grams finite. In general, we advocate penalizing more “complex” messages (in our setting, large finite-state acceptors). Complex messages are slower to construct, and slower to use in later steps. Our penalty term is formally similar to regularizers that encourage structured sparsity (Bach et al., 2011; Martins et al., 2011). Like a regularizer, it lets us use a more expressive family of distributions, secure in the knowledge that we will use only as many of the parameters as we really need for a “pretty good” fit. But why avoid using more parameters? Regularization seeks better generalization by not overfitting the model to the data. By contrast, we already have a model and are merely doing inference. We seek better runtime by not over-fussing about capturing the model’s marginal distributions. Our “penalized EP” (PEP) inference strategy is applicable to any graphical model with complex messages. In this paper,"
N15-1094,D13-1024,0,0.11146,"owever, `1 would not recognize that θ is simpler with the features {ab, abc, abd} than with the features {ab, pqr, xyz}. The former leads to a smaller WFSA encoding. In other words, it is cheaper to add abd once abc is already present, as a state already exists that represents the context ab. We would thus like the penalty to be the number of distinct prefixes in the set of nonzero features, |{u ∈ Σ∗ : (∃x ∈ Σ∗ ) θux 6= 0}|, (8) as this is the number of ordinary arcs in E NCODE(θ) (see Appendix B.4). Its convex surrogate is def Ω(θ) = X sX u∈Σ∗ x∈Σ∗ 2 θux (9) This tree-structured group lasso (Nelakanti et al., 2013) is an instance of group lasso (Yuan and Lin, 2006) where the string w = abd belongs to four groups, corresponding to its prefixes u = , u = a, u = ab, u = abd. Under group lasso, moving θw away from 0 increases Ω(θ) by λ|θw |(just as in `1 ) for each group in which w is the only nonzero feature. This penalizes for the new WFSA arcs needed for these groups. There are also increases due to w’s other groups, but these are smaller, especially for groups with many strongly weighted features. Our objective (3) is now the sum of a differentiable convex function (2) and a particular nondifferentiabl"
N15-1094,N12-1024,1,0.943634,"Active set method, showing the infinite tree of all features for the alphabet Σ = {a, b}. The green nodes currently have non-zero weights. The yellow nodes are on the frontier and are allowed to become non-zero, but the penalty function is still keeping them at 0. The red nodes are not yet considered, forcing them to remain at 0. a nearby point that improves the non-differentiable term. The proximal operator for tree-structured group lasso (9) can be implemented with an efficient recursive procedure (Jenatton et al., 2011). What if θ is ∞-dimensional because we allow all n-grams as features? Paul and Eisner (2012) used just this feature set in a dual decomposition algorithm. Like them, we rely on an active set method (Schmidt and Murphy, 2010). We fix abcd’s weight at 0 until abc’s weight becomes nonzero (if ever);7 only then does feature abc become “active.” Thus, at a given step, we only have to compute the gradient with respect to the currently nonzero features (green nodes in Figure 2) and their immediate children (yellow nodes). This hierarchical inclusion technique ensures that we only consider a small, finite subset of all n-grams at any given iteration of optimization. Closed-form with greedy g"
N15-1094,P06-1055,0,0.107808,"Missing"
N15-1094,Q15-1031,1,\N,Missing
N15-1094,J98-4003,0,\N,Missing
N16-1076,P16-1231,0,0.0213724,"te must also record a count in [0, 3] of immediately preceding INS edits. No INS edit arc is allowed from a state with counter 3. The counter is not considered by the arc weight function. et al. (2015) recently augmented the sequence-tosequence framework with a continuous analog of stack and queue data structures, to better handle long-range dependencies often found in linguistic data. Some recent papers have used LSTMs or BiLSTMs, as we do, to define probability distributions over action sequences that operate directly on an input sequence. Such actions are aligned to the input. For example, Andor et al. (2016) score edit sequences using a globally normalized model, and Dyer et al. (2015) evaluate the local probability of a parsing action given past actions (and their result) and future words. These architectures are powerful because their LSTMs can examine the output structure; but as a result they do not permit dynamic programming and must fall back on beam search. Our use of dynamic programming for efficient exact inference has long been common in non-neural architectures for sequence transduction, including FST systems that allow “phrasal” replacements s : t where |s|, |t |> 1 (Chen, 2003; Jiamp"
N16-1076,W02-1001,0,0.0326163,"atives of p(y∗ |x) with respect to the arc weights, essentially by using the forward-backward algorithm. We backpropagate further from these partials to find the gradient of (2) with respect to all our model parameters. We describe our gradient-based maximization procedure in section 10.3, along with regularization. Our model does not have to be trained with the conditional log likelihood objective. It could be trained with other objectives such as empirical risk or softmax-margin (Li and Eisner, 2009; Gimpel and Smith, 2010), or with error-driven updates such as in the structured perceptron (Collins, 2002). 7 Inference and Decoding For a new input x at test time, we can now construct a weighted FST, G, that defines a probability distribution over all aligned output strings. This can be manipulated to make various predictions about y∗ and its alignment. In our present experiments, we find the most probable (highest-weighted) path in G (Dijkstra, 1959), ˆ as our prediction. Note and use its output string y that Dijkstra’s algorithm is exact; no beam search is required as in some neural sequence models. ˆ may not be the most probOn the other hand, y able string—extracting that from a weighted FST"
N16-1076,P14-2102,1,0.806242,"output character immediately before the edit s : t, so the state label h0 is the history before the next edit, namely the final character of ht. For edits other than DEL, ht is a bigram of y, which can be evaluated (in context) by the arc weight function w = f (s, t, h, h0 , x, i, j). Naturally, a weighted version of this FST F is far too simple to do well on real NLP tasks (as we show in our experiments). The magic comes from instead weighting G so that we can pay attention to the input context γi:j . The above choice of F corresponds to the “(0, 1, 1) topology” in the more general scheme of Cotterell et al. (2014). For practical reasons, we actually modify it to limit the number of consecutive INS edits to 3.5 This trick bounds |y |to be < 4 · (|x |+ 1), ensuring that the pathsums in section 6 are finite regardless of the model parameters. This simplifies both the pathsum algorithm and the gradient-based training (Dreyer, 2011). Less importantly, since G becomes acyclic, Dijkstra’s algorithm in section 7 simplifies to the Viterbi algorithm. 9 Related Work Our model adds to recent work on linguistic sequence transduction using deep learning. Graves and Schmidhuber (2005) combined BiLSTMs with HMMs. Late"
N16-1076,D08-1113,1,0.935575,"Missing"
N16-1076,P15-1030,0,0.0142159,"ons (and their result) and future words. These architectures are powerful because their LSTMs can examine the output structure; but as a result they do not permit dynamic programming and must fall back on beam search. Our use of dynamic programming for efficient exact inference has long been common in non-neural architectures for sequence transduction, including FST systems that allow “phrasal” replacements s : t where |s|, |t |> 1 (Chen, 2003; Jiampojamarn et al., 2007; Bisani and Ney, 2008). Our work augments these FSTs with neural networks, much as others have augmented CRFs. In this vein, Durrett and Klein (2015) augment a CRF parser (Finkel et al., 2008) to score constituents with a feedforward neural network. Likewise, FitzGerald et al. (2015) employ feedforward nets as a factor in a graphical model for semantic role labeling. Many CRFs have incorporated feedforward neural networks (Bridle, 1990; Peng et al., 2009; Do and Artieres, 2010; Vinel et al., 2011; Fujii et al., 2012; Chen et al., 2015, and others). Some work augments CRFs with BiLSTMs: Huang et al. (2015) report results on part-of-speech tagging and named entity recognition with a linear-chain CRF-BiLSTM, and Kong et al. (2015) on Chinese"
N16-1076,P15-1033,0,0.00564781,"edit arc is allowed from a state with counter 3. The counter is not considered by the arc weight function. et al. (2015) recently augmented the sequence-tosequence framework with a continuous analog of stack and queue data structures, to better handle long-range dependencies often found in linguistic data. Some recent papers have used LSTMs or BiLSTMs, as we do, to define probability distributions over action sequences that operate directly on an input sequence. Such actions are aligned to the input. For example, Andor et al. (2016) score edit sequences using a globally normalized model, and Dyer et al. (2015) evaluate the local probability of a parsing action given past actions (and their result) and future words. These architectures are powerful because their LSTMs can examine the output structure; but as a result they do not permit dynamic programming and must fall back on beam search. Our use of dynamic programming for efficient exact inference has long been common in non-neural architectures for sequence transduction, including FST systems that allow “phrasal” replacements s : t where |s|, |t |> 1 (Chen, 2003; Jiampojamarn et al., 2007; Bisani and Ney, 2008). Our work augments these FSTs with"
N16-1076,P02-1001,1,0.777915,"s exact inference through dynamic programming. We illustrate our method on the tasks of morphological reinflection and lemmatization. 1 2 Introduction Mapping one character sequence to another is a structured prediction problem that arises frequently in NLP and computational linguistics. Common applications include grapheme-to-phoneme (G2P), transliteration, vowelization, normalization, morphology, and phonology. The two sequences may have different lengths. Traditionally, such settings have been modeled with weighted finite-state transducers (WFSTs) with parametric edge weights (Mohri, 1997; Eisner, 2002). This requires manual design of the transducer states and the features extracted from those states. Alternatively, deep learning has recently been tried Notation and Background Let Σx be a discrete input alphabet and Σy be a discrete output alphabet. Our goal is to define a conditional distribution p(y |x) where x ∈ Σ∗x and y ∈ Σ∗y and x and y may be of different lengths. We use italics for characters and boldface for strings. xi denotes the ith character of x, and xi:j denotes the substring xi+1 xi+2 · · · xj of length j − i ≥ 0. Note that xi:i = ε, the empty string. Let n = |x|. Our approac"
N16-1076,N16-1077,0,0.239616,"mportantly, since G becomes acyclic, Dijkstra’s algorithm in section 7 simplifies to the Viterbi algorithm. 9 Related Work Our model adds to recent work on linguistic sequence transduction using deep learning. Graves and Schmidhuber (2005) combined BiLSTMs with HMMs. Later, “sequence-to-sequence” models were applied to machine translation by Sutskever et al. (2014) and to parsing by Vinyals et al. (2015). That framework did not model any alignment between x and y, but adding an “attention” mechanism provides a kind of soft alignment that has improved performance on MT (Bahdanau et al., 2015). Faruqui et al. (2016) apply these methods to morphological reinflection (the only other application to morphology we know of). Grefenstette 5 This multiplies the number of states 4-fold, since each state must also record a count in [0, 3] of immediately preceding INS edits. No INS edit arc is allowed from a state with counter 3. The counter is not considered by the arc weight function. et al. (2015) recently augmented the sequence-tosequence framework with a continuous analog of stack and queue data structures, to better handle long-range dependencies often found in linguistic data. Some recent papers have used LS"
N16-1076,P08-1109,0,0.012501,"Missing"
N16-1076,D15-1112,0,0.00590533,"Missing"
N16-1076,N10-1112,0,0.0280597,"weights Eisner (2002) and Li and Eisner (2009) also explain how to compute the partial derivatives of p(y∗ |x) with respect to the arc weights, essentially by using the forward-backward algorithm. We backpropagate further from these partials to find the gradient of (2) with respect to all our model parameters. We describe our gradient-based maximization procedure in section 10.3, along with regularization. Our model does not have to be trained with the conditional log likelihood objective. It could be trained with other objectives such as empirical risk or softmax-margin (Li and Eisner, 2009; Gimpel and Smith, 2010), or with error-driven updates such as in the structured perceptron (Collins, 2002). 7 Inference and Decoding For a new input x at test time, we can now construct a weighted FST, G, that defines a probability distribution over all aligned output strings. This can be manipulated to make various predictions about y∗ and its alignment. In our present experiments, we find the most probable (highest-weighted) path in G (Dijkstra, 1959), ˆ as our prediction. Note and use its output string y that Dijkstra’s algorithm is exact; no beam search is required as in some neural sequence models. ˆ may not be"
N16-1076,N07-1047,0,0.0152793,"2016) score edit sequences using a globally normalized model, and Dyer et al. (2015) evaluate the local probability of a parsing action given past actions (and their result) and future words. These architectures are powerful because their LSTMs can examine the output structure; but as a result they do not permit dynamic programming and must fall back on beam search. Our use of dynamic programming for efficient exact inference has long been common in non-neural architectures for sequence transduction, including FST systems that allow “phrasal” replacements s : t where |s|, |t |> 1 (Chen, 2003; Jiampojamarn et al., 2007; Bisani and Ney, 2008). Our work augments these FSTs with neural networks, much as others have augmented CRFs. In this vein, Durrett and Klein (2015) augment a CRF parser (Finkel et al., 2008) to score constituents with a feedforward neural network. Likewise, FitzGerald et al. (2015) employ feedforward nets as a factor in a graphical model for semantic role labeling. Many CRFs have incorporated feedforward neural networks (Bridle, 1990; Peng et al., 2009; Do and Artieres, 2010; Vinel et al., 2011; Fujii et al., 2012; Chen et al., 2015, and others). Some work augments CRFs with BiLSTMs: Huang"
N16-1076,P07-2045,0,0.011159,"linear-chain CRF-BiLSTM, and Kong et al. (2015) on Chinese word segmentation and handwriting recognition with a semi-CRF-BiLSTM. 10 Experiments We evaluated our approach on two morphological generation tasks of reinflection (section 10.1) and lemmatization (section 10.2). In the reinflection task, the goal is to transduce verbs from one inflected form into another, whereas the lemmatization task requires the model to reduce an inflected verb to its 628 root form. We compare our WFST-LSTM against two standard baselines, a WFST with hand-engineered features and the Moses phrase-based MT system (Koehn et al., 2007), as well as the more complex latent-variable model of Dreyer et al. (2008). The comparison with Dreyer et al. (2008) is of noted interest since their latent variables are structured particularly for morphological transduction tasks—we are directly testing the ability of the LSTM to structure its hidden layer as effectively as linguistically motivated latent-variables. Additionally, we provide detailed ablation studies and learning curves which show that our neural-WFSA hybrid model can generalize even with very low amounts of training data. 10.1 Morphological Reinflection Following Dreyer (20"
N16-1076,D09-1005,1,0.807921,"s INS edits (for which j = i) a bit differently, using (exi+1 , γi:i+1 , exi , exi+2 ) rather than (eε , γi:i , exi , exi+1 ). This is conveniently the same vector that is used for all other competing edits at this i position (as they all have |s |= 1 in our present implementation); it provides an extra character of lookahead. 4 If an FST has cycles, such as the self-loops in the example of Figure 3, then the forward algorithm’s recurrence equations become cyclic, and must be solved as a linear system rather than sequentially. This is true regardless of how the FST’s weights Eisner (2002) and Li and Eisner (2009) also explain how to compute the partial derivatives of p(y∗ |x) with respect to the arc weights, essentially by using the forward-backward algorithm. We backpropagate further from these partials to find the gradient of (2) with respect to all our model parameters. We describe our gradient-based maximization procedure in section 10.3, along with regularization. Our model does not have to be trained with the conditional log likelihood objective. It could be trained with other objectives such as empirical risk or softmax-margin (Li and Eisner, 2009; Gimpel and Smith, 2010), or with error-driven"
N16-1076,P09-1067,1,0.79775,"hted) path in G (Dijkstra, 1959), ˆ as our prediction. Note and use its output string y that Dijkstra’s algorithm is exact; no beam search is required as in some neural sequence models. ˆ may not be the most probOn the other hand, y able string—extracting that from a weighted FST is NP-hard (Casacuberta and de la Higuera, 1999). The issue is that the total probability of each y is split over many paths. Still, this is a well-studied problem in NLP. Instead of the Viterbi approximation, we could have used a better approximation, such as crunching (May and Knight, 2006) or variational decoding (Li et al., 2009). We actually did try crunching the 10000-best outputs but got no significant improvement, so we do not report those results. 8 Transducer Topology In our experiments, we choose F to be a simple contextual edit FST as illustrated in Figure 2. Just as in Levenshtein distance (Levenshtein, 1966), it allows all edits s : t where |s |≤ 1, |t |≤ 1, |s |+ |t |= 6 0. We consider the edit type to be INS if s = ε, DEL if are defined. (For convenience, our experiments in this paper avoid cycles by limiting consecutive insertions: see section 8.) 627 t = ε, and SUB otherwise. Note that copy is a SUB edit"
N16-1076,N06-1045,0,0.0239768,"iments, we find the most probable (highest-weighted) path in G (Dijkstra, 1959), ˆ as our prediction. Note and use its output string y that Dijkstra’s algorithm is exact; no beam search is required as in some neural sequence models. ˆ may not be the most probOn the other hand, y able string—extracting that from a weighted FST is NP-hard (Casacuberta and de la Higuera, 1999). The issue is that the total probability of each y is split over many paths. Still, this is a well-studied problem in NLP. Instead of the Viterbi approximation, we could have used a better approximation, such as crunching (May and Knight, 2006) or variational decoding (Li et al., 2009). We actually did try crunching the 10000-best outputs but got no significant improvement, so we do not report those results. 8 Transducer Topology In our experiments, we choose F to be a simple contextual edit FST as illustrated in Figure 2. Just as in Levenshtein distance (Levenshtein, 1966), it allows all edits s : t where |s |≤ 1, |t |≤ 1, |s |+ |t |= 6 0. We consider the edit type to be INS if s = ε, DEL if are defined. (For convenience, our experiments in this paper avoid cycles by limiting consecutive insertions: see section 8.) 627 t = ε, and S"
N16-1076,J97-2003,0,0.0655592,"still permits exact inference through dynamic programming. We illustrate our method on the tasks of morphological reinflection and lemmatization. 1 2 Introduction Mapping one character sequence to another is a structured prediction problem that arises frequently in NLP and computational linguistics. Common applications include grapheme-to-phoneme (G2P), transliteration, vowelization, normalization, morphology, and phonology. The two sequences may have different lengths. Traditionally, such settings have been modeled with weighted finite-state transducers (WFSTs) with parametric edge weights (Mohri, 1997; Eisner, 2002). This requires manual design of the transducer states and the features extracted from those states. Alternatively, deep learning has recently been tried Notation and Background Let Σx be a discrete input alphabet and Σy be a discrete output alphabet. Our goal is to define a conditional distribution p(y |x) where x ∈ Σ∗x and y ∈ Σ∗y and x and y may be of different lengths. We use italics for characters and boldface for strings. xi denotes the ith character of x, and xi:j denotes the substring xi+1 xi+2 · · · xj of length j − i ≥ 0. Note that xi:i = ε, the empty string. Let n = |"
N18-1004,P17-1109,1,0.584802,"escribe and quantify the axes along which languages vary. One facet of language that has been the subject of heavy investigation is the nature of vowel inventories, i.e., which vowels a language contains. It is a cross-linguistic universal that all spoken languages have vowels (Gordon, 2016), and the underlying principles guiding vowel selection are understood: vowels must be both easily recognizable and well-dispersed (Schwartz et al., 2005). In this work, we offer a more formal treatment of the subject, deriving a generative probability model of vowel inventory typology. Our work builds on (Cotterell and Eisner, 2017) by investigating not just discrete IPA inventories but the cross-linguistic variation in acoustic formants. The philosophy behind our approach is that linguistic typology should be treated probabilistically 2 Acoustic Phonetics and Formants Much of human communication takes place through speech: one conversant emits a sound wave to be comprehended by a second. In this work, we consider the nature of the portions of such sound waves that correspond to vowels. We briefly review the relevant bits of acoustic phonetics so as to give an overview of the data we are actually modeling and develop our"
N18-1085,P15-1033,0,0.0141695,"ion for various published probabilistic sequence models pθ (y |x) that incorporate neural networks. These models usually have the form of §1.1. Most simply, (fθ , gθ ) can be instantiated as one time step in an RNN (Aharoni and Goldberg, 2017), but it is com6 mon to use enriched versions such as deep LSTMs. It is also common to have the state st contain not only a vector of manifold coordinates in Rd but also some unboundedly large representation of (x, y:t ) (cf. equation (4)), so the fθ neural network can refer to this material with an attentional (Bahdanau et al., 2015) or stack mechanism (Dyer et al., 2015). A few such papers have used globally normalized conditional models that can be viewed as approximating some OOHMM, e.g., the parsers of Dyer et al. (2016) and Andor et al. (2016). That is the case (§1.1) that particle smoothing aims to support. Most papers are locally normalized conditional models (e.g., Kann and Schütze, 2016; Aharoni and Goldberg, 2017); these simplify supervised training and can be viewed as approximating IOHMMs (footnote 5). For locally normalized models, Ht = 0 by construction, in which case particle filtering (which estimates Ht = 0) is just as good as particle smoothi"
N18-1085,N16-1024,0,0.233713,"gned generative model p(x, y) for the domain. In the neural case, however, inference in such models becomes intractable. It is hard to know what the model actually predicts and hard to compute gradients to improve its predictions. In such intractable settings, one generally falls back on approximate inference or sampling. In the NLP community, beam search and importance sampling are common. Unfortunately, beam search considers only the approximate-top-k taggings from an exponential set (Wiseman and Rush, 2016), and importance sampling requires the construction of a good proposal distribution (Dyer et al., 2016). In this paper we exploit the sequential structure of the tagging problem to do sequential importance sampling, which resembles beam search in that it constructs its proposed samples incrementally—one tag at a time, taking the actual model into account at every step. This method is known as particle filtering (Doucet and Johansen, 2009). We extend it here to take advantage of the fact that the sampler has access to the entire input string as it constructs its tagging, which allows it to look ahead or—as we will show— to use a neural network to approximate the effect of lookahead. Our resultin"
N18-1085,W06-1673,0,0.0919417,"Missing"
N18-1085,P16-2090,0,0.0207164,"to have the state st contain not only a vector of manifold coordinates in Rd but also some unboundedly large representation of (x, y:t ) (cf. equation (4)), so the fθ neural network can refer to this material with an attentional (Bahdanau et al., 2015) or stack mechanism (Dyer et al., 2015). A few such papers have used globally normalized conditional models that can be viewed as approximating some OOHMM, e.g., the parsers of Dyer et al. (2016) and Andor et al. (2016). That is the case (§1.1) that particle smoothing aims to support. Most papers are locally normalized conditional models (e.g., Kann and Schütze, 2016; Aharoni and Goldberg, 2017); these simplify supervised training and can be viewed as approximating IOHMMs (footnote 5). For locally normalized models, Ht = 0 by construction, in which case particle filtering (which estimates Ht = 0) is just as good as particle smoothing. Particle filtering is still useful for these models, but lookahead’s inability to help them is an expressive limitation (known as label bias) of locally normalized models. We hope the existence of particle smoothing (which learns an estimate Ht ) will make it easier to adopt, train, and decode globally normalized models, as"
N18-1085,D09-1011,1,0.861623,"Missing"
N18-1085,D08-1113,1,0.742587,"e uncertainly about the second half, by providing information about topics, referents, syntax, semantics, and discourse. This suggests that an accurate HMM language model p(x) would require very large k— as would a generative OOHMM model p(x, y) of annotated language. The situation is perhaps better for discriminative models p(y |x), since much of 932 the information for predicting yt: might be available in xt: . Still, it is important to let (x:t , y:t ) contribute enough additional information about yt: : even for short strings, making k too small (giving ≤ log2 k bits) may harm prediction (Dreyer et al., 2008). Of course, (4) says that an OOHMM can express any joint distribution for which the mutual information is finite,6 by taking k large enough for vt−1 to capture the relevant info from (x:t−1 , y:t−1 ). So why not just take k to be large—say, k = 230 to allow 30 bits of information? Unfortunately, evaluating GT then becomes very expensive—both computationally and statistically. As we have seen, if we define st to be the belief state Jαt K ∈ Rk , updating it at each observation (xt , yt ) (equation (3)) requires multiplication by a k × k matrix P . This takes time O(k 2 ), and requires enough da"
N18-1085,J93-2004,0,0.0618661,"input x, including samples from the beam search models, while constructing the experimental results graph.16 Thus, the offset KL divergence is a “best effort” lower bound on the true exclusive KL divergence KL(ˆ p||p). 7.2 Results In all experiments we compute the offset KL divergence for both the particle filtering samplers and the particle smoothing samplers, for varying ensemble sizes M . We also compare against a beam search baseline that keeps the highest-scoring M particles at each step (scored by exp Gt with no lookahead). The results are in Figures 2a–2d. Penn Treebank The PTB corpus (Marcus et al., 1993) provides English sentences, from which we use only the sentences of length ≤ 8. We interleave the words of J = 2 sentences. 14 We formally describe the generative process in Appendix G. 936 15 For the details of the training procedures and the specific neural architectures in our models, see Appendices C and D. 16 Thus, Y was collected across all samplings, iterations,and ensemble sizes M , in an attempt to make the summation over Y as complete as possible. For good measure, we added some extra particles: whenever we drew M particles via particle smoothing, we drew an additional 2M particles"
N18-1085,D15-1064,0,0.0166951,"iliar textbook application and reminds the reader that our formal setup (tagging) provides enough machinery to treat other tasks (chunking). English stressed syllable tagging This task tags a sequence of phonemes x, which form a word, with their stress markings y. Our training examples are the stressed words in the CMU pronunciation dictionary (Weide, 1998). We test the sampler on held-out unstressed words. Chinese social media NER This task does named entity recognition in Chinese, by tagging the characters of a Chinese sentence in a way that marks the named entities. We use the dataset from Peng and Dredze (2015), whose tagging scheme is a variant of the BIO scheme mentioned in §1. We test the sampler on held-out sentences. 6.2 String source separation This is an artificial task that provides a discrete analogue of speech source separation (Zibulevsky and Pearlmutter, 2001). The generative model is that J strings (possibly of different lengths) are generated 935 13 English, like many other languages, assigns stress from right to left (Hayes, 1995). IID from an RNN language model, and are then combined into a single string x according to a random interleaving string y.14 The posterior p(y |x) predicts"
N18-1085,D14-1162,0,0.0796828,"Missing"
N18-1085,D16-1137,0,0.314401,", such models often arise as the predictive conditional distribution p(y |x) corresponding to some well-designed generative model p(x, y) for the domain. In the neural case, however, inference in such models becomes intractable. It is hard to know what the model actually predicts and hard to compute gradients to improve its predictions. In such intractable settings, one generally falls back on approximate inference or sampling. In the NLP community, beam search and importance sampling are common. Unfortunately, beam search considers only the approximate-top-k taggings from an exponential set (Wiseman and Rush, 2016), and importance sampling requires the construction of a good proposal distribution (Dyer et al., 2016). In this paper we exploit the sequential structure of the tagging problem to do sequential importance sampling, which resembles beam search in that it constructs its proposed samples incrementally—one tag at a time, taking the actual model into account at every step. This method is known as particle filtering (Doucet and Johansen, 2009). We extend it here to take advantage of the fact that the sampler has access to the entire input string as it constructs its tagging, which allows it to look"
N18-1085,W03-3023,0,0.0748079,"vate our formal choices, we explain how our neural model and neural sampler can be viewed as low-dimensional but nonlinear approximations to working with HMMs over very large state spaces. 1 Introduction Many structured prediction problems in NLP can be reduced to labeling a length-T input string x with a length-T sequence y of tags. In some cases, these tags are annotations such as syntactic parts of speech. In other cases, they represent actions that incrementally build an output structure: IOB tags build a chunking of the input (Ramshaw and Marcus, 1999), shift-reduce actions build a tree (Yamada and Matsumoto, 2003), and finite-state transducer arcs build an output string (Pereira and Riley, 1997). One may wish to score the possible taggings using a recurrent neural network, which can learn to be sensitive to complex patterns in the training data. A globally normalized conditional probability model is particularly valuable because it quantifies uncertainty and does not suffer from label bias (Lafferty et al., 2001); also, such models often arise as the predictive conditional distribution p(y |x) corresponding to some well-designed generative model p(x, y) for the domain. In the neural case, however, infe"
N18-1085,E17-2120,0,\N,Missing
N18-1085,P17-1095,1,\N,Missing
N18-2085,K17-1003,0,0.0498703,"n languages are less complex than others: he claims that Creoles are simpler. M¨uller et al. (2012) compare LMs on EuroParl, but do not compare performance across languages. Related Work Recurrent neural language models can effectively learn complex dependencies, even in openvocabulary settings (Hwang and Sung, 2017; Kawakami et al., 2017). Whether the models are able to learn particular syntactic interactions is an intriguing question, and some methodologies have been presented to tease apart under what circumstances variously-trained models encode attested interactions (Linzen et al., 2016; Enguehard et al., 2017). While the sort of detailed, constructionspecific analyses in these papers is surely informative, our evaluation is language-wide. MT researchers have investigated whether an English sentence contains enough information to predict the fine-grained inflections used in its foreignlanguage translations (see Kirov et al., 2017). 7 Conclusion We have presented a clean method for the crosslinguistic comparison of language modeling: We assess whether a language modeling technique can compress a sentence and its translations equally well. We show an interesting correlation between the morphological r"
N18-2085,P82-1020,0,0.743223,"Missing"
N18-2085,N12-1043,0,0.119,"Missing"
N18-2085,P17-1137,0,0.266302,"se they appear in words that were (arbitrarily) designated as OOV in that language. Such models are known as “open-vocabulary” LMs. Notation. Let ∪˙ denote disjoint union, i.e., A ∪˙ B = C iff A ∪ B = C and A ∩ B = ∅. Let Σ be a discrete alphabet of characters, including a distinguished unknown-character symbol ?.2 A charQ|c|+1 acter LM then defines p(c) = i=1 p(ci |c&lt;i ), where we take c|c|+1 to be a distinguished end-ofstring symbol EOS. In this work, we consider two open-vocabulary LMs, as follows. LSTM LM. While neural language models can also take a hybrid approach (Hwang and Sung, 2017; Kawakami et al., 2017), recent advances indicate that full character-level modeling is now competitive with word-level modeling. A large part of this is due to the use of recurrent neural networks (Mikolov et al., 2010), which can generalize about Baseline n-gram LM. We train “flat” hybrid word/character open-vocabulary n-gram models (Bisani and Ney, 2005), defined over strings Σ+ 2 The set of graphemes in these languages can be assumed to be closed, but external graphemes may on rare occasion appear in random text samples. These are rare enough to not materially affect the metrics. 3 The model can be extended to h"
N18-2085,L18-1293,1,0.78621,"Missing"
N18-2085,sproat-etal-2014-database,0,0.0190663,"95 nl 0.90 de et hu lt sk 0.85 ro en sv da 0.80 pt pl cs el lv es sl bg it fi fr 0.9 1.0 BPEC (LSTM over forms) 1.1 Figure 2: Each dot is a language, and its coordinates are the BPEC values for the LSTM LMs over words and lemmata. The top and right margins show kernel density estimates of these two sets of BPEC values. All dots follow the blue regression, but stay below the green line (y = x), and the darker dots—which represent languages with higher counting complexity—tend to fall toward the right but not toward the top, since counting complexity is correlated only with the BPEC over words. Sproat et al. (2014) present a corpus of close translations of sentences in typologically diverse languages along with detailed morphosyntactic and morphosemantic annotations, as the means for assessing linguistic complexity for comparable messages, though they expressly do not take an information-theoretic approach to measuring complexity. In the linguistics literature, McWhorter (2001) argues that certain languages are less complex than others: he claims that Creoles are simpler. M¨uller et al. (2012) compare LMs on EuroParl, but do not compare performance across languages. Related Work Recurrent neural languag"
N18-2085,L16-1680,0,0.050938,"Missing"
N18-2085,E17-2018,1,0.880447,"Missing"
N18-2085,2005.mtsummit-papers.11,0,0.0633624,"anking under BPEC shows that the LSTM has the easiest time modeling English itself. Scandinavian languages Danish and Swedish have BPEC closest to English; these languages are typologically and genetically similar to English. Experiments and Results n-gram versus LSTM. As expected, the LSTM outperforms the baseline n-gram models across the board. In addition, however, n-gram modeling yields relatively poor performance on some languages, such as Dutch, with only modestly more complex inflectional morphology than English. Our experiments are conducted on the 21 languages of the Europarl corpus (Koehn, 2005). The corpus consists of utterances made in the European parliament and are aligned cross-linguistically by a unique utterance id. With the exceptions (noted in Table 1) of Finnish, Hungarian and Estonian, which are Uralic, the languages are Indo-European. 7 4 Characters appearing &lt; 100 times in train are ?. Other phenomena—e.g., perhaps, compounding— may also be poorly modeled by n-grams. The Impact of Inflectional Morphology. Another major take-away is that rich inflectional morphology is a difficulty for both n-gram and LSTM LMs. In this section we give numbers for the LSTMs. Studying Fig."
N18-2087,J96-2001,0,0.528279,"Missing"
N18-2087,D10-1056,0,0.0576364,"Missing"
N18-2087,K17-2001,1,0.914832,"Missing"
N18-2087,Q15-1031,1,0.791664,"digm by its lemma, which is the surface form that fills a certain designated slot such as the infinitive. We instead use lexemes because lemmas may be ambiguous: bank is the lemma for at least two nominal and two verbal paradigms. 2 form ht, ·, s, ·i, and otherwise model   pθ (s |t) ∝ exp u&gt; tanh (W · vt,s ) &gt; 0 (2) frequent ones in the test portion. This is a realistic evaluation: a training lexicon for a new language would tend to contain frequent types, so the system should be tested on its ability to extrapolate to rarer types that could not be looked up in that lexicon, as discussed by Cotterell et al. (2015). To make the split, we sample N word types without replacement, which is equivalent to collecting the first N distinct forms from an annotated corpus generated from the same unigram distribution. The fractional counts that our method estimates may also be useful for corpus linguistics—for example, tracking the frequency of specific lexemes over time, or comparing the rate of participles in the work of two different authors. Finally, the fractional counts can aid the training of NLP methods that operate on a raw corpus, such as distributional embedding of surface form types into a vector space"
N18-2087,petrov-etal-2012-universal,0,0.0719965,"Missing"
N18-2087,N13-1138,0,0.0945789,"(t, `, s |f ), we may disambiguate these counts in expectation, i.e., we attribute a count of c (talked) · pθ (V ERB, t alk, PAST _ PART |talked) to the past participle of the V ERB t alk. Our aim is the construction and unsupervised estimation of the distribution pθ (t, `, s |f ). While the task at hand is novel, what applications does it have? We are especially interested in sampling tuples ht, `, s, f i from an inflected lexicon. Sampling is a necessity for creating train-test splits for evaluating morphological inflectors, which has recently become a standard task in the literature (Durrett and DeNero, 2013; Hulden et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), and has seen two shared tasks (Cotterell et al., 2016, 2017). Creating train-test splits for training inflectors involves sampling without replacement so that all test types are unseen. Ideally, we would like more frequent word types in the training portion and less What is Syncretism? We say that a surface form f is syncretic if two slots s1 6= s2 exist such that some paradigm π(t, `) maps both s1 and s2 to f . In other words, a single form fills multiple slots in a paradigm: syncretism may be thought of as intra-paradigmatic"
N18-2087,L16-1680,0,0.0421778,"Missing"
N18-2087,N16-1077,0,0.0279486,"we attribute a count of c (talked) · pθ (V ERB, t alk, PAST _ PART |talked) to the past participle of the V ERB t alk. Our aim is the construction and unsupervised estimation of the distribution pθ (t, `, s |f ). While the task at hand is novel, what applications does it have? We are especially interested in sampling tuples ht, `, s, f i from an inflected lexicon. Sampling is a necessity for creating train-test splits for evaluating morphological inflectors, which has recently become a standard task in the literature (Durrett and DeNero, 2013; Hulden et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), and has seen two shared tasks (Cotterell et al., 2016, 2017). Creating train-test splits for training inflectors involves sampling without replacement so that all test types are unseen. Ideally, we would like more frequent word types in the training portion and less What is Syncretism? We say that a surface form f is syncretic if two slots s1 6= s2 exist such that some paradigm π(t, `) maps both s1 and s2 to f . In other words, a single form fills multiple slots in a paradigm: syncretism may be thought of as intra-paradigmatic ambiguity. This definition does depend on the exact annotation sc"
N18-2087,E17-1048,0,0.0632483,"Missing"
N18-2087,E14-1060,0,0.0509616,"mbiguate these counts in expectation, i.e., we attribute a count of c (talked) · pθ (V ERB, t alk, PAST _ PART |talked) to the past participle of the V ERB t alk. Our aim is the construction and unsupervised estimation of the distribution pθ (t, `, s |f ). While the task at hand is novel, what applications does it have? We are especially interested in sampling tuples ht, `, s, f i from an inflected lexicon. Sampling is a necessity for creating train-test splits for evaluating morphological inflectors, which has recently become a standard task in the literature (Durrett and DeNero, 2013; Hulden et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), and has seen two shared tasks (Cotterell et al., 2016, 2017). Creating train-test splits for training inflectors involves sampling without replacement so that all test types are unseen. Ideally, we would like more frequent word types in the training portion and less What is Syncretism? We say that a surface form f is syncretic if two slots s1 6= s2 exist such that some paradigm π(t, `) maps both s1 and s2 to f . In other words, a single form fills multiple slots in a paradigm: syncretism may be thought of as intra-paradigmatic ambiguity. This defi"
N18-2087,L18-1293,1,0.786957,"a generative probability distribution over annotated word forms, and fit the model parameters using the token counts of unannotated word forms. The resulting distribution predicts how to partition each form’s token count among its possible annotations. While our method actually deals with all ambiguous forms in the lexicon, it is particularly useful for syncretic forms because syncretism is often systematic and pervasive. Inflected lexicons—lists of morphologically inflected forms—are commonplace in NLP. Such lexicons currently exist for over 100 languages in a standardized annotation scheme (Kirov et al., 2018), making them one of the most multi-lingual annotated resources in existence. These lexicons are typically annotated at the type level, i.e., each word type is listed with its possible morphological analyses, divorced from sentential context. One might imagine that most word types are unambiguous. However, many inflectional systems are replete with a form of ambiguity termed syncretism—a systematic merger of morphological slots. In English, some verbs have five distinct inflected forms, but regular verbs (the vast majority) merge two of these and so distinguish only four. The verb sin g has"
N18-2087,N15-1093,0,0.0378831,"in expectation, i.e., we attribute a count of c (talked) · pθ (V ERB, t alk, PAST _ PART |talked) to the past participle of the V ERB t alk. Our aim is the construction and unsupervised estimation of the distribution pθ (t, `, s |f ). While the task at hand is novel, what applications does it have? We are especially interested in sampling tuples ht, `, s, f i from an inflected lexicon. Sampling is a necessity for creating train-test splits for evaluating morphological inflectors, which has recently become a standard task in the literature (Durrett and DeNero, 2013; Hulden et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), and has seen two shared tasks (Cotterell et al., 2016, 2017). Creating train-test splits for training inflectors involves sampling without replacement so that all test types are unseen. Ideally, we would like more frequent word types in the training portion and less What is Syncretism? We say that a surface form f is syncretic if two slots s1 6= s2 exist such that some paradigm π(t, `) maps both s1 and s2 to f . In other words, a single form fills multiple slots in a paradigm: syncretism may be thought of as intra-paradigmatic ambiguity. This definition does depend on"
N19-1024,P17-1183,0,0.024421,",7 but only one such path is marked by +IPA. But we leave a more thorough analysis to future work. 6 Related Work Recently, there has been work relating finite-state methods and neural architectures. For example, Schwartz et al. (2018) and Peng et al. (2018) have shown the equivalence between some neural models and WFSAs. The most important differences of our work is that in addition to classifying strings, NFSTs can also transduce strings. Moreover, NFSTs also allow free topology of FST design, and breaks the Markovian assumption. In addition to models we compare against in §4, we note that (Aharoni and Goldberg, 2017; Deng et al., 2018) are also similar to our work; in that they also marginalize over latent alignments, although they do not enforce the monotonicity constraint. Work that discusses globally normalized sequence models are relevant to our work. In this paper, we discuss a training strategy that bounds the partition function; other ways to train a globally normalized model (not necessarily probabilistic) include (Wiseman and Rush, 2016; Andor et al., 2016). On the other hand, our locally normalized FSTs bear resemblance to (Dyer et al., 2016), which was also locally normalized, and also employe"
N19-1024,P16-1231,0,0.0366334,"allow free topology of FST design, and breaks the Markovian assumption. In addition to models we compare against in §4, we note that (Aharoni and Goldberg, 2017; Deng et al., 2018) are also similar to our work; in that they also marginalize over latent alignments, although they do not enforce the monotonicity constraint. Work that discusses globally normalized sequence models are relevant to our work. In this paper, we discuss a training strategy that bounds the partition function; other ways to train a globally normalized model (not necessarily probabilistic) include (Wiseman and Rush, 2016; Andor et al., 2016). On the other hand, our locally normalized FSTs bear resemblance to (Dyer et al., 2016), which was also locally normalized, and also employed importance sampling for training. 7 ably with state-of-the-art neural models on transduction tasks. At the same time, it is easy to inject domain knowledge into NFSTs for inductive bias, and they offer interpretable paths. In this paper, we have used rather simple architectures for our RNNs; one could experiment with multiple layers and attention. One could also experiment with associating marks differently with arcs—the marks are able to convey useful"
N19-1024,W18-5407,0,0.157252,"ntly popular models of p(y |x) used in machine translation and morphology include seq2seq (Sutskever et al., 2014), seq2seq with attention (Bahdanau et al., 2015; Luong et al., 2015), the Transformer (Vaswani et al., 2017). These models Interpretability Like a WFST, an NFST can “explain” why it mapped x to y in terms of a latent path a, which specifies a hard monotonic labeled alignment. The posterior distribution p(a |x, y) specifies which paths a are the best explanations (e.g., Table 5). We conduct experiments on three tasks: grapheme-to-phoneme, phoneme-to-grapheme, and action-to-command (Bastings et al., 2018). Our results on these datasets show that our best models can improve over neural seq2seq and previously proposed hard alignment models. 2 2.1 Neuralizing Finite-State Transducers Neuralized FSTs An NFST is a pair (T , Gθ ), where T is an unweighted FST with accepting paths A and Gθ : A → R is a function that scores these paths. As explained earlier, we then refer to p˜(a) = exp Gθ (a) as the weight of path a ∈ A. A weighted relation between input and output strings is given by p˜(x, y), which is defined to be the total weight of all paths with input string x ∈ Σ∗ and output string y ∈ ∆∗ , wh"
N19-1024,P14-2102,1,0.836356,"t sum over a large set of paths—in a∈T p fact, an infinite set if T is cyclic. This sum may diverge for some values of the parameter vector θ, which complicates training of the model (Dreyer, 274 2011). Even if the sum is known to converge, it is in general intractable to compute it exactly. Thus, estimating the gradient of Z(T ) during training involves approximate sampling from the typically high-entropy distribution p(a). The resulting estimates are error-prone because the sample size tends to be too small and the approximate sampler is biased. A standard solution in the WFST setting (e.g. Cotterell et al., 2014) is to use a locally normalized model, in which Z(T ) is guaranteed to be 1.1 The big summation over all paths a is replaced by small summations—which can be computed explicitly— over just the outgoing edges from a given state. Formally, we define the unnormalized score of arc ai in the context of path a in the obvious way, by summing over the contextual scores of its marks: g˜θ (ai ) , k X gθ (st−1 , ωt ) algorithms below—rely on conditional sampling. In general, we would like to sample a path of T given the knowledge that its input and output strings fall into sets X and Y respectively.2 If"
N19-1024,N16-1024,0,0.1502,"tive bias of the learner, we partitioned the hidden state vector into three sub-vectors: st = [sat ; sxt ; syt ]. The mark scoring function fθ (st−1 , ωt ) was as before, but we restricted the form of gθ , the state update function. sat encodes all past marks and depends on the full hidden state so far: sat = gθa (st−1 , ωt ). However, we make sxt encode only the sequence of past input marks, ignoring all others. Thus, sxt = gθx (sxt−1 , ωt ) if ωt is an input mark, and sxt = sxt−1 otherwise. Symmetrically, syt encodes only the sequence of past output marks. This architecture is somewhat like Dyer et al. (2016), which also uses different sub-vectors to keep track of different aspects of the history. 2.4 Local normalization A difficulty with the general model form in equation P (1) is that the normalizing constant Z(T ) = ˜(a) must sum over a large set of paths—in a∈T p fact, an infinite set if T is cyclic. This sum may diverge for some values of the parameter vector θ, which complicates training of the model (Dreyer, 274 2011). Even if the sum is known to converge, it is in general intractable to compute it exactly. Thus, estimating the gradient of Z(T ) during training involves approximate sampling"
N19-1024,N09-1068,0,0.0891613,"Missing"
N19-1024,N18-1085,1,0.899731,"en in equation (1), the path a that aligns x and y is a latent variable. This is also true of the resulting conditional distribution p(y |x). We explore training and inference algorithms for various classes of NFST models (§3). Classical WFSTs (Mohri et al., 2008) and BiRNN-WFSTs (Rastogi et al., 2016) use restricted scoring functions and so admit exact dynamic programming algorithms. For general NFSTs, however, we must resort to approximate computation of the model’s training gradient, marginal probabilities, and predictions. In this paper, we will use sequential importance sampling methods (Lin and Eisner, 2018), leaving variational approximation methods to future work. Defining models using FSTs has several benefits: Output-sensitive encoding Currently popular models of p(y |x) used in machine translation and morphology include seq2seq (Sutskever et al., 2014), seq2seq with attention (Bahdanau et al., 2015; Luong et al., 2015), the Transformer (Vaswani et al., 2017). These models Interpretability Like a WFST, an NFST can “explain” why it mapped x to y in terms of a latent path a, which specifies a hard monotonic labeled alignment. The posterior distribution p(a |x, y) specifies which paths a are the"
N19-1024,D15-1166,0,0.352137,"ng functions and so admit exact dynamic programming algorithms. For general NFSTs, however, we must resort to approximate computation of the model’s training gradient, marginal probabilities, and predictions. In this paper, we will use sequential importance sampling methods (Lin and Eisner, 2018), leaving variational approximation methods to future work. Defining models using FSTs has several benefits: Output-sensitive encoding Currently popular models of p(y |x) used in machine translation and morphology include seq2seq (Sutskever et al., 2014), seq2seq with attention (Bahdanau et al., 2015; Luong et al., 2015), the Transformer (Vaswani et al., 2017). These models Interpretability Like a WFST, an NFST can “explain” why it mapped x to y in terms of a latent path a, which specifies a hard monotonic labeled alignment. The posterior distribution p(a |x, y) specifies which paths a are the best explanations (e.g., Table 5). We conduct experiments on three tasks: grapheme-to-phoneme, phoneme-to-grapheme, and action-to-command (Bastings et al., 2018). Our results on these datasets show that our best models can improve over neural seq2seq and previously proposed hard alignment models. 2 2.1 Neuralizing Finit"
N19-1024,D18-1152,0,0.0525763,"tter) and edit distance (lower the better) on G2P and P2G. The effectiveness of different FST designs. compared to the general FST baseline. This is a surprising result — one explanation is the IPA marks are not defined on all paths that transduce the intended input-output pairs: NFSTs are capable of recognizing phoneme-grapheme alignments in different paths,7 but only one such path is marked by +IPA. But we leave a more thorough analysis to future work. 6 Related Work Recently, there has been work relating finite-state methods and neural architectures. For example, Schwartz et al. (2018) and Peng et al. (2018) have shown the equivalence between some neural models and WFSAs. The most important differences of our work is that in addition to classifying strings, NFSTs can also transduce strings. Moreover, NFSTs also allow free topology of FST design, and breaks the Markovian assumption. In addition to models we compare against in §4, we note that (Aharoni and Goldberg, 2017; Deng et al., 2018) are also similar to our work; in that they also marginalize over latent alignments, although they do not enforce the monotonicity constraint. Work that discusses globally normalized sequence models are relevant"
N19-1024,N16-1076,1,0.851983,"t><i-h><C><o-θ> We introduce neural finite state transducers (NFSTs), a family of string transduction models defining joint and conditional probability distributions over pairs of strings. The probability of a string pair is obtained by marginalizing over all its accepting paths in a finite state transducer. In contrast to ordinary weighted FSTs, however, each path is scored using an arbitrary function such as a recurrent neural network, which breaks the usual conditional independence assumption (Markov property). NFSTs are more powerful than previous finite-state models with neural features (Rastogi et al., 2016). We present training and inference algorithms for locally and globally normalized variants of NFSTs. In experiments on different transduction tasks, they compete favorably against seq2seq models while offering interpretable paths that correspond to hard monotonic alignments. b:b <i-b><C><o-b> a:æ <i-a><V><o-æ> s0 ε:ε <BOS> s1 ε:ε <EOS> s2 a:? <i-a> ε:æ <o-æ> Figure 1: A marked finite-state transducer T . Each arc in T is associated with input and output substrings, listed above the arcs in the figure. The arcs are not labeled with weights as in WFSTs. Rather, each arc is labeled with a sequen"
N19-1024,P18-1028,0,0.0563544,"Missing"
N19-1024,D16-1137,0,0.0237649,"s. Moreover, NFSTs also allow free topology of FST design, and breaks the Markovian assumption. In addition to models we compare against in §4, we note that (Aharoni and Goldberg, 2017; Deng et al., 2018) are also similar to our work; in that they also marginalize over latent alignments, although they do not enforce the monotonicity constraint. Work that discusses globally normalized sequence models are relevant to our work. In this paper, we discuss a training strategy that bounds the partition function; other ways to train a globally normalized model (not necessarily probabilistic) include (Wiseman and Rush, 2016; Andor et al., 2016). On the other hand, our locally normalized FSTs bear resemblance to (Dyer et al., 2016), which was also locally normalized, and also employed importance sampling for training. 7 ably with state-of-the-art neural models on transduction tasks. At the same time, it is easy to inject domain knowledge into NFSTs for inductive bias, and they offer interpretable paths. In this paper, we have used rather simple architectures for our RNNs; one could experiment with multiple layers and attention. One could also experiment with associating marks differently with arcs—the marks are a"
N19-1024,D18-1473,0,0.036784,"Missing"
N19-1203,P17-1183,0,0.0301289,"vectors are passed to a bidirectional LSTM (Graves et al., 2005), where the corresponding hidden states are concatenated at each time step. We simply refer to the hidden state hi ∈ Rd as the result of said concatenation at the i-th step. Using hi , we can define the potential function as ψ (mi , mi−1 ) =  where Ami ,mi−1 is a exp Ami ,mi−1 + o> h , i mi transition weight matrix and omi ∈ Rd is a morphological tag embedding; both are learned. 2.3 The Morphological Inflector The conditional distribution p(wi |`i , mi ) is parameterized by a neural encoder–decoder model with hard attention from Aharoni and Goldberg (2017). The model was one of the top performers in the 2016 SIGMORPHON shared task (Cotterell et al., 2016); it achieved particularly high accuracy in the low-resource setting. Hard attention is motivated by the observation that alignment between the input and output sequences is often monotonic in inflection tasks. In the model, the input lemma is treated as a sequence of characters, and encoded using a bidirectional LSTM (Graves and Schmidhuber, 2005), to produce vectors xj for each character position j. Next the word wi = c = c1 · · · c|wi |is generated in a decoder character-by-character: p(cj |"
N19-1203,W05-0909,0,0.0318394,"trumental), as well as those not associated with specific prepositions, are less well predicted. In addition, we evaluated the model’s performance when all forms are replaced by their corresponding lemmata (as in two cat be sit). For freer word order languages such as Polish or Latin, we observe a substantial drop in performance because most information on inter-word relations and their roles (expressed by means of case system) is lost. 5 Related Work The primary evaluation for most contemporary language and translation modeling research is perplexity, BLEU (Papineni et al., 2002), or METEOR (Banerjee and Lavie, 2005). Undoubtedly, such metrics are necessary for extrinsic evaluation and comparison. However, relatively few studies have focused on intrinsic evaluation of the model’s mastery of grammaticality. Recently, Linzen et al. (2016) investigated the ability of an LSTM language model to capture sentential structure, by evaluating subject–verb agreement with respect to number, and showed that under strong supervision, the LSTM is able to approximate dependencies. Taking it from the other perspective, a truer measure of grammatical competence would be a task of mapping a meaning representation to text, w"
N19-1203,P17-1080,0,0.0256269,"version of language modeling. Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metric— perplexity—is dominated by the language model’s ability to predict content, which is where most of the uncertainty lies. Our task focuses on just the ability to reconstruct certain missing parts of the sentence—inflectional morphemes and their orthographic realization. This refocuses the modeling effort from semantic coherence to morphosyntactic coherence, an aspect of language that may take a back seat in current language models (see Linzen et al., 2016; Belinkov et al., 2017). Contextual inflection does not perfectly separate grammaticality modeling from content modeling: as illustrated in Tab. 1, mapping two cats _be_ sitting to the fully-inflected two cats were sitting does not require full knowledge of English grammar—the system does not have to predict the required word order nor the required auxiliary verb be, as these are supplied in the input. Conversely, this example does still require predicting some content—the semantic choice of past tense is not given by the input and must be guessed by the system.1 The primary contribution of this paper is a novel str"
N19-1203,W11-2832,0,0.0158478,"presentation specifies all necessary semantic content—content lemmata, dependency relations, and “inherent” closed-class morphemes (semantic features such as noun number, noun definiteness, and verb tense)—and the system is to realize this content according to the morphosyntactic conventions of a language, which means choosing word order, agreement morphemes, function words, and the surface forms of all words. Such tasks have been investigated to some extent—generating text from tectogrammatical trees (Hajic et al., 2002; Ptáˇcek and Žabokrtský, 2006) or from an AMR graph (Song et al., 2017). Belz et al. (2011) organized a related surface realization shared task on mapping unordered and uninflected dependency trees to properly ordered inflected sentences. The generated sentences were afterwards assessed by human annotators, making the task less scalable and more time consuming. Although our task is not perfectly matched to grammaticality modeling, the upside is that it is a “lightweight” task that works directly on text. No meaning representation is required. Thus, training and test data in any language can be prepared simply by lemmatizing a naturally occurring corpus. Finally, as a morphological i"
N19-1203,Q17-1010,0,0.0234037,"or decoding we use a greedy strategy where we first decode the CRF, that is, we solve the problem m? = argmaxm log p(m |`), using the Viterbi (1967) algorithm. We then use this decoded m? to generate forms from the inflector. Note that finding the one-best string under our neural inflector is intractable, and for this reason we use greedy search. 3 Experiments Dataset. We use the Universal Dependencies v1.2 dataset (Nivre et al., 2016) for our experiments. We include all the languages with information on their lemmata and fine-grained grammar tag annotation that also have fasttext embeddings (Bojanowski et al., 2017), which are used for word embedding initialization.5 Evaluation. We evaluate our model’s ability to predict: (i) the correct morphological tags from the lemma context, and (ii) the correct inflected forms. As our evaluation metric, we report 1-best accuracy for both tags and word form prediction. Configuration. We use a word and character embedding dimensionality of 300 and 100, respectively. The hidden state dimensionality is set to 200. All models are trained with Adam (Kingma and Ba, 2014), with a learning rate of 0.001 for 20 epochs. Baselines. We use two baseline systems: (1) the CoNLL–SI"
N19-1203,K18-3001,1,0.790791,"on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguisticallymotivated latent variables into NLP models. 1 Introduction NLP systems are often required to generate grammatical text, e.g., in machine translation, summarization, dialogue, and grammar correction. One component of grammaticality is the use of contextually appropriate closed-class morphemes. In this work, we study contextual inflection, which has been recently introduced in the CoNLLSIGMORPHON 2018 shared task (Cotterell et al., 2018) to directly investigate context-dependent morphology in NLP. There, a system must inflect partially lemmatized tokens in sentential context. For example, in English, the system must reconstruct the correct word sequence two cats are sitting from partially lemmatized sequence two _cat_ are sitting. Among other things, this requires: (1) identifying cat as a noun in this context, (2) recognizing that cat should be inflected as plural to agree with the nearby verb and numeral, and (3) realizing this inflection as the suffix s. Most past work in supervised computational morphology—including the p"
N19-1203,K17-2001,1,0.93262,"a system must inflect partially lemmatized tokens in sentential context. For example, in English, the system must reconstruct the correct word sequence two cats are sitting from partially lemmatized sequence two _cat_ are sitting. Among other things, this requires: (1) identifying cat as a noun in this context, (2) recognizing that cat should be inflected as plural to agree with the nearby verb and numeral, and (3) realizing this inflection as the suffix s. Most past work in supervised computational morphology—including the previous CoNLL-SIGMORPHON shared tasks on morphological reinflection (Cotterell et al., 2017)—has focused mainly on step (3) above. As the task has been introduced into the literature only recently, we provide some background. Contextual inflection amounts to a highly constrained version of language modeling. Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metric— perplexity—is dominated by the language model’s ability to predict content, which is where most of the uncertainty lies. Our task focuses on just the ability to reconstruct certain missing parts of the sentence—inflectional morphemes and their orthographic realization. Th"
N19-1203,N16-1030,0,0.0407715,"tured neural model 1 Y p(m |`) = ψ (mi , mi−1 , `) Z(`) (2) i=1 2 Although wi can sometimes be computed by concatenating `i with mi -specific affixes, it can also be irregular. 3 In case of partially lemmatized sequence we still train the model to predict the tags over the entire sequence, but evaluate it only for lemmatized slots. `1 where ψ(·, ·, ·) ≥ 0 is an arbitrary potential, Z(`) normalizes the distribution, and m0 is a distinguished start-of-sequence symbol. 2019 In this work, we opt for a recurrent neural potential—specifically, we adopt a parameterization similar to the one given by Lample et al. (2016). Our potential ψ is computed as follows. First, the sequence ` is encoded into a sequence of word vectors using the strategy described by Ling et al. (2015): word vectors are passed to a bidirectional LSTM (Graves et al., 2005), where the corresponding hidden states are concatenated at each time step. We simply refer to the hidden state hi ∈ Rd as the result of said concatenation at the i-th step. Using hi , we can define the potential function as ψ (mi , mi−1 ) =  where Ami ,mi−1 is a exp Ami ,mi−1 + o> h , i mi transition weight matrix and omi ∈ Rd is a morphological tag embedding; both ar"
N19-1203,D15-1176,0,0.0216304,"lso be irregular. 3 In case of partially lemmatized sequence we still train the model to predict the tags over the entire sequence, but evaluate it only for lemmatized slots. `1 where ψ(·, ·, ·) ≥ 0 is an arbitrary potential, Z(`) normalizes the distribution, and m0 is a distinguished start-of-sequence symbol. 2019 In this work, we opt for a recurrent neural potential—specifically, we adopt a parameterization similar to the one given by Lample et al. (2016). Our potential ψ is computed as follows. First, the sequence ` is encoded into a sequence of word vectors using the strategy described by Ling et al. (2015): word vectors are passed to a bidirectional LSTM (Graves et al., 2005), where the corresponding hidden states are concatenated at each time step. We simply refer to the hidden state hi ∈ Rd as the result of said concatenation at the i-th step. Using hi , we can define the potential function as ψ (mi , mi−1 ) =  where Ami ,mi−1 is a exp Ami ,mi−1 + o> h , i mi transition weight matrix and omi ∈ Rd is a morphological tag embedding; both are learned. 2.3 The Morphological Inflector The conditional distribution p(wi |`i , mi ) is parameterized by a neural encoder–decoder model with hard attentio"
N19-1203,Q16-1037,0,0.481413,"a highly constrained version of language modeling. Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metric— perplexity—is dominated by the language model’s ability to predict content, which is where most of the uncertainty lies. Our task focuses on just the ability to reconstruct certain missing parts of the sentence—inflectional morphemes and their orthographic realization. This refocuses the modeling effort from semantic coherence to morphosyntactic coherence, an aspect of language that may take a back seat in current language models (see Linzen et al., 2016; Belinkov et al., 2017). Contextual inflection does not perfectly separate grammaticality modeling from content modeling: as illustrated in Tab. 1, mapping two cats _be_ sitting to the fully-inflected two cats were sitting does not require full knowledge of English grammar—the system does not have to predict the required word order nor the required auxiliary verb be, as these are supplied in the input. Conversely, this example does still require predicting some content—the semantic choice of past tense is not given by the input and must be guessed by the system.1 The primary contribution of t"
N19-1203,L16-1262,0,0.0579627,"Missing"
N19-1203,P02-1040,0,0.103705,"shifting positions (such as the instrumental), as well as those not associated with specific prepositions, are less well predicted. In addition, we evaluated the model’s performance when all forms are replaced by their corresponding lemmata (as in two cat be sit). For freer word order languages such as Polish or Latin, we observe a substantial drop in performance because most information on inter-word relations and their roles (expressed by means of case system) is lost. 5 Related Work The primary evaluation for most contemporary language and translation modeling research is perplexity, BLEU (Papineni et al., 2002), or METEOR (Banerjee and Lavie, 2005). Undoubtedly, such metrics are necessary for extrinsic evaluation and comparison. However, relatively few studies have focused on intrinsic evaluation of the model’s mastery of grammaticality. Recently, Linzen et al. (2016) investigated the ability of an LSTM language model to capture sentential structure, by evaluating subject–verb agreement with respect to number, and showed that under strong supervision, the LSTM is able to approximate dependencies. Taking it from the other perspective, a truer measure of grammatical competence would be a task of mappi"
N19-1203,petrov-etal-2012-universal,0,0.0315183,"Missing"
N19-1203,P17-2002,0,0.014134,"where the meaning representation specifies all necessary semantic content—content lemmata, dependency relations, and “inherent” closed-class morphemes (semantic features such as noun number, noun definiteness, and verb tense)—and the system is to realize this content according to the morphosyntactic conventions of a language, which means choosing word order, agreement morphemes, function words, and the surface forms of all words. Such tasks have been investigated to some extent—generating text from tectogrammatical trees (Hajic et al., 2002; Ptáˇcek and Žabokrtský, 2006) or from an AMR graph (Song et al., 2017). Belz et al. (2011) organized a related surface realization shared task on mapping unordered and uninflected dependency trees to properly ordered inflected sentences. The generated sentences were afterwards assessed by human annotators, making the task less scalable and more time consuming. Although our task is not perfectly matched to grammaticality modeling, the upside is that it is a “lightweight” task that works directly on text. No meaning representation is required. Thus, training and test data in any language can be prepared simply by lemmatizing a naturally occurring corpus. Finally,"
P02-1001,E99-1017,0,0.0336565,"Missing"
P02-1001,knight-al-onaizan-1998-translation,0,0.0616408,"stic) relation and constitutes a statistical model. Such models can be efficiently restricted, manipulated or combined using rational operations as before. An artificial example will appear in §2. The availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical NLP. Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding,1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998). Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources. Unfortunately, there is a stumbling block: Where do the weights come from? After all, statistical models require supervised or unsupervised training. Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs. Not only do these methods require additional programming outside the toolkit, but they are limited to particular ki"
P02-1001,P96-1031,0,0.0237584,"s a weighted regular relation, then the following statements are equivalent: (1) f is a joint probabilistic relation; (2) f can be computed by a Markovian FST that halts with probability 1; (3) f can be expressed as a probabilistic regexp, i.e., a regexp built up from atomic expressions a : b (for a ∈ Σ ∪ {}, b ∈ ∆ ∪ {}) using concatenation, probabilistic union +p , and probabilistic closure ∗p . For defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways: (1) via FSTs as in Fig. 1c, (2) by compilation of weighted rewrite rules (Mohri and Sproat, 1996), (3) by compilation of decision trees (Sproat and Riley, 1996), (4) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation (Gerdemann and van Noord, 1999),5 (5) by conditionalization of a joint relation as discussed below. A central technique is to define a joint relation as a noisy-channel model, by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 (Pereira and Riley, 1997; Knight and Graehl, 1998). The general form is illustrated by 3 Conceptually, the parameters represent the probabili"
P02-1001,J00-1003,0,0.0141881,"er relation’s choice of how to replace a match. One can also get randomness through the choice of matches, ignoring match possibilities by randomly deleting markers in Gerdemann and van Noord’s construction. def P P (v, z) = w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines.6,7 There are also procedures for defining weighted FSTs that are not probabilistic (Berstel and Reutenauer, 1988). Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a reg:/2.7 exp (to be compiled into −→ arcs). A more subtle example is weighted FSAs that approximate PCFGs (Nederhof, 2000; Mohri and Nederhof, 2001), or to extend the idea, weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation. These are parameterized by the PCFG’s parameters, but add or remove strings of the PCFG to leave an improper probability distribution. Fortunately for those techniques, an FST with positive arc weights can be normalized to make it jointly or conditionally probabilistic: • An easy approach is to normalize the options at each state to make the FST Markovian. Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation"
P02-1001,P96-1029,0,0.00971384,"e equivalent: (1) f is a joint probabilistic relation; (2) f can be computed by a Markovian FST that halts with probability 1; (3) f can be expressed as a probabilistic regexp, i.e., a regexp built up from atomic expressions a : b (for a ∈ Σ ∪ {}, b ∈ ∆ ∪ {}) using concatenation, probabilistic union +p , and probabilistic closure ∗p . For defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways: (1) via FSTs as in Fig. 1c, (2) by compilation of weighted rewrite rules (Mohri and Sproat, 1996), (3) by compilation of decision trees (Sproat and Riley, 1996), (4) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation (Gerdemann and van Noord, 1999),5 (5) by conditionalization of a joint relation as discussed below. A central technique is to define a joint relation as a noisy-channel model, by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 (Pereira and Riley, 1997; Knight and Graehl, 1998). The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transduci"
P02-1001,J98-4003,0,\N,Missing
P02-1008,J95-4011,0,0.0452999,"Missing"
P02-1008,P97-1040,1,0.94362,"Missing"
P02-1008,C00-1038,1,0.829421,"Missing"
P02-1008,C94-2163,0,0.858444,"Missing"
P02-1008,J98-2006,0,0.180081,"Missing"
P02-1008,W00-1804,0,0.6223,"Missing"
P02-1008,J94-3001,0,0.387808,"Missing"
P02-1008,W98-1301,0,0.619673,"Missing"
P02-1008,P96-1049,0,\N,Missing
P03-2041,J00-1004,0,0.277642,"rt the latter. Thus we have the alignments (beaucoup d’, ) and (, quite often). These require innovations. The tree-internal deletion of beaucoup d’ is handled by an empty elementary tree in which the root is itself a frontier node. (The subject frontier node of kiss is replaced with this frontier node, which is then replaced with kids.) The tree-peripheral insertion of quite often requires an English frontier node that is paired with a French null. We also formulate STSGs flexibly enough that they can handle both phrase-structure trees and dependency trees. The latter are small and simple (Alshawi et al., 2000): tree nodes are words, and there need be no other structure to recover or align. Selectional preferences and other interactions can be accommodated by enriching the states. Any STSG has a weakly equivalent SCFG that generates the same string pairs. So STSG (unlike STAG) has no real advantage for modeling string pairs.3 But STSGs can generate a wider variety of tree pairs, e.g., non-isomorphic ones. So when actual trees are provided for training, STSG can be more flexible in aligning them. 1 Goodman (2002) presents efficient TSG parsing with unbounded elementary trees. Unfortunately, that clev"
P03-2041,J99-4004,0,0.0432668,"mentary tree t that fits c Q increment βc (t.q) by p(t |t.q) · d∈t.V f βd (t.s(d)) The β values are inside probabilities. After running the algorithm, if r is the root of T , then βr (Start) is the probability that the grammar generates T . p(t |q) in line 4 may be found by hash lookup if the grammar is stored explicitly, or else by some probabilistic model that analyzes the structure, labels, and states of the elementary tree t to compute its probability. One can mechanically transform this algorithm to compute outside probabilities, the Viterbi parse, the parse forest, and other quantities (Goodman, 1999). One can also apply agenda-based parsing strategies. For a fixed grammar, the runtime and space are only O(n) for a tree of n nodes. The grammar constant is the number of possible fits to a node c of a fixed tree. As noted above, there usually not many of these (unless the states are uncertain) and they are simple to enumerate. As discussed above, an inside-outside algorithm may be used to compute the expected number of times each elementary tree t appeared in the derivation of T . That is the E step of the EM algorithm. In the M step, these expected counts (collected over a corpus of trees)"
P03-2041,J99-4005,0,0.0615489,"s as contiguous, potentially idiomatic units (Och et al., 1999). Several researchers have tried putting “more syntax” into translation models: like us, they use statistical versions of synchronous grammars, which generate source and target sentences in parallel and so describe their correspondence.4 This approach offers four features absent from IBM-style models: (1) a recursive phrase-based translation, (2) a syntax-based language model, (3) the ability to condition a word’s translation on the translation of syntactically related words, and (4) polynomial-time optimal alignment and decoding (Knight, 1999). Previous work in statistical synchronous grammars has been limited to forms of synchronous context-free grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001). This means that a sentence and its translation must have isomorphic syntax trees, although they may have different numbers of surface words if null words  are allowed in one or both languages. This rigidity does not fully describe real data. The one exception is the synchronous DOP approach of (Poutsma, 2000), which obtains an STSG by decomposing aligned training trees in all possible ways (and using “naive” count-based pr"
P03-2041,A00-2023,0,0.0254643,"tly. • 1-best Alignment (if desired). This is just like training, except that we use the Viterbi algorithm to find the single best derivation of the input tree pair. This derivation can be regarded as the optimal syntactic alignment.7 6 A matching between A and B is a 1-to-1 correspondence between a subset of A and a subset of B. 7 As free-translation post-processing, one could try to match pairs of stray subtrees that could have aligned well, according to the chart, but were forced to align with null for global reasons. • Decoding. We create a forest of possible synchronous derivations (cf. (Langkilde, 2000)). We chart-parse T1 as much as in section 5, but fitting the left side of an elementary tree pair to each node. Roughly speaking: for c1 = null and then c1 ∈ T1 .V , in bottom-up order for each q ∈ Q, let βc1 (q) = −∞ for each probable Q t = (t1 , t2 , q, m, s) whose t1 fits c1 max p(t |q) · (d1 ,d2 )∈m β (s(d1 , d2 )) into βc1 (q) ¯ d1 1. 2. 3. 4. We then extract the max-probability synchronous derivation and return the T2 that it derives. This algorithm is essentially alignment to an unknown tree T2 ; we do not loop over its nodes c2 , but choose t2 freely. 7 Status of the Implementation We"
P03-2041,W99-0604,0,0.15655,"djunction operation, for dependency trees. 3 However, the binary-branching SCFGs used by Wu (1997) and Alshawi et al. (2000) are strictly less powerful than STSG. 3 Past Work Most statistical MT derives from IBM-style models (Brown et al., 1993), which ignore syntax and allow arbitrary word-to-word translation. Hence they are able to align any sentence pair, however mismatched. However, they have a tendency to translate long sentences into word salad. Their alignment and translation accuracy improves when they are forced to translate shallow phrases as contiguous, potentially idiomatic units (Och et al., 1999). Several researchers have tried putting “more syntax” into translation models: like us, they use statistical versions of synchronous grammars, which generate source and target sentences in parallel and so describe their correspondence.4 This approach offers four features absent from IBM-style models: (1) a recursive phrase-based translation, (2) a syntax-based language model, (3) the ability to condition a word’s translation on the translation of syntactically related words, and (4) polynomial-time optimal alignment and decoding (Knight, 1999). Previous work in statistical synchronous grammar"
P03-2041,C00-2092,0,0.139758,"anslation on the translation of syntactically related words, and (4) polynomial-time optimal alignment and decoding (Knight, 1999). Previous work in statistical synchronous grammars has been limited to forms of synchronous context-free grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001). This means that a sentence and its translation must have isomorphic syntax trees, although they may have different numbers of surface words if null words  are allowed in one or both languages. This rigidity does not fully describe real data. The one exception is the synchronous DOP approach of (Poutsma, 2000), which obtains an STSG by decomposing aligned training trees in all possible ways (and using “naive” count-based probability estimates). However, we would like to estimate a model from unaligned data. 4 A Probabilistic TSG Formalism For expository reasons (and to fill a gap in the literature), first we formally present non-synchronous TSG. Let Q be a set of states. Let L be a set of labels that may decorate nodes or edges. Node labels might be words or nonterminals. Edge labels might include grammatical roles such as Subject. In many trees, each node’s children have an order, recorded in labe"
P03-2041,C90-3045,0,0.819253,"apping (float down ↔ descend by floating). Such systematic mismatches should be learned by the model, and used during translation. It is even helpful to learn mismatches that merely tend to arise during free translation. Knowing that beaucoup d’ is often deleted will help in aligning the rest of the tree. When would learned tree-to-tree mappings be useful? Obviously, in MT, when one has parsers for both the source and target language. Systems for “deep” analysis and generation might wish to learn mappings between deep and surface trees (B¨ohmov´a et al., 2001) or between syntax and semantics (Shieber and Schabes, 1990). Systems for summarization or paraphrase could also be trained on tree pairs (Knight and Marcu, 2000). Non-NLP applications might include comparing studentwritten programs to one another or to the correct solution. Our methods can naturally extend to train on pairs of forests (including packed forests obtained by chart parsing). The correct tree is presumed to be an element of the forest. This makes it possible to train even when the correct parse is not fully known, or not known at all. enfants NP (0,Adv) (0,Adv) often (0,Adv) (0,Adv) quite kids Sam NP Sam The elementary trees represent idio"
P03-2041,J97-3002,0,0.547052,"variety of tree pairs, e.g., non-isomorphic ones. So when actual trees are provided for training, STSG can be more flexible in aligning them. 1 Goodman (2002) presents efficient TSG parsing with unbounded elementary trees. Unfortunately, that clever method does not permit arbitrary models of elementary tree probabilities, nor does it appear to generalize to our synchronous case. (It would need exponentially many nonterminals to keep track of an matching of unboundedly many frontier nodes.) 2 Or a sister-adjunction operation, for dependency trees. 3 However, the binary-branching SCFGs used by Wu (1997) and Alshawi et al. (2000) are strictly less powerful than STSG. 3 Past Work Most statistical MT derives from IBM-style models (Brown et al., 1993), which ignore syntax and allow arbitrary word-to-word translation. Hence they are able to align any sentence pair, however mismatched. However, they have a tendency to translate long sentences into word salad. Their alignment and translation accuracy improves when they are forced to translate shallow phrases as contiguous, potentially idiomatic units (Och et al., 1999). Several researchers have tried putting “more syntax” into translation models: l"
P03-2041,P01-1067,0,0.658258,"statistical versions of synchronous grammars, which generate source and target sentences in parallel and so describe their correspondence.4 This approach offers four features absent from IBM-style models: (1) a recursive phrase-based translation, (2) a syntax-based language model, (3) the ability to condition a word’s translation on the translation of syntactically related words, and (4) polynomial-time optimal alignment and decoding (Knight, 1999). Previous work in statistical synchronous grammars has been limited to forms of synchronous context-free grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001). This means that a sentence and its translation must have isomorphic syntax trees, although they may have different numbers of surface words if null words  are allowed in one or both languages. This rigidity does not fully describe real data. The one exception is the synchronous DOP approach of (Poutsma, 2000), which obtains an STSG by decomposing aligned training trees in all possible ways (and using “naive” count-based probability estimates). However, we would like to estimate a model from unaligned data. 4 A Probabilistic TSG Formalism For expository reasons (and to fill a gap in the lite"
P03-2041,C02-1049,0,\N,Missing
P03-2041,N01-1025,0,\N,Missing
P03-2041,W90-0102,0,\N,Missing
P04-1062,J90-2002,0,0.314832,"on. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers. Some tasks can thrive on a nearly pure diet of unlabeled data (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizing likelihood is not equivalent to maximizing task-defined accuracy (e.g., Merialdo, 1994). Here we f"
P04-1062,W99-0613,0,0.036962,"intains a local maximum as it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers. Some tasks can thrive on a nearly pure diet of unlabeled data (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizi"
P04-1062,N03-1006,0,0.0240312,"it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers. Some tasks can thrive on a nearly pure diet of unlabeled data (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizing likelihood is not equivalen"
P04-1062,A94-1009,0,0.143805,"Missing"
P04-1062,P02-1017,0,0.674977,"ately damaged performance only as much as EM did or did slightly better than EM (but still hurt). This is unsurprising: Merialdo’s result demonstrated that ML and maximizing accuracy are generally not the same; the EM algorithm consistently degraded the accuracy of his supervised models. SDA is simply another search algorithm with the same criterion as EM. SDA did do what it was expected to do—it used the initializer, repairing DA damage. 6 Grammar induction We turn next to the problem of statistical grammar induction: inducing parse trees over unlabeled text. An excellent recent result is by Klein and Manning (2002). The constituent-context model (CCM) they present is a generative, deficient channel model of POS tag strings given binary tree bracketings. We first review the model and describe a small modification that reduces the deficiency, then compare both models under EM and DA. 6.1 Constituent-context model Let (x, y) be a (tag sequence, binary tree) pair. xji denotes the subsequence of x from the ith to the jth word. Let yi,j be 1 if the yield from i to j is a constituent in the tree y and 0 if it is not. The CCM gives to a pair (x, y) the following h  probability:  Pr(x, y) = Pr(y) · Y 1≤i≤j≤|x|"
P04-1062,J94-2001,0,0.938591,"ation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizing likelihood is not equivalent to maximizing task-defined accuracy (e.g., Merialdo, 1994). Here we focus on the search error problem. Assume that one has a model for which improving likelihood really will improve accuracy (e.g., at predicting hidden part-of-speech (POS) tags or parse trees). Hence, we seek methods that tend to locate mountaintops rather than hilltops of the likelihood function. Alternatively, we might want methods that find hilltops with other desirable properties.1 1 Wang et al. (2003) suggest that one should seek a highIn §2 we review deterministic annealing (DA) and show how it generalizes the EM algorithm. §3 shows how DA can be used for parameter estimation f"
P04-1062,P93-1024,0,0.305449,"Missing"
P04-1062,P95-1026,0,0.0184867,"function and maintains a local maximum as it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers. Some tasks can thrive on a nearly pure diet of unlabeled data (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993,"
P04-3032,J98-2004,0,0.0347295,"riant algorithms. Although Dyna supports stack and queue (LIFO and FIFO) disciplines, its default is to use a priority queue prioritized by the size of the update. When parsing with real values, this quickly accumulates a good approximation of the inside probabilities, which permits heuristic early stopping before the agenda is empty. With viterbi values, it amounts to uniform-cost search for the best parse, and an item’s value is guaranteed not to change once it is nonzero. Dyna will soon allow user-defined priority functions (themselves dynamic programs), which can greatly speed up parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003). 2.4 Parameter Training Dyna provides facilities for training parameters. For example, from Fig. 1, it automatically derives the insideoutside (EM) algorithm for training PCFGs. How is this possible? Once the program of Fig. 1 has run, goal’s value is the probability of the input sentence under the grammar. This is a continuous function of the axiom values, which correspond to PCFG parameters (e.g., the weight of rewrite(np,Mary)). The function could be written out explicitly as a sum of products of sums of products of . . . of axiom values, with the details dependin"
P04-3032,P02-1001,1,0.876075,"Missing"
P04-3032,J99-4004,0,0.0268025,"rting at position 3. As usual, probabilistic, agenda-based lattice parsing comes for free, as does training. tempted similar syntheses (though without covering variant search and storage strategies, which Dyna handles). Shieber et al. (1995) (already noting that “many of the ideas we present are not new”) showed that several unweighted parsing algorithms can be specified in terms of inference rules, and used Prolog to implement an agendabased interpreter for such rules. McAllester (1999) made a similar case for static analysis algorithms, with a more rigorous discussion of indexing the chart. Goodman (1999) generalized this line of work to weighted parsing, using rules of the form c += a1 *a2 * · · · *ak (with side conditions allowed); he permitted values to fall in any semiring, and generalized the inside-outside algorithm. Our approach extends this to a wider variety of processing orders, and in particular shows how to use a prioritized agenda in the general case, using novel algorithms. We also extend to a wider class of formulas (e.g., neural networks). The closest implemented work we have found is PRISM (Zhou and Sato, 2003), a kind of probabilistic Prolog that claims to be efficient (thank"
P04-3032,N03-1016,0,0.0206753,"a supports stack and queue (LIFO and FIFO) disciplines, its default is to use a priority queue prioritized by the size of the update. When parsing with real values, this quickly accumulates a good approximation of the inside probabilities, which permits heuristic early stopping before the agenda is empty. With viterbi values, it amounts to uniform-cost search for the best parse, and an item’s value is guaranteed not to change once it is nonzero. Dyna will soon allow user-defined priority functions (themselves dynamic programs), which can greatly speed up parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003). 2.4 Parameter Training Dyna provides facilities for training parameters. For example, from Fig. 1, it automatically derives the insideoutside (EM) algorithm for training PCFGs. How is this possible? Once the program of Fig. 1 has run, goal’s value is the probability of the input sentence under the grammar. This is a continuous function of the axiom values, which correspond to PCFG parameters (e.g., the weight of rewrite(np,Mary)). The function could be written out explicitly as a sum of products of sums of products of . . . of axiom values, with the details depending on the sentence and gram"
P04-3032,P92-1017,0,0.0594185,"the Toolkit for Advanced Optimization (Benson et al., 2000) together with a softmax 6 DynaMITE = Dyna Module for Iterative Training and Estimation. technique to enforce sum-to-one constraints. It supports maximum-entropy training and the EM algorithm.7 DynaMITE provides an object-oriented API that allows independent variation of such diverse elements of training as the model parameterization, optimization algorithm, smoothing techniques, priors, and datasets. How about supervised or partly supervised training? The role of supervision is to permit some constituents to be built but not others (Pereira and Schabes, 1992). Lines 2–3 of Fig. 1 can simply be extended with an additional antecedent permitted(X,I,K), which must be either asserted or derived for constit(X,I,K) to be derived. In “soft” supervision, the permitted axioms may have values between 0 and 1.8 3 C++ Interface and Implementation A Dyna program compiles to a set of portable C++ classes that manage the items and perform inference. These classes can be used in a larger C++ application.9 This strategy keeps Dyna both small and convenient. A C++ chart object supports the computation of item values and gradients. It keeps track of built items, thei"
P04-3032,P00-1061,0,0.0198448,"efine the weights of certain terms. In the current implementation, every rule must have the restricted form c += a1 *a2 * · · · *ak (where each ai is an item or side condition and (X, +, *) is a semiring of values). The design for Dyna’s next version lifts this restriction to allow arbitrary, type-heterogeneous expressions on the right-hand side of an inference rule.11 7 It will eventually offer additional methods, such as deterministic annealing, simulated annealing, and iterative scaling. 8 Such item values are not probabilities. We are generally interested in log-linear models for parsing (Riezler et al., 2000) and other tasks. 9 We are also now developing a default application: a visual debugger that allows a user to assert axioms and explore the proof forest created during inference. 10 Interned values are hashed so that equal values are represented by equal pointers. It is very fast to compare and hash such representations. 11 That will make Dyna useful for a wider variety of non-NLP algo4 Some Further Applications Dyna is useful for any problem where partial hypotheses are assembled, or where consistency has to be maintained. It is already being used for parsing, syntax-based machine translation"
P04-3032,P90-1001,0,0.0745958,"to it. To enable fast lookup of the other items that participate in these inference rules, it generates code to maintain appropriate indices on the chart. Objects such as constit(vp,1,3) are called terms and may be recursively nested to any depth. (Items are just terms with values.) Dyna has a full first-order type system for terms, including primitive and disjunctive types, and permitting compile-time type inference. These types are compiled into C++ classes that support constructors and accessors, garbage-collection, subterm sharing (which may lead to asymptotic speedups, as in CCG parsing (Vijay-Shanker and Weir, 1990)), and interning.10 Dyna can import new primitive term types and value types from C++, as well as C++ functions to combine values and to user-define the weights of certain terms. In the current implementation, every rule must have the restricted form c += a1 *a2 * · · · *ak (where each ai is an item or side condition and (X, +, *) is a semiring of values). The design for Dyna’s next version lifts this restriction to allow arbitrary, type-heterogeneous expressions on the right-hand side of an inference rule.11 7 It will eventually offer additional methods, such as deterministic annealing, simul"
P04-3032,J97-3002,0,0.0388419,"ing algorithms for CFG and other formalisms can be simply written in terms of inference rules. Fig. 2 renders one such example in Dyna, namely Earley’s algorithm. Two features are worth noting: the use of recursively nested subterms such as lists, and the SIDE function, which evaluates to 1 or 0 according to whether its argument has a defined value yet. These side conditions are used here to prevent hypothesizing a constituent until there is a possible left context that calls for it. Several recent syntax-directed statistical machine translation models are easy to build in Dyna. The simplest (Wu, 1997) uses constit(np,3,5,np,4,8) to denote a NP spanning positions 3–5 in the English string that is aligned with an NP spanning positions 4–8 in the Chinese string. When training or decoding, the hypotheses of better-trained monolingual parsers can provide either hard or soft partial supervision (section 2.4). Dyna can manipulate finite-state transducers. For instance, the weighted arcs of the composed FST M1 ◦ M2 can be deduced from the arcs of M1 and M2 . Training M1 ◦ M2 back-propagates to train the original weights in M1 and M2 , as in (Eisner, 2002). 5 Speed and Code Size One of our future p"
P05-1044,J97-4005,0,0.00483211,"on function Z(θ) by adding up the u-scores of all paths through the WFSA. For a k-state WFSA, this equates to solving a linear system of k equations in k variables (Tarjan, 1981). But if the WFSA contains cycles this infinite sum may diverge. Alternatives to exact com2 These are exemplified by CRFs (Lafferty et al., 2001), which can be viewed alternately as undirected dynamic graphical models with a chain topology, as log-linear models over entire sequences with local features, or as WFSAs. Because “CRF” implies CL estimation, we use the term “WFSA.” putation, like random sampling (see, e.g., Abney, 1997), will not help to avoid this difficulty; in addition, convergence rates are in general unknown and bounds difficult to prove. We would prefer to sum over finitely many paths in Bi . 3.1 Parameter estimation (supervised) For log-linear models, both CL and JL estimation (Tab. 1) are available. In terms of Eq. 5, both set Ai = {(xi , yi∗ )}. The difference is in B: for JL, Bi = X × Y, so summing over Bi is equiva~ Belent to computing the partition function Z(θ). cause that sum is typically difficult, CL is preferred; Bi = {xi } × Y for xi , which is often tractable. For sequence models like WFSA"
P05-1044,W03-0407,0,0.0384081,"ov random field is over labeling configurations for all examples, not, as in our case, complex structured labels for a particular example. Hence their B (Eq. 5), though very large, was finite and could be sampled. 361 65 60 55 50 45 40 Tagging dictionary contains words with count ≥ 3: 85 50 8 lin g   el sp + m gr a   4 × 10 5 tri l D EL O RT RANS 1 T RANS 1 L ENGTH EM 1 smoothing parameter od e 0.1 m 0 m 80 85 75 80 75 70 70 65 65 60 60 55 55 50 50 45 45 40 gr a Foremost for future work is the “minimally supervised” paradigm in which a small amount of labeled data is available (see, e.g., Clark et al. (2003)). Unlike well-known “bootstrapping” approaches (Yarowsky, 1995), EM and CE have the possible advantage of maintaining posteriors over hidden labels (or structure) throughout learning; bootstrapping either chooses, for each example, a single label, or remains completely agnostic. One can envision a mixed objective function that tries to fit the labeled examples while discriminating unlabeled examples from their neighborhoods.8 Regardless of how much (if any) data are labeled, the question of good smoothing techniques requires more attention. Here we used a single zero-mean, constant-variance G"
P05-1044,P02-1001,1,0.390507,"(9) CE may also be viewed as an importance sampling approximation to EM, where the sample space X is replaced by N(xi ). We will demonstrate experimentally that CE is not just an approximation to EM; it makes sense from a modeling perspective. In §4, we will describe neighborhoods of sequences that can be represented as acyclic lattices built directly from an observed sequence. The sum over Bi is then the total u-score in our model of all paths in the neighborhood lattice. To compute this, intersect the WFSA and the lattice, obtaining a new acyclic WFSA, and sum the u-scores of all its paths (Eisner, 2002) using a simple dynamic programming algorithm akin to the forward algorithm. The sum over Ai may be computed similarly. CE with lattice neighborhoods is not confined to the WFSAs of this paper; when estimating weighted CFGs, the key algorithm is the inside algorithm for lattice parsing (Smith and Eisner, 2005). 3.3 Numerical optimization To maximize the neighborhood likelihood (Eq. 7), we apply a standard numerical optimization method (L-BFGS) that iteratively climbs the function using knowledge of its value and gradient (Liu and Nocedal, 1989). The partial derivative of LN with respect to the"
P05-1044,P99-1069,0,0.00849745,"Missing"
P05-1044,P01-1042,0,0.0302063,"any paths in Bi . 3.1 Parameter estimation (supervised) For log-linear models, both CL and JL estimation (Tab. 1) are available. In terms of Eq. 5, both set Ai = {(xi , yi∗ )}. The difference is in B: for JL, Bi = X × Y, so summing over Bi is equiva~ Belent to computing the partition function Z(θ). cause that sum is typically difficult, CL is preferred; Bi = {xi } × Y for xi , which is often tractable. For sequence models like WFSAs it is computed using a dynamic programming algorithm (the forward algorithm for WFSAs). Klein and Manning (2002) argue for CL on grounds of accuracy, but see also Johnson (2001). See Tab. 2; other contrast sets Bi are also possible. When Bi contains only xi paired with the current best competitor (ˆ y ) to yi∗ , we have a technique that resembles maximum margin training (Crammer and Singer, 2001). Note that yˆ will then change across training iterations, making Bi dynamic. 3.2 Parameter estimation (unsupervised) The difference between supervised and unsupervised learning is that in the latter case, Ai is forced to sum over label sequences y because they weren’t observed. In the unsupervised case, CE maximizes  X   Y LN θ~ = log u xi , y |θ~ y∈Y X i   u x, y |θ~"
P05-1044,W02-1002,0,0.00463799,"known and bounds difficult to prove. We would prefer to sum over finitely many paths in Bi . 3.1 Parameter estimation (supervised) For log-linear models, both CL and JL estimation (Tab. 1) are available. In terms of Eq. 5, both set Ai = {(xi , yi∗ )}. The difference is in B: for JL, Bi = X × Y, so summing over Bi is equiva~ Belent to computing the partition function Z(θ). cause that sum is typically difficult, CL is preferred; Bi = {xi } × Y for xi , which is often tractable. For sequence models like WFSAs it is computed using a dynamic programming algorithm (the forward algorithm for WFSAs). Klein and Manning (2002) argue for CL on grounds of accuracy, but see also Johnson (2001). See Tab. 2; other contrast sets Bi are also possible. When Bi contains only xi paired with the current best competitor (ˆ y ) to yi∗ , we have a technique that resembles maximum margin training (Crammer and Singer, 2001). Note that yˆ will then change across training iterations, making Bi dynamic. 3.2 Parameter estimation (unsupervised) The difference between supervised and unsupervised learning is that in the latter case, Ai is forced to sum over label sequences y because they weren’t observed. In the unsupervised case, CE ma"
P05-1044,J94-2001,0,0.98117,"nary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features. 1 Introduction Finding linguistic structure in raw text is not easy. The classical forward-backward and inside-outside algorithms try to guide probabilistic models to discover structure in text, but they tend to get stuck in local maxima (Charniak, 1993). Even when they avoid local maxima (e.g., through clever initialization) they typically deviate from human ideas of what the “right” structure is (Merialdo, 1994). One strategy is to incorporate domain knowledge into the model’s structure. Instead of blind HMMs or PCFGs, one could use models whose features ∗ This work was supported by a Fannie and John Hertz Foundation fellowship to the first author and NSF ITR grant IIS0313193 to the second author. The views expressed are not necessarily endorsed by the sponsors. The authors also thank three anonymous ACL reviewers for helpful comments, colleagues at JHU CLSP (especially David Smith and Roy Tromble) and Miles Osborne for insightful feedback, and Eric Goldlust and Markus Dreyer for Dyna language suppor"
P05-1044,P00-1061,0,0.0203746,"Missing"
P05-1044,N03-1028,0,0.102588,"Missing"
P05-1044,P04-1062,1,0.650356,"neighborhood lattice (via composition with the sentence, followed by determinization and minimization) is shown to its right. expectations in Eq. 10 are computed by the forwardbackward algorithm generalized to lattices. We emphasize that the function LN is not globally concave; our search will lead only to a local optimum.3 Therefore, as with all unsupervised statistical learning, the bias in the initialization of θ~ will affect the quality of the estimate and the performance of the method. In future we might wish to apply techniques for avoiding local optima, such as deterministic annealing (Smith and Eisner, 2004). 4 Lattice Neighborhoods We next consider some non-classical neighborhood functions for sequences. When X = Σ+ for some symbol alphabet Σ, certain kinds of neighborhoods have natural, compact representations. Given an input string x = hx1 , x2 , ..., xm i, we write xji for the substring hxi , xi+1 , ..., xj i and xm 1 for the whole string. Consider first the neighborhood consisting of all sequences generated by deleting a single symbol from the m-length sequence xm 1 : D EL 1W ORD(xm 1 ) = n o m x`−1 xm `+1 |1 ≤ ` ≤ m ∪ {x1 } 1 This set consists of m + 1 strings and can be compactly represent"
P05-1044,P95-1026,0,0.215723,"t, as in our case, complex structured labels for a particular example. Hence their B (Eq. 5), though very large, was finite and could be sampled. 361 65 60 55 50 45 40 Tagging dictionary contains words with count ≥ 3: 85 50 8 lin g   el sp + m gr a   4 × 10 5 tri l D EL O RT RANS 1 T RANS 1 L ENGTH EM 1 smoothing parameter od e 0.1 m 0 m 80 85 75 80 75 70 70 65 65 60 60 55 55 50 50 45 45 40 gr a Foremost for future work is the “minimally supervised” paradigm in which a small amount of labeled data is available (see, e.g., Clark et al. (2003)). Unlike well-known “bootstrapping” approaches (Yarowsky, 1995), EM and CE have the possible advantage of maintaining posteriors over hidden labels (or structure) throughout learning; bootstrapping either chooses, for each example, a single label, or remains completely agnostic. One can envision a mixed objective function that tries to fit the labeled examples while discriminating unlabeled examples from their neighborhoods.8 Regardless of how much (if any) data are labeled, the question of good smoothing techniques requires more attention. Here we used a single zero-mean, constant-variance Gaussian prior for all parameters. Better performance might be ac"
P05-1044,W03-0430,0,\N,Missing
P05-1044,W03-1019,0,\N,Missing
P06-1072,afonso-etal-2002-floresta,0,0.0287968,"Missing"
P06-1072,P96-1023,0,0.024445,"k, 1993). We seek here to capitalize on the intuition that, at least early in learning, the learner should search primarily for string-local structure, because most structure is local.1 By penalizing dependencies between two words that are farther apart in the string, we obtain consistent improvements in accuracy of the learned model (§3). We then explore how gradually changing δ over time affects learning (§4): we start out with a 2 Task and Model In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004). The model is a probabilistic head automaton grammar (Alshawi, 1996) with a “split” form that renders it parseable in cubic time (Eisner, 1997). Let x = hx1 , x2 , ..., xn i be the sentence. x0 is a special “wall” symbol, $, on the left of every sentence. A tree y is defined by a pair of functions yleft and yright (both {0, 1, 2, ..., n} → 2{1,2,...,n} ) that map each word to its sets of left and right dependents, respectively. The graph is constrained to be a projective tree rooted at $: each word except $ has a single parent, and there are no cycles ∗ This work was supported by a Fannie and John Hertz Foundation fellowship to the first author and NSF ITR gra"
P06-1072,W03-2405,0,0.0219645,"Missing"
P06-1072,H92-1030,0,0.0197222,"Missing"
P06-1072,J93-2003,0,0.00569574,"Missing"
P06-1072,W06-2920,0,0.0256923,"Missing"
P06-1072,P97-1003,0,0.225517,"Missing"
P06-1072,H05-1050,1,0.887179,"Missing"
P06-1072,P99-1059,1,0.867493,"Missing"
P06-1072,W05-1504,1,0.871521,"ll; see appendix.) Supervised model selection, which uses a small annotated development set, performs almost as well as the oracle, but unsupervised model selection, which selects the model that maximizes likelihood on an unannotated development set, is often much worse. 0 0.2 One way to bias a learner toward local explanations is to penalize longer attachments. This was done for supervised parsing in different ways by Collins (1997), Klein and Manning (2003), and McDonald et al. (2005), all of whom considered intervening material or coarse distance classes when predicting children in a tree. Eisner and Smith (2005) achieved speed and accuracy improvements by modeling distance directly in a ML-estimated (deficient) generative model. Here we use string distance to measure the length of a dependency link and consider the inclusion of a sum-of-lengths feature in the probabilistic model, for learning only. Keeping our original model, we will simply multiply into the probability of each tree another factor that penalizes long dependencies, giving:  3 -0.2 Figure 1: Test-set F1 performance of models trained by EM with a locality bias at varying δ. Each curve corresponds to a different language and shows perfo"
P06-1072,1997.iwpt-1.10,1,0.64705,"n learning, the learner should search primarily for string-local structure, because most structure is local.1 By penalizing dependencies between two words that are farther apart in the string, we obtain consistent improvements in accuracy of the learned model (§3). We then explore how gradually changing δ over time affects learning (§4): we start out with a 2 Task and Model In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004). The model is a probabilistic head automaton grammar (Alshawi, 1996) with a “split” form that renders it parseable in cubic time (Eisner, 1997). Let x = hx1 , x2 , ..., xn i be the sentence. x0 is a special “wall” symbol, $, on the left of every sentence. A tree y is defined by a pair of functions yleft and yright (both {0, 1, 2, ..., n} → 2{1,2,...,n} ) that map each word to its sets of left and right dependents, respectively. The graph is constrained to be a projective tree rooted at $: each word except $ has a single parent, and there are no cycles ∗ This work was supported by a Fannie and John Hertz Foundation fellowship to the first author and NSF ITR grant IIS-0313193 to the second author. The views expressed are not necessaril"
P06-1072,P02-1017,0,0.49026,"Missing"
P06-1072,P05-1012,0,0.0792022,"Missing"
P06-1072,W03-2403,0,0.0656734,"Missing"
P06-1072,P04-1062,1,0.903898,"Missing"
P06-1072,P05-1044,1,0.737676,"nd in the appendix. Introduction strong preference for short dependencies, then relax the preference. The new approach, structural annealing, often gives superior performance. An alternative structural bias is explored in §5. This approach views a sentence as a sequence of one or more yields of separate, independent trees. The points of segmentation are a hidden variable, and during learning all possible segmentations are entertained probabilistically. This allows the learner to accept hypotheses that explain the sentences as independent pieces. In §6 we briefly review contrastive estimation (Smith and Eisner, 2005a), relating it to the new method, and show its performance alone and when augmented with structural bias. Inducing a weighted context-free grammar from flat text is a hard problem. A common starting point for weighted grammar induction is the Expectation-Maximization (EM) algorithm (Dempster et al., 1977; Baker, 1979). EM’s mediocre performance (Table 1) reflects two problems. First, it seeks to maximize likelihood, but a grammar that makes the training data likely does not necessarily assign a linguistically defensible syntactic structure. Second, the likelihood surface is not globally conca"
P06-1072,E03-1008,0,0.036507,"Missing"
P06-1072,P95-1026,0,0.0145072,"Missing"
P06-1072,J93-2004,0,\N,Missing
P06-1072,P90-1034,0,\N,Missing
P06-1072,P04-1061,0,\N,Missing
P06-2101,P05-1022,0,0.0319783,"ch as (5) is really a function of γ ·θ, so the optimizer could exactly compensate for increased γ by decreasing the θ vector proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose γ and θ accordingly: 17.5 γ=∞ γ = 0.1 γ=1 γ = 10 −10 −5 0 5 10 Translation model 1 Figure 2: Loss and expected loss as one translation model’s weight varies: the gray line (γ = ∞) shows true B LEU (to be optimized in equation (2)). The black lines show the expected B LEU as γ in equation (5) increases from 0.1 toward ∞. for this honor, we follow Charniak and Johnson (2005) in summing their probabilities.2 Maximizing (4) is equivalent to minimizing P an upper bound on the expected 0/1 loss i (1 − pθ (yi∗ |xi )). Though the log makes it tractable, this remains a 0/1 objective that does not give partial credit to wrong answers, such as imperfect but useful translations. Most systems should be evaluated and preferably trained on less harsh metrics. 3 min Epγ,θ [L(yi,k )] − T · H(pγ,θ ) γ,θ In place of a schedule for raising γ, we now use a cooling schedule to lower T from ∞ to −∞, thereby weakening the preference for high entropy. The Lagrange multiplier T on entro"
P06-2101,W06-2929,1,0.739696,"ay subtract it from equation (4), which is equivalent to maximum a posteriori estimation with a diagonal Gaussian prior (Chen and Rosenfeld, 1999). The variance σd2 may reflect a prior belief about the potential usefulness of feature d, or may be tuned on heldout data. Another simple regularization method is to stop cooling before T reaches 0 (cf. Elidan and Friedman (2005)). If loss on heldout data begins to increase, we may be starting to overfit. This technique can be used along with annealing or quadratic regularization and can achieve additional accuracy gains, which we report elsewhere (Dreyer et al., 2006). 5 i i = E[log A] − E[log C] (8) P where the P integer random variables A = i ai and C = i ci count the number of posited and correctly posited elements over the whole corpus. To approximate E[g(A)], where g is any twicedifferentiable function (here g = log), we can approximate g locally by a quadratic, given by the Taylor expansion of g about A’s mean µA = E[A]: Computing Expected Loss E[g(A)] ≈ E[g(µA ) + (A − µA )g 0 (µA ) 1 + (A − µA )2 g 00 (µA )] 2 = g(µA ) + E[A − µA ]g 0 (µA ) 1 + E[(A − µA )2 ]g 00 (µA ) 2 1 2 00 = g(µA ) + σA g (µA ). 2 P 2 P 2 Here µA = i σai , since A i µai and σA"
P06-2101,P96-1024,0,0.0996297,"k annealing is significantly better than the other training methods, while minimum error is significantly worse. For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other. For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped. Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do. The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000). We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder. Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer e"
P06-2101,N03-1017,0,0.00784793,") as correct if the occurs at most twice in any reference translation of xi . This “clipping” does not affect the rest of our method. 8 Reasonable for a large corpus, by Lyapunov’s central limit theorem (allows non-identically distributed summands). 9 The component whose optimization achieved the lowest loss is then updated. The process iterates until no lower loss can be found. In contrast, Papineni (1999) proposed a linear programming method that may search along diagonal lines. 791 Optimization Procedure Max. like. Min. error Ann. min. risk and score the alignment template model’s phrases (Koehn et al., 2003). The Pharaoh phrase-based decoder uses precisely the setup of this paper. It scores a candidate translation (including its phrasal alignment to the original text) as θ · f , where f is a vector of the following 8 features: FinnishEnglish 5.02 10.27 16.43 FrenchEnglish 5.31 26.16 27.31 GermanEnglish 7.43 20.94 21.30 Table 1: B LEU 4n1 percentage on translating 2000sentence test corpora, after training the 8 experts on 100,000 sentence pairs and fitting their weights θ on 200 more, using settings tuned on a further 200. The current minimum risk annealing method achieved significant improvements"
P06-2101,N04-1022,0,0.34905,"of θ. Maximum likelihood is not sensitive to the starting value of θ because it has only a global optimum; annealed minimum risk is not sensitive to it either, because initially γ ≈ 0, making equation (6) flat. • maximum likelihood: a Gaussian prior with all σd2 at 0.25, 0.5, 1, or ∞ • minimum error: 1, 5, or 10 different random starting points, drawn from a uniform 792 Bleu 20 22 approximation from §5. We found this to perform significantly better on B LEU evaluation than if we trained with a “linearized” B LEU that summed per-sentence B LEU scores (as used in minimum Bayes risk decoding by Kumar and Byrne (2004)). Dependency Parsing 18 6.2 We trained dependency parsers for three different languages: Bulgarian, Dutch, and Slovenian.11 Input sentences to the parser were already tagged for parts of speech. Each parser employed 10 experts, each parameterized as a globally normalized loglinear model (Lafferty et al., 2001). For example, the 9th component of the feature vector fi,k (which described the k th parse of the ith sentence) was the log of that parse’s normalized probability according to the 9th expert. Each expert was trained separately to maximize the conditional probability of the correct parse"
P06-2101,P03-1021,0,0.659302,"ble translations or parse trees; or only the Ki most probable under ∗ This work was supported by an NSF graduate research fellowship for the first author and by NSF ITR grant IIS0313193 and ONR grant N00014-01-1-0685. The views expressed are not necessarily endorsed by the sponsors. We thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and the reviewers for helpful discussions and comments. 787 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 787–794, c Sydney, July 2006. 2006 Association for Computational Linguistics constant, hence not amenable to gradient descent. Och (2003) observed, however, that the piecewiseconstant property could be exploited to characterize the function exhaustively along any line in parameter space, and hence to minimize it globally along that line. By calling this global line minimization as a subroutine of multidimensional optimization, he was able to minimize (2) well enough to improve over likelihood maximization for training factored machine translation systems. Instead of considering only the best hypothesis for any θ, we can minimize risk, i.e., the expected loss under pθ across all analyses yi : XX def min Epθ L(yi,k ) = min L(yi,k"
P06-2101,P02-1040,0,0.0894142,"mbinations of models for dependency parsing and for machine translation. In machine translation, annealed minimum risk training achieves significant improvements in B LEU over standard minimum error training. We also show improvements in labeled dependency parsing. 1 Direct Minimization of Error Researchers in empirical natural language processing have expended substantial ink and effort in developing metrics to evaluate systems automatically against gold-standard corpora. The ongoing evaluation literature is perhaps most obvious in the machine translation community’s efforts to better B LEU (Papineni et al., 2002). Despite this research, parsing or machine translation systems are often trained using the much simpler and harsher metric of maximum likelihood. One reason is that in supervised training, the log-likelihood objective function is generally convex, meaning that it has a single global maximum that can be easily found (indeed, for supervised generative models, the parameters at this maximum may even have a closed-form solution). In contrast to the likelihood surface, the error surface for discrete structured prediction is not only riddled with local minima, but piecewise constant 2 Training Log-"
P06-2101,P04-1062,1,0.688242,"ccording to some schedule and optimize θ again. When γ is low, the smooth objective might allow us to pass over local minima that could open up at higher γ. Figure 3 shows how the smoothing is gradually weakened to reach the risk objective (3) as γ → 1 and approach the true error objective (2) as γ → ∞. Our risk minimization most resembles the work of Rao and Rose (2001), who trained an isolatedword speech recognition system for expected word-error rate. Deterministic annealing has also been used to tackle non-convex likelihood surfaces in unsupervised learning with EM (Ueda and Nakano, 1998; Smith and Eisner, 2004). Other work on “generalized probabilistic descent” minimizes a similar objective function but with γ held constant (Katagiri et al., 1998). Although the entropy is generally higher at lower values of γ, it varies as the optimization changes θ. In particular, a pure unregularized loglinear model such as (5) is really a function of γ ·θ, so the optimizer could exactly compensate for increased γ by decreasing the θ vector proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose γ and θ accordingly: 17.5 γ=∞ γ = 0.1 γ=1 γ = 10"
P06-2101,P05-1003,0,0.0175745,"ised generative models, the parameters at this maximum may even have a closed-form solution). In contrast to the likelihood surface, the error surface for discrete structured prediction is not only riddled with local minima, but piecewise constant 2 Training Log-Linear Models In this work, we focus on rescoring with loglinear models. In particular, our experiments consider log-linear combinations of a relatively small number of features over entire complex structures, such as trees or translations, known in some previous work as products of experts (Hinton, 1999) or logarithmic opinion pools (Smith et al., 2005). A feature in the combined model might thus be a log probability from an entire submodel. Giving this feature a small or negative weight can discount a submodel that is foolishly structured, badly trained, or redundant with the other features. For each sentence xi in our training corpus S, we are given Ki possible analyses yi,1 , . . . yi,Ki . (These may be all of the possible translations or parse trees; or only the Ki most probable under ∗ This work was supported by an NSF graduate research fellowship for the first author and by NSF ITR grant IIS0313193 and ONR grant N00014-01-1-0685. The v"
P06-2101,W04-3201,0,0.0139034,"ins on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000). We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder. Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004). Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones. The distinction is in using a loss function to calculate the required margins. 8 Conclusions Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems. Different methods can be used to attempt this global, non-convex optimization. We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to op"
P08-2021,W07-0414,1,0.83118,"2 shows tercomTERs of invWER-oracles (as computed by the aforementioned Dyna program) and oracle BLEU scores of the confusion networks. The confusion networks were generated using 9 MT systems applied to the Chinese GALE 2007 Dev set, which consists of roughly 550 Newswire segments, and 650 Weblog segments. The confusion networks which were generated with the ITGbased alignments gave significantly better oracle tercomTERs (significance tested with a Fisher sign test, p − 0.02) and better oracle BLEU scores. The BLEU oracle sentences were found using the dynamic-programming algorithm given in Dreyer et al. (2007) and measured using Philipp Koehn’s evaluation script. On the other hand, a comparison between the 1-best paths did not reveal significant differences that would favor one approach or the other (either in terms of tercomTER or BLEU). We also tried to understand which alignment method gives higher probability to paths “close” to the corresponding oracle. To do that, we computed the probability that a random path from a confusion network is within x edits from its oracle. This computation was done efficiently using finite-state-machine operations, and did not involve any randomization. Prelimina"
P08-2021,H05-1036,1,0.783103,"imple ITG that has one nonterminal and whose terminal symbols allow insertion, deletion, and substitution. The minimum-cost ITG tree can be found by dynamic programming. This leads to invWER (Leusch et al., 2003), which is defined as the minimum number of edits (insertions, deletions, substitutions and block shifts allowed by the ITG) needed to convert one string to another. In this paper, the minimuminvWER alignments are used for generating confusion networks. The alignments are found with a 11rule Dyna program (Dyna is an environment that facilitates the development of dynamic programs—see (Eisner et al., 2005) for more details). This program was further sped up (by about a factor of 2) with an A∗ search heuristic computed by additional code. Specifically, our admissible outside heuristic for aligning two substrings estimated the cost of aligning the words outside those substrings as if reordering those words were free. This was complicated somewhat by type/token issues and by the fact that we were aligning (possibly weighted) lattices. Moreover, the same Dyna program was used for the computation of the minimum invWER path in these confusion networks (oracle path), without having to invoke tercom nu"
P08-2021,2003.mtsummit-papers.32,0,0.803311,"because identifying the best hypothesis (relative to a known reference translation) means scoring all the composite hypotheses, of which there may be exponentially many. Given several systems’ automatic translations of the same sentence, we show how to combine them into a confusion network, whose various paths represent composite translations that could be considered in a subsequent rescoring step. We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings. Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better (in terms of BLEU and TER) than those in tercom-based confusion networks. 1 Introduction Large improvements in machine translation (MT) may result from combining different approaches to MT with mutually complementary strengths. System-level combination of translation outputs is a promising path towards such improvements. Yet there are some significant hurdles in this p"
P08-2021,E06-1005,0,0.266143,"choice among several phrases. Note that our contributions in this paper could be applied to arbitrary lattice topologies.) For example, Bangalore et al. (2001) show how to build a confusion network following a multistring alignment procedure of several MT outputs. The procedure (used primarily in biology, (Thompson et al., 1994)) yields monotone alignments that minimize the number of insertions, deletions, and substitutions. Unfortunately, monotone alignments are often poor, since machine translations (particularly from different models) can vary significantly in their word order. Thus, when Matusov et al. (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. The procedure described by Rosti et al. (2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment. (TER is defined as the minimum number of inser81 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 81–84, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics tions, deletions, substitutions and block shifts between two strings.) A remarkable feature of th"
P08-2021,P07-1040,0,0.571055,"ment step is difficult because different MT approaches usually reorder the translated words differently. Training the selection step is difficult because identifying the best hypothesis (relative to a known reference translation) means scoring all the composite hypotheses, of which there may be exponentially many. Given several systems’ automatic translations of the same sentence, we show how to combine them into a confusion network, whose various paths represent composite translations that could be considered in a subsequent rescoring step. We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings. Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better (in terms of BLEU and TER) than those in tercom-based confusion networks. 1 Introduction Large improvements in machine translation (MT) may result from combining different approaches to MT with mutually complementary strengt"
P08-2021,2006.amta-papers.25,0,0.391486,"ated words differently. Training the selection step is difficult because identifying the best hypothesis (relative to a known reference translation) means scoring all the composite hypotheses, of which there may be exponentially many. Given several systems’ automatic translations of the same sentence, we show how to combine them into a confusion network, whose various paths represent composite translations that could be considered in a subsequent rescoring step. We build our confusion networks using the method of Rosti et al. (2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings. Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better (in terms of BLEU and TER) than those in tercom-based confusion networks. 1 Introduction Large improvements in machine translation (MT) may result from combining different approaches to MT with mutually complementary strengths. System-level combination of translation outputs is a promising path towards su"
P08-2021,J97-3002,0,0.0363107,"k to be moved, it should have an exact match in its new position. However, this sometimes leads to counter-intuitive sequences of edits; for instance, for the sentence pair 82 “thomas jefferson says eat your vegetables” “eat your cereal thomas edison says”, tercom finds an edit sequence of cost 5, instead of the optimum 3. Furthermore, the block selection is done in a greedy manner, and the final outcome is dependent on the shift order, even when the above constraints are imposed. An alternative to tercom, considered in this paper, is to use the Inversion Transduction Grammar (ITG) formalism (Wu, 1997) which allows one to view the problem of alignment as a problem of bilingual parsing. Specifically, ITGs can be used to find the optimal edit sequence under the restriction that block moves must be properly nested, like parentheses. That is, if an edit sequence swaps adjacent substrings A and B of the original string, then any other block move that affects A (or B) must stay completely within A (or B). An edit sequence with this restriction corresponds to a synchronous parse tree under a simple ITG that has one nonterminal and whose terminal symbols allow insertion, deletion, and substitution."
P09-1067,N03-1017,0,0.0413473,"in natural language processing. Many systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree for a sentence. These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input. However, some systems resolve too many ambiguities. They recover additional latent variables— so-called nuisance variables—that are not of interest to the user.1 For example, though machine translation (MT) seeks to output a string, typical MT systems (Koehn et al., 2003; Chiang, 2007) 2 May and Knight (2006) have successfully used treeautomaton determinization to exactly marginalize out some of the nuisance variables, obtaining a distribution over parsed translations. However, they do not marginalize over these parse trees to obtain a distribution over translation strings. 1 These nuisance variables may be annotated in training data, but it is more common for them to be latent even there, i.e., there is no supervision as to their “correct” values. 593 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 593–601, c Suntec,"
P09-1067,W08-0402,1,0.591858,"pruning, which might be sensible for their relatively small task (e.g., input sentences of < 10 words). But, we are interested in improving the performance for a large-scale system, and thus their method is not a viable solution. Moreover, we observe in our experiments that using a larger n does not improve much over n = 2. 9 A reviewer asks about the interaction with backed-off language models. The issue is that the most compact finitestate representations of these (Allauzen et al., 2003), which exploit backoff structure, are not purely m-gram for any m. They yield more compact hypergraphs (Li and Khudanpur, 2008), but unfortunately those hypergraphs might not be treatable by Fig. 4—since where they back off to less than an n-gram, e is not informative enough for line 8 to find w. We sketch a method that works for any language model given by a weighted FSA, L. The variational family Q can be specified by any deterministic weighted FSA, Q, with weights parameterized by φ. One seeks φ to minimize (8). Intersect HG(x) with an “unweighted” version of Q in which all arcs have weight 1, so that Q does not prefer any string to another. By lifting weights into an expectation semiring (Eisner, 2002), it is then"
P09-1067,W09-0424,1,0.0903774,"Missing"
P09-1067,D07-1104,0,0.0215993,"ese to English translation task. Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the ngram matches between training and test sets in the foreign side. We also used a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the English side of the parallel corpora. We use GIZA++ (Och and Ney, 2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006)17 to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. We use standard beam-pruning and cube-pruning parameter settings, following Chiang (2007), when generating the hypergraphs. The NIST MT’03 set is used to tune model weights (e.g. those of (16)) and the scaling factor 5.3 Results of Different Variational Decoding Table 2 presents the BLEU results under different ways in using the variational models, as discussed in Section 3.2."
P09-1067,P05-1010,0,0.0117112,"in principle applied in a hypergraph with spurious ambiguity. 598 et al. (2008). In comparison, the optimal n-gram probabilities of (12) can be computed using the inside-outside algorithm, once and for all. Also, g(w |x) of (20) is not normalized over the history of w, while q ∗ (r(w) |h(w)) of (12) is. Lastly, the definition of the n-gram model is different. While the model (11) is a proper probabilistic model, the function of (22) is simply an approximation of the average n-gram precisions of y. A connection between variational decoding and minimum-risk decoding has been noted before (e.g., Matsuzaki et al. (2005)), but the derivation above makes the connection formal. DeNero et al. (2009) concurrently developed an alternate to MBR, called consensus decoding, which is similar to ours in practice although motivated quite differently. 5 Decoding scheme MT’04 Viterbi 35.4 MBR (K=1000) 35.8 Crunching (N =10000) 35.7 Crunching+MBR (N =10000) 35.8 Variational (1to4gram+wp+vt) 36.6 Table 1: BLEU scores for Viterbi, Crunching, MBR, and variational decoding. All the systems improve significantly over the Viterbi baseline (paired permutation test, p < 0.05). In each column, we boldface the best result as well as"
P09-1067,N06-1045,0,0.362842,"y systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree for a sentence. These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input. However, some systems resolve too many ambiguities. They recover additional latent variables— so-called nuisance variables—that are not of interest to the user.1 For example, though machine translation (MT) seeks to output a string, typical MT systems (Koehn et al., 2003; Chiang, 2007) 2 May and Knight (2006) have successfully used treeautomaton determinization to exactly marginalize out some of the nuisance variables, obtaining a distribution over parsed translations. However, they do not marginalize over these parse trees to obtain a distribution over translation strings. 1 These nuisance variables may be annotated in training data, but it is more common for them to be latent even there, i.e., there is no supervision as to their “correct” values. 593 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 593–601, c Suntec, Singapore, 2-7 August 2009. 2009 ACL an"
P09-1067,J05-2002,0,0.0329211,"Missing"
P09-1067,P00-1056,0,0.038583,"Experimental Setup We work on a Chinese to English translation task. Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the ngram matches between training and test sets in the foreign side. We also used a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the English side of the parallel corpora. We use GIZA++ (Och and Ney, 2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006)17 to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. We use standard beam-pruning and cube-pruning parameter settings, following Chiang (2007), when generating the hypergraphs. The NIST MT’03 set is used to tune model weights (e.g. those of (16)) and the scaling factor 5.3 Results of Different Variational Decoding Table 2 presents the BLEU results under different ways in using the variational models"
P09-1067,P03-1021,0,0.168639,"et al., 2001), which gives partial credit, we would also like to favor lower-order ngrams that are likely to appear in the reference, even if this means picking some less-likely highorder n-grams. For this reason, it is useful to interpolate different orders of variational models, X y ∗ = argmax θn · log qn∗ (y) (15) y∈T(x) n where n may include the value of zero, in which def case log q0∗ (y) = |y|, corresponding to a conventional word penalty feature. In the geometric interpolation above, the weight θn controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). Lastly, note that Viterbi and variational approximation are different ways to approximate the exact probability p(y |x), and each of them has pros and cons. Specifically, Viterbi approximation uses the correct probability of one complete 3.2.3 Decoding with q ∗ When translating x at runtime, the q ∗ constructed from HG(x) will be used as a surrogate for p during decoding. We want its most probable string: y ∗ = argmax q ∗ (y) whose edges correspond to n-grams (weighted with negative log-probabilities) and whose vertices correspond to (n −"
P09-1067,2001.mtsummit-papers.68,0,0.0732892,"number of hyperedges). y ∗ = argmax q ∗ (y) (14) y∈T(x) This ensures that y ∗ is a valid string in the original hypergraph HG(x), which will tend to rule out inadequate translations like “the mouse.” If our sole objective is to get a good approximation to p(y |x), we should just use a single n-gram model q ∗ whose order n is as large as possible, given computational constraints. This may be regarded as favoring n-grams that are likely to appear in the reference translation (because they are likely in the derivation forest). However, in order to score well on the BLEU metric for MT evaluation (Papineni et al., 2001), which gives partial credit, we would also like to favor lower-order ngrams that are likely to appear in the reference, even if this means picking some less-likely highorder n-grams. For this reason, it is useful to interpolate different orders of variational models, X y ∗ = argmax θn · log qn∗ (y) (15) y∈T(x) n where n may include the value of zero, in which def case log q0∗ (y) = |y|, corresponding to a conventional word penalty feature. In the geometric interpolation above, the weight θn controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or"
P09-1067,C96-2215,0,0.222629,"Missing"
P09-1067,P06-2101,1,0.849939,"t, we would also like to favor lower-order ngrams that are likely to appear in the reference, even if this means picking some less-likely highorder n-grams. For this reason, it is useful to interpolate different orders of variational models, X y ∗ = argmax θn · log qn∗ (y) (15) y∈T(x) n where n may include the value of zero, in which def case log q0∗ (y) = |y|, corresponding to a conventional word penalty feature. In the geometric interpolation above, the weight θn controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). Lastly, note that Viterbi and variational approximation are different ways to approximate the exact probability p(y |x), and each of them has pros and cons. Specifically, Viterbi approximation uses the correct probability of one complete 3.2.3 Decoding with q ∗ When translating x at runtime, the q ∗ constructed from HG(x) will be used as a surrogate for p during decoding. We want its most probable string: y ∗ = argmax q ∗ (y) whose edges correspond to n-grams (weighted with negative log-probabilities) and whose vertices correspond to (n − 1)-grams. However, because q ∗ only approximates p, y"
P09-1067,D08-1065,0,0.670982,"otal probability of its many derivations. However, finding the best string (e.g., during decoding) is then computationally intractable. Therefore, most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. Our particular variational distributions are parameterized as n-gram models. We also analytically show that interpolating these n-gram models for different n is similar to minimumrisk decoding for BLEU (Tromble et al., 2008). Experiments show that our approach improves the state of the art. 1 Introduction Ambiguity is a central issue in natural language processing. Many systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree for a sentence. These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input. However, some systems resolve too many ambiguities. They recover additional latent variables— so-called nuisance variables—that are not of int"
P09-1067,W06-3110,0,0.0137793,"n-gram types of different n, into several subsets Wn , each of which contains only the n-grams with a given length n. We can now rewrite (19) as follows, X θn · gn (y |x) (21) y ∗ = argmax Variational vs. Min-Risk Decoding In place of the MAP decoding, another commonly used decision rule is minimum Bayes risk (MBR): X l(y, y 0 )p(y 0 |x) y ∗ = argmin R(y) = argmin y y w∈N y y0 (17) n by assuming θw = θ|w |and, ( |y| if n = 0 gn (y |x) = P (22) w∈Wn g(w |x)cw (y) if n > 0 11 It would also be possible to interpolate with the N -best approximations (see Section 2.4), with some complications. 12 Zens and Ney (2006) use a similar decision rule as here and they also use posterior n-gram probabilities as feature functions, but their model estimation and decoding are over an N -best, which is trivial in terms of computation. 13 Already at (14), we explicitly ruled out translations y having no derivation at all in the hypergraph. However, suppose the hypergraph were very large (thanks to a large or smoothed translation model and weak pruning). Then (14)’s heuristic would fail to eliminate bad translations (“the mouse”), since nearly every string y ∈ Σ∗ would be derived as a translation with at least a tiny p"
P09-1067,P08-1025,0,0.0247397,"def entropy H(p, q) = − y p(y |x) log q(y), and 20 Both H(p, q) and Hd (p) involve an expectation over exponentially many derivations, but they can be computed in time only linear in the size of HG(x) using an expectation semiring P (Eisner, 2002). In particular, H(p, q) can be found as − d∈D(x) p(d |x) log q(Y(d)). 600 Many interesting research directions remain open. To approximate the intractable MAP decoding problem of (2), we can use different variational distributions other than the n-gram model of (11). Interpolation with other models is also interesting, e.g., the constituent model in Zhang and Gildea (2008). We might also attempt to minimize KL(q k p) rather than KL(p k q), in order to approximate the mode (which may be preferable since we care most about the 1-best translation under p) rather than the mean of p (Minka, 2005). One could also augment our n-gram models with non-local string features (Rosenfeld et al., 2001) provided that the expectations of these features could be extracted from the hypergraph. Variational inference can also be exploited to solve many other intractable problems in MT (e.g., word/phrase alignment and system combination). Finally, our method can be used for tasks be"
P09-1067,P03-1006,0,0.0398513,"whole translation string in the dynamic programming state. They alleviate the computation cost somehow by using aggressive beam pruning, which might be sensible for their relatively small task (e.g., input sentences of < 10 words). But, we are interested in improving the performance for a large-scale system, and thus their method is not a viable solution. Moreover, we observe in our experiments that using a larger n does not improve much over n = 2. 9 A reviewer asks about the interaction with backed-off language models. The issue is that the most compact finitestate representations of these (Allauzen et al., 2003), which exploit backoff structure, are not purely m-gram for any m. They yield more compact hypergraphs (Li and Khudanpur, 2008), but unfortunately those hypergraphs might not be treatable by Fig. 4—since where they back off to less than an n-gram, e is not informative enough for line 8 to find w. We sketch a method that works for any language model given by a weighted FSA, L. The variational family Q can be specified by any deterministic weighted FSA, Q, with weights parameterized by φ. One seeks φ to minimize (8). Intersect HG(x) with an “unweighted” version of Q in which all arcs have weigh"
P09-1067,P08-1024,0,0.150027,"decoding problem of (2) turns out to be NP-hard,4 as shown by Sima’an (1996) for a similar problem. 3 A hypergraph is analogous to a parse forest (Huang and Chiang, 2007). (A finite-state lattice is a special case.) It can be used to encode exponentially many hypotheses generated by a phrase-based MT system (e.g., Koehn et al. (2003)) or a syntax-based MT system (e.g., Chiang (2007)). 4 Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al., 2008). Maximum A Posterior (MAP) Decoding For a given input sentence x, a decoding method identifies a particular “best” output string y ∗ . The maximum a posteriori (MAP) decision rule is y ∗ = argmax p(y |x) #$ Figure 2: Tree ambiguity in syntax-based MT: two derivation trees yield the same translation string. In MT, spurious ambiguity occurs both in regular phrase-based systems (e.g., Koehn et al. (2003)), where different segmentations lead to the same translation string (Figure 1), and in syntax-based systems (e.g., Chiang (2007)), where different derivation trees yield the same string (Figure"
P09-1067,J07-2003,0,0.813337,"processing. Many systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree for a sentence. These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input. However, some systems resolve too many ambiguities. They recover additional latent variables— so-called nuisance variables—that are not of interest to the user.1 For example, though machine translation (MT) seeks to output a string, typical MT systems (Koehn et al., 2003; Chiang, 2007) 2 May and Knight (2006) have successfully used treeautomaton determinization to exactly marginalize out some of the nuisance variables, obtaining a distribution over parsed translations. However, they do not marginalize over these parse trees to obtain a distribution over translation strings. 1 These nuisance variables may be annotated in training data, but it is more common for them to be latent even there, i.e., there is no supervision as to their “correct” values. 593 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 593–601, c Suntec, Singapore, 2-7"
P09-1067,P09-1064,0,0.314107,"In comparison, the optimal n-gram probabilities of (12) can be computed using the inside-outside algorithm, once and for all. Also, g(w |x) of (20) is not normalized over the history of w, while q ∗ (r(w) |h(w)) of (12) is. Lastly, the definition of the n-gram model is different. While the model (11) is a proper probabilistic model, the function of (22) is simply an approximation of the average n-gram precisions of y. A connection between variational decoding and minimum-risk decoding has been noted before (e.g., Matsuzaki et al. (2005)), but the derivation above makes the connection formal. DeNero et al. (2009) concurrently developed an alternate to MBR, called consensus decoding, which is similar to ours in practice although motivated quite differently. 5 Decoding scheme MT’04 Viterbi 35.4 MBR (K=1000) 35.8 Crunching (N =10000) 35.7 Crunching+MBR (N =10000) 35.8 Variational (1to4gram+wp+vt) 36.6 Table 1: BLEU scores for Viterbi, Crunching, MBR, and variational decoding. All the systems improve significantly over the Viterbi baseline (paired permutation test, p < 0.05). In each column, we boldface the best result as well as all results that are statistically indistinguishable from it. In MBR, K is t"
P09-1067,P02-1001,1,0.263978,"(Li and Khudanpur, 2008), but unfortunately those hypergraphs might not be treatable by Fig. 4—since where they back off to less than an n-gram, e is not informative enough for line 8 to find w. We sketch a method that works for any language model given by a weighted FSA, L. The variational family Q can be specified by any deterministic weighted FSA, Q, with weights parameterized by φ. One seeks φ to minimize (8). Intersect HG(x) with an “unweighted” version of Q in which all arcs have weight 1, so that Q does not prefer any string to another. By lifting weights into an expectation semiring (Eisner, 2002), it is then possible to obtain expected transition counts in Q (where the expectation is taken under p), or other sufficient statistics needed to estimate φ. This takes only time O(|HG(x)|) when L is a left-to-right refinement of Q (meaning that any two prefix strings that reach the same state in L also reach the same state in Q), for then intersecting L or HG(x) with Q does not split any states. That is the case when L and Q are respectively pure m-gram and n-gram models with m ≥ n, as assumed in (12) and Figure 4. It is also the case when Q is a pure n-gram model and L is constructed not to"
P09-1067,W96-0214,0,0.1543,"Missing"
P09-1067,P07-1019,0,0.00627607,"scaling factor to adjust the sharpness of the distribution, the score s(x, y, d) is a learned linear combination of features of the triple (x, y, d), and Z(x) is a normalization constant. Note that p(y, d |x) = 0 if y 6= Y(d). Our derivation set D(x) is encoded in polynomial space, using a hypergraph or lattice.3 However, both |D(x)| and |T(x) |may be exponential in |x|. Since the marginalization needs to be carried out for each member of T(x), the decoding problem of (2) turns out to be NP-hard,4 as shown by Sima’an (1996) for a similar problem. 3 A hypergraph is analogous to a parse forest (Huang and Chiang, 2007). (A finite-state lattice is a special case.) It can be used to encode exponentially many hypotheses generated by a phrase-based MT system (e.g., Koehn et al. (2003)) or a syntax-based MT system (e.g., Chiang (2007)). 4 Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al., 2008). Maximum A Posterior (MAP) Decoding For a given input sentence x, a decoding method identifies a particular “best” output string y ∗ . The maximum a"
P09-1067,P02-1040,0,\N,Missing
P09-1067,N04-1022,0,\N,Missing
P13-1044,J85-1006,0,0.795823,"Missing"
P13-1044,P99-1059,1,0.719223,"l edges. Model Parameters For each set of model parameters Mc that should sum-to-one, we project the model parameters onto the Mc − 1 simplex by one of two methods: (1) normalize the infeasible parameters or (2) find the point on the simplex that has minimum Euclidean distance to the infeasible parameters using the algorithm of Chen and Ye (2011). For both methods, we can optionally apply add-λ smoothing before projecting. Parses Since we are interested in projecting the fractional parse onto the space of projective spanning trees, we can simply employ a dynamic programming parsing algorithm (Eisner and Satta, 1999) where the weight of each edge is given as the fraction of the edge variable. Only one of these projection techniques is needed. We then either use parsing to fill in the optimal parse trees given the projected model parameters, or use supervised parameter estimation to fill in the optimal model parameters given the projected parses. These correspond to the Viterbi E step and M step, respectively. We can locally 450 Experiments Upper bound on log!likelihood at root 7 We first analyze the behavior of our method on a toy synthetic dataset. Next, we compare various parameter settings for branch-a"
P13-1044,P10-2036,0,0.623972,"blem with local search is that it gets stuck in local optima. This is evident for grammar induction. An algorithm such as EM will find numerous different solutions when randomly initialized to different points (Charniak, 1993; Smith, 2006). A variety of ways to find better local optima have been explored, including heuristic initialization of the model parameters (Spitkovsky et al., 2010a), random restarts (Smith, 2006), and annealing (Smith and Eisner, 2006; Smith, 2006). Others have achieved accuracy improvements by enforcing linguistically motivated posterior constraints on the parameters (Gillenwater et al., 2010; Naseem et al., 2010), such as requiring most sentences to have verbs or encouraging nouns to be children of verbs or prepositions. Introduction Rich models with latent linguistic variables are popular in computational linguistics, but in general it is not known how to find their optimal parameters. In this paper, we present some “new” attacks for this common optimization setting, drawn from the mathematical programming toolbox. We focus on the well-studied but unsolved task of unsupervised dependency parsing (i.e., depen∗ This research was partially funded by the JHU Human Language Technolog"
P13-1044,N12-1069,0,0.400657,"Missing"
P13-1044,N10-1083,0,0.0609589,"Missing"
P13-1044,P04-1061,0,0.864426,"-467.5 can be pruned because no solution within its subspace could be better than the incumbent. We explore how one might instead search for a global optimum in parameter space, using branch-and-bound. Our method would eventually find the global maximum (up to a user-specified ) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum. dency grammar induction). This may be a particularly hard case, but its structure is typical. Many parameter estimation techniques have been attempted, including expectation-maximization (EM) (Klein and Manning, 2004; Spitkovsky et al., 2010a), contrastive estimation (Smith and Eisner, 2006; Smith, 2006), Viterbi EM (Spitkovsky et al., 2010b), and variational EM (Naseem et al., 2010; Cohen et al., 2009; Cohen and Smith, 2009). These are all local search techniques, which improve the parameters by hill-climbing. As an illustrative case, we study a generative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints. We use th"
P13-1044,E12-1042,0,0.0594862,"incorporates the tree constraints directly into our convex relaxation and embeds the relaxation in a branch-and-bound algorithm capable of solving the original DMV maximum-likelihood estimation problem. Spectral learning constitutes a wholly different family of consistent estimators, which achieve efficiency because they sidestep maximizing the nonconvex likelihood function. Hsu et al. (2009) introduced a spectral learner for a large class of HMMs. For supervised parsing, spectral learning has been used to learn latent variable PCFGs (Cohen et al., 2012) and hidden-state dependency grammars (Luque et al., 2012). Alas, there are not yet any spectral learning methods that recover latent tree structure, as in grammar induction. Several integer linear programming (ILP) formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009; Riedel et al., 2012) inspired our definition of grammar induction as a MP. Recent work uses branch-and-bound for decoding with non-local features (Qian and Liu, 2013). These differ from our work by treating the model parameters as constants, thereby yielding a linear objective. For semi-supervised dependency parsing, Wang et al. (2008) used a convex objecti"
P13-1044,N09-1009,0,0.0835568,"ould eventually find the global maximum (up to a user-specified ) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum. dency grammar induction). This may be a particularly hard case, but its structure is typical. Many parameter estimation techniques have been attempted, including expectation-maximization (EM) (Klein and Manning, 2004; Spitkovsky et al., 2010a), contrastive estimation (Smith and Eisner, 2006; Smith, 2006), Viterbi EM (Spitkovsky et al., 2010b), and variational EM (Naseem et al., 2010; Cohen et al., 2009; Cohen and Smith, 2009). These are all local search techniques, which improve the parameters by hill-climbing. As an illustrative case, we study a generative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints. We use the Reformulation Linearization Technique to produce convex relaxations during branch-and-bound. Although these techniques do not yet provide a practical solution to our instance of this NP-hard problem, they someti"
P13-1044,P10-1152,0,0.194974,"parameters are true log-probabilities. Concretely, (1) specifies the Viterbi EM objective: the total log-probability of the best parse trees under the parameters θ, given by a sum of log-probabilities θm of the individual steps needed to generate the tree, as encoded by the features fm . The (nonlinear) sum-to-one constraints on the 1 This objective might not be a great sacrifice: Spitkovsky et al. (2010b) present evidence that hard EM can outperform soft EM for grammar induction in a hill-climbing setting. We use it because it is a quadratic objective. However, maximizing it remains NP-hard (Cohen and Smith, 2010). 445 Variables: θm Log-probability for feature m fm Corpus-wide feature count for m esij Indicator of an arc from i to j in tree s Indices and constants: m Feature / model parameter index s Sentence index c Conditional distribution index M Number of model parameters C Number of conditional distributions Mc cth Set of feature indices that sum to 1.0 S Number of sentences Ns Number of words in the sth sentence Objective and constraints: X max θm fm (1) coming arcs, and the arcs form a connected graph. For each sentence, s, the variable φsij indicates the amount of flow traversing the arc from i"
P13-1044,P09-1039,0,0.418877,"simply specify the range of possible values for the model parameters and their integer count variables. Our experiments use the dependency model with valence (DMV) (Klein and Manning, 2004). This generative model defines a joint distribution over the sentences and their dependency trees. We encode the DMV using integer linear constraints on the arc variables e and feature counts f . These will constitute the model constraints in (3). The constraints must declaratively specify that the arcs form a valid dependency tree and that the resulting feature values are as defined by the DMV. (k,l)∈Xij (Martins et al., 2009) eskl ≤ Ns (1 − esij ), ∀s, i, j (9) DMV Feature Counts The DMV generates a dependency tree recursively as follows. First the head word of the sentence is generated, t ∼ Discrete(θ root ), where θ root is a subvector of θ. To generate its children on the left side, we flip a coin to decide whether an adjacent child is generated, d ∼ Bernoulli(θ dec.L.0,t ). If the coin flip d comes up continue, we sample the word of that child as t0 ∼ Discrete(θ child.L,t ). We continue generating non-adjacent children in this way, using coin weights θ dec.L.≥ 1,t until the coin comes up stop. We repeat this p"
P13-1044,P12-1024,0,0.0614964,"stead initialized EM on the DMV. By contrast, our approach incorporates the tree constraints directly into our convex relaxation and embeds the relaxation in a branch-and-bound algorithm capable of solving the original DMV maximum-likelihood estimation problem. Spectral learning constitutes a wholly different family of consistent estimators, which achieve efficiency because they sidestep maximizing the nonconvex likelihood function. Hsu et al. (2009) introduced a spectral learner for a large class of HMMs. For supervised parsing, spectral learning has been used to learn latent variable PCFGs (Cohen et al., 2012) and hidden-state dependency grammars (Luque et al., 2012). Alas, there are not yet any spectral learning methods that recover latent tree structure, as in grammar induction. Several integer linear programming (ILP) formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009; Riedel et al., 2012) inspired our definition of grammar induction as a MP. Recent work uses branch-and-bound for decoding with non-local features (Qian and Liu, 2013). These differ from our work by treating the model parameters as constants, thereby yielding a linear objective. For semi-supervised de"
P13-1044,P08-1061,0,0.359095,"dependency grammars (Luque et al., 2012). Alas, there are not yet any spectral learning methods that recover latent tree structure, as in grammar induction. Several integer linear programming (ILP) formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009; Riedel et al., 2012) inspired our definition of grammar induction as a MP. Recent work uses branch-and-bound for decoding with non-local features (Qian and Liu, 2013). These differ from our work by treating the model parameters as constants, thereby yielding a linear objective. For semi-supervised dependency parsing, Wang et al. (2008) used a convex objective, combining unsupervised least squares loss and a supervised large margin loss, This does not apply to our unsupervised setting. Branch-and-bound has also been applied to semi-supervised SVM training, a nonconvex search problem (Chapelle et al., 2007), with a relaxation derived from the dual. Projections A pessimistic bound, from the projecting step, will correspond to a feasible but not necessarily optimal solution to the original problem. We propose several methods for obtaining pessimistic bounds during the branch-and-bound search, by projecting and improving the sol"
P13-1044,D10-1120,0,0.515919,"ace, using branch-and-bound. Our method would eventually find the global maximum (up to a user-specified ) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum. dency grammar induction). This may be a particularly hard case, but its structure is typical. Many parameter estimation techniques have been attempted, including expectation-maximization (EM) (Klein and Manning, 2004; Spitkovsky et al., 2010a), contrastive estimation (Smith and Eisner, 2006; Smith, 2006), Viterbi EM (Spitkovsky et al., 2010b), and variational EM (Naseem et al., 2010; Cohen et al., 2009; Cohen and Smith, 2009). These are all local search techniques, which improve the parameters by hill-climbing. As an illustrative case, we study a generative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints. We use the Reformulation Linearization Technique to produce convex relaxations during branch-and-bound. Although these techniques do not yet provide a practical solution to our i"
P13-1044,petrov-etal-2012-universal,0,0.0788834,"Missing"
P13-1044,Q13-1004,0,0.0579955,"a spectral learner for a large class of HMMs. For supervised parsing, spectral learning has been used to learn latent variable PCFGs (Cohen et al., 2012) and hidden-state dependency grammars (Luque et al., 2012). Alas, there are not yet any spectral learning methods that recover latent tree structure, as in grammar induction. Several integer linear programming (ILP) formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009; Riedel et al., 2012) inspired our definition of grammar induction as a MP. Recent work uses branch-and-bound for decoding with non-local features (Qian and Liu, 2013). These differ from our work by treating the model parameters as constants, thereby yielding a linear objective. For semi-supervised dependency parsing, Wang et al. (2008) used a convex objective, combining unsupervised least squares loss and a supervised large margin loss, This does not apply to our unsupervised setting. Branch-and-bound has also been applied to semi-supervised SVM training, a nonconvex search problem (Chapelle et al., 2007), with a relaxation derived from the dual. Projections A pessimistic bound, from the projecting step, will correspond to a feasible but not necessarily op"
P13-1044,W06-1616,0,0.0353906,"learning constitutes a wholly different family of consistent estimators, which achieve efficiency because they sidestep maximizing the nonconvex likelihood function. Hsu et al. (2009) introduced a spectral learner for a large class of HMMs. For supervised parsing, spectral learning has been used to learn latent variable PCFGs (Cohen et al., 2012) and hidden-state dependency grammars (Luque et al., 2012). Alas, there are not yet any spectral learning methods that recover latent tree structure, as in grammar induction. Several integer linear programming (ILP) formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009; Riedel et al., 2012) inspired our definition of grammar induction as a MP. Recent work uses branch-and-bound for decoding with non-local features (Qian and Liu, 2013). These differ from our work by treating the model parameters as constants, thereby yielding a linear objective. For semi-supervised dependency parsing, Wang et al. (2008) used a convex objective, combining unsupervised least squares loss and a supervised large margin loss, This does not apply to our unsupervised setting. Branch-and-bound has also been applied to semi-supervised SVM training, a nonconvex se"
P13-1044,D12-1067,0,0.0208532,"of consistent estimators, which achieve efficiency because they sidestep maximizing the nonconvex likelihood function. Hsu et al. (2009) introduced a spectral learner for a large class of HMMs. For supervised parsing, spectral learning has been used to learn latent variable PCFGs (Cohen et al., 2012) and hidden-state dependency grammars (Luque et al., 2012). Alas, there are not yet any spectral learning methods that recover latent tree structure, as in grammar induction. Several integer linear programming (ILP) formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009; Riedel et al., 2012) inspired our definition of grammar induction as a MP. Recent work uses branch-and-bound for decoding with non-local features (Qian and Liu, 2013). These differ from our work by treating the model parameters as constants, thereby yielding a linear objective. For semi-supervised dependency parsing, Wang et al. (2008) used a convex objective, combining unsupervised least squares loss and a supervised large margin loss, This does not apply to our unsupervised setting. Branch-and-bound has also been applied to semi-supervised SVM training, a nonconvex search problem (Chapelle et al., 2007), with a"
P13-1044,P06-1072,1,0.946747,"than the incumbent. We explore how one might instead search for a global optimum in parameter space, using branch-and-bound. Our method would eventually find the global maximum (up to a user-specified ) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum. dency grammar induction). This may be a particularly hard case, but its structure is typical. Many parameter estimation techniques have been attempted, including expectation-maximization (EM) (Klein and Manning, 2004; Spitkovsky et al., 2010a), contrastive estimation (Smith and Eisner, 2006; Smith, 2006), Viterbi EM (Spitkovsky et al., 2010b), and variational EM (Naseem et al., 2010; Cohen et al., 2009; Cohen and Smith, 2009). These are all local search techniques, which improve the parameters by hill-climbing. As an illustrative case, we study a generative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints. We use the Reformulation Linearization Technique to produce convex relaxations durin"
P13-1044,N10-1116,0,0.324841,"use no solution within its subspace could be better than the incumbent. We explore how one might instead search for a global optimum in parameter space, using branch-and-bound. Our method would eventually find the global maximum (up to a user-specified ) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum. dency grammar induction). This may be a particularly hard case, but its structure is typical. Many parameter estimation techniques have been attempted, including expectation-maximization (EM) (Klein and Manning, 2004; Spitkovsky et al., 2010a), contrastive estimation (Smith and Eisner, 2006; Smith, 2006), Viterbi EM (Spitkovsky et al., 2010b), and variational EM (Naseem et al., 2010; Cohen et al., 2009; Cohen and Smith, 2009). These are all local search techniques, which improve the parameters by hill-climbing. As an illustrative case, we study a generative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints. We use the Reformulation Lineariza"
P13-1044,W10-2902,0,0.565358,"use no solution within its subspace could be better than the incumbent. We explore how one might instead search for a global optimum in parameter space, using branch-and-bound. Our method would eventually find the global maximum (up to a user-specified ) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum. dency grammar induction). This may be a particularly hard case, but its structure is typical. Many parameter estimation techniques have been attempted, including expectation-maximization (EM) (Klein and Manning, 2004; Spitkovsky et al., 2010a), contrastive estimation (Smith and Eisner, 2006; Smith, 2006), Viterbi EM (Spitkovsky et al., 2010b), and variational EM (Naseem et al., 2010; Cohen et al., 2009; Cohen and Smith, 2009). These are all local search techniques, which improve the parameters by hill-climbing. As an illustrative case, we study a generative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints. We use the Reformulation Lineariza"
P14-1073,D12-1032,1,0.94161,"by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multiple steps in the generative process: Taylor Swift → T-Swift → T-Swizzle Our model learns without supervision that these all refer to the the same entity. Such creative spellings are especially common on Twi"
P14-1073,P98-1012,0,0.882822,"5 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 775–785, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics §7 A minimum Bayes risk decoding procedure to pick an output clustering. The procedure is applicable to any model capable of producing a posterior over coreference decisions. We evaluate our approach by comparing to several baselines on datasets from three different genres: Twitter, newswire, and blogs. 2 Overview and Related Work Cross-document coreference resolution (CDCR) was first introduced by Bagga and Baldwin (1998b). Most approaches since then are based on the intuitions that coreferent names tend to have “similar” spellings and tend to appear in “similar” contexts. The distinguishing feature of our system is that both notions of similarity are learned together without supervision. We adopt a “phylogenetic” generative model of coreference. The basic insight is that coreference is created when an author thinks of an entity that was mentioned earlier in a similar context, and mentions it again in a similar way. The author may alter the name mention string when copying it, but both names refer to the same"
P14-1073,D08-1029,0,0.136929,"tic edit distance from known pairs of similar names (Ristad and Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multip"
P14-1073,D08-1113,1,0.913091,"Missing"
P14-1073,D13-1203,0,0.0253176,"on model, which we take to be a variant of stochastic edit distance (Ristad and Yianilos, 1996). Rather than fixing its parameters before we begin CDCR, we learn them (without supervision) as part of CDCR, by training from samples of reconstructed phylogenies. Name similarity is also an important component of within-document coreference resolution, and efforts in that area bear resemblance to our approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-dependent Chinese restaurant process is used to pick previous coreferent mentions within a document. Similarly, Durrett and Klein (2013) learn a mention similarity model based on labeled data. Our cross-document setting has no observed mention ordering and no observed entities: we must sum over all possibilities, a challenging inference problem. The second major component of CDCR is context-based disambiguation of similar or identical names that refer to the same entity. Like Kozareva and Ravi (2011) and Green et al. (2012) we use topics as the contexts, but learn mention topics jointly with other model parameters. 3 Generative Model of Coreference Let x = (x1 , . . . , xN ) denote an ordered sequence of distinct named-entity"
P14-1073,W11-2202,0,0.118784,"oximating w(p0 , x) as 1 or 0 according to whether p0 .n = x.n. We also omitted the IMH step from section 5.3. The other results we report do not use pragmatics at all, since we found that it gave only a slight improvement on Twitter. Unlike our other datasets, mentions are not annotated with entities: the reference consists of a table of 126 entities, where each row is the canonical name of one entity. Baselines. We compare to the system results reported in Figure 2 of Yogatama et al. (2012). This includes a baseline hierarchical clustering approach, the “EEA” name canonicalization system of Eisenstein et al. (2011), as well the model proposed by Yogatama et al. (2012). Like the output of our model, the output of their hierarchical clustering baseline is a mention clustering, and therefore must be mapped to a table of canonical entity names to compare to the reference table. Procedure & Results We tune our method as in previous experiments, on the initialization data used by Yogatama et al. (2012) which consists of a subset of 700 documents of the full dataset. The tuned model then produced a mention clustering on the full political blog corpus. As the mapping from clusters to a table is not fully detail"
P14-1073,N12-1007,1,0.944979,"who or what a name refers to. For instance, Wikipedia contains more than 100 variations of the name Barack Obama as redirects to the U.S. President article, including: President Obama Barack H. Obama, Jr. Barak Obamba Barry Soetoro To relate different names, one solution is to use specifically tailored measures of name similarity such as Jaro-Winkler similarity (Winkler, 1999; Cohen et al., 2003). This approach is brittle, however, and fails to adapt to the test data. Another option is to train a model like stochastic edit distance from known pairs of similar names (Ristad and Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 201"
P14-1073,N13-1122,0,0.183356,"Missing"
P14-1073,N10-1061,0,0.0246454,"tant component of a CDCR system is its model of name similarity (Winkler, 1999; Porter and Winkler, 1997), which is often fixed up front. This role is played in our system by the name mutation model, which we take to be a variant of stochastic edit distance (Ristad and Yianilos, 1996). Rather than fixing its parameters before we begin CDCR, we learn them (without supervision) as part of CDCR, by training from samples of reconstructed phylogenies. Name similarity is also an important component of within-document coreference resolution, and efforts in that area bear resemblance to our approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-dependent Chinese restaurant process is used to pick previous coreferent mentions within a document. Similarly, Durrett and Klein (2013) learn a mention similarity model based on labeled data. Our cross-document setting has no observed mention ordering and no observed entities: we must sum over all possibilities, a challenging inference problem. The second major component of CDCR is context-based disambiguation of similar or identical names that refer to the same entity. Like Kozareva and Ravi (2011) and Green et al. (2012) we use topics as"
P14-1073,P10-1117,0,0.0184333,"el would be a more appropriate fit for a far larger corpus. Larger corpora also offer stronger signals that might enable our Monte Carlo methods to mix faster and detect regularities more accurately. A common error of our system is to connect mentions that share long substrings, such as different PERSONs who share a last name, or different ORGANIZATIONs that contain University of. A more powerful name mutation than the one we use here would recognize entire words, for example inserting a common title or replacing a first name with its common nickname. Modeling the internal structure of names (Johnson, 2010; Eisenstein et al., 2011; Yogatama et al., 2012) in the mutation model is a promising future direction. 9 Conclusions Our primary contribution consists of new modeling ideas, and associated inference techniques, for the problem of cross-document coreference resolution. We have described how writers systematically plunder (φ) and then systematically modify (θ) the work of past writers. Inference under such models could also play a role in tracking evolving memes and social influence, not merely in establishing strict coreference. Our model also provides an alternative to the distance-dependent"
P14-1073,W11-2213,0,0.0198391,"t area bear resemblance to our approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-dependent Chinese restaurant process is used to pick previous coreferent mentions within a document. Similarly, Durrett and Klein (2013) learn a mention similarity model based on labeled data. Our cross-document setting has no observed mention ordering and no observed entities: we must sum over all possibilities, a challenging inference problem. The second major component of CDCR is context-based disambiguation of similar or identical names that refer to the same entity. Like Kozareva and Ravi (2011) and Green et al. (2012) we use topics as the contexts, but learn mention topics jointly with other model parameters. 3 Generative Model of Coreference Let x = (x1 , . . . , xN ) denote an ordered sequence of distinct named-entity mentions in documents d = (d1 , . . . , dD ). We assume that each document has a (single) known language, and that its mentions and their types have been identified by a named-entity recognizer. We use the objectoriented notation x.v for attribute v of mention x. Our model generates an ordered sequence x although we do not observe its order. Thus each mention x has l"
P14-1073,D12-1045,0,0.0583782,"n et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multiple steps in the generative process: Taylor Swift → T-Swift → T-Swizzle Our m"
P14-1073,C10-2121,1,0.904437,"Missing"
P14-1073,D11-1141,0,0.149773,"Missing"
P14-1073,P11-1080,0,0.123079,"Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multiple steps in the generative process: Taylor Swift → T-Swift"
P14-1073,P12-1040,0,0.237767,"Missing"
P14-1073,N09-1054,0,0.0156788,"ld) within-document coreference chain as the canonical mention, ignoring other mentions in the chain; we follow the same procedure in our experiments.11 Baselines & Procedure. We use the same baselines as in §8.1. On development data, modeling pragmatics as in §4.2 gave large improvements for organizations (8 points in F-measure), correcting the tendency to assume that short names like CIA were coincidental homonyms. Hence we allowed γ > 0 and tuned it on development data.12 Results are in Table 2. 8.3 Blogs Data. The CMU political blogs dataset consists of 3000 documents about U.S. politics (Yano et al., 2009). Preprocessed as described in Yogatama et al. (2012), the data consists of 10647 entity mentions. 11 10 Our single-threaded implementation took around 15 minutes per fold of the Twitter corpus on a personal laptop with a 2.3 Ghz Intel Core i7 processor (including time required to parse the data files). Typical acceptance rates for ordering and topic proposals ranged from 0.03 to 0.08. 782 That is, each within-document coreference chain is mapped to a single mention as a preprocessing step. 12 We used only a simplified version of the pragmatic model, approximating w(p0 , x) as 1 or 0 according"
P14-1073,P12-1072,0,0.345852,"stances of name variation that we would like our model to be able to learn. We also report the performance of different ablations of our full approach, in order to see which consistently helped across the different splits. We report additional experiments on the ACE 2008 corpus, and on a political blog corpus, to demonstrate that our approach is applicable in different settings. For Twitter and ACE 2008, we report the standard B3 metric (Bagga and Baldwin, 1998a). For the political blog dataset, the reference does not consist of entity annotations, and so we follow the evaluation procedure of Yogatama et al. (2012). From a single phylogeny p, we deterministically obtain a clustering e by removing the root ♦. Each of the resulting connected components corresponds to a cluster of mentions. Our model gives a distribution over phylogenies p (given observations x and learned parameters Φ)—and thus gives a posterior distribution over clusterings e, which can be used to answer various queries. A traditional query is to request a single clustering e. We prefer the clustering e∗ that minimizes Bayes risk (MBR) (Bickel and Doksum, 1977): X i,j: xi ∼xj where ∼ denotes coreference according to e0 . As explained abo"
P14-2102,P14-1073,1,0.827801,"a contextual edit tend to raise or lower its probability (and the regularizer encourages such generalization). Each contextual edit (C, e) can be characterized as a 5-tuple (s, t, C1 , C20 , C3 ): it replaces s ∈ Σx ∪ {} with t ∈ Σy ∪ {} when s falls between C1 and C20 (so C2 = sC20 ) and t is preceded by C3 . Then each of the 14 features of (C, e) indicates that a particular subset of this 5-tuple has a particular value. The subset always includes s, t, or both. It never includes C1 or C20 without s, and never includes C3 without t. (Mays et al., 1991; Church and Gale, 1991; Kukich, 1992; Andrews et al., 2014). To eliminate experimental confounds, we use no dictionary or language model as one would in practice, but directly evaluate our ability to model p(correct |misspelled). Consider (xk , yk ) = (feeel, feel). Our model defines p(y |xk ) for all y. Our training objective (section 6) tries to make this large for y = yk . A contextual edit model learns here that e 7→  is more likely in the context of ee. We report on test data how much probability mass lands on the true yk . We also report how much mass lands “near” yk , by measuring the expected edit distance of the predicted y to Pthe truth. Ex"
P14-2102,N10-1083,0,0.0420762,"Missing"
P14-2102,P05-1057,0,0.0422806,"1 If N2 = 0, so that we do not condition on xi+1 , we must still condition on whether xi+1 = EOS (a single bit). We gloss over special handling for N2 = 0; but it is in our code. 625 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 625–630, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics This model is reminiscent of conditional models in MT that perform stepwise generation of one string or structure from another—e.g., string alignment models with contextual features (Cherry and Lin, 2003; Liu et al., 2005; Dyer et al., 2013), or tree transducers (Knight and Graehl, 2005). 3 • For each state q, the halt action and the arcs from q with input label  must have total probability of 1. (These are the available choices if there is no next input character.) • Every state q must be co-accessible, i.e., there must be a path of probability > 0 from q to some qF . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label  and probability 1.) Probabilistic FSTs We will construct a probabilistic finite-state transducer (PFST"
P14-2102,P03-1012,0,0.0533545,"t alignment of y to x. 1 If N2 = 0, so that we do not condition on xi+1 , we must still condition on whether xi+1 = EOS (a single bit). We gloss over special handling for N2 = 0; but it is in our code. 625 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 625–630, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics This model is reminiscent of conditional models in MT that perform stepwise generation of one string or structure from another—e.g., string alignment models with contextual features (Cherry and Lin, 2003; Liu et al., 2005; Dyer et al., 2013), or tree transducers (Knight and Graehl, 2005). 3 • For each state q, the halt action and the arcs from q with input label  must have total probability of 1. (These are the available choices if there is no next input character.) • Every state q must be co-accessible, i.e., there must be a path of probability > 0 from q to some qF . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label  and probability 1.) Probabilistic FSTs We will construct a probabilistic finite-stat"
P14-2102,J98-2005,0,0.103253,"ns and functions (Mohri, 1997; Eisner, 2001). A PFST is a two-tape generalization of the wellknown nondeterministic finite-state acceptor. It is a finite directed multigraph where each arc is labeled with an input in Σx ∪ {}, an output in Σy ∪{}, and a probability in [0, 1]. ( is the empty string.) Each state (i.e., vertex) has a halt probability in [0, 1], and there is a single initial state qI . Each path from qI to a final state qF has We take the first two conditions to be part of the definition of a PFST. The final condition requires our PFST to be “tight” in the same sense as a PCFG (Chi and Geman, 1998), although the tightness conditions for a PCFG are more complex. In section 7, we discuss the costs and benefits of PFSTs relative to other options. 4 The Contextual Edit PFST We now define a PFST topology that concisely captures the contextual edit process of section 2. We are given the alphabets Σx , Σy and the context window sizes N1 , N2 , N3 ≥ 0. For each possible context triple C = (C1 , C2 , C3 ) as defined in section 2, we construct an edit state qC whose outgoing arcs correspond to the possible edit operations in that context. One might expect that the SUBST(t) edit operation that rea"
P14-2102,P96-1031,0,0.1924,"r any x ∈ Σ∗x , the total probability of all paths accepting x is 1, so that pθ (y |x) is truly a conditional probability distribution. This is guaranteed by the following sufficient conditions (we omit the proof for space), which do not seem to appear in previous literature: • For each state q and each symbol b ∈ Σx , the arcs from q with input label b or  must have total probability of 1. (These are the available choices if the next input character is x.) 2 Several authors have given recipes for finite-state transducers that perform a single contextual edit operation (Kaplan and Kay, 1994; Mohri and Sproat, 1996; Gerdemann and van Noord, 1999). Such “rewrite rules” can be individually more expressive than our simple edit operations of section 2; but it is unclear how to train a cascade of them to model p(y |x). 626 a b x _ ε:z a bc z _ / ε re a d T(z SER ert ins z /1 c ε:ε )) ,x ,bc a )|( p(IN c: aa bc ba x _ c,x) (a,b (b) | E LET / p(DE te b dele ε:y /p (SU su b PFST, T , has O(|Σx |N1 +N2 |Σy |N3 ) states and O(|Σx |N1 +N2 |Σy |N3 +1 ) arcs. Composing this T with deterministic FSAs takes time linear in the size of the result, using a left-to-right, on-the-fly implementation of the composition opera"
P14-2102,J97-2003,0,0.0482753,"bility > 0 from q to some qF . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label  and probability 1.) Probabilistic FSTs We will construct a probabilistic finite-state transducer (PFST) that compactly models pθ (y | x) for all (x, y) pairs.2 Then various computations with this distribution can be reduced to standard finite-state computations that efficiently employ dynamic programming over the structure of the PFST, and the PFST can be easily combined with other finite-state distributions and functions (Mohri, 1997; Eisner, 2001). A PFST is a two-tape generalization of the wellknown nondeterministic finite-state acceptor. It is a finite directed multigraph where each arc is labeled with an input in Σx ∪ {}, an output in Σy ∪{}, and a probability in [0, 1]. ( is the empty string.) Each state (i.e., vertex) has a halt probability in [0, 1], and there is a single initial state qI . Each path from qI to a final state qF has We take the first two conditions to be part of the definition of a PFST. The final condition requires our PFST to be “tight” in the same sense as a PCFG (Chi and Geman, 1998), althoug"
P14-2102,D08-1113,1,0.806276,"duce xk to yk , relative to competing edits from the same contexts C. This means raising θ · f (C, e) and/or lowering ZC . Thus, log pθ (yk |xk ) depends only on the probabilities of edit arcs in T that appear in xk ◦ T ◦ yk , and the competing edit arcs from the same edit states qC . The gradient ∇θ log pθ (yk |xk ) takes the form &quot; # X X c(C, e) f~(C, e) − pθ (e0 |C)f~(C, e0 ) C,e ping this requirement gives a weighted FST (WFST), whose path weights w(x, y) can be globally normalized (divided by a constant Zx ) to obtain probabilities p(y |x). WFST models of contextual edits were studied by Dreyer et al. (2008). PFSTs and WFSTs are respectively related to MEMMs (McCallum et al., 2000) and CRFs (Lafferty et al., 2001). They gain added power from hidden states and  transitions (although to permit a finite-state encoding, they condition on x in a more restricted way than MEMMs and CRFs). WFSTs are likely to beat PFSTs as linguistic models,4 just as CRFs beat MEMMs (Klein and Manning, 2002). A WFST’s advantage is that the probability of an edit can be indirectly affected by the weights of other edits at a distance. Also, one could construct WFSTs where an edit’s weight directly considers local right ou"
P14-2102,N13-1073,0,0.0294951,"hat we do not condition on xi+1 , we must still condition on whether xi+1 = EOS (a single bit). We gloss over special handling for N2 = 0; but it is in our code. 625 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 625–630, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics This model is reminiscent of conditional models in MT that perform stepwise generation of one string or structure from another—e.g., string alignment models with contextual features (Cherry and Lin, 2003; Liu et al., 2005; Dyer et al., 2013), or tree transducers (Knight and Graehl, 2005). 3 • For each state q, the halt action and the arcs from q with input label  must have total probability of 1. (These are the available choices if there is no next input character.) • Every state q must be co-accessible, i.e., there must be a path of probability > 0 from q to some qF . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label  and probability 1.) Probabilistic FSTs We will construct a probabilistic finite-state transducer (PFST) that compactly mod"
P14-2102,E99-1017,0,0.178288,"Missing"
P14-2102,J94-3001,0,0.128363,"care to ensure that for any x ∈ Σ∗x , the total probability of all paths accepting x is 1, so that pθ (y |x) is truly a conditional probability distribution. This is guaranteed by the following sufficient conditions (we omit the proof for space), which do not seem to appear in previous literature: • For each state q and each symbol b ∈ Σx , the arcs from q with input label b or  must have total probability of 1. (These are the available choices if the next input character is x.) 2 Several authors have given recipes for finite-state transducers that perform a single contextual edit operation (Kaplan and Kay, 1994; Mohri and Sproat, 1996; Gerdemann and van Noord, 1999). Such “rewrite rules” can be individually more expressive than our simple edit operations of section 2; but it is unclear how to train a cascade of them to model p(y |x). 626 a b x _ ε:z a bc z _ / ε re a d T(z SER ert ins z /1 c ε:ε )) ,x ,bc a )|( p(IN c: aa bc ba x _ c,x) (a,b (b) | E LET / p(DE te b dele ε:y /p (SU su b PFST, T , has O(|Σx |N1 +N2 |Σy |N3 ) states and O(|Σx |N1 +N2 |Σy |N3 +1 ) arcs. Composing this T with deterministic FSAs takes time linear in the size of the result, using a left-to-right, on-the-fly implementation"
P14-2102,W02-1002,0,0.0248954,") C,e ping this requirement gives a weighted FST (WFST), whose path weights w(x, y) can be globally normalized (divided by a constant Zx ) to obtain probabilities p(y |x). WFST models of contextual edits were studied by Dreyer et al. (2008). PFSTs and WFSTs are respectively related to MEMMs (McCallum et al., 2000) and CRFs (Lafferty et al., 2001). They gain added power from hidden states and  transitions (although to permit a finite-state encoding, they condition on x in a more restricted way than MEMMs and CRFs). WFSTs are likely to beat PFSTs as linguistic models,4 just as CRFs beat MEMMs (Klein and Manning, 2002). A WFST’s advantage is that the probability of an edit can be indirectly affected by the weights of other edits at a distance. Also, one could construct WFSTs where an edit’s weight directly considers local right output context C4 . So why are we interested in PFSTs? Because they do not require computing a separate normalizing contant Zx for every x. This makes it computationally tractable to use them in settings where x is uncertain because it is unobserved, partially observed (e.g., lacks syllable boundaries), or noisily observed. E.g., at the end of section 5, X represented an uncertain x."
P14-6006,P11-1048,0,0.0224407,"However, inference and learning in the models we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical u"
P14-6006,N12-1004,0,0.0253881,"d learning in the models we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical underpinnings of “what BP"
P14-6006,D09-1011,1,0.844629,"ons and latent variables. However, inference and learning in the models we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing,"
P14-6006,D12-1074,0,0.0193936,"we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical underpinnings of “what BP is doing” and how it rela"
P14-6006,D08-1016,1,0.630122,"lly meaningful interactions and latent variables. However, inference and learning in the models we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor gr"
P14-6006,N12-1013,1,0.838015,"ious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical underpinnings of “what BP is doing” and how it relates to other variational inf"
P15-5002,P11-1048,0,0.0142523,"However, inference and learning in the models we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical"
P15-5002,N12-1004,0,0.0168874,"d learning in the models we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical underpinnings of “what BP"
P15-5002,D09-1011,1,0.809011,"ons and latent variables. However, inference and learning in the models we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing"
P15-5002,D12-1074,0,0.0177473,"we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical underpinnings of “what BP is doing” and how it rel"
P15-5002,D08-1016,1,0.761304,"lly meaningful interactions and latent variables. However, inference and learning in the models we want often poses a serious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor g"
P15-5002,N12-1013,1,0.792937,"ious computational challenge. Belief propagation (BP) and its variants provide an attractive approximate solution, especially using recent training methods. These approaches can handle joint models of interacting components, are computationally efficient, and have extended the state-of-the-art on a number of common NLP tasks, including dependency parsing, modeling of morphological paradigms, CCG parsing, phrase extraction, semantic role labeling, and information extraction (Smith and Eisner, 2008; Dreyer and Eisner, 2009; Auli and Lopez, 2011; Burkett and Klein, 2012; Naradowsky et al., 2012; Stoyanov and Eisner, 2012). This tutorial delves into BP with an emphasis on recent advances that enable state-of-the-art performance in a variety of tasks. Our goal is to elucidate how these approaches can easily be applied to new problems. We also cover the theory underlying them. Our target audience is researchers in human language technologies; we do not assume familiarity with BP. In the first three sections, we discuss applications of BP to NLP problems, the basics of modeling with factor graphs and message passing, and the theoretical underpinnings of “what BP is doing” and how it relates to other inference tech"
P16-1156,N15-1140,1,0.856025,"Missing"
P16-1156,K15-1017,1,0.876559,"Missing"
P16-1156,W13-3512,0,0.247022,"ative model. More importantly, in addition to modifying vectors of observed words, our model can generate vectors for forms not observed in the training data. Wieting et al. (2015) compute compositional embeddings of phrases, with their simplest method being additive (like ours) over the phrase’s words. Their embeddings are tuned to fit observed phrase similarity scores from PPDB (Ganitkevitch et al., 2013), which allows them to smooth and extend PPDB just as we do to WORD 2 VEC output. Using morphological resources to enhance embeddings at training time has been examined by numerous authors. Luong et al. (2013) used M OR FESSOR (Creutz and Lagus, 2007), an unsupervised morphological induction algorithm, to segment the training corpus. They then trained a recursive neural network (Goller and Kuchler, 1996; Socher, 2014) to generate compositional word embeddings. Our model is much simpler and faster to train. Their evaluation was limited to English and focused on rare English words. dos Santos and Zadrozny (2014) introduced a neural tagging architecture (Collobert et al., 2011) with a characterlevel convolutional layer. Qiu et al. (2014) and Botha and Blunsom (2014) both use M ORFESSOR segmentations t"
P16-1156,Q15-1031,1,0.858459,"tion, we fit a directed Gaussian graphical model (GGM) that simultaneously considers (i) each word’s embedding (obtained from an embedding model like WORD 2 VEC) and (ii) its morphological analysis (obtained from a lexical resource). We then use this model to smooth the provided embeddings, and to generate embeddings for unseen inflections. For a lemma covered by the resource, the GGM can produce embeddings for all its forms (if at least one of these forms has a known embedding); this can be extended to words not covered using a guesser like M ORFESSOR (Creutz and Lagus, 2007) or C HIP M UNK (Cotterell et al., 2015a). A major difference of our approach from related techniques is that our model uses existing morphological resources (e.g., morphological lexicons or finite-state analyzers) rather than semantic resources (e.g., WordNet (Miller et al., 1990) and PPDB (Ganitkevitch et al., 2013)). The former tend to be larger: we often can analyze more words than we have semantic representations for. It would be possible to integrate our GGM into the training procedure for a word embedding system, making that system sensitive to morphological attributes. However, the postprocessing approach in our present pap"
P16-1156,N13-1090,0,0.511556,"in the literature. One of the most interesting discoveries is that these representations capture meaningful morpho-syntactic and semantic properties through very simple linear relations: in a semantic vector space, we observe that vtalked − vtalk ≈ vdrank − vdrink . (1) That this equation approximately holds across many morphologically related 4-tuples indicates that the learned embeddings capture a feature of English morphology—adding the past tense feature roughly corresponds to adding a certain vector. Moreover, manipulating this equation yields what we will call the vector offset method (Mikolov et al., 2013c) for approximating other vectors. For instance, if we only know the vectors for the Spanish words comieron (ate), comemos (eat) and bebieron (drank), we can produce an approximation of the vector for bebemos (drink), as shown in Figure 1. Many languages exhibit much richer morphology than English. While English nouns commonly take two forms – singular and plural— Czech nouns take 12 and Turkish nouns take over 30. This increase in word forms per lemma creates considerable data sparsity. Fortunately, for many languages there exist large morphological lexicons, or better yet, morphological too"
P16-1156,N15-1184,0,0.0886934,"omit i from all Wk . After convergence, set wi ← k∈Mi mk . 4 Note that it is not necessary to define it as λ0 I, introducing a new scale parameter λ0 , since doubling λ0 would have the same effect on the MAP update rules as halving λ and Σi . 1654 Viterbi EM can be regarded as block coordinate descent on the negative log-likelihood function, with E and M steps both improving this common objective along different variables. We update the parameters (M step above) after each 10 passes of updating the latent vectors (section 5’s E step). 7 Related Work Our postprocessing strategy is inspired by Faruqui et al. (2015), who designed a retrofitting procedure to modify pre-trained vectors such that their relations match those found in semantic lexicons. We focus on morphological resources, rather than semantic lexicons, and employ a generative model. More importantly, in addition to modifying vectors of observed words, our model can generate vectors for forms not observed in the training data. Wieting et al. (2015) compute compositional embeddings of phrases, with their simplest method being additive (like ours) over the phrase’s words. Their embeddings are tuned to fit observed phrase similarity scores from"
P16-1156,W15-1518,0,0.0367522,"Missing"
P16-1156,N13-1092,0,0.105894,"Missing"
P16-1156,C14-1015,0,0.0215187,"ddings at training time has been examined by numerous authors. Luong et al. (2013) used M OR FESSOR (Creutz and Lagus, 2007), an unsupervised morphological induction algorithm, to segment the training corpus. They then trained a recursive neural network (Goller and Kuchler, 1996; Socher, 2014) to generate compositional word embeddings. Our model is much simpler and faster to train. Their evaluation was limited to English and focused on rare English words. dos Santos and Zadrozny (2014) introduced a neural tagging architecture (Collobert et al., 2011) with a characterlevel convolutional layer. Qiu et al. (2014) and Botha and Blunsom (2014) both use M ORFESSOR segmentations to augment WORD 2 VEC and a logbilinear (LBL) language model (Mnih and Hinton, 2007), respectively. Similar to us, they have an additive model of the semantics of morphemes, i.e., the embedding of the word form is the sum of the embeddings of its constituents. In contrast to us, however, both include the word form itself in the sum. Finally, Cotterell and Sch¨utze (2015) jointly trained an LBL language model and a morphological tagger (Hajiˇc, 2000) to encourage the embeddings to encode rich morphology. With the exception of (Cott"
P16-1156,A00-2013,0,0.0304152,"Missing"
P16-1156,D09-1124,0,0.00984588,"e confined to rare predicting words.) See Appendix B for more analysis. 1658 Forms / Lemma Skip-Gram GGM English 1.8 58.9 58.9 German 6.3 36.2 37.6 Spanish 8.1 37.8 40.3 Table 4: Word similarity results (correlations) using the WS353 dataset in the three languages, in which it is available. Since all the words in WS-353 are lemmata, we report the average inflected form to lemma ratio for forms appearing in the datasets. 8.3 9 Experiment 3: Word Similarity As a third and final experiment, we consider word similarity using the WS-353 data set (Finkelstein et al., 2001), translated into Spanish (Hassan and Mihalcea, 2009) and German (Leviant, 2016).11 The datasets are composed of 353 pairs of words. Multiple native speakers were then asked to give an integral value between 1 and 10 indicating the similarity of that pair, and those values were then averaged. In each case, we train the GGM on the whole Wikipedia corpus for the language. Since in each language every word in the WS-353 set is in fact a lemma, we use the latent embedding our GGM learns in the experiment. In Spanish, for example, we use the learned latent morpheme embedding for the lemma BEBER (recall this takes information from every element in the"
P16-1156,P15-2111,0,0.0948168,"must instead select from its paradigm the word type, such as beb´eis, that expresses the contextually appropriate properties. Noun tokens in a language may similarly be required to be inflected for properties such as case, gender, and number. A content word is chosen by specifying a lemma (which selects a particular paradigm) together with some inflectional attributes (which select a particular slot within that paradigm). For example, [ Lemma=EAT, Person=3, Number=S INGULAR, Tense=P RESENT ] is a bundle of attribute-value pairs that would be jointly expressed in English by the word form eats (Sylak-Glassman et al., 2015). The regularities observed by Mikolov et al. (2013c) hold between words with similar attributevalue pairs. In Spanish, the word beben “they drink” (Table 1) can be analyzed as expressing the bundle [ Lemma=BEBER, Person=3, Number=P LURAL, Tense=P RESENT ]. Its vector similarity to bebemos “we drink” is due to the fact that both word forms have the same lemma BE BER . Likewise, the vector similarity of beben to comieron “they ate” is due to the conceptual similarity of their lemmas, BEBER “drink” and COMER “eat”. Conversely, that beben is similar to preguntan “they ask” is caused by shared inf"
P16-1156,W15-0105,0,0.0336122,"Missing"
P16-1156,P15-2008,0,0.0459499,"Missing"
P16-1156,Q15-1025,0,0.0118304,"long different variables. We update the parameters (M step above) after each 10 passes of updating the latent vectors (section 5’s E step). 7 Related Work Our postprocessing strategy is inspired by Faruqui et al. (2015), who designed a retrofitting procedure to modify pre-trained vectors such that their relations match those found in semantic lexicons. We focus on morphological resources, rather than semantic lexicons, and employ a generative model. More importantly, in addition to modifying vectors of observed words, our model can generate vectors for forms not observed in the training data. Wieting et al. (2015) compute compositional embeddings of phrases, with their simplest method being additive (like ours) over the phrase’s words. Their embeddings are tuned to fit observed phrase similarity scores from PPDB (Ganitkevitch et al., 2013), which allows them to smooth and extend PPDB just as we do to WORD 2 VEC output. Using morphological resources to enhance embeddings at training time has been examined by numerous authors. Luong et al. (2013) used M OR FESSOR (Creutz and Lagus, 2007), an unsupervised morphological induction algorithm, to segment the training corpus. They then trained a recursive neur"
P16-1175,W15-3001,1,0.861797,"Missing"
P16-1175,P07-1033,0,0.290306,"Missing"
P16-1175,D09-1011,1,0.780042,"ymbols with any e to be the mean PMI with e of all dictionary words, so that they are essentially uninformative. Inference According to our model, the probability that the user guesses Ei = eˆi is given by a marginal probability from the CRF. Computing these marginals is a combinatorial optimization problem that involves reasoning jointly about the possible values of each Ei (i ∈ / Obs), which range over the English vocabulary V e . We employ loopy belief propagation (Murphy et al., 1999) to obtain approximate marginals over the variables E. A tree-based schedule for message passing was used (Dreyer and Eisner, 2009, footnote 22). We run 3 iterations with a new random root for each iteration. We define the vocabulary V e to consist of all reference translations e∗i and normalized user (8) where the summation is over all submissions in our dataset. The gradient of each summand reduces to a difference between observed and expected values of the feature vector φ = (φef , φee ), summed over all factors in (1). The observed feaˆ. The tures are computed directly by setting E = e expected features (which arise from the log of the normalization constant of (1)) are computed approximately by loopy belief propagat"
P16-1175,K16-1013,1,0.883817,"real numbers: ( PMI(ei , ej ) if |i − j |&gt; 1 φee pmi (ei , ej ) = (6) 0 otherwise ( PMI1 (ei , ej ) if |i − j |= 1 φee pmi1 (ei , ej ) = (7) 0 otherwise 5 At least in short-term memory—this feature currently omits to consider any negative feedback from previous HITs. 1863 where the pointwise mutual information PMI(x, y) measures the degree to which the English words x, y tend to occur in the same English sentence, and PMI1 (x, y) measures how often they tend to occur in adjacent positions. These measurements are estimated from the English side of the WMT corpus, with smoothing performed as in Knowles et al. (2016). For example, if fi = Suppe, the user’s guess of Ei should be influenced by fj = Brot appearing in the same sentence, if the user suspects or observes that its translation is Ej = bread. The PMI feature knows that soup and bread tend to appear in the same English sentences, whereas PMI1 knows that they tend not to appear in the bigram soup bread or bread soup. 4.1.3 4.3 Parameter Estimation We learn our parameter vector θ to approximately maximize the regularized log-likelihood of the users’ guesses: X  ∗ ˆ log Pθ (E = e |EObs = eObs , f , history) −λ||θ||2 User-Specific Features Apart from"
P16-1175,P07-2045,1,0.00803577,"to collect data. Users qualified for tasks by completing a short quiz and survey about their language knowledge. Only users whose results indicated no knowledge of German and self-identified as native speakers of English were allowed to complete tasks. With German as the foreign language, we generated content by crawling a simplifiedGerman news website, nachrichtenleicht. de. We chose simplified German in order to minimize translation errors and to make the task more suitable for novice learners. We translated each German sentence using the Moses Statistical Machine Translation (SMT) toolkit (Koehn et al., 2007). The SMT system was trained on the German-English Commoncrawl parallel text used in WMT 2015 (Bojar et al., 2015). We used 200 German sentences, presenting each to 10 different users. In MTurk jargon, this yielded 2000 Human Intelligence Tasks (HITs). Each HIT required its user to participate in several rounds of guessing as the English translation was incrementally revealed. A user was paid US$0.12 per HIT, with a bonus of US$6 to any user who accumulated more than 2000 total points. Our HIT user interface is shown in the video at https://youtu.be/9PczEcnr4F8. 3.1 HITs and Submissions For ea"
P16-1175,N12-1003,0,0.0248197,"terface that uses macaronic sentences directly as a medium of language instruction: our companion paper (Renduchintala et al., 2016) gives an overview of that project. We briefly review previous work, then describe our data collection setup and the data obtained. Finally, we discuss our model of learner comprehension and validate our model’s predictions. 2 Previous Work Natural language processing (NLP) has long been applied to education, but the majority of this work focuses on evaluation and assessment. Prominent recent examples include Heilman and Madnani (2012), Burstein et al. (2013) and Madnani et al. (2012). Other works fall more along the lines of intelligent and adaptive tutoring systems designed to improve learning outcomes. Most of those are outside of the area of NLP (typically focusing on math or science). An overview of NLPbased work in the education sphere can be found in Litman (2016). There has also been work spe¨ cific to second language acquisition, such as Ozbal et al. (2014), where the focus has been to build a system to help learners retain new vocabulary. However, much of the existing work on incidental learning is found in the education and cognitive science literature rather th"
P16-1175,P14-2058,0,0.028296,"rocessing (NLP) has long been applied to education, but the majority of this work focuses on evaluation and assessment. Prominent recent examples include Heilman and Madnani (2012), Burstein et al. (2013) and Madnani et al. (2012). Other works fall more along the lines of intelligent and adaptive tutoring systems designed to improve learning outcomes. Most of those are outside of the area of NLP (typically focusing on math or science). An overview of NLPbased work in the education sphere can be found in Litman (2016). There has also been work spe¨ cific to second language acquisition, such as Ozbal et al. (2014), where the focus has been to build a system to help learners retain new vocabulary. However, much of the existing work on incidental learning is found in the education and cognitive science literature rather than NLP. Our work is related to Labutov and Lipson (2014), which also tries to leverage incidental learning using mixed L1 and L2 language. Where their work uses surprisal to choose contexts in which to insert L2 vocabulary, we consider both context features and other factors such as cognate features (described in detail in 4.1). We collect data that gives direct evidence of the user’s u"
P16-1175,D14-1162,0,0.0958725,"points as discussed below. Yellow and red shading then fades, to signal to the user that they may try entering a new guess. Correct guesses remain on the screen for the entire task. 3.4 Points Adding points to the process (Figures 1–2) adds a game-like quality and lets us incentivize users by paying them for good performance (see section 3). We award 10 points for each exactly correct guess (case-insensitive). We give additional “effort points” for a guess that is close to the cor1861 rect translation, as measured by cosine similarity in vector space. (We used pre-trained GLoVe word vectors (Pennington et al., 2014); when the guess or correct translation has multiple words, we take the average of the word vectors.) We deduct effort points for guesses that are careless or very poor. Our rubric for effort points is as follows:   −1, if eˆ is repeated or nonsense (red)     −1, if sim(ˆ e, e∗ ) &lt; 0 (red)  ep = 0, if 0 ≤ sim(ˆ e, e∗ ) &lt; 0.4 (red)    0, if eˆ is blank    10 × sim(ˆ e, e∗ ) otherwise (yellow) Here sim(ˆ e, e∗ ) is cosine similarity between the vector embeddings of the user’s guess eˆ and our reference translation e∗ . A “nonsense” guess contains a word that does not appear in the s"
P16-1175,P14-1053,0,0.13683,"he lines of intelligent and adaptive tutoring systems designed to improve learning outcomes. Most of those are outside of the area of NLP (typically focusing on math or science). An overview of NLPbased work in the education sphere can be found in Litman (2016). There has also been work spe¨ cific to second language acquisition, such as Ozbal et al. (2014), where the focus has been to build a system to help learners retain new vocabulary. However, much of the existing work on incidental learning is found in the education and cognitive science literature rather than NLP. Our work is related to Labutov and Lipson (2014), which also tries to leverage incidental learning using mixed L1 and L2 language. Where their work uses surprisal to choose contexts in which to insert L2 vocabulary, we consider both context features and other factors such as cognate features (described in detail in 4.1). We collect data that gives direct evidence of the user’s understanding of words (by asking them to provide English guesses) rather than indirectly (via questions about sentence validity, which runs the risk of overestimating their knowledge of a word, if, for instance, they’ve only learned whether it is animate or inanimate"
P16-1175,P16-4023,1,\N,Missing
P16-4023,J94-4004,0,0.0375436,"btained from Figure 5 only if unit u2 (as labeled in Figure 4) is rendered (in its current language) to the left of unit u3 , which we write as u2 < u3 . In this case, it is possible for the user to change the order of these units, because u3 < u2 in German. Table 2 shows the 8 possible combinations of ordering and translation choices for this pair of units. String Rendered . . .they run. . . . . .they laufen. . . . . .sie run. . . . . .sie laufen. . . . . .run they. . . . . .run sie. . . . . .laufen they. . . . . .laufen sie. . . Cross-linguistic divergences in the expression of information (Dorr, 1994) could be confusing. For example, when moving through macaronic space from Kaffee gef¨ allt Menschen (coffee pleases humans) to its translation humans like coffee, it may not be clear to the learner that the reordering is triggered by the fact that like is not a literal translation of gef¨ allt. One way to improve this might be to have the system pass smoothly through a range of intermediate translations from word-by-word glosses to idiomatic phrasal translations, rather than always directly translating idioms. We might also see benefit in guiding our gradual translations with cognates (for ex"
P16-4023,K16-1013,1,0.444886,"0/ with sample content. Our interface lets the user navigate through the spectrum from L2 to L1, going beyond the single-word or single-phrase translations offered by other online tools such as Swych (2015), or dictionary-like browser plugins. Finally, we discuss plans to extend this prototype and to integrate it with a continuously adapting user model. To this end, our companion paper (Renduchintala et al., 2016) develops an initial model of macaronic sentence comprehension by novice L2 learners, using data collected from human subjects via Amazon’s Mechanical Turk service. In another paper (Knowles et al., 2016), we carry out a controlled study of comprehension of individual L2 words in isolation and in L1 context. 2 (a) Initial sentence state. (b) Mouse hovered under Preis. (c) Preis translated to prize. (d) Mouse hovered above prize. Clicking above will revert the sentence back to the initial state 1a. (e) Sentence with 2 different words translated into English Figure 1: Actions that translate words. Using these fundamental interactions as building blocks, we create an interactive framework for a language learner to explore this continuum of “English-like” to “foreign-like” sentences. By repeated i"
P16-4023,P07-2045,1,0.00930471,"Constructing Macaronic Translations In this section, we describe the details of the underlying data structures needed to allow all the interactions mentioned in the previous section. A key requirement in the design of the data structure was to support orthogonal actions in each sentence. Making all translation and reordering actions independent of one another creates a large space of macaronic states for a learner to explore. At present, the input to our macaronic interface is bitext with word-to-word alignments provided by a phrase-based SMT system (or, if desired, by hand). We employ Moses (Koehn et al., 2007) to translate German sentences and generate phrase alignments. News articles written in simple German from nachrichtenleicht. de (Deutschlandfunk, 2016) were translated after training the SMT system on the WMT15 GermanEnglish corpus (Bojar et al., 2015). We convert the word alignments into “minimal alignments” that are either one-to-one, oneto-many or many-to-one.4 This step ensures consistent reversibility of actions and prevents large phrases from being translated with a single click.5 The resulting bipartite graph can be regarded as Figure 5: A possible state of the sentence, which renders"
P16-4023,W08-1006,0,0.0186697,"rphemes might yield ge-talk or sprech-ed before reaching talked. This could guide learners towards an understanding of German tense marking and stem changes. The space of possible orderings for a sentence pair is defined by a bracketing ITG tree (Wu, 1997), which transforms the German ordering of the units into the English ordering by a collection of nested binary swaps of subsequences.7 The ordering state of the macaronic sentence is given by the subset of these swaps that have been performed. A reordering action toggles one of the swaps in this collection. Since we have a parser for German (Rafferty and Manning, 2008), we take care to select an ITG tree that is “compatible” with the German sentence’s dependency structure, in the following sense: if the ITG tree combines two spans A and B, then there are not dependencies from words in A to words in B and vice-versa. 4 4.1 4.2 User Adaptation and Evaluation We would prefer to show the learner a macaronic sentence that provides just enough clues for the learner to be able to comprehend it, while still pushing them to figure out new vocabulary or new structures. Thus, we plan to situate this interface in a framework that continuously adapts as the user progres"
P16-4023,P16-1175,1,0.712109,"ically generated using existing statistical machine translation (SMT) methods, enabling learners or teachers to choose their own texts to read. Our prototype is currently running on http: //www.clsp.jhu.edu:3030/ with sample content. Our interface lets the user navigate through the spectrum from L2 to L1, going beyond the single-word or single-phrase translations offered by other online tools such as Swych (2015), or dictionary-like browser plugins. Finally, we discuss plans to extend this prototype and to integrate it with a continuously adapting user model. To this end, our companion paper (Renduchintala et al., 2016) develops an initial model of macaronic sentence comprehension by novice L2 learners, using data collected from human subjects via Amazon’s Mechanical Turk service. In another paper (Knowles et al., 2016), we carry out a controlled study of comprehension of individual L2 words in isolation and in L1 context. 2 (a) Initial sentence state. (b) Mouse hovered under Preis. (c) Preis translated to prize. (d) Mouse hovered above prize. Clicking above will revert the sentence back to the initial state 1a. (e) Sentence with 2 different words translated into English Figure 1: Actions that translate word"
P16-4023,J97-3002,0,0.0390427,"cognate Karotte as an intermediate step). Unit Ordering {u2 } < {u3 } {u2 } > {u3 } Table 2: Generating reordered strings using units. We also plan to transition through words that are macaronic at the sub-word level. For example, hovering over the unfamiliar German word gesprochen might decompose it into ge-sprochen; then clicking on one of those morphemes might yield ge-talk or sprech-ed before reaching talked. This could guide learners towards an understanding of German tense marking and stem changes. The space of possible orderings for a sentence pair is defined by a bracketing ITG tree (Wu, 1997), which transforms the German ordering of the units into the English ordering by a collection of nested binary swaps of subsequences.7 The ordering state of the macaronic sentence is given by the subset of these swaps that have been performed. A reordering action toggles one of the swaps in this collection. Since we have a parser for German (Rafferty and Manning, 2008), we take care to select an ITG tree that is “compatible” with the German sentence’s dependency structure, in the following sense: if the ITG tree combines two spans A and B, then there are not dependencies from words in A to wor"
P17-1095,D12-1075,0,0.0258911,"abilities in Hy are easy to compute because Hy has the form of a language model, and prefix probabilities in Py are therefore also easy to compute (using a prefix tree for efficiency). This concludes the description of the segmental sampler. Note that the particle Gibbs procedure is unchanged. 4 Inducing parts-of-speech with type-level supervision Automatically inducing parts-of-speech from raw text is a challenging problem (Goldwater et al., 2005). Our focus here is on the easier problem of type-supervised part-of-speech induction, in which (partial) dictionaries are used to guide inference (Garrette and Baldridge, 2012; Li et al., 2012). Conditioned on the unlabeled corpus and dictionary, we use the MCMC procedure described in §3.1 to impute the latent parts-of-speech. Since dictionaries are freely available for hundreds of languages,6 we see this as a mild additional requirement in practice over the purely unsupervised setting. In prior work, dictionaries have been used as constraints on possible parts-of-speech: words appearing in the dictionary take one of their known parts1034 6 https://www.wiktionary.org/ of-speech. In our setting, however, the dictionaries are not constraints but evidence. If monthly"
P17-1095,P07-1094,0,0.0518943,"d with this but did not observe any consistent advantage to doing so in our setting. 4 The label sequence is terminated by a distinguished endof-sequence label, again written as $. 1031 x1:T are then generated conditioned on the label sequence via the corresponding Py distribution (defined in §2.3). All observations with the same label y are drawn from the same Py , and thus this subsequence of observations is distributed according to the Chinese restaurant process (1). We model y using another sequence memoizer model. This is similar to other hierarchical Bayesian models of latent sequences (Goldwater and Griffiths, 2007; Blunsom and Cohn, 2010), but again, it does not limit the Markov order (the number of preceding labels that are conditioned on). Thus, the probability of a sequence of latent types is computed in the same way as the base distribution in §2.4, that is, p(y1:T ) := T Y t=1  Gy1:t−1 (yt ) Gy1:T ($) (4) where Gv (y) denotes the conditional probability of latent label y ∈ Y given the left context v ∈ Y ∗ . Each Gv is a distribution over Y, defined recursively as G ∼ PYP(d , α , UY ) (5) Gv ∼ PYP(d|v |, α|v |, Gσ(v) ) The probability of transitioning to label yt depends on the assignments of"
P17-1095,U11-1004,0,0.0151687,"r (4).  The emission distribution p(xt |Yt = y) depends on the emissions observed from any earlier tokens of y, because of the Chinese restaurant process (1). When  is the only complication, block Metropolis-Hastings samplers have proven effective (Johnson et al., 2007). However, this approach uses dynamic programming to sample from a proposal distribution efficiently, which ¬ precludes in our case. Instead, we use sequential Monte Carlo (SMC)—sometimes called particle filtering—as a proposal distribution. Particle filtering is typically used in online settings, including word segmentation (Borschinger and Johnson, 2011), to make decisions before all of x has been observed. However, we are interested in the inference (or smoothing) problem that conditions on all of x (Dubbin and Blunsom, 2012; Tripuraneni et al., 2015). SMC employs a proposal distribution q(y |x) 1032 whose definition decomposes as follows: q(y1 |x1 ) T Y t=2 q(yt |y1:t−1 , x1:t ) (6) for T = |x|. To sample a sequence of latent labels, first sample an initial label y1 from q1 , then proceed incrementally by sampling yt from qt (· |y1:t−1 , x1:t ) for t = 2, . . . , T . The final sampled sequence y is called a particle, and is given an unnorma"
P17-1095,P11-1061,0,0.0282037,"section, we are interested in evaluating our proposed Bayesian model in the context of low-resource NER. Experiments We follow the experimental procedure described in Li et al. (2012), and use their released code and data to compare to their best model: a second-order maximum entropy Markov model parametrized with log-linear features (SHMM - ME). This model uses hand-crafted features designed to distinguish between different parts-of-speech, and it has special handling for rare words. This approach is surprisingly effective and outperforms alternate approaches such as cross-lingual transfer (Das and Petrov, 2011). However, it also has limitations, since words that do not appear in the dictionary will be unconstrained, and spurious or incorrect lexical entries may lead to propagation of errors. The lexicons are taken from the Wiktionary project; their size and coverage are documented by (Li et al., 2012). We evaluate our model on multi-lingual data released as part of the CoNLL 2007 and CoNLL-X shared tasks. In particular, we use the same set of languages as Li et al. (2012).7 For our method, we impute the parts-of-speech by running particle Gibbs for 100 epochs, where one epoch consists of resampling"
P17-1095,D11-1057,1,0.836785,"pes to appear ≥ c times in an unbounded sequence of IID draws from Py . When c = 1, this is equivalent to modeling the lexicon as my draws without replacement from Py .2 Unfortunately, draws without replacement are no longer IID or exchangeable: order matters. It would therefore become difficult to condition inference and learning on an observed lexicon, because we would need to explicitly sum or sample over the possibilities for the latent sequence of tokens (or stick segments). We therefore adopt the simpler deficient model. A version of our lexicon model (with c = 1) was previously used by Dreyer and Eisner (2011, Appendix C), who observed a list of verb paradigm types rather than word or entity-name types. 2.3 Prior distribution over Py We assume a priori that Py was drawn from a Pitman-Yor process (PYP) (Pitman and Yor, 1997). Both the lexicon and the ordinary corpus are observations that provide information about Py . The PYP is defined by three parameters: a concentration parameter α, a discount parameter d, and a base distribution Hy . In our case, Hy is a distribution over X = Σ∗ , the set of possible strings over a finite character alphabet Σ. For example, HLOC is used to choose new place names"
P17-1095,W12-1907,0,0.0155307,"mplication, block Metropolis-Hastings samplers have proven effective (Johnson et al., 2007). However, this approach uses dynamic programming to sample from a proposal distribution efficiently, which ¬ precludes in our case. Instead, we use sequential Monte Carlo (SMC)—sometimes called particle filtering—as a proposal distribution. Particle filtering is typically used in online settings, including word segmentation (Borschinger and Johnson, 2011), to make decisions before all of x has been observed. However, we are interested in the inference (or smoothing) problem that conditions on all of x (Dubbin and Blunsom, 2012; Tripuraneni et al., 2015). SMC employs a proposal distribution q(y |x) 1032 whose definition decomposes as follows: q(y1 |x1 ) T Y t=2 q(yt |y1:t−1 , x1:t ) (6) for T = |x|. To sample a sequence of latent labels, first sample an initial label y1 from q1 , then proceed incrementally by sampling yt from qt (· |y1:t−1 , x1:t ) for t = 2, . . . , T . The final sampled sequence y is called a particle, and is given an unnormalized importance weight of w ˜=w ˜T · p($ |y1:T ) where w ˜T was built up via w ˜t := w ˜t−1 · p(y1:t , x1:t ) p(y1:t−1 , x1:t−1 ) q(yt |y1:t−1 , x1:t ) (7) The SMC procedure"
P17-1095,P05-1045,0,0.0620648,"Missing"
P17-1095,D12-1127,0,0.0333437,"Missing"
P17-1095,P09-1012,0,0.0338895,"that generative models are typically less feature-rich than their globally normalized discriminative counterparts (e.g. conditional random fields). In designing our approach—the hierarchical sequence memoizer (HSM)—we aim to be reasonably expressive while retaining practically useful inference algorithms. We propose a Bayesian nonparametric model to serve as a generative distribution responsible for both lexicon and corpus data. The proposed model memoizes previously used lexical entries (words or phrases) but backs off to a character-level distribution when generating novel types (Teh, 2006; Mochihashi et al., 2009). We propose an efficient inference algorithm for the proposed model using particle Gibbs sampling (§3). Our code is available at https://github.com/noa/bayesner. 1029 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1029–1039 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1095 2 Model Our goal is to fit a model that can automatically annotate text. We observe a supervised or unsupervised training corpus. For each label y in the annotation scheme, we also observe a lexic"
P17-1095,P05-1003,0,0.0357877,"we use lexical resources to guide part-of-speech induction (§4) and to bootstrap named-entity recognizers in low-resource languages (§5). Given their success, it is perhaps surprising that incorporating gazetteers or dictionaries into discriminative models (e.g. conditional random fields) may sometimes hurt performance. This phenomena is called weight under-training, in which lexical features—which detect whether a name is listed in the dictionary or gazetteer—are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (Smith et al., 2005; Sutton et al., 2006; Smith and Osborne, 2006). Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here. Our observation is that Bayesian modeling provides a principled solution. The lexicon is itself a dataset that was generated by some process. Practically, this means that lexicon entries (words or phrases) may be treated as additional observations. As a result, these entries provide information about how names are spelled. The presence of the lexicon theref"
P17-1095,W06-2918,0,0.0369595,"f-speech induction (§4) and to bootstrap named-entity recognizers in low-resource languages (§5). Given their success, it is perhaps surprising that incorporating gazetteers or dictionaries into discriminative models (e.g. conditional random fields) may sometimes hurt performance. This phenomena is called weight under-training, in which lexical features—which detect whether a name is listed in the dictionary or gazetteer—are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (Smith et al., 2005; Sutton et al., 2006; Smith and Osborne, 2006). Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here. Our observation is that Bayesian modeling provides a principled solution. The lexicon is itself a dataset that was generated by some process. Practically, this means that lexicon entries (words or phrases) may be treated as additional observations. As a result, these entries provide information about how names are spelled. The presence of the lexicon therefore now improves training of the spelling featu"
P17-1095,L16-1521,0,0.0289298,"not fully documented. 5 5.1 Boostrapping NER with type-level supervision Data Most languages do not have corpora annotated for parts-of-speech, named-entities, syntactic parses, or other linguistic annotations. Therefore, rapidly deploying natural language technologies in a new language may be challenging. In the context of facilitating relief responses in emergencies such as natural disasters, the DARPA LORELEI (Low Resource Languages for Emergent Incidents) program has sponsored the development and release of representative “language packs” for Turkish and Uzbek with more languages planned (Strassel and Tracey, 2016). We use the named-entity annotations as part of these language packs which include persons, locations, organizations, and geo-political entities, in order to explore bootstrapping named-entity recognition from small amounts of data. We consider two types of data: ¬ in-context annotations, where sentences are fully annotated for named-entities, and  lexical resources. The LORELEI language packs lack adequate indomain lexical resources for our purposes. Therefore, we simulate in-domain lexical resources by holding out portions of the annotated development data and deriving dictionaries and nam"
P17-1095,N06-1012,0,0.0356646,"urces to guide part-of-speech induction (§4) and to bootstrap named-entity recognizers in low-resource languages (§5). Given their success, it is perhaps surprising that incorporating gazetteers or dictionaries into discriminative models (e.g. conditional random fields) may sometimes hurt performance. This phenomena is called weight under-training, in which lexical features—which detect whether a name is listed in the dictionary or gazetteer—are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (Smith et al., 2005; Sutton et al., 2006; Smith and Osborne, 2006). Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here. Our observation is that Bayesian modeling provides a principled solution. The lexicon is itself a dataset that was generated by some process. Practically, this means that lexicon entries (words or phrases) may be treated as additional observations. As a result, these entries provide information about how names are spelled. The presence of the lexicon therefore now improves trai"
P17-1095,P06-1124,0,0.0429109,"ownside is that generative models are typically less feature-rich than their globally normalized discriminative counterparts (e.g. conditional random fields). In designing our approach—the hierarchical sequence memoizer (HSM)—we aim to be reasonably expressive while retaining practically useful inference algorithms. We propose a Bayesian nonparametric model to serve as a generative distribution responsible for both lexicon and corpus data. The proposed model memoizes previously used lexical entries (words or phrases) but backs off to a character-level distribution when generating novel types (Teh, 2006; Mochihashi et al., 2009). We propose an efficient inference algorithm for the proposed model using particle Gibbs sampling (§3). Our code is available at https://github.com/noa/bayesner. 1029 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1029–1039 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1095 2 Model Our goal is to fit a model that can automatically annotate text. We observe a supervised or unsupervised training corpus. For each label y in the annotation schem"
P17-1095,P11-1087,0,\N,Missing
P17-1095,N07-1018,0,\N,Missing
P17-1109,P15-2085,0,0.0169058,"IPA along the x-axis). in NLP. Determinantal point processes have found a home in the literature in tasks that require diversity. E.g., DPPs have achieved state-of-the-art results on multi-document document summarization (Kulesza and Taskar, 2011), news article selection (Affandi et al., 2012) recommender systems (Gartrell et al., 2017), joint clustering of verbal lexical semantic properties (Reichart and Korhonen, 2013), inter alia. Poisson point processes have also been applied to NLP problems: Yee et al. (2015) model the emerging topic on social media using a homogeneous point process and Lukasik et al. (2015) apply a log-Gaussian point process, a variant of the Poisson point process, to rumor detection in Twitter. We are unaware of previous attempts to probabilistically model vowel inventory typology. Future Work. This work lends itself to several technical extensions. One could expand the function f to more completely characterize each vowel’s acoustic properties, perceptual properties, or distinctive features (footnote 7). One could generalize our point process models to sample finite subsets from the continuous space of vowels (footnote 3). One could consider augmenting the MPP with a new facto"
P17-1109,P13-1085,0,0.0201066,"10 0 0 i u a o e ɔ ɛ ɪ y ʊ ɑ ø æ ə ɨ œ ʏ ɯ ʌ ɤ ɒ ɵ ʉ ɜ ɐ e ö Figure 4: Percentage of the vowel inventories (y-axis) in the Becker-Kristal corpus (Becker-Kristal, 2010) that have a given vowel (shown in IPA along the x-axis). in NLP. Determinantal point processes have found a home in the literature in tasks that require diversity. E.g., DPPs have achieved state-of-the-art results on multi-document document summarization (Kulesza and Taskar, 2011), news article selection (Affandi et al., 2012) recommender systems (Gartrell et al., 2017), joint clustering of verbal lexical semantic properties (Reichart and Korhonen, 2013), inter alia. Poisson point processes have also been applied to NLP problems: Yee et al. (2015) model the emerging topic on social media using a homogeneous point process and Lukasik et al. (2015) apply a log-Gaussian point process, a variant of the Poisson point process, to rumor detection in Twitter. We are unaware of previous attempts to probabilistically model vowel inventory typology. Future Work. This work lends itself to several technical extensions. One could expand the function f to more completely characterize each vowel’s acoustic properties, perceptual properties, or distinctive fe"
P17-1109,W15-0815,0,0.0148679,"es (y-axis) in the Becker-Kristal corpus (Becker-Kristal, 2010) that have a given vowel (shown in IPA along the x-axis). in NLP. Determinantal point processes have found a home in the literature in tasks that require diversity. E.g., DPPs have achieved state-of-the-art results on multi-document document summarization (Kulesza and Taskar, 2011), news article selection (Affandi et al., 2012) recommender systems (Gartrell et al., 2017), joint clustering of verbal lexical semantic properties (Reichart and Korhonen, 2013), inter alia. Poisson point processes have also been applied to NLP problems: Yee et al. (2015) model the emerging topic on social media using a homogeneous point process and Lukasik et al. (2015) apply a log-Gaussian point process, a variant of the Poisson point process, to rumor detection in Twitter. We are unaware of previous attempts to probabilistically model vowel inventory typology. Future Work. This work lends itself to several technical extensions. One could expand the function f to more completely characterize each vowel’s acoustic properties, perceptual properties, or distinctive features (footnote 7). One could generalize our point process models to sample finite subsets fro"
P19-1491,P18-1007,0,0.0157771,"We restrict the set of characters to those that we see at least 25 times in the training set, replacing all others with a new symbol ^, as is common and easily defensible in openvocabulary language modeling (Mielke and Eisner, 2018). We make an exception for Chinese, where we only require each character to appear at least twice. These thresholds result in negligible “out-of-alphabet” rates for all languages. 7 In practice, in both training and testing, we only evaluate the probability of the canonical segmentation of the held-out string, rather than the total probability of all segmentations (Kudo, 2018; Mielke and Eisner, 2018, Appendix D.2). 8 Figure 2 shows the 21 languages of the Europarl dataset. Optimal values: 0.2 (et); 0.3 (fi, lt); 0.4 (de, es, hu, lv, sk, sl); 0.5 (da, fr, pl, sv); 0.6 (bg, ru); 0.7 (el); 0.8 (en); 0.9 (it, pt). 3 6 5.8 5.6 5.4 ·106 hu nl pl el ro cs pt es it en et fr sk bg sl fi lv lt sv da 3.2 de hu pl el cs nl pt ro it es et en sk fr sl lt bg fi lv da sv hu de hu pl cs el pt nl et it ro es fr en sl sk bg lt fi lv sv da 0.2 hu de pl cs ro nl el et pt es it fi lt fr sk en sl bg lv sv da hu de hu de de pl cs ro nl et el it pt es lt fi sk sl fr lv en bg pl cs et nl"
P19-1491,E12-1026,0,0.0141882,") the raw character sequence length—are statistically significant indicators of modeling difficulty within our large set of languages. In contrast, we fail to reproduce our earlier results from Cotterell et al. (2018),1 which suggested morphological complexity as an indicator of modeling complexity. In fact, we find no tenable correlation to a wide variety of typological features, taken from the WALS dataset and other sources. Additionally, exploiting our model’s ability to handle missing data, we directly test the hypothesis that translationese leads to easier language-modeling (Baker, 1993; Lembersky et al., 2012). We ultimatelycast doubt on this claim, showing that, under the strictest controls, translationese is different, but not any easier to model according to our notion of difficulty. We conclude with a recommendation: The world being small, typology is in practice a small-data problem. there is a real danger that cross-linguistic studies will under-sample and thus over-extrapolate. We outline directions for future, more robust, investigations, and further caution that future work of this sort should focus on datasets with far more languages, something our new methods now allow. 2 The Surprisal o"
P19-1491,P16-1162,0,0.00976653,".5 If we were to assume that our language models were perfect in the sense that they captured the true probability distribution of a language, we could make the former claim; but we suspect that much of the difference can be explained by our imperfect LMs rather than inherent differences in the expressed information (see the discussion in footnote 3). 2.3 BPE-RNNLM BPE-based open-vocabulary language models make use of sub-word units instead of either words or characters and are a strong baseline on multiple languages (Mielke and Eisner, 2018). Before training the RNN, byte pair encoding (BPE; Sennrich et al., 2016) is applied globally to the training corpus, splitting each word (i.e., each space-separated substring) into one or more units. The RNN is then trained over the sequence of units, which looks like this: “The |ex|os|kel|eton |is |gener|ally |blue”. The set of subword units is finite and determined from training data only, but it is a superset of the alphabet, making it possible to explain any novel word in held-out data via some segmentation.7 One important thing to note is that the size of this set can be tuned by specifying the number of BPE merges, allowing us to smoothly vary between a word"
P19-1491,L16-1680,0,0.0569772,"Missing"
P19-1491,K17-3009,0,0.0601694,"Missing"
P19-1491,H01-1035,0,0.0770762,"led by the data that it did not make much sense to spend time on them. Specifically, for full inference, we implemented all models in STAN (Carpenter et al., 2017), a 4.2 14 One could also use a Cauchy distribution instead of the Laplace distribution to get even heavier tails, but we saw little difference between the two in practice. 15 Further enhancements are possible: we discuss our “Model 3” in Appendix B, but it did not seem to fit better. The Bible: 62 Languages The Bible is a religious text that has been used for decades as a dataset for massively multilingual NLP (Resnik et al., 1999; Yarowsky et al., 2001; Agi´c et al., 2016). Concretely, we use the 5 de pl hu fr lt da el it bg fi cs ro nl pt es et sl en sk sv lv hardly affected when tuning the number of BPE merges per-language instead of globally, validating our approach of using the BPE model for our experiments. A bigger difference seems to be the choice of char-RNNLM vs. BPE-RNNLM, which changes the ranking of languages both on Europarl data and on Bibles. We still see German as the hardest language, but almost all other languages switch places. Specifically, we can see that the variance of the char-RNNLM is much higher. chars BPE (0.4|V |"
P19-1491,N18-1202,0,0.0467576,"al properties that make certain languages harder to language-model than others. One of the oldest tasks in NLP (Shannon, 1951) is language modeling, which attempts to estimate a distribution ?(x) over strings x of a language. Recent years have seen impressive improvements with recurrent neural language models (e.g., Merity et al., 2018). Language modeling is an important component of tasks such as speech recognition, machine translation, and text normalization. It has also enabled the construction of contextual word embeddings that provide impressive performance gains in many other NLP tasks (Peters et al., 2018)—though those downstream evaluations, too, have focused on a small number of (mostly English) datasets. In prior work (Cotterell et al., 2018), we compared languages in terms of the difficulty of language modeling, controlling for differences in content by using a multi-lingual, fully parallel text corpus. Few such corpora exist: in that paper, we made Introduction Do current NLP tools serve all languages? Technically, yes, as there are rarely hard constraints that prohibit application to specific languages, as long as there is data annotated for the task. However, in practice, the answer is m"
P19-1491,W09-0106,0,\N,Missing
P19-1491,2005.mtsummit-papers.11,0,\N,Missing
P19-1491,mayer-cysouw-2014-creating,0,\N,Missing
P19-1491,N18-2085,1,\N,Missing
P19-1491,L18-1293,1,\N,Missing
P19-1491,D18-1312,0,\N,Missing
P19-1491,Q15-1030,0,\N,Missing
P96-1011,P89-1033,0,0.0622596,"losion of redundant CCG derivations. In particular, it is proved in §4.2 that the method constructs exactly one syntactic structure per semantic reading--e.g., just two parses for (3). All other parses are suppressed by simple normal-form constraints that are enforced throughout the parsing process. This approach works because CCG's spurious ambiguities arise (as is shown) in only a small set of circumstances. Although similar work has been attempted in the past, with varying degrees of success (Karttunen, 1986; Wittenburg, 1986; Pareschi & Steedman, 1987; Bouma, 1989; Hepple & Morrill, 1989; KSnig, 1989; Vijay-Shanker & Weir, 1990; Hepple, 1990; Moortgat, 1990; ttendriks, 1993; Niv, 1994), this appears to be the first full normal-form result for a categorial formalism having more than contextfree power. Introduction Combinatory Categorial Grammar (Steedman, 1990), like other &quot;flexible&quot; categorial grammars, suffers from spurious ambiguity (Wittenburg, 1986). The non-standard constituents that are so crucial to CCG's analyses in (1), and in its account of intonational focus (Prevost ~ Steedman, 1994), remain available even in simpler sentences. This renders (2) syntactically ambiguous. (1) a."
P96-1011,P94-1018,0,0.558304,"constructs exactly one syntactic structure per semantic reading--e.g., just two parses for (3). All other parses are suppressed by simple normal-form constraints that are enforced throughout the parsing process. This approach works because CCG's spurious ambiguities arise (as is shown) in only a small set of circumstances. Although similar work has been attempted in the past, with varying degrees of success (Karttunen, 1986; Wittenburg, 1986; Pareschi & Steedman, 1987; Bouma, 1989; Hepple & Morrill, 1989; KSnig, 1989; Vijay-Shanker & Weir, 1990; Hepple, 1990; Moortgat, 1990; ttendriks, 1993; Niv, 1994), this appears to be the first full normal-form result for a categorial formalism having more than contextfree power. Introduction Combinatory Categorial Grammar (Steedman, 1990), like other &quot;flexible&quot; categorial grammars, suffers from spurious ambiguity (Wittenburg, 1986). The non-standard constituents that are so crucial to CCG's analyses in (1), and in its account of intonational focus (Prevost ~ Steedman, 1994), remain available even in simpler sentences. This renders (2) syntactically ambiguous. (1) a. C o o r d i n a t i o n : [[John likes]s/NP, and [Mary pretends to like]s/NP], the big"
P96-1011,P90-1001,0,0.337795,"undant CCG derivations. In particular, it is proved in §4.2 that the method constructs exactly one syntactic structure per semantic reading--e.g., just two parses for (3). All other parses are suppressed by simple normal-form constraints that are enforced throughout the parsing process. This approach works because CCG's spurious ambiguities arise (as is shown) in only a small set of circumstances. Although similar work has been attempted in the past, with varying degrees of success (Karttunen, 1986; Wittenburg, 1986; Pareschi & Steedman, 1987; Bouma, 1989; Hepple & Morrill, 1989; KSnig, 1989; Vijay-Shanker & Weir, 1990; Hepple, 1990; Moortgat, 1990; ttendriks, 1993; Niv, 1994), this appears to be the first full normal-form result for a categorial formalism having more than contextfree power. Introduction Combinatory Categorial Grammar (Steedman, 1990), like other &quot;flexible&quot; categorial grammars, suffers from spurious ambiguity (Wittenburg, 1986). The non-standard constituents that are so crucial to CCG's analyses in (1), and in its account of intonational focus (Prevost ~ Steedman, 1994), remain available even in simpler sentences. This renders (2) syntactically ambiguous. (1) a. C o o r d i n a t i o n : [["
P96-1011,E89-1002,0,0.647583,"at prevents any such explosion of redundant CCG derivations. In particular, it is proved in §4.2 that the method constructs exactly one syntactic structure per semantic reading--e.g., just two parses for (3). All other parses are suppressed by simple normal-form constraints that are enforced throughout the parsing process. This approach works because CCG's spurious ambiguities arise (as is shown) in only a small set of circumstances. Although similar work has been attempted in the past, with varying degrees of success (Karttunen, 1986; Wittenburg, 1986; Pareschi & Steedman, 1987; Bouma, 1989; Hepple & Morrill, 1989; KSnig, 1989; Vijay-Shanker & Weir, 1990; Hepple, 1990; Moortgat, 1990; ttendriks, 1993; Niv, 1994), this appears to be the first full normal-form result for a categorial formalism having more than contextfree power. Introduction Combinatory Categorial Grammar (Steedman, 1990), like other &quot;flexible&quot; categorial grammars, suffers from spurious ambiguity (Wittenburg, 1986). The non-standard constituents that are so crucial to CCG's analyses in (1), and in its account of intonational focus (Prevost ~ Steedman, 1994), remain available even in simpler sentences. This renders (2) syntactically ambig"
P96-1011,P87-1011,0,0.198419,"djoining Grammars (TAGs). Most work on spurious ambiguity has focused on categorial formalisms with substantially less power. (Hepple, 1990) and (Hendriks, 1993), the most rigorous pieces of work, each establish a normal form for the syntactic calculus of (Lambek, 1958), which is weakly context-free. (Kbnig, 1989; Moortgat, 1990) have also studied the Lambek calculus case. (Hepple & Morrill, 1989), who introduced the idea of normalform parsing, consider only a small CCG fragment that lacks backward or order-changing composition; (Niv, 1994) extends this result but does not show completeness. (Wittenburg, 1987) assumes a CCG fragment lacking order-changing or higherorder composition; furthermore, his revision of the combinators creates new, conjoinable constituents that conventional CCG rejects. (Bouma, 1989) proposes to replace composition with a new combinator, but the resulting product-grammar scheme as(5) a. b. C. d. e. f. [John]s/(ssp) [likes](SNP)/Np [John likes]s/N P [Mary]NP [likes Mary]sN P [[John likes] Mary]s ~ to be disallowed g, [John [likes Mary]Is The proposal is to construct all constituents shown in (5) except for (5f). If we slightly constrain the use of the grammar rules, the p"
P96-1011,E89-1003,0,\N,Missing
P96-1011,P87-1012,0,\N,Missing
P96-1011,J93-4002,0,\N,Missing
P97-1040,P96-1014,0,0.141459,"Missing"
P97-1040,J94-1003,0,\N,Missing
P97-1040,C94-2163,0,\N,Missing
P99-1059,P96-1023,0,0.664127,"f particular pairs of words in particular roles. The acceptability of ""Nora convened the "" The authors were supported respectivelyunder ARPA Grant N6600194-C-6043 ""Human LanguageTechnology"" and Ministero dell&apos;Universitk e della Ricerca Scientifica e Tecnologicaproject ""Methodologiesand Tools of High Performance Systems for Multimedia Applications."" 457 party"" then depends on the grammar writer&apos;s assessment of whether parties can be convened. Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). The rationale is that soft selectional restrictions play a crucial role in disambiguation, i The chart parsing algorithms used by most of the above authors run in time O(nS), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size O(n 2) in practice). Heavy probabilistic pruning is therefore needed to get acceptable runtimes. But in this paper we show that the complexity is not so bad after all: • For bilexicalized context-free grammars, O(n 4) is possible. • The O(n 4) result also holds for head aut"
P99-1059,P98-1035,0,0.0259716,"m was previously known (Eisner, 1997), the grammar constant can be reduced without harming the O(n 3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol1Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). lows (Harrison, 1978; Hopcroft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT, P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol. Set P is a finite set of productions having the form A --+ a, where A E VN, a E (VN U VT)*. If every production in P has the form A -+ B C or A --+ a, for A , B , C E VN,a E VT, then the grammar is said to be in Chomsky Normal Form (CNF). 2 Every language that can be generated by a CFG can also be generated by a CFG in CNF. In this paper we adopt the following"
P99-1059,W95-0103,0,0.0265967,"s where an O(n 3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the O(n 3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol1Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). lows (Harrison, 1978; Hopcroft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT, P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol. Set P is a finite set of productions having the form A --+ a, where A E VN, a E (VN U VT)*. If every production in P has the form A -+ B C or A --+ a, for A , B , C E VN,a E VT, then the grammar is said to be in Chomsky Normal Form (CNF). 2 Every language that can be generated by a CFG can also be generated by a CFG in CNF. In this pa"
P99-1059,P97-1003,0,0.177939,"es. The acceptability of ""Nora convened the "" The authors were supported respectivelyunder ARPA Grant N6600194-C-6043 ""Human LanguageTechnology"" and Ministero dell&apos;Universitk e della Ricerca Scientifica e Tecnologicaproject ""Methodologiesand Tools of High Performance Systems for Multimedia Applications."" 457 party"" then depends on the grammar writer&apos;s assessment of whether parties can be convened. Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). The rationale is that soft selectional restrictions play a crucial role in disambiguation, i The chart parsing algorithms used by most of the above authors run in time O(nS), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size O(n 2) in practice). Heavy probabilistic pruning is therefore needed to get acceptable runtimes. But in this paper we show that the complexity is not so bad after all: • For bilexicalized context-free grammars, O(n 4) is possible. • The O(n 4) result also holds for head automaton grammars. • For a very common special c"
P99-1059,1997.iwpt-1.10,1,0.797835,"mation Science University of Pennsylvania 200 South 33rd Street, Philadelphia, PA 19104 USA j eisner@linc, cis. upenn, edu Dip. di E l e t t r o n i c a e I n f o r m a t i c a Universit£ di Padova via Gradenigo 6/A, 35131 Padova, Italy satt a@dei, unipd, it Abstract Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words. We present O(n 4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of O(n5). For a common special case that was known to allow O(n 3) parsing (Eisner, 1997), we present an O(n 3) algorithm with an improved grammar constant. 1 Introduction Lexicalized grammar formalisms are of both theoretical and practical interest to the computational linguistics community. Such formalisms specify syntactic facts about each word of the language--in particular, the type of arguments that the word can or must take. Early mechanisms of this sort included categorial grammar (Bar-Hillel, 1953) and subcategorization frames (Chomsky, 1965). Other lexicalized formalisms include (Schabes et al., 1988; Mel&apos;~uk, 1988; Pollard and Sag, 1994). Besides the possible arguments"
P99-1059,P95-1037,0,0.0650139,"of these grammars where an O(n 3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the O(n 3) property. Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents. We use dynamic programming to assemble such subderivations into a full parse. 2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars. Our notation fol1Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship (Lafferty et al., 1992; Magerman, 1995; Collins and Brooks, 1995; Chelba and Jelinek, 1998). lows (Harrison, 1978; Hopcroft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT, P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol. Set P is a finite set of productions having the form A --+ a, where A E VN, a E (VN U VT)*. If every production in P has the form A -+ B C or A --+ a, for A , B , C E VN,a E VT, then the grammar is said to be in Chomsky Normal Form (CNF). 2 Every language that can be generated by a CFG can also be generated b"
P99-1059,C88-2121,0,0.0120233,"ds of O(n5). For a common special case that was known to allow O(n 3) parsing (Eisner, 1997), we present an O(n 3) algorithm with an improved grammar constant. 1 Introduction Lexicalized grammar formalisms are of both theoretical and practical interest to the computational linguistics community. Such formalisms specify syntactic facts about each word of the language--in particular, the type of arguments that the word can or must take. Early mechanisms of this sort included categorial grammar (Bar-Hillel, 1953) and subcategorization frames (Chomsky, 1965). Other lexicalized formalisms include (Schabes et al., 1988; Mel&apos;~uk, 1988; Pollard and Sag, 1994). Besides the possible arguments of a word, a natural-language grammar does well to specify possible head words for those arguments. ""Convene"" requires an NP object, but some NPs are more semantically or lexically appropriate here than others, and the appropriateness depends largely on the NP&apos;s head (e.g., ""meeting""). We use the general term bilexical for a grammar that records such facts. A bilexical grammar makes many stipulations about the compatibility of particular pairs of words in particular roles. The acceptability of ""Nora convened the "" The auth"
P99-1059,C92-2066,0,0.0140956,"Missing"
P99-1059,C98-1035,0,\N,Missing
Q15-1031,N15-1094,1,0.837701,"ynamically restrict to a finite support set of plausible values for m. We take this to be the union of the 20-best lists of all messages sent to m.9 We then prune those messages so that they give weight 0 to all strings outside m’s support set. As a result, m’s outgoing messages and belief are also confined to its support set. Note that the support set is not handspecified, but determined automatically by taking the top hypotheses under the probability model. Improved approaches with no pruning are possible. After submitting this paper, we developed a penalized expectation propagation method (Cotterell and Eisner, 2015). It dynamically approximates messages using log-linear functions (based on variable-order n-gram features) whose support is the entire space Σ∗ . We also developed a dual decomposition method (Peng et al., 2015), which if it converges, exactly recovers the single most 8 This is standard—although the uniform distribution over the space of strings is actually an improper distribution. It is expressed by a single-state WFSM whose arcs have weight 1. It can be shown that the beliefs are proper distributions after one iteration, though the upward messages may not be. 9 In general, we should update"
Q15-1031,P14-2102,1,0.677762,"(after erasing #) to the surface [wER@r]. At the point shown, it is applying the “intervocalic alveolar flapping” rule, replacing /t/ in this context by applying SUBST(R). cape such low-likelihood solutions, much as backtracking escapes zero-likelihood solutions. 3.2 Mapping URs to SRs: The phonology Sθ We currently model Sθ (s |u) as the probability that a left-to-right stochastic contextual edit process (Figure 2) would edit u into s. This probability is a sum over all edit sequences that produce s from u—that is, all s-to-u alignments. Stochastic contextual edit processes were described by Cotterell et al. (2014). Such a process writes surface string s ∈ Σ∗s while reading the underlying string u ∈ Σ∗u . If the process has so far consumed some prefix of the input and produced some prefix of the output, it will next make a stochastic choice among 2|Σs |+ 1 possible edits. Edits of the form SUBST(c) or INSERT(c) (for c ∈ Σs ) append c to the output string. Edits of the form SUBST(c) or DELETE will (also) consume the next input phoneme; if no input phonemes remain, the only possible edits are INSERT(c) or HALT. The stochastic choice of edit, given context, is governed by a conditional log-linear distribut"
Q15-1031,D09-1011,1,0.359415,"vely. Each variable’s distribution is conditioned on the values of its parents, if any. Layer 1 represents the unknown M (a) for various a. Notice that each M (a) is softly constrained by the prior Mφ , and also by its need to help produce various observed surface words via Sθ . Each underlying word u at level 2 is a concatenation of its underlying morphs M (ai ) at level 1. Thus, the topology at levels 1–2 is given by supervision. We would have to learn this topology if the word’s morphemes ai were not known. Our approach captures the unbounded generative capacity of language. In contrast to Dreyer and Eisner (2009) (see section 8), we have defined a directed graphical model. Hence new unobserved descendants can be added without changing the posterior distribution over the existing variables. So our finite network can be viewed as a subgraph of an infinite graph. That is, we make no closed-vocabulary assumption, but implicitly include (and predict the surface forms of) any unobserved words that could result from combining morphemes, even morphemes not in our dataset. While the present paper focuses on word types, we could extend the model to consider tokens as well. In Figure 1, each phonological surface"
Q15-1031,D11-1057,1,0.865251,"Missing"
Q15-1031,D10-1056,0,0.0117644,"ctor may be the increased error rate of the phonological learner, visible even with oracle data. Thus, we suspect that a Sθ model with better generalization would improve our results at all training sizes. Note that weakening Sθ —allowing only “noisy concatenation”—clearly harms the method, proving the need for true phonological modeling. 8 Related Work We must impute the inputs to the phonological noisy channel Sθ (URs) because we observe only the outputs (SRs). Other NLP problems of this form include unsupervised text normalization (Yang and Eisenstein, 2013), unsupervised training of HMMs (Christodoulopoulos et al., 2010), and particularly unsupervised lexicon acquisition from phonological data (Elsner et al., 2012). However, unlike these studies, we currently use some indirect supervision—we know each SR’s morpheme sequence, though not the actual morphs. Jarosz (2013, §2) and Tesar (2014, chapters 5– 6) review work on learning the phonology Sθ . Phonologists pioneered stochastic-gradient and passive-aggressive training methods—the Gradual Learning Algorithm (Boersma, 1998) and ErrorDriven Constraint Demotion (Tesar and Smolensky, 1998)—for structured prediction of the surface word s from the underlying word u"
Q15-1031,D08-1113,1,0.747087,"morphs, either identifying a small set of plausible morphs or prohibiting segmental insertion/deletion. We use finite-state methods so that it is possible to consider the space Σ∗u of all strings. 13 She still assumes that word SRs are annotated with morpheme boundaries, and that a small set of possible morphs is given. These assumptions are relaxed by Eisenstat (2009). On the other hand, we are divided from previous work by our inability to use an OT grammar (Prince and Smolensky, 2004), a stochastic OT grammar (Boersma, 1997), or even a maximum entropy grammar (Goldwater and Johnson, 2003; Dreyer et al., 2008; Eisenstat, 2009). The reason is that our BP method inverts the phonological mapping Sθ to find possible word URs. Given a word SR s, we construct a WFSM (message) that scores every possible UR u ∈ Σ∗u —the score of u is Sθ (s |u). For this to be possible without approximation, Sθ itself must be represented as a WFSM (section 3.2). Unfortunately, the WFSM for a maximum entropy grammar does not compute Sθ but only an unnormalized version, with a different normalizing constant Zu needed for each u. We plan to confront this issue in future work. In the NLP community, Elsner et al. (2013) resembl"
Q15-1031,P02-1008,1,0.80485,"y motivation for probabilistic models of phonology (Pierrehumbert, 2003) has been to explain “soft” phenomena: synchronic variation (Sankoff, 1978; Boersma and Hayes, 2001) or graded acceptability judgments on novel surface forms (Hayes and Wilson, 2008). These applications are orthogonal to our motivation, as we do not observe any variation or gradience in our present experiments. Fundamentally, we use probabilities to measure irregularity—which simply means unpredictability and is a matter of degree. Our objective function will quantitatively favor explanations that show greater regularity (Eisner, 2002b). A probabilistic treatment also allows relatively simple learning methods (e.g., Boersma and Hayes (2001)) since inference never has to backtrack from a contradiction. Our method searches a continuous space of phonologies Sθ , all of which are consistent with every mapping S. That is, we always have Sθ (s |u) > 0 for all u, s, so our current guess of Sθ is always capable of explaining the observed words, albeit perhaps with low probability. Our EM learner tunes Sθ (and Mφ ) so as to raise the probability of the observed surface forms, marginalizing over the reconstructed lexicon M of underl"
Q15-1031,P12-1020,0,0.139946,"spect that a Sθ model with better generalization would improve our results at all training sizes. Note that weakening Sθ —allowing only “noisy concatenation”—clearly harms the method, proving the need for true phonological modeling. 8 Related Work We must impute the inputs to the phonological noisy channel Sθ (URs) because we observe only the outputs (SRs). Other NLP problems of this form include unsupervised text normalization (Yang and Eisenstein, 2013), unsupervised training of HMMs (Christodoulopoulos et al., 2010), and particularly unsupervised lexicon acquisition from phonological data (Elsner et al., 2012). However, unlike these studies, we currently use some indirect supervision—we know each SR’s morpheme sequence, though not the actual morphs. Jarosz (2013, §2) and Tesar (2014, chapters 5– 6) review work on learning the phonology Sθ . Phonologists pioneered stochastic-gradient and passive-aggressive training methods—the Gradual Learning Algorithm (Boersma, 1998) and ErrorDriven Constraint Demotion (Tesar and Smolensky, 1998)—for structured prediction of the surface word s from the underlying word u. If s is not fully observed during training (layer 4 of Figure 1 is observed, not layer 3), the"
Q15-1031,D13-1005,0,0.197829,"n, 2003; Dreyer et al., 2008; Eisenstat, 2009). The reason is that our BP method inverts the phonological mapping Sθ to find possible word URs. Given a word SR s, we construct a WFSM (message) that scores every possible UR u ∈ Σ∗u —the score of u is Sθ (s |u). For this to be possible without approximation, Sθ itself must be represented as a WFSM (section 3.2). Unfortunately, the WFSM for a maximum entropy grammar does not compute Sθ but only an unnormalized version, with a different normalizing constant Zu needed for each u. We plan to confront this issue in future work. In the NLP community, Elsner et al. (2013) resembles our work in many respects. Like us, they recover a latent underlying lexicon (using the same simple prior Mφ ) and use EM to learn a phonology (rather similar to our Sθ , though less powerful).14 Unlike us, they do not assume annotation of the (abstract) morpheme sequence, but jointly learn a nonparametric bigram model to discover the morphemes. Their evaluation is quite different, as their aim is actually to recover underlying words from phonemically transcribed child-directed English utterances. However, nothing in their model distinguishes words from morphemes—indeed, sometimes t"
Q15-1031,E12-1068,0,0.0103876,"obabilistic finite-state transducer). Layer 4 derives observable phonetic forms from layer 3. This deletes unpronounced symbols such as syllable boundaries, and translates the phonemes into an observed phonetic, articulatory, or acoustic representation. However, our present paper simply merges layers 3 and 4: our layer 3 does not currently make use of any unpronounced symbols (e.g., syllable boundaries) and we observe it directly. to model surface spellings (as needed for MT on text). Good morphological analysis has been used to improve NLP tasks such as machine translation, parsing, and NER (Fraser et al., 2012; Hohensee and Bender, 2012; Yeniterzi, 2011). Using loopy belief propagation, this paper attacks larger-scale learning problems than prior work on this task (section 8). We also develop a new evaluation paradigm that examines how well an inferred grammar predicts held-out SRs. Unlike previous algorithms, we do not pre-restrict the possible URs for each morpheme to a small or structured finite set, but use weighted finite-state machines to reason about the infinite space of all strings. Our graphical model captures the standard assumption that each morpheme has a single UR, unlike some probabi"
Q15-1031,P10-1105,0,0.0723142,"the partition function and requires retraining. By contrast, our trained directed model is a productive phonological system that can generate unboundedly many new words (see section 4.1). By analogy, n samples from a Gaussian would be described with a directed model, and inferring the Gaussian parameters predicts any number of future samples n + 1, n + 2, . . .. Bouchard-Cˆot´e et al., in several papers from 2007 through 2013, have used directed graphical models over strings, like ours though without loops, to model diachronic sound change. Sometimes they use belief propagation for inference (Hall and Klein, 2010). Their goal is to recover latent historical forms (conceptually, surface forms) rather than latent underlying forms. The results are evaluated against manual reconstructions. None of this work has segmented words into morphs, although Dreyer et al. (2008) did segment surface words into latent “regions.” Creutz and Lagus (2005) and Goldsmith (2006) segment an unannotated collection of words into reusable morphs, but without modeling contextual sound change, i.e., phonology. 9 Conclusions and Future Work We have laid out a probabilistic model for generative phonology. This lets us infer likely"
Q15-1031,N12-1032,0,0.0283479,"ate transducer). Layer 4 derives observable phonetic forms from layer 3. This deletes unpronounced symbols such as syllable boundaries, and translates the phonemes into an observed phonetic, articulatory, or acoustic representation. However, our present paper simply merges layers 3 and 4: our layer 3 does not currently make use of any unpronounced symbols (e.g., syllable boundaries) and we observe it directly. to model surface spellings (as needed for MT on text). Good morphological analysis has been used to improve NLP tasks such as machine translation, parsing, and NER (Fraser et al., 2012; Hohensee and Bender, 2012; Yeniterzi, 2011). Using loopy belief propagation, this paper attacks larger-scale learning problems than prior work on this task (section 8). We also develop a new evaluation paradigm that examines how well an inferred grammar predicts held-out SRs. Unlike previous algorithms, we do not pre-restrict the possible URs for each morpheme to a small or structured finite set, but use weighted finite-state machines to reason about the infinite space of all strings. Our graphical model captures the standard assumption that each morpheme has a single UR, unlike some probabilistic learners. However, w"
Q15-1031,W09-0803,0,0.0294032,"Any other WFSMs could be substituted. We are on rather firm ground in restricting to finite-state (regular) models of Sθ . The apparent regularity of natural-language phonology was first observed by Johnson (1972), so computational phonology has generally preferred grammar formalisms that compile into (unweighted) finite-state machines, whether the formalism is based on rewrite rules (Kaplan and Kay, 1994) or constraints (Eisner, 2002a; Riggle, 2004). Similarly, U could be any finite-state relation,7 not just concatenation as in section 2. Thus our framework could handle templatic morphology (Hulden, 2009), infixation, or circumfixation. Although only regular factors are allowed in our graphical model, a loopy graphical model with multiple such factors can actually capture nonregular phenomena, for example by using auxiliary variables (Dreyer and Eisner, 2009, §3.4). Approximate inference then proceeds by loopy BP on this model. In particular, reduplication is not regular if unbounded, but we can adopt morphological doubling theory (Inkelas and Zoll, 2005) and model it by having U concatenate two copies of the same morph. During inference of URs, this morph exchanges messages with two substring"
Q15-1031,W06-3207,0,0.0268686,"the SRs. (Tesar and Merchant instead used binary variables, one for each segmental feature in each UR—requiring the simplifying assumption that the URs are known except for their segmental features. They assume that SRs are annotated with morph boundaries and that the phonology only changes segmental features, never inserting or deleting segments.) On the other hand, Tesar and Merchant reason globally about the constraint ranking, whereas in this paper, we only locally improve the phonology—we use EM, rather than the full Bayesian approach that treats the parameters θ~ as variables within BP. Jarosz (2006) is closest to our work in that she uses EM, just as we do, to maximize the probability of observed surface forms whose constituent morphemes (but not morphs) are known.13 Her model is a probabilistic analogue of Apoussidou (2006), who uses a latent-variable structured perceptron. A non-standard aspect of this model (defended by Pater et al. (2012)) is that a morpheme a can stochastically choose different morphs M (a) when it appears in different words. To obtain a single shared morph, one could penalize this distribution’s entropy, driving it toward 0 as learning proceeds. Such an approach—wh"
Q15-1031,J94-3001,0,0.484864,"t layers 1, 2, and 3 given the values at their parents. As section 3 models these, for any φ and θ, we can represent Mφ as a 1-tape WFSM (acceptor), U as a multi-tape WFSM, and Sθ as a 2-tape WFSM (transducer).6 Any other WFSMs could be substituted. We are on rather firm ground in restricting to finite-state (regular) models of Sθ . The apparent regularity of natural-language phonology was first observed by Johnson (1972), so computational phonology has generally preferred grammar formalisms that compile into (unweighted) finite-state machines, whether the formalism is based on rewrite rules (Kaplan and Kay, 1994) or constraints (Eisner, 2002a; Riggle, 2004). Similarly, U could be any finite-state relation,7 not just concatenation as in section 2. Thus our framework could handle templatic morphology (Hulden, 2009), infixation, or circumfixation. Although only regular factors are allowed in our graphical model, a loopy graphical model with multiple such factors can actually capture nonregular phenomena, for example by using auxiliary variables (Dreyer and Eisner, 2009, §3.4). Approximate inference then proceeds by loopy BP on this model. In particular, reduplication is not regular if unbounded, but we c"
Q15-1031,D09-1005,1,0.27958,"al edits (section 3.2), averaged over edit contexts in proportion to how many times those contexts were likely encountered. The latent alignment makes the objective non-concave. In our EM setting, uk is not known. So our Mstep P replaces log Sθ (sk |uk ) with its expectation, uk bk (uk ) log Sθ (sk |uk ), where bk is the normalized belief about uk computed by the previous E-step. Since bk and Sθ are both represented by WFSMs (with 1 and 2 tapes respectively), it is possible to compute this quantity and its gradient exactly, using finite-state composition in a secondorder expectation semiring (Li and Eisner, 2009). For speed, however, we currently prune bk back to the 5-best values of uk . This lets us use a simpler and faster approach: a weighted average over 5 runs of the Cotterell et al. (2014) algorithm. Our asymptotic runtime benefits from the fact that our graphical model is directed (so our objective does not have to contrast with all other values of uk ) and the fact that Sθ is locally normalized (so our objective does not have to contrast with all other values of sk for each uk ). In practice we are far faster than Dreyer and Eisner (2009). We initialized the parameter vector θ to ~0, except f"
Q15-1031,W12-2308,0,0.0252249,"ents.) On the other hand, Tesar and Merchant reason globally about the constraint ranking, whereas in this paper, we only locally improve the phonology—we use EM, rather than the full Bayesian approach that treats the parameters θ~ as variables within BP. Jarosz (2006) is closest to our work in that she uses EM, just as we do, to maximize the probability of observed surface forms whose constituent morphemes (but not morphs) are known.13 Her model is a probabilistic analogue of Apoussidou (2006), who uses a latent-variable structured perceptron. A non-standard aspect of this model (defended by Pater et al. (2012)) is that a morpheme a can stochastically choose different morphs M (a) when it appears in different words. To obtain a single shared morph, one could penalize this distribution’s entropy, driving it toward 0 as learning proceeds. Such an approach—which builds on a suggestion by Eisenstat (2009, §5.4)—would loosely resemble dual decomposition (Peng et al., 2015). Unlike our BP approach, it would maximize rather than marginalize over possible underlying morphs. Our work has focused on scaling up inference. For the phonology S, the above papers learn the weights or rankings of just a few plausib"
Q15-1031,D15-1108,1,0.912515,"utside m’s support set. As a result, m’s outgoing messages and belief are also confined to its support set. Note that the support set is not handspecified, but determined automatically by taking the top hypotheses under the probability model. Improved approaches with no pruning are possible. After submitting this paper, we developed a penalized expectation propagation method (Cotterell and Eisner, 2015). It dynamically approximates messages using log-linear functions (based on variable-order n-gram features) whose support is the entire space Σ∗ . We also developed a dual decomposition method (Peng et al., 2015), which if it converges, exactly recovers the single most 8 This is standard—although the uniform distribution over the space of strings is actually an improper distribution. It is expressed by a single-state WFSM whose arcs have weight 1. It can be shown that the beliefs are proper distributions after one iteration, though the upward messages may not be. 9 In general, we should update this support set dynamically as inference and learning improve the messages. But in our present experiments, that appears unnecessary, since the initial support set always appears to contain the “correct” UR. B"
Q15-1031,D13-1007,0,0.015784,"(Maori) in all the training SRs. However, a contributing factor may be the increased error rate of the phonological learner, visible even with oracle data. Thus, we suspect that a Sθ model with better generalization would improve our results at all training sizes. Note that weakening Sθ —allowing only “noisy concatenation”—clearly harms the method, proving the need for true phonological modeling. 8 Related Work We must impute the inputs to the phonological noisy channel Sθ (URs) because we observe only the outputs (SRs). Other NLP problems of this form include unsupervised text normalization (Yang and Eisenstein, 2013), unsupervised training of HMMs (Christodoulopoulos et al., 2010), and particularly unsupervised lexicon acquisition from phonological data (Elsner et al., 2012). However, unlike these studies, we currently use some indirect supervision—we know each SR’s morpheme sequence, though not the actual morphs. Jarosz (2013, §2) and Tesar (2014, chapters 5– 6) review work on learning the phonology Sθ . Phonologists pioneered stochastic-gradient and passive-aggressive training methods—the Gradual Learning Algorithm (Boersma, 1998) and ErrorDriven Constraint Demotion (Tesar and Smolensky, 1998)—for struc"
Q15-1031,P11-3019,0,0.0257895,"rives observable phonetic forms from layer 3. This deletes unpronounced symbols such as syllable boundaries, and translates the phonemes into an observed phonetic, articulatory, or acoustic representation. However, our present paper simply merges layers 3 and 4: our layer 3 does not currently make use of any unpronounced symbols (e.g., syllable boundaries) and we observe it directly. to model surface spellings (as needed for MT on text). Good morphological analysis has been used to improve NLP tasks such as machine translation, parsing, and NER (Fraser et al., 2012; Hohensee and Bender, 2012; Yeniterzi, 2011). Using loopy belief propagation, this paper attacks larger-scale learning problems than prior work on this task (section 8). We also develop a new evaluation paradigm that examines how well an inferred grammar predicts held-out SRs. Unlike previous algorithms, we do not pre-restrict the possible URs for each morpheme to a small or structured finite set, but use weighted finite-state machines to reason about the infinite space of all strings. Our graphical model captures the standard assumption that each morpheme has a single UR, unlike some probabilistic learners. However, we do not try to le"
Q15-1035,P11-1048,0,0.0891706,"Missing"
Q15-1035,P14-1098,0,0.0954955,"Missing"
Q15-1035,W06-2920,0,0.068668,"mit. Finding this difficult to optimize, we introduce a new simpler objective function based on the L2 distance between the approximate marginals and the “true” marginals from the gold data. The goal of this work is to account for the approximations made by a system rooted in structured belief propagation. Taking such approximations into account during training enables us to improve the speed and accuracy of inference at test time. We compare our training method with the standard approach of conditional log-likelihood (CLL) training. We evaluate our parser on 19 languages from the CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks as well as the English Penn Treebank (Marcus et al., 1993). On English, the resulting parser obtains higher accuracy with fewer iterations of BP than CLL. On the CoNLL languages, we find that on average it yields higher accuracy parsers than CLL, particularly when limited to few BP iterations. 2 Dependency Parsing by Belief Propagation This section describes the parser that we will train. Model A factor graph (Frey et al., 1997; Kschischang et al., 2001) defines the factorization of a probability distribution over a set of variables {Y1 , Y2 ,"
Q15-1035,N12-1004,0,0.165629,"Missing"
Q15-1035,D07-1101,0,0.868337,"pected loss on the true data distribution over sentence/parse pairs (X, Y ): θ ∗ = argminθ E[`(hθ (X), Y )] (9) Since the true data distribution is unknown, we substitute the expected loss over the training sample, and regularize our objective in order to reduce sampling variance. Specifically, we aim to minimize the regularized empirical risk, given by (8) with J(θ; x(d) , y (d) ) set to `(hθ (x(d) ), y (d) ). Note that 5 How slow is exact inference for dependency parsing? For certain choices of higher-order factors, polynomial time is possible via dynamic programming (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). However, BP will typically be asymptotically faster (for a fixed number of iterations) and faster in practice. In some other settings, exact inference is NPhard. In particular, non-projective parsing becomes NP-hard with even second-order factors (McDonald and Pereira, 2006). BP can handle this case in polynomial time by replacing the PT REE factor with a T REE factor that allows edges to cross. 6 θ is initialized to 0 when not otherwise specified. 492 this loss function would not be differentiable—a key issue we will take up below. This is the “ERMA” method of Stoyan"
Q15-1035,D09-1011,1,0.880326,"pproximates the marginal distribution over yi values. Messages continue to change indefinitely if the factor graph is cyclic, but in the limit, the messages may converge. Although the equations above update all messages in parallel, convergence is much faster if only one message is updated per timestep, in some well-chosen serial order.4 For the PT REE factor, the summation over vari(t) able assignments required for mα→i (yi ) in Eq. (5) equates to a summation over exponentially many projective parse trees. However, we can use an inside-outside variant of Eisner (1996)’s algorithm 4 Following Dreyer and Eisner (2009, footnote 22), we choose an arbitrary directed spanning tree of the factor graph rooted at the PT REE factor. We visit the nodes in topologically sorted order (from leaves to root) and update any message from the node being visited to a node that is later in the order. We then reverse this order and repeat, so that every message has been passed once. This constitutes one iteration of BP. to compute this in polynomial time (we describe this as hypergraph parsing in §3). The resulting “structured BP” inference procedure—detailed by Smith and Eisner (2008)—is exact for first-order dependency par"
Q15-1035,P15-1030,0,0.0671212,"Missing"
Q15-1035,W05-1504,1,0.869335,"h (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work. Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train a first-order model with CLL and for each token prune any parents for which the marginal probability is less than 0.0001 times the maximum parent marginal for that token. On a per-token basis, we further restrict to the ten parents with highest marginal probability as in Martins et al. (2009) (but we avoid pruning the fully right-branching tree, so that"
Q15-1035,C96-1058,1,0.765392,"alized such that yi bi (yi ) = 1 and approximates the marginal distribution over yi values. Messages continue to change indefinitely if the factor graph is cyclic, but in the limit, the messages may converge. Although the equations above update all messages in parallel, convergence is much faster if only one message is updated per timestep, in some well-chosen serial order.4 For the PT REE factor, the summation over vari(t) able assignments required for mα→i (yi ) in Eq. (5) equates to a summation over exponentially many projective parse trees. However, we can use an inside-outside variant of Eisner (1996)’s algorithm 4 Following Dreyer and Eisner (2009, footnote 22), we choose an arbitrary directed spanning tree of the factor graph rooted at the PT REE factor. We visit the nodes in topologically sorted order (from leaves to root) and update any message from the node being visited to a node that is later in the order. We then reverse this order and repeat, so that every message has been passed once. This constitutes one iteration of BP. to compute this in polynomial time (we describe this as hypergraph parsing in §3). The resulting “structured BP” inference procedure—detailed by Smith and Eisne"
Q15-1035,W08-0804,1,0.820726,"exactly (Stoyanov et al., 2011). 496 Experiments 7.1 Setup Features As the focus of this work is on a novel approach to training, we look to prior work for model and feature design (§2). We add O(n3 ) second-order grandparent and arbitrary-sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work. Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train a first-order model with CLL and for each token prune any parents for which the"
Q15-1035,W96-0214,0,0.658521,"dependency parsing, our loss function is the number of missing edges in the predicted parse ˆ , relative to the reference (or “gold”) parse y ∗ : y P `(ˆ y , y ∗ ) = i: yˆi =OFF I(yi∗ = ON) (1) ˆ and y ∗ each I is the indicator function. Because y specify exactly one parent per word token, `(ˆ y, y∗) equals the directed dependency error: the number of word tokens whose parent is predicted incorrectly. Decoder To obtain a single parse as output, we use a minimum Bayes risk (MBR) decoder, which returns the tree with minimum expected loss under the model’s distribution (Bickel and Doksum, 1977; Goodman, 1996). Our ` gives the decision rule: hθ (x) = argmin Ey∼pθ (· |x) [`(ˆ y , y)] ˆ y = argmax ˆ y X i: yˆi =ON pθ (yi = ON |x) (2) (3) ˆ ranges over well-formed parses. Thus, our Here y parser seeks a well-formed parse hθ (x) whose individual edges have a high probability of being correct according to pθ (since it lacks knowledge y ∗ of which edges are truly correct). MBR is the principled way to take a loss function into account under a probabilistic model. By contrast, maximum a posteriori (MAP) decoding does not consider the loss function. It would return the single highestprobability parse even"
Q15-1035,Q15-1035,1,0.0609026,"repeated application of the chain rule. Backpropagating through an algorithm proceeds by similar application of the chain rule, where the intermediate quantities are determined by the topology of the circuit—just as in Figure 2. Running backwards through the circuit, backprop computes the partial derivatives of the objective J(θ; x, y ∗ ) with respect to each intermediate quantity u—or more concisely ∗) the adjoint of u: ðu = ∂J(θ;x,y . This section ∂u gives a summary of the adjoint computations we require. Due to space constraints, we direct the reader to the extended version of this paper (Gormley et al., 2015a) for full details of all the adjoints. 5.1 Backpropagation of Decoder / Loss The adjoint of the objective itself ðJ(θ; x, y ∗ ) is always 1. So the first adjoints we must compute are those of the beliefs: ðbi (yi ) and ðbα (y α ). This corresponds to the backward pass through Figure 2 (E). Consider the simple case where J is L2 distance from (12): the variable belief adjoint is ðbi (yi ) = 2(bi (yi ) − b∗i (yi )) and trivially ðbα (y α ) = 0. If J is annealed risk from (11), we compute ðbi (yi ) by applying backpropagation recursively to our algorithm for J from §4.1. This sub-algorithm defi"
Q15-1035,D15-1205,1,0.901608,"repeated application of the chain rule. Backpropagating through an algorithm proceeds by similar application of the chain rule, where the intermediate quantities are determined by the topology of the circuit—just as in Figure 2. Running backwards through the circuit, backprop computes the partial derivatives of the objective J(θ; x, y ∗ ) with respect to each intermediate quantity u—or more concisely ∗) the adjoint of u: ðu = ∂J(θ;x,y . This section ∂u gives a summary of the adjoint computations we require. Due to space constraints, we direct the reader to the extended version of this paper (Gormley et al., 2015a) for full details of all the adjoints. 5.1 Backpropagation of Decoder / Loss The adjoint of the objective itself ðJ(θ; x, y ∗ ) is always 1. So the first adjoints we must compute are those of the beliefs: ðbi (yi ) and ðbα (y α ). This corresponds to the backward pass through Figure 2 (E). Consider the simple case where J is L2 distance from (12): the variable belief adjoint is ðbi (yi ) = 2(bi (yi ) − b∗i (yi )) and trivially ðbα (y α ) = 0. If J is annealed risk from (11), we compute ðbi (yi ) by applying backpropagation recursively to our algorithm for J from §4.1. This sub-algorithm defi"
Q15-1035,N12-1015,0,0.0385781,"des empirical risk, Domke (2011) refers to it as “learning with truncated message passing.” Our primary contribution is the application of this approximation-aware learning method in the parsing setting, for which the graphical model involves a global constraint. Smith and Eisner (2008) previously showed how to run BP in this setting (by calling the inside-outside algorithm as a subroutine). We must backpropagate the downstream objective 1 For perceptron training, utilizing inexact inference as a drop-in replacement for exact inference can badly mislead the learner (Kulesza and Pereira, 2008; Huang et al., 2012). 489 Transactions of the Association for Computational Linguistics, vol. 3, pp. 489–501, 2015. Action Editor: Sebastian Riedel. Submission batch: 4/2015; Published 8/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. function through their algorithm so that we can follow its gradient. We carefully define an empirical risk objective function (`a la ERMA) to be smooth and differentiable, yet equivalent to accuracy of the minimum Bayes risk (MBR) parse in the limit. Finding this difficult to optimize, we introduce a new simpler objective function based"
Q15-1035,W01-1812,0,0.332468,"are simple. In the next subsection, we explain how to backpropagate through the insideoutside algorithm. Though we focus here on projective dependency parsing, our techniques are also applicable to non-projective parsing and the T REE factor; we leave this to future work. 5.4 Backprop of Hypergraph Inside-Outside Both the annealed risk loss function (§4.1) and the computation of messages from the PT REE factor (§5.3) use the inside-outside algorithm for dependency parsing. Here we describe inside-outside and the accompanying backpropagation algorithm over a hypergraph. This general treatment (Klein and Manning, 2001; Li and Eisner, 2009) enables our method to be applied to other tasks such as constituency parsing, HMM forward-backward, and hierarchical machine translation. In the case of dependency parsing, the structure of the hypergraph is given by the dynamic programming algorithm of Eisner (1996). For the forward pass of the inside-outside module, the input variables are the hyperedge weights we ∀e and the outputs are the marginal probabilities pw (i)∀i of each node i in the hypergraph. The latter are a function of the inside βi and outside αj probabilities. We initialize αroot = 1. X Y βi = we βj (1"
Q15-1035,P10-1001,0,0.822359,"he true data distribution over sentence/parse pairs (X, Y ): θ ∗ = argminθ E[`(hθ (X), Y )] (9) Since the true data distribution is unknown, we substitute the expected loss over the training sample, and regularize our objective in order to reduce sampling variance. Specifically, we aim to minimize the regularized empirical risk, given by (8) with J(θ; x(d) , y (d) ) set to `(hθ (x(d) ), y (d) ). Note that 5 How slow is exact inference for dependency parsing? For certain choices of higher-order factors, polynomial time is possible via dynamic programming (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). However, BP will typically be asymptotically faster (for a fixed number of iterations) and faster in practice. In some other settings, exact inference is NPhard. In particular, non-projective parsing becomes NP-hard with even second-order factors (McDonald and Pereira, 2006). BP can handle this case in polynomial time by replacing the PT REE factor with a T REE factor that allows edges to cross. 6 θ is initialized to 0 when not otherwise specified. 492 this loss function would not be differentiable—a key issue we will take up below. This is the “ERMA” method of Stoyanov and Eisner (2012). We"
Q15-1035,D09-1005,1,0.946816,"ubsection, we explain how to backpropagate through the insideoutside algorithm. Though we focus here on projective dependency parsing, our techniques are also applicable to non-projective parsing and the T REE factor; we leave this to future work. 5.4 Backprop of Hypergraph Inside-Outside Both the annealed risk loss function (§4.1) and the computation of messages from the PT REE factor (§5.3) use the inside-outside algorithm for dependency parsing. Here we describe inside-outside and the accompanying backpropagation algorithm over a hypergraph. This general treatment (Klein and Manning, 2001; Li and Eisner, 2009) enables our method to be applied to other tasks such as constituency parsing, HMM forward-backward, and hierarchical machine translation. In the case of dependency parsing, the structure of the hypergraph is given by the dynamic programming algorithm of Eisner (1996). For the forward pass of the inside-outside module, the input variables are the hyperedge weights we ∀e and the outputs are the marginal probabilities pw (i)∀i of each node i in the hypergraph. The latter are a function of the inside βi and outside αj probabilities. We initialize αroot = 1. X Y βi = we βj (13) e∈I(i) αj = X j∈T ("
Q15-1035,J93-2004,0,0.0515215,"etween the approximate marginals and the “true” marginals from the gold data. The goal of this work is to account for the approximations made by a system rooted in structured belief propagation. Taking such approximations into account during training enables us to improve the speed and accuracy of inference at test time. We compare our training method with the standard approach of conditional log-likelihood (CLL) training. We evaluate our parser on 19 languages from the CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks as well as the English Penn Treebank (Marcus et al., 1993). On English, the resulting parser obtains higher accuracy with fewer iterations of BP than CLL. On the CoNLL languages, we find that on average it yields higher accuracy parsers than CLL, particularly when limited to few BP iterations. 2 Dependency Parsing by Belief Propagation This section describes the parser that we will train. Model A factor graph (Frey et al., 1997; Kschischang et al., 2001) defines the factorization of a probability distribution over a set of variables {Y1 , Y2 , . . .}. It is a bipartite graph between variables Yi and factors α. Edges connect each factor α to a subset"
Q15-1035,P09-1039,0,0.136933,", we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train a first-order model with CLL and for each token prune any parents for which the marginal probability is less than 0.0001 times the maximum parent marginal for that token. On a per-token basis, we further restrict to the ten parents with highest marginal probability as in Martins et al. (2009) (but we avoid pruning the fully right-branching tree, so that some parse always exists).8 This lets us simplify the factor graph, removing variables yi corresponding to pruned edges and specializing their factors to assume yi = OFF. We train the full model’s parameters to work well on this pruned graph. Data We consider 19 languages from the CoNLL2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks. We also convert the English Penn Treebank (PTB) (Marcus et al., 1993) to dependencies using the head rules from Yamada and Matsumoto (2003) (PTB-YM). We evaluate unlabe"
Q15-1035,D10-1004,0,0.0360135,"f infinite training data drawn from the model being used (Wainwright, 2006). Despite this, the surrogate likelihood objective is commonly used to train CRFs. CLL and approximation-aware training are not mutually exclusive. Training a standard factor graph with ERMA and a log-likelihood objective recovers CLL exactly (Stoyanov et al., 2011). 496 Experiments 7.1 Setup Features As the focus of this work is on a novel approach to training, we look to prior work for model and feature design (§2). We add O(n3 ) second-order grandparent and arbitrary-sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work. Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by"
Q15-1035,P13-2109,0,0.227989,"Missing"
Q15-1035,E06-1011,0,0.301204,"ically, we aim to minimize the regularized empirical risk, given by (8) with J(θ; x(d) , y (d) ) set to `(hθ (x(d) ), y (d) ). Note that 5 How slow is exact inference for dependency parsing? For certain choices of higher-order factors, polynomial time is possible via dynamic programming (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). However, BP will typically be asymptotically faster (for a fixed number of iterations) and faster in practice. In some other settings, exact inference is NPhard. In particular, non-projective parsing becomes NP-hard with even second-order factors (McDonald and Pereira, 2006). BP can handle this case in polynomial time by replacing the PT REE factor with a T REE factor that allows edges to cross. 6 θ is initialized to 0 when not otherwise specified. 492 this loss function would not be differentiable—a key issue we will take up below. This is the “ERMA” method of Stoyanov and Eisner (2012). We will also consider simpler choices of J—akin to the loss functions used by Domke (2011). Gradient Computation To compute the gradient ∇θ J(θ; x, y ∗ ) of the loss on a single sentence (x, y ∗ ) = (x(d) , y (d) ), we apply automatic differentiation (AD) in the reverse mode (Gr"
Q15-1035,P05-1012,0,0.721936,"r aim is to minimize expected loss on the true data distribution over sentence/parse pairs (X, Y ): θ ∗ = argminθ E[`(hθ (X), Y )] (9) Since the true data distribution is unknown, we substitute the expected loss over the training sample, and regularize our objective in order to reduce sampling variance. Specifically, we aim to minimize the regularized empirical risk, given by (8) with J(θ; x(d) , y (d) ) set to `(hθ (x(d) ), y (d) ). Note that 5 How slow is exact inference for dependency parsing? For certain choices of higher-order factors, polynomial time is possible via dynamic programming (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). However, BP will typically be asymptotically faster (for a fixed number of iterations) and faster in practice. In some other settings, exact inference is NPhard. In particular, non-projective parsing becomes NP-hard with even second-order factors (McDonald and Pereira, 2006). BP can handle this case in polynomial time by replacing the PT REE factor with a T REE factor that allows edges to cross. 6 θ is initialized to 0 when not otherwise specified. 492 this loss function would not be differentiable—a key issue we will take up below. This is the “ERMA”"
Q15-1035,C12-1122,0,0.416682,"Missing"
Q15-1035,petrov-etal-2012-universal,0,0.0136683,"MA and a log-likelihood objective recovers CLL exactly (Stoyanov et al., 2011). 496 Experiments 7.1 Setup Features As the focus of this work is on a novel approach to training, we look to prior work for model and feature design (§2). We add O(n3 ) second-order grandparent and arbitrary-sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work. Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train a first-order model with CLL"
Q15-1035,N10-1117,0,0.204886,"even under the assumption of infinite training data drawn from the model being used (Wainwright, 2006). Despite this, the surrogate likelihood objective is commonly used to train CRFs. CLL and approximation-aware training are not mutually exclusive. Training a standard factor graph with ERMA and a log-likelihood objective recovers CLL exactly (Stoyanov et al., 2011). 496 Experiments 7.1 Setup Features As the focus of this work is on a novel approach to training, we look to prior work for model and feature design (§2). We add O(n3 ) second-order grandparent and arbitrary-sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work. Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner a"
Q15-1035,N12-1054,0,0.0629473,"train CRFs. CLL and approximation-aware training are not mutually exclusive. Training a standard factor graph with ERMA and a log-likelihood objective recovers CLL exactly (Stoyanov et al., 2011). 496 Experiments 7.1 Setup Features As the focus of this work is on a novel approach to training, we look to prior work for model and feature design (§2). We add O(n3 ) second-order grandparent and arbitrary-sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work. Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed"
Q15-1035,P06-2101,1,0.876998,"pergraph (§5.4) for use in two modules: the softened decoder (§5.1) and computation of messages from the PT REE factor (§5.3). This allows us to go beyond Stoyanov et al. (2011) and train structured BP in an approximation-aware and lossaware fashion. 4 Differentiable Objective Functions 4.1 Annealed Risk Minimizing the test-time loss is the appropriate goal for training an approximate system like ours. That loss is estimated by the empirical risk on a large amount of in-domain supervised training data. Alas, this risk is nonconvex and piecewise constant, so we turn to deterministic annealing (Smith and Eisner, 2006) and clever initialization. Directed dependency error, `(hθ (x), y ∗ ), is not differentiable due to the argmax in the decoder hθ . So we redefine J(θ; x, y ∗ ) to be a new differentiable loss function, 1/T the annealed risk Rθ (x, y ∗ ), which approaches the loss `(hθ (x), y ∗ ) as the temperature T → 0. Our first step is to define a distribution over parses, which takes the marginals pθ (yi = ON |x) as input, or in practice, their BP approximations bi (ON): P  1/T pθ (yi =ON |x) qθ (ˆ y |x) ∝ exp (10) i:ˆ yi =ON T 493 (E) Decode and Loss (E.3) Expected Recall J(θ; x, y ∗ ) = (E.2) Inside-O"
Q15-1035,D08-1016,1,0.0656886,"ts to dependency parsing accuracy have been driven by higher-order features. Such a feature can look beyond just the parent and child words connected by a single edge to also consider siblings, grandparents, etc. By including increasingly global information, these features provide more information for the parser—but they also complicate inference. The resulting higher-order parsers depend on approximate inference and decoding procedures, which may prevent them from predicting the best parse. For example, consider the dependency parser we will train in this paper, which is based on the work of Smith and Eisner (2008). Ostensibly, this parser finds the minimum Bayes risk (MBR) parse under a probability distribution defined by a higher-order dependency parsing model. In reality, it achieves O(n3 tmax ) runtime by relying on three approximations during inference: (1) variational inference by loopy belief propagation (BP) on a factor graph, (2) truncating inference after tmax iterations prior to convergence, and (3) a first-order pruning model to limit the number of edges considered in the higherorder model. Such parsers are traditionally trained as if the inference had been exact.1 In contrast, we train the"
Q15-1035,N12-1013,1,0.95271,"uts the parse with maximum expected recall—but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by backpropagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as others have done for loopy CRFs (Domke, 2010; Stoyanov et al., 2011; Domke, 2011; Stoyanov and Eisner, 2012). The resulting parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood. 1 Introduction Recent improvements to dependency parsing accuracy have been driven by higher-order features. Such a feature can look beyond just the parent and child words connected by a single edge to also consider siblings, grandparents, etc. By including increasingly global information, these features provide more information for the parser—but they also complicate inference. The resulting higher-order parsers depend on approximate inference and decoding"
Q15-1035,W03-3023,0,0.345475,"Missing"
Q15-1035,D07-1096,0,\N,Missing
Q16-1035,P15-1119,0,0.103576,"lthough they have no close real neighbors. 7 An Experiment We now illustrate the use of GD by studying how expanding the set of available treebanks can improve a simple NLP method, related to Figure 4. 7.1 Single-source transfer Dependency parsing of low-resource languages has been intensively studied for years. A simple method is called “single-source transfer”: parsing a target language T with a parser that was trained on a source language S, where the two languages are syntactically similar. Such single-source transfer parsers (Ganchev et al., 2010; McDonald et al., 2011; Ma and Xia, 2014; Guo et al., 2015; Duong et al., 2015; Rasooli and Collins, 2015) are not state-ofthe-art, but they have shown substantial improvements over fully unsupervised grammar induction systems (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2013). It is permitted for S and T to have different vocabularies. The S parser can nonetheless parse T (as in Figure 4)—provided that it is a “delexicalized” parser that only cares about the POS tags of the input words. In this case, we require only that the target sentences have already been POS tagged using the same tagset as S: in our case, the UD tagset."
Q16-1035,Q15-1031,1,0.363605,"tion method, for example, will discover structure in new images. The limited resources even make it challenging to develop methods that handle new languages by unsupervised, semi-supervised, or transfer learning. Some such projects evaluate their methods on new sentences of the same languages that were used to develop the methods in the first place—which leaves one worried that the methods may be inadvertently tuned to the development languages and may not be able to discover correct structure in other languages. Other projects take care to hold out languages for evaluation (Spitkovsky, 2013; Cotterell et al., 2015), but then are left with only a few development languages on which to experiment with different unsupervised methods and their hyperparameters. If we had many languages, then we could develop better unsupervised language learners. Even better, we could treat linguistic structure discovery as a supervised learning problem. That is, we could train a system to extract features from the surface of a language that are predictive of its deeper structure. Principles & Parameters theory (Chomsky, 1981) conjectures that such features exist and that the juvenile human brain is adapted to extract them. O"
Q16-1035,P11-2067,0,0.0296746,"by first generating synthetic (utterance, logical form) pairs and then asking human annotators to paraphrase the synthetic utterances into more natural human language. In speech recognition, morphology-based “vocabulary expansion” creates synthetic word forms (Rasooli et al., 2014; Varjokallio and Klakow, 2016). Machine translation researchers have often tried to automatically preprocess parse trees of a source language to more closely resemble those of the target language, using either hand-crafted or automatically extracted rules (Dorr et al., 2002; Collins et al., 2005, etc.; see review by Howlett and Dras, 2011). 3 Synthetic Language Generation A treebank is a corpus of parsed sentences of some language. We propose to derive each synthetic treebank from some real treebank. By manipulating the existing parse trees, we obtain a useful corpus for our synthetic language—a corpus that is already tagged, parsed, and partitioned into training/development/test sets. Additional data in the synthetic language can be obtained, if desired, by automatically parsing additional real-language sentences and manipulating these trees in the same way. 3.1 Method We begin with the Universal Dependencies collection versio"
Q16-1035,dorr-etal-2002-duster,0,0.316941,"(2015) obtain data to train semantic parsers in a new domain by first generating synthetic (utterance, logical form) pairs and then asking human annotators to paraphrase the synthetic utterances into more natural human language. In speech recognition, morphology-based “vocabulary expansion” creates synthetic word forms (Rasooli et al., 2014; Varjokallio and Klakow, 2016). Machine translation researchers have often tried to automatically preprocess parse trees of a source language to more closely resemble those of the target language, using either hand-crafted or automatically extracted rules (Dorr et al., 2002; Collins et al., 2005, etc.; see review by Howlett and Dras, 2011). 3 Synthetic Language Generation A treebank is a corpus of parsed sentences of some language. We propose to derive each synthetic treebank from some real treebank. By manipulating the existing parse trees, we obtain a useful corpus for our synthetic language—a corpus that is already tagged, parsed, and partitioned into training/development/test sets. Additional data in the synthetic language can be obtained, if desired, by automatically parsing additional real-language sentences and manipulating these trees in the same way. 3."
Q16-1035,P04-1061,0,0.038921,"ed to Figure 4. 7.1 Single-source transfer Dependency parsing of low-resource languages has been intensively studied for years. A simple method is called “single-source transfer”: parsing a target language T with a parser that was trained on a source language S, where the two languages are syntactically similar. Such single-source transfer parsers (Ganchev et al., 2010; McDonald et al., 2011; Ma and Xia, 2014; Guo et al., 2015; Duong et al., 2015; Rasooli and Collins, 2015) are not state-ofthe-art, but they have shown substantial improvements over fully unsupervised grammar induction systems (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2013). It is permitted for S and T to have different vocabularies. The S parser can nonetheless parse T (as in Figure 4)—provided that it is a “delexicalized” parser that only cares about the POS tags of the input words. In this case, we require only that the target sentences have already been POS tagged using the same tagset as S: in our case, the UD tagset. 7.2 Experimental Setup We evaluate single-source transfer when the pool of m source languages consists of n real UD languages, plus m − n synthetic GD languages derived by “remixing” just these"
Q16-1035,P14-1126,0,0.0165983,"is small sample, although they have no close real neighbors. 7 An Experiment We now illustrate the use of GD by studying how expanding the set of available treebanks can improve a simple NLP method, related to Figure 4. 7.1 Single-source transfer Dependency parsing of low-resource languages has been intensively studied for years. A simple method is called “single-source transfer”: parsing a target language T with a parser that was trained on a source language S, where the two languages are syntactically similar. Such single-source transfer parsers (Ganchev et al., 2010; McDonald et al., 2011; Ma and Xia, 2014; Guo et al., 2015; Duong et al., 2015; Rasooli and Collins, 2015) are not state-ofthe-art, but they have shown substantial improvements over fully unsupervised grammar induction systems (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2013). It is permitted for S and T to have different vocabularies. The S parser can nonetheless parse T (as in Figure 4)—provided that it is a “delexicalized” parser that only cares about the POS tags of the input words. In this case, we require only that the target sentences have already been POS tagged using the same tagset as S: in our cas"
Q16-1035,D10-1120,0,0.0145712,"istinguished by case according to their role in the clause structure; wh-words continue to ccommand gaps; different verbs (e.g., transitive vs. intransitive) continue to be associated with different subcategorization frames; and so on. These im493 portant properties would not be captured by a simple context-free model of dependency trees, which is why we modify real sentences rather than generating new sentences from such a model. In addition, our method obviously preserves the basic context-free properties, such as the fact that verbs typically subcategorize for one or two nominal arguments (Naseem et al., 2010). Second, by drawing on real superstrate languages, we ensure that our synthetic languages use plausible word orders. For example, if RV is a V2 language that favors SVO word order but also allows OVS, then S 0 will match these proportions. Similarly, S 0 will place adverbs in reasonable positions with respect to the verb. We note, however, that our synthetic languages might violate some typological universals or typological tendencies. For example, RV might prescribe head-initial verb orderings while RN prescribes head-final noun orderings, yielding an unusual language. Worse, we could synthe"
Q16-1035,E14-2005,0,0.039303,"le(S − {T }) 5: L0 ← random.shuffle(S 0 ) 6: for n = 1 to |L |do 7: L00 ← a filtered version of L0 that excludes languages with substrates or superstrates outside {L1 , . . . , Ln } 8: for n0 = 1 to |L00 |do 9: P ← {L1 , . . . , Ln , L001 , . . . , L00n0 } 10: m ← |P| 11: Dsup ← Dsup ∪ {(n, m, UASsup (P, T ))} 12: Dunsup ← Dunsup ∪ {(n, m, UASunsup (P, T ))} 13: B return (Dsup , Dunsup ) Experiment with Noisy Tags Table 4 repeats the single-source transfer experiment using noisy automatic POS tags for T for both parser input and unsupervised selection. We obtained the tags using RDRPOSTagger (Nguyen et al., 2014) trained on just 100 gold-tagged sentences (the same set used for supervised selection). The low tagging accuracy does considerably degrade UAS and muddies the usefulness of the synthetic sources. tag target bg nl et la proiel da en grc grc proiel fi got Avg. 78.33 71.70 72.88 71.83 78.04 77.33 68.80 72.93 65.65 76.66 73.42 unsupervised real +synth 53.24 55.08 39.40 38.99 45.19 54.81 37.25 38.26 47.98 43.40 48.29 44.40 32.15 32.15 42.46 41.39 29.59 28.81 44.77 44.05 42.03 42.13 (weakly) superv. real +synth 53.24 53.24 42.42 42.75 56.07 55.09 37.25 38.10 47.98 45.89 48.29 48.15 33.52 34.36 43.4"
Q16-1035,P05-1013,0,0.0372639,"otherwise alter the structure of the tree (Dorr, 1993). These options may produce implausible languages. To mitigate this, we could filter or reweight our sample of synthetic languages—via rejection sampling or importance sampling—so that they are distributed more like real languages, as measured by their parsabilities, dependency lengths, and estimated WALS features (Dryer and Haspelmath, 2013). 2. Currently, our reordering method only generates projective dependency trees. We should extend it to allow non-projective trees as well—for example, by pseudo-projectivizing the substrate treebank (Nivre and Nilsson, 2005) and then deprojectivizing it after reordering. 3. The treebanks of real languages can typically be augmented with larger unannotated corpora in those languages (Majliˇs, 2011), which can be used to train word embeddings and language models, and can also be used for self-training and bootstrapping methods. We plan to release comparable unannotated corpora for our synthetic languages, by au9 Our current handling of punctuation produces unnatural results, and not merely because we treat all tokens with tag PUNCT as interchangeable. Proper handling of punctuation and capitalization would require"
Q16-1035,D15-1039,0,0.0236221,"ment score on its dev sentences after training on its train sentences. T is the percentage of GD tokens that are touched by reordering (namely N, V, and their dependents). R ∈ [0, 1] measures the freeness of the language’s word order, as the conditional cross-entropy of our trained ordering model pθ relH(p,p ˜ θ) ative to that of a uniform distribution: R = H( p,p ˜ unif ) = meanx [− log2 pθ (π ∗ (x)|x)] meanx [− log2 1/n(x)!] , where x ranges over all N and V tokens in the dev sentences, n(x) is 1 + the number of dependents of x, and π ∗ (x) is the observed ordering at x. parser (Rasooli and Tetreault, 2015), a fast arc-eager transition-based projective dependency parser, with beam size of 8. We train only delexicalized parsers, whose input is the sequence of POS tags. Parsing accuracy is evaluated by the unlabeled attachment score (UAS), that is, the fraction of word tokens in held-out (dev) data that are assigned their correct parent. For language modeling, we train simple trigram backoff language models with add-1 smoothing, and we measure predictive accuracy as the perplexity of held-out (dev) data. Figures 2–3 show how the parsability and perplexity of a real training language usually get wo"
Q16-1035,P14-1127,0,0.022817,"prehension questions and their answers, based on a large set of newssummarization pairs, for training machine readers. Serban et al. (2016) used RNNs to generate 30 million factoid questions about Freebase, with answers, for training question-answering systems. Wang et al. 492 (2015) obtain data to train semantic parsers in a new domain by first generating synthetic (utterance, logical form) pairs and then asking human annotators to paraphrase the synthetic utterances into more natural human language. In speech recognition, morphology-based “vocabulary expansion” creates synthetic word forms (Rasooli et al., 2014; Varjokallio and Klakow, 2016). Machine translation researchers have often tried to automatically preprocess parse trees of a source language to more closely resemble those of the target language, using either hand-crafted or automatically extracted rules (Dorr et al., 2002; Collins et al., 2005, etc.; see review by Howlett and Dras, 2011). 3 Synthetic Language Generation A treebank is a corpus of parsed sentences of some language. We propose to derive each synthetic treebank from some real treebank. By manipulating the existing parse trees, we obtain a useful corpus for our synthetic languag"
Q16-1035,N10-1018,0,0.0104636,"rm of virtual training examples. While datasets have grown in recent years, so have models: e.g., neural networks have many parameters to train. Thus, it is still common to create synthetic training examples—often by adding noise to real inputs or otherwise transforming them in ways that are expected to preserve their labels. Domains where it is easy to exploit these invariances include image recognition (Simard et al., 2003; Krizhevsky et al., 2012), speech recognition (Jaitly and Hinton, 2013; Cui et al., 2015), information retrieval (Vilares et al., 2011), and grammatical error correction (Rozovskaya and Roth, 2010). Synthetic datasets have also arisen recently for semantic tasks in natural language processing. bAbI is a dataset of facts, questions, and answers, generated by random simulation, for training machines to do simple logic (Weston et al., 2016). Hermann et al. (2015) generate reading comprehension questions and their answers, based on a large set of newssummarization pairs, for training machine readers. Serban et al. (2016) used RNNs to generate 30 million factoid questions about Freebase, with answers, for training question-answering systems. Wang et al. 492 (2015) obtain data to train semant"
Q16-1035,P16-1056,0,0.0612283,"Missing"
Q16-1035,P11-2120,0,0.0153611,"ous parsers, many of which achieve similar results on the training sentences of T . Furthermore, in the UD treebanks, the test sentences of T are sometimes drawn from a different distribution than the training sentences.) Unsupervised selection selects the S whose training sentences had the best “coverage” of the POS tag sequences in the actual data from T that we aim to parse. More precisely, we choose the S that maximizes pS (tag sequences from T )—in other words, the maximum-likelihood S—where pS is our trigram language model for the tag sequences of S. This approach is loosely inspired by Søgaard (2011). 7.3 Results Our most complete visualization is Figure 5, which we like to call the “kite graph” for its appearance. We plot the UAS on the development treebank of T as a function of n, m, and the selection method. As Appendix A details, each point on this graph is actually an average over 10,000 experiments that make random choices of T (from the UD development languages), the n real languages (from the UD training languages), and the m − n synthetic languages (from the GD languages derived from the n real lan8 The Yara parser can only produce projective parses. It attempts to parse all test"
Q16-1035,D13-1204,0,0.0725322,"ency parsing of low-resource languages has been intensively studied for years. A simple method is called “single-source transfer”: parsing a target language T with a parser that was trained on a source language S, where the two languages are syntactically similar. Such single-source transfer parsers (Ganchev et al., 2010; McDonald et al., 2011; Ma and Xia, 2014; Guo et al., 2015; Duong et al., 2015; Rasooli and Collins, 2015) are not state-ofthe-art, but they have shown substantial improvements over fully unsupervised grammar induction systems (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2013). It is permitted for S and T to have different vocabularies. The S parser can nonetheless parse T (as in Figure 4)—provided that it is a “delexicalized” parser that only cares about the POS tags of the input words. In this case, we require only that the target sentences have already been POS tagged using the same tagset as S: in our case, the UD tagset. 7.2 Experimental Setup We evaluate single-source transfer when the pool of m source languages consists of n real UD languages, plus m − n synthetic GD languages derived by “remixing” just these real languages.7 We try various values of n and m"
Q16-1035,P16-2029,0,0.0171315,"nd their answers, based on a large set of newssummarization pairs, for training machine readers. Serban et al. (2016) used RNNs to generate 30 million factoid questions about Freebase, with answers, for training question-answering systems. Wang et al. 492 (2015) obtain data to train semantic parsers in a new domain by first generating synthetic (utterance, logical form) pairs and then asking human annotators to paraphrase the synthetic utterances into more natural human language. In speech recognition, morphology-based “vocabulary expansion” creates synthetic word forms (Rasooli et al., 2014; Varjokallio and Klakow, 2016). Machine translation researchers have often tried to automatically preprocess parse trees of a source language to more closely resemble those of the target language, using either hand-crafted or automatically extracted rules (Dorr et al., 2002; Collins et al., 2005, etc.; see review by Howlett and Dras, 2011). 3 Synthetic Language Generation A treebank is a corpus of parsed sentences of some language. We propose to derive each synthetic treebank from some real treebank. By manipulating the existing parse trees, we obtain a useful corpus for our synthetic language—a corpus that is already tagg"
Q16-1035,P15-1129,0,0.0478821,"Missing"
Q17-1011,N09-1067,0,0.0841771,"Missing"
Q17-1011,Q16-1031,0,0.330061,"ionality numbers in [0, 1] as fine-grained and robust typological descriptors. We believe that these directionalities could also be used to help define an initializer, prior, or regularizer for tasks like grammar induction or syntax-based machine translation. Finally, the vector of directionalities—or the feature vector that our method extracts in order to predict the directionalities—can be regarded as a language embedding computed from the POStagged corpus. This language embedding may be useful as an input to multilingual NLP systems, such as the cross-linguistic neural dependency parser of Ammar et al. (2016). In fact, some multilingual NLP systems already condition on typological properties looked up in the World Atlas of Language Structures, or WALS (Dryer and Haspelmath, 2013), as 147 Transactions of the Association for Computational Linguistics, vol. 5, pp. 147–161, 2017. Action Editor: Mark Steedman. Submission batch: 11/2016; Revision batch: 2/2017; Published 6/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Typology Verb-Object (English) Example dobj She gave me a raise dobj Object-Verb (Hindi) She me a raise gave vah mujhe ek uthaane diya Prep"
Q17-1011,W09-0106,0,0.0724462,"at left. We ask whether the average expected loss on these 17 real target languages is reduced by augmenting the training pool of 20 UD languages with +20*21*21 GD languages. For completeness, we extend the table with the cross-validation results on the training pool. The “Avg.” lines report the average of 17 test or 37 training+testing languages. We mark both “+GD” averages with “*” as they are significantly better than their “UD” counterparts (paired permutation test by language, p < 0.05). 8 Related Work Typological properties can usefully boost the performance of cross-linguistic systems (Bender, 2009; O’Horan et al., 2016). These systems mainly aim to annotate low-resource languages with help from models trained on similar high-resource languages. Naseem et al. (2012) introduce a “selective sharing” technique for generative parsing, in which a Subject-Verb language will use parameters shared with other Subject-Verb languages. T¨ackstr¨om et al. (2013) and Zhang and Barzilay (2015) extend this idea to discriminative parsing and gain further improvements by conjoining regular parsing features with typological features. The cross-linguistic neural parser of Ammar et al. (2016) conditions on"
Q17-1011,W14-4012,0,0.031471,"Missing"
Q17-1011,W15-2112,0,0.0134869,"yesian approach to discover new universals. Georgi et al. (2010) impute missing properties of a language, not by using universals, but by backing off to the language’s typological cluster. Murawaki (2015) use WALS to help recover the evolutionary tree of human languages; Daum´e III (2009) considers the geographic distribution of WALS properties. Attempts at automatic typological classification are relatively recent. Lewis and Xia (2008) predict typological properties from induced trees, but guess those trees from aligned bitexts, not by monolingual grammar induction as in §7.6. Liu (2010) and Futrell et al. (2015) show that the directionality of (gold) dependencies is indicative of “basic” word order and freeness of word order. Those papers predict typological properties from trees that are automatically (noisily) annotated or manually (expensively) annotated. An alternative is to predict the typology directly from raw or POS-tagged text, as we do. Saha Roy et al. (2014) first explored this idea, building a system that correctly predicts adposition typology on 19/23 languages with only word cooccurrence statistics. Zhang et al. (2016) evaluate semi-supervised POS tagging by asking whether the induced t"
Q17-1011,C10-1044,0,0.113709,"heir ordering in a corpus. Moving from engineering to science, linguists seek typological universals of human language (Greenberg, 1963; Croft, 2002; Song, 2014; Hawkins, 2014), e.g., “languages with dominant Verb-Subject-Object order are always prepositional.” Dryer and Haspelmath (2013) characterize 2679 world languages with 192 typological properties. Their WALS database can supply features to NLP systems (see previous paragraph) or gold standard labels for typological classifiers. Daum´e III and Campbell (2007) take WALS as input and propose a Bayesian approach to discover new universals. Georgi et al. (2010) impute missing properties of a language, not by using universals, but by backing off to the language’s typological cluster. Murawaki (2015) use WALS to help recover the evolutionary tree of human languages; Daum´e III (2009) considers the geographic distribution of WALS properties. Attempts at automatic typological classification are relatively recent. Lewis and Xia (2008) predict typological properties from induced trees, but guess those trees from aligned bitexts, not by monolingual grammar induction as in §7.6. Liu (2010) and Futrell et al. (2015) show that the directionality of (gold) dep"
Q17-1011,P04-1061,0,0.26854,"r prediction approach is supervised like ours, although developed separately and trained on different data. They more simply predict 6 binaryvalued WALS properties, using 6 independent binary classifiers based on POS bigram and trigrams. Our task is rather close to grammar induction, which likewise predicts a set of real numbers giving the relative probabilities of competing syntactic configurations. Most previous work on grammar induction begins with maximum likelihood estimation of some generative model—such as a PCFG (Lari and Young, 1990; Carroll and Charniak, 1992) or dependency grammar (Klein and Manning, 2004)— 158 though it may add linguistically-informed inductive bias (Ganchev et al., 2010; Naseem et al., 2010). Most such methods use local search and must wrestle with local optima (Spitkovsky et al., 2013). Finegrained typological classification might supplement this approach, by cutting through the initial combinatorial challenge of establishing the basic wordorder properties of the language. In this paper we only quantify the directionality of each relation type, ignoring how tokens of these relations interact locally to give coherent parse trees. Grammar induction methods like EM could natura"
Q17-1011,I08-2093,0,0.0905868,"can supply features to NLP systems (see previous paragraph) or gold standard labels for typological classifiers. Daum´e III and Campbell (2007) take WALS as input and propose a Bayesian approach to discover new universals. Georgi et al. (2010) impute missing properties of a language, not by using universals, but by backing off to the language’s typological cluster. Murawaki (2015) use WALS to help recover the evolutionary tree of human languages; Daum´e III (2009) considers the geographic distribution of WALS properties. Attempts at automatic typological classification are relatively recent. Lewis and Xia (2008) predict typological properties from induced trees, but guess those trees from aligned bitexts, not by monolingual grammar induction as in §7.6. Liu (2010) and Futrell et al. (2015) show that the directionality of (gold) dependencies is indicative of “basic” word order and freeness of word order. Those papers predict typological properties from trees that are automatically (noisily) annotated or manually (expensively) annotated. An alternative is to predict the typology directly from raw or POS-tagged text, as we do. Saha Roy et al. (2014) first explored this idea, building a system that corre"
Q17-1011,P13-1028,0,0.406816,"Missing"
Q17-1011,N15-1036,0,0.0193255,"002; Song, 2014; Hawkins, 2014), e.g., “languages with dominant Verb-Subject-Object order are always prepositional.” Dryer and Haspelmath (2013) characterize 2679 world languages with 192 typological properties. Their WALS database can supply features to NLP systems (see previous paragraph) or gold standard labels for typological classifiers. Daum´e III and Campbell (2007) take WALS as input and propose a Bayesian approach to discover new universals. Georgi et al. (2010) impute missing properties of a language, not by using universals, but by backing off to the language’s typological cluster. Murawaki (2015) use WALS to help recover the evolutionary tree of human languages; Daum´e III (2009) considers the geographic distribution of WALS properties. Attempts at automatic typological classification are relatively recent. Lewis and Xia (2008) predict typological properties from induced trees, but guess those trees from aligned bitexts, not by monolingual grammar induction as in §7.6. Liu (2010) and Futrell et al. (2015) show that the directionality of (gold) dependencies is indicative of “basic” word order and freeness of word order. Those papers predict typological properties from trees that are au"
Q17-1011,D10-1120,0,0.627874,"od for each edge, we label the edge deterministically with a POS pair such as r = (parent = NOUN, child = ADJ). Thus, we will attempt to predict the directionality of each POS-pair relation type. For comparison, we retrain our supervised system to do the same thing. For the grammar induction system, we try the implementation of DMV with stop-probability estimation by Mareˇcek and Straka (2013), which is a common baseline for grammar induction (Le and Zuidema, 2015) because it is language-independent, reasonably accurate, fast, and convenient to use. We also try the grammar induction system of Naseem et al. (2010), which is the state-of-the-art system on UD (Noji et al., 2016). Naseem et al. (2010)’s method, like ours, has prior knowledge of what typical human languages look like. Table 5 shows the results. Compared to Mareˇcek and Straka (2013), Naseem et al. (2010) gets only a small (insignificant) improvement—whereas our “UD” system halves the loss, and the “+GD” system halves it again. Even our baseline systems are significantly more accurate than the grammar induction systems, showing the effectiveness of casting the problem as supervised prediction. 7.7 Fine-grained analysis Beyond reporting the"
Q17-1011,P12-1066,0,0.439476,"languages. For completeness, we extend the table with the cross-validation results on the training pool. The “Avg.” lines report the average of 17 test or 37 training+testing languages. We mark both “+GD” averages with “*” as they are significantly better than their “UD” counterparts (paired permutation test by language, p < 0.05). 8 Related Work Typological properties can usefully boost the performance of cross-linguistic systems (Bender, 2009; O’Horan et al., 2016). These systems mainly aim to annotate low-resource languages with help from models trained on similar high-resource languages. Naseem et al. (2012) introduce a “selective sharing” technique for generative parsing, in which a Subject-Verb language will use parameters shared with other Subject-Verb languages. T¨ackstr¨om et al. (2013) and Zhang and Barzilay (2015) extend this idea to discriminative parsing and gain further improvements by conjoining regular parsing features with typological features. The cross-linguistic neural parser of Ammar et al. (2016) conditions on typological features by supplying a “language embedding” as input. Zhang et al. (2012) use typological properties to convert language-specific POS tags to UD POS tags, bas"
Q17-1011,E14-2005,0,0.164598,"Missing"
Q17-1011,D16-1004,0,0.244901,"ers in [0, 1] as fine-grained and robust typological descriptors. We believe that these directionalities could also be used to help define an initializer, prior, or regularizer for tasks like grammar induction or syntax-based machine translation. Finally, the vector of directionalities—or the feature vector that our method extracts in order to predict the directionalities—can be regarded as a language embedding computed from the POStagged corpus. This language embedding may be useful as an input to multilingual NLP systems, such as the cross-linguistic neural dependency parser of Ammar et al. (2016). In fact, some multilingual NLP systems already condition on typological properties looked up in the World Atlas of Language Structures, or WALS (Dryer and Haspelmath, 2013), as 147 Transactions of the Association for Computational Linguistics, vol. 5, pp. 147–161, 2017. Action Editor: Mark Steedman. Submission batch: 11/2016; Revision batch: 2/2017; Published 6/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Typology Verb-Object (English) Example dobj She gave me a raise dobj Object-Verb (Hindi) She me a raise gave vah mujhe ek uthaane diya Prep"
Q17-1011,C16-1123,0,0.0822505,"Missing"
Q17-1011,C14-1098,0,0.117981,"c typological classification are relatively recent. Lewis and Xia (2008) predict typological properties from induced trees, but guess those trees from aligned bitexts, not by monolingual grammar induction as in §7.6. Liu (2010) and Futrell et al. (2015) show that the directionality of (gold) dependencies is indicative of “basic” word order and freeness of word order. Those papers predict typological properties from trees that are automatically (noisily) annotated or manually (expensively) annotated. An alternative is to predict the typology directly from raw or POS-tagged text, as we do. Saha Roy et al. (2014) first explored this idea, building a system that correctly predicts adposition typology on 19/23 languages with only word cooccurrence statistics. Zhang et al. (2016) evaluate semi-supervised POS tagging by asking whether the induced tag sequences can predict typological properties. Their prediction approach is supervised like ours, although developed separately and trained on different data. They more simply predict 6 binaryvalued WALS properties, using 6 independent binary classifiers based on POS bigram and trigrams. Our task is rather close to grammar induction, which likewise predicts a"
Q17-1011,N10-1116,0,0.0149505,"to i ≤ 1e4 (the first 10,000 sentences). 152 neural-feature system, and α ∈ [0, 1] is a hyperparameter to balance the two. sH (u) and sN (u) were trained separately. At test time, we use (11) to combine them linearly before the logistic transform (6). This yields a weighted-product-of-experts model. 6.4 Training procedure Length thresholding. By default, our feature vector π(u) is extracted from those sentences in u with length ≤ 40 tokens. In §7.3, however, we try concatenating this feature vector with one that is extracted in the same way from just sentences with length ≤ 10. The intuition (Spitkovsky et al., 2010) is that the basic word order of the language can be most easily discerned from short, simple sentences. Initialization. We initialize the model of (6)–(7) so that the estimated directionality pˆ(→ |r, L), regardless of L, is initially a weighted mean of r’s directionalities in the training languages, namely X p¯r ≡ wL (r) p∗ (→ |r, L) (12) L where wL (r) ≡ ∗ P p (r|L) ∗ 0 0 L p (r|L ) (13) This is done by setting V = 0 and the bias (bV )r = p¯r log 1−¯ pr , clipped to the range [−10, 10]. As a result, we make sensible initial predictions even for rare relations r, which allows us to converge"
Q17-1011,D13-1204,0,0.122146,"based on POS bigram and trigrams. Our task is rather close to grammar induction, which likewise predicts a set of real numbers giving the relative probabilities of competing syntactic configurations. Most previous work on grammar induction begins with maximum likelihood estimation of some generative model—such as a PCFG (Lari and Young, 1990; Carroll and Charniak, 1992) or dependency grammar (Klein and Manning, 2004)— 158 though it may add linguistically-informed inductive bias (Ganchev et al., 2010; Naseem et al., 2010). Most such methods use local search and must wrestle with local optima (Spitkovsky et al., 2013). Finegrained typological classification might supplement this approach, by cutting through the initial combinatorial challenge of establishing the basic wordorder properties of the language. In this paper we only quantify the directionality of each relation type, ignoring how tokens of these relations interact locally to give coherent parse trees. Grammar induction methods like EM could naturally consider those local interactions for a more refined analysis, when guided by our predicted global directionalities. 9 Conclusions and Future Work We introduced a typological classification task, whi"
Q17-1011,N13-1126,0,0.518073,"Missing"
Q17-1011,Q16-1035,1,0.71823,"separately. Why hasn’t this been done before? Our setting presents unusually sparse data for supervised learning, since each training example is an entire language. The world presumably does not offer enough natural languages—particularly with machine-readable corpora—to train a good classifier to detect, say, Object-Verb-Subject (OVS) languages, especially given the class imbalance problem that OVS languages are empirically rare, and the non-IID problem that the available OVS languages may be evolutionarily related.1 We mitigate this issue by training on the Galactic Dependencies treebanks (Wang and Eisner, 2016), a collection of more than 50,000 human-like synthetic languages. The treebank of each synthetic language is generated by stochastically permuting the subtrees in a given real treebank to match the word order of other real languages. Thus, we have many synthetic languages that are Object-Verb like Hindi but also Noun-Adjective like French. We know the true directionality of each synthetic language and we would like our classifier to predict that directionality, just as it would for a real language. We will show that our system’s accuracy benefits from fleshing out the training set in this way"
Q17-1011,D15-1213,0,0.296702,"es with “*” as they are significantly better than their “UD” counterparts (paired permutation test by language, p < 0.05). 8 Related Work Typological properties can usefully boost the performance of cross-linguistic systems (Bender, 2009; O’Horan et al., 2016). These systems mainly aim to annotate low-resource languages with help from models trained on similar high-resource languages. Naseem et al. (2012) introduce a “selective sharing” technique for generative parsing, in which a Subject-Verb language will use parameters shared with other Subject-Verb languages. T¨ackstr¨om et al. (2013) and Zhang and Barzilay (2015) extend this idea to discriminative parsing and gain further improvements by conjoining regular parsing features with typological features. The cross-linguistic neural parser of Ammar et al. (2016) conditions on typological features by supplying a “language embedding” as input. Zhang et al. (2012) use typological properties to convert language-specific POS tags to UD POS tags, based on their ordering in a corpus. Moving from engineering to science, linguists seek typological universals of human language (Greenberg, 1963; Croft, 2002; Song, 2014; Hawkins, 2014), e.g., “languages with dominant V"
Q17-1011,D12-1125,0,0.127473,"esource languages with help from models trained on similar high-resource languages. Naseem et al. (2012) introduce a “selective sharing” technique for generative parsing, in which a Subject-Verb language will use parameters shared with other Subject-Verb languages. T¨ackstr¨om et al. (2013) and Zhang and Barzilay (2015) extend this idea to discriminative parsing and gain further improvements by conjoining regular parsing features with typological features. The cross-linguistic neural parser of Ammar et al. (2016) conditions on typological features by supplying a “language embedding” as input. Zhang et al. (2012) use typological properties to convert language-specific POS tags to UD POS tags, based on their ordering in a corpus. Moving from engineering to science, linguists seek typological universals of human language (Greenberg, 1963; Croft, 2002; Song, 2014; Hawkins, 2014), e.g., “languages with dominant Verb-Subject-Object order are always prepositional.” Dryer and Haspelmath (2013) characterize 2679 world languages with 192 typological properties. Their WALS database can supply features to NLP systems (see previous paragraph) or gold standard labels for typological classifiers. Daum´e III and Cam"
Q17-1011,N16-1156,0,0.034394,"s, not by monolingual grammar induction as in §7.6. Liu (2010) and Futrell et al. (2015) show that the directionality of (gold) dependencies is indicative of “basic” word order and freeness of word order. Those papers predict typological properties from trees that are automatically (noisily) annotated or manually (expensively) annotated. An alternative is to predict the typology directly from raw or POS-tagged text, as we do. Saha Roy et al. (2014) first explored this idea, building a system that correctly predicts adposition typology on 19/23 languages with only word cooccurrence statistics. Zhang et al. (2016) evaluate semi-supervised POS tagging by asking whether the induced tag sequences can predict typological properties. Their prediction approach is supervised like ours, although developed separately and trained on different data. They more simply predict 6 binaryvalued WALS properties, using 6 independent binary classifiers based on POS bigram and trigrams. Our task is rather close to grammar induction, which likewise predicts a set of real numbers giving the relative probabilities of competing syntactic configurations. Most previous work on grammar induction begins with maximum likelihood est"
Q17-1019,Q15-1039,0,0.023993,"that are accurate, or at least helpful to the accuracy of some downstream task. Pruning methods14 can use classifiers not only to select spans but also to prune at other granularities (Roark and Hollingshead, 2008; Bodenstab et al., 2011). Prioritization methods do not prune substructures, but instead delay their processing until they are needed—if ever (Caraballo and Charniak, 1998). This paper focuses on learning pruning heuristics that have trainable parameters. In the same way, Stoyanov and Eisner (2012) learn to turn off unneeded factors in a graphical model, and Jiang et al. (2012) and Berant and Liang (2015) train prioritization heuristics (using policy gradient). In both of those 2012 papers, we explicitly sought to maximize accuracy − λ · runtime as we do here. Some previous “coarse-to-fine” work does not optimize heuris14 We focus here on parsing, but pruning is generally useful in structured prediction. E.g., Xu et al. (2013) train a classifier to prune (latent) alignments in a machine translation system. System Dyer et al. (2016a); Dyer et al. (2016b) Zhu et al. (2013) Fernández-González and Martins (2015) Petrov and Klein (2007) Crabbé (2015) Our most accurate parser Bodenstab (2012) w/ SpM"
Q17-1019,J98-2004,0,0.312548,"roblem, which we tackle with the LOLS algorithm. LOLS training must continually compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms. We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime. 1 Introduction Decades of research have been dedicated to heuristics for speeding up inference in natural language processing tasks, such as constituency parsing (Pauls and Klein, 2009; Caraballo and Charniak, 1998) and machine translation (Petrov et al., 2008; Xu et al., 2013). Such research is necessary because of a trend toward richer models, which improve accuracy at the cost of slower inference. For example, state-of-theart constituency parsers use grammars with millions of rules, while dependency parsers routinely use millions of features. Without heuristics, these parsers take minutes to process a single sentence. To speed up inference, we will learn a pruning policy. During inference, the pruning policy is invoked to decide whether to keep or prune various parts of the search space, based on feat"
Q17-1019,N06-1022,0,0.0357916,"ystem. System Dyer et al. (2016a); Dyer et al. (2016b) Zhu et al. (2013) Fernández-González and Martins (2015) Petrov and Klein (2007) Crabbé (2015) Our most accurate parser Bodenstab (2012) w/ SpMV Bodenstab (2012) w/o SpMV Hall et al. (2014) F1 words/sec 93.3 90.4 90.2 90.1 90.0 88.9 88.8 88.7 88.6 – 1290 957 169 2150 218 1581 188 12 Figure 5: Comparison among fast and accurate parsers. Runtimes are computed on different machines and parsers are implemented in different programming languages, so runtime is not a controlled comparison. tics directly but rather derives heuristics for pruning (Charniak et al., 2006; Petrov and Klein, 2007; Weiss and Taskar, 2010; Rush and Petrov, 2012) or prioritization (Klein and Manning, 2003; Pauls and Klein, 2009) from a coarser version of the model. Combining these automatic methods with LOLS would require first enriching their heuristics with trainable parameters, or parameterizing the coarse-to-fine hierarchy itself as in the “feature pruning” work of He et al. (2013) and Strubell et al. (2015). Dynamic features are ones that depend on previous actions. In our setting, a policy could in principle benefit from considering the full state of the chart at Alg. 1 line"
Q17-1019,D14-1082,0,0.0356382,"where the only training signal is a joint assessment of the entire sequence of actions. It is an alternative to policy gradient, which does not scale well to our long trajectories because of high variance in the estimated gradient and because random exploration around (even good) pruning policies most often results in no parse at all. LOLS uses controlled comparisons, resulting in more precise “credit assignment” and tighter exploration. We would be remiss not to note that current transition-based parsers—for constituency parsing (Zhu et al., 2013; Crabbé, 2015) as well as dependency parsing (Chen and Manning, 2014)—are both incredibly fast and surprisingly accurate. This may appear to undermine the motivation for our work, or at least for its application to fast parsing.16 However, transition-based parsers do not produce marginal probabilities of substructures, which can be useful features for downstream tasks. Indeed, the transitionbased approach is essentially greedy and so it may fail on tasks with more ambiguity than parsing. Current transition-based parsers also require step-by-step supervision, whereas our method can also be used to train in the presence of incomplete supervision, latent structure"
Q17-1019,W12-3159,0,0.0275987,"ly. It has usually been treated with some form of simple imitation learning, using a heuristic training signal much as in our baseline (Jiang, 2014; He et al., 2013). LOLS would be a more principled way to train such features, but for efficiency, our present paper restricts to static features that only access the state via π(w, i, k). This permits our fast CP and DP rollout algorithms. It also reduces the time and space cost of dataset aggregation.15 LOLS attempts to do end-to-end training of a sequential decision-making system, without falling back on black-box optimization tools (Och, 2003; Chung and Galley, 2012) that ignore the sequential structure. In NLP, sequential decisions are more commonly trained with step-by-step supervision 15 LOLS repeatedly evaluates actions given (w, i, k). We consolidate the resulting training examples by summing their reward vectors rb, so the aggregated dataset does not grow over time. 275 (Kuhlmann et al., 2011), using methods such as local classification (Punyakanok and Roth, 2001) or beam search with early update (Collins and Roark, 2004). LOLS tackles the harder setting where the only training signal is a joint assessment of the entire sequence of actions. It is an"
Q17-1019,P04-1015,0,0.104825,"to do end-to-end training of a sequential decision-making system, without falling back on black-box optimization tools (Och, 2003; Chung and Galley, 2012) that ignore the sequential structure. In NLP, sequential decisions are more commonly trained with step-by-step supervision 15 LOLS repeatedly evaluates actions given (w, i, k). We consolidate the resulting training examples by summing their reward vectors rb, so the aggregated dataset does not grow over time. 275 (Kuhlmann et al., 2011), using methods such as local classification (Punyakanok and Roth, 2001) or beam search with early update (Collins and Roark, 2004). LOLS tackles the harder setting where the only training signal is a joint assessment of the entire sequence of actions. It is an alternative to policy gradient, which does not scale well to our long trajectories because of high variance in the estimated gradient and because random exploration around (even good) pruning policies most often results in no parse at all. LOLS uses controlled comparisons, resulting in more precise “credit assignment” and tighter exploration. We would be remiss not to note that current transition-based parsers—for constituency parsing (Zhu et al., 2013; Crabbé, 201"
Q17-1019,D15-1212,0,0.0176292,"oark, 2004). LOLS tackles the harder setting where the only training signal is a joint assessment of the entire sequence of actions. It is an alternative to policy gradient, which does not scale well to our long trajectories because of high variance in the estimated gradient and because random exploration around (even good) pruning policies most often results in no parse at all. LOLS uses controlled comparisons, resulting in more precise “credit assignment” and tighter exploration. We would be remiss not to note that current transition-based parsers—for constituency parsing (Zhu et al., 2013; Crabbé, 2015) as well as dependency parsing (Chen and Manning, 2014)—are both incredibly fast and surprisingly accurate. This may appear to undermine the motivation for our work, or at least for its application to fast parsing.16 However, transition-based parsers do not produce marginal probabilities of substructures, which can be useful features for downstream tasks. Indeed, the transitionbased approach is essentially greedy and so it may fail on tasks with more ambiguity than parsing. Current transition-based parsers also require step-by-step supervision, whereas our method can also be used to train in t"
Q17-1019,N16-1024,0,0.0822675,"Missing"
Q17-1019,W16-5901,1,0.849878,"exactly The naive rollout algorithm runs the parser T times— r˜ + ∆ · ∂ r˜/∂mik (3) once for each variation of the pruning mask. The Z + ∆ · ∂Z/∂mik reader may be reminded of the finite difference approximation to the gradient of a function, which also It remains to compute these partial derivatives. All measures the effects from perturbing each input value partials can be jointly computed by back-propagation, individually. In fact, for certain reward functions, the which equivalent to another dynamic program known naive algorithm can be precisely regarded as comput- as the outside algorithm (Eisner, 2016). ing a gradient—and thus we can use a more efficient The inside algorithm only needs to visit the |E 0 | algorithm, back-propagation, which finds the entire unpruned edges, but the outside algorithm must also gradient vector of reward as fast (in the big-O sense) visit some pruned edges, to determine the effect of as computing the reward once. The overall algorithm “unpruning” them (changing their mik input from 0 is O(|E |+ T ) where |E |is the total number of hy- to 1) by finding ∂ r˜/∂mik and ∂Z/∂mik . On the peredges, whereas the naive algorithm is O(|E 0 |·T ) other hand, these partials"
Q17-1019,P15-1147,0,0.250422,"reward. 8 Related work Our experiments have focused on using LOLS to improve a reasonable baseline. Fig. 5 shows that our resulting parser fits reasonably among state-of-the-art constituency parsers trained and tested on the Penn Treebank. These parsers include a variety of techniques that improve speed or accuracy. Many are quite orthogonal to our work here—e.g., the SpMV method (which is necessary for Bodenstab’s parser to beat ours) is a set of cache-efficient optimizations (Dunlop, 2014) that could be added to our parser (just as it was added to Bodenstab’s), while Hall et al. (2014) and Fernández-González and Martins (2015) replace the grammar with faster scoring models that have more conditional independence. Overall, other fast parsers could also be trained using LOLS, so that 274 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00 coarse grammar fine grammar p(diff) p(diff|gold) p(diff|¬ gold) 0.5 [regret|diff] regret |diff 0.4 [regret|diff, gold] 0.3 [regret|diff, ¬ gold] 0.2 0.1 0.0 0.020 [regret ] 0.015 regret Effect of λ: Aggressive pruning (large λ) reduces accuracy, so its effect on the top row is similar to that of using a coarse grammar. Aggressive pruning also has an effect on the middle row: there is more"
Q17-1019,P08-1109,0,0.0973393,"Missing"
Q17-1019,P14-1022,0,0.167202,"t. Shape features map a word or phrase into a string of character classes (uppercase, lowercase, numeric, spaces); we truncate substrings of identical classes to length two; punctuation chars are never modified in any way. Width buckets use the following partition: 2, 3, 4, 5, [6, 10], [11, 20], [21, ∞). We use feature hashing (Weinberger et al., 2009) with MurmurHash3 (Appleby, 2008) and project to 222 features. Conjunctions are taken at positions (i−1, i), (k, k +1), (i−1, k +1) and (i, k). We use special begin and end symbols when a template accesses positions beyond the sentence boundary. Hall et al. (2014) give examples motivating our feature templates and show experimentally that they are effective in multiple languages. Boundary words are strong surface cues for phrase boundaries. Span shape features are also useful as they (minimally) check for matched parentheses and quotation marks. 7 Experimental design and results Reward functions and surrogates: Each user has a personal reward function. In this paper, we choose to specify our true reward as accuracy − λ · runtime, where accuracy is given by labeled F1 percentage and runtime by mega-pushes (mpush), millions of calls per sentence to lines"
Q17-1019,D13-1152,1,0.858861,"omputed on different machines and parsers are implemented in different programming languages, so runtime is not a controlled comparison. tics directly but rather derives heuristics for pruning (Charniak et al., 2006; Petrov and Klein, 2007; Weiss and Taskar, 2010; Rush and Petrov, 2012) or prioritization (Klein and Manning, 2003; Pauls and Klein, 2009) from a coarser version of the model. Combining these automatic methods with LOLS would require first enriching their heuristics with trainable parameters, or parameterizing the coarse-to-fine hierarchy itself as in the “feature pruning” work of He et al. (2013) and Strubell et al. (2015). Dynamic features are ones that depend on previous actions. In our setting, a policy could in principle benefit from considering the full state of the chart at Alg. 1 line 14. While coarse-to-fine methods implicitly use certain dynamic features, training with dynamic features is a fairly new goal that is challenging to treat efficiently. It has usually been treated with some form of simple imitation learning, using a heuristic training signal much as in our baseline (Jiang, 2014; He et al., 2013). LOLS would be a more principled way to train such features, but for e"
Q17-1019,C08-5001,0,0.0123116,".2, we leverage the fact that rollouts will be similar to one another (differing by a single pruning decision). In §5.3, we show that the reward of all T rollouts can be computed simultaneously by dynamic programming under some assumptions about the structure of the reward function (described later). We found these algorithms to be crucial to training in a “reasonable” amount of time (see the empirical comparison in §7.2). 5.1 Background: Parsing as hypergraphs It is convenient to present our efficient rollout algorithms in terms of the hypergraph structure of Alg. 1 (Klein and Manning, 2001; Huang, 2008; Li and Eisner, 2009; Eisner and Blatz, 2007). A hypergraph describes the information flow among related quantities in a dynamic programming algorithm. Many computational tricks apply generically to hypergraphs. A hypergraph edge e (or hyperedge) is a “generalized arrow” e.head ≺ e.Tail with one output and a list of inputs. We regard each quantity βikx , mik , or G(. . .) in Alg. 1 as the value of a correspond˙ . .). Thus, ing hypergraph vertex β˙ ikx , m ˙ ik , or G(. value(v) ˙ = v for any vertex v. ˙ Each m ˙ ik ’s value is computed by the policy π or chosen by a rollout in˙ value is given"
Q17-1019,W01-1812,0,0.134725,"ng work among them. In §5.2, we leverage the fact that rollouts will be similar to one another (differing by a single pruning decision). In §5.3, we show that the reward of all T rollouts can be computed simultaneously by dynamic programming under some assumptions about the structure of the reward function (described later). We found these algorithms to be crucial to training in a “reasonable” amount of time (see the empirical comparison in §7.2). 5.1 Background: Parsing as hypergraphs It is convenient to present our efficient rollout algorithms in terms of the hypergraph structure of Alg. 1 (Klein and Manning, 2001; Huang, 2008; Li and Eisner, 2009; Eisner and Blatz, 2007). A hypergraph describes the information flow among related quantities in a dynamic programming algorithm. Many computational tricks apply generically to hypergraphs. A hypergraph edge e (or hyperedge) is a “generalized arrow” e.head ≺ e.Tail with one output and a list of inputs. We regard each quantity βikx , mik , or G(. . .) in Alg. 1 as the value of a correspond˙ . .). Thus, ing hypergraph vertex β˙ ikx , m ˙ ik , or G(. value(v) ˙ = v for any vertex v. ˙ Each m ˙ ik ’s value is computed by the policy π or chosen by a rollout in˙ v"
Q17-1019,N03-1016,0,0.0765091,"trov and Klein (2007) Crabbé (2015) Our most accurate parser Bodenstab (2012) w/ SpMV Bodenstab (2012) w/o SpMV Hall et al. (2014) F1 words/sec 93.3 90.4 90.2 90.1 90.0 88.9 88.8 88.7 88.6 – 1290 957 169 2150 218 1581 188 12 Figure 5: Comparison among fast and accurate parsers. Runtimes are computed on different machines and parsers are implemented in different programming languages, so runtime is not a controlled comparison. tics directly but rather derives heuristics for pruning (Charniak et al., 2006; Petrov and Klein, 2007; Weiss and Taskar, 2010; Rush and Petrov, 2012) or prioritization (Klein and Manning, 2003; Pauls and Klein, 2009) from a coarser version of the model. Combining these automatic methods with LOLS would require first enriching their heuristics with trainable parameters, or parameterizing the coarse-to-fine hierarchy itself as in the “feature pruning” work of He et al. (2013) and Strubell et al. (2015). Dynamic features are ones that depend on previous actions. In our setting, a policy could in principle benefit from considering the full state of the chart at Alg. 1 line 14. While coarse-to-fine methods implicitly use certain dynamic features, training with dynamic features is a fair"
Q17-1019,P11-1068,0,0.0275936,"s permits our fast CP and DP rollout algorithms. It also reduces the time and space cost of dataset aggregation.15 LOLS attempts to do end-to-end training of a sequential decision-making system, without falling back on black-box optimization tools (Och, 2003; Chung and Galley, 2012) that ignore the sequential structure. In NLP, sequential decisions are more commonly trained with step-by-step supervision 15 LOLS repeatedly evaluates actions given (w, i, k). We consolidate the resulting training examples by summing their reward vectors rb, so the aggregated dataset does not grow over time. 275 (Kuhlmann et al., 2011), using methods such as local classification (Punyakanok and Roth, 2001) or beam search with early update (Collins and Roark, 2004). LOLS tackles the harder setting where the only training signal is a joint assessment of the entire sequence of actions. It is an alternative to policy gradient, which does not scale well to our long trajectories because of high variance in the estimated gradient and because random exploration around (even good) pruning policies most often results in no parse at all. LOLS uses controlled comparisons, resulting in more precise “credit assignment” and tighter explor"
Q17-1019,D09-1005,1,0.912517,"ge the fact that rollouts will be similar to one another (differing by a single pruning decision). In §5.3, we show that the reward of all T rollouts can be computed simultaneously by dynamic programming under some assumptions about the structure of the reward function (described later). We found these algorithms to be crucial to training in a “reasonable” amount of time (see the empirical comparison in §7.2). 5.1 Background: Parsing as hypergraphs It is convenient to present our efficient rollout algorithms in terms of the hypergraph structure of Alg. 1 (Klein and Manning, 2001; Huang, 2008; Li and Eisner, 2009; Eisner and Blatz, 2007). A hypergraph describes the information flow among related quantities in a dynamic programming algorithm. Many computational tricks apply generically to hypergraphs. A hypergraph edge e (or hyperedge) is a “generalized arrow” e.head ≺ e.Tail with one output and a list of inputs. We regard each quantity βikx , mik , or G(. . .) in Alg. 1 as the value of a correspond˙ . .). Thus, ing hypergraph vertex β˙ ikx , m ˙ ik , or G(. value(v) ˙ = v for any vertex v. ˙ Each m ˙ ik ’s value is computed by the policy π or chosen by a rollout in˙ value is given by the grammar. terv"
Q17-1019,J93-2004,0,0.0581012,"precision arithmetic libraries. 268 Note that the DP method computes only the accuracies of rollouts—not the runtimes. In this paper, we will combine DP with a very simple runtime measure that is trivial to roll out (see §7). An alternative would be to use CP to roll out the runtimes. This is very efficient: to measure just runtime, CP only needs to update the record of which constituents or edges are built, and not their scores, so the changes are easier to compute than in §5.2, and peter out more quickly. 6 Parser details7 Setup: We use the standard English parsing setup: the Penn Treebank (Marcus et al., 1993) with the standard train/dev/test split, and standard tree normalization.8 For efficiency during training, we restrict the length of sentences to ≤ 40. We do not restrict the length of test sentences. We experiment with two grammars: coarse, the “no frills” left-binarized treebank grammar, and fine, a variant of the Berkeley split-merge level-6 grammar (Petrov et al., 2006) as provided by Dunlop (2014, ch. 5). The parsing algorithms used during training are described in §5. Our test-time parsing algorithm uses the left-child loop implementation of CKY (Dunlop et al., 2010). All algorithms allo"
Q17-1019,P05-1010,0,0.121629,"cause cascading errors, while others are “worked around” in the sense that the grammar still selects a mostly-gold parse. Similarly, actions that prune a non-gold constituent are not equally good—some provide more overall speedup (e.g., pruning narrow constituents prevents wider ones from being built), and some even improve accuracy by suppressing an incorrect but high-scoring parse. More generally, the gold vs. non-gold distinction is not even available in NLP tasks where one is pruning potential elements of a latent structure, such as an alignment (Xu et al., 2013) or a finer-grained parse (Matsuzaki et al., 2005). Yet our approach can still be used in such settings, by evaluating the reward on the downstream task that the latent structure serves. Past work on optimizing end-to-end performance is discussed in §8. One might try to scale these techniques to learning to prune, but in this work we take a different approach. Given a policy, we can easily find small ways to improve it on specific sentences by varying individual pruning actions (e.g., if π currently prunes a span then try keeping it instead). Given a batch of improved action sequences (trajectories), the remaining step is to search for a poli"
Q17-1019,P03-1021,0,0.0425871,"t efficiently. It has usually been treated with some form of simple imitation learning, using a heuristic training signal much as in our baseline (Jiang, 2014; He et al., 2013). LOLS would be a more principled way to train such features, but for efficiency, our present paper restricts to static features that only access the state via π(w, i, k). This permits our fast CP and DP rollout algorithms. It also reduces the time and space cost of dataset aggregation.15 LOLS attempts to do end-to-end training of a sequential decision-making system, without falling back on black-box optimization tools (Och, 2003; Chung and Galley, 2012) that ignore the sequential structure. In NLP, sequential decisions are more commonly trained with step-by-step supervision 15 LOLS repeatedly evaluates actions given (w, i, k). We consolidate the resulting training examples by summing their reward vectors rb, so the aggregated dataset does not grow over time. 275 (Kuhlmann et al., 2011), using methods such as local classification (Punyakanok and Roth, 2001) or beam search with early update (Collins and Roark, 2004). LOLS tackles the harder setting where the only training signal is a joint assessment of the entire sequ"
Q17-1019,N09-1063,0,0.116251,"cult machine learning problem, which we tackle with the LOLS algorithm. LOLS training must continually compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms. We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime. 1 Introduction Decades of research have been dedicated to heuristics for speeding up inference in natural language processing tasks, such as constituency parsing (Pauls and Klein, 2009; Caraballo and Charniak, 1998) and machine translation (Petrov et al., 2008; Xu et al., 2013). Such research is necessary because of a trend toward richer models, which improve accuracy at the cost of slower inference. For example, state-of-theart constituency parsers use grammars with millions of rules, while dependency parsers routinely use millions of features. Without heuristics, these parsers take minutes to process a single sentence. To speed up inference, we will learn a pruning policy. During inference, the pruning policy is invoked to decide whether to keep or prune various parts of"
Q17-1019,N07-1051,0,0.325035,"Missing"
Q17-1019,P06-1055,0,0.026501,"ch constituents or edges are built, and not their scores, so the changes are easier to compute than in §5.2, and peter out more quickly. 6 Parser details7 Setup: We use the standard English parsing setup: the Penn Treebank (Marcus et al., 1993) with the standard train/dev/test split, and standard tree normalization.8 For efficiency during training, we restrict the length of sentences to ≤ 40. We do not restrict the length of test sentences. We experiment with two grammars: coarse, the “no frills” left-binarized treebank grammar, and fine, a variant of the Berkeley split-merge level-6 grammar (Petrov et al., 2006) as provided by Dunlop (2014, ch. 5). The parsing algorithms used during training are described in §5. Our test-time parsing algorithm uses the left-child loop implementation of CKY (Dunlop et al., 2010). All algorithms allow unary rules (though not chains). We evaluate accuracy at test time with the F1 score from the official EVALB script (Sekine and Collins, 1997). Training: Note that we never retrain the grammar weights—we train only the pruning policy. To TRAIN our classifiers (Alg. 2 line 13), we use L2 -regularized logistic regression, trained with L-BFGS optimization. We always rescale"
Q17-1019,D08-1012,0,0.0178575,"raining must continually compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms. We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime. 1 Introduction Decades of research have been dedicated to heuristics for speeding up inference in natural language processing tasks, such as constituency parsing (Pauls and Klein, 2009; Caraballo and Charniak, 1998) and machine translation (Petrov et al., 2008; Xu et al., 2013). Such research is necessary because of a trend toward richer models, which improve accuracy at the cost of slower inference. For example, state-of-theart constituency parsers use grammars with millions of rules, while dependency parsers routinely use millions of features. Without heuristics, these parsers take minutes to process a single sentence. To speed up inference, we will learn a pruning policy. During inference, the pruning policy is invoked to decide whether to keep or prune various parts of the search space, based on features of the input and (potentially) the state"
Q17-1019,C08-1094,0,0.0316509,"suboptimal on its own system π1 and dashed lines show where it is suboptimal on the LOLS -improved system π∗ . Each plot shows an overall quantity in black as well as that quantity broken down by gold and non-gold spans. Top: Fraction of states in which oracles differ. Middle: Expected regret per state in which oracles differ. Bottom: Expected regret per state. See §7.3 for discussion. they quickly find parses that are accurate, or at least helpful to the accuracy of some downstream task. Pruning methods14 can use classifiers not only to select spans but also to prune at other granularities (Roark and Hollingshead, 2008; Bodenstab et al., 2011). Prioritization methods do not prune substructures, but instead delay their processing until they are needed—if ever (Caraballo and Charniak, 1998). This paper focuses on learning pruning heuristics that have trainable parameters. In the same way, Stoyanov and Eisner (2012) learn to turn off unneeded factors in a graphical model, and Jiang et al. (2012) and Berant and Liang (2015) train prioritization heuristics (using policy gradient). In both of those 2012 papers, we explicitly sought to maximize accuracy − λ · runtime as we do here. Some previous “coarse-to-fine” w"
Q17-1019,N12-1054,0,0.0189945,") Fernández-González and Martins (2015) Petrov and Klein (2007) Crabbé (2015) Our most accurate parser Bodenstab (2012) w/ SpMV Bodenstab (2012) w/o SpMV Hall et al. (2014) F1 words/sec 93.3 90.4 90.2 90.1 90.0 88.9 88.8 88.7 88.6 – 1290 957 169 2150 218 1581 188 12 Figure 5: Comparison among fast and accurate parsers. Runtimes are computed on different machines and parsers are implemented in different programming languages, so runtime is not a controlled comparison. tics directly but rather derives heuristics for pruning (Charniak et al., 2006; Petrov and Klein, 2007; Weiss and Taskar, 2010; Rush and Petrov, 2012) or prioritization (Klein and Manning, 2003; Pauls and Klein, 2009) from a coarser version of the model. Combining these automatic methods with LOLS would require first enriching their heuristics with trainable parameters, or parameterizing the coarse-to-fine hierarchy itself as in the “feature pruning” work of He et al. (2013) and Strubell et al. (2015). Dynamic features are ones that depend on previous actions. In our setting, a policy could in principle benefit from considering the full state of the chart at Alg. 1 line 14. While coarse-to-fine methods implicitly use certain dynamic feature"
Q17-1019,P06-2101,1,0.728791,"|·T ) other hand, these partials are 0 when some other where |E 0 |≤ |E |is the maximum number of hyper- input to the hyperedge is 0. This case is common edges actually visited on any rollout. when the hypergraph is heavily pruned (|E 0 | |E|), What accuracy measure must we use? Let r(d) de- and means that back-propagation need not descend note the recall of a derivation d—the fraction of gold further through that hyperedge. constituents that appear as vertices in the derivation. 6 In theory, we could anneal from expected to 1-best recall A simple accuracy metric would be 1-best recall, the (Smith and Eisner, 2006). We experimented extensively with b of the highest-scoring derivation db that recall r(d) annealing but found it to be too numerically unstable for our was not pruned. In this section, we relax that to ex- purposes, even with high-precision arithmetic libraries. 268 Note that the DP method computes only the accuracies of rollouts—not the runtimes. In this paper, we will combine DP with a very simple runtime measure that is trivial to roll out (see §7). An alternative would be to use CP to roll out the runtimes. This is very efficient: to measure just runtime, CP only needs to update the recor"
Q17-1019,P15-1015,0,0.019598,"machines and parsers are implemented in different programming languages, so runtime is not a controlled comparison. tics directly but rather derives heuristics for pruning (Charniak et al., 2006; Petrov and Klein, 2007; Weiss and Taskar, 2010; Rush and Petrov, 2012) or prioritization (Klein and Manning, 2003; Pauls and Klein, 2009) from a coarser version of the model. Combining these automatic methods with LOLS would require first enriching their heuristics with trainable parameters, or parameterizing the coarse-to-fine hierarchy itself as in the “feature pruning” work of He et al. (2013) and Strubell et al. (2015). Dynamic features are ones that depend on previous actions. In our setting, a policy could in principle benefit from considering the full state of the chart at Alg. 1 line 14. While coarse-to-fine methods implicitly use certain dynamic features, training with dynamic features is a fairly new goal that is challenging to treat efficiently. It has usually been treated with some form of simple imitation learning, using a heuristic training signal much as in our baseline (Jiang, 2014; He et al., 2013). LOLS would be a more principled way to train such features, but for efficiency, our present pape"
Q17-1019,P13-2063,0,0.337901,"lly compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms. We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime. 1 Introduction Decades of research have been dedicated to heuristics for speeding up inference in natural language processing tasks, such as constituency parsing (Pauls and Klein, 2009; Caraballo and Charniak, 1998) and machine translation (Petrov et al., 2008; Xu et al., 2013). Such research is necessary because of a trend toward richer models, which improve accuracy at the cost of slower inference. For example, state-of-theart constituency parsers use grammars with millions of rules, while dependency parsers routinely use millions of features. Without heuristics, these parsers take minutes to process a single sentence. To speed up inference, we will learn a pruning policy. During inference, the pruning policy is invoked to decide whether to keep or prune various parts of the search space, based on features of the input and (potentially) the state of the inference"
Q17-1019,P13-1043,0,0.0194893,"ate (Collins and Roark, 2004). LOLS tackles the harder setting where the only training signal is a joint assessment of the entire sequence of actions. It is an alternative to policy gradient, which does not scale well to our long trajectories because of high variance in the estimated gradient and because random exploration around (even good) pruning policies most often results in no parse at all. LOLS uses controlled comparisons, resulting in more precise “credit assignment” and tighter exploration. We would be remiss not to note that current transition-based parsers—for constituency parsing (Zhu et al., 2013; Crabbé, 2015) as well as dependency parsing (Chen and Manning, 2014)—are both incredibly fast and surprisingly accurate. This may appear to undermine the motivation for our work, or at least for its application to fast parsing.16 However, transition-based parsers do not produce marginal probabilities of substructures, which can be useful features for downstream tasks. Indeed, the transitionbased approach is essentially greedy and so it may fail on tasks with more ambiguity than parsing. Current transition-based parsers also require step-by-step supervision, whereas our method can also be use"
Q18-1046,Q16-1022,0,0.136449,"Missing"
Q18-1046,Q16-1031,0,0.0864408,"ages to use for training, even nearest-neighbor could be an effective method. That is, one could obtain the parser for a test corpus simply by copying the trained parser for the most similar training corpus (under some metric). Wang and Eisner (2016) explored this approach of “single-source transfer” from synthetic languages. Yet with only thousands of synthetic languages, perhaps no single training corpus is sufficiently similar.5 To draw on patterns in many training corpora to figure out how to parse the test corpus, we will train a single parser that can handle all of the training corpora (Ammar et al., 2016), much as we trained our typological classifier in earlier work (Wang and Eisner, 2017). 4 details of these two components in §6 and §7. We assume in this paper that the input sentence x is given as a POS sequence: that is, our parser is delexicalized. This spares us from also needing language-specific lexical parameters associated with the specific vocabulary of each language, a problem that we leave to future work. We will choose our universal parameter values by minimizing an estimate of their expected loss, ˆ = argmin mean Loss(Θ; x(`) , y(`) , u(`) ) Θ `∈Ltrain Θ (1) where Ltrain is a col"
Q18-1046,Q13-1032,0,0.0292571,"ld grounding (with or without the POS tags). Even this starting point has proved surprisingly difficult over decades of research, so it has not been clear whether the POS sequences even contain the necessary information. 2 This is a general approach to developing an unsupervised method for a specific type of dataset: tune its structure and hyperparameters so that it works well on actual datasets of that sort, and then apply it to new datasets. For example, consider clustering—the canonical unsupervised problem. What constitutes a useful cluster depends on the type of data and the application. Basu et al. (2013) develop a text clustering system specifically to aid teachers. Their “Powergrading” system can group all the student-written answers to a novel question, having been trained on human judgments of answer similarity for other questions. Their novel questions are analogous to our novel languages: their unsupervised system is specifically tailored to match teachers’ semantic similarity judgments within any corpus of student answers, just as ours is tailored to match linguists’ syntactic judgments within any corpus of human-language POS sequences. Other NLP work on supervised tuning of Yet this ta"
Q18-1046,C96-1058,1,0.186379,"elated to the word order typology—for example, if ADPs are more likely to have closely following than closely preceding NOUNs −w −w w w (πNOUN|ADP //πNOUN &gt; πNOUN|ADP //πNOUN ), the language is more likely to be prepositional than postpositional. . . . . . . f2 avg-pooling . . . . . . ⇡(u) Figure 2: Computing the neural feature vector π(u). parser first computes an unlabeled projective tree argmax score(x, y; u) (4) y∈Y(x) where, letting a range over the arcs in tree y, score(x, y; u) = X s(φ(a; x, u)) (5) a∈y With this definition, the argmax in (4) is computed efficiently by the algorithm of Eisner (1996). s(·) is a neural scoring function on vectors, Neural features. In contrast, our neural features automatically learn to extract arbitrary predictive configurations. As Figure 2 shows, we encode each POS-tagged sentence ui ∈ u using a recurrent neural network, which reads one-hot POS embeddings from left to right, then outputs its final hidden state vector f i as the encoding. The final neural π(u) is the average encoding of all sentences (average-pooling): that is, the average of all sentence-level configurations. We specifically use a gated recurrent unit (GRU) network (Cho et al., 2014). Th"
Q18-1046,H05-1050,1,0.667485,"g of gold POS sequences is an artificial task, to be sure.2 Nonetheless, it is a starting point 1 We also include an experiment on noisy POS sequences. It is clearly not the task setting faced by human language learners. Nor is it a plausible engineering setting: a language with gold POS sequences often also has at least a small treebank of gold parses, or at least parallel text in a language from which noisy parses can be noisily projected (Agi´c et al., 2016). There is also no practical reason to consider POS tags without their attached words. 2 668 unsupervised learners includes strapping (Eisner and Karakos, 2005; Karakos et al., 2007), which tunes with the help of both real and synthetic datasets, just as we will (§3). Are such systems really “unsupervised”? Yes, in the sense that they are able to discover desirable structure in a new dataset. Unsupervised learners are normally crafted using assumptions about the data domain. Their structure and hyperparameters may have been manually tuned to produce pleasing results for typical datasets in that domain. In the domain of POS corpora, we simply scale up this practice to automatically tune a large set of parameters, which later guide our system’s search"
Q18-1046,W14-4012,0,0.0306436,"Missing"
Q18-1046,P10-2036,0,0.0736846,"Missing"
Q18-1046,D17-1177,0,0.0346795,"Missing"
Q18-1046,N12-1069,0,0.0266222,"1998) and grammatical bigrams (Paskin, 2001, 2002). The dependency model with valence (DMV) (Klein and Manning, 2004) was the first 5 Wang and Eisner (2018) do investigate synthesis “on demand” of a permuted training corpus that is as similar as possible to the test corpus. 670 method to beat a simple right-branching heuristic. Headden III et al. (2009) and Spitkovsky et al. (2012) made the DMV more expressive by considering higher-order valency or punctuation. To reduce search error, strategies for eliminating or escaping local optima have included convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), smart initialization (Klein and Manning, 2004; Mareˇcek and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013). Unsupervised parsing (which is also our task) tries to turn the same corpus directly into a treebank, without necessarily finding a grammar. We discuss some recent milestones here. Grave and Elhadad (2015) propose a transductive learning objective for unsupervised parsing, and a convex relaxation of it. (Jiang et al. (2017) co"
Q18-1046,N07-1032,1,0.67744,"an artificial task, to be sure.2 Nonetheless, it is a starting point 1 We also include an experiment on noisy POS sequences. It is clearly not the task setting faced by human language learners. Nor is it a plausible engineering setting: a language with gold POS sequences often also has at least a small treebank of gold parses, or at least parallel text in a language from which noisy parses can be noisily projected (Agi´c et al., 2016). There is also no practical reason to consider POS tags without their attached words. 2 668 unsupervised learners includes strapping (Eisner and Karakos, 2005; Karakos et al., 2007), which tunes with the help of both real and synthetic datasets, just as we will (§3). Are such systems really “unsupervised”? Yes, in the sense that they are able to discover desirable structure in a new dataset. Unsupervised learners are normally crafted using assumptions about the data domain. Their structure and hyperparameters may have been manually tuned to produce pleasing results for typical datasets in that domain. In the domain of POS corpora, we simply scale up this practice to automatically tune a large set of parameters, which later guide our system’s search for linguist-approved"
Q18-1046,P13-1044,1,0.853359,"g corpus that is as similar as possible to the test corpus. 670 method to beat a simple right-branching heuristic. Headden III et al. (2009) and Spitkovsky et al. (2012) made the DMV more expressive by considering higher-order valency or punctuation. To reduce search error, strategies for eliminating or escaping local optima have included convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), smart initialization (Klein and Manning, 2004; Mareˇcek and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013). Unsupervised parsing (which is also our task) tries to turn the same corpus directly into a treebank, without necessarily finding a grammar. We discuss some recent milestones here. Grave and Elhadad (2015) propose a transductive learning objective for unsupervised parsing, and a convex relaxation of it. (Jiang et al. (2017) combined that work with grammar induction.) Martínez Alonso et al. (2017) create an unsupervised dependency parser that is formally similar to ours in that it uses cross-linguistic knowledge as well as statistics compute"
Q18-1046,D17-1302,0,0.112165,"Missing"
Q18-1046,P15-1133,0,0.153209,"Missing"
Q18-1046,Q16-1023,0,0.445679,"ammar induction involve optimizing a highly non-convex objective function such as likelihood. The optimization is typically NP-hard (Cohen and Smith, 2012), and approximate local search methods tend to get stuck in local optima. Introduction Dependency parsing is one of the core natural language processing tasks. It aims to parse a given sentence into its dependency tree: a directed graph of labeled syntactic relations between words. Supervised dependency parsers—which are trained using a “treebank” of known parses in the target language—have been very successful (McDonald, 2006; Nivre, 2008; Kiperwasser and Goldberg, 2016). By contrast, the progress • Model error: Likelihood does not correlate well with parsing accuracy anyway (Smith, 2006, Figure 3.2). Likelihood optimization seeks latent trees that help to predict the observed sentences, but these unsupervised trees may use a non-standard syntactic analysis or even be optimized to predict nonsyntactic properties such as topic. We seek a standard syntactic analysis—what Smith (2006) calls the M ATCH L INGUIST task. We address both difficulties by using a supervised learning framework—one whose objective function is easier to optimize and explicitly tries to ma"
Q18-1046,P04-1061,0,0.0990707,"Then it parses sentence x while taking T(u) as an additional input. We will give mean (x,y)∈(x(`) ,y(`) ) (2) (`) loss(ParseΘ (x; u ), y) | {z } yˆ where loss(. . .) is a task-specific per-sentence loss (defined in §8.1) that evaluates the parser’s output yˆ on sentence x against x’s correct tree y. 5 5.1 Related Work Per-Language Learning Many papers rely on some universal learning procedure to determine T(u) (see §4) for a target language. For example, T(·) may be the ExpectationMaximization (EM) algorithm, yielding a PCFG T(u) that fully determines a CKY parser (Carroll and Charniak, 1992; Klein and Manning, 2004). Since EM and CKY are fixed algorithms, this approach has no trainable parameters. Grammar induction tries to turn an unsupervised corpus into a generative grammar. The approach of the previous paragraph is often modified to reduce model error or search error (§1). To reduce model error, many papers have used dependency grammar, with training objectives that incorporate notions like lexical attraction (Yuret, 1998) and grammatical bigrams (Paskin, 2001, 2002). The dependency model with valence (DMV) (Klein and Manning, 2004) was the first 5 Wang and Eisner (2018) do investigate synthesis “on"
Q18-1046,N16-1121,0,0.164847,"pendency parsing on low-resource languages (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014, inter alia). McDonald et al. (2011) extend this approach to multiple source languages by projected transfer. Later work in this vein mainly tries to improve the approximate parses, including translating the source treebanks into the target language with an off-the-shelf machine translation system (Tiedemann et al., 2014), augmenting the trees with weights (Agi´c et al., 2016), and using only partial trees with high-confidence alignments (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016). 5.4 The Typology Component T(u) = ψ(W π(u) + bW ) ∈ Rh (3) where π(u) ∈ Rd is the surface features of u, W ∈ Rh×d maps π(u) into a h-dimensional space, bW ∈ Rh is a bias vector, and ψ is an element-wise activation function. While equation (3) has only 1 layer, we explore versions with from 0 to 3 layers (where T(u) = π(u) in the 0-layer case). A 2-layer version is shown in Figure 1. The number of layers is chosen by crossvalidation, as are h and the ψ function. 6.1 Design of the Surface Features π(u) To define π(u), we used development data to select the following fast but effective subset o"
Q18-1046,P05-1012,0,0.0970049,"ages—presumably not enough to generalize well to novel languages. We therefore augment our training dataset Ltrain with thousands of synthetic languages from the GD dataset (§3), as already discussed in §3.1. (8) where sH (a) and sN (a) are the scores produced by separately trained systems using, respectively, the hand-engineered and neural features from §6.1. Hyperparameter λ ∈ [0, 1] is tuned on dev data. 8 (9) Training the System Training Objective We exactly follow the training method of Kiperwasser and Goldberg (2016), who minimize a structured max-margin hinge loss (Taskar et al., 2004; McDonald et al., 2005; LeCun et al., 2007). Stochastic gradient descent (SGD).10 Treating each language as a single large example during training would lead to slow SGD steps. Instead, we take our SGD examples to be individual sentences, by regarding equations (1)–(2) together as 8 An alternative would be to concatenate T(u) with the representation computed by the BiLSTM. This gets empirically worse results, probably because the BiLSTM does not have advance knowledge of language-specific word order as it reads the sentence. We also tried an architecture that does both, with no notable improvement. 9 Formally, for"
Q18-1046,D11-1006,0,0.486255,"rks well. We show experimentally across multiple languages: (1) Features computed from the unparsed corpus improve parsing accuracy. (2) Including thousands of synthetic languages in the training yields further improvement. (3) Despite being computed from unparsed corpora, our learned task-specific features beat previous work’s interpretable typological features that require parsed corpora or expert categorization of the language. Our best method improved attachment scores on held-out test languages by an average of 5.6 percentage points over past work that does not inspect the unparsed data (McDonald et al., 2011), and by 20.7 points over past “grammar induction” work that does not use training languages (Naseem et al., 2010). 1 • Search error: Most formulations of grammar induction involve optimizing a highly non-convex objective function such as likelihood. The optimization is typically NP-hard (Cohen and Smith, 2012), and approximate local search methods tend to get stuck in local optima. Introduction Dependency parsing is one of the core natural language processing tasks. It aims to parse a given sentence into its dependency tree: a directed graph of labeled syntactic relations between words. Super"
Q18-1046,P12-1066,0,0.699564,"). Model-based. This method trains a single language-agnostic model. McDonald et al. (2011) train a delexicalized parser on the concatenation of all source treebanks, achieving a large gain over grammar induction. This parser can learn universals such as the preference for determiners to attach to nouns (which was hard-coded by Naseem et al. (2010)). However, it is expected to parse a sentence x without being told the language ` or even a corpus u, possibly by guessing properties of the language from the configurations it encounters in the single sentence x alone. Further gains were achieved (Naseem et al., 2012; Täckström et al., 2013b; Zhang and Barzilay, 2015; Ammar et al., 2016) by providing the parser with about 10 typological properties of x’s language—for example, whether direct objects generally fall to the right of the verb—as listed in the World Atlas of Linguistic Structures (Dryer and Haspelmath, 2013). However, relying on WALS raises some issues. (1) The unknown language might not be in WALS.7 (2) Some typological features are missing for some languages. (3) All the WALS features are categorical values, which loses useful information about tendencies (for example, how often the canonical"
Q18-1046,D10-1120,0,0.303604,"arsing accuracy. (2) Including thousands of synthetic languages in the training yields further improvement. (3) Despite being computed from unparsed corpora, our learned task-specific features beat previous work’s interpretable typological features that require parsed corpora or expert categorization of the language. Our best method improved attachment scores on held-out test languages by an average of 5.6 percentage points over past work that does not inspect the unparsed data (McDonald et al., 2011), and by 20.7 points over past “grammar induction” work that does not use training languages (Naseem et al., 2010). 1 • Search error: Most formulations of grammar induction involve optimizing a highly non-convex objective function such as likelihood. The optimization is typically NP-hard (Cohen and Smith, 2012), and approximate local search methods tend to get stuck in local optima. Introduction Dependency parsing is one of the core natural language processing tasks. It aims to parse a given sentence into its dependency tree: a directed graph of labeled syntactic relations between words. Supervised dependency parsers—which are trained using a “treebank” of known parses in the target language—have been ver"
Q18-1046,P13-1028,0,0.0481038,"Missing"
Q18-1046,E14-2005,0,0.0187369,"g. tagging accuracy 100 Figure 6: Performance on noisy input over 16 training languages. Each dot is an experiment annotated by the number of sentences used to train the tagger. (The rightmost “∞” point uses gold tags instead of a tagger, which is the result from Table 1.) The x-axis gives the average accuracy of the trained RDRPOSTagger. The y-axis gives the average parsing performance. 9.8 consist of noisy POS tags rather than gold POS tags. Following Wang and Eisner (2016, Appendix B), at test time, the gold POS tags in a corpus are replaced by a noisy version produced by the RDRPOSTagger (Nguyen et al., 2014) trained on a subset of the original gold-tagged corpus.17 Figure 6 shows a linear relationship between the performance of our best model (H+N with +GD) and the noisiness of the POS tags, which is controlled by altering the amount of training data. With only 100 training sentences, the performance suffers greatly—the UAS drops from 70.65 to 51.57. Nonetheless, even this is comparable to Naseem et al. (2010) on gold POS tags, which yields a UAS of 50.00. That system was the first grammar induction approach to exploit knowledge of the distribution of natural languages, and remained state-of-the-"
Q18-1046,J08-4003,0,0.0423794,"lations of grammar induction involve optimizing a highly non-convex objective function such as likelihood. The optimization is typically NP-hard (Cohen and Smith, 2012), and approximate local search methods tend to get stuck in local optima. Introduction Dependency parsing is one of the core natural language processing tasks. It aims to parse a given sentence into its dependency tree: a directed graph of labeled syntactic relations between words. Supervised dependency parsers—which are trained using a “treebank” of known parses in the target language—have been very successful (McDonald, 2006; Nivre, 2008; Kiperwasser and Goldberg, 2016). By contrast, the progress • Model error: Likelihood does not correlate well with parsing accuracy anyway (Smith, 2006, Figure 3.2). Likelihood optimization seeks latent trees that help to predict the observed sentences, but these unsupervised trees may use a non-standard syntactic analysis or even be optimized to predict nonsyntactic properties such as topic. We seek a standard syntactic analysis—what Smith (2006) calls the M ATCH L INGUIST task. We address both difficulties by using a supervised learning framework—one whose objective function is easier to op"
Q18-1046,E17-1022,0,0.0300841,"Missing"
Q18-1046,D15-1039,0,0.300904,"l. (2001), and then applied to dependency parsing on low-resource languages (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014, inter alia). McDonald et al. (2011) extend this approach to multiple source languages by projected transfer. Later work in this vein mainly tries to improve the approximate parses, including translating the source treebanks into the target language with an off-the-shelf machine translation system (Tiedemann et al., 2014), augmenting the trees with weights (Agi´c et al., 2016), and using only partial trees with high-confidence alignments (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016). 5.4 The Typology Component T(u) = ψ(W π(u) + bW ) ∈ Rh (3) where π(u) ∈ Rd is the surface features of u, W ∈ Rh×d maps π(u) into a h-dimensional space, bW ∈ Rh is a bias vector, and ψ is an element-wise activation function. While equation (3) has only 1 layer, we explore versions with from 0 to 3 layers (where T(u) = π(u) in the 0-layer case). A 2-layer version is shown in Figure 1. The number of layers is chosen by crossvalidation, as are h and the ψ function. 6.1 Design of the Surface Features π(u) To define π(u), we used development data to select the followin"
Q18-1046,D16-1004,0,0.0152374,"ned on a subset of the original gold-tagged corpus.17 Figure 6 shows a linear relationship between the performance of our best model (H+N with +GD) and the noisiness of the POS tags, which is controlled by altering the amount of training data. With only 100 training sentences, the performance suffers greatly—the UAS drops from 70.65 to 51.57. Nonetheless, even this is comparable to Naseem et al. (2010) on gold POS tags, which yields a UAS of 50.00. That system was the first grammar induction approach to exploit knowledge of the distribution of natural languages, and remained state-of-the-art (Noji et al., 2016) until the work of Mareˇcek (2016) and Martínez Alonso et al. (2017). 9.7 Final Evaluation on Test Data In all previous sections, we evaluated on the 16 languages in the training set by cross-validation. For the final test, we combine all the 20 treebanks and train the system with the hyperparameters given in §9.5, then test on the 15 unseen test languages. Table 2 displays results on these 15 test languages (top) as well as the cross-validation results on the 16 languages (bottom). We see that we improve significantly over baseline on almost every language. Indeed, on the test languages, +T(u"
Q18-1046,Q17-1020,0,0.578363,"Missing"
Q18-1046,P15-2040,0,0.614388,"y, the set of POS-to-POS dependencies that are allowed by the UD annotation scheme, and the typical directions for some of these dependencies. The only corpus statistic extracted from u is whether ADPNOMINAL or NOMINAL-ADP bigrams are more frequent,6 which distinguishes prepositional from postpositional languages. The actual parser starts by identifying the head word as the most “central” word according to a PageRank (Page et al., 1999) analysis of the graph of candidate edges, and proceeds by greedily attaching words of decreasing PageRank at lower depths in the tree. 5.2 similarity measure (Rosa and Žabokrtský, 2015a) considers the probability of the target language’s POS-corpus u under a trigram language model of source-language POS sequences. Single-source transfer (SST) (Rosa and Žabokrtský, 2015a; Wang and Eisner, 2016) simply uses the parser for the most similar source treebank. Multi-source transfer (MST) (Rosa and Žabokrtský, 2015a) parses the target POS sequence with each of the source parsers, and then combines these parses into a consensus tree using the Chu-Liu-Edmonds algorithm (Chu, 1965; Edmonds, 1967). As a faster variant, model interpolation (Rosa and Žabokrtský, 2015b) builds a consensus"
Q18-1046,W15-2209,0,0.0844583,"y, the set of POS-to-POS dependencies that are allowed by the UD annotation scheme, and the typical directions for some of these dependencies. The only corpus statistic extracted from u is whether ADPNOMINAL or NOMINAL-ADP bigrams are more frequent,6 which distinguishes prepositional from postpositional languages. The actual parser starts by identifying the head word as the most “central” word according to a PageRank (Page et al., 1999) analysis of the graph of candidate edges, and proceeds by greedily attaching words of decreasing PageRank at lower depths in the tree. 5.2 similarity measure (Rosa and Žabokrtský, 2015a) considers the probability of the target language’s POS-corpus u under a trigram language model of source-language POS sequences. Single-source transfer (SST) (Rosa and Žabokrtský, 2015a; Wang and Eisner, 2016) simply uses the parser for the most similar source treebank. Multi-source transfer (MST) (Rosa and Žabokrtský, 2015a) parses the target POS sequence with each of the source parsers, and then combines these parses into a consensus tree using the Chu-Liu-Edmonds algorithm (Chu, 1965; Edmonds, 1967). As a faster variant, model interpolation (Rosa and Žabokrtský, 2015b) builds a consensus"
Q18-1046,D09-1086,1,0.836122,"rpus u, in effect predicting syntactic structure from superficial features. Like them, we compute a hidden layer T(u) using a standard multilayer perceptron architecture, for example, Annotation projection. Given aligned bitext, one can generate an approximate parse for a target sentence by “projecting” the parse tree of the corresponding source sentence. A target-language parser can then be trained from these approximate parses. The idea was originally proposed by Yarowsky et al. (2001), and then applied to dependency parsing on low-resource languages (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014, inter alia). McDonald et al. (2011) extend this approach to multiple source languages by projected transfer. Later work in this vein mainly tries to improve the approximate parses, including translating the source treebanks into the target language with an off-the-shelf machine translation system (Tiedemann et al., 2014), augmenting the trees with weights (Agi´c et al., 2016), and using only partial trees with high-confidence alignments (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016). 5.4 The Typology Component T(u) = ψ(W π(u) + bW ) ∈ Rh (3) where π(u) ∈ Rd is the s"
Q18-1046,P06-1072,1,0.72824,"Missing"
Q18-1046,D12-1063,0,0.0170738,"oach of the previous paragraph is often modified to reduce model error or search error (§1). To reduce model error, many papers have used dependency grammar, with training objectives that incorporate notions like lexical attraction (Yuret, 1998) and grammatical bigrams (Paskin, 2001, 2002). The dependency model with valence (DMV) (Klein and Manning, 2004) was the first 5 Wang and Eisner (2018) do investigate synthesis “on demand” of a permuted training corpus that is as similar as possible to the test corpus. 670 method to beat a simple right-branching heuristic. Headden III et al. (2009) and Spitkovsky et al. (2012) made the DMV more expressive by considering higher-order valency or punctuation. To reduce search error, strategies for eliminating or escaping local optima have included convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), smart initialization (Klein and Manning, 2004; Mareˇcek and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013). Unsupervised parsing (which is also our task) tries to turn the same corpus directly into"
Q18-1046,Q16-1035,1,0.158725,"timate D—which would embody a distribution over semantic content and a full theory of universal grammar! The GD treebanks were derived more simply and more conservatively by “interpolation” among the actual UD corpora. They combine observed parse trees (which provide attested semantic content) with stochastic word order models trained on observed languages (which attempt to mimic attested patterns for presenting that content). GD’s sampling ˆ still offers moderately varied syndistribution D thetic datasets, which remain moderately realistic, as they are limited to phenomena observed in UD. As Wang and Eisner (2016) pointed out, synthetic examples have been used in many other supervised machine learning settings. A common technique is to exploit invariance: if real image z should be classified as a cat, then so should a rotated version of image z. Our technique is the same! We assume that if real corpus u should be parsed as having certain dependencies among the word tokens, then so should a version of corpus u in which those tokens have been systematically permuted in a linguistically plausible way.4 This is analogous to how rotation sytematically transforms the image (rotating all pixels through the sa"
Q18-1046,D13-1204,0,0.0166943,"orpus. 670 method to beat a simple right-branching heuristic. Headden III et al. (2009) and Spitkovsky et al. (2012) made the DMV more expressive by considering higher-order valency or punctuation. To reduce search error, strategies for eliminating or escaping local optima have included convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), smart initialization (Klein and Manning, 2004; Mareˇcek and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013). Unsupervised parsing (which is also our task) tries to turn the same corpus directly into a treebank, without necessarily finding a grammar. We discuss some recent milestones here. Grave and Elhadad (2015) propose a transductive learning objective for unsupervised parsing, and a convex relaxation of it. (Jiang et al. (2017) combined that work with grammar induction.) Martínez Alonso et al. (2017) create an unsupervised dependency parser that is formally similar to ours in that it uses cross-linguistic knowledge as well as statistics computed from a corpus of POS sequences in the target langu"
Q18-1046,Q17-1011,1,0.731535,"thout any labeled examples in that language. 3 of resource-rich synthetic languages aims to propel the development of NLP systems that can handle diverse natural languages, such as multilingual parsers and taggers. 3.1 Why Synthetic Training Languages? We hope for our system to do well, on average, at matching real linguist-parsed corpora of real human languages. We therefore tune its parameters Θ on such treebanks. UD provides training examples actually drawn from that distribution D over treebanks—but alas, rather few. Thus to better estimate the expected performance of Θ under D, we follow Wang and Eisner (2017) and augment our training data with GD’s synthetic treebanks. Ideally we would have sampled these synthetic ˆ of D: for extreebanks from a careful estimate D ample, the mean of a Bayesian posterior for D, derived from prior assumptions and UD evidence. However, such adventurous “extrapolation” of unseen languages would have required actually conˆ structing such an estimate D—which would embody a distribution over semantic content and a full theory of universal grammar! The GD treebanks were derived more simply and more conservatively by “interpolation” among the actual UD corpora. They combine"
Q18-1046,D18-1163,1,0.798639,"ke if English had been both VSO (like Irish) and postpositional (like Japanese). This typologically diverse collection 3 While it might have been preferable to use the expanded and revised UD version 2.0, we wished to compare fairly with GD 1.0, which is based on UD 1.2. 4 669 Another example is back-translation. thetic data is feasible. In our case, the synthetic corpus then provides many sentences that have been similarly permuted, which may jointly provide enough clues to guess the word order of this synthetic language (for example, VSO vs. VOS in §2) and thus recover the dependencies. See Wang and Eisner (2018, §2) for related discussion. With enough good synthetic languages to use for training, even nearest-neighbor could be an effective method. That is, one could obtain the parser for a test corpus simply by copying the trained parser for the most similar training corpus (under some metric). Wang and Eisner (2016) explored this approach of “single-source transfer” from synthetic languages. Yet with only thousands of synthetic languages, perhaps no single training corpus is sufficiently similar.5 To draw on patterns in many training corpora to figure out how to parse the test corpus, we will train"
Q18-1046,Q13-1001,0,0.264081,"method trains a single language-agnostic model. McDonald et al. (2011) train a delexicalized parser on the concatenation of all source treebanks, achieving a large gain over grammar induction. This parser can learn universals such as the preference for determiners to attach to nouns (which was hard-coded by Naseem et al. (2010)). However, it is expected to parse a sentence x without being told the language ` or even a corpus u, possibly by guessing properties of the language from the configurations it encounters in the single sentence x alone. Further gains were achieved (Naseem et al., 2012; Täckström et al., 2013b; Zhang and Barzilay, 2015; Ammar et al., 2016) by providing the parser with about 10 typological properties of x’s language—for example, whether direct objects generally fall to the right of the verb—as listed in the World Atlas of Linguistic Structures (Dryer and Haspelmath, 2013). However, relying on WALS raises some issues. (1) The unknown language might not be in WALS.7 (2) Some typological features are missing for some languages. (3) All the WALS features are categorical values, which loses useful information about tendencies (for example, how often the canonical word order is violated)"
Q18-1046,P08-1061,0,0.0282988,"attraction (Yuret, 1998) and grammatical bigrams (Paskin, 2001, 2002). The dependency model with valence (DMV) (Klein and Manning, 2004) was the first 5 Wang and Eisner (2018) do investigate synthesis “on demand” of a permuted training corpus that is as similar as possible to the test corpus. 670 method to beat a simple right-branching heuristic. Headden III et al. (2009) and Spitkovsky et al. (2012) made the DMV more expressive by considering higher-order valency or punctuation. To reduce search error, strategies for eliminating or escaping local optima have included convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), smart initialization (Klein and Manning, 2004; Mareˇcek and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013). Unsupervised parsing (which is also our task) tries to turn the same corpus directly into a treebank, without necessarily finding a grammar. We discuss some recent milestones here. Grave and Elhadad (2015) propose a transductive learning objective for unsupervised parsing, and a convex relaxation of it"
Q18-1046,N13-1126,0,0.689507,"method trains a single language-agnostic model. McDonald et al. (2011) train a delexicalized parser on the concatenation of all source treebanks, achieving a large gain over grammar induction. This parser can learn universals such as the preference for determiners to attach to nouns (which was hard-coded by Naseem et al. (2010)). However, it is expected to parse a sentence x without being told the language ` or even a corpus u, possibly by guessing properties of the language from the configurations it encounters in the single sentence x alone. Further gains were achieved (Naseem et al., 2012; Täckström et al., 2013b; Zhang and Barzilay, 2015; Ammar et al., 2016) by providing the parser with about 10 typological properties of x’s language—for example, whether direct objects generally fall to the right of the verb—as listed in the World Atlas of Linguistic Structures (Dryer and Haspelmath, 2013). However, relying on WALS raises some issues. (1) The unknown language might not be in WALS.7 (2) Some typological features are missing for some languages. (3) All the WALS features are categorical values, which loses useful information about tendencies (for example, how often the canonical word order is violated)"
Q18-1046,H01-1035,0,0.17726,"(bW ) are suppressed for readability. 6 Wang and Eisner (2017) extract typological properties of a language from its POS-tagged corpus u, in effect predicting syntactic structure from superficial features. Like them, we compute a hidden layer T(u) using a standard multilayer perceptron architecture, for example, Annotation projection. Given aligned bitext, one can generate an approximate parse for a target sentence by “projecting” the parse tree of the corresponding source sentence. A target-language parser can then be trained from these approximate parses. The idea was originally proposed by Yarowsky et al. (2001), and then applied to dependency parsing on low-resource languages (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014, inter alia). McDonald et al. (2011) extend this approach to multiple source languages by projected transfer. Later work in this vein mainly tries to improve the approximate parses, including translating the source treebanks into the target language with an off-the-shelf machine translation system (Tiedemann et al., 2014), augmenting the trees with weights (Agi´c et al., 2016), and using only partial trees with high-confidence alignments (Rasooli a"
Q18-1046,W04-3201,0,0.0314258,"des a few dozen languages—presumably not enough to generalize well to novel languages. We therefore augment our training dataset Ltrain with thousands of synthetic languages from the GD dataset (§3), as already discussed in §3.1. (8) where sH (a) and sN (a) are the scores produced by separately trained systems using, respectively, the hand-engineered and neural features from §6.1. Hyperparameter λ ∈ [0, 1] is tuned on dev data. 8 (9) Training the System Training Objective We exactly follow the training method of Kiperwasser and Goldberg (2016), who minimize a structured max-margin hinge loss (Taskar et al., 2004; McDonald et al., 2005; LeCun et al., 2007). Stochastic gradient descent (SGD).10 Treating each language as a single large example during training would lead to slow SGD steps. Instead, we take our SGD examples to be individual sentences, by regarding equations (1)–(2) together as 8 An alternative would be to concatenate T(u) with the representation computed by the BiLSTM. This gets empirically worse results, probably because the BiLSTM does not have advance knowledge of language-specific word order as it reads the sentence. We also tried an architecture that does both, with no notable improv"
Q18-1046,D15-1213,0,0.571573,"nguage-agnostic model. McDonald et al. (2011) train a delexicalized parser on the concatenation of all source treebanks, achieving a large gain over grammar induction. This parser can learn universals such as the preference for determiners to attach to nouns (which was hard-coded by Naseem et al. (2010)). However, it is expected to parse a sentence x without being told the language ` or even a corpus u, possibly by guessing properties of the language from the configurations it encounters in the single sentence x alone. Further gains were achieved (Naseem et al., 2012; Täckström et al., 2013b; Zhang and Barzilay, 2015; Ammar et al., 2016) by providing the parser with about 10 typological properties of x’s language—for example, whether direct objects generally fall to the right of the verb—as listed in the World Atlas of Linguistic Structures (Dryer and Haspelmath, 2013). However, relying on WALS raises some issues. (1) The unknown language might not be in WALS.7 (2) Some typological features are missing for some languages. (3) All the WALS features are categorical values, which loses useful information about tendencies (for example, how often the canonical word order is violated). (4) Not all WALS features"
Q18-1046,C14-1175,0,0.0691369,"ting syntactic structure from superficial features. Like them, we compute a hidden layer T(u) using a standard multilayer perceptron architecture, for example, Annotation projection. Given aligned bitext, one can generate an approximate parse for a target sentence by “projecting” the parse tree of the corresponding source sentence. A target-language parser can then be trained from these approximate parses. The idea was originally proposed by Yarowsky et al. (2001), and then applied to dependency parsing on low-resource languages (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014, inter alia). McDonald et al. (2011) extend this approach to multiple source languages by projected transfer. Later work in this vein mainly tries to improve the approximate parses, including translating the source treebanks into the target language with an off-the-shelf machine translation system (Tiedemann et al., 2014), augmenting the trees with weights (Agi´c et al., 2016), and using only partial trees with high-confidence alignments (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016). 5.4 The Typology Component T(u) = ψ(W π(u) + bW ) ∈ Rh (3) where π(u) ∈ Rd is the surface features o"
Q18-1046,W14-1614,0,0.22237,"Missing"
Q19-1021,J92-1002,0,0.61358,"Missing"
Q19-1021,K18-3001,1,0.871881,"ge word type is observed too rarely for a learner to memorize an irregular surface form for it. Yet even in such a language, some word types are frequent, because some lexemes and some slots are especially useful. Thus, if learnability of the lexicon is indeed the driving force,13 then we should make the finer-grained prediction that irregularity may survive in the more frequently observed word types, regardless of paradigm size. Rarer forms are more likely to be predictable—meaning that they are either regular, or else irregular in a way that is predictable from a related frequent irregular (Cotterell et al., 2018a). perform a nonparametric permutation test that destroys the claimed correlation between the e-complexity and i-complexity values. From our observed points {(x1 , y1 ), . . . , (xm , ym )}, we can stochastically construct a new set of points {(x1 , yσ(1) ), . . . , (xm , yσ(m) )} where σ is a permutation of 1, 2, . . . , m selected uniformly at random. The resulting scatterplot is what we would expect under the null hypothesis of no correlation. Our p-value is the probability that the new scatterplot has an even emptier upper righthand corner—that is, the probability that the area under the"
Q19-1021,K17-2001,1,0.929128,"Missing"
Q19-1021,D07-1093,0,0.124261,"Missing"
Q19-1021,P14-2102,1,0.839978,"mitigate sample bias caused by variable-sized dictionaries in our database. In many languages, irregular words are also very frequent and may be more likely to be included in a dictionary first. If that’s the case, smaller dictionaries might have lexical statistics skewed toward irregulars more so than larger dictionaries. In general, larger dictionaries should be more representative samples of a language’s broader lexicon. 10 In the computer science literature, it is far more common to construct distributions with support over Σ∗ (Paz, 2003; Bouchard-Cˆot´e et al., 2007; Dreyer et al., 2008; Cotterell et al., 2014), which do not have this problem. 336 tween paradigm slots. We call this the ‘‘green scheme.’’ it is difficult to use them in a lower-resource scenario. To estimate a language’s e-complexity (§2.2.1), we average over all paradigms in the UniMorph inflected lexicon. To estimate i-complexity, we first partition those paradigms into training, development and test sets. We identify the paradigm shapes from the training set (§4.1). We also use the training set to train the parameters θ of our conditional distribution (§4.3), then estimate conditional entropies on the development set and use Edmonds"
Q19-1021,J86-2003,0,0.451974,"for Archi, for example). Our hypothesis is subtly different in that we postulate that morphological systems face a trade-off between e-complexity and i-complexity: a system may be complex under either metric, but not under both. The amount of e-complexity permitted is higher when i-complexity is low. This line of thinking harks back to the equal complexity conjecture of Hockett, who stated: ‘‘objective measurement is difficult, but impressionistically it would seem that the total grammatical complexity of any language, counting both the morphology and syntax, is about the same as any other’’ (Hockett, 1958, pp. 180–181). Similar trade-offs have been found in other branches of linguistics (see Oh [2015] for a review). For example, there is a trade-off between rate of speech and syllable complexity (Pellegrino et al., 2011): This means that even though Spanish speakers utter many more syllables per second than Chinese, the overall information rate is quite similar as Chinese syllables carry more information (they contain tone information). Hockett’s equal complexity conjecture is controversial: some languages (such as Riau Indonesian) do seem low in complexity across morphology and syntax (Gil, 1"
Q19-1021,Q15-1031,1,0.850215,"nese, the overall information rate is quite similar as Chinese syllables carry more information (they contain tone information). Hockett’s equal complexity conjecture is controversial: some languages (such as Riau Indonesian) do seem low in complexity across morphology and syntax (Gil, 1994). This is why Ackerman and Malouf instead posit that a linguistic system has bounded integrative complexity—it must not be too high, though it can be low, as indeed it is in isolating languages like Chinese and Thai. 3 3.1 Paradigm Entropy Morphology as a Distribution Following Dreyer and Eisner (2009) and Cotterell et al. (2015), we identify a language’s inflectional system with a probability distribution p(M = m) 329 over possible paradigms.4 Our measure of i-complexity will be related to the entropy of this distribution. For instance, knowing the behavior of the English verb system essentially means knowing a joint distribution over 5-tuples of surface forms such as (run, runs, ran, run, running). More precisely, one knows probabilities such as p(M .pres = run, M .3s = runs, M .past = ran, M .pastp = run, M .presp = running). We do not observe p directly, but each observed paradigm (5-tuple) can help us estimate it"
Q19-1021,P16-2090,0,0.0965679,"Missing"
Q19-1021,E17-2120,1,0.94135,"r setting, both the training and the test examples are paradigms from a given inflected lexicon. 4 A Generative Model of the Paradigm To fit q given the training set, we need a tractable family Q of joint distributions over paradigms, with parameters θ . The structure of the model and the number of parameters θ will be determined automatically from the training set: A language with more slots overall or more paradigm shapes will require more parameters. This means that Q is technically a semi-parametric family. 4.1 4.2 A Tree-Structured Distribution Next, conditioned on the shape s, we follow Cotterell et al. (2017b) and generate all the forms of the paradigm using a tree-structured Bayesian network—a directed graphical model in which the form at each slot is generated conditionally on the form at a single parent slot. Figure 1 illustrates two possible tree structures for Spanish verbs. Each paradigm shape s has its own tree structure. If slot σ exists in shape s, we denote its Paradigm Shapes We say that two paradigms m, m0 have the same shape if they define the same slots (that is, domain(m) = domain(m0 )) and the same pairs of slots are syncretic in both paradigms (that is, 331 parent in our shape s"
Q19-1021,D17-1074,1,0.932816,"r setting, both the training and the test examples are paradigms from a given inflected lexicon. 4 A Generative Model of the Paradigm To fit q given the training set, we need a tractable family Q of joint distributions over paradigms, with parameters θ . The structure of the model and the number of parameters θ will be determined automatically from the training set: A language with more slots overall or more paradigm shapes will require more parameters. This means that Q is technically a semi-parametric family. 4.1 4.2 A Tree-Structured Distribution Next, conditioned on the shape s, we follow Cotterell et al. (2017b) and generate all the forms of the paradigm using a tree-structured Bayesian network—a directed graphical model in which the form at each slot is generated conditionally on the form at a single parent slot. Figure 1 illustrates two possible tree structures for Spanish verbs. Each paradigm shape s has its own tree structure. If slot σ exists in shape s, we denote its Paradigm Shapes We say that two paradigms m, m0 have the same shape if they define the same slots (that is, domain(m) = domain(m0 )) and the same pairs of slots are syncretic in both paradigms (that is, 331 parent in our shape s"
Q19-1021,L18-1293,1,0.883669,"Missing"
Q19-1021,D09-1011,1,0.762439,"syllables per second than Chinese, the overall information rate is quite similar as Chinese syllables carry more information (they contain tone information). Hockett’s equal complexity conjecture is controversial: some languages (such as Riau Indonesian) do seem low in complexity across morphology and syntax (Gil, 1994). This is why Ackerman and Malouf instead posit that a linguistic system has bounded integrative complexity—it must not be too high, though it can be low, as indeed it is in isolating languages like Chinese and Thai. 3 3.1 Paradigm Entropy Morphology as a Distribution Following Dreyer and Eisner (2009) and Cotterell et al. (2015), we identify a language’s inflectional system with a probability distribution p(M = m) 329 over possible paradigms.4 Our measure of i-complexity will be related to the entropy of this distribution. For instance, knowing the behavior of the English verb system essentially means knowing a joint distribution over 5-tuples of surface forms such as (run, runs, ran, run, running). More precisely, one knows probabilities such as p(M .pres = run, M .3s = runs, M .past = ran, M .pastp = run, M .presp = running). We do not observe p directly, but each observed paradigm (5-tu"
Q19-1021,P17-4012,0,0.0417736,"Missing"
Q19-1021,D11-1057,1,0.856952,") from p. Any novel verb paradigm in the future would be drawn from p as well. The distribution p represents the inflectional system because it describes what regular paradigms and plausible irregular paradigms tend to look like. The fact that some paradigms are used more frequently than others (more tokens in a corpus) does not mean that they have higher probability under the morphological system p(m). Rather, their higher usage reflects the higher probability of their lexemes. That is due to unrelated factors—the probability of a lexeme may be modeled separately by a stick-breaking process (Dreyer and Eisner, 2011), or may reflect the semantic meaning associated to that lexeme. The role of p(m) in the model is only to serve as the base distribution from which a lexeme type ` selects the tuple of strings m = M (`) that will be used thereafter to express `. We expect the system to place low probability on implausible paradigms: For example, p(run, , , run, running) is close to zero. Moreover, we expect it to assign high conditional probability to the result of applying highly regular processes: For example, for p(M .presp |M .3s) in English, we have p(wugging |wugs) ≈ p(running |runs) ≈ 1, where wug is a"
Q19-1021,D08-1113,1,0.711343,"ges should also help mitigate sample bias caused by variable-sized dictionaries in our database. In many languages, irregular words are also very frequent and may be more likely to be included in a dictionary first. If that’s the case, smaller dictionaries might have lexical statistics skewed toward irregulars more so than larger dictionaries. In general, larger dictionaries should be more representative samples of a language’s broader lexicon. 10 In the computer science literature, it is far more common to construct distributions with support over Σ∗ (Paz, 2003; Bouchard-Cˆot´e et al., 2007; Dreyer et al., 2008; Cotterell et al., 2014), which do not have this problem. 336 tween paradigm slots. We call this the ‘‘green scheme.’’ it is difficult to use them in a lower-resource scenario. To estimate a language’s e-complexity (§2.2.1), we average over all paradigms in the UniMorph inflected lexicon. To estimate i-complexity, we first partition those paradigms into training, development and test sets. We identify the paradigm shapes from the training set (§4.1). We also use the training set to train the parameters θ of our conditional distribution (§4.3), then estimate conditional entropies on the develo"
Q19-1021,P15-2111,1,0.923479,"Missing"
Q19-1021,W16-2002,1,\N,Missing
Q19-1023,P14-2102,1,0.830857,"the chosen edit. At each step it selects the highest-priority local rewrite rule that can apply, and applies it as far left as possible. When no more rules can apply, the final state of the string is returned as x. Simplifying the formalism Markov algorithms are Turing complete. Fortunately, Johnson (1972) noted that in practice, phonological u 7! x maps described in this formalism can usually be implemented with finite-state transducers (FSTs). For computational simplicity, we will formulate our punctuation model as a probabilistic FST (PFST)—a locally normalized left-to-right rewrite model (Cotterell et al., 2014). The probabilities for each language must be learned, using gradient descent. Normally we expect most probabilities to be near 0 or 1, making the PFST nearly deterministic (i.e., close to a subsequential FST). However, permitting low-probability choices remains useful to account for typographical errors, dialectal differences, and free variation in the training corpus. Our PFST generates a surface string, but the invertibility of FSTs will allow us to work backwards when analyzing a surface string (§3). 2.3 Training Objective Building on equation (2), we train ✓, to locally maximize the regul"
Q19-1023,Q15-1031,1,0.841055,"d machine translation (Zens et al., 2002), punctuation marks are usually either removed or treated as ˇ uˇrek and Sojka, 2010). ordinary words (Reh˚ Yet to us, building a parse tree on a surface sentence seems as inappropriate as morphologically segmenting a surface word. In both cases, one should instead analyze the latent underlying form, jointly with recovering that form. For example, the proper segmentation of English hoping is not hop-ing but hope-ing (with underlying e), and the proper segmentation of stopping is neither stopp-ing nor stop-ping but stop-ing (with only one underlying p). Cotterell et al. (2015, 2016) get this right for morphology. We attempt to do the same for punctuation. Punctuation is helpful Surface punctuation remains correlated with syntactic phrase structure. NLP systems for generating or editing text must be able to deploy surface punctuation as human writers do. Parsers and grammar induction systems benefit from the presence of surface punctuation marks (Jones, 1994; Spitkovsky et al., 2011). It is plausible that they could do better with a linguistically informed model that explains exactly why the surface punctuation appears where it does. Patterns of punctuation usage c"
Q19-1023,W11-0705,0,0.0298466,"2 T , the ATTACH process stochastically attaches a left puncteme l and a right puncteme r, which may be empty. The resulting tree T 0 has underlying punctuation u. Each slot’s punctuation ui 2 u is rewritten to xi 2 x by N OISY C HANNEL. Punctuation is meaningful Pang et al. (2002) use question and exclamation marks as clues to sentiment. Similarly, quotation marks may be used to mark titles, quotations, reported speech, or dubious terminology (University of Chicago, 2010). Because of examples like this, methods for determining the similarity or meaning of syntax trees, such as a tree kernel (Agarwal et al., 2011) or a recursive neural network (Tai et al., 2015), should ideally be able to consider where the underlying punctuation marks attach. In tasks such as word embedding induction (Mikolov et al., 2013; Pennington et al., 2014) and machine translation (Zens et al., 2002), punctuation marks are usually either removed or treated as ˇ uˇrek and Sojka, 2010). ordinary words (Reh˚ Yet to us, building a parse tree on a surface sentence seems as inappropriate as morphologically segmenting a surface word. In both cases, one should instead analyze the latent underlying form, jointly with recovering that for"
Q19-1023,N16-1080,0,0.0583243,"Missing"
Q19-1023,D16-1111,0,0.0172702,"s to segment a sentence: this &quot;divide and rule&quot; strategy reduces ambiguity in parsing of long Chinese sentences. Punctuation can similarly be used to constrain syntactic structure during grammar induction (Spitkovsky et al., 2011). Punctuation restoration (§ 6.1) is useful for transcribing text from unpunctuated speech. The task is usually treated by tagging each slot with zero or more punctuation tokens, using a traditional sequence labeling method: conditional random fields (Lui and Wang, 2013; Lu and Ng, 2010), recurrent neural networks (Tilk and Alumäe, 2016), or transition-based systems (Ballesteros and Wanner, 2016). Full 553.4 78.7 75.4 91.8 239.3 Table 6: Perplexity (evaluated on the train split to avoid evaluating generalization) of a trigram language model trained (with add-0.001 smoothing) on different versions of rephrased training sentences. “Punctuation” only evaluates perplexity on the trigrams that have punctuation. “All” evaluates on all the trigrams. “Base” permutes all surface dependents including punctuation (Wang and Eisner, 2016). “Full” is our full approach: recover underlying punctuation, permute remaining dependents, regenerate surface punctuation. “Half” is like “Full” but it permutes"
Q19-1023,W06-1628,0,0.127554,"Missing"
Q19-1023,P04-1054,0,0.0123749,"berg’s (1990) formal grammar for English punctuation, but is probabilistic and trainable. We give exact algorithms for training and inference. We trained Nunberg-like models for 5 languages and L2 English. We compared the English model to Nunberg’s, and showed how the trained models can be used across languages for punctuation restoration, correction, and adjustment. In the future, we would like to study the usefulness of the recovered underlying trees on tasks such as syntactically sensitive sentiment analysis (Tai et al., 2015), machine translation (Cowan et al., 2006), relation extraction (Culotta and Sorensen, 2004), and coreference resolution (Kong et al., 2010). We would also like to investigate how underlying punctuation could aid parsing. For discriminative parsing, features for scoring the tree could refer to the underlying punctuation, not just the surface punctuation. For generative parsing (§3), we could follow the ˆthe caper failed ,If true ,. We ˆthe caper failed ,If true . leave the handling of capitalization to future work. We test the naturalness of the permuted sentences by asking how well a word trigram language model trained on them could predict the original sentences.23 As shown in Tabl"
Q19-1023,N16-1024,0,0.0706246,"Missing"
Q19-1023,C94-1069,0,0.472677,"r segmentation of English hoping is not hop-ing but hope-ing (with underlying e), and the proper segmentation of stopping is neither stopp-ing nor stop-ping but stop-ing (with only one underlying p). Cotterell et al. (2015, 2016) get this right for morphology. We attempt to do the same for punctuation. Punctuation is helpful Surface punctuation remains correlated with syntactic phrase structure. NLP systems for generating or editing text must be able to deploy surface punctuation as human writers do. Parsers and grammar induction systems benefit from the presence of surface punctuation marks (Jones, 1994; Spitkovsky et al., 2011). It is plausible that they could do better with a linguistically informed model that explains exactly why the surface punctuation appears where it does. Patterns of punctuation usage can also help identify the writer’s native language (Markov et al., 2018). 2 Formal Model We propose a probabilistic generative model of sentences (Figure 1): (1) P ¯ (T 0 )) p(¯ x) = T,T 0 psyn (T ) · p✓ (T 0 |T ) · p (¯ x|u Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles"
Q19-1023,C96-1058,1,0.279961,"reebank annotation guidelines for punctuation tend to adopt simple heuristics like “attach to the highest possible node that preserves projectivity” (Bies et al., 1995; Nivre et al., 2018).1 Many dependency parsing works exclude punctuation from evaluation (Nivre et al., 2007b; Koo and Collins, 2010; Chen and Manning, 2014; Lei et al., 2014; Kiperwasser and Goldberg, 2016), although some others retain punctuation (Nivre et al., 2007a; Goldberg and Elhadad, 2010; Dozat and Manning, 2017). First, an unpunctuated dependency tree T is stochastically generated by some recursive process psyn (e.g., Eisner, 1996, Model C).2 Second, each constituent (i.e., dependency subtree) sprouts optional underlying punctuation at its left and right edges, according to a probability distribution p✓ that depends on the constituent’s syntactic role (e.g., dobj for “direct object”). This punctuated ¯ = u ¯ (T 0 ), tree T 0 yields the underlying string u which is edited by a finite-state noisy channel p ¯. to arrive at the surface sentence x 1 http://universaldependencies.org/u/ dep/punct.html 2 Our model could be easily adapted to work on constituency trees instead. 358 1. Point Absorption 3. Period Absorption „ 7! ,"
Q19-1023,W16-5901,1,0.9227,"erlying punctemes at w and all its descendants. These punctemes determine an s-to-final path at i, then initial-to-final paths at i + 1 through k 1, then an initial-to-t path at k. The weight of the extended path is the product of all the WFSA weights on these paths (which correspond to transition probabilities in p PFST) times the probability of the choice of punctemes (from p✓ ). This inside algorithm computes quantities needed for training (§ 2.3). Useful variants arise via well-known methods for weighted derivation forests (Berstel and Reutenauer, 1988; Goodman, 1999; Li and Eisner, 2009; Eisner, 2016). Specifically, to modify Algorithm 1 to maximize over T 0 values (§§ 6.2–6.3) instead of summing over them, we switch to the derivation semiring (Goodman, 1999), as follows. Whereas I N(w)st used to store the total weight of all extended paths from state s at slot i to state t at slot j, now it will store the weight of the best such extended path. It will also store that extended path’s choice of underlying punctemes, in the form of a punctemeannotated version of the subtree of T that is rooted at w. This is a potential subtree of T 0 . Thus, each element of I N(w) has the form (r, D) where r"
Q19-1023,Q16-1023,0,0.0187934,"sentences (Figure 1): (1) P ¯ (T 0 )) p(¯ x) = T,T 0 psyn (T ) · p✓ (T 0 |T ) · p (¯ x|u Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles. Treebank annotation guidelines for punctuation tend to adopt simple heuristics like “attach to the highest possible node that preserves projectivity” (Bies et al., 1995; Nivre et al., 2018).1 Many dependency parsing works exclude punctuation from evaluation (Nivre et al., 2007b; Koo and Collins, 2010; Chen and Manning, 2014; Lei et al., 2014; Kiperwasser and Goldberg, 2016), although some others retain punctuation (Nivre et al., 2007a; Goldberg and Elhadad, 2010; Dozat and Manning, 2017). First, an unpunctuated dependency tree T is stochastically generated by some recursive process psyn (e.g., Eisner, 1996, Model C).2 Second, each constituent (i.e., dependency subtree) sprouts optional underlying punctuation at its left and right edges, according to a probability distribution p✓ that depends on the constituent’s syntactic role (e.g., dobj for “direct object”). This punctuated ¯ = u ¯ (T 0 ), tree T 0 yields the underlying string u which is edited by a finite-sta"
Q19-1023,C10-1068,0,0.0214641,"is probabilistic and trainable. We give exact algorithms for training and inference. We trained Nunberg-like models for 5 languages and L2 English. We compared the English model to Nunberg’s, and showed how the trained models can be used across languages for punctuation restoration, correction, and adjustment. In the future, we would like to study the usefulness of the recovered underlying trees on tasks such as syntactically sensitive sentiment analysis (Tai et al., 2015), machine translation (Cowan et al., 2006), relation extraction (Culotta and Sorensen, 2004), and coreference resolution (Kong et al., 2010). We would also like to investigate how underlying punctuation could aid parsing. For discriminative parsing, features for scoring the tree could refer to the underlying punctuation, not just the surface punctuation. For generative parsing (§3), we could follow the ˆthe caper failed ,If true ,. We ˆthe caper failed ,If true . leave the handling of capitalization to future work. We test the naturalness of the permuted sentences by asking how well a word trigram language model trained on them could predict the original sentences.23 As shown in Table 6, our permutation approach reduces the perple"
Q19-1023,P10-1001,0,0.0145622,"). 2 Formal Model We propose a probabilistic generative model of sentences (Figure 1): (1) P ¯ (T 0 )) p(¯ x) = T,T 0 psyn (T ) · p✓ (T 0 |T ) · p (¯ x|u Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles. Treebank annotation guidelines for punctuation tend to adopt simple heuristics like “attach to the highest possible node that preserves projectivity” (Bies et al., 1995; Nivre et al., 2018).1 Many dependency parsing works exclude punctuation from evaluation (Nivre et al., 2007b; Koo and Collins, 2010; Chen and Manning, 2014; Lei et al., 2014; Kiperwasser and Goldberg, 2016), although some others retain punctuation (Nivre et al., 2007a; Goldberg and Elhadad, 2010; Dozat and Manning, 2017). First, an unpunctuated dependency tree T is stochastically generated by some recursive process psyn (e.g., Eisner, 1996, Model C).2 Second, each constituent (i.e., dependency subtree) sprouts optional underlying punctuation at its left and right edges, according to a probability distribution p✓ that depends on the constituent’s syntactic role (e.g., dobj for “direct object”). This punctuated ¯ = u ¯ (T 0"
Q19-1023,N10-1115,0,0.0217842,"ation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles. Treebank annotation guidelines for punctuation tend to adopt simple heuristics like “attach to the highest possible node that preserves projectivity” (Bies et al., 1995; Nivre et al., 2018).1 Many dependency parsing works exclude punctuation from evaluation (Nivre et al., 2007b; Koo and Collins, 2010; Chen and Manning, 2014; Lei et al., 2014; Kiperwasser and Goldberg, 2016), although some others retain punctuation (Nivre et al., 2007a; Goldberg and Elhadad, 2010; Dozat and Manning, 2017). First, an unpunctuated dependency tree T is stochastically generated by some recursive process psyn (e.g., Eisner, 1996, Model C).2 Second, each constituent (i.e., dependency subtree) sprouts optional underlying punctuation at its left and right edges, according to a probability distribution p✓ that depends on the constituent’s syntactic role (e.g., dobj for “direct object”). This punctuated ¯ = u ¯ (T 0 ), tree T 0 yields the underlying string u which is edited by a finite-state noisy channel p ¯. to arrive at the surface sentence x 1 http://universaldependencies.o"
Q19-1023,P14-1130,0,0.0144485,"nerative model of sentences (Figure 1): (1) P ¯ (T 0 )) p(¯ x) = T,T 0 psyn (T ) · p✓ (T 0 |T ) · p (¯ x|u Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles. Treebank annotation guidelines for punctuation tend to adopt simple heuristics like “attach to the highest possible node that preserves projectivity” (Bies et al., 1995; Nivre et al., 2018).1 Many dependency parsing works exclude punctuation from evaluation (Nivre et al., 2007b; Koo and Collins, 2010; Chen and Manning, 2014; Lei et al., 2014; Kiperwasser and Goldberg, 2016), although some others retain punctuation (Nivre et al., 2007a; Goldberg and Elhadad, 2010; Dozat and Manning, 2017). First, an unpunctuated dependency tree T is stochastically generated by some recursive process psyn (e.g., Eisner, 1996, Model C).2 Second, each constituent (i.e., dependency subtree) sprouts optional underlying punctuation at its left and right edges, according to a probability distribution p✓ that depends on the constituent’s syntactic role (e.g., dobj for “direct object”). This punctuated ¯ = u ¯ (T 0 ), tree T 0 yields the underlying string"
Q19-1023,J99-4004,0,0.0312395,"d path is defined by a choice of underlying punctemes at w and all its descendants. These punctemes determine an s-to-final path at i, then initial-to-final paths at i + 1 through k 1, then an initial-to-t path at k. The weight of the extended path is the product of all the WFSA weights on these paths (which correspond to transition probabilities in p PFST) times the probability of the choice of punctemes (from p✓ ). This inside algorithm computes quantities needed for training (§ 2.3). Useful variants arise via well-known methods for weighted derivation forests (Berstel and Reutenauer, 1988; Goodman, 1999; Li and Eisner, 2009; Eisner, 2016). Specifically, to modify Algorithm 1 to maximize over T 0 values (§§ 6.2–6.3) instead of summing over them, we switch to the derivation semiring (Goodman, 1999), as follows. Whereas I N(w)st used to store the total weight of all extended paths from state s at slot i to state t at slot j, now it will store the weight of the best such extended path. It will also store that extended path’s choice of underlying punctemes, in the form of a punctemeannotated version of the subtree of T that is rooted at w. This is a potential subtree of T 0 . Thus, each element o"
Q19-1023,D08-1025,0,0.0424338,"l)M0 Mk (r) 15: E E + p · l,r have unmatched punc Inference 17: In principle, working with the model (1) is straightforward, thanks to the closure properties of formal languages. Provided that psyn can be encoded as a weighted CFG, it can be composed with the weighted tree transducer p✓ and the weighted FST p to yield a new weighted CFG (similarly to Bar-Hillel et al., 1961; Nederhof and Satta, 2003). Under this new grammar, one can recover the opti¯ by dynamic programming, or sum mal T, T 0 for x over T, T 0 by the inside algorithm to get the likelihood p(¯ x). A similar approach was used by Levy (2008) with a different FST noisy channel. In this paper we assume that T is observed, allowing us to work with equation (2). This cuts the computation time from O(n3 ) to O(n).9 Whereas the inside algorithm for (1) must consider O(n2 ) ¯ and O(n) ways of buildpossible constituents of x ing each, our algorithm for (2) only needs to iterate over the O(n) true constituents of T and the 1 true way of building each. However, it must still consider the |Wd |puncteme pairs for each constituent. 18: 3.1 return M Mroot I N(root(T )) > return 0 Mroot ⇢n , E . RNi ⇥Nk . R, R Algorithms ¯ of length n, our job"
Q19-1023,W03-3016,0,0.10159,"headword 8: Mleft ( w0 2leftkids(w) I N(w0 ))⇢j 1 > Q 0 9: Mright j ( w0 2rightkids(w) I N (w )) 10: M0 Mleft · 1 · Mright . RNj ⇥1 , R1⇥Nj 11: M 0 . RNi ⇥Nk 12: for (l, r) 2 Wd(w) do 13: p p✓ (l, r |w) 14: M M + p · Mi (l)M0 Mk (r) 15: E E + p · l,r have unmatched punc Inference 17: In principle, working with the model (1) is straightforward, thanks to the closure properties of formal languages. Provided that psyn can be encoded as a weighted CFG, it can be composed with the weighted tree transducer p✓ and the weighted FST p to yield a new weighted CFG (similarly to Bar-Hillel et al., 1961; Nederhof and Satta, 2003). Under this new grammar, one can recover the opti¯ by dynamic programming, or sum mal T, T 0 for x over T, T 0 by the inside algorithm to get the likelihood p(¯ x). A similar approach was used by Levy (2008) with a different FST noisy channel. In this paper we assume that T is observed, allowing us to work with equation (2). This cuts the computation time from O(n3 ) to O(n).9 Whereas the inside algorithm for (1) must consider O(n2 ) ¯ and O(n) ways of buildpossible constituents of x ing each, our algorithm for (2) only needs to iterate over the O(n) true constituents of T and the 1 true way"
Q19-1023,I05-2002,0,0.0786985,"he surface punctuation from the distribution modeled by the trained PFST, yielding 367 Arabic Chinese English Hindi Spanish Punctuation Base Half Full 156.0 231.3 186.1 165.2 110.0 61.4 98.4 74.5 51.0 10.8 11.0 9.7 266.2 259.2 194.5 Base 540.8 205.0 140.9 118.4 346.3 All Half 590.3 174.4 131.4 118.8 343.4 The parser of Ma et al. (2014) takes a different approach and treats punctuation marks as features of their neighboring words. Zhang et al. (2013) use a generative model for punctuated sentences, leting them restore punctuation marks during transition-based parsing of unpunctuated sentences. Li et al. (2005) use punctuation marks to segment a sentence: this &quot;divide and rule&quot; strategy reduces ambiguity in parsing of long Chinese sentences. Punctuation can similarly be used to constrain syntactic structure during grammar induction (Spitkovsky et al., 2011). Punctuation restoration (§ 6.1) is useful for transcribing text from unpunctuated speech. The task is usually treated by tagging each slot with zero or more punctuation tokens, using a traditional sequence labeling method: conditional random fields (Lui and Wang, 2013; Lu and Ng, 2010), recurrent neural networks (Tilk and Alumäe, 2016), or trans"
Q19-1023,W14-1701,0,0.0282755,"ailored for the punctuation correction task. where we is the node in Te0 , and p(l, r |we ) is a similar log-linear model to equation (4) with additional features (Appendix C4 ) which look at we . Finally, we reconstruct xc based on the noisy channel p (xc |Tc0 ) in § 2.2. During training, is regularized to be close to the noisy channel parameters in the punctuation model trained on en_cesl. We use the same MBR decoder as in § 6.1 to choose the best action. We evaluate using AED as in § 6.1. As a second metric, we use the script from the CoNLL 2014 Shared Task on Grammatical Error Correction (Ng et al., 2014): it computes the F0.5 -measure of the set of edits found by the system, relative to the true set of edits. As shown in Table 5, our method achieves better performance than the punctuation restoration baselines (which ignore input punctuation). On the other hand, it is soundly beaten by a new BiLSTM-CRF that we trained specifically for the task of punctuation correction. This is the same as the BiLSTM-CRF in the previous section, except that the BiLSTM now reads a punctuated input sentence (with possibly erroneous punctuation). To be precise, at step 0  i  n, the BiLSTM reads a concatenation"
Q19-1023,D09-1005,1,0.650378,"ed by a choice of underlying punctemes at w and all its descendants. These punctemes determine an s-to-final path at i, then initial-to-final paths at i + 1 through k 1, then an initial-to-t path at k. The weight of the extended path is the product of all the WFSA weights on these paths (which correspond to transition probabilities in p PFST) times the probability of the choice of punctemes (from p✓ ). This inside algorithm computes quantities needed for training (§ 2.3). Useful variants arise via well-known methods for weighted derivation forests (Berstel and Reutenauer, 1988; Goodman, 1999; Li and Eisner, 2009; Eisner, 2016). Specifically, to modify Algorithm 1 to maximize over T 0 values (§§ 6.2–6.3) instead of summing over them, we switch to the derivation semiring (Goodman, 1999), as follows. Whereas I N(w)st used to store the total weight of all extended paths from state s at slot i to state t at slot j, now it will store the weight of the best such extended path. It will also store that extended path’s choice of underlying punctemes, in the form of a punctemeannotated version of the subtree of T that is rooted at w. This is a potential subtree of T 0 . Thus, each element of I N(w) has the form"
Q19-1023,U13-1020,0,0.0297786,"ore punctuation marks during transition-based parsing of unpunctuated sentences. Li et al. (2005) use punctuation marks to segment a sentence: this &quot;divide and rule&quot; strategy reduces ambiguity in parsing of long Chinese sentences. Punctuation can similarly be used to constrain syntactic structure during grammar induction (Spitkovsky et al., 2011). Punctuation restoration (§ 6.1) is useful for transcribing text from unpunctuated speech. The task is usually treated by tagging each slot with zero or more punctuation tokens, using a traditional sequence labeling method: conditional random fields (Lui and Wang, 2013; Lu and Ng, 2010), recurrent neural networks (Tilk and Alumäe, 2016), or transition-based systems (Ballesteros and Wanner, 2016). Full 553.4 78.7 75.4 91.8 239.3 Table 6: Perplexity (evaluated on the train split to avoid evaluating generalization) of a trigram language model trained (with add-0.001 smoothing) on different versions of rephrased training sentences. “Punctuation” only evaluates perplexity on the trigrams that have punctuation. “All” evaluates on all the trigrams. “Base” permutes all surface dependents including punctuation (Wang and Eisner, 2016). “Full” is our full approach: re"
Q19-1023,P14-2128,0,0.0229467,"a writing assistance tool (Heidorn, 2000), or subtree deletions in compressive summarization (Knight and Marcu, 2002). mark root. ,advcl, det nsubj ˆ, If true, the caper failed . by the maximizing variant of Algorithm 1 (§ 3.1). Then, we permute the underlying tree and sample the surface punctuation from the distribution modeled by the trained PFST, yielding 367 Arabic Chinese English Hindi Spanish Punctuation Base Half Full 156.0 231.3 186.1 165.2 110.0 61.4 98.4 74.5 51.0 10.8 11.0 9.7 266.2 259.2 194.5 Base 540.8 205.0 140.9 118.4 346.3 All Half 590.3 174.4 131.4 118.8 343.4 The parser of Ma et al. (2014) takes a different approach and treats punctuation marks as features of their neighboring words. Zhang et al. (2013) use a generative model for punctuated sentences, leting them restore punctuation marks during transition-based parsing of unpunctuated sentences. Li et al. (2005) use punctuation marks to segment a sentence: this &quot;divide and rule&quot; strategy reduces ambiguity in parsing of long Chinese sentences. Punctuation can similarly be used to constrain syntactic structure during grammar induction (Spitkovsky et al., 2011). Punctuation restoration (§ 6.1) is useful for transcribing text from"
Q19-1023,C18-1293,0,0.0311786,"do the same for punctuation. Punctuation is helpful Surface punctuation remains correlated with syntactic phrase structure. NLP systems for generating or editing text must be able to deploy surface punctuation as human writers do. Parsers and grammar induction systems benefit from the presence of surface punctuation marks (Jones, 1994; Spitkovsky et al., 2011). It is plausible that they could do better with a linguistically informed model that explains exactly why the surface punctuation appears where it does. Patterns of punctuation usage can also help identify the writer’s native language (Markov et al., 2018). 2 Formal Model We propose a probabilistic generative model of sentences (Figure 1): (1) P ¯ (T 0 )) p(¯ x) = T,T 0 psyn (T ) · p✓ (T 0 |T ) · p (¯ x|u Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles. Treebank annotation guidelines for punctuation tend to adopt simple heuristics like “attach to the highest possible node that preserves projectivity” (Bies et al., 1995; Nivre et al., 2018).1 Many dependency parsing works exclude punctuation from evaluation (Nivre et al., 2007b; K"
Q19-1023,D14-1162,0,0.0877368,"y N OISY C HANNEL. Punctuation is meaningful Pang et al. (2002) use question and exclamation marks as clues to sentiment. Similarly, quotation marks may be used to mark titles, quotations, reported speech, or dubious terminology (University of Chicago, 2010). Because of examples like this, methods for determining the similarity or meaning of syntax trees, such as a tree kernel (Agarwal et al., 2011) or a recursive neural network (Tai et al., 2015), should ideally be able to consider where the underlying punctuation marks attach. In tasks such as word embedding induction (Mikolov et al., 2013; Pennington et al., 2014) and machine translation (Zens et al., 2002), punctuation marks are usually either removed or treated as ˇ uˇrek and Sojka, 2010). ordinary words (Reh˚ Yet to us, building a parse tree on a surface sentence seems as inappropriate as morphologically segmenting a surface word. In both cases, one should instead analyze the latent underlying form, jointly with recovering that form. For example, the proper segmentation of English hoping is not hop-ing but hope-ing (with underlying e), and the proper segmentation of stopping is neither stopp-ing nor stop-ping but stop-ing (with only one underlying p"
Q19-1023,W11-0303,0,0.130287,"n of English hoping is not hop-ing but hope-ing (with underlying e), and the proper segmentation of stopping is neither stopp-ing nor stop-ping but stop-ing (with only one underlying p). Cotterell et al. (2015, 2016) get this right for morphology. We attempt to do the same for punctuation. Punctuation is helpful Surface punctuation remains correlated with syntactic phrase structure. NLP systems for generating or editing text must be able to deploy surface punctuation as human writers do. Parsers and grammar induction systems benefit from the presence of surface punctuation marks (Jones, 1994; Spitkovsky et al., 2011). It is plausible that they could do better with a linguistically informed model that explains exactly why the surface punctuation appears where it does. Patterns of punctuation usage can also help identify the writer’s native language (Markov et al., 2018). 2 Formal Model We propose a probabilistic generative model of sentences (Figure 1): (1) P ¯ (T 0 )) p(¯ x) = T,T 0 psyn (T ) · p✓ (T 0 |T ) · p (¯ x|u Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles. Treebank annotation guid"
Q19-1023,P15-1150,0,0.34039,"ft puncteme l and a right puncteme r, which may be empty. The resulting tree T 0 has underlying punctuation u. Each slot’s punctuation ui 2 u is rewritten to xi 2 x by N OISY C HANNEL. Punctuation is meaningful Pang et al. (2002) use question and exclamation marks as clues to sentiment. Similarly, quotation marks may be used to mark titles, quotations, reported speech, or dubious terminology (University of Chicago, 2010). Because of examples like this, methods for determining the similarity or meaning of syntax trees, such as a tree kernel (Agarwal et al., 2011) or a recursive neural network (Tai et al., 2015), should ideally be able to consider where the underlying punctuation marks attach. In tasks such as word embedding induction (Mikolov et al., 2013; Pennington et al., 2014) and machine translation (Zens et al., 2002), punctuation marks are usually either removed or treated as ˇ uˇrek and Sojka, 2010). ordinary words (Reh˚ Yet to us, building a parse tree on a surface sentence seems as inappropriate as morphologically segmenting a surface word. In both cases, one should instead analyze the latent underlying form, jointly with recovering that form. For example, the proper segmentation of Englis"
Q19-1023,W16-5907,0,0.0500477,"Missing"
Q19-1023,Q16-1035,1,0.864955,"as the BiLSTM-CRF in the previous section, except that the BiLSTM now reads a punctuated input sentence (with possibly erroneous punctuation). To be precise, at step 0  i  n, the BiLSTM reads a concatenation of the embedding of word i (or BOS if i = 0) with an embedding of the punctuation token sequence xi . The BiLSTMCRF wins because it is a discriminative model tailored for this task: the BiLSTM can extract arbitrary contextual features of slot i that are correlated with whether xi is correct in context. 6.3 For our experiment, we evaluate an interesting case of syntactic transformation. Wang and Eisner (2016) consider a systematic rephrasing procedure by rearranging the order of dependent subtrees within a UD treebank, in order to synthesize new languages with different word order that can then be used to help train multi-lingual systems (i.e., data augmentation with synthetic data). As Wang and Eisner acknowledge (2016, footnote 9), their permutations treat surface punctuation tokens like ordinary words, which can result in synthetic sentences whose punctuation is quite unlike that of real languages. In our experiment, we use Wang and Eisner’s (2016) “self-permutation” setting, where the dependen"
Q19-1023,W08-1703,0,0.0284705,"ive parsing (§3), we could follow the ˆthe caper failed ,If true ,. We ˆthe caper failed ,If true . leave the handling of capitalization to future work. We test the naturalness of the permuted sentences by asking how well a word trigram language model trained on them could predict the original sentences.23 As shown in Table 6, our permutation approach reduces the perplexity over the baseline on 4 of the 5 languages, often dramatically. 7 Conclusion and Future Work Related Work Punctuation can aid syntactic analysis, since it signals phrase boundaries and sentence structure. Briscoe (1994) and White and Rajkumar (2008) parse punctuated sentences using hand-crafted constraint-based grammars that implement Nunberg’s approach in a declarative way. These grammars treat surface punctuation symbols as ordinary words, but annotate the nonterminal categories so as to effectively keep track of the underlying punctuation. This is tantamount to crafting a grammar for underlyingly punctuated sentences and composing it with a finite-state noisy channel. 23 So the two approaches to permutation yield different training data, but are compared fairly on the same test data. 368 scheme in equation (1). For example, the psyn f"
Q19-1023,2002.tmi-tutorials.2,0,0.0772995,"et al. (2002) use question and exclamation marks as clues to sentiment. Similarly, quotation marks may be used to mark titles, quotations, reported speech, or dubious terminology (University of Chicago, 2010). Because of examples like this, methods for determining the similarity or meaning of syntax trees, such as a tree kernel (Agarwal et al., 2011) or a recursive neural network (Tai et al., 2015), should ideally be able to consider where the underlying punctuation marks attach. In tasks such as word embedding induction (Mikolov et al., 2013; Pennington et al., 2014) and machine translation (Zens et al., 2002), punctuation marks are usually either removed or treated as ˇ uˇrek and Sojka, 2010). ordinary words (Reh˚ Yet to us, building a parse tree on a surface sentence seems as inappropriate as morphologically segmenting a surface word. In both cases, one should instead analyze the latent underlying form, jointly with recovering that form. For example, the proper segmentation of English hoping is not hop-ing but hope-ing (with underlying e), and the proper segmentation of stopping is neither stopp-ing nor stop-ping but stop-ing (with only one underlying p). Cotterell et al. (2015, 2016) get this ri"
Q19-1023,P13-1074,0,0.0267007,"2002). mark root. ,advcl, det nsubj ˆ, If true, the caper failed . by the maximizing variant of Algorithm 1 (§ 3.1). Then, we permute the underlying tree and sample the surface punctuation from the distribution modeled by the trained PFST, yielding 367 Arabic Chinese English Hindi Spanish Punctuation Base Half Full 156.0 231.3 186.1 165.2 110.0 61.4 98.4 74.5 51.0 10.8 11.0 9.7 266.2 259.2 194.5 Base 540.8 205.0 140.9 118.4 346.3 All Half 590.3 174.4 131.4 118.8 343.4 The parser of Ma et al. (2014) takes a different approach and treats punctuation marks as features of their neighboring words. Zhang et al. (2013) use a generative model for punctuated sentences, leting them restore punctuation marks during transition-based parsing of unpunctuated sentences. Li et al. (2005) use punctuation marks to segment a sentence: this &quot;divide and rule&quot; strategy reduces ambiguity in parsing of long Chinese sentences. Punctuation can similarly be used to constrain syntactic structure during grammar induction (Spitkovsky et al., 2011). Punctuation restoration (§ 6.1) is useful for transcribing text from unpunctuated speech. The task is usually treated by tagging each slot with zero or more punctuation tokens, using a"
Q19-1023,P11-2033,0,0.0260543,"distribution given by the sample. This can be evaluated in O(m2 ) time. Performance on Extrinsic Tasks 19 To depunctuate a treebank sentence, we remove all tokens with POS-tag PUNCT or dependency relation punct. These are almost always leaves; else we omit the sentence. 20 Ideally, rather than maximize, one would integrate over possible trees T , in practice by sampling many Svalues Tk ¯ ) and replacing S(T ) in (10) with k S(Tk ). from psyn (· |u 21 Specifically, the Yara parser (Rasooli and Tetreault, 2015), a fast non-probabilistic transition-based parser that uses rich non-local features (Zhang and Nivre, 2011). We evaluate the trained punctuation model by using it in the following three tasks. 17 [en] Earlier, Kerry said, “Just because you get an honorable discharge does not, in fact, answer that question.” 18 [en] Sections 1, 2, 5, 6, 7, and 8 will survive any termination of this License. 365 We evaluate on Arabic, English, Chinese, Hindi, and Spanish. For each language, we train both the parser and the punctuation model on the training split of that UD treebank (§4), and evaluate on held-out data. We compare to the BiLSTM-CRF baseline in §4 (Xu et al., 2016).22 We also compare to a “trivial” dete"
Q19-1023,D10-1018,0,\N,Missing
Q19-1023,W02-1011,0,\N,Missing
Q19-1023,D14-1082,0,\N,Missing
Q19-1023,D07-1096,0,\N,Missing
W00-1803,W98-1301,0,\N,Missing
W00-1803,J98-2006,0,\N,Missing
W00-1803,C00-1038,1,\N,Missing
W00-1803,C94-2163,0,\N,Missing
W00-1803,P97-1040,1,\N,Missing
W00-2011,P96-1023,0,0.0406615,"Missing"
W00-2011,1997.iwpt-1.6,0,0.0319964,"Missing"
W00-2011,E99-1025,0,0.056552,"Missing"
W00-2011,C96-1058,1,0.741947,"Missing"
W00-2011,1997.iwpt-1.10,1,0.872288,"Missing"
W00-2011,P99-1059,1,0.910111,"Missing"
W00-2011,C92-2066,0,0.0783217,"Missing"
W00-2011,C92-2065,0,0.0765918,"Missing"
W00-2011,J94-1004,0,0.050216,"Missing"
W00-2011,J99-2004,0,0.0850161,"Missing"
W00-2011,H86-1020,0,\N,Missing
W00-2011,P97-1003,0,\N,Missing
W00-2011,J93-4002,0,\N,Missing
W00-2011,C88-2121,0,\N,Missing
W00-2011,1997.iwpt-1.11,0,\N,Missing
W00-2011,P85-1011,0,\N,Missing
W02-0102,A00-1031,0,0.0133198,"Missing"
W02-0102,A88-1019,0,0.430424,"Missing"
W02-0102,J88-1003,0,0.156021,"Missing"
W02-0102,P02-1001,1,0.859889,"Missing"
W02-0102,J94-2001,0,0.0372316,"Missing"
W02-0102,J96-1002,0,\N,Missing
W02-0102,P01-1042,0,\N,Missing
W02-1009,A00-2018,0,\N,Missing
W02-1009,J91-3003,0,\N,Missing
W02-1009,J98-4004,0,\N,Missing
W02-1009,W97-1010,0,\N,Missing
W02-1009,W98-1505,0,\N,Missing
W02-1009,J93-2004,0,\N,Missing
W02-1009,P97-1003,0,\N,Missing
W02-1009,C96-1058,1,\N,Missing
W02-1009,J99-4002,0,\N,Missing
W02-1009,P02-1001,1,\N,Missing
W05-1504,W04-3224,0,0.014868,"cussion and Elliott Dr´abek and Markus Dreyer for insights on (respectively) Chinese and German parsing. They also thank an anonymous reviewer for suggesting the German experiments. 1 In a phrase-structure parse, if phrase X headed by word token x is a subconstituent of phrase Y headed by word token y 6= x, then x is said to depend on y. In a more powerful compositional formalism like LTAG or CCG, dependencies can be extracted from the derivation tree. 2 It has recently been questioned whether these “bilexical” features actually contribute much to parsing performance (Klein and Manning, 2003; Bikel, 2004), at least when one has only a million words of training. typical parsing features in that they cannot be determined from tree-local information. Though lengths are not usually considered, we will see that bilexical dynamic-programming parsing algorithms can easily consider them as they build the parse. Soft constraints. Like any other feature of trees, dependency lengths can be explicitly used as features in a probability model that chooses among trees. Such a model will tend to disfavor long dependencies (at least of some kinds), as these are empirically rare. In the first part of the paper,"
W05-1504,E99-1016,0,0.0192424,"ly yields a superset of the original context-free language. Subset and superset approximations of (weighted) CFLs by (weighted) regular languages, usually by preventing center-embedding, have been widely explored; Nederhof (2000) gives a thorough review. We limit all dependency lengths (not just centerembedding).27 Further, we derive weights from a modified treebank rather than by approximating the true weights. And though regular grammar approximations are useful for other purposes, we argue that for parsing it is more efficient to perform the approximation in the parser, not in the grammar. Brants (1999) described a parser that encoded the grammar as a set of cascaded Markov models. The decoder was applied iteratively, with each iteration transforming the best (or n-best) output from the previous one until only the root symbol remained. This is a greedy variant of CFG parsing where the grammar is in Backus-Naur form. Bertsch and Nederhof (1999) gave a linear-time recognition algorithm for the recognition of the regular closure of deterministic context-free languages. Our result is related; instead of a closure of deterministic CFLs, we deal in a closure of CFLs that are assumed (by the parser"
W05-1504,J98-2004,0,0.0703624,"d the best feasible parse, as constructed from the gold-standard parse by grafting: see §4.2). Runtime is measured as the number of items per word   27 Of course, this still allows right-branching or leftbranching to unbounded depth. 20 H H X y XX y X  , (i.e., @ @ , , , , @ @ ) built by the agenda parser. The “soft constraint” point marked with × represents the p(∆ |d, h, c)-augmented model from §3. Second, fast approximate parsing may play a role in more accurate parsing. It might be used to rapidly compute approximate outside-probability estimates to prioritize best-first search (e.g., Caraballo and Charniak, 1998). It might also be used to speed up the early iterations of training a weighted parsing model, which for modern training methods tends to require repeated parsing (either for the best parse, as by Taskar et al., 2004, or all parses, as by Miyao and Tsujii, 2002). Third, it would be useful to investigate algorithmic techniques and empirical benefits for limiting dependency length in more powerful grammar formalisms. Our runtime reduction from O(n3 ) → O(nk 2 ) for a length-k bound applies only to a “split” bilexical grammar.28 Various kinds of synchronous grammars, in particular, are becoming i"
W05-1504,P97-1003,0,0.541457,"ct parses exhibit a “shortdependency preference”: a word’s dependents tend to be close to it in the string.3 If the j th word of a sentence depends on the ith word, then |i−j |tends to be 3 In this paper, we consider only a crude notion of “closeness”: the number of intervening words. Other distance measures could be substituted or added (following the literature on heavy-shift and sentence comprehension), including the phonological, morphological, syntactic, or referential (given/new) complexity of the intervening material (Gibson, 1998). In parsing, the most relevant previous work is due to Collins (1997), who considered three binary features of the intervening material: did it contain (a) any word tokens at all, (b) any verbs, (c) any commas or colons? Note that (b) is effective because it measures the length of a dependency in terms of the number of alternative attachment sites that the dependent skipped over, a notion that could be generalized. Similarly, McDonald et al. (2005) separately considered each of the intervening POS tags. 30 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 30–41, c Vancouver, October 2005. 2005 Association for Computational Li"
W05-1504,H05-1036,1,0.259835,") will assemble the resulting fragments into a vine parse. In this case, ATTACH -R IGHT should also be restricted to h &gt; 0, to prevent duplicate derivations. The runtime is O(nk(k + t0 )tg 2 ), dominated by the ATTACH rules; the rules in (b) require only O(nktg 2 + ngtt0 ) time. Each algorithm is specified as a collection of deductive inference rules. Once one has derived all antecedent items above the horizontal line and any side conditions to the right of the line, one may derive the consequent item below the line. Weighted agenda-based deduction is handled in the usual way (Nederhof, 2003; Eisner et al., 2005). The probabilities governing the automaton Lw , 0 namely p(start at q), p(q −w→r |q), and p(stop | q), are respectively associated with the axiomatic T REE -R IGHT: q −w→r ∈ Rw @ @ 0:$ n accept F E ND -V INE: 0 @ @ 0:$ S TART-V INE: q ∈ init(R$ ) q q −→r ∈ R$ w i @ @ i:w rX y X @  @ q @ @ i h0 : w 0 G RAFT-V INE: h:w q h0 : w 0 j :x 0:$ H H S EAL -L EFT:   i @ @ F q ∈ final(Rw ) h:w F h : w h0 : w 0 C OMPLETE -R IGHT: q F h:w F   j :x @  @ i:w i:w q @  @ q T REE -L EFT: i:w 0:$ F h:w q h:w   q h:w r   h0 : w 0 i:w @  @ i−1 i q 0:$ @ @ q i h0 : w 0 T REE -S TART: i F C OMPLETE"
W05-1504,P99-1059,1,0.657514,"r dependency lengths. We confine ourselves (throughout the paper) to parsing part-ofspeech (POS) tag sequences. This allows us to ignore data sparseness, out-of-vocabulary, smoothing, and pruning issues, but it means that our accuracy measures are not state-of-the-art. Our techniques could be straightforwardly adapted to (bi)lexicalized parsers on actual word sequences, though not necessarily with the same success. 3.1 Grammar Formalism Throughout this paper we will use split bilexical grammars, or SBGs (Eisner, 2000), a notationally simpler variant of split head-automaton grammars, or SHAGs (Eisner and Satta, 1999). The formalism is context-free. We define here a probabilistic version,6 which we use for the baseline models in our experiments. They are only baselines because the SBG generative process does not take note of dependency length. An SBG is an tuple G = (Σ, $, L, R). Σ is an alphabet of words. (In our experiments, we parse only POS tag sequences, so Σ is actually an alphabet of tags.) $ 6∈ Σ is a distinguished root symbol; ¯ = Σ ∪ {$}. L and R are functions from Σ ¯ let Σ to probabilistic -free finite-state automata over Σ. ¯ the SBG specifies “left” and Thus, for each w ∈ Σ, “right” probabil"
W05-1504,P90-1034,0,0.0134826,"abstractly as the average number of items (i.e., @ @ , built per word. Model size is measured as the number of nonzero parameters.) either of which would allow the non-regular language {an bcn : 0 < n < ∞}. It does allow arbitrarily deep right- or left-branching structures. 4.1 Vine Grammars The tighter the bound on dependency length, the fewer parse trees we allow and the faster we can find them using the algorithm of Fig. 2a. If the bound is too tight to allow the correct parse of some sentence, we would still like to allow an accurate partial parse: a sequence of accurate parse fragments (Hindle, 1990; Abney, 1991; Appelt et al., 1993; Chen, 1995; Grefenstette, 1996). Furthermore, we would like to use the fact that some fragment sequences are presumably more likely than others. Our partial parses will look like the one in Fig. 1b. where 4 subtrees rather than 1 are dependent on $. This is easy to arrange in the SBG formalism. We merely need to construct our SBG so that the automaton R$ is now permitted to generate multiple children—the roots of parse fragments. This R$ is a probabilistic finite-state automaton that describes legal or likely root sequences in Σ∗ . In our experiments in this"
W05-1504,C90-3029,0,0.0480492,"ncouver, October 2005. 2005 Association for Computational Linguistics small. This implies that neither i nor j is modified by complex phrases that fall between i and j. In terms of phrase structure, it implies that the phrases modifying word i from a given side tend to be (1) few in number, (2) ordered so that the longer phrases fall farther from i, and (3) internally structured so that the bulk of each phrase falls on the side of j away from i. These principles can be blamed for several linguistic phenomena. (1) helps explain the “late closure” or “attach low” heuristic (e.g., Frazier, 1979; Hobbs and Bear, 1990): a modifier such as a PP is more likely to attach to the closest appropriate head. (2) helps account for heavy-shift: when an NP is long and complex, take NP out, put NP on the table, and give NP to Mary are likely to be rephrased as take out NP, put on the table NP, and give Mary NP. (3) explains certain non-canonical word orders: in English, a noun’s left modifier must become a right modifier if and only if it is right-heavy (a taller politician vs. a politician taller than all her rivals4 ), and a verb’s left modifier may extrapose its rightheavy portion (An aardvark walked in who had circ"
W05-1504,P04-1061,0,0.0172847,"w’s left dependents are conditionally independent of one another given w. In model C (the best), each automaton }−→}  has an extra state q0 that allows the first (closest) dependent to be chosen differently from the rest. Model B is a compromise:7 it is like model A, but each type w ∈ Σ may have an elevated or reduced probability of having no dependents at all. This is accomplished by using automata }−→}  as in model C, which allows the stopping probabilities p(STOP |q0 ) and p(STOP |q1 ) to differ, but tying the conditional dis7 It is equivalent to the “dependency model with valence” of Klein and Manning (2004). 32 3.3 3.4 Length-Sensitive Models Parsing Algorithm Fig. 2a gives a variant of Eisner and Satta’s (1999) SHAG parsing algorithm, adapted to SBGs, which are easier to understand.9 (We will modify this algorithm later in §4.) The algorithm obtains O(n3 ) runtime, despite the need to track the position of head words, by exploiting the conditional independence between a head’s left children and right children. It builds “half-constituents” denoted by @ @ (a head word together with some modifying phrases on the right, i.e., wα1 . . . αr ) and (a head word together with some modifying phrases on"
W05-1504,P05-1012,0,0.280182,"shift and sentence comprehension), including the phonological, morphological, syntactic, or referential (given/new) complexity of the intervening material (Gibson, 1998). In parsing, the most relevant previous work is due to Collins (1997), who considered three binary features of the intervening material: did it contain (a) any word tokens at all, (b) any verbs, (c) any commas or colons? Note that (b) is effective because it measures the length of a dependency in terms of the number of alternative attachment sites that the dependent skipped over, a notion that could be generalized. Similarly, McDonald et al. (2005) separately considered each of the intervening POS tags. 30 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 30–41, c Vancouver, October 2005. 2005 Association for Computational Linguistics small. This implies that neither i nor j is modified by complex phrases that fall between i and j. In terms of phrase structure, it implies that the phrases modifying word i from a given side tend to be (1) few in number, (2) ordered so that the longer phrases fall farther from i, and (3) internally structured so that the bulk of each phrase falls on the side of j away f"
W05-1504,J00-1003,0,0.0198261,"ecific bounds 0.6 0.55 0.5 k=1 0 20 15 20 40 60 runtime (items/word) 80 100 1 Chinese 0.9 0.8 0.7 15 0.6 2 0.5 0 20 40 20 3 Model C, baseline soft constraint single bound type-specific bounds 60 80 k=1 0.4 100 120 140 160 runtime (items/word) 1 German 0.95 0.9 0.85 0.8 Future Work F 6 1 0.95 F language. Our “vines” then let us concatenate several strings in this subset, which typically yields a superset of the original context-free language. Subset and superset approximations of (weighted) CFLs by (weighted) regular languages, usually by preventing center-embedding, have been widely explored; Nederhof (2000) gives a thorough review. We limit all dependency lengths (not just centerembedding).27 Further, we derive weights from a modified treebank rather than by approximating the true weights. And though regular grammar approximations are useful for other purposes, we argue that for parsing it is more efficient to perform the approximation in the parser, not in the grammar. Brants (1999) described a parser that encoded the grammar as a set of cascaded Markov models. The decoder was applied iteratively, with each iteration transforming the best (or n-best) output from the previous one until only the"
W05-1504,J03-1006,0,0.150199,"combines @ @ + , the annotations on @ @ and suffice to determine the dependency’s length ∆ = |h − h0 |, direction d = sign(h − h0 ), head word w, and child word w0 .12 So the additional cost of such a dependency, e.g. p(∆ |d, w, w0 ), can be included as the weight of an extra antecedent to the rule, and so included in  H  H the weight of the resulting or . To execute the inference rules in Fig. 2a, we use a prioritized agenda. Derived items such as  H  H @ @ , , , and are prioritized by their Viterbi-inside probabilities. This is known as uniform-cost search or shortest-hyperpath search (Nederhof, 2003). We halt as soon as a full parse (the accept item) pops from the agenda, since uniform-cost search (as a special case of the A∗ algorithm) guarantees this to be the maximumprobability parse. No other pruning is done. 11 Confusion-set parsing may be regarded as parsing a particular lattice with n states and ng arcs. The algorithm can be generalized to lattice parsing, in which case it has runtime O(m2 (n + t0 )t) for a lattice of n states and m arcs. Roughly, h : w is replaced by an arc, while i is replaced by a state and i − 1 is replaced by the same state. 12 For general lattice parsing, it"
W05-1504,2003.mtsummit-semit.11,0,0.0249507,"sing (either for the best parse, as by Taskar et al., 2004, or all parses, as by Miyao and Tsujii, 2002). Third, it would be useful to investigate algorithmic techniques and empirical benefits for limiting dependency length in more powerful grammar formalisms. Our runtime reduction from O(n3 ) → O(nk 2 ) for a length-k bound applies only to a “split” bilexical grammar.28 Various kinds of synchronous grammars, in particular, are becoming important in statistical machine translation. Their high runtime complexity might be reduced by limiting monolingual dependency length (for a related idea see Schafer and Yarowsky, 2003). Finally, consider the possibility of limiting dependency length during grammar induction. We reason that a learner might start with simple structures that focus on local relationships, and gradually relax this restriction to allow more complex models. 7 Conclusion We have described a novel reason for identifying headword-to-headword dependencies while parsing: to consider their length. We have demonstrated that simple bilexical parsers of English, Chinese, and German can exploit a “short-dependency preference.” Notably, soft constraints on dependency length can improve both speed and accurac"
W05-1504,W04-3201,0,0.0306737,"ed depth. 20 H H X y XX y X  , (i.e., @ @ , , , , @ @ ) built by the agenda parser. The “soft constraint” point marked with × represents the p(∆ |d, h, c)-augmented model from §3. Second, fast approximate parsing may play a role in more accurate parsing. It might be used to rapidly compute approximate outside-probability estimates to prioritize best-first search (e.g., Caraballo and Charniak, 1998). It might also be used to speed up the early iterations of training a weighted parsing model, which for modern training methods tends to require repeated parsing (either for the best parse, as by Taskar et al., 2004, or all parses, as by Miyao and Tsujii, 2002). Third, it would be useful to investigate algorithmic techniques and empirical benefits for limiting dependency length in more powerful grammar formalisms. Our runtime reduction from O(n3 ) → O(nk 2 ) for a length-k bound applies only to a “split” bilexical grammar.28 Various kinds of synchronous grammars, in particular, are becoming important in statistical machine translation. Their high runtime complexity might be reduced by limiting monolingual dependency length (for a related idea see Schafer and Yarowsky, 2003). Finally, consider the possibi"
W05-1504,P03-1054,0,\N,Missing
W05-1504,P95-1031,0,\N,Missing
W06-1638,H94-1028,0,0.0346484,"elow. Like equation (5), it is P (X[α] → Y Z) times a sum of up to 4 products, corresponding to the 4 passpattern cases. ∗ ×P (NP[2] → He) ×P (VP[2] →∗ loves cookies) 320 4 terms, the I NHERIT model has L(R3 + R2 + R1 ) + 3R3 − N Training latent PCFG models, like training most other unsupervised models, requires non-convex optimization. To find good parameter values, it is often helpful to train a simpler model first and use its parameters to derive a starting guess for the harder optimization problem. A well-known example is the training of the IBM models for statistical machine translation (Berger et al., 1994). In this vein, we did an experiment in which we gradually increased L during EM training of the P CFG -L A and I NHERIT models. Whenever the training likelihood began to converge, we manually and globally increased L, simply doubling or tripling it (see “clone all” in Table 3 and Fig. 5). The probability of X[α] → Y [β]Z[γ] under the new model was initialized to be proportional to the probability of X[α mod L] → Y [β mod L]Z[γ mod L] (where L refers to the old L),5 times a random ”jitter” to break symmetry. In a second annealing experiment (“clone some”) we addressed a weakness of the P CFG L"
W06-1638,C02-1126,0,0.332875,". Section 5 describes the motivation and results of our experiments. We finish by discussing future work and conclusions in sections 6–7. 1 Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank. 2 Probabilistic context-free grammar with latent annotations. 317 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317–326, c Sydney, July 2006. 2006 Association for Computational Linguistics Citation Collins (1997) Lari and Young (1990) Pereira and Schabes (1992) Klein and Manning (2001) Chiang and Bikel (2002) Matsuzaki et al. (2005) I NHERIT model (this paper) Observed data Treebank tree with head child annotated on each nonterminal Words Words and partial brackets Part-of-speech tags Treebank tree Treebank tree Treebank tree and head child heuristics Hidden data No hidden data. Degenerate EM case. Parse tree Parse tree Parse tree Head child on each nonterminal Integer feature on each nonterminal Integer feature on each nonterminal Table 1: Observed and hidden data in PCFG grammar learning. 2 Partially supervised EM learning for S but for every nonterminal. Their partially supervised learning proc"
W06-1638,P96-1025,0,0.273714,"nough information; thus PCFGs estimated straightforwardly from the Penn Treebank (Bies et al., 1995) work only moderately well (Charniak, 1996). To address this problem, researchers have used heuristics to add more information. Eisner (1996), Charniak (1997), Collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without subjects. He Section 2 describes previous work in finding hidden information in treebanks. Section 3 describes automatically induced feature grammars. We start by describing the P CFG -L A model, then introduce new models that use specific agreement patterns to propagate features through the tree. Section 4 describes annealing-like procedures for training latent-annotation models. Section 5 describes the motivation and results of our experiments. We finish by discussing future work and conclusions in section"
W06-1638,P97-1003,0,0.457198,"ed ways. Our models therefore allocate a supply of free parameters differently, allowing more fine-grained nonterminals but less finegrained control over the probabilities of rewriting them. We also present simple methods for deciding selectively (during training) which nonterminals to split and how. Introduction Treebanks never contain enough information; thus PCFGs estimated straightforwardly from the Penn Treebank (Bies et al., 1995) work only moderately well (Charniak, 1996). To address this problem, researchers have used heuristics to add more information. Eisner (1996), Charniak (1997), Collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without subjects. He Section 2 describes previous work in finding hidden information in treebanks. Section 3 describes automatically induced feature grammars. We start by describing"
W06-1638,P05-1010,0,0.169318,"ssing Johns Hopkins University 3400 North Charles Street, Baltimore, MD 21218 USA {markus,jason}@clsp.jhu.edu Abstract also modified the treebank to contain different labels for standard and for base noun phrases. Klein and Manning (2003) identified nonterminals that could valuably be split into fine-grained ones using hand-written linguistic rules. Their unlexicalized parser combined several such heuristics with rule markovization and reached a performance similar to early lexicalized parsers. We study unsupervised methods for learning refinements of the nonterminals in a treebank. Following Matsuzaki et al. (2005) and Prescher (2005), we may for example split NP without supervision into NP[0] and NP[1], which behave differently. We first propose to learn a PCFG that adds such features to nonterminals in such a way that they respect patterns of linguistic feature passing: each node’s nonterminal features are either identical to, or independent of, those of its parent. This linguistic constraint reduces runtime and the number of parameters to be learned. However, it did not yield improvements when training on the Penn Treebank. An orthogonal strategy was more successful: to improve the performance of the"
W06-1638,P92-1017,0,0.351799,"ike procedures for training latent-annotation models. Section 5 describes the motivation and results of our experiments. We finish by discussing future work and conclusions in sections 6–7. 1 Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank. 2 Probabilistic context-free grammar with latent annotations. 317 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317–326, c Sydney, July 2006. 2006 Association for Computational Linguistics Citation Collins (1997) Lari and Young (1990) Pereira and Schabes (1992) Klein and Manning (2001) Chiang and Bikel (2002) Matsuzaki et al. (2005) I NHERIT model (this paper) Observed data Treebank tree with head child annotated on each nonterminal Words Words and partial brackets Part-of-speech tags Treebank tree Treebank tree Treebank tree and head child heuristics Hidden data No hidden data. Degenerate EM case. Parse tree Parse tree Parse tree Head child on each nonterminal Integer feature on each nonterminal Integer feature on each nonterminal Table 1: Observed and hidden data in PCFG grammar learning. 2 Partially supervised EM learning for S but for every nont"
W06-1638,H05-1036,1,0.845843,"Missing"
W06-1638,P93-1024,0,0.0947452,"he treebank trees—hence the one-iteration downtick in Figure 5). where P1 and P2 choose child features as if they were separate single-feature I NHERIT models. We omit discussion of dynamic programming speedups for I NHERIT 2. Empirically, the hope is that the two features when learned with the EM algorithm will pick out different linguistic properties of the constituents in the treebank tree. 321 Jensen-Shannon divergence is defined as      1 q+r q+r D(q, r) = D q || + D r || 2 2 2 These experiments are a kind of “poor man’s version” of the deterministic annealing clustering algorithm (Pereira et al., 1993; Rose, 1998), which gradually increases the number of clusters during the clustering process. In deterministic annealing, one starts in principle with a very large number of clusters, but maximizes likelihood only under a constraint that the joint distribution p(point, cluster ) must have very high entropy. This drives all of the cluster centroids to coincide exactly, redundantly representing just one effective cluster. As the entropy is permitted to decrease, some of the cluster centroids find it worthwhile to drift apart.6 In future work, we would like to apply this technique to split nonte"
W06-1638,C96-1058,1,0.779318,"n certain linguistically motivated ways. Our models therefore allocate a supply of free parameters differently, allowing more fine-grained nonterminals but less finegrained control over the probabilities of rewriting them. We also present simple methods for deciding selectively (during training) which nonterminals to split and how. Introduction Treebanks never contain enough information; thus PCFGs estimated straightforwardly from the Penn Treebank (Bies et al., 1995) work only moderately well (Charniak, 1996). To address this problem, researchers have used heuristics to add more information. Eisner (1996), Charniak (1997), Collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without subjects. He Section 2 describes previous work in finding hidden information in treebanks. Section 3 describes automatically induced feature"
W06-1638,W05-1512,0,0.327142,"ty 3400 North Charles Street, Baltimore, MD 21218 USA {markus,jason}@clsp.jhu.edu Abstract also modified the treebank to contain different labels for standard and for base noun phrases. Klein and Manning (2003) identified nonterminals that could valuably be split into fine-grained ones using hand-written linguistic rules. Their unlexicalized parser combined several such heuristics with rule markovization and reached a performance similar to early lexicalized parsers. We study unsupervised methods for learning refinements of the nonterminals in a treebank. Following Matsuzaki et al. (2005) and Prescher (2005), we may for example split NP without supervision into NP[0] and NP[1], which behave differently. We first propose to learn a PCFG that adds such features to nonterminals in such a way that they respect patterns of linguistic feature passing: each node’s nonterminal features are either identical to, or independent of, those of its parent. This linguistic constraint reduces runtime and the number of parameters to be learned. However, it did not yield improvements when training on the Penn Treebank. An orthogonal strategy was more successful: to improve the performance of the EM learner by treeb"
W06-1638,1997.iwpt-1.13,0,0.0603896,"onterminals. Thus we have L3 R3 + L2 R2 + LR1 − LN 3.2 Feature Passing: The I NHERIT Model Many linguistic theories assume that features get passed from the mother node to their children or some of their children. In many cases it is the head child that gets passed its feature value from its mother (e.g., Kaplan and Bresnan (1982), Pollard and Sag (1994)). In some cases the feature is passed to both the head and the non-head child, or perhaps even to the non-head alone. (3) Figure 2: Features are passed to different children at different positions in the tree. degrees of freedom. We note that Goodman (1997) mentioned possible ways to factor the probability 1, making independence assumptions in order to reduce the number of parameters. In the example in Fig. 2, the tense feature (pres) is always passed to the head child (underlined). How the number feature (sg/pl) is passed depends on the rewrite rule: S → NP VP passes it to both children, to enforce subject-verb agreement, while VP → V NP only passes it to the head child, since the object NP is free not to agree with the verb. A feature grammar can incorporate such patterns of feature passing. We introduce additional parameters that define the p"
W06-1638,J98-4004,0,0.143211,"ple methods for deciding selectively (during training) which nonterminals to split and how. Introduction Treebanks never contain enough information; thus PCFGs estimated straightforwardly from the Penn Treebank (Bies et al., 1995) work only moderately well (Charniak, 1996). To address this problem, researchers have used heuristics to add more information. Eisner (1996), Charniak (1997), Collins (1997), and many subsequent researchers1 annotated every node with lexical features passed up from its “head child,” in order to more precisely reflect the node’s “inside” contents. Charniak (1997) and Johnson (1998) annotated each node with its parent and grandparent nonterminals, to more precisely reflect its “outside” context. Collins (1996) split the sentence label S into two versions, representing sentences with and without subjects. He Section 2 describes previous work in finding hidden information in treebanks. Section 3 describes automatically induced feature grammars. We start by describing the P CFG -L A model, then introduce new models that use specific agreement patterns to propagate features through the tree. Section 4 describes annealing-like procedures for training latent-annotation models."
W06-1638,P06-1072,1,0.821994,"model P (X[α] → Y [β] Z[γ]) with a log-linear model.10 Feature functions would consider the values of variously sized, overlapping subsets of X, Y, Z, α, β, γ. For example, a certain feature might fire when X[α] = NP[1] and Z[γ] = N[2]. This approach can be extended to the multi-feature case, as in I NHERIT 2. Inheritance as in the I NHERIT model can then be expressed by features like α = β, or α = β and X = VP. During early iterations, we could use a prior to encourage a strong positive weight on these inheritance features, and gradually relax this bias—akin to the “structural annealing” of (Smith and Eisner, 2006). When modeling the lexical rule P (X[α] → w), we could use features that consider the spelling of the word w in conjunction with the value of α. Thus, we might learn that V [1] is particularly likely to rewrite as a word ending in -s. Spelling features that are predictable from string context are important clues to the existence and behavior of the hidden annotations we wish to induce. A final remark is that “inheritance” does not necessarily have to mean that α = β. It is enough that α and β should have high mutual information, so that one can be predicted from the other; they do not actuall"
W06-1638,W01-0714,0,0.0404794,"Missing"
W06-1638,P03-1054,0,0.656284,"ild on each nonterminal Integer feature on each nonterminal Integer feature on each nonterminal Table 1: Observed and hidden data in PCFG grammar learning. 2 Partially supervised EM learning for S but for every nonterminal. Their partially supervised learning procedure observes trees that are fully bracketed and fully labeled, except for the integer subscript used to annotate each node. After automatically inducing the annotations with EM, their resulting parser performs just as well as one learned from a treebank whose nonterminals were manually refined through linguistic and error analysis (Klein and Manning, 2003). In Matsuzaki’s P CFG -L A model, rewrite rules take the form The parameters of a PCFG can be learned with or without supervision. In the supervised case, the complete tree is observed, and the rewrite rule probabilities can be estimated directly from the observed rule counts. In the unsupervised case, only the words are observed, and the learning method must induce the whole structure above them. (See Table 1.) In the partially supervised case we will consider, some part of the tree is observed, and the remaining information has to be induced. Pereira and Schabes (1992) estimate PCFG paramet"
W06-1638,P95-1037,0,0.0828925,"hout subjects. He Section 2 describes previous work in finding hidden information in treebanks. Section 3 describes automatically induced feature grammars. We start by describing the P CFG -L A model, then introduce new models that use specific agreement patterns to propagate features through the tree. Section 4 describes annealing-like procedures for training latent-annotation models. Section 5 describes the motivation and results of our experiments. We finish by discussing future work and conclusions in sections 6–7. 1 Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank. 2 Probabilistic context-free grammar with latent annotations. 317 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 317–326, c Sydney, July 2006. 2006 Association for Computational Linguistics Citation Collins (1997) Lari and Young (1990) Pereira and Schabes (1992) Klein and Manning (2001) Chiang and Bikel (2002) Matsuzaki et al. (2005) I NHERIT model (this paper) Observed data Treebank tree with head child annotated on each nonterminal Words Words and partial brackets Part-of-speech tags Treebank tree Treebank tr"
W06-1638,J93-2004,0,0.0336561,"H C D looks like after binarization and markovization. The binarization process starts at the head of the sentence and moves to the right, inserting an auxiliary node for each picked up child, then moving to the left. Each auxiliary node consists of the parent label, the direction (L or R) and the label of the child just picked up. Experiments Setup We ran several experiments to compare the I N HERIT with the P CFG -L A model and look into the effect of different Treebank preprocessing and the annealing-like procedures. We used sections 2–20 of the Penn Treebank 2 Wall Street Journal corpus (Marcus et al., 1993) for training, section 22 as development set and section 23 for testing. Following Matsuzaki et al. (2005), words occurring fewer than 4 times in the training corpus were replaced by unknown-word symbols that encoded certain suffix and capitalization information. All experiments used simple add-lambda smoothing (λ=0.1) during the reestimation step (M step) of training. Figure 4: Horizontal and vertical markovization and center-parent binarization of the rule X → A B H C D where H is the head child. Binarization and Markovization. Before extracting the backbone PCFG and running the constrained"
W06-3104,J00-1004,0,0.0255115,"ransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V aligned to V0 and NP align"
W06-3104,J93-2003,0,0.0297629,"ition of the PP auf diese Frage either syntactically (by invoking scrambling) or semantically (by describing a deep analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V"
W06-3104,P05-1067,0,0.0992068,"equivalent conditional process for generating T2 , A given T1 . 24 deterministically assembles the latter rules into an actual tree T2 and reads off its yield S2 . What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1 . It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3 , or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1 , as IBM Models 1–5 do. Our alternative is to define a “quasi-synchronous” stochastic process. It generates T2 in a way that is not in thrall to T1 but is “inspired by it.” (A human tr"
W06-3104,J94-4004,0,0.0456684,"tactic alignment. Any portion of T2 can align to any portion of T1 , or to NULL. Nodes that are syntactically related in T1 do not have to translate into nodes that are syntactically related in T2 —although (1) is usually higher if they do. This property makes our approach especially promising for aligning freely, or erroneously, translated sentences, and for coping with syntactic diver23 Proceedings of the Workshop on Statistical Machine Translation, pages 23–30, c New York City, June 2006. 2006 Association for Computational Linguistics gences observed between even closely related languages (Dorr, 1994; Fox, 2002). We can patch together an alignment without accounting for all the details of the translation process. For instance, perhaps a source NP (figure 1) or PP (figure 2) appears “out of place” in the target sentence. A linguist might account for the position of the PP auf diese Frage either syntactically (by invoking scrambling) or semantically (by describing a deep analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it"
W06-3104,P03-2041,1,0.938449,"ead). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V aligned to V0 and NP aligned to NP0 . The process then continues"
W06-3104,W02-1039,0,0.013396,"ment. Any portion of T2 can align to any portion of T1 , or to NULL. Nodes that are syntactically related in T1 do not have to translate into nodes that are syntactically related in T2 —although (1) is usually higher if they do. This property makes our approach especially promising for aligning freely, or erroneously, translated sentences, and for coping with syntactic diver23 Proceedings of the Workshop on Statistical Machine Translation, pages 23–30, c New York City, June 2006. 2006 Association for Computational Linguistics gences observed between even closely related languages (Dorr, 1994; Fox, 2002). We can patch together an alignment without accounting for all the details of the translation process. For instance, perhaps a source NP (figure 1) or PP (figure 2) appears “out of place” in the target sentence. A linguist might account for the position of the PP auf diese Frage either syntactically (by invoking scrambling) or semantically (by describing a deep analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible"
W06-3104,P03-1011,0,0.0479976,"T researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V aligned to V0 and NP aligned to NP0 . The process then continues recursively by"
W06-3104,W04-3228,0,0.0127172,"h0 , p) = Y Y dir∈{l,r} c∈depsD (p,dir) 0 P (D(c) |a, a , c) × pstop (nostop |p, dir, adj) ×pchoose (c |p, dir) ×pconf ig (config) × ptrans (a |a0 ) pstop (stop |p, dir, adj) 4 Experiments We claim that for modeling human-translated bitext, it is better to project syntax only loosely. To evaluate this claim, we train quasi-synchronous dependency grammars that allow progressively more divergence from monotonic tree alignment. We evaluate these models on cross-entropy over held-out data and on error rate in a word-alignment task. One might doubt the use of dependency trees for alignment, since Gildea (2004) found that constituency trees aligned better. That experiment, however, aligned only the 1-best parse trees. We too will consider only the 1-best source tree T1 , but in constrast to Gildea, we will search for the target tree T2 that aligns best with T1 . Finding T2 and the alignment is simply a matter of parsing S2 with the QG derived from T1 . 4.1 Data and Training We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002). We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Mann"
W06-3104,P02-1050,0,0.0212717,"Missing"
W06-3104,P03-1054,0,0.00606541,"Gildea (2004) found that constituency trees aligned better. That experiment, however, aligned only the 1-best parse trees. We too will consider only the 1-best source tree T1 , but in constrast to Gildea, we will search for the target tree T2 that aligns best with T1 . Finding T2 and the alignment is simply a matter of parsing S2 with the QG derived from T1 . 4.1 Data and Training We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002). We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Manning, 2003). Initial estimates of lexical translation probabilities 28 came from the IBM Model 4 translation tables produced by G IZA ++ (Brown et al., 1993; Och and Ney, 2003). All text was lowercased and numbers of two or more digits were converted to an equal number of hash signs. The bitext was divided into training sets of 1K, 10K, and 100K sentence pairs. We held out one thousand sentences for evaluating the crossentropy of the various models and hand-aligned 100 sentence pairs to evaluate alignment error rate (AER). We trained the model parameters on bitext using the Expectation-Maximization (EM)"
W06-3104,P04-1061,0,0.166708,"linear model. When generating the children of a node in T2 , the process first generates their tags using monolingual parameters (fluency), and then fills in in the words using bilingual parameters (adequacy) that select and translate words from T1 .5 Concretely, each node in T2 is labeled by a triple (tag, word, aligned word). Given a parent node (p, h, h0 ) in T2 , we wish to generate sequences of left and right child nodes, of the form (c, a, a0 ). Our monolingual parameters come from a simple generative model of syntax used for grammar induction: the Dependency Model with Valence (DMV) of Klein and Manning (2004). In scoring dependency attachments, DMV uses tags rather than words. The parameters of the model are: 1. pchoose (c |p, dir): the probability of generating c as the next child tag in the sequence of dir children, where dir ∈ {lef t, right}. 2. pstop (s |h, dir, adj): the probability of generating no more child tags in the sequence of dir children. This is conditioned in part on the “adjacency” adj ∈ {true, f alse}, which indicates whether the sequence of dir children is empty so far. Our bilingual parameters score word-to-word translation and aligned dependency configurations. We thus use the"
W06-3104,P04-1084,0,0.0134172,"reads off its yield S2 . What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1 . It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3 , or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1 , as IBM Models 1–5 do. Our alternative is to define a “quasi-synchronous” stochastic process. It generates T2 in a way that is not in thrall to T1 but is “inspired by it.” (A human translator might be imagined to behave similarly.) When choosing how to expand nodes of T2 , we are influenced both by the structure of T1 and"
W06-3104,P04-1083,0,0.060538,"ay not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V aligned to V0 and NP aligned to NP0 . The process then continues recursively by choosing moves"
W06-3104,J03-1002,0,0.00527481,"e T1 , but in constrast to Gildea, we will search for the target tree T2 that aligns best with T1 . Finding T2 and the alignment is simply a matter of parsing S2 with the QG derived from T1 . 4.1 Data and Training We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002). We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Manning, 2003). Initial estimates of lexical translation probabilities 28 came from the IBM Model 4 translation tables produced by G IZA ++ (Brown et al., 1993; Och and Ney, 2003). All text was lowercased and numbers of two or more digits were converted to an equal number of hash signs. The bitext was divided into training sets of 1K, 10K, and 100K sentence pairs. We held out one thousand sentences for evaluating the crossentropy of the various models and hand-aligned 100 sentence pairs to evaluate alignment error rate (AER). We trained the model parameters on bitext using the Expectation-Maximization (EM) algorithm. The T1 tree is fully observed, but we parse the target language. As noted, the initial lexical translation probabilities came from IBM Model 4. We initial"
W06-3104,P05-1034,0,0.0402607,"process for generating T2 , A given T1 . 24 deterministically assembles the latter rules into an actual tree T2 and reads off its yield S2 . What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1 . It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3 , or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1 , as IBM Models 1–5 do. Our alternative is to define a “quasi-synchronous” stochastic process. It generates T2 in a way that is not in thrall to T1 but is “inspired by it.” (A human translator might be ima"
W06-3104,C90-3045,0,0.0855646,"use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V aligned to V0 and NP aligned to NP0 . The process then continues recursively by choosing moves to expand these children. One can regard this stochastic process as an instance of analysis-transfer-synthesis MT. Analysis chooses a parse T1 given S1 . Transfer map"
W06-3104,J97-3002,0,0.150153,"analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V alig"
W06-3104,P01-1067,0,0.216614,"ess in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V aligned to V0 and NP aligned to NP0 . The process t"
W06-3104,W99-0604,0,\N,Missing
W06-3104,N04-1035,0,\N,Missing
W06-3104,J95-4002,0,\N,Missing
W06-3104,J06-2001,0,\N,Missing
W06-3104,N01-1026,0,\N,Missing
W06-3104,J03-4003,0,\N,Missing
W06-3104,P99-1059,1,\N,Missing
W06-3104,N03-1017,0,\N,Missing
W06-3104,N04-1021,1,\N,Missing
W06-3104,J99-4004,0,\N,Missing
W06-3104,W04-3312,0,\N,Missing
W06-3104,N03-1021,0,\N,Missing
W06-3104,W90-0102,0,\N,Missing
W13-3403,W08-0211,1,0.886205,"Missing"
W13-3411,W02-0102,1,0.723359,"wo decades in virtual manipulatives implemented in software, including the creation of the National Library of Virtual Manipulatives.7 Both Clements and McMillen (1996) and Moyer et al. (2002) provide accessible overviews of virtual manipulatives in early math education. Virtual manipulatives give students the ability to effect changes on a complex system and so learn its underlying properties (Moyer et al., 2002). This last point is particularly relevant to log-linear models. Members of the NLP and speech communities have previously explored manipulatives and the idea of “learning by doing.” Eisner (2002) implemented HMM posterior inference and forwardbackward training on a spreadsheet, so that editing the data or initial parameters changed the numerical computations and the resulting graphs. VISPER, an applied educational tool that wrapped various speech technologies, was targeted toward understanding the acoustics and overall recognition pipeline (Nouza et al., 1997). Light et al. (2005) developed web interfaces for a number of core NLP technologies and systems, such as parsers, part-of-speech taggers, and finite-state 5 Our Log-Linear Virtual Manipulative Figure 1 shows a screenshot of the"
W13-3411,P08-1109,0,0.0318042,"Missing"
W13-3411,P99-1069,0,0.078146,"rmalizer Z(x) involves summing over a large space Y(x) (footnote 3). One 17 Even predicting the single next word in a sentence can be broken down into a sequence of binary decisions in this way. This avoids normalizing over the large vocabulary (Mnih and Hinton, 2008). 18 E.g., case, number, gender, tense, aspect, mood, lexical head. In the case of a terminal rule, the spelling or morphology of the terminal symbol can be considered. Graduating to Real Applications At the time of writing, 3266 papers in the ACL Anthology mention log-linear models, with 137 74 can restrict Y(x) before training (Johnson et al., 1999). More common is to sum efficiently by dynamic programming or sampling, as is typical in linear-chain conditional random fields (Lafferty et al., 2001), whole-sentence language modeling (Rosenfeld et al., 2001), and CRF CFGs (Finkel et al., 2008). This topic is properly deferred until such algorithmic techniques are introduced later in an NLP class, for example in a unit on parsing (see discussion in section 2). We prepare students for it by mentioning this point in our final lesson.19 Our final lesson also leads to a web page where we link to log-linear software and to various pencil-and-pape"
W13-3411,W05-0105,0,0.0317419,"properties (Moyer et al., 2002). This last point is particularly relevant to log-linear models. Members of the NLP and speech communities have previously explored manipulatives and the idea of “learning by doing.” Eisner (2002) implemented HMM posterior inference and forwardbackward training on a spreadsheet, so that editing the data or initial parameters changed the numerical computations and the resulting graphs. VISPER, an applied educational tool that wrapped various speech technologies, was targeted toward understanding the acoustics and overall recognition pipeline (Nouza et al., 1997). Light et al. (2005) developed web interfaces for a number of core NLP technologies and systems, such as parsers, part-of-speech taggers, and finite-state 5 Our Log-Linear Virtual Manipulative Figure 1 shows a screenshot of the tool, available at http://cs.jhu.edu/˜jason/ tutorials/loglin/. We encourage you to play with it as you read. 5.1 Student Interface Successive lessons introduce various challenges or subleties. In each lesson, the user experiments with modeling some given dataset D using some given set of K features. Dataset: For each context x, the outcomes y ∈ Y(x) are displayed as shapes, images or word"
W13-3411,N10-1083,0,0.0797913,"Missing"
W13-3411,Q13-1014,0,0.0176444,"objective has been regarded as the optimization dual of a maximum entropy problem (Berger et al., 1996), motivating the log-linear form of (2). We have considered adding a maximum entropy view to our manipulative. 5 Likewise for Markov or hidden Markov models. • using them as component distributions within larger probability models or decision rules, • generalizing the algorithms for working with (1) and (2) to settings where one cannot easily enumerate Y. 68 4 (Virtual) Manipulatives transducers. Matt Post created a Model 1 stack decoder visualization for a recent machine translation class (Lopez et al., 2013).8 Most manipulatives/interfaces targeted at NLP have been virtual, but a notable exception is van Halteren (2002), who created a (physical) board game for parsing. In machine learning, there is a plethora of virtual manipulatives demonstrating central concepts such as decision boundaries and kernel methods.9 There are also several systems for teaching artificial intelligence: these tend to to involve controlling virtual robots10 or physical ones (Tokic and Bou Ammar, 2012). Overall, manipulatives for NLP and ML seem to be a successful pedagogical direction that we hope will continue. Next, we"
W13-3411,J96-1002,0,0.205952,"-outside approach to computing the posterior expectation of rule counts and feature counts. Immediate variants include CRF CFGs (Finkel • using them in NLP applications that have their own complexities, 3 A caveat is that generic log-linear training tools will iterate over the set Y(x) in order to maximize (1) and to compute the constant of proportionality in (1) and the gradient of (2). This is impractical when Y(x) is large, as in language modeling or structured prediction. See Section 8. 4 Historically, this objective has been regarded as the optimization dual of a maximum entropy problem (Berger et al., 1996), motivating the log-linear form of (2). We have considered adding a maximum entropy view to our manipulative. 5 Likewise for Markov or hidden Markov models. • using them as component distributions within larger probability models or decision rules, • generalizing the algorithms for working with (1) and (2) to settings where one cannot easily enumerate Y. 68 4 (Virtual) Manipulatives transducers. Matt Post created a Model 1 stack decoder visualization for a recent machine translation class (Lopez et al., 2013).8 Most manipulatives/interfaces targeted at NLP have been virtual, but a notable exc"
W13-3411,D08-1091,0,0.0141354,"gn regularized estimators that prevent overfitting (the bias-variance tradeoff)? What is the effect of the regularization constant on small and large datasets? On rare and frequent contexts? On rare and frequent features? On useful features (including features that always or never fire) and useless ones? Students must internalize this concept and the meaning of the two counts above. This prepares them to understand the extension to structured prediction, where these counts can be more difficult to compute (see Section 8). It also prepares them to generalize to training latent-variable models (Petrov and Klein, 2008). In that setting, the observed count can no longer be observed but is replaced by another expectation under the model, conditioned on the partial training data. (4) also includes a weight decay term for regularization. We allow both `1 and `2 regularization: ~ = kθk ~ 1 versus R(θ) ~ = kθk ~ 2 . One can see R(θ) 2 experimentally that strong `1 regularization tries to use a few larger weights and leave the rest at 0, while strong `2 regularization tries to share the work among many smaller weights. One can observe how for a given C, the regularization term is 15 Specifically and in order, d3 ("
W13-3411,H94-1048,0,0.0775801,"aper title. These cover a wide range of applications that can be considered in lectures or homework projects. Early papers may cover the most fundamental applications and the clearest motivation. Conditional log-linear models were first popularized in computational linguistics by a group of researchers associated with the IBM speech and language group, who called them “maximum entropy models,” after a principle that can be used to motivate their form (Jaynes, 1957). They applied the method to various binary or multiclass classification problems in NLP, such as prepositional phrase attachment (Ratnaparkhi et al., 1994), text categorization (Nigam et al., 1999), and boundary prediction (Beeferman et al., 1999). Log-linear models can be also used for structured prediction problems in NLP such as tagging, parsing, chunking, segmentation, and language modeling. A simple strategy is to reduce structured prediction to a sequence of multiclass predictions, which can be individually made with a conditional log-linear model (Ratnaparkhi, 1998). A more fully probabilistic approach—used in the original “maximum entropy” papers—is to use (1) to define the conditional probabilities of the steps in a generative process t"
W13-3411,W02-0101,0,0.0871755,"Missing"
W13-3411,W02-2018,0,\N,Missing
W16-2002,P14-2102,1,0.883884,"Missing"
W16-2002,W16-2007,0,0.0605324,"eral tack as the previous two systems—they used a pipelined approach that first discovered an alignment between the string pairs and then discriminatively trained a transduction. The alignment algorithm employed is the same as that of the baseline system, which relies on a rich-get-richer scheme based on the Chinese restaurant process (Sudoh et al., 2013), as discussed in §5. After obtaining the alignments, they extracted edit operations based on the alignments and used a semi-Markov CRF to apply the edits in a manner very similar to the work of Durrett and DeNero (2013). BIU-MIT The BIU-MIT (Aharoni et al., 2016) team submitted two systems. Their first model, like LMU, built upon the sequence-to-sequence architecture (Sutskever et al., 2014; Bahdanau et al., 2014; Faruqui et al., 2016), but with several im16 System LMU-1 LMU-2 BIU/MIT-1 BIU/MIT-2 HEL MSU CU EHU COL/NYU OSU UA O RACLE.E Task 1 1.0 (95.56) 2.0 (95.56) — — — 3.8 (84.06) 4.6 (81.02) 5.5 (79.24) 6.5 (67.86) — 4.6 (81.83) 97.49 Standard Task 2 1.0 (96.35) 2.0 (96.23) — — — 3.6 (86.06) 5.0 (72.98) — 4.7 (75.59) — 4.7 (74.06) 98.15 Task 3 1.0 (95.83) 2.0 (95.83) — — — 3.8 (84.87) 5.0 (71.75) — 4.8 (67.61) — 4.4 (71.23) 97.97 Task 1 1.0 (95.56"
W16-2002,Q15-1031,1,0.850026,"nd then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many of the approaches taken by the shared task participants drew inspiration from work on paradigm completion. Some work, however, has considered full reinflection. For example, Dreyer and Eisner (2009) and Cotterell et al. (2015) apply graphical models with string-valued variables to model the paradigm jointly. In such models it is possible to predict values for cells in the paradigm conditioned on sets of other cells, which are not required to include the lemma. 2 a a 3 t t 4 t 5 o o 6 ssa - In general, each edit has the form copy, insert(string), delete(number), or subst(string), where subst(w) has the same effect as delete(|w|) followed by insert(w). The system treats edit sequence prediction as a sequential decision-making problem, greedily choosing each edit action given the previously chosen actions. This choice"
W16-2002,E14-1060,1,0.267684,"nd suffixes. Prefixes and suffixes are directly associated with morphological features. Stems within paradigms are further processed, using either linguistic intuitions or an empirical approach based on string alignments, to extract the stem letters that undergo changes across inflections. The extracted patterns are intended to capture stem-internal changes, such as vowel changes in Arabic. Reinflection is performed by selecting a set of changes to apply to a stem, and attaching appropriate affixes to the result. Moscow State The Moscow State system (Sorokin, 2016) is derived from the work of Ahlberg et al. (2014) and Ahlberg et al. (2015). The general idea is to use finite-state techniques to compactly model all paradigms in an abstract form called an ‘abstract paradigm’. Roughly speaking, an abstract paradigm is a set of rule transformations that derive all slots from the shared string subsequences present in each slot. Their method relies on the computation of longest common subsequence (Gusfield, 1997) to derive the abstract paradigms, which is similar to its use in the related task of lemmatization (Chrupała et al., 2008; ¨ Helsinki The Helsinki system (Ostling, 2016), like LMU and BIU-MIT, built"
W16-2002,D09-1011,1,0.861261,"tions at the paradigm level and then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many of the approaches taken by the shared task participants drew inspiration from work on paradigm completion. Some work, however, has considered full reinflection. For example, Dreyer and Eisner (2009) and Cotterell et al. (2015) apply graphical models with string-valued variables to model the paradigm jointly. In such models it is possible to predict values for cells in the paradigm conditioned on sets of other cells, which are not required to include the lemma. 2 a a 3 t t 4 t 5 o o 6 ssa - In general, each edit has the form copy, insert(string), delete(number), or subst(string), where subst(w) has the same effect as delete(|w|) followed by insert(w). The system treats edit sequence prediction as a sequential decision-making problem, greedily choosing each edit action given the previously"
W16-2002,N15-1107,1,0.936397,"work on computational approaches to inflectional morphology has focused on a special case of reinflection, where the input form is always the lemma (i.e. the citation form). Thus, the task is to generate all inflections in a paradigm from the lemma and often goes by the name of paradigm completion in the literature. There has been a flurry of recent work in this vein: Durrett and DeNero (2013) heuristically extracted transformational rules and learned a statistical model to apply the rules, Nicolai et al. (2015) tackled the problem using standard tools from discriminative string transduction, Ahlberg et al. (2015) used a finite-state construction to extract complete candidate inflections at the paradigm level and then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many of the approaches taken by the shared task participants drew inspiration from work on paradigm completio"
W16-2002,D11-1057,1,0.266047,"for the addition of bound morphemes, but also incorporation, which involves combining lexical stems that are often used to form independent words (Mithun, 1984). Such languages combine the need to decompound, generate derivational alternatives, and accurately inflect any resulting words. implemented as finite state transducers (Beesley and Karttunen, 2003), often return all morphologically plausible analyses if there is ambiguity. Learning to mimic the behavior of a hand-written analyzer in this respect could offer a more challenging task, and one that is useful within unsupervised learning (Dreyer and Eisner, 2011) as well as parsing. Existing wide-coverage morphological analyzers could be leveraged in the design of a more interactive shared task, where handcoded models or approximate surface rules could serve as informants for grammatical inference algorithms. The current task design did not explore all potential inflectional complexities in the languages included. For example, cliticization processes were generally not present in the language data. Adding such inflectional elements to the task can potentially make it more realistic in terms of real-world data sparsity in L1 learning scenarios. For exa"
W16-2002,W16-2004,0,0.0554931,"em. Like the Colorado system, they followed Durrett and DeNero (2013) and used a semi-Markov CRF to apply the edit operations. In contrast to Durrett and DeNero (2013), who employed a 0th-order model, the OSU system used a 1st-order model. A major drawback of the system was the cost of inference. The unpruned set of edit operations had over 500 elements. As the cost of inference in the model is quadratic in the size of the state space (the number of edit operations), this created a significant slowdown with over 15 days required to train in some cases. CRF (Sarawagi and Cohen, 2004). EHU EHU (Alegria and Etxeberria, 2016) took an approach based on standard grapheme-tophoneme machinery. They extend the Phonetisaurus (Novak et al., 2012) toolkit, based on the OpenFST WFST library (Allauzen et al., 2007), to the task of morphological reinflection. Their system is organized as a pipeline. Given pairs of input and output strings, the first step involves an unsupervised algorithm to extract an alignment (many-to-one or one-to-many). Then, they train the weights of the WFSTs using the imputed alignments, introducing morphological tags as symbols on the input side of the transduction. Alberta The Alberta system (Nicol"
W16-2002,D08-1113,1,0.900508,"Missing"
W16-2002,C04-1022,0,0.0394056,"inct word forms for any given 1 This latter figure rises to 52 if the entire imperfectiveperfective pair (e.g. govorit’/skazat’ ‘speak, tell’) is considered to be a single lemma. 10 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 10–22, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics forms, the probability of encountering any single word form decreases, reducing the effectiveness of frequency-based techniques in performing tasks like word alignment and language modeling (Koehn, 2010; Duh and Kirchhoff, 2004). Techniques like lemmatization and stemming can ameliorate data sparsity (Goldwater and McClosky, 2005), but these rely on morphological knowledge, particularly the mapping from inflected forms to lemmas and the list of morphs together with their ordering. Developing systems that can accurately learn and capture these mappings, overt affixes, and the principles that govern how those affixes combine is crucial to maximizing the crosslinguistic capabilities of most human language technology. The goal of the 2016 SIGMORPHON Shared Task2 was to spur the development of systems that could accuratel"
W16-2002,N13-1138,0,0.0757217,"06 1.05 Table 3: Descriptive statistics on data released to shared task participants. Figures represent averages across tasks. Abbreviations in the headers: ‘Lem’ = lemmas, ‘Full’ = number of full tags, T2T = average occurrences of tag-to-tag pairs, I-Tag & O-Tag = average occurrences of each input or output tag, resp., and ‘Sync’ = average forms per tag (syncretism). • Standard Release: Arabic, Finnish, Georgian, German, Navajo, Russian, Spanish, and Turkish • Surprise: Hungarian and Maltese Finnish, German, and Spanish have been the subject of much recent work, due to data made available by Durrett and DeNero (2013), while the other datasets used in the shared task are released here for the first time. For all languages, the word forms in the data are orthographic (not phonological) strings in the native script, except in the case of Arabic, where we used the romanized forms available from Wiktionary. An accented letter is treated as a single character. Descriptive statistics of the data are provided in Table 3. The typological character of these languages varies widely. German and Spanish inflection generation has been studied extensively, and the morphological character of the languages is similar: Bot"
W16-2002,P15-1033,0,0.00992926,"the context and they represent individual morphosyntactic attributes as well. In addition, they include template-inspired components to better cope with the templatic morphology of Arabic and Maltese. The second architecture, while also neural, more radically departs from previously proposed sequence-to-sequence models. The aligner from the baseline system is used to create a series of edit actions, similar to the systems in Camp 1. Rather than use a CRF, the BIU-MIT team predicted the sequence of edit actions using a neural model, much in the same way as a transition-based LSTM parser does (Dyer et al., 2015; Kiperwasser and Goldberg, 2016). The architectural consequence of this is that it replaces the soft alignment mechanism of (Bahdanau et al., 2014) with a hard attention mechanism, similar to Rastogi et al. (2016). Camp 3: Time for Some Linguistics The third camp relied on linguistics-inspired heuristics to reduce the problem to multi-way classification. This camp is less unified than the other two, as both teams used very different heuristics. Columbia – New York University Abu Dhabi The system developed jointly by Columbia and NYUAD (Taji et al., 2016) is based on the work of Eskander et al"
W16-2002,D13-1105,0,0.0212324,"r et al., 2015; Kiperwasser and Goldberg, 2016). The architectural consequence of this is that it replaces the soft alignment mechanism of (Bahdanau et al., 2014) with a hard attention mechanism, similar to Rastogi et al. (2016). Camp 3: Time for Some Linguistics The third camp relied on linguistics-inspired heuristics to reduce the problem to multi-way classification. This camp is less unified than the other two, as both teams used very different heuristics. Columbia – New York University Abu Dhabi The system developed jointly by Columbia and NYUAD (Taji et al., 2016) is based on the work of Eskander et al. (2013). It is unique among the submitted systems in that the first step in the pipeline is segmentation of the input words into prefixes, stems, and suffixes. Prefixes and suffixes are directly associated with morphological features. Stems within paradigms are further processed, using either linguistic intuitions or an empirical approach based on string alignments, to extract the stem letters that undergo changes across inflections. The extracted patterns are intended to capture stem-internal changes, such as vowel changes in Arabic. Reinflection is performed by selecting a set of changes to apply t"
W16-2002,N16-1077,0,0.544713,"(i.e. the citation form). Thus, the task is to generate all inflections in a paradigm from the lemma and often goes by the name of paradigm completion in the literature. There has been a flurry of recent work in this vein: Durrett and DeNero (2013) heuristically extracted transformational rules and learned a statistical model to apply the rules, Nicolai et al. (2015) tackled the problem using standard tools from discriminative string transduction, Ahlberg et al. (2015) used a finite-state construction to extract complete candidate inflections at the paradigm level and then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many of the approaches taken by the shared task participants drew inspiration from work on paradigm completion. Some work, however, has considered full reinflection. For example, Dreyer and Eisner (2009) and Cotterell et al. (2015) apply graphical models wi"
W16-2002,W14-4012,0,0.0716839,"Missing"
W16-2002,chrupala-etal-2008-learning,0,0.161107,"Missing"
W16-2002,L16-1498,1,0.655386,"Missing"
W16-2002,H05-1085,0,0.0534859,"pair (e.g. govorit’/skazat’ ‘speak, tell’) is considered to be a single lemma. 10 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 10–22, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics forms, the probability of encountering any single word form decreases, reducing the effectiveness of frequency-based techniques in performing tasks like word alignment and language modeling (Koehn, 2010; Duh and Kirchhoff, 2004). Techniques like lemmatization and stemming can ameliorate data sparsity (Goldwater and McClosky, 2005), but these rely on morphological knowledge, particularly the mapping from inflected forms to lemmas and the list of morphs together with their ordering. Developing systems that can accurately learn and capture these mappings, overt affixes, and the principles that govern how those affixes combine is crucial to maximizing the crosslinguistic capabilities of most human language technology. The goal of the 2016 SIGMORPHON Shared Task2 was to spur the development of systems that could accurately generate morphologically inflected words for a set of 10 languages based on a range of training parame"
W16-2002,J10-4005,0,0.0133976,"al of 10 distinct word forms for any given 1 This latter figure rises to 52 if the entire imperfectiveperfective pair (e.g. govorit’/skazat’ ‘speak, tell’) is considered to be a single lemma. 10 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 10–22, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics forms, the probability of encountering any single word form decreases, reducing the effectiveness of frequency-based techniques in performing tasks like word alignment and language modeling (Koehn, 2010; Duh and Kirchhoff, 2004). Techniques like lemmatization and stemming can ameliorate data sparsity (Goldwater and McClosky, 2005), but these rely on morphological knowledge, particularly the mapping from inflected forms to lemmas and the list of morphs together with their ordering. Developing systems that can accurately learn and capture these mappings, overt affixes, and the principles that govern how those affixes combine is crucial to maximizing the crosslinguistic capabilities of most human language technology. The goal of the 2016 SIGMORPHON Shared Task2 was to spur the development of sy"
W16-2002,W16-2006,0,0.0709506,"Missing"
W16-2002,N07-1047,0,0.0429849,"e. Given pairs of input and output strings, the first step involves an unsupervised algorithm to extract an alignment (many-to-one or one-to-many). Then, they train the weights of the WFSTs using the imputed alignments, introducing morphological tags as symbols on the input side of the transduction. Alberta The Alberta system (Nicolai et al., 2016) is derived from the earlier work by Nicolai et al. (2015) and is methodologically quite similar to that of EHU—an unsupervised alignment model is first applied to the training pairs to impute an alignment. In this case, they employ the M2M-aligner (Jiampojamarn et al., 2007). In contrast to EHU, Nicolai et al. (2016) do allow many-to-many alignments. After computing the alignments, they discriminatively learn a stringto-string mapping using the DirectTL+ model (Jiampojamarn et al., 2008). This model is stateof-the-art for the grapheme-to-phoneme task and is very similar to the EHU system in that it assumes a monotonic alignment and could therefore be encoded as a WFST. Despite the similarity to the EHU system, the model performs much better overall. This increase in performance may be attributable to the extensive use of language-specific heuristics, detailed in"
W16-2002,D15-1272,1,0.145283,"Missing"
W16-2002,N15-1093,0,0.142748,"te(3). This results in the output katto, via the following alignment: 1 k k Previous Work Much previous work on computational approaches to inflectional morphology has focused on a special case of reinflection, where the input form is always the lemma (i.e. the citation form). Thus, the task is to generate all inflections in a paradigm from the lemma and often goes by the name of paradigm completion in the literature. There has been a flurry of recent work in this vein: Durrett and DeNero (2013) heuristically extracted transformational rules and learned a statistical model to apply the rules, Nicolai et al. (2015) tackled the problem using standard tools from discriminative string transduction, Ahlberg et al. (2015) used a finite-state construction to extract complete candidate inflections at the paradigm level and then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many"
W16-2002,P08-1103,0,0.0216551,"s, introducing morphological tags as symbols on the input side of the transduction. Alberta The Alberta system (Nicolai et al., 2016) is derived from the earlier work by Nicolai et al. (2015) and is methodologically quite similar to that of EHU—an unsupervised alignment model is first applied to the training pairs to impute an alignment. In this case, they employ the M2M-aligner (Jiampojamarn et al., 2007). In contrast to EHU, Nicolai et al. (2016) do allow many-to-many alignments. After computing the alignments, they discriminatively learn a stringto-string mapping using the DirectTL+ model (Jiampojamarn et al., 2008). This model is stateof-the-art for the grapheme-to-phoneme task and is very similar to the EHU system in that it assumes a monotonic alignment and could therefore be encoded as a WFST. Despite the similarity to the EHU system, the model performs much better overall. This increase in performance may be attributable to the extensive use of language-specific heuristics, detailed in the paper, or the application of a discriminative reranker. 6.2 Camp 2: Revenge of the RNN A surprising result of the shared task is the large performance gap between the top performing neural models and the rest of t"
W16-2002,W16-2005,0,0.0679067,"Missing"
W16-2002,W16-2010,0,0.168052,"Missing"
W16-2002,W12-6208,0,0.0181364,"In contrast to Durrett and DeNero (2013), who employed a 0th-order model, the OSU system used a 1st-order model. A major drawback of the system was the cost of inference. The unpruned set of edit operations had over 500 elements. As the cost of inference in the model is quadratic in the size of the state space (the number of edit operations), this created a significant slowdown with over 15 days required to train in some cases. CRF (Sarawagi and Cohen, 2004). EHU EHU (Alegria and Etxeberria, 2016) took an approach based on standard grapheme-tophoneme machinery. They extend the Phonetisaurus (Novak et al., 2012) toolkit, based on the OpenFST WFST library (Allauzen et al., 2007), to the task of morphological reinflection. Their system is organized as a pipeline. Given pairs of input and output strings, the first step involves an unsupervised algorithm to extract an alignment (many-to-one or one-to-many). Then, they train the weights of the WFSTs using the imputed alignments, introducing morphological tags as symbols on the input side of the transduction. Alberta The Alberta system (Nicolai et al., 2016) is derived from the earlier work by Nicolai et al. (2015) and is methodologically quite similar to"
W16-2002,W16-2003,0,0.0522292,"rived from the work of Ahlberg et al. (2014) and Ahlberg et al. (2015). The general idea is to use finite-state techniques to compactly model all paradigms in an abstract form called an ‘abstract paradigm’. Roughly speaking, an abstract paradigm is a set of rule transformations that derive all slots from the shared string subsequences present in each slot. Their method relies on the computation of longest common subsequence (Gusfield, 1997) to derive the abstract paradigms, which is similar to its use in the related task of lemmatization (Chrupała et al., 2008; ¨ Helsinki The Helsinki system (Ostling, 2016), like LMU and BIU-MIT, built off of the sequenceto-sequence architecture, augmenting the system with several innovations. First, a single decoder was used, rather than a unique one for all possible morphological tags, which allows for additional parameter sharing, similar to LMU. More LSTM layers were also added to the decoder, creating a deeper network. Finally, a convolutional layer over the character inputs was used, which was found to significantly increase performance over models without the convolutional layers. 17 trained a neural model to predict edit operations, consistently ranked b"
W16-2002,W16-2008,0,0.0168371,"eline system are given in Table 5. Most participants in the shared task were able to outperform the baseline, often by a significant margin. 6.1 Camp 1: Align and Transduce Most of the systems in this camp drew inspiration from the work of Durrett and DeNero (2013), who extracted a set of edit operations and applied the transformations with a semi-Markov 8 Note that at training time, we know the correct lemma for S thanks to the task 1 data, which is permitted for use by task 2 in the standard track. This is also why task 2 is permitted to use the trained task 1 system. 15 OSU The OSU system (King, 2016) also used a pipelined approach. They first extracted sequences of edit operations using Hirschberg’s algorithm (Hirschberg, 1975). This reduces the string-to-string mapping problem to a sequence tagging problem. Like the Colorado system, they followed Durrett and DeNero (2013) and used a semi-Markov CRF to apply the edit operations. In contrast to Durrett and DeNero (2013), who employed a 0th-order model, the OSU system used a 1st-order model. A major drawback of the system was the cost of inference. The unpruned set of edit operations had over 500 elements. As the cost of inference in the mo"
W16-2002,Q16-1023,0,0.0186037,"ey represent individual morphosyntactic attributes as well. In addition, they include template-inspired components to better cope with the templatic morphology of Arabic and Maltese. The second architecture, while also neural, more radically departs from previously proposed sequence-to-sequence models. The aligner from the baseline system is used to create a series of edit actions, similar to the systems in Camp 1. Rather than use a CRF, the BIU-MIT team predicted the sequence of edit actions using a neural model, much in the same way as a transition-based LSTM parser does (Dyer et al., 2015; Kiperwasser and Goldberg, 2016). The architectural consequence of this is that it replaces the soft alignment mechanism of (Bahdanau et al., 2014) with a hard attention mechanism, similar to Rastogi et al. (2016). Camp 3: Time for Some Linguistics The third camp relied on linguistics-inspired heuristics to reduce the problem to multi-way classification. This camp is less unified than the other two, as both teams used very different heuristics. Columbia – New York University Abu Dhabi The system developed jointly by Columbia and NYUAD (Taji et al., 2016) is based on the work of Eskander et al. (2013). It is unique among the"
W16-2002,N16-1076,1,0.132588,"nd architecture, while also neural, more radically departs from previously proposed sequence-to-sequence models. The aligner from the baseline system is used to create a series of edit actions, similar to the systems in Camp 1. Rather than use a CRF, the BIU-MIT team predicted the sequence of edit actions using a neural model, much in the same way as a transition-based LSTM parser does (Dyer et al., 2015; Kiperwasser and Goldberg, 2016). The architectural consequence of this is that it replaces the soft alignment mechanism of (Bahdanau et al., 2014) with a hard attention mechanism, similar to Rastogi et al. (2016). Camp 3: Time for Some Linguistics The third camp relied on linguistics-inspired heuristics to reduce the problem to multi-way classification. This camp is less unified than the other two, as both teams used very different heuristics. Columbia – New York University Abu Dhabi The system developed jointly by Columbia and NYUAD (Taji et al., 2016) is based on the work of Eskander et al. (2013). It is unique among the submitted systems in that the first step in the pipeline is segmentation of the input words into prefixes, stems, and suffixes. Prefixes and suffixes are directly associated with mo"
W16-2002,W16-2009,0,0.0638163,"of the input words into prefixes, stems, and suffixes. Prefixes and suffixes are directly associated with morphological features. Stems within paradigms are further processed, using either linguistic intuitions or an empirical approach based on string alignments, to extract the stem letters that undergo changes across inflections. The extracted patterns are intended to capture stem-internal changes, such as vowel changes in Arabic. Reinflection is performed by selecting a set of changes to apply to a stem, and attaching appropriate affixes to the result. Moscow State The Moscow State system (Sorokin, 2016) is derived from the work of Ahlberg et al. (2014) and Ahlberg et al. (2015). The general idea is to use finite-state techniques to compactly model all paradigms in an abstract form called an ‘abstract paradigm’. Roughly speaking, an abstract paradigm is a set of rule transformations that derive all slots from the shared string subsequences present in each slot. Their method relies on the computation of longest common subsequence (Gusfield, 1997) to derive the abstract paradigms, which is similar to its use in the related task of lemmatization (Chrupała et al., 2008; ¨ Helsinki The Helsinki sy"
W16-2002,D13-1021,0,0.118112,"Missing"
W16-2002,P15-2111,1,0.178102,"Missing"
W16-5901,P99-1070,0,0.164182,"resents a different motivation for the inside algorithm—which then leads to an attractive independent derivation of the inside-outside algorithm. 7.1 Constructing the parse forest as a PCFG The inner weights serve to enable the construction of a convenient representation—as a PCFG— of the distribution (1). Using a grammar to represent the packed forest of all parses of w was originally discussed by Bar-Hillel et al. (1961) and Billot and Lang (1989). The construction below Nederhof and Satta (2003) takes the weighted version of such a grammar and “renormalizes” it into a PCFG (Thompson, 1974; Abney et al., 1999; Chi, 1999). This PCFG G 0 generates only parses of w: that is, it assigns weight 0 to any derivation with fringe 6= w. G 0 uses the original terminals Σ, but a different nonterminal set—namely the anchored nonterminals N 0 = {Aki : A ∈ N , 0 ≤ i &lt; k ≤ n}, with root symbol ROOT0 = ROOTn0 . The CNF rules over these nonterminals are the anchored rules R0 . The function G 0 : R0 → R≥0 can now be defined in terms of the inner weights: G 0 (Aki → Bij Cjk ) def = G(A → B C) · β[Bij ] · β[Cjk ] G 0 (Akk−1 β[Aki ] def G(A → wk ) → wk ) = =1 β[Akk−1 ] (17) (18) for A, B, C ∈ N and 0 ≤ i &lt; j &lt; k ≤ n, a"
W16-5901,N10-1083,0,0.158502,"Missing"
W16-5901,P89-1018,0,0.253757,"Pc /G(R) = (G(R) · α[R])/G(R) = α[R], since c cZc is the total weight computed in the previous paragraph. 7 The Forest PCFG For additional understanding, this section presents a different motivation for the inside algorithm—which then leads to an attractive independent derivation of the inside-outside algorithm. 7.1 Constructing the parse forest as a PCFG The inner weights serve to enable the construction of a convenient representation—as a PCFG— of the distribution (1). Using a grammar to represent the packed forest of all parses of w was originally discussed by Bar-Hillel et al. (1961) and Billot and Lang (1989). The construction below Nederhof and Satta (2003) takes the weighted version of such a grammar and “renormalizes” it into a PCFG (Thompson, 1974; Abney et al., 1999; Chi, 1999). This PCFG G 0 generates only parses of w: that is, it assigns weight 0 to any derivation with fringe 6= w. G 0 uses the original terminals Σ, but a different nonterminal set—namely the anchored nonterminals N 0 = {Aki : A ∈ N , 0 ≤ i &lt; k ≤ n}, with root symbol ROOT0 = ROOTn0 . The CNF rules over these nonterminals are the anchored rules R0 . The function G 0 : R0 → R≥0 can now be defined in terms of the inner weights:"
W16-5901,C96-1058,1,0.531048,"can be computationally advantageous, in fact. After preprocessing a PCFG, samples can be drawn in time O(n) per tree independent of the size of the grammar, by using alias sampling (Vose, 1991) to draw each rule in O(1) time. 8 Other Settings Many types of weighted grammar are used in computational linguistics. Hence one often needs to construct new inside and inside-outside algorithms (Goodman, 1998, 1999). As examples, the weighted version of Earley’s (1970) algorithm (Stolcke, 1995) handles arbitrary WCFGs (not restricted to CNF). Vijay-Shanker and Weir (1993) treat tree-adjoining grammar. Eisner (1996) handles projective dependency grammars. Smith and Smith (2007) and Koo et al. (2007) handle non-projective dependency grammars, using an inside algorithm with a different structure (not a dynamic programming algorithm). In typical settings, each T is a derivation tree (Vijay-Shankar et al., 1987)—a tree-structured recipe for assembling some syntactic description of the input sentence.QThe parse probability p(T |w) takes the form Z1 t∈T exp θt , where t ranges over certain configurations in T . Z is the total weight of all T . Then the core insight of section 4.2 holds up: given an “inside” al"
W16-5901,P02-1001,1,0.794819,"r and Blatz, 2007). 10 to partially supervised parsing (Pereira and Schabes, 1992; Matsuzaki et al., 2005), where the output tree is not fully unobserved. 8.3 Synchronous grammars In a synchronous grammar (Shieber and Schabes, 1990), a parse T is a derivation tree that produces aligned syntactic representations of a pair of sentences. These cases are handled as before. The input to the inside algorithm is a pair of aligned or unaligned sentences, or a single sentence. The simplest synchronous grammar is a finitestate transducer. Here T is an alignment of the two sentences, tagged with states. Eisner (2002) generalized the forward-backward algorithm to this setting and drew a connection to gradients. 8.4 Conditional grammars In the conditional random field (CRF) approach, p(T |w) is defined as Gw (T )/Z, where Gw is a specialized grammar constructed given the input w. Generally Gw defines weights for the anchored rules R0 , allowing the weight of a rule to be positionspecific. (This slightly affects lines 5 and 11 of Algorithm 1.) Any weighted grammar formalism may be used: e.g., the formalisms in section 2 and section 8.1 respectively yield the CRF-CFG (Finkel et al., 2008) and the popular line"
W16-5901,H05-1036,1,0.87162,"Related work Other papers have also provided significant insight into this subject. In particular, Goodman (1998, 1999) unifies most parsing algorithms as semiringweighted theorem proving, with discussion of both 2 inside and outside computations. Klein and Manning (2001) regard the resulting proof forests— traditionally called parse forests—as weighted hypergraphs. Li and Eisner (2009) show how to compute various expectations and gradients over such hypergraphs, by techniques including the insideoutside algorithm, and clarify the “wonderful” connection between expected counts and gradients. Eisner et al. (2005, section 5) observe without details that for real-weighted proof systems, the expected counts of the axioms (in our setting, grammar rules) can be obtained by applying backpropagation. They detail two ways to apply backpropagation, noting inside-outside as an example. Graphical models are like context-free grammars in that they also specify log-linear distributions over structures.1 Darwiche (2003) shows how to compute marginal posteriors (i.e., expected counts) in a graphical model by the same technique given here. 2 Definitions and Notation Assume a given alphabet Σ of terminal symbols and"
W16-5901,W06-1673,0,0.0530228,"s. By thus sampling a tree T 0 from G 0 , and then simplifying each internal node label Aki to A, we obtain a parse T of w, distributed according to p(T |w) as desired. This method is equivalent to traditional presentations of sampling from p(T |w) (Bod, 1995, 7 When β[Aki ] = 0, these relative contributions are indeterminate (quotients are 0/0). Then G 0 can specify any probability distribution over the rules R0 [Aki ]. That distribution will never be used: the PCFG G 0 has probability 0 of reaching the anchored nonterminal Aki and needing to expand it. 8 p. 56; Goodman, 1998, section 4.4.1; Finkel et al., 2006). Our presentation merely exposes the construction of G 0 as an intermediate step.8 7.3 Expected counts from the forest PCFG The above reparameterization of p(T |w) as a PCFG G 0 makes it easier to find c(Aki ). The probability of finding Aki in a parse must be the probability of encountering it when sampling a parse top-down from G 0 (the hitting probability). Observe that the top-down sampling procedure starts at ROOT0 . If it reaches Aki , it has probability G 0 (Aki → Bij Cjk ) of reaching Bij as well as Cjk on the next step. Thus, the hitting probability c(Aki ) of an anchored nonterminal"
W16-5901,P08-1109,0,0.272598,"Missing"
W16-5901,W96-0214,0,0.05418,"he substring wi+1P . . . wk is probably a constituent of type A. If also j c(Aki → Bij Cjk ) = 0.75, this A probably splits into subconstituents B and C. Even for the parsing task itself, the anchored probabilities are useful for decoding—that is, selecting a single “best” parse tree Tˆ. If the system will be rewarded for finding correct constituents, the expected reward of Tˆ is the sum of the anchored probabilities of the anchored constituents included in Tˆ. The Tˆ that maximizes this sum4 can be selected by a Viterbi-style algorithm, once all the anchored probabilities have been computed (Goodman, 1996; Matsuzaki et al., 2005). 5 Deriving the Inside-Outside Algorithm 5.1 Section 4.2 showed that the expected counts can be obtained as the partial derivatives of log Z. However, we will start by obtaining the partial derivatives of Z. This will lead to a more standard presentation of the inside-outside algorithm, exposing quantities such as the outer weights α that are both intuitive and useful. The inside algorithm can be regarded as evaluating an arithmetic circuit that has many inputs {G(R) : R ∈ R} and one output Z. Each non-input node of the circuit is a β value, which is defined as a sum"
W16-5901,J99-4004,0,0.568258,"tand, teach, develop, and implement many core algorithms of the field. Good follow-up reading includes • how to use the inside algorithm within a larger neural network that can be differentiated endto-end (Gormley et al., 2015; Gormley, 2015); • how to efficiently obtain the partial derivatives of the expected counts by differentiating the algorithm a second time (Li and Eisner, 2009); • how to navigate through the space of possible inside algorithms (Eisner and Blatz, 2007); • how to convert an inside algorithm into variant algorithms such as Viterbi parsing, k-best parsing, and recognition (Goodman, 1999); • how to apply the same ideas to graphical models (Aji and McEliece, 2000; Darwiche, 2003). Acknowledgments Thanks to anonymous referees and to Tim Vieira for useful comments that improved the paper. A Pseudocode for Inside-Outside Variants The core of this paper is Algorithms 1 and 2 in the main text. For easy reference, this appendix provides concrete pseudocode for some close variants of Algorithm 2 that are discussed in the main text, highlighting the differences from Algorithm 2. All of these variants compute the same expected counts c, with only small constant-factor differences in eff"
W16-5901,J99-1004,0,0.521958,"Z is that a probability distribution over the parses T ∈ T (w) is given by def p(T |w) = G(T )/Z (1) When G is a PCFG representing a prior distribution on parses T , (1) is its posterior after observing the fringe w. When G is a WCFG, (1) directly defines a conditional distribution on parses. Z is a sum of exponentially many products, since |T (w) |is exponential in n = |w|. Fortunately, many of the sub-products are shared across multiple summands, and can be factored out using the distributive property. This strategy leads to the above 2 For this statement to hold even for “non-tight” PCFGs (Chi, 1999), we must consider the uncountable space of all finite and infinite derivations. That requires equipping this space with an appropriate σ-algebra and defining the measure G more precisely. 3 Algorithm 1 The inside algorithm 1: function I NSIDE(G, w) 2: initialize all β[· · · ] to 0 3: for k := 1 to n : . width-1 constituents 4: for A ∈ N : 5: β[Akk−1 ] += G(A → wk ) 11: for width := 2 to n : . wider constituents for i := 0 to n − width : . start point k := i + width . end point for j := i + 1 to k − 1 : . midpoint for A, B, C ∈ N : β[Aki ] += G(A → B C)β[Bij ]β[Cjk ] 12: return Z := β[ROOTn0 ]"
W16-5901,Q15-1035,1,0.887726,"Missing"
W16-5901,P99-1069,0,0.125383,"y Markov model over the taggings. The forward pass now finds the hitting probabilities in this Markov model (cf. section 7.3), which describe how often the random walk will reach specific anchored nonterminals or traverse edges between them. These are the expected counts of tags and tag bigrams at specific positions. 8.2 Other grammar formalisms Many grammar formalisms have weighted versions that produce exponential-family distributions over tree-structured derivations: • PCFGs or WCFGs whose nonterminals are lexicalized or extended with other attributes, including unification-based grammars (Johnson et al., 1999) • Categorial grammars, which use an unbounded set of nonterminals N (bounded for any given input sentence) • Tree substitution grammars and tree adjoining grammars (Schabes, 1992), in which the derivation tree is distinct from the derived tree • History-based stochasticizations such as the structured language model (Jelinek, 2004) • Projective and non-projective dependency grammars as mentioned earlier • Semi-Markov models for chunking (Sarawagi and Cohen, 2004), with runtime O(n2 ) Each formalism has one or more inside algorithms10 that efficiently compute Z, typically in time O(n2 ) to O(n6"
W16-5901,W01-1812,0,0.0221657,"er, converting it into a non-stationary Markov model; the forward-backward algorithm computes hitting probabilities in this model. Section 8 discusses other settings where the same approach can be applied, starting with the forwardbackward algorithm for Hidden Markov Models. Two appendices work through some additional variants of the algorithms. 1.3 Related work Other papers have also provided significant insight into this subject. In particular, Goodman (1998, 1999) unifies most parsing algorithms as semiringweighted theorem proving, with discussion of both 2 inside and outside computations. Klein and Manning (2001) regard the resulting proof forests— traditionally called parse forests—as weighted hypergraphs. Li and Eisner (2009) show how to compute various expectations and gradients over such hypergraphs, by techniques including the insideoutside algorithm, and clarify the “wonderful” connection between expected counts and gradients. Eisner et al. (2005, section 5) observe without details that for real-weighted proof systems, the expected counts of the axioms (in our setting, grammar rules) can be obtained by applying backpropagation. They detail two ways to apply backpropagation, noting inside-outside"
W16-5901,D07-1015,0,0.500707,"can be drawn in time O(n) per tree independent of the size of the grammar, by using alias sampling (Vose, 1991) to draw each rule in O(1) time. 8 Other Settings Many types of weighted grammar are used in computational linguistics. Hence one often needs to construct new inside and inside-outside algorithms (Goodman, 1998, 1999). As examples, the weighted version of Earley’s (1970) algorithm (Stolcke, 1995) handles arbitrary WCFGs (not restricted to CNF). Vijay-Shanker and Weir (1993) treat tree-adjoining grammar. Eisner (1996) handles projective dependency grammars. Smith and Smith (2007) and Koo et al. (2007) handle non-projective dependency grammars, using an inside algorithm with a different structure (not a dynamic programming algorithm). In typical settings, each T is a derivation tree (Vijay-Shankar et al., 1987)—a tree-structured recipe for assembling some syntactic description of the input sentence.QThe parse probability p(T |w) takes the form Z1 t∈T exp θt , where t ranges over certain configurations in T . Z is the total weight of all T . Then the core insight of section 4.2 holds up: given an “inside” algorithm to compute log Z, we can differentiate to obtain an “inside-outside” algorith"
W16-5901,D09-1005,1,0.960075,"is model. Section 8 discusses other settings where the same approach can be applied, starting with the forwardbackward algorithm for Hidden Markov Models. Two appendices work through some additional variants of the algorithms. 1.3 Related work Other papers have also provided significant insight into this subject. In particular, Goodman (1998, 1999) unifies most parsing algorithms as semiringweighted theorem proving, with discussion of both 2 inside and outside computations. Klein and Manning (2001) regard the resulting proof forests— traditionally called parse forests—as weighted hypergraphs. Li and Eisner (2009) show how to compute various expectations and gradients over such hypergraphs, by techniques including the insideoutside algorithm, and clarify the “wonderful” connection between expected counts and gradients. Eisner et al. (2005, section 5) observe without details that for real-weighted proof systems, the expected counts of the axioms (in our setting, grammar rules) can be obtained by applying backpropagation. They detail two ways to apply backpropagation, noting inside-outside as an example. Graphical models are like context-free grammars in that they also specify log-linear distributions ov"
W16-5901,P05-1010,0,0.435546,"Missing"
W16-5901,W03-3017,0,0.0573794,"d rules in the pruned parse forest, which consists of the ˆ parse trees—with total weight Z—explored by the approximate algorithm. Many clever single-pass or multi-pass strategies exist for including the most important updates. Strategies are usually based either on direct pruning of constituents or prioritized updates with early stopping. A key insight is that β[Aki ] · ðβ[Aki ] is the total contribution of β[Aki ] to Z, so one should be sure to include the update β[Aki ] += ∆ if ∆ · α[Aki ] is predicted to be large. Weighted automata are popular for parsing, particularly dependency parsing (Nivre, 2003). In this case, T is similar to a derivation tree: it is a sequence of operations (e.g., shift and reduce) that constructs a syntactic representation. Here an approximate Zˆ is usually computed by beam search, and can be differentiated as above. 8.6 Inside-outside should be as fast as inside In all cases, it is good practice to derive one’s inside-outside algorithm by differentiation—manual or automatic—to ensure correctness and efficiency. It should be a red flag if a proposed inside-outside algorithm is asymptotically slower than its inside algorithm.11 Why? Because automatic differentiation"
W16-5901,P92-1017,0,0.661046,"has one or more inside algorithms10 that efficiently compute Z, typically in time O(n2 ) to O(n6 ) via dynamic programming. These inside algorithms can all be differentiated using the same recipe. The trick continues to work when one of these inside algorithms is extended to the case of prefix parsing or lattice parsing (Nederhof and Satta, 2003), where the input sentence is not fully observed, or 10 Even basic WCFGs admit multiple algorithms—Earley’s algorithm, unary cycle elimination, the “hook trick,” and more (see Goodman, 1998; Eisner and Blatz, 2007). 10 to partially supervised parsing (Pereira and Schabes, 1992; Matsuzaki et al., 2005), where the output tree is not fully unobserved. 8.3 Synchronous grammars In a synchronous grammar (Shieber and Schabes, 1990), a parse T is a derivation tree that produces aligned syntactic representations of a pair of sentences. These cases are handled as before. The input to the inside algorithm is a pair of aligned or unaligned sentences, or a single sentence. The simplest synchronous grammar is a finitestate transducer. Here T is an alignment of the two sentences, tagged with states. Eisner (2002) generalized the forward-backward algorithm to this setting and drew"
W16-5901,C92-2066,0,0.628596,"anchored nonterminals or traverse edges between them. These are the expected counts of tags and tag bigrams at specific positions. 8.2 Other grammar formalisms Many grammar formalisms have weighted versions that produce exponential-family distributions over tree-structured derivations: • PCFGs or WCFGs whose nonterminals are lexicalized or extended with other attributes, including unification-based grammars (Johnson et al., 1999) • Categorial grammars, which use an unbounded set of nonterminals N (bounded for any given input sentence) • Tree substitution grammars and tree adjoining grammars (Schabes, 1992), in which the derivation tree is distinct from the derived tree • History-based stochasticizations such as the structured language model (Jelinek, 2004) • Projective and non-projective dependency grammars as mentioned earlier • Semi-Markov models for chunking (Sarawagi and Cohen, 2004), with runtime O(n2 ) Each formalism has one or more inside algorithms10 that efficiently compute Z, typically in time O(n2 ) to O(n6 ) via dynamic programming. These inside algorithms can all be differentiated using the same recipe. The trick continues to work when one of these inside algorithms is extended to"
W16-5901,C90-3045,0,0.554759,"n all be differentiated using the same recipe. The trick continues to work when one of these inside algorithms is extended to the case of prefix parsing or lattice parsing (Nederhof and Satta, 2003), where the input sentence is not fully observed, or 10 Even basic WCFGs admit multiple algorithms—Earley’s algorithm, unary cycle elimination, the “hook trick,” and more (see Goodman, 1998; Eisner and Blatz, 2007). 10 to partially supervised parsing (Pereira and Schabes, 1992; Matsuzaki et al., 2005), where the output tree is not fully unobserved. 8.3 Synchronous grammars In a synchronous grammar (Shieber and Schabes, 1990), a parse T is a derivation tree that produces aligned syntactic representations of a pair of sentences. These cases are handled as before. The input to the inside algorithm is a pair of aligned or unaligned sentences, or a single sentence. The simplest synchronous grammar is a finitestate transducer. Here T is an alignment of the two sentences, tagged with states. Eisner (2002) generalized the forward-backward algorithm to this setting and drew a connection to gradients. 8.4 Conditional grammars In the conditional random field (CRF) approach, p(T |w) is defined as Gw (T )/Z, where Gw is a spe"
W16-5901,D07-1014,0,0.133848,"eprocessing a PCFG, samples can be drawn in time O(n) per tree independent of the size of the grammar, by using alias sampling (Vose, 1991) to draw each rule in O(1) time. 8 Other Settings Many types of weighted grammar are used in computational linguistics. Hence one often needs to construct new inside and inside-outside algorithms (Goodman, 1998, 1999). As examples, the weighted version of Earley’s (1970) algorithm (Stolcke, 1995) handles arbitrary WCFGs (not restricted to CNF). Vijay-Shanker and Weir (1993) treat tree-adjoining grammar. Eisner (1996) handles projective dependency grammars. Smith and Smith (2007) and Koo et al. (2007) handle non-projective dependency grammars, using an inside algorithm with a different structure (not a dynamic programming algorithm). In typical settings, each T is a derivation tree (Vijay-Shankar et al., 1987)—a tree-structured recipe for assembling some syntactic description of the input sentence.QThe parse probability p(T |w) takes the form Z1 t∈T exp θt , where t ranges over certain configurations in T . Z is the total weight of all T . Then the core insight of section 4.2 holds up: given an “inside” algorithm to compute log Z, we can differentiate to obtain an “in"
W16-5901,W03-3016,0,0.398698,"cZc is the total weight computed in the previous paragraph. 7 The Forest PCFG For additional understanding, this section presents a different motivation for the inside algorithm—which then leads to an attractive independent derivation of the inside-outside algorithm. 7.1 Constructing the parse forest as a PCFG The inner weights serve to enable the construction of a convenient representation—as a PCFG— of the distribution (1). Using a grammar to represent the packed forest of all parses of w was originally discussed by Bar-Hillel et al. (1961) and Billot and Lang (1989). The construction below Nederhof and Satta (2003) takes the weighted version of such a grammar and “renormalizes” it into a PCFG (Thompson, 1974; Abney et al., 1999; Chi, 1999). This PCFG G 0 generates only parses of w: that is, it assigns weight 0 to any derivation with fringe 6= w. G 0 uses the original terminals Σ, but a different nonterminal set—namely the anchored nonterminals N 0 = {Aki : A ∈ N , 0 ≤ i &lt; k ≤ n}, with root symbol ROOT0 = ROOTn0 . The CNF rules over these nonterminals are the anchored rules R0 . The function G 0 : R0 → R≥0 can now be defined in terms of the inner weights: G 0 (Aki → Bij Cjk ) def = G(A → B C) · β[Bij ] ·"
W16-5901,D16-1206,1,0.803667,"ower than its inside algorithm.11 Why? Because automatic differentiation produces an adjoint circuit that is at most twice the size of the original circuit (assuming binary operators). That means ∇ log Z always can be evaluated with the same asymptotic runtime as log Z. Good researchers do sometimes slip up and publish less efficient algorithms. Koo et al. (2007) present the O(n3 ) algorithms for non-projective dependency parsing: they point out that a contemporaneous IWPT paper with the same O(n3 ) inside algorithm had somehow raised inside-outside’s runtime from O(n3 ) to O(n5 ). Similarly, Vieira et al. (2016) provide efficient algorithms for variableorder linear-chain CRFs, but note that a JMLR paper with the same forward algorithm had raised the grammar constant in forward-backward’s runtime. 11 Unless inside-outside has been deliberately slowed down to reduce its space requirements, as in the discard-and-recompute scheme of Zweig and Padmanabhan (2000). Absent such a scheme, inside-outside may need asymptotically more space than inside: though the adjoint circuit is not much bigger than the original, evaluating it may require keeping more of the original in memory at once (unless time is traded"
W16-5901,P87-1015,0,0.55804,"Missing"
W16-5901,J93-4002,0,0.364154,"that they cancel out as paths are extended. 8 Exposing G 0 can be computationally advantageous, in fact. After preprocessing a PCFG, samples can be drawn in time O(n) per tree independent of the size of the grammar, by using alias sampling (Vose, 1991) to draw each rule in O(1) time. 8 Other Settings Many types of weighted grammar are used in computational linguistics. Hence one often needs to construct new inside and inside-outside algorithms (Goodman, 1998, 1999). As examples, the weighted version of Earley’s (1970) algorithm (Stolcke, 1995) handles arbitrary WCFGs (not restricted to CNF). Vijay-Shanker and Weir (1993) treat tree-adjoining grammar. Eisner (1996) handles projective dependency grammars. Smith and Smith (2007) and Koo et al. (2007) handle non-projective dependency grammars, using an inside algorithm with a different structure (not a dynamic programming algorithm). In typical settings, each T is a derivation tree (Vijay-Shankar et al., 1987)—a tree-structured recipe for assembling some syntactic description of the input sentence.QThe parse probability p(T |w) takes the form Z1 t∈T exp θt , where t ranges over certain configurations in T . Z is the total weight of all T . Then the core insight o"
W16-5901,H92-1024,0,\N,Missing
W19-4439,E17-2025,0,0.0372202,"from among the exponentially many choices. Our approach assumes a 1-to-1 correspondence (i.e. gloss) is available for each L1 token. Clearly, this is not true in general, so we only focus on mixed-language configurations when 1-to-1 glosses are possible. If a particular L1 token does not have a gloss, we only consider configurations where that token is always represented in L1. 1 By “meaning” we mean the L1 token that was originally in the sentence before it was replaced by an L2 gloss. Note that the softmax layer also uses the word embedding matrix E when generating the output distribution (Press and Wolf, 2017). This cloze language model encodes left-and-right contextual dependence rather than the typical sequence dependence of standard (unidirectional) language models. We train the parameters θ = [θ f ; θ b ; θ h ; E] using P Adam (Kingma and Ba, 2014) to maximize x L(x), where the summation is over sentences x in a large L1 training corpus. X L(x) = log p(xt |[hf t : hb t ]) (4) t We assume that the resulting model represents the entirety of the student’s L1 knowledge, and that the L1 parameters θ will not change further. 2.2 Incremental L2 Vocabulary Learning The model so far can assign probabili"
W19-4439,Q17-1010,0,0.0932253,"Missing"
W19-4439,N19-1423,0,0.0525518,"Missing"
W19-4439,P14-1053,0,0.512754,"ries, articles etc.). The machine teacher will take a sentence in the student’s native language (L1) and replace certain words with their foreign-language (L2) translations, resulting in a mixed-language sentence. We hope that reading mixed-language documents does not feel like a traditional vocabulary learning drill even though novel L2 words can be picked up over time. We envision our method being used alongside traditional foreign-language instruction. Typically, a machine teacher would require supervised data, meaning data on student behaviors and capabilities (Renduchintala et al., 2016; Labutov and Lipson, 2014). This step is expensive, not only from a data collection point of view, but also from the point of view of students, as they would have to give feedback (i.e. generate labeled data) on the actions of an initially untrained machine teacher. However, our machine teacher requires no supervised data from human students. Instead, it uses a cloze language model trained on corpora from the student’s native language as a proxy for a human student. Our machine teacher consults this proxy to guide its construction of mixed-language data. Moreover, we create an evaluation dataset that allows us to deter"
W19-4439,E17-1096,0,0.139818,"s are expected to use the surrounding words to make their guesses). Furthermore, we select the best performing variant and evaluate if participants can actually learn the L2 words by letting participants read a mixed-language passage and give a L2 vocabulary quiz at the end of passage, where the L2 words are presented in isolation. 2 Approach Student Proxy Model Before we address the aforementioned question, we must introduce our student proxy model. Concretely, our student proxy model is a cloze language model that uses bidirectional LSTMs to predicts L1 words from their surrounding context (Mousa and Schuller, 2017; Hochreiter and Schmidhuber, 1997). We refer to it as the cLM (cloze language model). Given a L1 sentence [x1 , x2 , ... , xT ], the model defines a distribution p(xt |[hf : hf ]) at each position in the sentence. Here, hf and hb are D−dimensional hidden states from forward and backward LSTMs. hf t = LSTMf ([x1 ,...,xt−1 ];θ f ) b b b h t = LSTM ([xt+1 ,...,xT ];θ ) (1) (2) The cLM assumes a fixed L1 vocabulary of size V , and the vectors xt above are embeddings of these word types, which correspond to the rows of a matrix E ∈ RV ×D . The output distribution (over V word types) is obtained by"
W19-4439,P16-1175,1,0.824807,"occur in narrative text (stories, articles etc.). The machine teacher will take a sentence in the student’s native language (L1) and replace certain words with their foreign-language (L2) translations, resulting in a mixed-language sentence. We hope that reading mixed-language documents does not feel like a traditional vocabulary learning drill even though novel L2 words can be picked up over time. We envision our method being used alongside traditional foreign-language instruction. Typically, a machine teacher would require supervised data, meaning data on student behaviors and capabilities (Renduchintala et al., 2016; Labutov and Lipson, 2014). This step is expensive, not only from a data collection point of view, but also from the point of view of students, as they would have to give feedback (i.e. generate labeled data) on the actions of an initially untrained machine teacher. However, our machine teacher requires no supervised data from human students. Instead, it uses a cloze language model trained on corpora from the student’s native language as a proxy for a human student. Our machine teacher consults this proxy to guide its construction of mixed-language data. Moreover, we create an evaluation data"
