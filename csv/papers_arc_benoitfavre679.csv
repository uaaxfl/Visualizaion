2020.sigdial-1.25,Filtering conversations through dialogue acts labels for improving corpus-based convergence studies,2020,-1,-1,2,0,14953,simone fuscone,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Cognitive models of conversation and research on user-adaptation in dialogue systems involves a better understanding of speakers convergence in conversation. Convergence effects have been established on controlled data sets, for various acoustic and linguistic variables. Tracking interpersonal dynamics on generic corpora has provided positive but more contrasted outcomes. We propose here to enrich large conversational corpora with dialogue act (DA) information. We use DA-labels as filters in order to create data sub sets featuring homogeneous conversational activity. Those data sets allow a more precise comparison between speakers{'} speech variables. Our experiences consist of comparing convergence on low level variables (Energy, Pitch, Speech Rate) measured on raw data sets, with human and automatically DA-labelled data sets. We found that such filtering does help in observing convergence suggesting that studies on interpersonal dynamics should consider such high level dialogue activity types and their related NLP topics as important ingredients of their toolboxes."
2020.jeptalnrecital-demos.19,Analyse s{\\'e}mantique robuste par apprentissage antagoniste pour la g{\\'e}n{\\'e}ralisation de domaine (Robust Semantic Parsing with Adversarial Learning for Domain Generalization ),2020,-1,-1,4,0.666667,184,gabriel marzinotto,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 4 : D{\\'e}monstrations et r{\\'e}sum{\\'e}s d'articles internationaux",0,"Nous pr{\'e}sentons des r{\'e}sum{\'e}s en fran{\c{c}}ais et en anglais de l{'}article (Marzinotto et al., 2019) pr{\'e}sent{\'e} {\`a} la conf{\'e}rence North American Chapter of the Association for Computational Linguistics : Human Language Technologies en 2019."
2020.cmcl-1.7,Development of Multi-level Linguistic Alignment in Child-adult Conversations,2020,-1,-1,2,0,21830,thomas misiek,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,0,"Interactive alignment is a major mechanism of linguistic coordination. Here we study the way this mechanism emerges in development across the lexical, syntactic, and conceptual levels. We leverage NLP tools to analyze a large-scale corpus of child-adult conversations between 2 and 5 years old. We found that, across development, children align consistently to adults above chance and that adults align consistently more to children than vice versa (even controlling for language production abilities). Besides these consistencies, we found a diversity of developmental trajectories across linguistic levels. These corpus-based findings provide strong support for an early onset of multi-level linguistic alignment in children and invites new experimental work."
N19-2021,Robust Semantic Parsing with Adversarial Learning for Domain Generalization,2019,22,3,4,0.666667,184,gabriel marzinotto,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)",0,"This paper addresses the issue of generalization for Semantic Parsing in an adversarial framework. Building models that are more robust to inter-document variability is crucial for the integration of Semantic Parsing technologies in real applications. The underlying question throughout this study is whether adversarial learning can be used to train models on a higher level of abstraction in order to increase their robustness to lexical and stylistic variations. We propose to perform Semantic Parsing with a domain classification adversarial task, covering various use-cases with or without explicit knowledge of the domain. The strategy is first evaluated on a French corpus of encyclopedic documents, annotated with FrameNet, in an information retrieval perspective. This corpus constitutes a new public benchmark, gathering documents from various thematic domains and various sources. We show that adversarial learning yields improved results when using explicit domain classification as the adversarial task. We also propose an unsupervised domain discovery approach that yields equivalent improvements. The latter is also evaluated on a PropBank Semantic Role Labeling task on the CoNLL-2005 benchmark and is shown to increase the model{'}s generalization capabilities on out-of-domain data."
N19-1393,Typological Features for Multilingual Delexicalised Dependency Parsing,2019,0,1,4,1,26277,manon scholivet,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,The existence of universal models to describe the syntax of languages has been debated for decades. The availability of resources such as the Universal Dependencies treebanks and the World Atlas of Language Structures make it possible to study the plausibility of universal grammar from the perspective of dependency parsing. Our work investigates the use of high-level language descriptions in the form of typological features for multilingual dependency parsing. Our experiments on multilingual parsing for 40 languages show that typological information can indeed guide parsers to share information between similar languages beyond simple language identification.
W18-4933,{V}eyn at {PARSEME} Shared Task 2018: Recurrent Neural Networks for {VMWE} Identification,2018,0,1,4,0,23917,nicolas zampieri,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"This paper describes the Veyn system, submitted to the closed track of the PARSEME Shared Task 2018 on automatic identification of verbal multiword expressions (VMWEs). Veyn is based on a sequence tagger using recurrent neural networks. We represent VMWEs using a variant of the begin-inside-outside encoding scheme combined with the VMWE category tag. In addition to the system description, we present development experiments to determine the best tagging scheme. Veyn is freely available, covers 19 languages, and was ranked ninth (MWE-based) and eight (Token-based) among 13 submissions, considering macro-averaged F1 across languages."
L18-1716,Adding Syntactic Annotations to Flickr30k Entities Corpus for Multimodal Ambiguous Prepositional-Phrase Attachment Resolution,2018,0,1,4,1,30320,sebastien delecraz,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,We propose in this paper to add to the captions of the Flickr30k Entities corpus some syntactic annotations in order to study the joint processing of image and language features for the Preposition-Phrase attachment disambiguation task. The annotation has been performed on the English version of the captions and automatically projected on their French and German translations.
2018.jeptalnrecital-long.13,Correction automatique d{'}attachements pr{\\'e}positionnels par utilisation de traits visuels ({PP}-attachement resolution using visual features),2018,-1,-1,3,1,30320,sebastien delecraz,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"La d{\'e}sambigu{\""\i}sation des rattachements pr{\'e}positionnels est une t{\^a}che syntaxique qui demande des connaissances s{\'e}mantiques, pouvant {\^e}tre extraites d{'}une image associ{\'e}e au texte trait{\'e}. Nous pr{\'e}sentons et analysons les difficult{\'e}s de cette t{\^a}che pour laquelle nous construisons un syst{\`e}me complet entra{\^\i}n{\'e} sur une version {\'e}tendue des annotations du corpus Flickr30k Entities. Lorsque la s{\'e}mantique lexicale n{'}est pas disponible, l{'}information visuelle apporte 3 {\%} d{'}am{\'e}lioration."
2018.jeptalnrecital-court.4,Evaluation automatique de la satisfaction client {\\`a} partir de conversations de type {``}chat{''} par r{\\'e}seaux de neurones r{\\'e}currents avec m{\\'e}canisme d{'}attention (Customer satisfaction prediction with attention-based {RNN}s from a chat contact center corpus),2018,-1,-1,4,1,27341,jeremy auguste,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,Cet article pr{\'e}sente des m{\'e}thodes permettant l{'}{\'e}valuation de la satisfaction client {\`a} partir de tr{\`e}s vastes corpus de conversation de type {``}chat{''} entre des clients et des op{\'e}rateurs. Extraire des connaissances dans ce contexte demeure un d{\'e}fi pour les m{\'e}thodes de traitement automatique des langues de par la dimension interactive et les propri{\'e}t{\'e}s de ce nouveau type de langage {\`a} l{'}intersection du langage {\'e}crit et parl{\'e}. Nous pr{\'e}sentons une {\'e}tude utilisant des r{\'e}ponses {\`a} des sondages utilisateurs comme supervision faible permettant de pr{\'e}dire la satisfaction des usagers d{'}un service en ligne d{'}assistance technique et commerciale.
2018.jeptalnrecital-court.5,D{\\'e}tection d{'}erreurs dans des transcriptions {OCR} de documents historiques par r{\\'e}seaux de neurones r{\\'e}currents multi-niveau (Combining character level and word level {RNN}s for post-{OCR} error detection),2018,-1,-1,3,0,30983,thibault magallon,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Le traitement {\`a} posteriori de transcriptions OCR cherche {\`a} d{\'e}tecter les erreurs dans les sorties d{'}OCR pour tenter de les corriger, deux t{\^a}ches {\'e}valu{\'e}es par la comp{\'e}tition ICDAR-2017 Post-OCR Text Correction. Nous pr{\'e}senterons dans ce papier un syst{\`e}me de d{\'e}tection d{'}erreurs bas{\'e} sur un mod{\`e}le {\`a} r{\'e}seaux r{\'e}currents combinant une analyse du texte au niveau des mots et des caract{\`e}res en deux temps. Ce syst{\`e}me a {\'e}t{\'e} class{\'e} second dans trois cat{\'e}gories {\'e}valu{\'e}es parmi 11 candidats lors de la comp{\'e}tition."
W17-6311,Correcting prepositional phrase attachments using multimodal corpora,2017,7,1,4,1,30320,sebastien delecraz,Proceedings of the 15th International Conference on Parsing Technologies,0,"PP-attachments are an important source of errors in parsing natural language. We propose in this article to use data coming from a multimodal corpus, combining textual, visual and conceptual information, as well as a correction strategy, to propose alternative attachments in the output of a parser."
W17-5304,Evaluation of word embeddings against cognitive processes: primed reaction times in lexical decision and naming tasks,2017,0,3,3,1,27341,jeremy auguste,Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP},0,"This work presents a framework for word similarity evaluation grounded on cognitive sciences experimental data. Word pair similarities are compared to reaction times of subjects in large scale lexical decision and naming tasks under semantic priming. Results show that GloVe embeddings lead to significantly higher correlation with experimental measurements than other controlled and off-the-shelf embeddings, and that the choice of a training corpus is less important than that of the algorithm. Comparison of rankings with other datasets shows that the cognitive phenomenon covers more aspects than simply word relatedness or similarity."
W17-1001,{M}ulti{L}ing 2017 Overview,2017,7,2,8,1,17116,george giannakopoulos,Proceedings of the {M}ulti{L}ing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres,0,"In this brief report we present an overview of the MultiLing 2017 effort and workshop, as implemented within EACL 2017. MultiLing is a community-driven initiative that pushes the state-of-the-art in Automatic Summarization by providing data sets and fostering further research and development of summarization systems. This year the scope of the workshop was widened, bringing together researchers that work on summarization across sources, languages and genres. We summarize the main tasks planned and implemented this year, the contributions received, and we also provide insights on next steps."
2017.jeptalnrecital-demo.5,Apprentissage d{'}agents conversationnels pour la gestion de relations clients (Training chatbots for customer relation management),2017,-1,-1,1,1,14954,benoit favre,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 3 - D{\\'e}monstrations,0,"Ce travail d{\'e}montre la faisabilit{\'e} d{'}entra{\^\i}ner des chatbots sur des traces de conversations dans le domaine de la relation client. Des syst{\`e}mes {\`a} base de mod{\`e}les de langage, de recherche d{'}information et de traduction sont compar{\'e}s pour la t{\^a}che."
2017.jeptalnrecital-court.7,D{\\'e}tection de cor{\\'e}f{\\'e}rences de bout en bout en fran{\\c{c}}ais (End-to-end coreference resolution for {F}rench),2017,-1,-1,2,0,33232,elisabeth godbert,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"Notre objectif est l{'}{\'e}laboration d{'}un syst{\`e}me de d{\'e}tection automatique de relations de cor{\'e}f{\'e}rence le plus g{\'e}n{\'e}ral possible, pour le traitement des anaphores pronominales et les cor{\'e}f{\'e}rences directes. Nous d{\'e}crivons dans cet article les diff{\'e}rentes {\'e}tapes de traitement des textes dans le syst{\`e}me que nous avons d{\'e}velopp{\'e} : (i) l{'}annotation en traits lexicaux et syntaxiques par le syst{\`e}me Macaon ; (ii) le rep{\'e}rage des mentions par un mod{\`e}le obtenu par apprentissage sur le corpus ANCOR ; (iii) l{'}annotation s{\'e}mantique des mentions {\`a} partir de deux ressources : le DEM et le LVF ; (iv) l{'}annotation en cor{\'e}f{\'e}rences par un syst{\`e}me {\`a} base de r{\`e}gles. Le syst{\`e}me est {\'e}valu{\'e} sur le corpus ANCOR."
S16-1030,{SENSEI}-{LIF} at {S}em{E}val-2016 Task 4: Polarity embedding fusion for robust sentiment analysis,2016,19,20,2,0,32351,mickael rouvier,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"This paper describes the system developed at LIF for the SemEval-2016 evaluation campaign. The goal of Task 4.A was to identify sentiment polarity in tweets. The system extends the Convolutional Neural Networks (CNN) state of the art approach. We initialize the input representations with embeddings trained on different units: lexical, partof-speech, and sentiment embeddings. Neural networks for each input space are trained separately, and then the representations extracted from their hidden layers are concatenated as input of a fusion neural network. The system ranked 2nd at SemEval-2016 and obtained an average F1 of 63.0%."
L16-1046,Word Embedding Evaluation and Combination,2016,0,31,2,0,862,sahar ghannay,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Word embeddings have been successfully used in several natural language processing tasks (NLP) and speech processing. Different approaches have been introduced to calculate word embeddings through neural networks. In the literature, many studies focused on word embedding evaluation, but for our knowledge, there are still some gaps. This paper presents a study focusing on a rigorous comparison of the performances of different kinds of word embeddings. These performances are evaluated on different NLP and linguistic tasks, while all the word embeddings are estimated on the same training data using the same vocabulary, the same number of dimensions, and other similar characteristics. The evaluation results reported in this paper match those in the literature, since they point out that the improvements achieved by a word embedding in one task are not consistently observed across all tasks. For that reason, this paper investigates and evaluates approaches to combine word embeddings in order to take advantage of their complementarity, and to look for the effective word embeddings that can achieve good performances on all tasks. As a conclusion, this paper provides new perceptions of intrinsic qualities of the famous word embedding families, which can be different from the ones provided by works previously published in the scientific literature."
L16-1070,A Document Repository for Social Media and Speech Conversations,2016,4,1,3,0,33317,adam funk,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a successfully implemented document repository REST service for flexible SCRUD (search, crate, read, update, delete) storage of social media conversations, using a GATE/TIPSTER-like document object model and providing a query language for document features. This software is currently being used in the SENSEI research project and will be published as open-source software before the project ends. It is, to the best of our knowledge, the first freely available, general purpose data repository to support large-scale multimodal (i.e., speech or text) conversation analytics."
L16-1701,Summarizing Behaviours: An Experiment on the Annotation of Call-Centre Conversations,2016,0,1,4,0,2753,morena danieli,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Annotating and predicting behavioural aspects in conversations is becoming critical in the conversational analytics industry. In this paper we look into inter-annotator agreement of agent behaviour dimensions on two call center corpora. We find that the task can be annotated consistently over time, but that subjectivity issues impacts the quality of the annotation. The reformulation of some of the annotated dimensions is suggested in order to improve agreement."
2016.jeptalnrecital-long.5,D{\\'e}tection de concepts pertinents pour le r{\\'e}sum{\\'e} automatique de conversations par recombinaison de patrons (Relevant concepts detection for the automatic summary of conversations using patterns recombination ),2016,-1,-1,2,1,35943,jeremy trione,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"automatique de conversations par recombinaison de patrons J{\'e}r{\'e}my Trione Benoit Favre Fr{\'e}d{\'e}ric B{\'e}chet Aix-Marseille Universit{\'e}, CNRS, LIF UMR 7279, 13000, Marseille, France pr{\'e}nom.nom@lif.univ-mrs.fr R {\'E}SUM{\'E} Ce papier d{\'e}crit une approche pour cr{\'e}er des r{\'e}sum{\'e}s de conversations parl{\'e}es par remplissage de patrons. Les patrons sont g{\'e}n{\'e}r{\'e}s automatiquement {\`a} partir de fragments g{\'e}n{\'e}ralis{\'e}s depuis un corpus de r{\'e}sum{\'e}s d{'}apprentissage. Les informations n{\'e}cessaires pour remplir les patrons sont d{\'e}tect{\'e}es dans les transcriptions des conversations et utilis{\'e}es pour s{\'e}lectionner les fragments candidats. L{'}approche obtient un score ROUGE-2 de 0.116 sur le corpus RATP-DECODA. Les r{\'e}sultats obtenus montrent que cette approche abstractive est plus performante que les approches extractives utilis{\'e}es habituellement dans le domaine du r{\'e}sum{\'e} automatique."
2016.jeptalnrecital-jep.41,Fusion d{'}espaces de repr{\\'e}sentations multimodaux pour la reconnaissance du r{\\^o}le du locuteur dans des documents t{\\'e}l{\\'e}visuels (Multimodal embedding fusion for robust speaker role recognition in video broadcast ),2016,-1,-1,3,1,30320,sebastien delecraz,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 1 : JEP,0,"L{'}identification du r{\^o}le d{'}un locuteur dans des {\'e}missions de t{\'e}l{\'e}vision est un probl{\`e}me de classification de personne selon une liste de r{\^o}les comme pr{\'e}sentateur, journaliste, invit{\'e}, etc. {\`A} cause de la nonsynchronie entre les modalit{\'e}s, ainsi que par le manque de corpus de vid{\'e}os annot{\'e}es dans toutes les modalit{\'e}s, seulement une des modalit{\'e}s est souvent utilis{\'e}e. Nous pr{\'e}sentons dans cet article une fusion multimodale des espaces de repr{\'e}sentations de l{'}audio, du texte et de l{'}image pour la reconnaissance du r{\^o}le du locuteur pour des donn{\'e}es asynchrones. Les espaces de repr{\'e}sentations monomodaux sont entra{\^\i}n{\'e}s sur des corpus de donn{\'e}es exog{\`e}nes puis ajust{\'e}s en utilisant des r{\'e}seaux de neurones profonds sur un corpus d{'}{\'e}missions fran{\c{c}}aises pour notre t{\^a}che de classification. Les exp{\'e}riences r{\'e}alis{\'e}es sur le corpus de donn{\'e}es REPERE ont mis en {\'e}vidence les gains d{'}une fusion au niveau des espaces de repr{\'e}sentations par rapport aux m{\'e}thodes de fusion tardive standard."
W15-4633,Call Centre Conversation Summarization: A Pilot Task at Multiling 2015,2015,11,5,1,1,14954,benoit favre,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"This paper describes the results of the Call Centre Conversation Summarization task at Multilingxe2x80x9915. The CCCS task consists in generating abstractive synopses from call centre conversations between a caller and an agent. Synopses are summaries of the problem of the caller, and how it is solved by the agent. Generating them is a very challenging task given that deep analysis of the dialogs and text generation are necessary. Three languages were addressed: French, Italian and English translations of conversations from those two languages. The official evaluation metric was ROUGE-2. Two participants submitted a total of four systems which had trouble beating the extractive baselines. The datasets released for the task will allow more research on abstractive dialog summarization."
W15-4638,"{M}ulti{L}ing 2015: Multilingual Summarization of Single and Multi-Documents, On-line Fora, and Call-center Conversations",2015,16,22,5,1,17116,george giannakopoulos,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"In this paper we present an overview of MultiLing 2015, a special session at SIGdial 2015. MultiLing is a communitydriven initiative that pushes the state-ofthe-art in Automatic Summarization by providing data sets and fostering further research and development of summarization systems. There were in total 23 participants this year submitting their system outputs to one or more of the four tasks of MultiLing: MSS, MMS, OnForumS and CCCS. We provide a brief overview of each task and its participation and evaluation."
W15-0212,Rapid {F}rame{N}et annotation of spoken conversation transcripts,2015,8,3,3,1,35943,jeremy trione,Proceedings of the 11th Joint {ACL}-{ISO} Workshop on Interoperable Semantic Annotation ({ISA}-11),0,None
D15-1220,Concept-based Summarization using Integer Linear Programming: From Concept Pruning to Multiple Optimal Solutions,2015,14,26,3,0,4245,florian boudin,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"In concept-based summarization, sentence selection is modelled as a budgeted maximum coverage problem. As this problem is NP-hard, pruning low-weight concepts is required for the solver to find optimal solutions efficiently. This work shows that reducing the number of concepts in the model leads to lower Rouge scores, and more importantly to the presence of multiple optimal solutions. We address these issues by extending the model to provide a single optimal solution, and eliminate the need for concept pruning using an approximation algorithm that achieves comparable performance to exact inference."
hong-etal-2014-repository,A Repository of State of the Art and Competitive Baseline Summaries for Generic News Summarization,2014,32,75,3,0,37752,kai hong,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In the period since 2004, many novel sophisticated approaches for generic multi-document summarization have been developed. Intuitive simple approaches have also been shown to perform unexpectedly well for the task. Yet it is practically impossible to compare the existing approaches directly, because systems have been evaluated on different datasets, with different evaluation measures, against different sets of comparison systems. Here we present a corpus of summaries produced by several state-of-the-art extractive summarization systems or by popular baseline systems. The inputs come from the 2004 DUC evaluation, the latest year in which generic summarization was addressed in a shared task. We use the same settings for ROUGE automatic evaluation to compare the systems directly and analyze the statistical significance of the differences in performance. We show that in terms of average scores the state-of-the-art systems appear similar but that in fact they produce very different summaries. Our corpus will facilitate future research on generic summarization and motivates the need for development of more sensitive evaluation measures and for approaches to system combination in summarization."
nasr-etal-2014-automatically,Automatically enriching spoken corpora with syntactic information for linguistic studies,2014,7,7,3,0,5812,alexis nasr,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,Syntactic parsing of speech transcriptions faces the problem of the presence of disfluencies that break the syntactic structure of the utterances. We propose in this paper two solutions to this problem. The first one relies on a disfluencies predictor that detects disfluencies and removes them prior to parsing. The second one integrates the disfluencies in the syntactic structure of the utterances and train a disfluencies aware parser.
W12-3412,Generative Constituent Parsing and Discriminative Dependency Reranking: Experiments on {E}nglish and {F}rench,2012,26,3,2,0.89685,5824,joseph roux,Proceedings of the {ACL} 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages,0,We present an architecture for parsing in two steps. A phrase-structure parser builds for each sentence ann-best list of analyses which are converted to dependency trees. These dependency structures are then rescored by a discriminative reranker. Our method is language agnostic and enables the incorporation of additional information which are useful for the choice of the best parse candidate. We test our approach on the the Penn Treebank and the French Treebank. Evaluation shows a significative improvement on different parse metrics.
bazillon-etal-2012-syntactic,Syntactic annotation of spontaneous speech: application to call-center conversation data,2012,10,8,5,0,39875,thierry bazillon,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the syntactic annotation process of the DECODA corpus. This corpus contains manual transcriptions of spoken conversations recorded in the French call-center of the Paris Public Transport Authority (RATP). Three levels of syntactic annotation have been performed with a semi-supervised approach: POS tags, Syntactic Chunks and Dependency parses. The main idea is to use off-the-shelf NLP tools and models, originaly developped and trained on written text, to perform a first automatic annotation on the manually transcribed corpus. At the same time a fully manual annotation process is performed on a subset of the original corpus, called the GOLD corpus. An iterative process is then applied, consisting in manually correcting errors found in the automatic annotations, retraining the linguistic models of the NLP tools on this corrected corpus, then checking the quality of the adapted models on the fully manual annotations of the GOLD corpus. This process iterates until a certain error rate is reached. This paper describes this process, the main issues raising when adapting NLP tools to process speech transcriptions, and presents the first evaluations performed with these new adapted tools."
lefevre-etal-2012-leveraging,Leveraging study of robustness and portability of spoken language understanding systems across languages and domains: the {PORTMEDIA} corpora,2012,20,9,7,0,27634,fabrice lefevre,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The PORTMEDIA project is intended to develop new corpora for the evaluation of spoken language understanding systems. The newly collected data are in the field of human-machine dialogue systems for tourist information in French in line with the MEDIA corpus. Transcriptions and semantic annotations, obtained by low-cost procedures, are provided to allow a thorough evaluation of the systems' capabilities in terms of robustness and portability across languages and domains. A new test set with some adaptation data is prepared for each case: in Italian as an example of a new language, for ticket reservation as an example of a new domain. Finally the work is complemented by the proposition of a new high level semantic annotation scheme well-suited to dialogue data."
F12-1070,Percol0 - un syst{\\`e}me multimodal de d{\\'e}tection de personnes dans des documents vid{\\'e}o (Percol0 - A multimodal person detection system in video documents) [in {F}rench],2012,-1,-1,6,0,17997,frederic bechet,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 1: JEP",0,None
F12-1098,Robustesse et portabilit{\\'e}s multilingue et multi-domaines des syst{\\`e}mes de compr{\\'e}hension de la parole : les corpus du projet {P}ort{M}edia (Robustness and portability of spoken language understanding systems among languages and domains : the {PORTMEDIA} project) [in {F}rench],2012,0,1,7,0,27634,fabrice lefevre,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 1: JEP",0,None
W11-2835,{\\textless}{S}tu{M}a{B}a{\\textgreater}: From Deep Representation to Surface,2011,7,11,3,0,16528,bernd bohnet,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"We realize the full generation pipeline, from the deep (= semantic) representation (SemR), over the shallow (= surface-syntactic) representation (SSyntR) to the surface. To account systematically for the non-isomorphic projection between SemR and SSyntR, we introduce an intermediate representation: the so-called deep-syntactic representation (DSyntR), which does not contain yet (all) function words (as SemR), but which already contains grammatical function relation labels (as SSyntR)."
P11-4015,{MACAON} An {NLP} Tool Suite for Processing Word Lattices,2011,17,30,4,0,5812,alexis nasr,Proceedings of the {ACL}-{HLT} 2011 System Demonstrations,0,"MACAON is a tool suite for standard NLP tasks developed for French. MACAON has been designed to process both human-produced text and highly ambiguous word-lattices produced by NLP tools. MACAON is made of several native modules for common tasks such as a tokenization, a part-of-speech tagging or syntactic parsing, all communicating with each other through XML files. In addition, exchange protocols with external tools are easily definable. MACAON is a fast, modular and open tool, distributed under GNU Public License."
2011.jeptalnrecital-long.26,Mod{\\`e}les g{\\'e}n{\\'e}ratif et discriminant en analyse syntaxique : exp{\\'e}riences sur le corpus arbor{\\'e} de {P}aris 7 (Generative and discriminative models in parsing: experiments on the {P}aris 7 Treebank),2011,-1,-1,2,0.89685,5824,joseph roux,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous pr{\'e}sentons une architecture pour l{'}analyse syntaxique en deux {\'e}tapes. Dans un premier temps un analyseur syntagmatique construit, pour chaque phrase, une liste d{'}analyses qui sont converties en arbres de d{\'e}pendances. Ces arbres sont ensuite r{\'e}{\'e}valu{\'e}s par un r{\'e}ordonnanceur discriminant. Cette m{\'e}thode permet de prendre en compte des informations auxquelles l{'}analyseur n{'}a pas acc{\`e}s, en particulier des annotations fonctionnelles. Nous validons notre approche par une {\'e}valuation sur le corpus arbor{\'e} de Paris 7. La seconde {\'e}tape permet d{'}am{\'e}liorer significativement la qualit{\'e} des analyses retourn{\'e}es, quelle que soit la m{\'e}trique utilis{\'e}e."
W10-4230,The {UMUS} System for Named Entity Generation at {GREC} 2010,2010,2,0,1,1,14954,benoit favre,Proceedings of the 6th International Natural Language Generation Conference,0,We present the UMUS (Universite du Maine/Universitat Stuttgart) submission for the NEG task at GREC'10. We refined and tuned our 2009 system but we still rely on predicting generic labels and then choosing from the list of expressions that match those labels. We handled recursive expressions with care by generating specific labels for all the possible embeddings. The resulting system performs at a type accuracy of 0.84 an a string accuracy of 0.81 on the development set.
W09-2818,{ICSI}-{CRF}: The Generation of References to the Main Subject and Named Entities Using Conditional Random Fields,2009,2,2,1,1,14954,benoit favre,Proceedings of the 2009 Workshop on Language Generation and Summarisation ({UCNLG}+{S}um 2009),0,"In this paper, we describe our contribution to the Generation Challenge 2009 for the tasks of generating Referring Expressions to the Main Subject References (MSR) and Named Entities Generation (NEG). To generate the referring expressions, we employ the Conditional Random Fields (CRF) learning technique due to the fact that the selection of an expression depends on the selection of the previous references. CRFs fit very well to this task since they are designed for the labeling of sequences. For the MSR task, our system has a String Accuracy of 0.68 and a REG08-Type Accuracy of 0.76 and for the NEG task a String Accuracy of 0.79 and REG08-Type Accuracy of 0.83."
W09-1802,A Scalable Global Model for Summarization,2009,26,199,2,0,34727,dan gillick,Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing,0,"We present an Integer Linear Program for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the sub-sentence or concept-level, to a sentence-level model, previously solved with an ILP. Our model scales more efficiently to larger problems because it does not require a quadratic number of variables to address redundancy in pairs of selected sentences. We also show how to include sentence compression in the ILP formulation, which has the desirable property of performing compression and sentence selection simultaneously. The resulting system performs at least as well as the best systems participating in the recent Text Analysis Conference, as judged by a variety of automatic and manual content-based metrics."
H05-1062,Robust Named Entity Extraction from Large Spoken Archives,2005,15,37,1,1,14954,benoit favre,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Traditional approaches to Information Extraction (IE) from speech input simply consist in applying text based methods to the output of an Automatic Speech Recognition (ASR) system. If it gives satisfaction with low Word Error Rate (WER) transcripts, we believe that a tighter integration of the IE and ASR modules can increase the IE performance in more difficult conditions. More specifically this paper focuses on the robust extraction of Named Entities from speech input where a temporal mismatch between training and test corpora occurs. We describe a Named Entity Recognition (NER) system, developed within the French Rich Broadcast News Transcription program ESTER, which is specifically optimized to process ASR transcripts and can be integrated into the search process of the ASR modules. Finally we show how some metadata information can be collected in order to adapt NER and ASR models to new conditions and how they can be used in a task of Named Entity indexation of spoken archives."
