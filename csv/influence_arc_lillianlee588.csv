2020.acl-main.775,J17-4005,0,0.0536259,"Missing"
2020.acl-main.775,N16-1127,0,0.0408486,"Missing"
2020.acl-main.775,W17-7622,0,0.0440481,"however, they pose an interesting representational challenge. Dependency-graph formalisms for syntactic structure represent lexical items as nodes and headdominates-modifier/argument relations between lexical items as directed arcs on the corresponding pair of nodes. Most words can be assigned clear linguistically-motivated syntactic heads, but several frequently occurring phenomena do not easily fit into this framework, including punctuation, coordinating conjunctions, and “flat”, or headless MWEs. While the proper treatment of headless constructions in dependency formalisms remains debated (Kahane et al., 2017; Gerdes et al., 2018), many well-known dependency treebanks handle MWEs by giving their component words a “default head”, which is not indicative of a true dominance relation, but rather as “a tree encoding of a flat structure without a syntactic head” (de Marneffe and Nivre, 2019, pg. 213). Fig. 1 shows an example: the headless MWE Mellon Capital has its first word, Mellon, marked as the “head” of Capital. Despite the special status of flat structures in dependency tree annotations, most state-of-theart dependency parsers treat all annotated relations equally, and thus do not distinguish bet"
2020.acl-main.775,P17-2068,0,0.0205539,"pans receive B-tag, I-tag, attachment and relation scores that correspond to the two consistent views of the same structure. The time complexity for this decoding algorithm remains the same Opn3 q as the original Eisner algorithm. During training, we let the parser and the tagger share the same contextualized representation x and optimize a linearly interpolated joint objective Ljoint “ λLparse ` p1 ´ λqLtag , 6 In the labeled case, the parser further adds the arc-labeling scores to the R- MWE and LINK rules. 4 Experiments Data We perform experiments on the MWEAware English Dependency Corpus (Kato et al., 2017) and treebanks selected from Universal Dependencies 2.2 (UD; Nivre et al., 2018) for having frequent occurrences of headless MWE structures. The MWE-Aware English Dependency Corpus provides automatically unified named-entity annotations based on OntoNotes 5.0 (Weischedel et al., 2013) and Stanford-style dependency trees (de Marneffe and Manning, 2008). We extract MWE spans according to mwe_NNP dependency relations. We choose the UD treebanks based on two basic properties that hold for flat structures 7 The joint decoder combines tagging and parsing scores regardless of whether the two modules"
2020.acl-main.775,Q16-1023,0,0.0354588,"int decoder (§3.4) that finds the global maximum among consistent (tree structure, tag sequence) pairs. 3.1 Preliminaries Given a length-n sentence w—which we henceforth denote with the variable x for consistency with machine-learning conventions—we first extract contextualized representations from the input to associate each word with a vector x0 (for the dummy word “root”), x1 , . . . , xn . We consider two common choices of feature extractors: (1) bidirectional long short-term memory networks (biLSTMs; Graves and Schmidhuber, 2005) which 8783 have been widely adopted in dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) and sequence tagging (Ma and Hovy, 2016); and (2) the Transformer-based (Vaswani et al., 2017) BERT feature extractor (Devlin et al., 2019), pre-trained on large corpora and known to provide superior accuracies on both tasks (Kitaev et al., 2019; Kondratyuk and Straka, 2019). For BERT models, we fine-tune the representations from the final layer for our parsing and tagging tasks. When the BERT tokenizer renders multiple tokens from a single pre-tokenized word, we follow Kitaev et al. (2019) and use the BERT features from the last token as its representation. 3.2 (Edg"
2020.acl-main.775,P19-1340,0,0.0316332,"esentations from the input to associate each word with a vector x0 (for the dummy word “root”), x1 , . . . , xn . We consider two common choices of feature extractors: (1) bidirectional long short-term memory networks (biLSTMs; Graves and Schmidhuber, 2005) which 8783 have been widely adopted in dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) and sequence tagging (Ma and Hovy, 2016); and (2) the Transformer-based (Vaswani et al., 2017) BERT feature extractor (Devlin et al., 2019), pre-trained on large corpora and known to provide superior accuracies on both tasks (Kitaev et al., 2019; Kondratyuk and Straka, 2019). For BERT models, we fine-tune the representations from the final layer for our parsing and tagging tasks. When the BERT tokenizer renders multiple tokens from a single pre-tokenized word, we follow Kitaev et al. (2019) and use the BERT features from the last token as its representation. 3.2 (Edge-Factored) Parsing Since we consider headless structures that are embedded inside parse trees, it is natural to identify them through a rule-based post-processing step after full parsing. Our parsing component replicates that of the state-of-the-art Che et al. (2018) par"
2020.acl-main.775,D19-1279,0,0.0313171,"input to associate each word with a vector x0 (for the dummy word “root”), x1 , . . . , xn . We consider two common choices of feature extractors: (1) bidirectional long short-term memory networks (biLSTMs; Graves and Schmidhuber, 2005) which 8783 have been widely adopted in dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) and sequence tagging (Ma and Hovy, 2016); and (2) the Transformer-based (Vaswani et al., 2017) BERT feature extractor (Devlin et al., 2019), pre-trained on large corpora and known to provide superior accuracies on both tasks (Kitaev et al., 2019; Kondratyuk and Straka, 2019). For BERT models, we fine-tune the representations from the final layer for our parsing and tagging tasks. When the BERT tokenizer renders multiple tokens from a single pre-tokenized word, we follow Kitaev et al. (2019) and use the BERT features from the last token as its representation. 3.2 (Edge-Factored) Parsing Since we consider headless structures that are embedded inside parse trees, it is natural to identify them through a rule-based post-processing step after full parsing. Our parsing component replicates that of the state-of-the-art Che et al. (2018) parser, which has the same parsin"
2020.acl-main.775,C14-1177,0,0.0484391,"Missing"
2020.acl-main.775,P16-1101,0,0.0213858,"re, tag sequence) pairs. 3.1 Preliminaries Given a length-n sentence w—which we henceforth denote with the variable x for consistency with machine-learning conventions—we first extract contextualized representations from the input to associate each word with a vector x0 (for the dummy word “root”), x1 , . . . , xn . We consider two common choices of feature extractors: (1) bidirectional long short-term memory networks (biLSTMs; Graves and Schmidhuber, 2005) which 8783 have been widely adopted in dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) and sequence tagging (Ma and Hovy, 2016); and (2) the Transformer-based (Vaswani et al., 2017) BERT feature extractor (Devlin et al., 2019), pre-trained on large corpora and known to provide superior accuracies on both tasks (Kitaev et al., 2019; Kondratyuk and Straka, 2019). For BERT models, we fine-tune the representations from the final layer for our parsing and tagging tasks. When the BERT tokenizer renders multiple tokens from a single pre-tokenized word, we follow Kitaev et al. (2019) and use the BERT features from the last token as its representation. 3.2 (Edge-Factored) Parsing Since we consider headless structures that are"
2020.acl-main.775,P15-1108,0,0.0211782,"proaches do not directly transfer to dependencybased representations since dependency trees do not explicitly represent phrases. In the context of dependency parsing, Eryi˘git et al. (2011) report that MWE annotations have a large impact on parsing. They find that the dependency parsers are more accurate when MWE spans are not unified into single lexical items. Similar to the phrase-structure case, Candito and Constant (2014) consider MWE identification as a side product of dependency parsing into joint representations. This parse-then-extract strategy is widely adopted (Vincze et al., 2013; Nasr et al., 2015; Simkó et al., 2017). Waszczuk et al. (2019) introduce additional parameterized scoring functions for the arc labelers and use global decoding to produce consistent structures during arc-labeling steps once unlabeled dependency parse trees are predicted. Our work additionally proposes a joint decoder that combines the scores from both parsers and taggers. Alternative approaches to graph-based joint parsing and MWE identification include transition-based (Constant and Nivre, 2016) and easy-first (Constant et al., 2016) dependency parsing. These approaches typically rely on greedy decoding, whe"
2020.acl-main.775,P05-1013,0,0.0373733,"simpler model designs in downstream applications. Our study has been limited to a few treebanks in UD partially due to large variations and inconsistencies across different treebanks. Future community efforts on a unified representation of flat structures for all languages would facilitate further research on linguistically-motivated treatments of headless structures in “headful” dependency treebanks. Another limitation of our current work is that our joint decoder only produces projective dependency parse trees. To handle non-projectivity, one possible solution is pseudo-projective parsing (Nivre and Nilsson, 2005). We leave it to future work to design a non-projective decoder for joint parsing and headless structure extraction. Acknowledgments We thank the three anonymous reviewers for their comments, and Igor Malioutov, Ana Smith and the Cornell NLP group for discussion and comments. TS was supported by a Bloomberg Data Science Ph.D. Fellowship. References Abhishek Arun and Frank Keller. 2005. Lexicalization in crosslinguistic probabilistic parsing: The case of French. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 306–313, Ann Arbor, Michiga"
2020.acl-main.775,K18-2016,0,0.0519661,"Missing"
2020.acl-main.775,W17-6523,0,0.0543019,"Missing"
2020.acl-main.775,W95-0107,0,0.140972,"of a flat structure without a syntactic head” (de Marneffe and Nivre, 2019, pg. 213). Fig. 1 shows an example: the headless MWE Mellon Capital has its first word, Mellon, marked as the “head” of Capital. Despite the special status of flat structures in dependency tree annotations, most state-of-theart dependency parsers treat all annotated relations equally, and thus do not distinguish between headed and headless constructions. When headless-span identification (e.g., as part of namedentity recognition (NER)) is the specific task at hand, begin-chunk/inside-chunk/outside-chunk (BIO) tagging (Ramshaw and Marcus, 1995) is generally adopted. It is therefore natural to ask whether parsers are as accurate as taggers in identifying these “flat branches” in dependency trees. Additionally, since parsing and tagging represent 8780 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8780–8794 c July 5 - 10, 2020. 2020 Association for Computational Linguistics two different views of the same underlying structures, can joint decoding that combines scores from the two modules and/or joint training under a multitask learning (MTL) framework derive more accurate models than par"
2020.acl-main.775,L18-1279,0,0.0429376,"Missing"
2020.acl-main.775,I13-1024,0,0.0285892,"gnition. These ap8787 proaches do not directly transfer to dependencybased representations since dependency trees do not explicitly represent phrases. In the context of dependency parsing, Eryi˘git et al. (2011) report that MWE annotations have a large impact on parsing. They find that the dependency parsers are more accurate when MWE spans are not unified into single lexical items. Similar to the phrase-structure case, Candito and Constant (2014) consider MWE identification as a side product of dependency parsing into joint representations. This parse-then-extract strategy is widely adopted (Vincze et al., 2013; Nasr et al., 2015; Simkó et al., 2017). Waszczuk et al. (2019) introduce additional parameterized scoring functions for the arc labelers and use global decoding to produce consistent structures during arc-labeling steps once unlabeled dependency parse trees are predicted. Our work additionally proposes a joint decoder that combines the scores from both parsers and taggers. Alternative approaches to graph-based joint parsing and MWE identification include transition-based (Constant and Nivre, 2016) and easy-first (Constant et al., 2016) dependency parsing. These approaches typically rely on g"
2020.acl-main.775,W19-5113,0,0.0118018,"pendencybased representations since dependency trees do not explicitly represent phrases. In the context of dependency parsing, Eryi˘git et al. (2011) report that MWE annotations have a large impact on parsing. They find that the dependency parsers are more accurate when MWE spans are not unified into single lexical items. Similar to the phrase-structure case, Candito and Constant (2014) consider MWE identification as a side product of dependency parsing into joint representations. This parse-then-extract strategy is widely adopted (Vincze et al., 2013; Nasr et al., 2015; Simkó et al., 2017). Waszczuk et al. (2019) introduce additional parameterized scoring functions for the arc labelers and use global decoding to produce consistent structures during arc-labeling steps once unlabeled dependency parse trees are predicted. Our work additionally proposes a joint decoder that combines the scores from both parsers and taggers. Alternative approaches to graph-based joint parsing and MWE identification include transition-based (Constant and Nivre, 2016) and easy-first (Constant et al., 2016) dependency parsing. These approaches typically rely on greedy decoding, whereas our joint decoder finds the globally opt"
2020.acl-main.775,N15-1177,0,0.0208213,"es the scores from both parsers and taggers. Alternative approaches to graph-based joint parsing and MWE identification include transition-based (Constant and Nivre, 2016) and easy-first (Constant et al., 2016) dependency parsing. These approaches typically rely on greedy decoding, whereas our joint decoder finds the globally optimal solution through dynamic programming. Our work only focuses on a subset of MWEs that do not have internal structures. There is substantial research interest in the broad area of MWEs (Sag et al., 2002; Constant et al., 2017) including recent releases of datasets (Schneider and Smith, 2015), editions of shared tasks (Savary et al., 2017; Ramisch et al., 2018) and workshops (Savary et al., 2018, 2019). We leave it to future work to extend the comparison and combination of taggers and dependency parsers to other MWE constructions. 6 Conclusion and Further Directions Our paper provides an empirical comparison of different strategies for extracting headless MWEs from dependency parse trees: parsing, tagging, and joint modeling. Experiments on the MWE-Aware English Dependency Corpus and UD 2.2 across five languages show that tagging, a widely-used methodology for extracting spans fro"
2020.acl-main.775,K18-2001,0,0.0244056,"Missing"
2020.acl-main.775,W17-1705,0,0.0162674,"rectly transfer to dependencybased representations since dependency trees do not explicitly represent phrases. In the context of dependency parsing, Eryi˘git et al. (2011) report that MWE annotations have a large impact on parsing. They find that the dependency parsers are more accurate when MWE spans are not unified into single lexical items. Similar to the phrase-structure case, Candito and Constant (2014) consider MWE identification as a side product of dependency parsing into joint representations. This parse-then-extract strategy is widely adopted (Vincze et al., 2013; Nasr et al., 2015; Simkó et al., 2017). Waszczuk et al. (2019) introduce additional parameterized scoring functions for the arc labelers and use global decoding to produce consistent structures during arc-labeling steps once unlabeled dependency parse trees are predicted. Our work additionally proposes a joint decoder that combines the scores from both parsers and taggers. Alternative approaches to graph-based joint parsing and MWE identification include transition-based (Constant and Nivre, 2016) and easy-first (Constant et al., 2016) dependency parsing. These approaches typically rely on greedy decoding, whereas our joint decode"
2020.acl-main.775,E17-1063,0,0.0248703,"agging tasks. When the BERT tokenizer renders multiple tokens from a single pre-tokenized word, we follow Kitaev et al. (2019) and use the BERT features from the last token as its representation. 3.2 (Edge-Factored) Parsing Since we consider headless structures that are embedded inside parse trees, it is natural to identify them through a rule-based post-processing step after full parsing. Our parsing component replicates that of the state-of-the-art Che et al. (2018) parser, which has the same parsing model as Dozat and Manning (2017). We treat unlabelled parsing as a head selection problem (Zhang et al., 2017) with deep biaffine attention scoring: mattach “ MLPattach-mod pxj q j si,j “ rhattach ; 1sJ U attach rmattach ; 1s i j P phj “ i |xq “ softmaxi ps:,j q, where MLPattach-head and MLPattach-mod are multilayer perceptrons (MLPs) that project contextualized representations into a d-dimensional space; r¨; 1s indicates appending an extra entry of 1 to the vector; U att P Rpd`1qˆpd`1q generates a score si,j for wj attaching to wi (which we can then refer to as the head of wj , hj ); a softmax function defines a probability distribution over all syntactic head candidates in the argument vector (we us"
2020.acl-main.775,solberg-etal-2014-norwegian,0,0.0606054,"Missing"
2020.acl-main.775,P05-1038,0,\N,Missing
2020.acl-main.775,P14-1070,0,\N,Missing
2020.acl-main.775,Q14-1016,0,\N,Missing
2020.acl-main.775,P16-1016,0,\N,Missing
2020.acl-main.775,W17-0403,0,\N,Missing
2020.acl-main.775,D18-1217,0,\N,Missing
2020.emnlp-main.62,N19-1056,0,0.0326578,"Missing"
2020.emnlp-main.62,N19-1166,1,0.830216,"ur tasks. LXMERT represents images using 36 predicted bounding boxes, each of which is asregularization strength (10**(-7,-6,-5,-4,-3,-2,-1,0,1.0)), and loss type (logistic vs. squared hinge). We train models using lightning (Blondel and Pedregosa, 2016). 10 For reproducibility, we used ResNet-18 features for the Kruk et al. (2019) datasets; more detail in Appendix E. 11 Feature extraction approaches are known to produce competitive results relative to full fine-tuning (Devlin et al., 2019, §5.3); in some cases, mean pooling has been found to be similarly competitive relative to LSTM pooling (Hessel and Lee, 2019). 12 We again use grid search to optimize: polynomial kernel degree (2 vs. 3), regularization strength (10**(-5,-4,-3,2,-1,0)), and gamma (1, 10, 100). 13 Parameters are optimized with the Adam optimizer (Kingma and Ba, 2015). We decay the learning rate when validation loss plateaus. The hyperparameters optimized in grid search are: number of layers (2, 3, 4), initial learning rate (.01, .001, .0001), activation function (relu vs. gelu), hidden dimension (128, 256), and batch norm (use vs. don’t). 5.1 Results Our main prediction results are summarized in Table 4. For all tasks, the performance"
2020.emnlp-main.62,P17-1152,0,0.0265511,"trained EfficientNet B410 (Tan and Le, 2019). To represent text, we extract RoBERTa (Liu et al., 2019) token features, and mean pool.11 Our singlemodal baselines are linear models fit over EfficientNet/RoBERTa features directly. Interactive Models Kernel SVM. We train an SVM with a polynomial kernel using RoBERTa text features and EfficientNet-B4 image features as input. A polynomial kernel endows the model with capacity to model multiplicative interactions between features.12 Neural Network. We train a feed-forward neural network using the RoBERTa/EfficientNet-B4 features as input. Following Chen et al. (2017), we first project text and image features via an affine transform layer to representations t and v, respectively, of the same dimension. Then, we extract new features, feeding the concatenated feature vector [t; v; v − t; v t] to a multi-layer, feedforward network.13 Fine-tuning a Pretrained Transformer. We fine-tuned LXMERT (Tan and Bansal, 2019) for our tasks. LXMERT represents images using 36 predicted bounding boxes, each of which is asregularization strength (10**(-7,-6,-5,-4,-3,-2,-1,0,1.0)), and loss type (logistic vs. squared hinge). We train models using lightning (Blondel and Pedreg"
2020.emnlp-main.62,N19-1423,0,0.0310587,"a multi-layer, feedforward network.13 Fine-tuning a Pretrained Transformer. We fine-tuned LXMERT (Tan and Bansal, 2019) for our tasks. LXMERT represents images using 36 predicted bounding boxes, each of which is asregularization strength (10**(-7,-6,-5,-4,-3,-2,-1,0,1.0)), and loss type (logistic vs. squared hinge). We train models using lightning (Blondel and Pedregosa, 2016). 10 For reproducibility, we used ResNet-18 features for the Kruk et al. (2019) datasets; more detail in Appendix E. 11 Feature extraction approaches are known to produce competitive results relative to full fine-tuning (Devlin et al., 2019, §5.3); in some cases, mean pooling has been found to be similarly competitive relative to LSTM pooling (Hessel and Lee, 2019). 12 We again use grid search to optimize: polynomial kernel degree (2 vs. 3), regularization strength (10**(-5,-4,-3,2,-1,0)), and gamma (1, 10, 100). 13 Parameters are optimized with the Adam optimizer (Kingma and Ba, 2015). We decay the learning rate when validation loss plateaus. The hyperparameters optimized in grid search are: number of layers (2, 3, 4), initial learning rate (.01, .001, .0001), activation function (relu vs. gelu), hidden dimension (128, 256), an"
2020.emnlp-main.62,D19-1224,0,0.0107328,"cally and cleverly balanced to resist language-only or visual-only models; examples are VQA 2.0 (Goyal et al., 2017), NLVR2 861 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 861–877, c November 16–20, 2020. 2020 Association for Computational Linguistics 2 mance comparisons are an opaque tool for analysis, especially for deep neural networks: performance differences versus baselines, frequently small in magnitude, can often be attributed to hyperparameter search schemes, random seeds, the number of models compared, etc. (Yogatama and Smith, 2015; Dodge et al., 2019). Thus, while model comparisons are an acceptable starting point for demonstrating whether or not a model is learning an interesting set of (or any!) cross-modal factors, they provide rather indirect evidence. We propose Empirical Multimodally-Additive1 function Projection (EMAP) as an additional diagnostic for analyzing multimodal classification models. Instead of comparing two different models, a single multimodal classifier’s predictions are projected onto a less-expressive space: the result is equivalent to a set of predictions made by the closest possible ensemble of text-only and visualo"
2020.emnlp-main.62,P19-1272,0,0.123652,"Missing"
2020.emnlp-main.62,D15-1251,0,0.0273093,"datasets that are specifically and cleverly balanced to resist language-only or visual-only models; examples are VQA 2.0 (Goyal et al., 2017), NLVR2 861 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 861–877, c November 16–20, 2020. 2020 Association for Computational Linguistics 2 mance comparisons are an opaque tool for analysis, especially for deep neural networks: performance differences versus baselines, frequently small in magnitude, can often be attributed to hyperparameter search schemes, random seeds, the number of models compared, etc. (Yogatama and Smith, 2015; Dodge et al., 2019). Thus, while model comparisons are an acceptable starting point for demonstrating whether or not a model is learning an interesting set of (or any!) cross-modal factors, they provide rather indirect evidence. We propose Empirical Multimodally-Additive1 function Projection (EMAP) as an additional diagnostic for analyzing multimodal classification models. Instead of comparing two different models, a single multimodal classifier’s predictions are projected onto a less-expressive space: the result is equivalent to a set of predictions made by the closest possible ensemble of"
2020.emnlp-main.62,D19-1514,0,0.101645,"and text, but the models we consider do not yet take full advantage of them (we return to this point in item 6). Our goal, rather, is to provide diagnostic tools that can provide additional clarity on the remaining shortcomings of current models and reporting schemes. Balanced VQA Tasks Our next sanity check is to verify that EMAP hurts the performance of interactive models on two real multimodal classification tasks that are specifically balanced to require modeling cross-modal feature interactions: VQA 2.0 (Goyal et al., 2017) and GQA (Hudson and Manning, 2019). First, we fine-tuned LXMERT (Tan and Bansal, 2019), a multimodally-interactive, pretrained, 14layer Transformer model (See §5 for full description) that achieves SOTA on both datasets. The LXMERT authors frame question-answering as a multi-class image/text-pair classification problem — 3.1K candidate answers for VQA2, and 1.8K for GQA. In Table 2, we compare, in crossvalidation, the means of: 1) accuracy of LXMERT, 2) accuracy of the EMAP of LXMERT, and 3) accuracy of simply predicting the most common answer for all questions. As expected, EMAP decreases accuracy on VQA2/GQA by 30/19 absolute accuracy points, respectively: this suggests LXMER"
2020.findings-emnlp.167,C16-1291,0,0.396332,"replaced by manual alignments. Induced attention refers to the base model (§4). 5.1 Supervised Attention Our annotated lexical alignments resemble our base model’s attention mechanisms. At the encoding stage, question tokens and the relevant columns are aligned (e.g., “who” ↔ column “athlete”) which should induce higher weights in both question-tocolumn and column-to-question attention (Eq. (3) and Eq. (4)); similarly, for decoding, annotation reflects which question words are most relevant to the current output token. Inspired by improvements from supervised attention in machine translation (Liu et al., 2016; Mi et al., 2016), we train the base model’s attention mechanisms to minimize the Euclidean distance5 between the human-annotated alignment vector a? and the model-generated attention vector a: Similarly, we define literal string copying from q with another bilinear scoring matrix W STR . 5 ACCLF (Dev) 1 Latt = ka − a? k2 . 2 The vector a? is a one-hot vector when the annotation aligns to a single element, or a? represents a uniform distribution over the subset in cases where the annotation aligns multiple elements. 5.2 Oracle Experiments with Manual Alignments To present the potential of ali"
2020.findings-emnlp.167,D16-1249,0,0.159887,"alignments. Induced attention refers to the base model (§4). 5.1 Supervised Attention Our annotated lexical alignments resemble our base model’s attention mechanisms. At the encoding stage, question tokens and the relevant columns are aligned (e.g., “who” ↔ column “athlete”) which should induce higher weights in both question-tocolumn and column-to-question attention (Eq. (3) and Eq. (4)); similarly, for decoding, annotation reflects which question words are most relevant to the current output token. Inspired by improvements from supervised attention in machine translation (Liu et al., 2016; Mi et al., 2016), we train the base model’s attention mechanisms to minimize the Euclidean distance5 between the human-annotated alignment vector a? and the model-generated attention vector a: Similarly, we define literal string copying from q with another bilinear scoring matrix W STR . 5 ACCLF (Dev) 1 Latt = ka − a? k2 . 2 The vector a? is a one-hot vector when the annotation aligns to a single element, or a? represents a uniform distribution over the subset in cases where the annotation aligns multiple elements. 5.2 Oracle Experiments with Manual Alignments To present the potential of alignment annotations"
2020.findings-emnlp.167,P15-1142,0,0.428845,"The table-question-answer triplets come from W IKI TABLE Q UESTIONS. We provide the logical forms as SQL plus alignments between question and logical form. In the bottom example, for instance, “the highest” ↔ ORDER BY and LIMIT 1, as indicated by both matching highlight color ( blue ) and circled-number labels ( 2 ). We address this lack by introducing S QUALL,1 the first large-scale semantic-parsing dataset with manual lexical-to-logical alignments; and we investigate the potential accuracy boosts achievable from such alignments. The starting point for S QUALL is W IKI TABLE Q UESTIONS (WTQ; Pasupat and Liang, 2015), containing data tables, English questions regarding the tables, and table-based answers. We manually enrich the 11,276-instance subset of WTQ’s training data that is translatable to SQL 1 Equal contribution; listed in alphabetical order. S QUALL =“SQL+QUestion pairs ALigned Lexically”. 1849 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1849–1864 c November 16 - 20, 2020. 2020 Association for Computational Linguistics by providing expert annotations, consisting not only of target logical forms in SQL, but also labeled alignments between the input question tokens"
2020.findings-emnlp.167,P17-1089,0,0.0288395,"dataset release. Dua et al. (2020) show that these annotator rationales improve model accuracy for a given annotation budget on machine reading comprehension. The alignments we provide could, at a stretch, be considered a type of rationale for the output SQL annotation. Text-to-SQL Datasets There is growing interest in both the database and NLP communities in text-to-SQL applications. Widely-used domainspecific datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996; Popescu et al., 2003), Restaurants (Tang and Mooney, 2000; Popescu et al., 2003), and Scholar (Iyer et al., 2017). WikiSQL (Zhong et al., 2017) is among the first large-scale datasets with questionlogical form pairs querying a wide range of data tables extracted from Wikipedia, but WikiSQL’s logical forms are generated from a limited set of templates. In contrast, WTQ questions are authored by humans under no specific constraints, and as a result WTQ includes more diverse semantics and logical operations. The family of Spider datasets (Yu et al., 2018, 2019a,b) contain queries even more complex than in WTQ, including a higher percentage of nested queries and multiple table joins. We leave extensions of l"
2020.findings-emnlp.167,P16-1003,0,0.0413284,"Missing"
2020.findings-emnlp.167,N19-1357,0,0.0233556,"e entropy of the attention distributions in the question-to-column (q2c), column-to-question (c2q) and decoder-to-question (d2q) modules, comparing models trained with supervised encoder/decoder attention, none (SEQ 2 SEQ+ ), or both strategies (ALIGN). judgments. This is an arguably surprising benefit, since the supervised decoder was not trained with q2c supervision, and so one might have expected it to perform similarly to SEQ 2 SEQ+ . However, one needs to be careful in interpreting these results, as machine-induced attention distributions are not intended for direct human interpretation (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Qualitative Analysis Our additional supervision helps when the question has little textual overlap with the referred columns. Figure 3 shows an example. With finer-grained supervision, ALIGN learns the column “Serial Name” corresponds to the question word “show”, but SEQ 2 SEQ+ selects the wrong column “Co-Star”. 7 Related Work Attention and Alignments Explicit supervision for attention mechanisms (Bahdanau et al., 2015) is helpful for many tasks, including machine translation (Liu et al., 2016; Mi et al., 2016), image captioning (Liu et al., 2017), and visual qu"
2020.findings-emnlp.167,P18-1033,0,0.0521508,"Missing"
2020.findings-emnlp.167,D14-1162,0,0.0824568,"Missing"
2020.findings-emnlp.167,2020.acl-main.742,0,0.597059,"es correspond to spans in the input questions. We used our alignment to generate gold selection spans, filtering out instances where literal values could not be reconstructed through fuzzy match from the gold spans. After post-processing, S QUALL contained 11,276 table-question-answer triplets with logical form and lexical alignment annotations. (State-of-the-Art)4 Base Model: Seq2seq with Attention and Copying 4 Recent state-of-the-art text-to-SQL models extend the sequence-to-sequence (seq2seq) framework with attention and copying mechanisms (Zhong et al., 2017; Dong and Lapata, 2016, 2018; Suhr et al., 2020, inter alia). We adopt this strong neural paradigm as our base model. The seq2seq model generates one output token at a time via a probability distribution conditioned on both the input sequence representations and the partially-generated Q|y| output sequence: P (y |x) = i=1 P (yi |y<i , x), where x and y are the feature representations for the input and output sequences, and <i denotes a prefix. The last token of y must be a special <STOP> token that terminates the output generation. The per-token probability distribution is modeled through Long-Short Term Memory networks (LSTMs, Hochreiter"
2021.acl-long.557,W10-2927,0,0.028597,"n and single-word bubble boundaries.) Bubbles explicitly encode the scope of the shared modifier hot with respect to the nested coordination, whereas the UD tree gives both tea and bun identical relationships to hot. Coordination structures are prevalent in treebank data (Ficler and Goldberg, 2016a), especially in long sentences (Kurohashi and Nagao, 1994), and they are among the most challenging constructions for NLP models. Difficulties in correctly identifying coordination structures have consistently contributed to a significant portion of errors in stateof-the-art parsers (Collins, 2003; Goldberg and Elhadad, 2010; Ficler and Goldberg, 2017). These errors can further propagate to downstream NLP modules and applications, and limit their performance and utility. For example, Saha et al. (2017) report that missing conjuncts account for two-thirds of the errors in recall made by their open information extraction system. Coordination constructions are particularly challenging for the widely-adopted dependency-based paradigm of syntactic analysis, since the asymmetric definition of head-modifier dependency relations is not directly compatible with the symmetric Code at github.com/tzshi/bubble-parser-acl21. c"
2021.acl-long.557,Q13-1033,0,0.0207481,"Training and Inference Our parser is a locallytrained greedy parser. In training, we optimize the model parameters to maximize the log-likelihoods of predicting the target transitions and labels along the paths generating the gold bubble trees, and the log-likelihoods of the correct attachments in rescoring;9 during inference, the parser greedily commits to the highest-scoring transition and label for each of its current parser configurations, and after reaching a terminal configuration, it rescores and readjusts all boundary subtree attachments. 9 We leave the definition of dynamic oracles (Goldberg and Nivre, 2013) for bubble tree parsing to future work. – 75.01 77.83 81.63 85.26 72.70 73.74 77.74 78.30 84.46 76.10 77.25 80.06 84.03 86.22 Table 2: F1 scores on the PTB test set. See Appendix C for precision, recall and dev set results. ◦ vd(c,t) ])), Boundary Subtree Rescoring In our preliminary error analysis, we find that our models tend to make more mistakes at the boundaries of full coordination phrases than at the internal conjunct boundaries, due to incorrect attachments of children choosing between the phrasal bubble and the first/last conjunct. For example, our initial model predicts “if you owne"
2021.acl-long.557,J93-2004,0,0.108147,"rsued, for reasons unknown to us. Given its appealing and intuitive treatment of coordination phenomena, we revisit the bubble tree formalism, introducing and implementing a transition-based solution for parsing bubble trees. Our transition system, BubbleHybrid, extends the Arc-Hybrid transition system (Kuhlmann et al., 2011) with three bubble-specific transitions, each corresponding to opening, expanding, and closing bubbles. We show that our transition system is both sound and complete with respect to projective bubble trees (defined in § 2.2). Experiments on the English Penn Treebank (PTB; Marcus et al., 1993) extended with coordination annotation (Ficler and Goldberg, 2016a) and the English GENIA treebank (Kim et al., 2003) demonstrate the effectiveness of our proposed transition-based bubble parsing on the task of coordination structure prediction. Our method achieves state-of-the-art performance on both datasets and improves accuracy on the subset of sentences exhibiting complex coordination structures. 2 2.1 Dependency Trees and Bubble Trees Dependency-based Representations for Coordination Structures A dependency tree encodes syntactic relations via directed bilexical dependency edges. These a"
2021.acl-long.557,C94-2151,0,0.705827,"ase with one of its conjuncts without the sentence becoming incoherent; this idea has resulted in improved open information extraction (Saha and Mausam, 2018). Using these principles may further improve our parser. Coordination in Constituency Grammar While our paper mainly focuses on enhancing dependency-based syntactic analysis with coordination structures, coordination is a well-studied topic in constituency-based syntax (Zhang, 2009), including proposals and treatments under lexical Non-constituent Coordination Seemingly incomplete (non-constituent) conjuncts are particularly challenging (Milward, 1994), and our bubble parser currently has no special mechanism for them. Dependency-based analyses have adapted by extending to a graph structure (Gerdes and Kahane, 2015) or explicitly representing elided elements (Schuster et al., 2017). It may be straightforward to integrate the latter into our parser, à la Kahane’s (1997) proposal of phonologically-empty bubbles. 7 Conclusion We revisit Kahane’s (1997) bubble tree representations for explicitly encoding coordination boundaries as a viable alternative to existing mechanisms in dependency-based analysis of coordination structures. We introduce a"
2021.acl-long.557,2020.findings-emnlp.294,0,0.0914378,"Missing"
2021.acl-long.557,P06-1033,0,0.0977214,"al to distinguish shared modifiers from private ones (e.g., in the UD tree at the bottom of Figure 1, it is difficult to tell that “hot” is private to “coffee” and “tea”, which share it, but “hot” does not modify “bun”). Another choice is let one of the coordinators dominate the phrase (Hajiˇc et al., 2001, 2020), but the coordinator does not directly capture the syntactic category of the coordinated phrase. Decisions on which of these dependency-based fixes is more workable are further complicated by the interaction between representation styles and their learnability in statistical parsing (Nilsson et al., 2006; Johansson and Nugues, 2007; Rehbein et al., 2017). Enhanced UD A tactic used by many recent releases of UD treebanks is to introduce certain extra edges and non-lexical nodes (Schuster and Manning, 2016; Nivre et al., 2018; Bouma et al., 2020). While some of the theoretical issues still persist in this approach with respect to capturing the symmetric nature of relations between conjuncts, this solution better represents shared modifiers in coordinations, and so is a promising direction. In work concurrent with our own, Grünewald et al. (2021) manually correct the coordination structure annot"
2021.acl-long.557,J08-4003,0,0.0261439,"); • Contained projections: for α1 , α2 ∈ B, if ∗ α1 → α2 , then either ψ(α2 ) ⊂ φ(α1 ) or ψ(α2 ) ∩ φ(α1 ) = ∅. 3 Our Transition System for Parsing Bubble Trees Although, as we have seen, bubble trees have theoretical benefits in representing coordination structures that interface with an overall dependencybased analysis, there has been a lack of parser implementations capable of handling such representations. In this section, we fill this gap by introducing a transition system that can incrementally build projective bubble trees. Transition-based approaches are popular in dependency parsing (Nivre, 2008; Kübler et al., 2009). We propose to extend the Arc-Hybrid transition system (Kuhlmann et al., 2011) with transitions specific to bubble structures.5 3.1 Bubble-Hybrid Transition System A transition system consists of a data structure describing the intermediate parser states, called configurations; specifications of the initial and terminal configurations; and an inventory of transitions that advance the parser in configuration space towards reaching a terminal configuration. Our transition system uses a similar configuration data structure to that of Arc-Hybrid, which consists of a stack, a"
2021.acl-long.557,N10-1049,0,0.0136316,"aper, we explore Kahane’s (1997) alternative solution: extend the dependency-tree representation by introducing bubble structures to explicitly encode coordination boundaries. The coheads within a bubble enjoy a symmetric relationship, as befits a model of conjunction. Further, bubble trees support representation of nested coordination, with the scope of shared modifiers identifiable by the attachment sites of bubble arcs. Figure 1 compares a bubble tree against a Universal Dependencies (UD; Nivre et al., 2016, 2020) tree for the same sentence. Yet, despite theses advantages, implementation 2 Rambow (2010) comments on other divergences between syntactic representation and syntactic phenomena. 7167 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7167–7182 August 1–6, 2021. ©2021 Association for Computational Linguistics of the formalism was not broadly pursued, for reasons unknown to us. Given its appealing and intuitive treatment of coordination phenomena, we revisit the bubble tree formalism, introducing and implementing a transition-based solution for parsing bubble trees."
2021.acl-long.557,W17-6525,0,0.012645,"es (e.g., in the UD tree at the bottom of Figure 1, it is difficult to tell that “hot” is private to “coffee” and “tea”, which share it, but “hot” does not modify “bun”). Another choice is let one of the coordinators dominate the phrase (Hajiˇc et al., 2001, 2020), but the coordinator does not directly capture the syntactic category of the coordinated phrase. Decisions on which of these dependency-based fixes is more workable are further complicated by the interaction between representation styles and their learnability in statistical parsing (Nilsson et al., 2006; Johansson and Nugues, 2007; Rehbein et al., 2017). Enhanced UD A tactic used by many recent releases of UD treebanks is to introduce certain extra edges and non-lexical nodes (Schuster and Manning, 2016; Nivre et al., 2018; Bouma et al., 2020). While some of the theoretical issues still persist in this approach with respect to capturing the symmetric nature of relations between conjuncts, this solution better represents shared modifiers in coordinations, and so is a promising direction. In work concurrent with our own, Grünewald et al. (2021) manually correct the coordination structure annotations in an English treebank under the enhanced UD"
2021.acl-long.557,C18-1194,0,0.0153294,"i and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012), since coordinated conjuncts tend to be semantically and syntactically similar (Hogan, 2007), as attested to by psycholinguistic evidence of structural parallelism (Frazier et al., 1984, 2000; Dubey et al., 2005). Ficler and Goldberg (2016a) and Teranishi et al. (2017) additionally leverage the linguistic principle of replaceability — one can typically replace a coordinated phrase with one of its conjuncts without the sentence becoming incoherent; this idea has resulted in improved open information extraction (Saha and Mausam, 2018). Using these principles may further improve our parser. Coordination in Constituency Grammar While our paper mainly focuses on enhancing dependency-based syntactic analysis with coordination structures, coordination is a well-studied topic in constituency-based syntax (Zhang, 2009), including proposals and treatments under lexical Non-constituent Coordination Seemingly incomplete (non-constituent) conjuncts are particularly challenging (Milward, 1994), and our bubble parser currently has no special mechanism for them. Dependency-based analyses have adapted by extending to a graph structure (G"
2021.acl-long.557,P17-2050,0,0.0125842,"tical relationships to hot. Coordination structures are prevalent in treebank data (Ficler and Goldberg, 2016a), especially in long sentences (Kurohashi and Nagao, 1994), and they are among the most challenging constructions for NLP models. Difficulties in correctly identifying coordination structures have consistently contributed to a significant portion of errors in stateof-the-art parsers (Collins, 2003; Goldberg and Elhadad, 2010; Ficler and Goldberg, 2017). These errors can further propagate to downstream NLP modules and applications, and limit their performance and utility. For example, Saha et al. (2017) report that missing conjuncts account for two-thirds of the errors in recall made by their open information extraction system. Coordination constructions are particularly challenging for the widely-adopted dependency-based paradigm of syntactic analysis, since the asymmetric definition of head-modifier dependency relations is not directly compatible with the symmetric Code at github.com/tzshi/bubble-parser-acl21. conj I prefer hot coffee or tea and a bun Introduction 1 conj amod We propose a transition-based bubble parser to perform coordination structure identification and dependency-based s"
2021.acl-long.557,P10-3004,0,0.0416002,"performs better than previous methods on complex sentences with multiple coordination structures and/or more than two conjuncts, especially when we use BERT as feature extractor. Tesnière Dependency Structure Sangati and Mazza (2009) propose a representation that is faithful to Tesnière’s (1959) original framework. Similar to bubble trees, their structures include special attention to coordination structures respecting conjunct symmetry, but they also include constructs to handle other syntactic notions currently beyond our parser’s scope.13 Such representations have been used for re-ranking (Sangati, 2010), but not for (direct) parsing. Perhaps our work can inspire a future Tesnière Dependency Structure parser. 6 Related Work Coordination Structure Prediction Very early work with heuristic, non-learning-based approaches (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994) typically report difficulties in distinguishing shared modifiers from private ones, although such heuristics have been recently incorporated in unsupervised work (Sawada et al., 2020). Generally, researchers have focused on symmetry principles, seeking to align conjuncts (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Har"
2021.acl-long.557,2020.lrec-1.497,0,0.0315758,"Missing"
2021.acl-long.557,C96-2103,0,0.655488,"tion-based decoder. Additionally, our 11 We report results for the extended model of TSM17 as described by Teranishi et al. (2019). 12 Results are not strictly comparable with previous PTB evaluations that mostly focus on non-UD dependency conversions. Table 4 makes a self-contained comparison using the same UD-based and coordination-merged data conversions. 7174 bubble parser simultaneously predicts the boundaries of each coordinated phrase and conjuct, while a typical dependency parser cannot produce such structures. functional grammar (Kaplan and Maxwell III, 1988), tree-adjoining grammar (Sarkar and Joshi, 1996; Han and Sarkar, 2017), and combinatory categorial grammar (Steedman, 1996, 2000). Model Analysis Table 5 shows results of our models with alternative bubble-feature composition functions and varying feature-set sizes. We find that the parameterized form of composition function g performs better, and the F1 scores mostly degrade as we use fewer features from the stack. Interestingly, the importance of our rescoring module becomes more prominent when we use fewer features. Our results resonate with Shi et al.’s (2017) findings on Arc-Hybrid that we need at least one stack item but not necessar"
2021.acl-long.557,I17-1027,0,0.244805,"tailed results per constituent type. 5 Empirical Results Task and Evaluation We validate the utility of our transition-based parser using the task of coordination structure prediction. Given an input sentence, the task is to identify all coordination structures and the spans for all their conjuncts within that sentence. We mainly evaluate based on exact metrics which count a prediction of a coordination structure as correct if and only if all of its conjunct spans are correct. To facilitate comparison with pre-existing systems that do not attempt to identify all conjunct boundaries, following Teranishi et al. (2017, 2019), we also consider inner (=only consider the correctness of the two conjuncts adjacent to the coordinator) and whole (=only consider the boundary of the whole coordinated phrase) metrics. Data and Experimental Setup We experiment with two English datasets, the Penn Treebank (PTB; Marcus et al., 1993, newswire) with added coordination annotations (Ficler and Goldberg, 2016a) and the GENIA treebank (Kim et al., 2003, research abstracts). We use the conversion tool distributed with the Stanford Parser (Schuster and Manning, 2016) to extract UD trees from the PTBstyle phrase-structure annot"
2021.acl-long.557,N19-1343,0,0.258762,"94.24 98.70 98.01 93.20 94.61 ... 92.52 92.52 98.14 95.18 99.13 97.72 93.45 93.43 ... 93.20 95.81 94.46 UAS LAS Complexity TSM17 TSM19 Ours +BERT 92.85 98.26 95.02 99.06 97.92 93.62 93.95 ... 93.04 Ours (bi-LSTM) • g = mean • {vs2 , vs1 , vb1 } • {vs1 , vb1 } • {vb1 } • +BERT Complex 66.09 70.90 72.97 80.07 72.90 78.16 79.97 83.74 50.37 54.16 56.82 71.59 95.99 94.56 + − 77.10 75.51 76.05 76.33 50.27 84.40 76.27 74.16 74.87 73.85 35.14 83.70 try and replaceability properties; Teranishi et al. (2017, TSM17) directly predict boundaries of coordinated phrases and then split them into conjuncts;11 Teranishi et al. (2019, TSM19) use separate neural models to score the inner and outer boundaries of conjuncts relative to the coordinators, and then use a chart parser to find the globallyoptimal coordination structures. Main Results Table 2 and Table 3 show the main evaluation results on the PTB and GENIA datasets. Our models surpass all prior results on both datasets. While the BERT improvements may not seem surprising, we note that Teranishi et al. (2019) report that their pre-trained language models — specifically, static ELMo embeddings — fail to improve their model performance. Table 5: Exact F1 scores of di"
2021.acl-long.557,P16-1079,0,0.266736,"ll University tianze@cs.cornell.edu Lillian Lee Cornell University llee@cs.cornell.edu Bubble Tree: Abstract obj 1 nsubj UD Tree: nsubj cc conj cc det conj conj obj amod conj cc cc det I prefer hot coffee or tea and a bun Figure 1: Bubble tree and (basic) UD tree for the same example sentence. (For clarity, we omit punctuation and single-word bubble boundaries.) Bubbles explicitly encode the scope of the shared modifier hot with respect to the nested coordination, whereas the UD tree gives both tea and bun identical relationships to hot. Coordination structures are prevalent in treebank data (Ficler and Goldberg, 2016a), especially in long sentences (Kurohashi and Nagao, 1994), and they are among the most challenging constructions for NLP models. Difficulties in correctly identifying coordination structures have consistently contributed to a significant portion of errors in stateof-the-art parsers (Collins, 2003; Goldberg and Elhadad, 2010; Ficler and Goldberg, 2017). These errors can further propagate to downstream NLP modules and applications, and limit their performance and utility. For example, Saha et al. (2017) report that missing conjuncts account for two-thirds of the errors in recall made by their"
2021.acl-long.557,D16-1003,0,0.337616,"ll University tianze@cs.cornell.edu Lillian Lee Cornell University llee@cs.cornell.edu Bubble Tree: Abstract obj 1 nsubj UD Tree: nsubj cc conj cc det conj conj obj amod conj cc cc det I prefer hot coffee or tea and a bun Figure 1: Bubble tree and (basic) UD tree for the same example sentence. (For clarity, we omit punctuation and single-word bubble boundaries.) Bubbles explicitly encode the scope of the shared modifier hot with respect to the nested coordination, whereas the UD tree gives both tea and bun identical relationships to hot. Coordination structures are prevalent in treebank data (Ficler and Goldberg, 2016a), especially in long sentences (Kurohashi and Nagao, 1994), and they are among the most challenging constructions for NLP models. Difficulties in correctly identifying coordination structures have consistently contributed to a significant portion of errors in stateof-the-art parsers (Collins, 2003; Goldberg and Elhadad, 2010; Ficler and Goldberg, 2017). These errors can further propagate to downstream NLP modules and applications, and limit their performance and utility. For example, Saha et al. (2017) report that missing conjuncts account for two-thirds of the errors in recall made by their"
2021.acl-long.557,P09-1109,0,0.0743533,"t their pre-trained language models — specifically, static ELMo embeddings — fail to improve their model performance. Table 5: Exact F1 scores of different model variations on the PTB dev set, w/ and w/o the rescoring module. ble trees. We follow prior work in reporting PTB results on its standard splits and GENIA results using 5-fold cross-validation.10 During training (but not test), we discard all non-projective sentences. See Appendix A for dataset pre-processing and statistics and Appendix B for implementation details. Baseline Systems We compare our models with several baseline systems. Hara et al. (2009, HSOM09) use edit graphs to explicitly align coordinated conjuncts based on the idea that they are usually similar; Ficler and Goldberg (2016b, FG16) score candidate coordinations extracted from a phrase-structure parser by modeling their symme10 Simple Table 6: Per-sentence exact match on the PTB test set. Simple includes sentences with only one two-conjunct coordination, and complex contains the other cases. Table 4: PTB test-set results, comparing our transitionbased bubble parser and an edge-factored graph-based parser, both using a BERT-based feature encoder. The relation labels are orde"
2021.acl-long.557,L16-1376,0,0.0611439,"Missing"
2021.acl-long.557,D14-1162,0,0.0841108,"Missing"
2021.iwpt-1.23,2020.iwpt-1.24,0,0.0321716,"earning-based approaches to convert a basic UD tree into an enhanced UD graph (Heinecke, 2020; Dehouck et al., 2020; Attardi et al., 2020; Ek and Bernardy, 2020).2 • Graph-based: alternatively, one can directly focus on the enhanced UD graph with a semantic dependency graph parser that predicts the existence and label of each candidate dependency edge. But there is generally no guarantee that the set of predicted edges will form a connected graph, so a postprocessing step is typically employed to fix any connectivity issues. This category of approaches includes the work of Wang et al. (2020), Barry et al. (2020), and Grünewald and Friedrich (2020).3 • Transition-based: Hershcovich et al. (2020) adapt a transition-based solution. Their system explicitly handles empty nodes through a specialized transition for inserting them; it relies on additional post-processing to ensure connectivity. • Tree-Graph Integrated: He and Choi (2020) integrate a tree parser and a graph parser,4 where the tree parser produces the basic UD tree, and the graph parser predicts any additional edges. During inference, all nodes are automatically connected through the tree parser, and the graph parser allows flexibility in prod"
2021.iwpt-1.23,W13-2308,0,0.0316248,"Russian Italian English Arabic 103 104 105 Number of Training Sentences Official Evaluation The shared task performs evaluation on UD treebanks that have enhanced UD annotations across 17 languages: Arabic (Hajiˇc et al., 2009), Bulgarian (Simov et al., 2004), Czech (Hladká et al., 2010; Bejˇcek et al., 2013; Jelínek, 2017), Dutch (van der Beek et al., 2002; Bouma and van Noord, 2017), English (Silveira et al., 2014; Zeldes, 2017), Estonian (Muischnek et al., 2014, 2019), Finnish (Haverinen et al., 2014; Pyysalo et al., 2015), French (Candito et al., 2014; Seddah and Candito, 2016), Italian (Bosco et al., 2013), Latvian (Pretkalnin, a et al., 2018), Lithuanian (Bielinskien˙e et al., 2016), Polish (Patejuk and Przepiórkowski, 2018; Wróblewska, 2018), Russian (Droganova et al., 2018), Slovak (Zeman, 2018), Swedish (Nivre and Megyesi, 2007), Tamil (Ramasamy and Žabokrtský, 2012), Ukrainian (Kotsyba et al., 2016), and multilingual parallel treebanks (Zeman et al., 2017). Figure 2: The per-language delta ELAS between our submission and the best performing system other than ours, as a function of (the log of the) number of training sentences. (For Italian, the difference is quite small but still positive."
2021.iwpt-1.23,2020.iwpt-1.16,0,0.0747834,"cludes a large collection of treebanks (202 for 114 languages in UD 2.8). Progress on the UD parsing problem has been steady (Zeman et al., 2017, 2018), but existing approaches mostly focus on parsing into basic UD trees, where bilexical dependency relations among surface words must form single-rooted trees. While these trees indeed contain rich syntactic information, the adherence to tree representations can be insufficient for certain constructions including coordination, gapping, relative clauses, and argument sharing through control and raising (Schuster and Manning, 2016). The IWPT 2020 (Bouma et al., 2020) and 2021 (Bouma et al., 2021) shared tasks focus on parsing into enhanced UD format, where the representation is connected graphs, rather than rooted trees. The extension from trees to graphs allows direct treatment of a wider range of syntactic phenomena, but it also poses a research challenge: how to design parsers suitable for such enhanced UD graphs. To address this setting, we propose to use a treegraph hybrid parser leveraging the following key observation: since an enhanced UD graph must be connected, it must contain a spanning tree as a subgraph. These spanning trees may differ from b"
2021.iwpt-1.23,2021.iwpt-1.15,0,0.012184,"reebanks (202 for 114 languages in UD 2.8). Progress on the UD parsing problem has been steady (Zeman et al., 2017, 2018), but existing approaches mostly focus on parsing into basic UD trees, where bilexical dependency relations among surface words must form single-rooted trees. While these trees indeed contain rich syntactic information, the adherence to tree representations can be insufficient for certain constructions including coordination, gapping, relative clauses, and argument sharing through control and raising (Schuster and Manning, 2016). The IWPT 2020 (Bouma et al., 2020) and 2021 (Bouma et al., 2021) shared tasks focus on parsing into enhanced UD format, where the representation is connected graphs, rather than rooted trees. The extension from trees to graphs allows direct treatment of a wider range of syntactic phenomena, but it also poses a research challenge: how to design parsers suitable for such enhanced UD graphs. To address this setting, we propose to use a treegraph hybrid parser leveraging the following key observation: since an enhanced UD graph must be connected, it must contain a spanning tree as a subgraph. These spanning trees may differ from basic UD trees, but still allow"
2021.iwpt-1.23,W17-0403,0,0.0261572,"Missing"
2021.iwpt-1.23,2020.iwpt-1.26,0,0.0696894,"Missing"
2021.iwpt-1.23,2020.acl-main.747,0,0.358473,"to a given node, include that basic UD edge in the spanning tree. • Otherwise, there must be multiple incoming edges, none of which are present in the basic UD tree. We pick the parent node that is the “highest”, i.e., the closest to the root node, in the basic tree. The above head assignment steps do not formally guarantee that the extracted structures will be trees, but empirically, we observe that the extraction results are indeed trees for all training sentences.6 Representation The first step is to extract contextual representations. For this purpose, we use the pre-trained XLM-R model (Conneau et al., 2020), which is trained on multilingual CommonCrawl data and supports all 17 languages in the shared task. The XLM-R feature extractor is finetuned along with model training. Given a length-n input sentence x = x1 , . . . , xn and layer l, we extract [xl0 , xl1 , . . . , xln ] = XLM-Rl (&lt;s>, x1 , . . . , xn , &lt;/s>), where inputs to the XLM-R model are a concatenated sequence of word pieces from each UD word, we denote the layer-l vector corresponding to the last word piece in the word xi as xli , and the dummy root representations x0 s are taken from the special &lt;s> token at the beginning of the se"
2021.iwpt-1.23,2020.iwpt-1.20,0,0.0256146,"ategories: • Tree-based: since the overlap between the enhanced UD graphs and the basic UD trees are typically significant, and any deviations tend to be localized and tied to one of several certain syntactic constructions (e.g, argument sharing in a control structure), one can repurpose tree-based parsers for producing enhanced UD graphs. This category of approaches include packing the additional edges from an enhanced graph into the basic tree (Kanerva et al., 2020) and using either rule-based or learning-based approaches to convert a basic UD tree into an enhanced UD graph (Heinecke, 2020; Dehouck et al., 2020; Attardi et al., 2020; Ek and Bernardy, 2020).2 • Graph-based: alternatively, one can directly focus on the enhanced UD graph with a semantic dependency graph parser that predicts the existence and label of each candidate dependency edge. But there is generally no guarantee that the set of predicted edges will form a connected graph, so a postprocessing step is typically employed to fix any connectivity issues. This category of approaches includes the work of Wang et al. (2020), Barry et al. (2020), and Grünewald and Friedrich (2020).3 • Transition-based: Hershcovich et al. (2020) adapt a tra"
2021.iwpt-1.23,N19-1423,0,0.0204341,"n high-resource languages (e.g., −0.02 on Estonian and −0.03 on Russian). Additionally, we find that the language-generic model achieves reasonably competitive performance when compared with the set of models directly trained on each individual language. This suggests that practitioners may opt to use a single model for parsing all languages if there is a need to lower disk and memory footprints, without much loss in accuracy. 4 Pre-TGIF: Pre-Training Grants Improvements Full-Stack Inspired by the recent success of pre-trained language models on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Conneau et al., 2020, inter alia), we build our own text processing pipeline based on pre-trained language models. Due to limited time and resources, we only focus on components relevant to the shared task, which include tokenization, sentence splitting, and multiword token (MWT) expansion. 216 4.1 Tokenizers with Character-Level Masked Language Model Pre-Training 4.3 We follow state-of-the-art strategies (Qi et al., 2020; Nguyen et al., 2021) for tokenization and model the task as a tagging problem on sequences of characters. But in contrast to prior methods where tokenization and sentence"
2021.iwpt-1.23,2021.eacl-main.67,0,0.0224722,"em is a tree-graph integrated-format parser that combines the strengths of the available global inference algorithms for tree parsing and the flexibility of a graph parser, without the need to use post-processing to fix connectivity issues. 1 Enhanced UD graphs additionally allow insertion of phonologically-empty nodes to recover elided elements in gapping constructions. This is currently beyond the scope our system and we use pre- and post-processing collapsing steps to handle empty nodes (§5). 214 2 The same idea has also been applied to the task of conjunction propagation prediction (e.g., Grünewald et al., 2021). 3 Barry et al.’s (2020) parsers use basic UD trees as features, but the output space is not restricted by the basic trees. 4 He and Choi (2020) describe their combo as an “ensemble” but we prefer the term “integration” for both their method and ours (which is inspired by theirs), since the two components are not, strictly speaking, targeting same structures. 5 The main difference from the tree-based approaches is that the search space for additional graph edges is unaffected by the predictions of basic UD trees in an integrated approach. acl:relcl 2.3 Parameterization obj det the book det Ou"
2021.iwpt-1.23,2020.iwpt-1.19,0,0.336342,"ch candidate dependency edge. But there is generally no guarantee that the set of predicted edges will form a connected graph, so a postprocessing step is typically employed to fix any connectivity issues. This category of approaches includes the work of Wang et al. (2020), Barry et al. (2020), and Grünewald and Friedrich (2020).3 • Transition-based: Hershcovich et al. (2020) adapt a transition-based solution. Their system explicitly handles empty nodes through a specialized transition for inserting them; it relies on additional post-processing to ensure connectivity. • Tree-Graph Integrated: He and Choi (2020) integrate a tree parser and a graph parser,4 where the tree parser produces the basic UD tree, and the graph parser predicts any additional edges. During inference, all nodes are automatically connected through the tree parser, and the graph parser allows flexibility in producing graph structures.5 The tree-based approaches are prone to error propagation, since the predictions of the enhanced layer rely heavily on the accuracy of basic UD tree parsing. The graph-based and transition-based approaches natively produce graph structures, but they require post-processing to ensure connectivity. Ou"
2021.iwpt-1.23,P18-2077,0,0.0548369,"Missing"
2021.iwpt-1.23,2020.iwpt-1.18,0,0.0275364,"into four main categories: • Tree-based: since the overlap between the enhanced UD graphs and the basic UD trees are typically significant, and any deviations tend to be localized and tied to one of several certain syntactic constructions (e.g, argument sharing in a control structure), one can repurpose tree-based parsers for producing enhanced UD graphs. This category of approaches include packing the additional edges from an enhanced graph into the basic tree (Kanerva et al., 2020) and using either rule-based or learning-based approaches to convert a basic UD tree into an enhanced UD graph (Heinecke, 2020; Dehouck et al., 2020; Attardi et al., 2020; Ek and Bernardy, 2020).2 • Graph-based: alternatively, one can directly focus on the enhanced UD graph with a semantic dependency graph parser that predicts the existence and label of each candidate dependency edge. But there is generally no guarantee that the set of predicted edges will form a connected graph, so a postprocessing step is typically employed to fix any connectivity issues. This category of approaches includes the work of Wang et al. (2020), Barry et al. (2020), and Grünewald and Friedrich (2020).3 • Transition-based: Hershcovich et"
2021.iwpt-1.23,2020.iwpt-1.23,0,0.0269326,"etween the enhanced UD graphs and the basic UD trees are typically significant, and any deviations tend to be localized and tied to one of several certain syntactic constructions (e.g, argument sharing in a control structure), one can repurpose tree-based parsers for producing enhanced UD graphs. This category of approaches include packing the additional edges from an enhanced graph into the basic tree (Kanerva et al., 2020) and using either rule-based or learning-based approaches to convert a basic UD tree into an enhanced UD graph (Heinecke, 2020; Dehouck et al., 2020; Attardi et al., 2020; Ek and Bernardy, 2020).2 • Graph-based: alternatively, one can directly focus on the enhanced UD graph with a semantic dependency graph parser that predicts the existence and label of each candidate dependency edge. But there is generally no guarantee that the set of predicted edges will form a connected graph, so a postprocessing step is typically employed to fix any connectivity issues. This category of approaches includes the work of Wang et al. (2020), Barry et al. (2020), and Grünewald and Friedrich (2020).3 • Transition-based: Hershcovich et al. (2020) adapt a transition-based solution. Their system explicitl"
2021.iwpt-1.23,2020.iwpt-1.17,0,0.0698043,"Missing"
2021.iwpt-1.23,D18-1477,0,0.0325508,"Missing"
2021.iwpt-1.23,2021.eacl-demos.10,0,0.3547,"Missing"
2021.iwpt-1.23,2020.lrec-1.497,0,0.0737454,"Missing"
2021.iwpt-1.23,N18-1202,0,0.0193394,"ecreased accuracies on high-resource languages (e.g., −0.02 on Estonian and −0.03 on Russian). Additionally, we find that the language-generic model achieves reasonably competitive performance when compared with the set of models directly trained on each individual language. This suggests that practitioners may opt to use a single model for parsing all languages if there is a need to lower disk and memory footprints, without much loss in accuracy. 4 Pre-TGIF: Pre-Training Grants Improvements Full-Stack Inspired by the recent success of pre-trained language models on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Conneau et al., 2020, inter alia), we build our own text processing pipeline based on pre-trained language models. Due to limited time and resources, we only focus on components relevant to the shared task, which include tokenization, sentence splitting, and multiword token (MWT) expansion. 216 4.1 Tokenizers with Character-Level Masked Language Model Pre-Training 4.3 We follow state-of-the-art strategies (Qi et al., 2020; Nguyen et al., 2021) for tokenization and model the task as a tagging problem on sequences of characters. But in contrast to prior methods where token"
2021.iwpt-1.23,W15-1821,0,0.0189306,"del. Ukrainian 4 Dutch French 2 0 Swedish Estonian Latvian Polish Czech Finnish Bulgarian Russian Italian English Arabic 103 104 105 Number of Training Sentences Official Evaluation The shared task performs evaluation on UD treebanks that have enhanced UD annotations across 17 languages: Arabic (Hajiˇc et al., 2009), Bulgarian (Simov et al., 2004), Czech (Hladká et al., 2010; Bejˇcek et al., 2013; Jelínek, 2017), Dutch (van der Beek et al., 2002; Bouma and van Noord, 2017), English (Silveira et al., 2014; Zeldes, 2017), Estonian (Muischnek et al., 2014, 2019), Finnish (Haverinen et al., 2014; Pyysalo et al., 2015), French (Candito et al., 2014; Seddah and Candito, 2016), Italian (Bosco et al., 2013), Latvian (Pretkalnin, a et al., 2018), Lithuanian (Bielinskien˙e et al., 2016), Polish (Patejuk and Przepiórkowski, 2018; Wróblewska, 2018), Russian (Droganova et al., 2018), Slovak (Zeman, 2018), Swedish (Nivre and Megyesi, 2007), Tamil (Ramasamy and Žabokrtský, 2012), Ukrainian (Kotsyba et al., 2016), and multilingual parallel treebanks (Zeman et al., 2017). Figure 2: The per-language delta ELAS between our submission and the best performing system other than ours, as a function of (the log of the) number"
2021.iwpt-1.23,2020.acl-demos.14,0,0.325029,"4 Pre-TGIF: Pre-Training Grants Improvements Full-Stack Inspired by the recent success of pre-trained language models on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Conneau et al., 2020, inter alia), we build our own text processing pipeline based on pre-trained language models. Due to limited time and resources, we only focus on components relevant to the shared task, which include tokenization, sentence splitting, and multiword token (MWT) expansion. 216 4.1 Tokenizers with Character-Level Masked Language Model Pre-Training 4.3 We follow state-of-the-art strategies (Qi et al., 2020; Nguyen et al., 2021) for tokenization and model the task as a tagging problem on sequences of characters. But in contrast to prior methods where tokenization and sentence segmentation are bundled into the same prediction stage, we tackle tokenization in isolation, and for each character, we make a binary prediction as to whether a token ends at the current character position or not. An innovation in our tokenization is that we finetune character-based language models trained on Wikipedia data. In contrast, existing approaches typically use randomly-initialized models (Qi et al., 2020) or use"
2021.iwpt-1.23,ramasamy-zabokrtsky-2012-prague,0,0.0837021,"Missing"
2021.iwpt-1.23,L16-1376,0,0.227882,"Missing"
2021.iwpt-1.23,L16-1375,0,0.053248,"Missing"
2021.iwpt-1.23,silveira-etal-2014-gold,0,0.0308003,"Missing"
2021.iwpt-1.23,2020.iwpt-1.22,0,0.543403,"ther rule-based or learning-based approaches to convert a basic UD tree into an enhanced UD graph (Heinecke, 2020; Dehouck et al., 2020; Attardi et al., 2020; Ek and Bernardy, 2020).2 • Graph-based: alternatively, one can directly focus on the enhanced UD graph with a semantic dependency graph parser that predicts the existence and label of each candidate dependency edge. But there is generally no guarantee that the set of predicted edges will form a connected graph, so a postprocessing step is typically employed to fix any connectivity issues. This category of approaches includes the work of Wang et al. (2020), Barry et al. (2020), and Grünewald and Friedrich (2020).3 • Transition-based: Hershcovich et al. (2020) adapt a transition-based solution. Their system explicitly handles empty nodes through a specialized transition for inserting them; it relies on additional post-processing to ensure connectivity. • Tree-Graph Integrated: He and Choi (2020) integrate a tree parser and a graph parser,4 where the tree parser produces the basic UD tree, and the graph parser predicts any additional edges. During inference, all nodes are automatically connected through the tree parser, and the graph parser allow"
2021.iwpt-1.23,W18-6020,0,0.0278764,"ks that have enhanced UD annotations across 17 languages: Arabic (Hajiˇc et al., 2009), Bulgarian (Simov et al., 2004), Czech (Hladká et al., 2010; Bejˇcek et al., 2013; Jelínek, 2017), Dutch (van der Beek et al., 2002; Bouma and van Noord, 2017), English (Silveira et al., 2014; Zeldes, 2017), Estonian (Muischnek et al., 2014, 2019), Finnish (Haverinen et al., 2014; Pyysalo et al., 2015), French (Candito et al., 2014; Seddah and Candito, 2016), Italian (Bosco et al., 2013), Latvian (Pretkalnin, a et al., 2018), Lithuanian (Bielinskien˙e et al., 2016), Polish (Patejuk and Przepiórkowski, 2018; Wróblewska, 2018), Russian (Droganova et al., 2018), Slovak (Zeman, 2018), Swedish (Nivre and Megyesi, 2007), Tamil (Ramasamy and Žabokrtský, 2012), Ukrainian (Kotsyba et al., 2016), and multilingual parallel treebanks (Zeman et al., 2017). Figure 2: The per-language delta ELAS between our submission and the best performing system other than ours, as a function of (the log of the) number of training sentences. (For Italian, the difference is quite small but still positive.) Our models achieve larger improvements on lower-resource languages. Table 3 shows the official ELAS evaluation results of all 9 participat"
2021.iwpt-1.23,K18-2001,0,0.129555,"Missing"
2021.naacl-main.234,N09-1009,0,0.0325624,"lower overall recalls. 5 Related Work Unsupervised Parsing Our distantly-supervised setting is similar to unsupervised in the sense that it does not require syntactic annotations. Typically, lack of annotations implies that unsupervised parsers induce grammar from a raw stream of lexical or part-of-speech tokens (Clark, 2001; Klein, 2005) along with carefully designed inductive biases on parameter priors (Liang et al., 2007; Wang and Blunsom, 2013), language universals (Naseem et al., 2010; Martínez Alonso et al., 2017), cross-linguistic (Snyder et al., 2009; BergKirkpatrick and Klein, 2010; Cohen and Smith, 2009; Han et al., 2019) and cross-modal (Shi et al., 2019) signals, structural constraints (Gillenwater et al., 2010; Noji et al., 2016; Jin et al., 2018), etc. The models are usually generative and learn from (re)constructing sentences based on induced structures (Shen et al., 2018, 2019; Drozdov et al., 2019; Kim et al., 2019a,b). Alternatively, one may use reinforcement learning to induce syntactic structures using rewards defined by end tasks (Yogatama et al., 2017; Choi et al., 2018; Havrylov et al., 2019). Our method is related to learning from constituency tests (Cao et al., 2020), but our"
2021.naacl-main.234,N19-1423,0,0.00982208,"ally conflict with the reference trees and can thus potentially harm training. The QA-SRL data seems more promising for inducing betterquality syntactic structures, as there are more bracketings available across a diverse set of constituent types. 3 Parsing Model Preliminaries The inputs to our learning algorithm are tuples (w, B), where w = w1 , . . . , wn is a length-n sentence and B = {(bk , ek )} is a set of naturally-occurring bracketings, denoted by the beginning and ending indices bk and ek into the sentence w. As a first step, we extract BERT-based contextualized word representations (Devlin et al., 2019) to associate each token wi with a vector xi .5 See Appendix B for details. Scoring Spans Based on the xi vectors, we assign a score sij to each candidate span (i, j) in the sentence indicating its appropriateness as a constituent in the output structure. We adopt a biaffine scoring function (Dozat and Manning, 2017): sij = [li ; 1]T W [rj ; 1], where [v; 1] appends 1 to the end of vector v, and li = MLPleft (xi ) and rj = MLPright (xj ) loss function to encourage a large margin of at least ∆(y, y ∗ ) between the gold tree y ∗ and any predicted tree y: l = max [s(y) + ∆(y, y ∗ )] − s(y ∗ ), y∈"
2021.naacl-main.234,P10-1131,0,0.0494559,"Missing"
2021.naacl-main.234,P19-1340,0,0.0321868,"Missing"
2021.naacl-main.234,P16-1033,0,0.0159696,"f Kim et al. (2019a). More specifically, we dis- native solution of training parsers from partiallyannotated data has attracted considerable research card punctuation and trivial spans (single-word and attention, especially within the context of active full-sentence spans) during evaluation and report learning for dependency parsing (Sassano, 2005; sentence-level F1 scores as our main metrics. Sassano and Kurohashi, 2010; Mirroshandel and Results Table 2 shows the evaluation results of Nasr, 2011; Flannery et al., 2011; Flannery and our models trained on naturally-occurring brack- Mori, 2015; Li et al., 2016; Zhang et al., 2017) etings (NOB); Table 3 breaks down the recall and grammar induction for constituency parsing 2944 (Pereira and Schabes, 1992; Hwa, 1999; Riezler et al., 2002). These works typically require expert annotators to generate gold-standard, though partial, annotations. In contrast, our work considers the setting and the challenge of learning from noisy bracketing data, which is more comparable to Spreyer and Kuhn (2009) and Spreyer et al. (2010) on transfer learning for dependency parsing. 6 Conclusion and Future Work We argue that naturally-occurring bracketings are a rich reso"
2021.naacl-main.234,D07-1072,0,0.0862877,"sentence (Table 1). Finally, our Wikipedia data has a larger relative percentage of ADJP bracketings, which explains the higher ADJP recall of the models trained on Wikipedia, despite their lower overall recalls. 5 Related Work Unsupervised Parsing Our distantly-supervised setting is similar to unsupervised in the sense that it does not require syntactic annotations. Typically, lack of annotations implies that unsupervised parsers induce grammar from a raw stream of lexical or part-of-speech tokens (Clark, 2001; Klein, 2005) along with carefully designed inductive biases on parameter priors (Liang et al., 2007; Wang and Blunsom, 2013), language universals (Naseem et al., 2010; Martínez Alonso et al., 2017), cross-linguistic (Snyder et al., 2009; BergKirkpatrick and Klein, 2010; Cohen and Smith, 2009; Han et al., 2019) and cross-modal (Shi et al., 2019) signals, structural constraints (Gillenwater et al., 2010; Noji et al., 2016; Jin et al., 2018), etc. The models are usually generative and learn from (re)constructing sentences based on induced structures (Shen et al., 2018, 2019; Drozdov et al., 2019; Kim et al., 2019a,b). Alternatively, one may use reinforcement learning to induce syntactic struct"
2021.naacl-main.234,J93-2004,0,0.0747349,"models can mitigate the fact that our distant supervision data are either out-of-domain (Wikipedia) or small in size (QA-SRL). 6 This is inspired by span-based supervised constituencyparsing methods (Stern et al., 2017), which in turn was based on Wang and Chang (2016). These papers look at the difference vectors between two boundary points, while our scoring function directly uses the vectors at the boundaries (which is more expressive than only using difference vectors). cstrict (i, j, y˜) = 1(span (i, j) not in y˜), 4 Experiments and Results Data and Implementation We evaluate on the PTB (Marcus et al., 1993) with the standard splits 7 One may also consider a linear interpolation of cloose and cstrict , but that would introduce an additional hyper-parameter. 2943 Model PLM Mean 19.2 Random Trees Left Branching Right Branching Max 19.5 8.7 39.5 84.3 Upper bound URNNG (Kim et al., 2019b) PRPN (Shen et al., 2018) ON (Shen et al., 2019) DIORA (Drozdov et al., 2019) CPCFG (Kim et al., 2019a) S-DIORA (Drozdov et al., 2020) Constituency Tests (Cao et al., 2020) +URNNG (Cao et al., 2020) This work: NOBQA-SRL , cloose NOBQA-SRL , cstrict NOBWikipedia , cloose NOBWikipedia , cstrict ◦ • • — 47.3 48.1 — 55.2"
2021.naacl-main.234,E17-1022,0,0.0443594,"Missing"
2021.naacl-main.234,W11-2917,0,0.085184,"Missing"
2021.naacl-main.234,D10-1120,0,0.0466576,"tive percentage of ADJP bracketings, which explains the higher ADJP recall of the models trained on Wikipedia, despite their lower overall recalls. 5 Related Work Unsupervised Parsing Our distantly-supervised setting is similar to unsupervised in the sense that it does not require syntactic annotations. Typically, lack of annotations implies that unsupervised parsers induce grammar from a raw stream of lexical or part-of-speech tokens (Clark, 2001; Klein, 2005) along with carefully designed inductive biases on parameter priors (Liang et al., 2007; Wang and Blunsom, 2013), language universals (Naseem et al., 2010; Martínez Alonso et al., 2017), cross-linguistic (Snyder et al., 2009; BergKirkpatrick and Klein, 2010; Cohen and Smith, 2009; Han et al., 2019) and cross-modal (Shi et al., 2019) signals, structural constraints (Gillenwater et al., 2010; Noji et al., 2016; Jin et al., 2018), etc. The models are usually generative and learn from (re)constructing sentences based on induced structures (Shen et al., 2018, 2019; Drozdov et al., 2019; Kim et al., 2019a,b). Alternatively, one may use reinforcement learning to induce syntactic structures using rewards defined by end tasks (Yogatama et al., 2017; Cho"
2021.naacl-main.234,D16-1004,0,0.281064,"Missing"
2021.naacl-main.234,P92-1017,0,0.825425,"rable research card punctuation and trivial spans (single-word and attention, especially within the context of active full-sentence spans) during evaluation and report learning for dependency parsing (Sassano, 2005; sentence-level F1 scores as our main metrics. Sassano and Kurohashi, 2010; Mirroshandel and Results Table 2 shows the evaluation results of Nasr, 2011; Flannery et al., 2011; Flannery and our models trained on naturally-occurring brack- Mori, 2015; Li et al., 2016; Zhang et al., 2017) etings (NOB); Table 3 breaks down the recall and grammar induction for constituency parsing 2944 (Pereira and Schabes, 1992; Hwa, 1999; Riezler et al., 2002). These works typically require expert annotators to generate gold-standard, though partial, annotations. In contrast, our work considers the setting and the challenge of learning from noisy bracketing data, which is more comparable to Spreyer and Kuhn (2009) and Spreyer et al. (2010) on transfer learning for dependency parsing. 6 Conclusion and Future Work We argue that naturally-occurring bracketings are a rich resource for inducing syntactic structures. They reflect human judgment of what constitutes a phrase and what does not. More importantly, they requir"
2021.naacl-main.234,C16-1059,0,0.0625032,"Missing"
2021.naacl-main.234,P02-1035,0,0.298314,"ivial spans (single-word and attention, especially within the context of active full-sentence spans) during evaluation and report learning for dependency parsing (Sassano, 2005; sentence-level F1 scores as our main metrics. Sassano and Kurohashi, 2010; Mirroshandel and Results Table 2 shows the evaluation results of Nasr, 2011; Flannery et al., 2011; Flannery and our models trained on naturally-occurring brack- Mori, 2015; Li et al., 2016; Zhang et al., 2017) etings (NOB); Table 3 breaks down the recall and grammar induction for constituency parsing 2944 (Pereira and Schabes, 1992; Hwa, 1999; Riezler et al., 2002). These works typically require expert annotators to generate gold-standard, though partial, annotations. In contrast, our work considers the setting and the challenge of learning from noisy bracketing data, which is more comparable to Spreyer and Kuhn (2009) and Spreyer et al. (2010) on transfer learning for dependency parsing. 6 Conclusion and Future Work We argue that naturally-occurring bracketings are a rich resource for inducing syntactic structures. They reflect human judgment of what constitutes a phrase and what does not. More importantly, they require low annotation expertise and eff"
2021.naacl-main.234,I05-1008,0,0.054623,"20), but our use of bracketing data permits discriminative parsing models, which focus directly on the syntactic objective. Learning from Partial Annotations Full syntactic annotations are costly to obtain, so the alterEvaluation We follow the evaluation setting of Kim et al. (2019a). More specifically, we dis- native solution of training parsers from partiallyannotated data has attracted considerable research card punctuation and trivial spans (single-word and attention, especially within the context of active full-sentence spans) during evaluation and report learning for dependency parsing (Sassano, 2005; sentence-level F1 scores as our main metrics. Sassano and Kurohashi, 2010; Mirroshandel and Results Table 2 shows the evaluation results of Nasr, 2011; Flannery et al., 2011; Flannery and our models trained on naturally-occurring brack- Mori, 2015; Li et al., 2016; Zhang et al., 2017) etings (NOB); Table 3 breaks down the recall and grammar induction for constituency parsing 2944 (Pereira and Schabes, 1992; Hwa, 1999; Riezler et al., 2002). These works typically require expert annotators to generate gold-standard, though partial, annotations. In contrast, our work considers the setting and t"
2021.naacl-main.234,P10-1037,0,0.0378563,"rsing models, which focus directly on the syntactic objective. Learning from Partial Annotations Full syntactic annotations are costly to obtain, so the alterEvaluation We follow the evaluation setting of Kim et al. (2019a). More specifically, we dis- native solution of training parsers from partiallyannotated data has attracted considerable research card punctuation and trivial spans (single-word and attention, especially within the context of active full-sentence spans) during evaluation and report learning for dependency parsing (Sassano, 2005; sentence-level F1 scores as our main metrics. Sassano and Kurohashi, 2010; Mirroshandel and Results Table 2 shows the evaluation results of Nasr, 2011; Flannery et al., 2011; Flannery and our models trained on naturally-occurring brack- Mori, 2015; Li et al., 2016; Zhang et al., 2017) etings (NOB); Table 3 breaks down the recall and grammar induction for constituency parsing 2944 (Pereira and Schabes, 1992; Hwa, 1999; Riezler et al., 2002). These works typically require expert annotators to generate gold-standard, though partial, annotations. In contrast, our work considers the setting and the challenge of learning from noisy bracketing data, which is more comparab"
2021.naacl-main.234,P19-1180,0,0.189328,"g Our distantly-supervised setting is similar to unsupervised in the sense that it does not require syntactic annotations. Typically, lack of annotations implies that unsupervised parsers induce grammar from a raw stream of lexical or part-of-speech tokens (Clark, 2001; Klein, 2005) along with carefully designed inductive biases on parameter priors (Liang et al., 2007; Wang and Blunsom, 2013), language universals (Naseem et al., 2010; Martínez Alonso et al., 2017), cross-linguistic (Snyder et al., 2009; BergKirkpatrick and Klein, 2010; Cohen and Smith, 2009; Han et al., 2019) and cross-modal (Shi et al., 2019) signals, structural constraints (Gillenwater et al., 2010; Noji et al., 2016; Jin et al., 2018), etc. The models are usually generative and learn from (re)constructing sentences based on induced structures (Shen et al., 2018, 2019; Drozdov et al., 2019; Kim et al., 2019a,b). Alternatively, one may use reinforcement learning to induce syntactic structures using rewards defined by end tasks (Yogatama et al., 2017; Choi et al., 2018; Havrylov et al., 2019). Our method is related to learning from constituency tests (Cao et al., 2020), but our use of bracketing data permits discriminative parsing"
2021.naacl-main.234,P09-1009,0,0.249078,"call of the models trained on Wikipedia, despite their lower overall recalls. 5 Related Work Unsupervised Parsing Our distantly-supervised setting is similar to unsupervised in the sense that it does not require syntactic annotations. Typically, lack of annotations implies that unsupervised parsers induce grammar from a raw stream of lexical or part-of-speech tokens (Clark, 2001; Klein, 2005) along with carefully designed inductive biases on parameter priors (Liang et al., 2007; Wang and Blunsom, 2013), language universals (Naseem et al., 2010; Martínez Alonso et al., 2017), cross-linguistic (Snyder et al., 2009; BergKirkpatrick and Klein, 2010; Cohen and Smith, 2009; Han et al., 2019) and cross-modal (Shi et al., 2019) signals, structural constraints (Gillenwater et al., 2010; Noji et al., 2016; Jin et al., 2018), etc. The models are usually generative and learn from (re)constructing sentences based on induced structures (Shen et al., 2018, 2019; Drozdov et al., 2019; Kim et al., 2019a,b). Alternatively, one may use reinforcement learning to induce syntactic structures using rewards defined by end tasks (Yogatama et al., 2017; Choi et al., 2018; Havrylov et al., 2019). Our method is related to learn"
2021.naacl-main.234,W17-6310,0,0.0416453,"Missing"
2021.naacl-main.234,W11-0303,0,0.0350163,"es, (2) we incorporate two new cost functions into structured ramp loss to train parsers with noisy bracketings, and (3) our distantly-supervised models achieve results competitive with the state of the art of unsupervised constituency parsing despite training with smaller data size (QA-SRL) or out-of-domain data (Wikipedia). 2 Naturally-Occurring Bracketings Constituents are naturally reflected in various human cognitive processes, including speech production and perception (Garrett et al., 1966; Gee and Grosjean, 1983), reading behaviors (Hale, 2001; Boston et al., 2008), punctuation marks (Spitkovsky et al., 2011), and keystroke dynamics (Plank, 2016). Conversely, these externalized signals help us gain insight into constituency representations. We consider two such data sources: a) Answer fragments When questions are answered with fragments instead of full sentences, those fragments tend to form constituents. This phenomenon corresponds to a well-established constituency test in the linguistics literature (Carnie, 2012, pg. 98, inter alia). 2 We worked with articles in English. Dataset QA-SRL Wikipedia Number of sentences Brackets/sentence 1,241 6.26 926,077 0.89 Single word Constituent in reference C"
2021.naacl-main.234,P10-1130,0,0.0842567,"Missing"
2021.naacl-main.234,W09-1104,0,0.0338027,"oshandel and Results Table 2 shows the evaluation results of Nasr, 2011; Flannery et al., 2011; Flannery and our models trained on naturally-occurring brack- Mori, 2015; Li et al., 2016; Zhang et al., 2017) etings (NOB); Table 3 breaks down the recall and grammar induction for constituency parsing 2944 (Pereira and Schabes, 1992; Hwa, 1999; Riezler et al., 2002). These works typically require expert annotators to generate gold-standard, though partial, annotations. In contrast, our work considers the setting and the challenge of learning from noisy bracketing data, which is more comparable to Spreyer and Kuhn (2009) and Spreyer et al. (2010) on transfer learning for dependency parsing. 6 Conclusion and Future Work We argue that naturally-occurring bracketings are a rich resource for inducing syntactic structures. They reflect human judgment of what constitutes a phrase and what does not. More importantly, they require low annotation expertise and effort; for example, webpage hyperlinks can be extracted essentially for free. Empirically, our models trained on QA-SRL and Wikipedia bracketings achieve competitive results with the state of the art on unsupervised constituency parsing. Structural probes have"
2021.naacl-main.234,spreyer-etal-2010-training,0,0.0926994,"Missing"
2021.naacl-main.234,P17-1076,0,0.0458362,"ting where y˜ refers to a fully-annotated tree y ∗ without conflicting span boundaries, cstrict is equal to cloose and the resulting ∆(y, y ∗ ) cost functions both correspond to the Hamming distance between y and y ∗ . Learning Large-margin training (Taskar et al., 2005) is a typical choice for supervised training of constituency parsers. It defines the following 5 The use of pre-trained language models can mitigate the fact that our distant supervision data are either out-of-domain (Wikipedia) or small in size (QA-SRL). 6 This is inspired by span-based supervised constituencyparsing methods (Stern et al., 2017), which in turn was based on Wang and Chang (2016). These papers look at the difference vectors between two boundary points, while our scoring function directly uses the vectors at the boundaries (which is more expressive than only using difference vectors). cstrict (i, j, y˜) = 1(span (i, j) not in y˜), 4 Experiments and Results Data and Implementation We evaluate on the PTB (Marcus et al., 1993) with the standard splits 7 One may also consider a linear interpolation of cloose and cstrict , but that would introduce an additional hyper-parameter. 2943 Model PLM Mean 19.2 Random Trees Left Bran"
2021.naacl-main.234,W13-3519,0,0.0248228,"Finally, our Wikipedia data has a larger relative percentage of ADJP bracketings, which explains the higher ADJP recall of the models trained on Wikipedia, despite their lower overall recalls. 5 Related Work Unsupervised Parsing Our distantly-supervised setting is similar to unsupervised in the sense that it does not require syntactic annotations. Typically, lack of annotations implies that unsupervised parsers induce grammar from a raw stream of lexical or part-of-speech tokens (Clark, 2001; Klein, 2005) along with carefully designed inductive biases on parameter priors (Liang et al., 2007; Wang and Blunsom, 2013), language universals (Naseem et al., 2010; Martínez Alonso et al., 2017), cross-linguistic (Snyder et al., 2009; BergKirkpatrick and Klein, 2010; Cohen and Smith, 2009; Han et al., 2019) and cross-modal (Shi et al., 2019) signals, structural constraints (Gillenwater et al., 2010; Noji et al., 2016; Jin et al., 2018), etc. The models are usually generative and learn from (re)constructing sentences based on induced structures (Shen et al., 2018, 2019; Drozdov et al., 2019; Kim et al., 2019a,b). Alternatively, one may use reinforcement learning to induce syntactic structures using rewards define"
2021.naacl-main.234,P16-1218,0,0.0263718,"∗ without conflicting span boundaries, cstrict is equal to cloose and the resulting ∆(y, y ∗ ) cost functions both correspond to the Hamming distance between y and y ∗ . Learning Large-margin training (Taskar et al., 2005) is a typical choice for supervised training of constituency parsers. It defines the following 5 The use of pre-trained language models can mitigate the fact that our distant supervision data are either out-of-domain (Wikipedia) or small in size (QA-SRL). 6 This is inspired by span-based supervised constituencyparsing methods (Stern et al., 2017), which in turn was based on Wang and Chang (2016). These papers look at the difference vectors between two boundary points, while our scoring function directly uses the vectors at the boundaries (which is more expressive than only using difference vectors). cstrict (i, j, y˜) = 1(span (i, j) not in y˜), 4 Experiments and Results Data and Implementation We evaluate on the PTB (Marcus et al., 1993) with the standard splits 7 One may also consider a linear interpolation of cloose and cstrict , but that would introduce an additional hyper-parameter. 2943 Model PLM Mean 19.2 Random Trees Left Branching Right Branching Max 19.5 8.7 39.5 84.3 Upper"
2021.naacl-main.234,I17-1006,0,0.0135304,"9a). More specifically, we dis- native solution of training parsers from partiallyannotated data has attracted considerable research card punctuation and trivial spans (single-word and attention, especially within the context of active full-sentence spans) during evaluation and report learning for dependency parsing (Sassano, 2005; sentence-level F1 scores as our main metrics. Sassano and Kurohashi, 2010; Mirroshandel and Results Table 2 shows the evaluation results of Nasr, 2011; Flannery et al., 2011; Flannery and our models trained on naturally-occurring brack- Mori, 2015; Li et al., 2016; Zhang et al., 2017) etings (NOB); Table 3 breaks down the recall and grammar induction for constituency parsing 2944 (Pereira and Schabes, 1992; Hwa, 1999; Riezler et al., 2002). These works typically require expert annotators to generate gold-standard, though partial, annotations. In contrast, our work considers the setting and the challenge of learning from noisy bracketing data, which is more comparable to Spreyer and Kuhn (2009) and Spreyer et al. (2010) on transfer learning for dependency parsing. 6 Conclusion and Future Work We argue that naturally-occurring bracketings are a rich resource for inducing syn"
A00-2032,P98-1068,0,0.0388679,"Missing"
A00-2032,A92-1022,0,0.0220787,"relatively few sequences; if it is low, then a human doing postprocessing would have to correct many sequences. 85 1 ................................................................................................................................................................... 8O ,¢:: ~- 75 o CHASEN JUMAN optimize optimizerecall optir#.zeF Ixacir,~ Figure 5: Morpheme accuracy. rect with respect to any reasonable annotation. Our novel metrics account for two types of errors. The first, a crossing bracket, is a proposed bracket that overlaps but is not contained within an annotation bracket (Grishman et al., 1992). Crossing brackets cannot coexist with annotation brackets, and it is unlikely that another human would create such brackets. The second type of error, a morpheme-dividing bracket, subdivides a morpheme-level annotation bracket; by definition, such a bracket results in a loss of meaning. See Figure 6 for some examples. We define a compatible bracket as a proposed bracket that is neither crossing nor morphemedividing. The compatible brackets rate is simply the compatible brackets precision. Note that this metric accounts for different levels of segmentation simultaneously, which is beneficial"
A00-2032,P98-1108,0,0.0958911,"eds of thousands of words (the size of Chasen's and Juman's default lexicons) is clearly much larger than annotating the small parameter-training sets for our algorithm. We also avoid the need to segment a large amount of parameter-training data because our algorithm draws almost all its information from an unsegmented corpus. Indeed, the only human effort involved in our algorithm is pre-segmenting the five 50-sequence parameter training sets, which took only 42 minutes. In contrast, previously proposed supervised approaches have used segmented training sets ranging from 1000-5000 sentences (Kashioka et al., 1998) to 190,000 sentences (Nagata, 1996a). To test how much annotated training data is actually necessary, we experimented with using miniscule parameter-training sets: five sets of only five strings each (from which any sequences repeated in the test data were discarded). It took only 4 minutes to perform the hand segmentation in this case. As shown in Figure 8, relative word performance was not degraded and sometimes even slightly better. In fact, from the last column of Figure 8 we see that even if our algorithm has access to only five annotated sequences when Juman has access to ten times as m"
A00-2032,H93-1045,0,0.060196,"Missing"
A00-2032,C94-1101,0,0.0162839,"aced at all locations ~ such that either: • VN(g) > VN(e -- 1) and VN(g) > VN(e + 1) (that is, e is a local maximum), or • VN (2) > t, a threshold parameter. The second condition is necessary to allow for single-character words (see Figure 3). Note that it also controls the granularity of the segmentation: low thresholds encourage shorter segments. Both the count acquisition and the testing phase are efficient. Computing n-gram statistics for all possible values of n simultaneously can be done in O(m log m) time using suffix arrays, where m is the training corpus size (Manber and Myers, 1993; Nagao and Mori, 1994). However, if the set N of n-gram orders is known in advance, conceptually simpler algorithms suffice. Memory allocation for :Note that we do not take into account the magnitude of the difference between the two frequencies; see section 5 for discussion. v~(k) A B [C DI W X IY]Z Figure 3: Determining word boundaries. The X- Y boundary is created by the threshold criterion, the other three by the local maximum condition. count tables can be significantly reduced by omitting n-grams occurring only once and assuming the count of unseen n-grams to be one. In the application phase, the algorithm is"
A00-2032,C94-1032,0,0.0795445,"Missing"
A00-2032,W96-0205,0,0.0546714,"'s and Juman's default lexicons) is clearly much larger than annotating the small parameter-training sets for our algorithm. We also avoid the need to segment a large amount of parameter-training data because our algorithm draws almost all its information from an unsegmented corpus. Indeed, the only human effort involved in our algorithm is pre-segmenting the five 50-sequence parameter training sets, which took only 42 minutes. In contrast, previously proposed supervised approaches have used segmented training sets ranging from 1000-5000 sentences (Kashioka et al., 1998) to 190,000 sentences (Nagata, 1996a). To test how much annotated training data is actually necessary, we experimented with using miniscule parameter-training sets: five sets of only five strings each (from which any sequences repeated in the test data were discarded). It took only 4 minutes to perform the hand segmentation in this case. As shown in Figure 8, relative word performance was not degraded and sometimes even slightly better. In fact, from the last column of Figure 8 we see that even if our algorithm has access to only five annotated sequences when Juman has access to ten times as many, we still achieve better precis"
A00-2032,C96-2136,0,0.102041,"'s and Juman's default lexicons) is clearly much larger than annotating the small parameter-training sets for our algorithm. We also avoid the need to segment a large amount of parameter-training data because our algorithm draws almost all its information from an unsegmented corpus. Indeed, the only human effort involved in our algorithm is pre-segmenting the five 50-sequence parameter training sets, which took only 42 minutes. In contrast, previously proposed supervised approaches have used segmented training sets ranging from 1000-5000 sentences (Kashioka et al., 1998) to 190,000 sentences (Nagata, 1996a). To test how much annotated training data is actually necessary, we experimented with using miniscule parameter-training sets: five sets of only five strings each (from which any sequences repeated in the test data were discarded). It took only 4 minutes to perform the hand segmentation in this case. As shown in Figure 8, relative word performance was not degraded and sometimes even slightly better. In fact, from the last column of Figure 8 we see that even if our algorithm has access to only five annotated sequences when Juman has access to ten times as many, we still achieve better precis"
A00-2032,W97-0120,0,0.0291403,"Missing"
A00-2032,P97-1041,0,0.0911542,"Missing"
A00-2032,H94-1054,0,0.248975,"Missing"
A00-2032,J96-3004,0,0.0659203,"Missing"
A00-2032,P98-2206,0,0.173894,"Missing"
A00-2032,Y95-1022,0,0.0307168,"Missing"
A00-2032,W97-1008,0,0.0491394,"Missing"
A00-2032,W96-0113,0,0.0384502,"Missing"
A00-2032,H93-1037,0,0.0253752,"Missing"
A00-2032,C98-2201,0,\N,Missing
A00-2032,C98-1104,0,\N,Missing
A00-2032,H93-1087,0,\N,Missing
A00-2032,C98-1065,0,\N,Missing
C08-2004,H05-1042,0,0.0133662,"es of the minimum-cut framework. 1 Claire Cardie and Lillian Lee Dept. of Computer Science Cornell University {cardie,llee}@cs.cornell.edu Introduction Classification algorithms based on formulating the classification task as one of finding minimum s-t cuts in edge-weighted graphs — henceforth minimum cuts or min cuts — have been successfully employed in vision, computational biology, and natural language processing. Within NLP, applications include sentiment-analysis problems (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006) and content selection for text generation (Barzilay and Lapata, 2005). 2 Method 2.1 Min-cut classification framework c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Binary classification problems are usually approached by considering each classification decision in isolation. More formally, let Xtest = 15 Coling 2008: Companion volume – Posters and Demonstrations, pages 15–18 Manchester, August 2008 {x1 , x2 , . . . , xn } be the test instances, drawn from some universe X, and let C = {c1 , c2 } be the two possible classes. Then, the"
C08-2004,P04-1035,1,0.0626428,"abel-disagreement information in a way that improves classification accuracy while preserving the efficiency guarantees of the minimum-cut framework. 1 Claire Cardie and Lillian Lee Dept. of Computer Science Cornell University {cardie,llee}@cs.cornell.edu Introduction Classification algorithms based on formulating the classification task as one of finding minimum s-t cuts in edge-weighted graphs — henceforth minimum cuts or min cuts — have been successfully employed in vision, computational biology, and natural language processing. Within NLP, applications include sentiment-analysis problems (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006) and content selection for text generation (Barzilay and Lapata, 2005). 2 Method 2.1 Min-cut classification framework c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Binary classification problems are usually approached by considering each classification decision in isolation. More formally, let Xtest = 15 Coling 2008: Companion volume – Posters and Demonstrations, pages 15–18 Manchester, August 2008 {x1 , x2 ,"
C08-2004,W06-1639,1,0.632297,"edge weights, respectively; but in general, the inclusion of even a relatively small number of negative edge weights makes finding a minimum cut NP-hard (McCormick et al., 2003). To avoid this computational issue, we propose several heuristics that encode disagreement information with non-negative edge weights. We instantiate our approach on a sentiment-polarity classification task — determining whether individual conversational turns in U.S. Congressional floor debates support or oppose some given legislation. Our preliminary results demonstrate promising improvements over the prior work of Thomas et al. (2006), who considered only the use of agreement information in this domain. Treating classification as seeking minimum cuts in the appropriate graph has proven effective in a number of applications. The power of this approach lies in its ability to incorporate label-agreement preferences among pairs of instances in a provably tractable way. Label disagreement preferences are another potentially rich source of information, but prior NLP work within the minimum-cut paradigm has not explicitly incorporated it. Here, we report on work in progress that examines several novel heuristics for incorporating"
C08-2019,C00-1044,0,0.028631,"hree labels: “subj”, “obj”, or “unknown”. First, discard the sentences labeled “unknown”. Then, rank the documents by decreasing percentage of subjective sentences among those sentences that are left. In the case of ties, we use the ranking produced by the initial search engine. We also considered a more lightweight way to incorporate linguistic knowledge: score each document according the percentage of adjectives within the set of tokens it contains. The motivation is previous work suggesting that the presence of adjectives is a strong indicator of the subjectivity of the enclosing sentence (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2004). Defining search-set rarity There are various ways to define a search-set rarity function on terms. Inspired by the efficacy of the inverse document frequency (IDF) in information retrieval, we consider several definitions for Rarityss (t). Let nss (t) be the number of documents in the search set (not the entire corpus) that contain the term t. Due to space constraints, we 1 1 , nss (t) 2 There are actually two versions. We used the accuracyoptimized version, as it outperformed the precision-optimized version. 3 OpinionFinder will only process documents in which all strin"
C08-2019,W03-1014,0,0.105215,"or topic-based information retrieval, may be important for subjectivity detection. Therefore, we used a 102-item list1 based solely on frequencies in the British National Corpus. 2.1 def Rarityss (t) = which is linearly increasing in 1/nss (t), (as befits a measure of “idiosyncrasy”). The other definitions we considered were logarithmic or polynomial in 1/nss (t), and performed similarly to the linear function. 2.2 Comparison algorithms OpinionFinder is a state-of-the-art publicly available software package for sentiment analysis that can be applied to determining sentence-level subjectivity (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005). It employs a number of pre-processing steps, including sentence splitting, part-of-speech tagging, stemming, and shallow parsing. Shallow parsing is needed to identify the extraction patterns that the sentence classifiers incorporate. We used OpinionFinder’s sentence-level output2 to perform document-level subjectivity re-ranking as follows. The result of running OpinionFinder’s sentence classifier is that each valid sentence3 is annotated with one of three labels: “subj”, “obj”, or “unknown”. First, discard the sentences labeled “unknown”. Then, rank the documents b"
C08-2019,J04-3002,0,0.201085,"ple statistics for review search: An exploration Bo Pang Yahoo! Research bopang@yahoo-inc.com Lillian Lee Computer Science Department, Cornell University llee@cs.cornell.edu Abstract search results by placing those that have the least idiosyncratic term distributions, with respect to the statistics of the top k results, at the head of the list. The fact that it is the least, not the most, rare terms with respect to the search results that are most indicative of subjectivity may at first seem rather counterintuitive; indeed, previous work has found rare terms to be important subjectivity cues (Wiebe et al., 2004). However, reviews within a given set of search results may tend to resemble each other because they tend to all discuss salient attributes of the topic in question. We report on work in progress on using very simple statistics in an unsupervised fashion to re-rank search engine results when review-oriented queries are issued; the goal is to bring opinionated or subjective results to the top of the results list. We find that our proposed technique performs comparably to methods that rely on sophisticated pre-encoded linguistic knowledge, and that both substantially improve the initial results"
D17-1002,P15-1033,0,0.167904,"ity Lillian Lee Cornell University tianze@cs.cornell.edu liang.huang.sh@gmail.com llee@cs.cornell.edu Abstract rithms do exist, having been introduced by Huang and Sagae (2010) and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6 q (originally reported as Opn7 q) for a length-n sentence. As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well. Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).1 Inspired by thei"
D17-1002,P16-1231,0,0.288465,"abeled attachment score reported (to our knowledge) on the Chinese Treebank and the “second-best-in-class” result on the English Penn Treebank. 1 Introduction It used to be the case that the most accurate dependency parsers made global decisions and employed exact decoding. But transition-based dependency parsers (TBDPs) have recently achieved state-of-the-art performance, despite the fact that for efficiency reasons, they are usually trained to make local, rather than global, decisions and the decoding process is done approximately, rather than exactly (Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016). The key efficiency issue for decoding is as follows. In order to make accurate (local) attachment decisions, historically, TBDPs have required a large set of features in order to access rich information about particular positions in the stack and buffer of the current parser configuration. But consulting many positions means that although polynomial-time exact-decoding algo1 We note that K&G were not focused on minimizing positions, although they explicitly noted the implications of doing so: “While not explored in this work, [fewer positions] results in very compact state signatures, [which"
D17-1002,C96-1058,0,0.279625,"o deploy them. Interestingly, for both the transition- and graphbased paradigms, the optimal algorithms build dependency trees bottom-up from local structures. It is thus natural to wonder if there are deeper, more formal connections between the two. In previous work, Kuhlmann et al. (2011) related the arc-standard system to the classic CKY algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) in a manner clearly suggested by Figure 1a; CKY can be viewed as a very simple graph-based approach. G´omez-Rodr´ıguez et al. (2008, 2011) formally prove that sequences of steps in the edgefactored GBDP (Eisner, 1996) can be used to emulate any individual step in the arc-hybrid system (Yamada and Matsumoto, 2003) and the Eisner and Satta (1999, Figure 1d) version. However, they did not draw an explicitly direct connection between Eisner and Satta (1999) and TBDPs. Here, we provide an update to these previous findings, stated in terms of the expressiveness of scoring functions, considered as parameterization. For the edge-factored GBDP, we write the score ÑÐ ÑÐ for an edge as fG p h, mq, where h is the head and m the modifier. A tree’s score is the sum of its edge scores. We say that a parameterized depende"
D17-1002,D16-1211,0,0.0597856,"Missing"
D17-1002,P99-1059,0,0.347613,") or not (b “ 0) after the transition sequence is applied. Kuhlmann et al. (2011) show that all three deduction systems can be directly “tabularized” and dynamic programming (DP) can be applied, such that, ignoring for the moment the issue of incorporating complex features (we return to this later), time and space needs are low-order polynomial. Specifically, as the two-index shorthand ri, js suggests, arc-eager and arc-hybrid systems can be implemented to take Opn2 q space and Opn3 q time; the arc-standard system requires Opn3 q space and Opn4 q time (if one applies the so-called hook trick (Eisner and Satta, 1999)). Since an Opn4 q running time is not sufficiently practical even in the simple-feature case, in the remainder of this paper we consider only the archybrid and arc-eager systems, not arc-standard. the collection of left modifiers before right modifiers via its b0 -modifier reð transition. This contrasts with arc-standard, where the attachment of left and right modifiers can be interleaved on the stack. shrpσ, b0 |β, Aqs “ pσ|b0 , β, Aq reñ rpσ|s1 |s0 , β, Aqs “ pσ|s1 , β, A Y tps1 , s0 quq reð rpσ|s0 , b0 |β, Aqs “ pσ, b0 |β, A Y tpb0 , s0 quq Arc-Eager In contrast to the former two systems,"
D17-1002,J13-1002,0,0.0275636,"rently see a concrete way to test the following hypothesis. ÑÐ ÑÐ With t s 0 , b 0 u, in the arc-standard case, situations can arise where there are multiple possible transitions with missing information. In contrast, in the arc-hybrid case, there is only one possible transition with missing information (namely, ÑÐ reñ , introduced in §3.1); perhaps s 1 is therefore not so crucial for arc-hybrid in practice? 4 For simplicity, we only present unlabeled parsing here. See Shi et al. (2017) for labeled-parsing results. 5 Other presentations place ROOT at the end of the buffer or omit it entirely (Ballesteros and Nivre, 2013). 14 assertion rib , js is an indicator variable for whether wi has been attached to its head (b “ 1) or not (b “ 0) after the transition sequence is applied. Kuhlmann et al. (2011) show that all three deduction systems can be directly “tabularized” and dynamic programming (DP) can be applied, such that, ignoring for the moment the issue of incorporating complex features (we return to this later), time and space needs are low-order polynomial. Specifically, as the two-index shorthand ri, js suggests, arc-eager and arc-hybrid systems can be implemented to take Opn2 q space and Opn3 q time; the"
D17-1002,D07-1101,0,0.0281178,"rceptron (MLP) with 1 hidden layer which has 256 hidden units and activation function tanh. We set the dropout rate for the bi-LSTM (Gal and Ghahramani, 2016) and MLP (Srivastava et al., 2014) for each model according to development-set performance.6 All parameters except the word embedThe extra expressiveness of the arc-eager model comes from the scoring functions fsh and fre that capture structural contexts other than headmodifier relations. Unlike traditional higher-order graph-based parsing that directly models relations such as siblinghood (McDonald and Pereira, 2006) or grandparenthood (Carreras, 2007), however, the arguments in those two functions do not have any fixed type of structural interactions. 6 Experiments Data and Evaluation We experimented with English and Chinese. For English, we used the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (via the Stanford parser 3.3.0) of the Penn Treebank (Marcus et al., 1993, PTB). As is standard, we used §2-21 of the Wall Street Journal for training, §22 for development, 6 For bi-LSTM input and recurrent connections, we consider dropout rates in t0., 0.2u, and for MLP, t0., 0.4u. 18 90.5 ?15 Our all global 90.0 ?5 Our arc-eage"
D17-1002,D14-1082,0,0.753131,"data structures in their configurations: (1) a stack of partially parsed subtrees and (2) a buffer (mostly) of unprocessed sentence tokens. To featurize configurations for use in a scoring function, it is common to have features that extract information about the first several elements on the stack and the buffer, such as their word forms and part-of-speech (POS) tags. We refer to these as positional features, as each feature relates to a particular position in the stack or buffer. Typically, millions of sparse indicator features (often developed via manual engineering) are used. In contrast, Chen and Manning (2014) introduce a feature set consisting of dense word-, POS-, and dependency-label embeddings. While dense, these features are for the same 18 positions that have been typically used in prior work. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) adopt bi-directional LSTMs, which have nice expressiveness and context-sensitivity properties, to reduce the number of positions considered down to four and three, for different transition systems, respectively. This naturally begs the question, what is the lower limit on the number of positional features necessary for a parser to pe"
D17-1002,C12-1059,0,0.0220663,"ng Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating from the gold transition path during training to better simulate the test-time environment. There are many directions for further exploration. Two possibilities are to create even better training methods, and to find some way to extend our run-time improvements to other transition systems. It would also be interesting to further investigate relationships between graph-based an"
D17-1002,D16-1238,0,0.0484593,"Missing"
D17-1002,Q13-1033,0,0.0161886,"hen, for a given parser configuration, stack positions ÑÐ ÑÐ are represented by s j , defined as wipsj q where ipsj q gives the position in the sentence of the token that is the head of the tree in sj . Similarly, ÑÐ buffer positions are represented by b j , defined as ÑÐ wipbj q for the token at buffer position j. Finally, as in Chen and Manning (2014), we use a multilayer perceptron to score possible transitions from the given configuration, where the input is the conÑÐ ÑÐ catenation of some selection of the s j s and b k s. We use greedy decoders, and train the models with dynamic oracles (Goldberg and Nivre, 2013). Table 1 reports the parsing accuracy that results for feature sets of size four, three, two, and one for three commonly-used transition systems. The data is the development section of the English Penn Treebank (PTB), and experimental settings are as described in our other experimental section, §6. We see that we can go down to three or, in the arc-hybrid and arc-eager transition systems, even two positions with very little loss in performance, ÑÐ ÑÐ but not further. We therefore call t s 0 , b 0 u our minimal feature set with respect to arc-hybrid and arc-eager, and empirically confirm that"
D17-1002,D16-1257,0,0.0242681,"for each head-modifier pair; a maximum spanning tree algorithm is used to find the tree with the highest sum of edge scores. For this model, we use Dozat and Man7 Comparison with State-of-the-Art Models Figure 2 compares our algorithms’ results with those of the state-of-the-art.9 Our models are competitive and an ensemble of 15 globallytrained models (5 models each for arc-eager DP, arc-hybrid DP and edge-factored) achieves 95.33 and 90.22 on PTB and CTB, respectively, reach8 The same architecture and model size as other transitionbased global models is used for fair comparison. 9 We exclude Choe and Charniak (2016), Kuncoro et al. (2017) and Liu and Zhang (2017), which convert constituentbased parses to dependency parses. They produce higher PTB UAS, but access more training information and do not directly apply to datasets without constituency annotation. See https://github.com/tzshi/dp-parser-emnlp17 . 19 ing the highest reported UAS on the CTB dataset, and the second highest reported on the PTB dataset among dependency-based approaches. 7 2011) have a run-time dependence on the number of positional features, using our mere two effective positional features results in a running time of Opn3 q, feasibl"
D17-1002,P08-1110,0,0.453412,"Missing"
D17-1002,J11-3004,0,0.120993,"Missing"
D17-1002,P16-2006,1,0.921336,") and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6 q (originally reported as Opn7 q) for a length-n sentence. As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well. Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).1 Inspired by their work, we arrive at a minimal feature set for arc-hybrid and arc-eager: it contains only two positional bi-LSTM vectors, suffers almost no loss in performance in comparison to l"
D17-1002,D16-1001,1,0.746358,") and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6 q (originally reported as Opn7 q) for a length-n sentence. As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well. Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).1 Inspired by their work, we arrive at a minimal feature set for arc-hybrid and arc-eager: it contains only two positional bi-LSTM vectors, suffers almost no loss in performance in comparison to l"
D17-1002,N03-1014,0,0.0345858,"ments to other transition systems. It would also be interesting to further investigate relationships between graph-based and dependency-based parsing. In §5 we have mentioned important earlier work in this regard, and provided an update to those formal findings. In our work, we have brought exact decoding, which was formerly the province solely of graphbased parsing, to the transition-based paradigm. We hope that the future will bring more inspiration from an integration of the two perspectives. Neural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers (Henderson, 2003, 2004; Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power. Recently, Stern et al. (2017) have proposed minimal span-based features for constituency parsing. Recurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree (Le and Zuidema, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016b), but these models cannot be readily combined with DP approaches, because their state spaces cannot be merged into smaller sets and"
D17-1002,P04-1013,0,0.0284527,"Missing"
D17-1002,P10-1110,1,0.662064,"tains only two positional bi-LSTM vectors, suffers almost no loss in performance in comparison to larger sets, and out-performs a single position. (Details regarding the situation with arc-standard can be found in §2.) Our minimal feature set plugs into Huang and Sagae’s and Kuhlmann et al.’s dynamic programWe first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae (2010) and Kuhlmann et al. (2011) to produce the first implementation of worst-case Opn3 q exact decoders for arc-hybrid and arceager transition systems. With our minimal features, we also present Opn3 q global training methods. Finally, using ensembles including our new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the “second-best-in-class” result on the English Penn Treebank. 1 Introduction It used to be the case that the most accurate dependency parsers made global decisions and employed exact decoding. But transition-based depend"
D17-1002,D16-1262,0,0.0437026,"Missing"
D17-1002,J93-2004,0,0.0606409,"Missing"
D17-1002,Q16-1023,0,0.23437,"n introduced by Huang and Sagae (2010) and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6 q (originally reported as Opn7 q) for a length-n sentence. As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well. Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).1 Inspired by their work, we arrive at a minimal feature set for arc-hybrid and arc-eager: it contains only two positional bi-LSTM vectors, suffers almost no loss in pe"
D17-1002,E06-1011,0,0.0166149,"ature vectors is passed through a multi-layer perceptron (MLP) with 1 hidden layer which has 256 hidden units and activation function tanh. We set the dropout rate for the bi-LSTM (Gal and Ghahramani, 2016) and MLP (Srivastava et al., 2014) for each model according to development-set performance.6 All parameters except the word embedThe extra expressiveness of the arc-eager model comes from the scoring functions fsh and fre that capture structural contexts other than headmodifier relations. Unlike traditional higher-order graph-based parsing that directly models relations such as siblinghood (McDonald and Pereira, 2006) or grandparenthood (Carreras, 2007), however, the arguments in those two functions do not have any fixed type of structural interactions. 6 Experiments Data and Evaluation We experimented with English and Chinese. For English, we used the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (via the Stanford parser 3.3.0) of the Penn Treebank (Marcus et al., 1993, PTB). As is standard, we used §2-21 of the Wall Street Journal for training, §22 for development, 6 For bi-LSTM input and recurrent connections, we consider dropout rates in t0., 0.2u, and for MLP, t0., 0.4u. 18 90.5 ?15"
D17-1002,Q16-1032,0,0.129429,"n introduced by Huang and Sagae (2010) and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time. For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6 q (originally reported as Opn7 q) for a length-n sentence. As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming. Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well. Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).1 Inspired by their work, we arrive at a minimal feature set for arc-hybrid and arc-eager: it contains only two positional bi-LSTM vectors, suffers almost no loss in pe"
D17-1002,P03-1054,0,0.070388,"Work Not Yet Mentioned • Combining exact decoding with global training — which is also enabled by our minimal feature set — with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank: these are, to our knowledge, the best and secondbest results to date on these data sets among “purely” dependency-based approaches. Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage conf"
D17-1002,P11-1068,0,0.0604016,"Missing"
D17-1002,E17-1117,0,0.0645445,"Missing"
D17-1002,W03-3017,0,0.0703598,"ning time is not sufficiently practical even in the simple-feature case, in the remainder of this paper we consider only the archybrid and arc-eager systems, not arc-standard. the collection of left modifiers before right modifiers via its b0 -modifier reð transition. This contrasts with arc-standard, where the attachment of left and right modifiers can be interleaved on the stack. shrpσ, b0 |β, Aqs “ pσ|b0 , β, Aq reñ rpσ|s1 |s0 , β, Aqs “ pσ|s1 , β, A Y tps1 , s0 quq reð rpσ|s0 , b0 |β, Aqs “ pσ, b0 |β, A Y tpb0 , s0 quq Arc-Eager In contrast to the former two systems, the arc-eager system (Nivre, 2003) makes attachments as early as possible — even if a modifier has not yet received all of its own modifiers. This behavior is accomplished by decomposing the right-reduce transition into two independent transitions, one making the attachment (ra) and one reducing the right-attached child (re). shrpσ, b0 |β, Aqs “ pσ|b0 , β, Aq reð rpσ|s0 , b0 |β, Aqs “ pσ, b0 |β, A Y tpb0 , s0 quq 4 Practical Optimal Algorithms Enabled By Our Minimal Feature Set (precondition: s0 not attached to any word) rarpσ|s0 , b0 |β, Aqs “ pσ|s0 |b0 , β, A Y tps0 , b0 quq Until now, no one had suggested a set of positiona"
D17-1002,W04-0308,0,0.156308,"erent parts of the buffer or stack; our convention is to depict the buffer first element leftmost, and to depict the stack first element rightmost). All terminal configurations have an empty buffer and a stack containing only ROOT. Dynamic Programming for TBDPs As stated in the introduction, our minimal feature set from §2 plugs into Huang and Sagae and Kuhlmann et al.’s dynamic programming (DP) framework. To help explain the connection, this section provides an overview of the DP framework. We draw heavily from the presentation of Kuhlmann et al. (2011). Arc-Standard The arc-standard system (Nivre, 2004) is motivated by bottom-up parsing: each dependent has to be complete before being attached. The three transitions, shift (sh, move a token from the buffer to the stack), right-reduce (reñ , reduce and attach a right modifier), and left-reduce (reð , reduce and attach a left modifier), are defined as: 3.1 Three Transition Systems shrpσ, b0 |β, Aqs “ pσ|b0 , β, Aq reñ rpσ|s1 |s0 , β, Aqs “ pσ|s1 , β, A Y tps1 , s0 quq Transition-based parsing (Nivre, 2008; K¨ubler et al., 2009) is an incremental parsing framework based on transitions between parser configurareð rpσ|s1 |s0 , β, Aqs “ pσ|s0 , β,"
D17-1002,D16-1180,0,0.0204462,"Missing"
D17-1002,J08-4003,0,0.0229758,"ides an overview of the DP framework. We draw heavily from the presentation of Kuhlmann et al. (2011). Arc-Standard The arc-standard system (Nivre, 2004) is motivated by bottom-up parsing: each dependent has to be complete before being attached. The three transitions, shift (sh, move a token from the buffer to the stack), right-reduce (reñ , reduce and attach a right modifier), and left-reduce (reð , reduce and attach a left modifier), are defined as: 3.1 Three Transition Systems shrpσ, b0 |β, Aqs “ pσ|b0 , β, Aq reñ rpσ|s1 |s0 , β, Aqs “ pσ|s1 , β, A Y tps1 , s0 quq Transition-based parsing (Nivre, 2008; K¨ubler et al., 2009) is an incremental parsing framework based on transitions between parser configurareð rpσ|s1 |s0 , β, Aqs “ pσ|s0 , β, A Y tps0 , s1 quq Arc-Hybrid The arc-hybrid system (Yamada and Matsumoto, 2003; G´omez-Rodr´ıguez et al., 2008; Kuhlmann et al., 2011) has the same definitions of sh and reñ as arc-standard, but forces 3 We tentatively conjecture that the following might explain the observed phenomena, but stress that we don’t currently see a concrete way to test the following hypothesis. ÑÐ ÑÐ With t s 0 , b 0 u, in the arc-standard case, situations can arise where ther"
D17-1002,D14-1081,0,0.0118321,"the future will bring more inspiration from an integration of the two perspectives. Neural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers (Henderson, 2003, 2004; Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power. Recently, Stern et al. (2017) have proposed minimal span-based features for constituency parsing. Recurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree (Le and Zuidema, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016b), but these models cannot be readily combined with DP approaches, because their state spaces cannot be merged into smaller sets and thus remain exponentially large. 8 Acknowledgments: an author-reviewer success story We sincerely thank all the reviewers for their extraordinarily careful and helpful comments. Indeed, this paper originated as a short paper submission by TS&LL to ACL 2017, where an anonymous reviewer explained in the review comments how, among other things, the DP runtime could be improved from Opn4 q to Opn3 q. In their author"
D17-1002,D14-1162,0,0.0970433,"oof Sketch. We leverage the fact that the arceager model divides the sh transition in the archybrid model into two separate transitions, sh and ra. When we constrain the parameters fsh “ fra in the arc-eager model, the model hypothesis space becomes exactly the same as arc-hybrid’s. Implementation Details Our model structures reproduce those of Kiperwasser and Goldberg (2016a). We use 2-layer bi-directional LSTMs with 256 hidden cell units. Inputs are concatenations of 28-dimensional randomly-initialized partof-speech embeddings and 100-dimensional word vectors initialized from GloVe vectors (Pennington et al., 2014) (English) and pre-trained skipgram-model vectors (Mikolov et al., 2013) (Chinese). The concatenation of the bi-LSTM feature vectors is passed through a multi-layer perceptron (MLP) with 1 hidden layer which has 256 hidden units and activation function tanh. We set the dropout rate for the bi-LSTM (Gal and Ghahramani, 2016) and MLP (Srivastava et al., 2014) for each model according to development-set performance.6 All parameters except the word embedThe extra expressiveness of the arc-eager model comes from the scoring functions fsh and fre that capture structural contexts other than headmodif"
D17-1002,P83-1021,0,0.424626,"pσ|s0 , b0 |β, Aqs “ pσ|s0 |b0 , β, A Y tps0 , b0 quq Until now, no one had suggested a set of positional features that was both information-rich enough for accurate parsing and small enough to obtain the Opn3 q running-time promised above. Fortunately, ÑÐ ÑÐ our bi-LSTM-based t s 0 , b 0 u feature set qualifies, and enables the fast optimal procedures described in this section. rerpσ|s0 , β, Aqs “ pσ, β, Aq (precondition: s0 has been attached to its head) 3.2 Deduction and Dynamic Programming Kuhlmann et al. (2011) reformulate the three transition systems just discussed as deduction systems (Pereira and Warren, 1983; Shieber et al., 1995), wherein transitions serve as inference rules; these are given as the lefthand sides of the first three subfigures in Figure 1. For a given w “ w1 , ..., wn , assertions take the form ri, j, ks (or, when applicable, a two-index shorthand to be discussed soon), meaning that there exists a sequence of transitions that, starting from a configuration wherein head ps0 q “ wi , results in an ending configuration wherein head ps0 q “ wj and head pb0 q “ wk . If we define w0 as ROOT and wn`1 as an endof-sentence marker, then the goal theorem can be stated as r0, 0, n ` 1s. For"
D17-1002,Q16-1014,0,0.0327473,"11), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating from the gold transition path during training to better simulate the test-time environment. There are many directions for further exploration. Two possibilities are to create even better training methods, and to find some way to extend our run-time improvements to other transition systems. It would also be interesting to further investigate relationships between graph-based and dependency-based parsing. In §5 we have mentioned important earlier work"
D17-1002,P06-2089,0,0.080573,"Combining exact decoding with global training — which is also enabled by our minimal feature set — with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank: these are, to our knowledge, the best and secondbest results to date on these data sets among “purely” dependency-based approaches. Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating fr"
D17-1002,P16-1218,0,0.024492,"Missing"
D17-1002,D07-1111,0,0.0247084,"ng with global training — which is also enabled by our minimal feature set — with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank: these are, to our knowledge, the best and secondbest results to date on these data sets among “purely” dependency-based approaches. Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating from the gold transition p"
D17-1002,P15-1032,0,0.0441803,"ur new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the “second-best-in-class” result on the English Penn Treebank. 1 Introduction It used to be the case that the most accurate dependency parsers made global decisions and employed exact decoding. But transition-based dependency parsers (TBDPs) have recently achieved state-of-the-art performance, despite the fact that for efficiency reasons, they are usually trained to make local, rather than global, decisions and the decoding process is done approximately, rather than exactly (Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016). The key efficiency issue for decoding is as follows. In order to make accurate (local) attachment decisions, historically, TBDPs have required a large set of features in order to access rich information about particular positions in the stack and buffer of the current parser configuration. But consulting many positions means that although polynomial-time exact-decoding algo1 We note that K&G were not focused on minimizing positions, although they explicitly noted the implications of doing so: “While not explored in this work, [fewer positions] results"
D17-1002,K17-3003,1,0.753254,", but forces 3 We tentatively conjecture that the following might explain the observed phenomena, but stress that we don’t currently see a concrete way to test the following hypothesis. ÑÐ ÑÐ With t s 0 , b 0 u, in the arc-standard case, situations can arise where there are multiple possible transitions with missing information. In contrast, in the arc-hybrid case, there is only one possible transition with missing information (namely, ÑÐ reñ , introduced in §3.1); perhaps s 1 is therefore not so crucial for arc-hybrid in practice? 4 For simplicity, we only present unlabeled parsing here. See Shi et al. (2017) for labeled-parsing results. 5 Other presentations place ROOT at the end of the buffer or omit it entirely (Ballesteros and Nivre, 2013). 14 assertion rib , js is an indicator variable for whether wi has been attached to its head (b “ 1) or not (b “ 0) after the transition sequence is applied. Kuhlmann et al. (2011) show that all three deduction systems can be directly “tabularized” and dynamic programming (DP) can be applied, such that, ignoring for the moment the issue of incorporating complex features (we return to this later), time and space needs are low-order polynomial. Specifically, a"
D17-1002,C02-1145,0,0.0500423,"s 0, b 0u Global t h , mu t s 0, b 0u t s 0, b 0u ÑÐ ÑÐ t s 0, b 0u t s 0, b 0u ÑÐ ÑÐ Table 2: Test set performance for different training regimes and feature sets. The models use the same decoders for testing and training. For each setting, the average and standard deviation across 5 runs with different random initializations are reported. Boldface: best (averaged) result per dataset/measure. ÑÐ ÑÐ ÑÐ ÑÐ ÑÐ ÑÐ and §23 for testing; POS tags were predicted using 10-way jackknifing with the Stanford max entropy tagger (Toutanova et al., 2003). For Chinese, we used the Penn Chinese Treebank 5.1 (Xue et al., 2002, CTB), with the same splits and head-finding rules for conversion to dependencies as Zhang and Clark (2008). We adopted the CTB’s goldstandard tokenization and POS tags. We report unlabeled attachment score (UAS) and sentencelevel unlabeled exact match (UEM). Following prior work, all punctuation is excluded from evaluation. For each model, we initialized the network parameters with 5 different random seeds and report performance average and standard deviation. fra (wk ,wi ) “ fG pwk , wi q and fre pwi , wj q “ 0. The parameterization we arrive at emulates exactly the scoring model of fG . We"
D17-1002,W03-3023,0,0.289532,"as to be complete before being attached. The three transitions, shift (sh, move a token from the buffer to the stack), right-reduce (reñ , reduce and attach a right modifier), and left-reduce (reð , reduce and attach a left modifier), are defined as: 3.1 Three Transition Systems shrpσ, b0 |β, Aqs “ pσ|b0 , β, Aq reñ rpσ|s1 |s0 , β, Aqs “ pσ|s1 , β, A Y tps1 , s0 quq Transition-based parsing (Nivre, 2008; K¨ubler et al., 2009) is an incremental parsing framework based on transitions between parser configurareð rpσ|s1 |s0 , β, Aqs “ pσ|s0 , β, A Y tps0 , s1 quq Arc-Hybrid The arc-hybrid system (Yamada and Matsumoto, 2003; G´omez-Rodr´ıguez et al., 2008; Kuhlmann et al., 2011) has the same definitions of sh and reñ as arc-standard, but forces 3 We tentatively conjecture that the following might explain the observed phenomena, but stress that we don’t currently see a concrete way to test the following hypothesis. ÑÐ ÑÐ With t s 0 , b 0 u, in the arc-standard case, situations can arise where there are multiple possible transitions with missing information. In contrast, in the arc-hybrid case, there is only one possible transition with missing information (namely, ÑÐ reñ , introduced in §3.1); perhaps s 1 is ther"
D17-1002,D08-1059,0,0.0124844,"t performance for different training regimes and feature sets. The models use the same decoders for testing and training. For each setting, the average and standard deviation across 5 runs with different random initializations are reported. Boldface: best (averaged) result per dataset/measure. ÑÐ ÑÐ ÑÐ ÑÐ ÑÐ ÑÐ and §23 for testing; POS tags were predicted using 10-way jackknifing with the Stanford max entropy tagger (Toutanova et al., 2003). For Chinese, we used the Penn Chinese Treebank 5.1 (Xue et al., 2002, CTB), with the same splits and head-finding rules for conversion to dependencies as Zhang and Clark (2008). We adopted the CTB’s goldstandard tokenization and POS tags. We report unlabeled attachment score (UAS) and sentencelevel unlabeled exact match (UEM). Following prior work, all punctuation is excluded from evaluation. For each model, we initialized the network parameters with 5 different random seeds and report performance average and standard deviation. fra (wk ,wi ) “ fG pwk , wi q and fre pwi , wj q “ 0. The parameterization we arrive at emulates exactly the scoring model of fG . We further claim that the arc-eager model is more expressive than not only the edge-factored GBDP, but also th"
D17-1002,P17-1076,0,0.0450955,"ed important earlier work in this regard, and provided an update to those formal findings. In our work, we have brought exact decoding, which was formerly the province solely of graphbased parsing, to the transition-based paradigm. We hope that the future will bring more inspiration from an integration of the two perspectives. Neural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers (Henderson, 2003, 2004; Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power. Recently, Stern et al. (2017) have proposed minimal span-based features for constituency parsing. Recurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree (Le and Zuidema, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016b), but these models cannot be readily combined with DP approaches, because their state spaces cannot be merged into smaller sets and thus remain exponentially large. 8 Acknowledgments: an author-reviewer success story We sincerely thank all the reviewers for their extraordinarily careful and helpful comments"
D17-1002,J11-1005,0,0.0162033,"ches. Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating from the gold transition path during training to better simulate the test-time environment. There are many directions for further exploration. Two possibilities are to create even better training methods, and to find some way to extend our run-time improvements to other transition systems. It would also be interesting to further inves"
D17-1002,D13-1071,1,0.883547,"— which is also enabled by our minimal feature set — with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank: these are, to our knowledge, the best and secondbest results to date on these data sets among “purely” dependency-based approaches. Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding. Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework. Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016). Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating from the gold transition path during training"
D17-1002,P15-1148,0,0.0288784,"Missing"
D17-1002,N03-1033,0,0.0951612,"0.52 Local Local Global t s 2, s 1, s 0, b 0u Local Local Global t s 2, s 1, s 0, b 0u Global t h , mu t s 0, b 0u t s 0, b 0u ÑÐ ÑÐ t s 0, b 0u t s 0, b 0u ÑÐ ÑÐ Table 2: Test set performance for different training regimes and feature sets. The models use the same decoders for testing and training. For each setting, the average and standard deviation across 5 runs with different random initializations are reported. Boldface: best (averaged) result per dataset/measure. ÑÐ ÑÐ ÑÐ ÑÐ ÑÐ ÑÐ and §23 for testing; POS tags were predicted using 10-way jackknifing with the Stanford max entropy tagger (Toutanova et al., 2003). For Chinese, we used the Penn Chinese Treebank 5.1 (Xue et al., 2002, CTB), with the same splits and head-finding rules for conversion to dependencies as Zhang and Clark (2008). We adopted the CTB’s goldstandard tokenization and POS tags. We report unlabeled attachment score (UAS) and sentencelevel unlabeled exact match (UEM). Following prior work, all punctuation is excluded from evaluation. For each model, we initialized the network parameters with 5 different random seeds and report performance average and standard deviation. fra (wk ,wi ) “ fG pwk , wi q and fre pwi , wj q “ 0. The param"
D18-1159,P96-1023,0,0.151865,"Missing"
D18-1159,P13-2107,0,0.0306855,".97 / 82.90 92.18 / 92.61 / 92.39 92.78 / 92.63 / 92.70 77.99 / 78.22 / 78.08 79.54 / 78.11 / 78.78 Table 5: Experimental results involving analyzing PPs as valency patterns. 9 Further Related Work Supertagging Supertagging (Bangalore and Joshi, 2010) has been proposed for and used in parsing TAG (Bangalore and Joshi, 1999; Nasr and Rambow, 2004), CCG (Curran and Clark, 2003; Curran et al., 2006), and HPSG (Ninomiya et al., 2006; Blunsom and Baldwin, 2006). Within dependency parsing, supertags have also been explored in the literature, but prior work mostly treats them as additional features. Ambati et al. (2013, 2014) use CCG supertags to improve dependency parsing results, while Ouchi et al. (2014, 2016) leverage dependency-based supertags as features. Fale´nska et al. (2015) compare supertagging to parser stacking, where they extract supertags from base parsers to provide additional features for stacked parsers, instead of having a supertagger as a separate component. Constrained Dependency Grammar Another line of research (Wang and Harper, 2004; Foth et al., 2006; Foth and Menzel, 2006; Bharati et al., 2002, 2009; Husain et al., 2011) utilizes supertags in dependency parsing within the framework"
D18-1159,E14-4031,0,0.0592255,"Missing"
D18-1159,P11-1158,0,0.0117946,"the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 1998; Sagae and Lavie, 2006), and then to dependency parsing (Sagae and Tsujii, 2007; Zhao et al., 2013; Vaswani and Sagae, 2016). Our probability factorization permits a simple yet effective A* heuristic. A* parsing was introduced for parsing PCFGs (Klein and Manning, 2003; Pauls and Klein, 2009), and has been widely used for grammar formalisms and parsers with large search spaces, for example CCG (Auli and Lopez, 2011) and TAG (Waszczuk et al., 2016, 2017). Our decoder is similar to the supertag and dependency factored A* CCG parser (Yoshikawa et al., 2017), which in turn builds upon the work of Lewis and Steedman (2014) and Lewis et al. (2016). Our model additionally adds syntactic relations into the probability factorizations. 10 Conclusions We have presented a probability factorization and decoding process that integrates valency patterns into the parsing process. The joint decoder favors syntactic analyses with higher valency-pattern supertagging probabilities. Experiments on a large set of languages fr"
D18-1159,D15-1041,0,0.0156543,"train a tagger jointly with our main parser components (Zhang and Weiss, 2016). 5.5 Feature Extraction We adopt bi-directional long short-term memory networks (bi-LSTMs; Hochreiter and Schmidhuber, 1997) as our feature extractors, since they have proven successful in a variety of syntactic parsing tasks (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016; Stern et al., 2017; Shi et al., 2017a). As inputs to the bi-LSTMs, we concatenate one pre-trained word embedding, one randomly-initialized word embedding, and the output of character-level LSTMs for capturing sub-token level information (Ballesteros et al., 2015). The bi-LSTM output vectors at each timestep are then assigned to each token as its contextualized representation wi . 6 Experiments Data and Evaluation Our main experiments are based on UD version 2.0, which was prepared for the CoNLL 2017 shared task (Zeman et al., 2017). We used 53 of the treebanks9 across 41 languages that have train and development splits given for the shared task. In contrast to the shared-task setting, where word and sentence segmentation are to be performed by the system, we directly use the testset gold segmentations in order to focus directly on parsing; but this do"
D18-1159,N09-2047,0,0.0321362,"ers, can be treated as a dependency representation (Rambow and Joshi, 1997). We follow prior art and use Chen’s (2001) automatic conversion of the Penn Treebank (Marcus et al., 1993) into TAG derivation trees. The dataset annotation has labels 0, 1 and 2, corresponding to subject, direct object, and indirect object; we treat these as our core argument subset in valency analysis.10 Additionally, we also analyze CO (co-head for phrasal verbs) as a separate singleton subset. We leave out adj (adjuncts) in defining our valency patterns. We strictly follow the experiment protocol of previous work (Bangalore et al., 2009; Chung et al., 2016; Friedman et al., 2017; Kasai et al., 2017, 2018), and report the results in Table 4. The findings are consistent with our main experiments: MTL helps parsing performance, and joint decoding further improves on core argument F1 scores, reaching a new state-of-the-art result of 92.59 LAS. The precision recall trade-off is pronounced for the CO relation subset. 8 Case Study on PP Attachment Although valency information has traditionally been used to analyze complements or core arguments,11 in this section, we show the utility of our approach in analyzing other types of synta"
D18-1159,J99-2004,0,0.0796665,"59 / 77.68 / 79.04 Core + PP MTL + Joint Decoding 87.70 83.77 87.80 83.91 81.62 / 81.81 / 81.71 84.18 / 81.97 / 83.05 91.93 / 92.52 / 92.22 91.68 / 92.65 / 92.16 77.93 / 78.25 / 78.08 79.71 / 78.03 / 78.83 Core + Func. + PP MTL + Joint Decoding 87.67 83.75 87.81 83.94 81.35 / 81.68 / 81.50 83.88 / 81.97 / 82.90 92.18 / 92.61 / 92.39 92.78 / 92.63 / 92.70 77.99 / 78.22 / 78.08 79.54 / 78.11 / 78.78 Table 5: Experimental results involving analyzing PPs as valency patterns. 9 Further Related Work Supertagging Supertagging (Bangalore and Joshi, 2010) has been proposed for and used in parsing TAG (Bangalore and Joshi, 1999; Nasr and Rambow, 2004), CCG (Curran and Clark, 2003; Curran et al., 2006), and HPSG (Ninomiya et al., 2006; Blunsom and Baldwin, 2006). Within dependency parsing, supertags have also been explored in the literature, but prior work mostly treats them as additional features. Ambati et al. (2013, 2014) use CCG supertags to improve dependency parsing results, while Ouchi et al. (2014, 2016) leverage dependency-based supertags as features. Fale´nska et al. (2015) compare supertagging to parser stacking, where they extract supertags from base parsers to provide additional features for stacked pars"
D18-1159,W09-3812,0,0.0827249,"Missing"
D18-1159,W06-1620,0,0.0439173,"2.22 91.68 / 92.65 / 92.16 77.93 / 78.25 / 78.08 79.71 / 78.03 / 78.83 Core + Func. + PP MTL + Joint Decoding 87.67 83.75 87.81 83.94 81.35 / 81.68 / 81.50 83.88 / 81.97 / 82.90 92.18 / 92.61 / 92.39 92.78 / 92.63 / 92.70 77.99 / 78.22 / 78.08 79.54 / 78.11 / 78.78 Table 5: Experimental results involving analyzing PPs as valency patterns. 9 Further Related Work Supertagging Supertagging (Bangalore and Joshi, 2010) has been proposed for and used in parsing TAG (Bangalore and Joshi, 1999; Nasr and Rambow, 2004), CCG (Curran and Clark, 2003; Curran et al., 2006), and HPSG (Ninomiya et al., 2006; Blunsom and Baldwin, 2006). Within dependency parsing, supertags have also been explored in the literature, but prior work mostly treats them as additional features. Ambati et al. (2013, 2014) use CCG supertags to improve dependency parsing results, while Ouchi et al. (2014, 2016) leverage dependency-based supertags as features. Fale´nska et al. (2015) compare supertagging to parser stacking, where they extract supertags from base parsers to provide additional features for stacked parsers, instead of having a supertagger as a separate component. Constrained Dependency Grammar Another line of research (Wang and Harper,"
D18-1159,C94-2195,0,0.130328,"gs are consistent with our main experiments: MTL helps parsing performance, and joint decoding further improves on core argument F1 scores, reaching a new state-of-the-art result of 92.59 LAS. The precision recall trade-off is pronounced for the CO relation subset. 8 Case Study on PP Attachment Although valency information has traditionally been used to analyze complements or core arguments,11 in this section, we show the utility of our approach in analyzing other types of syntactic relations. We choose the long-standing problem of prepositional phrase (PP) attachment (Hindle and Rooth, 1993; Brill and Resnik, 1994; Collins and Brooks, 1995; de Kok et al., 2017), which is known to be a major source of parsing mistakes (Kummerfeld et al., 2012; Ng and Curran, 2015). In UD analysis, PPs usually have the labels obl or nmod with respect to their syntactic parents, whereas adpositions are attached via a case relation, which is included in the functional relation subset. Thus, we add another relation subset, obl and nmod, to our valency analysis. Table 5 presents the results for different combinations of valency relation subsets. We find that PP-attachment decisions are generally harder to make, compared with"
D18-1159,J98-2004,0,0.00894452,"2008; Flanigan et al., 2014; Täckström et al., 2015; Peng et al., 2017; He et al., 2017) incorporates several types of constraints, including uniqueness and determinism constraints that require that certain labels appear as arguments for a particular predicate only once. They perform inference through integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 1998; Sagae and Lavie, 2006), and then to dependency parsing (Sagae and Tsujii, 2007; Zhao et al., 2013; Vaswani and Sagae, 2016). Our probability factorization permits a simple yet effective A* heuristic. A* parsing was introduced for parsing PCFGs (Klein and Manning, 2003; Pauls and Klein, 2009), and has been widely used for grammar formalisms and parsers with large search spaces, for example CCG (Auli and Lopez, 2011) and TAG (Waszczuk et al., 2016, 2017). Our decoder is similar to the supertag and dependency factored A* CCG parser (Yoshikawa et al., 2017), which in turn builds upon the work of"
D18-1159,K17-3005,0,0.0305678,"Missing"
D18-1159,W16-3309,0,0.0217028,"dependency representation (Rambow and Joshi, 1997). We follow prior art and use Chen’s (2001) automatic conversion of the Penn Treebank (Marcus et al., 1993) into TAG derivation trees. The dataset annotation has labels 0, 1 and 2, corresponding to subject, direct object, and indirect object; we treat these as our core argument subset in valency analysis.10 Additionally, we also analyze CO (co-head for phrasal verbs) as a separate singleton subset. We leave out adj (adjuncts) in defining our valency patterns. We strictly follow the experiment protocol of previous work (Bangalore et al., 2009; Chung et al., 2016; Friedman et al., 2017; Kasai et al., 2017, 2018), and report the results in Table 4. The findings are consistent with our main experiments: MTL helps parsing performance, and joint decoding further improves on core argument F1 scores, reaching a new state-of-the-art result of 92.59 LAS. The precision recall trade-off is pronounced for the CO relation subset. 8 Case Study on PP Attachment Although valency information has traditionally been used to analyze complements or core arguments,11 in this section, we show the utility of our approach in analyzing other types of syntactic relations. We c"
D18-1159,P97-1003,0,0.411585,"elations in Swedish with specificallydesigned features such as animacy and definiteness that are useful in argument realization. Jakubıˇcek and Kováˇr (2013) leverage external lexicons of verb valency frames for reranking. Mirroshandel et al. (2012, 2013) and Mirroshandel and Nasr (2016) extract selectional constraints and subcategorization frames from large unannotated corpora, and enforce them through forest reranking. Our approach does not rely on external resources or lexicons, but directly extracts valency patterns from labeled dependency parse trees. Earlier works in this spirit include Collins (1997). Semantic Dependency Parsing and Semantic Role Labeling The notion of valency is also used to describe predicate-argument structures that are adopted in semantic dependency parsing and semantic role labeling (Surdeanu et al., 1284 2008; Hajiˇc et al., 2009; Oepen et al., 2014, 2015). While semantic frames clearly have patterns, previous work (Punyakanok et al., 2008; Flanigan et al., 2014; Täckström et al., 2015; Peng et al., 2017; He et al., 2017) incorporates several types of constraints, including uniqueness and determinism constraints that require that certain labels appear as arguments f"
D18-1159,W95-0103,0,0.114121,"ur main experiments: MTL helps parsing performance, and joint decoding further improves on core argument F1 scores, reaching a new state-of-the-art result of 92.59 LAS. The precision recall trade-off is pronounced for the CO relation subset. 8 Case Study on PP Attachment Although valency information has traditionally been used to analyze complements or core arguments,11 in this section, we show the utility of our approach in analyzing other types of syntactic relations. We choose the long-standing problem of prepositional phrase (PP) attachment (Hindle and Rooth, 1993; Brill and Resnik, 1994; Collins and Brooks, 1995; de Kok et al., 2017), which is known to be a major source of parsing mistakes (Kummerfeld et al., 2012; Ng and Curran, 2015). In UD analysis, PPs usually have the labels obl or nmod with respect to their syntactic parents, whereas adpositions are attached via a case relation, which is included in the functional relation subset. Thus, we add another relation subset, obl and nmod, to our valency analysis. Table 5 presents the results for different combinations of valency relation subsets. We find that PP-attachment decisions are generally harder to make, compared with core and functional relat"
D18-1159,P16-2006,0,0.0164068,"torization from §5.1. This can be thought of as an instance of multi-task learning (MTL; Caruana, 1997), which has been shown to be useful in parsing (Kasai et al., 2018). To further reduce error propagation, instead of using part-of-speech tags as features, we train a tagger jointly with our main parser components (Zhang and Weiss, 2016). 5.5 Feature Extraction We adopt bi-directional long short-term memory networks (bi-LSTMs; Hochreiter and Schmidhuber, 1997) as our feature extractors, since they have proven successful in a variety of syntactic parsing tasks (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016; Stern et al., 2017; Shi et al., 2017a). As inputs to the bi-LSTMs, we concatenate one pre-trained word embedding, one randomly-initialized word embedding, and the output of character-level LSTMs for capturing sub-token level information (Ballesteros et al., 2015). The bi-LSTM output vectors at each timestep are then assigned to each token as its contextualized representation wi . 6 Experiments Data and Evaluation Our main experiments are based on UD version 2.0, which was prepared for the CoNLL 2017 shared task (Zeman et al., 2017). We used 53 of the treebanks9 across 41 languages that have"
D18-1159,E03-1071,0,0.0285956,"83.77 87.80 83.91 81.62 / 81.81 / 81.71 84.18 / 81.97 / 83.05 91.93 / 92.52 / 92.22 91.68 / 92.65 / 92.16 77.93 / 78.25 / 78.08 79.71 / 78.03 / 78.83 Core + Func. + PP MTL + Joint Decoding 87.67 83.75 87.81 83.94 81.35 / 81.68 / 81.50 83.88 / 81.97 / 82.90 92.18 / 92.61 / 92.39 92.78 / 92.63 / 92.70 77.99 / 78.22 / 78.08 79.54 / 78.11 / 78.78 Table 5: Experimental results involving analyzing PPs as valency patterns. 9 Further Related Work Supertagging Supertagging (Bangalore and Joshi, 2010) has been proposed for and used in parsing TAG (Bangalore and Joshi, 1999; Nasr and Rambow, 2004), CCG (Curran and Clark, 2003; Curran et al., 2006), and HPSG (Ninomiya et al., 2006; Blunsom and Baldwin, 2006). Within dependency parsing, supertags have also been explored in the literature, but prior work mostly treats them as additional features. Ambati et al. (2013, 2014) use CCG supertags to improve dependency parsing results, while Ouchi et al. (2014, 2016) leverage dependency-based supertags as features. Fale´nska et al. (2015) compare supertagging to parser stacking, where they extract supertags from base parsers to provide additional features for stacked parsers, instead of having a supertagger as a separate co"
D18-1159,P06-1088,0,0.0359858,"/ 81.81 / 81.71 84.18 / 81.97 / 83.05 91.93 / 92.52 / 92.22 91.68 / 92.65 / 92.16 77.93 / 78.25 / 78.08 79.71 / 78.03 / 78.83 Core + Func. + PP MTL + Joint Decoding 87.67 83.75 87.81 83.94 81.35 / 81.68 / 81.50 83.88 / 81.97 / 82.90 92.18 / 92.61 / 92.39 92.78 / 92.63 / 92.70 77.99 / 78.22 / 78.08 79.54 / 78.11 / 78.78 Table 5: Experimental results involving analyzing PPs as valency patterns. 9 Further Related Work Supertagging Supertagging (Bangalore and Joshi, 2010) has been proposed for and used in parsing TAG (Bangalore and Joshi, 1999; Nasr and Rambow, 2004), CCG (Curran and Clark, 2003; Curran et al., 2006), and HPSG (Ninomiya et al., 2006; Blunsom and Baldwin, 2006). Within dependency parsing, supertags have also been explored in the literature, but prior work mostly treats them as additional features. Ambati et al. (2013, 2014) use CCG supertags to improve dependency parsing results, while Ouchi et al. (2014, 2016) leverage dependency-based supertags as features. Fale´nska et al. (2015) compare supertagging to parser stacking, where they extract supertags from base parsers to provide additional features for stacked parsers, instead of having a supertagger as a separate component. Constrained D"
D18-1159,K17-3002,0,0.0321092,"Missing"
D18-1159,P81-1022,0,0.366436,"Missing"
D18-1159,C96-1058,0,0.0359597,"Missing"
D18-1159,P99-1059,0,0.2155,"ability distribution over all potential syntactic heads in the sentence (Zhang et al., 2017). After we have selected the head of wi to be whi , we decide on the syntactic relation label based on another probability distribution. We use two softmax functions: P phi |wi q 9 exppscoreHEAD pwhi , wi qq, P pri |wi , hi q 9 exppscoreLABEL pwhi , wi qq, ri where both scoreHEAD and scoreLABEL are parameterized by deep biaffine scoring functions (Dozat and Manning, 2017). 5.3 Decoding For joint decoding, we adopt the Eisner’s (1996) algorithm annotated with valency patterns as the state information in Eisner and Satta (1999). The algorithm is depicted in Fig. 2. For each complete and incomplete span, visualized as triangles and trapezoids respectively, we annotate the head with its valency pattern. We adopt Earley’s (1970) notation of ‚ to outward-delimit the portion of a valency pattern, starting from the center word ˛, that has already been collected within the span. I NIT generates a minimal complete span with hypothesized valency pattern; the ‚ is put adjacent to ˛. C OMB matches an incomplete span to a complete span with compatible valency pattern, yielding a complete analysis on the relevant side of ˛. L IN"
D18-1159,W15-2215,0,0.0231687,"Missing"
D18-1159,P14-1134,0,0.033741,"enforce them through forest reranking. Our approach does not rely on external resources or lexicons, but directly extracts valency patterns from labeled dependency parse trees. Earlier works in this spirit include Collins (1997). Semantic Dependency Parsing and Semantic Role Labeling The notion of valency is also used to describe predicate-argument structures that are adopted in semantic dependency parsing and semantic role labeling (Surdeanu et al., 1284 2008; Hajiˇc et al., 2009; Oepen et al., 2014, 2015). While semantic frames clearly have patterns, previous work (Punyakanok et al., 2008; Flanigan et al., 2014; Täckström et al., 2015; Peng et al., 2017; He et al., 2017) incorporates several types of constraints, including uniqueness and determinism constraints that require that certain labels appear as arguments for a particular predicate only once. They perform inference through integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 199"
D18-1159,P06-1037,0,0.046926,"in dependency parsing, supertags have also been explored in the literature, but prior work mostly treats them as additional features. Ambati et al. (2013, 2014) use CCG supertags to improve dependency parsing results, while Ouchi et al. (2014, 2016) leverage dependency-based supertags as features. Fale´nska et al. (2015) compare supertagging to parser stacking, where they extract supertags from base parsers to provide additional features for stacked parsers, instead of having a supertagger as a separate component. Constrained Dependency Grammar Another line of research (Wang and Harper, 2004; Foth et al., 2006; Foth and Menzel, 2006; Bharati et al., 2002, 2009; Husain et al., 2011) utilizes supertags in dependency parsing within the framework of constraint dependency grammar (CDG; Maruyama, 1990; Heinecke et al., 1998). Constraints in CDG may be expressed in very general terms (and are usually hand-crafted for specific languages), so prior work in CDG involves a constraint solver that iteratively or greedily update hypotheses without optimality guarantees. In contrast, our work focuses on a special form of constraints — the valency patterns of syntactic dependents within a subset of relations — and"
D18-1159,P06-1041,0,0.0519766,"ng, supertags have also been explored in the literature, but prior work mostly treats them as additional features. Ambati et al. (2013, 2014) use CCG supertags to improve dependency parsing results, while Ouchi et al. (2014, 2016) leverage dependency-based supertags as features. Fale´nska et al. (2015) compare supertagging to parser stacking, where they extract supertags from base parsers to provide additional features for stacked parsers, instead of having a supertagger as a separate component. Constrained Dependency Grammar Another line of research (Wang and Harper, 2004; Foth et al., 2006; Foth and Menzel, 2006; Bharati et al., 2002, 2009; Husain et al., 2011) utilizes supertags in dependency parsing within the framework of constraint dependency grammar (CDG; Maruyama, 1990; Heinecke et al., 1998). Constraints in CDG may be expressed in very general terms (and are usually hand-crafted for specific languages), so prior work in CDG involves a constraint solver that iteratively or greedily update hypotheses without optimality guarantees. In contrast, our work focuses on a special form of constraints — the valency patterns of syntactic dependents within a subset of relations — and we provide an efficien"
D18-1159,W17-6213,0,0.01369,"tation (Rambow and Joshi, 1997). We follow prior art and use Chen’s (2001) automatic conversion of the Penn Treebank (Marcus et al., 1993) into TAG derivation trees. The dataset annotation has labels 0, 1 and 2, corresponding to subject, direct object, and indirect object; we treat these as our core argument subset in valency analysis.10 Additionally, we also analyze CO (co-head for phrasal verbs) as a separate singleton subset. We leave out adj (adjuncts) in defining our valency patterns. We strictly follow the experiment protocol of previous work (Bangalore et al., 2009; Chung et al., 2016; Friedman et al., 2017; Kasai et al., 2017, 2018), and report the results in Table 4. The findings are consistent with our main experiments: MTL helps parsing performance, and joint decoding further improves on core argument F1 scores, reaching a new state-of-the-art result of 92.59 LAS. The precision recall trade-off is pronounced for the CO relation subset. 8 Case Study on PP Attachment Although valency information has traditionally been used to analyze complements or core arguments,11 in this section, we show the utility of our approach in analyzing other types of syntactic relations. We choose the long-standing"
D18-1159,W09-1201,0,0.106785,"Missing"
D18-1159,P17-1044,0,0.0156287,"on external resources or lexicons, but directly extracts valency patterns from labeled dependency parse trees. Earlier works in this spirit include Collins (1997). Semantic Dependency Parsing and Semantic Role Labeling The notion of valency is also used to describe predicate-argument structures that are adopted in semantic dependency parsing and semantic role labeling (Surdeanu et al., 1284 2008; Hajiˇc et al., 2009; Oepen et al., 2014, 2015). While semantic frames clearly have patterns, previous work (Punyakanok et al., 2008; Flanigan et al., 2014; Täckström et al., 2015; Peng et al., 2017; He et al., 2017) incorporates several types of constraints, including uniqueness and determinism constraints that require that certain labels appear as arguments for a particular predicate only once. They perform inference through integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 1998; Sagae and Lavie, 2006), and then to dependency parsing (Sa"
D18-1159,P98-1086,0,0.158563,"results, while Ouchi et al. (2014, 2016) leverage dependency-based supertags as features. Fale´nska et al. (2015) compare supertagging to parser stacking, where they extract supertags from base parsers to provide additional features for stacked parsers, instead of having a supertagger as a separate component. Constrained Dependency Grammar Another line of research (Wang and Harper, 2004; Foth et al., 2006; Foth and Menzel, 2006; Bharati et al., 2002, 2009; Husain et al., 2011) utilizes supertags in dependency parsing within the framework of constraint dependency grammar (CDG; Maruyama, 1990; Heinecke et al., 1998). Constraints in CDG may be expressed in very general terms (and are usually hand-crafted for specific languages), so prior work in CDG involves a constraint solver that iteratively or greedily update hypotheses without optimality guarantees. In contrast, our work focuses on a special form of constraints — the valency patterns of syntactic dependents within a subset of relations — and we provide an efficient A*-based exact decoding algorithm. Valency in Parsing To the best of our knowledge, there have been few attempts to utilize lexical valency information or to improve specifically on core a"
D18-1159,J93-1005,0,0.416188,"s in Table 4. The findings are consistent with our main experiments: MTL helps parsing performance, and joint decoding further improves on core argument F1 scores, reaching a new state-of-the-art result of 92.59 LAS. The precision recall trade-off is pronounced for the CO relation subset. 8 Case Study on PP Attachment Although valency information has traditionally been used to analyze complements or core arguments,11 in this section, we show the utility of our approach in analyzing other types of syntactic relations. We choose the long-standing problem of prepositional phrase (PP) attachment (Hindle and Rooth, 1993; Brill and Resnik, 1994; Collins and Brooks, 1995; de Kok et al., 2017), which is known to be a major source of parsing mistakes (Kummerfeld et al., 2012; Ng and Curran, 2015). In UD analysis, PPs usually have the labels obl or nmod with respect to their syntactic parents, whereas adpositions are attached via a case relation, which is included in the functional relation subset. Thus, we add another relation subset, obl and nmod, to our valency analysis. Table 5 presents the results for different combinations of valency relation subsets. We find that PP-attachment decisions are generally harde"
D18-1159,P82-1020,0,0.779396,"Missing"
D18-1159,W11-3807,0,0.0292574,"ture, but prior work mostly treats them as additional features. Ambati et al. (2013, 2014) use CCG supertags to improve dependency parsing results, while Ouchi et al. (2014, 2016) leverage dependency-based supertags as features. Fale´nska et al. (2015) compare supertagging to parser stacking, where they extract supertags from base parsers to provide additional features for stacked parsers, instead of having a supertagger as a separate component. Constrained Dependency Grammar Another line of research (Wang and Harper, 2004; Foth et al., 2006; Foth and Menzel, 2006; Bharati et al., 2002, 2009; Husain et al., 2011) utilizes supertags in dependency parsing within the framework of constraint dependency grammar (CDG; Maruyama, 1990; Heinecke et al., 1998). Constraints in CDG may be expressed in very general terms (and are usually hand-crafted for specific languages), so prior work in CDG involves a constraint solver that iteratively or greedily update hypotheses without optimality guarantees. In contrast, our work focuses on a special form of constraints — the valency patterns of syntactic dependents within a subset of relations — and we provide an efficient A*-based exact decoding algorithm. Valency in Pa"
D18-1159,D17-1180,0,0.0163673,"i, 1997). We follow prior art and use Chen’s (2001) automatic conversion of the Penn Treebank (Marcus et al., 1993) into TAG derivation trees. The dataset annotation has labels 0, 1 and 2, corresponding to subject, direct object, and indirect object; we treat these as our core argument subset in valency analysis.10 Additionally, we also analyze CO (co-head for phrasal verbs) as a separate singleton subset. We leave out adj (adjuncts) in defining our valency patterns. We strictly follow the experiment protocol of previous work (Bangalore et al., 2009; Chung et al., 2016; Friedman et al., 2017; Kasai et al., 2017, 2018), and report the results in Table 4. The findings are consistent with our main experiments: MTL helps parsing performance, and joint decoding further improves on core argument F1 scores, reaching a new state-of-the-art result of 92.59 LAS. The precision recall trade-off is pronounced for the CO relation subset. 8 Case Study on PP Attachment Although valency information has traditionally been used to analyze complements or core arguments,11 in this section, we show the utility of our approach in analyzing other types of syntactic relations. We choose the long-standing problem of preposit"
D18-1159,N18-1107,0,0.178258,"arse then return p else if p R C then C.A DDppq // Extend the chart for p1 P C.RULESppq do A.I NSERTpp1 q every token outside the span the best possible valency pattern, best possible attachment and best relation label. 5.4 Training We train all components jointly and optimize for the cross entropy between our model prediction and the gold standard, or, equivalently, the sum of the log-probabilities for the three distributions comprising our factorization from §5.1. This can be thought of as an instance of multi-task learning (MTL; Caruana, 1997), which has been shown to be useful in parsing (Kasai et al., 2018). To further reduce error propagation, instead of using part-of-speech tags as features, we train a tagger jointly with our main parser components (Zhang and Weiss, 2016). 5.5 Feature Extraction We adopt bi-directional long short-term memory networks (bi-LSTMs; Hochreiter and Schmidhuber, 1997) as our feature extractors, since they have proven successful in a variety of syntactic parsing tasks (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016; Stern et al., 2017; Shi et al., 2017a). As inputs to the bi-LSTMs, we concatenate one pre-trained word embedding, one randomly-initialized word emb"
D18-1159,Q16-1023,0,0.0464007,"distributions comprising our factorization from §5.1. This can be thought of as an instance of multi-task learning (MTL; Caruana, 1997), which has been shown to be useful in parsing (Kasai et al., 2018). To further reduce error propagation, instead of using part-of-speech tags as features, we train a tagger jointly with our main parser components (Zhang and Weiss, 2016). 5.5 Feature Extraction We adopt bi-directional long short-term memory networks (bi-LSTMs; Hochreiter and Schmidhuber, 1997) as our feature extractors, since they have proven successful in a variety of syntactic parsing tasks (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016; Stern et al., 2017; Shi et al., 2017a). As inputs to the bi-LSTMs, we concatenate one pre-trained word embedding, one randomly-initialized word embedding, and the output of character-level LSTMs for capturing sub-token level information (Ballesteros et al., 2015). The bi-LSTM output vectors at each timestep are then assigned to each token as its contextualized representation wi . 6 Experiments Data and Evaluation Our main experiments are based on UD version 2.0, which was prepared for the CoNLL 2017 shared task (Zeman et al., 2017). We used 53 of the treebanks9 across"
D18-1159,N03-1016,0,0.0455962,"y perform inference through integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 1998; Sagae and Lavie, 2006), and then to dependency parsing (Sagae and Tsujii, 2007; Zhao et al., 2013; Vaswani and Sagae, 2016). Our probability factorization permits a simple yet effective A* heuristic. A* parsing was introduced for parsing PCFGs (Klein and Manning, 2003; Pauls and Klein, 2009), and has been widely used for grammar formalisms and parsers with large search spaces, for example CCG (Auli and Lopez, 2011) and TAG (Waszczuk et al., 2016, 2017). Our decoder is similar to the supertag and dependency factored A* CCG parser (Yoshikawa et al., 2017), which in turn builds upon the work of Lewis and Steedman (2014) and Lewis et al. (2016). Our model additionally adds syntactic relations into the probability factorizations. 10 Conclusions We have presented a probability factorization and decoding process that integrates valency patterns into the parsing p"
D18-1159,E17-2050,0,0.0517535,"Missing"
D18-1159,D12-1096,0,0.0118761,"F1 scores, reaching a new state-of-the-art result of 92.59 LAS. The precision recall trade-off is pronounced for the CO relation subset. 8 Case Study on PP Attachment Although valency information has traditionally been used to analyze complements or core arguments,11 in this section, we show the utility of our approach in analyzing other types of syntactic relations. We choose the long-standing problem of prepositional phrase (PP) attachment (Hindle and Rooth, 1993; Brill and Resnik, 1994; Collins and Brooks, 1995; de Kok et al., 2017), which is known to be a major source of parsing mistakes (Kummerfeld et al., 2012; Ng and Curran, 2015). In UD analysis, PPs usually have the labels obl or nmod with respect to their syntactic parents, whereas adpositions are attached via a case relation, which is included in the functional relation subset. Thus, we add another relation subset, obl and nmod, to our valency analysis. Table 5 presents the results for different combinations of valency relation subsets. We find that PP-attachment decisions are generally harder to make, compared with core and functional relations. Including them during training distracts other parsing objectives (compare Core + PP with only ana"
D18-1159,D14-1107,0,0.153137,"lysis. This algorithm can be easily extended to cases where we analyze multiple subsets of valency relations simultaneously: we just need to annotate each head with multiple layers of valency patterns, one for each subset.8 The time complexity of a naïve dynamic programming implementation is Op|V |2 |α|n3 q, where |V |is the number of valency patterns and |α |is the maximum length of a valency pattern. In practice, |V |is usually larger than n, making the algorithm prohibitively slow. We thus turn to A* parsing for a more practical solution. A* parsing We take inspiration from A* CCG parsing (Lewis and Steedman, 2014; Lewis et al., 2016; Yoshikawa et al., 2017). The idea (see Alg. 1) is to estimate the best compatible full parse for every chart item (in our case, complete and incomplete spans), and expand the chart based on the estimated priority scores. Our factorization of probability scores allows the following admissible heuristic: for each span, we can optimistically estimate its best full parse score by assigning to 8 To allow our model to account for unseen patterns in new data, we create a special wildcard valency pattern that allows dependents with arbitrary relations in the decoding process, and"
D18-1159,K17-3006,0,0.0367736,"Missing"
D18-1159,J93-2004,0,0.0606384,"treebanks with lower baseline performances, that is, on “harder” languages. 7 Parsing Tree Adjoining Grammar Dependency and valency relations also play an important role in formalisms other than dependency grammar. In this section, we apply our proposed valency analysis to Tree Adjoining Grammar (TAG; Joshi and Schabes, 1997), because TAG derivation trees, representing the process of inserting obligatory arguments and adjoining modifiers, can be treated as a dependency representation (Rambow and Joshi, 1997). We follow prior art and use Chen’s (2001) automatic conversion of the Penn Treebank (Marcus et al., 1993) into TAG derivation trees. The dataset annotation has labels 0, 1 and 2, corresponding to subject, direct object, and indirect object; we treat these as our core argument subset in valency analysis.10 Additionally, we also analyze CO (co-head for phrasal verbs) as a separate singleton subset. We leave out adj (adjuncts) in defining our valency patterns. We strictly follow the experiment protocol of previous work (Bangalore et al., 2009; Chung et al., 2016; Friedman et al., 2017; Kasai et al., 2017, 2018), and report the results in Table 4. The findings are consistent with our main experiments"
D18-1159,J16-1002,0,0.019326,"within a subset of relations — and we provide an efficient A*-based exact decoding algorithm. Valency in Parsing To the best of our knowledge, there have been few attempts to utilize lexical valency information or to improve specifically on core arguments in syntactic parsing apart from CDG. Øvrelid and Nivre (2007) target parsing core relations in Swedish with specificallydesigned features such as animacy and definiteness that are useful in argument realization. Jakubıˇcek and Kováˇr (2013) leverage external lexicons of verb valency frames for reranking. Mirroshandel et al. (2012, 2013) and Mirroshandel and Nasr (2016) extract selectional constraints and subcategorization frames from large unannotated corpora, and enforce them through forest reranking. Our approach does not rely on external resources or lexicons, but directly extracts valency patterns from labeled dependency parse trees. Earlier works in this spirit include Collins (1997). Semantic Dependency Parsing and Semantic Role Labeling The notion of valency is also used to describe predicate-argument structures that are adopted in semantic dependency parsing and semantic role labeling (Surdeanu et al., 1284 2008; Hajiˇc et al., 2009; Oepen et al., 2"
D18-1159,P12-1082,0,0.0436321,"Missing"
D18-1159,N13-1024,0,0.06618,"Missing"
D18-1159,W04-3308,0,0.0660255,"P MTL + Joint Decoding 87.70 83.77 87.80 83.91 81.62 / 81.81 / 81.71 84.18 / 81.97 / 83.05 91.93 / 92.52 / 92.22 91.68 / 92.65 / 92.16 77.93 / 78.25 / 78.08 79.71 / 78.03 / 78.83 Core + Func. + PP MTL + Joint Decoding 87.67 83.75 87.81 83.94 81.35 / 81.68 / 81.50 83.88 / 81.97 / 82.90 92.18 / 92.61 / 92.39 92.78 / 92.63 / 92.70 77.99 / 78.22 / 78.08 79.54 / 78.11 / 78.78 Table 5: Experimental results involving analyzing PPs as valency patterns. 9 Further Related Work Supertagging Supertagging (Bangalore and Joshi, 2010) has been proposed for and used in parsing TAG (Bangalore and Joshi, 1999; Nasr and Rambow, 2004), CCG (Curran and Clark, 2003; Curran et al., 2006), and HPSG (Ninomiya et al., 2006; Blunsom and Baldwin, 2006). Within dependency parsing, supertags have also been explored in the literature, but prior work mostly treats them as additional features. Ambati et al. (2013, 2014) use CCG supertags to improve dependency parsing results, while Ouchi et al. (2014, 2016) leverage dependency-based supertags as features. Fale´nska et al. (2015) compare supertagging to parser stacking, where they extract supertags from base parsers to provide additional features for stacked parsers, instead of having a"
D18-1159,P15-1111,0,0.0210702,"state-of-the-art result of 92.59 LAS. The precision recall trade-off is pronounced for the CO relation subset. 8 Case Study on PP Attachment Although valency information has traditionally been used to analyze complements or core arguments,11 in this section, we show the utility of our approach in analyzing other types of syntactic relations. We choose the long-standing problem of prepositional phrase (PP) attachment (Hindle and Rooth, 1993; Brill and Resnik, 1994; Collins and Brooks, 1995; de Kok et al., 2017), which is known to be a major source of parsing mistakes (Kummerfeld et al., 2012; Ng and Curran, 2015). In UD analysis, PPs usually have the labels obl or nmod with respect to their syntactic parents, whereas adpositions are attached via a case relation, which is included in the functional relation subset. Thus, we add another relation subset, obl and nmod, to our valency analysis. Table 5 presents the results for different combinations of valency relation subsets. We find that PP-attachment decisions are generally harder to make, compared with core and functional relations. Including them during training distracts other parsing objectives (compare Core + PP with only analyzing Core in §6). Ho"
D18-1159,W06-1619,0,0.0444177,"Missing"
D18-1159,S15-2153,0,0.231067,"Missing"
D18-1159,S14-2008,0,0.0202786,"and Nasr (2016) extract selectional constraints and subcategorization frames from large unannotated corpora, and enforce them through forest reranking. Our approach does not rely on external resources or lexicons, but directly extracts valency patterns from labeled dependency parse trees. Earlier works in this spirit include Collins (1997). Semantic Dependency Parsing and Semantic Role Labeling The notion of valency is also used to describe predicate-argument structures that are adopted in semantic dependency parsing and semantic role labeling (Surdeanu et al., 1284 2008; Hajiˇc et al., 2009; Oepen et al., 2014, 2015). While semantic frames clearly have patterns, previous work (Punyakanok et al., 2008; Flanigan et al., 2014; Täckström et al., 2015; Peng et al., 2017; He et al., 2017) incorporates several types of constraints, including uniqueness and determinism constraints that require that certain labels appear as arguments for a particular predicate only once. They perform inference through integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing ch"
D18-1159,E14-4030,0,0.0234609,"1 / 78.78 Table 5: Experimental results involving analyzing PPs as valency patterns. 9 Further Related Work Supertagging Supertagging (Bangalore and Joshi, 2010) has been proposed for and used in parsing TAG (Bangalore and Joshi, 1999; Nasr and Rambow, 2004), CCG (Curran and Clark, 2003; Curran et al., 2006), and HPSG (Ninomiya et al., 2006; Blunsom and Baldwin, 2006). Within dependency parsing, supertags have also been explored in the literature, but prior work mostly treats them as additional features. Ambati et al. (2013, 2014) use CCG supertags to improve dependency parsing results, while Ouchi et al. (2014, 2016) leverage dependency-based supertags as features. Fale´nska et al. (2015) compare supertagging to parser stacking, where they extract supertags from base parsers to provide additional features for stacked parsers, instead of having a supertagger as a separate component. Constrained Dependency Grammar Another line of research (Wang and Harper, 2004; Foth et al., 2006; Foth and Menzel, 2006; Bharati et al., 2002, 2009; Husain et al., 2011) utilizes supertags in dependency parsing within the framework of constraint dependency grammar (CDG; Maruyama, 1990; Heinecke et al., 1998). Constraint"
D18-1159,P09-1108,0,0.012917,"gh integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 1998; Sagae and Lavie, 2006), and then to dependency parsing (Sagae and Tsujii, 2007; Zhao et al., 2013; Vaswani and Sagae, 2016). Our probability factorization permits a simple yet effective A* heuristic. A* parsing was introduced for parsing PCFGs (Klein and Manning, 2003; Pauls and Klein, 2009), and has been widely used for grammar formalisms and parsers with large search spaces, for example CCG (Auli and Lopez, 2011) and TAG (Waszczuk et al., 2016, 2017). Our decoder is similar to the supertag and dependency factored A* CCG parser (Yoshikawa et al., 2017), which in turn builds upon the work of Lewis and Steedman (2014) and Lewis et al. (2016). Our model additionally adds syntactic relations into the probability factorizations. 10 Conclusions We have presented a probability factorization and decoding process that integrates valency patterns into the parsing process. The joint decode"
D18-1159,P17-1186,0,0.0836623,"Missing"
D18-1159,J08-2005,0,0.0180597,"unannotated corpora, and enforce them through forest reranking. Our approach does not rely on external resources or lexicons, but directly extracts valency patterns from labeled dependency parse trees. Earlier works in this spirit include Collins (1997). Semantic Dependency Parsing and Semantic Role Labeling The notion of valency is also used to describe predicate-argument structures that are adopted in semantic dependency parsing and semantic role labeling (Surdeanu et al., 1284 2008; Hajiˇc et al., 2009; Oepen et al., 2014, 2015). While semantic frames clearly have patterns, previous work (Punyakanok et al., 2008; Flanigan et al., 2014; Täckström et al., 2015; Peng et al., 2017; He et al., 2017) incorporates several types of constraints, including uniqueness and determinism constraints that require that certain labels appear as arguments for a particular predicate only once. They perform inference through integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Cara"
D18-1159,W97-0301,0,0.090433,"Punyakanok et al., 2008; Flanigan et al., 2014; Täckström et al., 2015; Peng et al., 2017; He et al., 2017) incorporates several types of constraints, including uniqueness and determinism constraints that require that certain labels appear as arguments for a particular predicate only once. They perform inference through integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 1998; Sagae and Lavie, 2006), and then to dependency parsing (Sagae and Tsujii, 2007; Zhao et al., 2013; Vaswani and Sagae, 2016). Our probability factorization permits a simple yet effective A* heuristic. A* parsing was introduced for parsing PCFGs (Klein and Manning, 2003; Pauls and Klein, 2009), and has been widely used for grammar formalisms and parsers with large search spaces, for example CCG (Auli and Lopez, 2011) and TAG (Waszczuk et al., 2016, 2017). Our decoder is similar to the supertag and dependency factored A* CCG parser (Yoshikawa et al., 2017), which i"
D18-1159,Q16-1010,0,0.0153611,"ere α indicates a sequence and the letters L and R distinguish left dependencies from right dependencies. We make our definition of valency patterns dependent on choice of R not only because some dependency relations are more often obligatory and closer to the original theoretical definition of valency, but also because the utility of different types of syntactic relations can depend on the downstream task. For example, purely functional dependency labels are semantically vacuous, so they are often omitted in the semantic representations extracted from dependency trees for question answering (Reddy et al., 2016, 2017). There are also recent proposals for parser evaluation that downplay the importance of functional syntactic relations (Nivre and Fang, 2017). dependents. We choose to encode linearity since it appears that most languages empirically exhibit word order preferences even if they allow for relatively free word order. 3 UD core and functional relations are listed in Table 1. 4 The (possibly counterintuitive) direction for that and to is a result of UD’s choice of a content-word-oriented design. 1278 4 Pilot Study: Sanity Checks We consider two questions that need to be addressed at the outs"
D18-1159,D17-1009,0,0.0385287,"Missing"
D18-1159,P06-2089,0,0.0147978,"äckström et al., 2015; Peng et al., 2017; He et al., 2017) incorporates several types of constraints, including uniqueness and determinism constraints that require that certain labels appear as arguments for a particular predicate only once. They perform inference through integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 1998; Sagae and Lavie, 2006), and then to dependency parsing (Sagae and Tsujii, 2007; Zhao et al., 2013; Vaswani and Sagae, 2016). Our probability factorization permits a simple yet effective A* heuristic. A* parsing was introduced for parsing PCFGs (Klein and Manning, 2003; Pauls and Klein, 2009), and has been widely used for grammar formalisms and parsers with large search spaces, for example CCG (Auli and Lopez, 2011) and TAG (Waszczuk et al., 2016, 2017). Our decoder is similar to the supertag and dependency factored A* CCG parser (Yoshikawa et al., 2017), which in turn builds upon the work of Lewis and Steedman (201"
D18-1159,D07-1111,0,0.00988415,"7) incorporates several types of constraints, including uniqueness and determinism constraints that require that certain labels appear as arguments for a particular predicate only once. They perform inference through integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 1998; Sagae and Lavie, 2006), and then to dependency parsing (Sagae and Tsujii, 2007; Zhao et al., 2013; Vaswani and Sagae, 2016). Our probability factorization permits a simple yet effective A* heuristic. A* parsing was introduced for parsing PCFGs (Klein and Manning, 2003; Pauls and Klein, 2009), and has been widely used for grammar formalisms and parsers with large search spaces, for example CCG (Auli and Lopez, 2011) and TAG (Waszczuk et al., 2016, 2017). Our decoder is similar to the supertag and dependency factored A* CCG parser (Yoshikawa et al., 2017), which in turn builds upon the work of Lewis and Steedman (2014) and Lewis et al. (2016). Our model additionally adds"
D18-1159,D17-1002,1,0.853727,"Missing"
D18-1159,K17-3003,1,0.896634,"old-standard annotation (lower half), and we highlight the core argument valency relations of the verb bothers in bold. The system incorrectly predicts how come to be a clausal subject. The first question checks the feasibility of learning valency patterns from a limited amount of data; the second probes the potential for any valencyinformed parsing approach to improve over current state-of-the-art systems. To answer these questions, we use the UD 2.0 dataset for the CoNLL 2017 shared task (Zeman et al., 2017) and the system outputs6 of the top five performing submissions (Dozat et al., 2017; Shi et al., 2017b; Björkelund et al., 2017; Che et al., 2017; Lim and Poibeau, 2017). Selection of treebanks is the same as in §6. We extract valency patterns relative to the set of 6 UD core arguments given in Table 1 because they are close to the original notion of valency and we hypothesize that these patterns should exhibit few variations. This is indeed the case: the average number of valency patterns we extract is 110.4 per training treebank, with Turkish (tr) having the fewest at 34, and Galician (gl) having the most at 298 patterns. We observe that in general, languages with higher degree of flexibili"
D18-1159,P17-1076,0,0.0164182,"his can be thought of as an instance of multi-task learning (MTL; Caruana, 1997), which has been shown to be useful in parsing (Kasai et al., 2018). To further reduce error propagation, instead of using part-of-speech tags as features, we train a tagger jointly with our main parser components (Zhang and Weiss, 2016). 5.5 Feature Extraction We adopt bi-directional long short-term memory networks (bi-LSTMs; Hochreiter and Schmidhuber, 1997) as our feature extractors, since they have proven successful in a variety of syntactic parsing tasks (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016; Stern et al., 2017; Shi et al., 2017a). As inputs to the bi-LSTMs, we concatenate one pre-trained word embedding, one randomly-initialized word embedding, and the output of character-level LSTMs for capturing sub-token level information (Ballesteros et al., 2015). The bi-LSTM output vectors at each timestep are then assigned to each token as its contextualized representation wi . 6 Experiments Data and Evaluation Our main experiments are based on UD version 2.0, which was prepared for the CoNLL 2017 shared task (Zeman et al., 2017). We used 53 of the treebanks9 across 41 languages that have train and developmen"
D18-1159,Q15-1003,0,0.0165117,"orest reranking. Our approach does not rely on external resources or lexicons, but directly extracts valency patterns from labeled dependency parse trees. Earlier works in this spirit include Collins (1997). Semantic Dependency Parsing and Semantic Role Labeling The notion of valency is also used to describe predicate-argument structures that are adopted in semantic dependency parsing and semantic role labeling (Surdeanu et al., 1284 2008; Hajiˇc et al., 2009; Oepen et al., 2014, 2015). While semantic frames clearly have patterns, previous work (Punyakanok et al., 2008; Flanigan et al., 2014; Täckström et al., 2015; Peng et al., 2017; He et al., 2017) incorporates several types of constraints, including uniqueness and determinism constraints that require that certain labels appear as arguments for a particular predicate only once. They perform inference through integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 1998; Sagae and Lavie, 2006"
D18-1159,Q16-1014,0,0.014393,"s, including uniqueness and determinism constraints that require that certain labels appear as arguments for a particular predicate only once. They perform inference through integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 1998; Sagae and Lavie, 2006), and then to dependency parsing (Sagae and Tsujii, 2007; Zhao et al., 2013; Vaswani and Sagae, 2016). Our probability factorization permits a simple yet effective A* heuristic. A* parsing was introduced for parsing PCFGs (Klein and Manning, 2003; Pauls and Klein, 2009), and has been widely used for grammar formalisms and parsers with large search spaces, for example CCG (Auli and Lopez, 2011) and TAG (Waszczuk et al., 2016, 2017). Our decoder is similar to the supertag and dependency factored A* CCG parser (Yoshikawa et al., 2017), which in turn builds upon the work of Lewis and Steedman (2014) and Lewis et al. (2016). Our model additionally adds syntactic relations into the probability fact"
D18-1159,W04-0307,0,0.064947,"nd Baldwin, 2006). Within dependency parsing, supertags have also been explored in the literature, but prior work mostly treats them as additional features. Ambati et al. (2013, 2014) use CCG supertags to improve dependency parsing results, while Ouchi et al. (2014, 2016) leverage dependency-based supertags as features. Fale´nska et al. (2015) compare supertagging to parser stacking, where they extract supertags from base parsers to provide additional features for stacked parsers, instead of having a supertagger as a separate component. Constrained Dependency Grammar Another line of research (Wang and Harper, 2004; Foth et al., 2006; Foth and Menzel, 2006; Bharati et al., 2002, 2009; Husain et al., 2011) utilizes supertags in dependency parsing within the framework of constraint dependency grammar (CDG; Maruyama, 1990; Heinecke et al., 1998). Constraints in CDG may be expressed in very general terms (and are usually hand-crafted for specific languages), so prior work in CDG involves a constraint solver that iteratively or greedily update hypotheses without optimality guarantees. In contrast, our work focuses on a special form of constraints — the valency patterns of syntactic dependents within a subset"
D18-1159,C16-1042,0,0.0152604,"first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 1998; Sagae and Lavie, 2006), and then to dependency parsing (Sagae and Tsujii, 2007; Zhao et al., 2013; Vaswani and Sagae, 2016). Our probability factorization permits a simple yet effective A* heuristic. A* parsing was introduced for parsing PCFGs (Klein and Manning, 2003; Pauls and Klein, 2009), and has been widely used for grammar formalisms and parsers with large search spaces, for example CCG (Auli and Lopez, 2011) and TAG (Waszczuk et al., 2016, 2017). Our decoder is similar to the supertag and dependency factored A* CCG parser (Yoshikawa et al., 2017), which in turn builds upon the work of Lewis and Steedman (2014) and Lewis et al. (2016). Our model additionally adds syntactic relations into the probability factorizations. 10 Conclusions We have presented a probability factorization and decoding process that integrates valency patterns into the parsing process. The joint decoder favors syntactic analyses with higher valency-pattern supertagging probabilities. Experiments on a large set of languages from UD show that our parsers are"
D18-1159,E17-1063,0,0.0137497,"ˆR αL ˛ α1R ‚ aα2R α ˆL ˛ α ˆR‚ R-C OMB: L ‚α ˆL ˛ α ˆR r hÝ Ñ j, r R R ‚α ˆL ˛ α ˆR j Figure 2: Eisner’s (1996)/Eisner and Satta’s (1999) algorithm, with valency-pattern annotations, incorporated as state information, shown explicitly. We show only the R-rules; the L-rules are symmetric. 5.2 Parameterization We parameterize P pvi |wi q as a softmax distribution over all candidate valency patterns: P pvi |wi q 9 exppscoreVAL vi pwi qq, where scoreVAL is a multi-layer perceptron (MLP). For each word wi , we generate a probability distribution over all potential syntactic heads in the sentence (Zhang et al., 2017). After we have selected the head of wi to be whi , we decide on the syntactic relation label based on another probability distribution. We use two softmax functions: P phi |wi q 9 exppscoreHEAD pwhi , wi qq, P pri |wi , hi q 9 exppscoreLABEL pwhi , wi qq, ri where both scoreHEAD and scoreLABEL are parameterized by deep biaffine scoring functions (Dozat and Manning, 2017). 5.3 Decoding For joint decoding, we adopt the Eisner’s (1996) algorithm annotated with valency patterns as the state information in Eisner and Satta (1999). The algorithm is depicted in Fig. 2. For each complete and incomple"
D18-1159,P16-1147,0,0.0454399,"Missing"
D18-1159,D13-1071,0,0.0147106,"types of constraints, including uniqueness and determinism constraints that require that certain labels appear as arguments for a particular predicate only once. They perform inference through integer linear programming, which is usually solved approximately, and cannot easily encode linear ordering constraints for the arguments. A* parsing Best-first search uses a heuristic to expand the parsing chart instead of doing so exhaustively. It was first applied to PCFGs (Ratnaparkhi, 1997; Caraballo and Charniak, 1998; Sagae and Lavie, 2006), and then to dependency parsing (Sagae and Tsujii, 2007; Zhao et al., 2013; Vaswani and Sagae, 2016). Our probability factorization permits a simple yet effective A* heuristic. A* parsing was introduced for parsing PCFGs (Klein and Manning, 2003; Pauls and Klein, 2009), and has been widely used for grammar formalisms and parsers with large search spaces, for example CCG (Auli and Lopez, 2011) and TAG (Waszczuk et al., 2016, 2017). Our decoder is similar to the supertag and dependency factored A* CCG parser (Yoshikawa et al., 2017), which in turn builds upon the work of Lewis and Steedman (2014) and Lewis et al. (2016). Our model additionally adds syntactic relations"
D18-1159,W17-6209,0,0.0255981,"Missing"
D18-1159,P17-1026,0,0.0597289,"o cases where we analyze multiple subsets of valency relations simultaneously: we just need to annotate each head with multiple layers of valency patterns, one for each subset.8 The time complexity of a naïve dynamic programming implementation is Op|V |2 |α|n3 q, where |V |is the number of valency patterns and |α |is the maximum length of a valency pattern. In practice, |V |is usually larger than n, making the algorithm prohibitively slow. We thus turn to A* parsing for a more practical solution. A* parsing We take inspiration from A* CCG parsing (Lewis and Steedman, 2014; Lewis et al., 2016; Yoshikawa et al., 2017). The idea (see Alg. 1) is to estimate the best compatible full parse for every chart item (in our case, complete and incomplete spans), and expand the chart based on the estimated priority scores. Our factorization of probability scores allows the following admissible heuristic: for each span, we can optimistically estimate its best full parse score by assigning to 8 To allow our model to account for unseen patterns in new data, we create a special wildcard valency pattern that allows dependents with arbitrary relations in the decoding process, and during training, treat valency patterns occu"
D19-1210,D16-1091,0,0.0917708,"ll et al., 2010, 2013; Kazemzadeh et al., 2014; Karpathy et al., 2014; Plummer et al., 2015; Hu et al., 2016b; Rohrbach et al., 2016; Nagaraja et al., 2016; Hu et al., 2016a; Yu et al., 2016; Peyre et al., 2017; Margffoy-Tuay et al., 2018). In general, a supervised approach is taken (Mao et al., 2016; Krishna et al., 2017; Johnson et al., 2017). Related tasks involving multi-image/multisentence data include: generating captions/stories for image streams or videos (Park and Kim, 2015; Huang et al., 2016; Shin et al., 2016; Liu et al., 2017), sorting aligned (image, caption) pairs into stories (Agrawal et al., 2016), image/textual cloze tasks (Iyyer et al., 2017; Yagcioglu et al., 2018), augmentation of Wikipedia articles with 3D models (Russell et al., 2013), question-answering (Kembhavi et al., 2017), and aligning books with their film adaptations (Zhu et al., 2015); these tasks are usually supervised, or rely on a search engine. 8 Conclusion and Future Directions We have demonstrated that a family of models for learning fine-grained image-sentence links within documents can produce good test-time results even if only given access to document-level cooccurrence at training time. Future work could incor"
D19-1210,N18-1199,1,0.876931,"Missing"
D19-1210,N16-1147,0,0.0308084,"the task of identifying objects in single images that are referred to by natural language descriptions (Mitchell et al., 2010, 2013; Kazemzadeh et al., 2014; Karpathy et al., 2014; Plummer et al., 2015; Hu et al., 2016b; Rohrbach et al., 2016; Nagaraja et al., 2016; Hu et al., 2016a; Yu et al., 2016; Peyre et al., 2017; Margffoy-Tuay et al., 2018). In general, a supervised approach is taken (Mao et al., 2016; Krishna et al., 2017; Johnson et al., 2017). Related tasks involving multi-image/multisentence data include: generating captions/stories for image streams or videos (Park and Kim, 2015; Huang et al., 2016; Shin et al., 2016; Liu et al., 2017), sorting aligned (image, caption) pairs into stories (Agrawal et al., 2016), image/textual cloze tasks (Iyyer et al., 2017; Yagcioglu et al., 2018), augmentation of Wikipedia articles with 3D models (Russell et al., 2013), question-answering (Kembhavi et al., 2017), and aligning books with their film adaptations (Zhu et al., 2015); these tasks are usually supervised, or rely on a search engine. 8 Conclusion and Future Directions We have demonstrated that a family of models for learning fine-grained image-sentence links within documents can produce good te"
D19-1210,D14-1086,0,0.0221518,"it labels (Karpathy et al., 2014; Karpathy and Fei-Fei, 2015; Jiang et al., 2015; Rohrbach et al., 2016; Datta et al., 2019); similar tasks have been addressed in supervised (Plummer et al., 2015) and semi-supervised (Rohrbach et al., 2016) settings. Our models operate at the larger granularity of entire images/sentences. Integer programs like AP have been used to align visual and textual content in videos, e.g., Bojanowski et al. (2015) Prior work has addressed the task of identifying objects in single images that are referred to by natural language descriptions (Mitchell et al., 2010, 2013; Kazemzadeh et al., 2014; Karpathy et al., 2014; Plummer et al., 2015; Hu et al., 2016b; Rohrbach et al., 2016; Nagaraja et al., 2016; Hu et al., 2016a; Yu et al., 2016; Peyre et al., 2017; Margffoy-Tuay et al., 2018). In general, a supervised approach is taken (Mao et al., 2016; Krishna et al., 2017; Johnson et al., 2017). Related tasks involving multi-image/multisentence data include: generating captions/stories for image streams or videos (Park and Kim, 2015; Huang et al., 2016; Shin et al., 2016; Liu et al., 2017), sorting aligned (image, caption) pairs into stories (Agrawal et al., 2016), image/textual cloze tas"
D19-1210,W10-4210,0,0.0332891,"Missing"
D19-1210,P15-1107,0,0.0780276,"Missing"
D19-1210,N16-1174,0,0.123099,"Missing"
D19-1210,D14-1179,0,\N,Missing
D19-1210,D18-1166,0,\N,Missing
J00-2011,P98-1035,0,0.0279568,"ographical errors, risks leaving the reader with neither the ability nor the confidence to develop EM formulations in his or her own work. Finally, if FSNLP had been organized around a set of theories, it could have been more focused. In part, this is because it could have been more selective in its choice of research paper summaries. Of the many recent publications covered, some are surely, sadly, not destined to make a substantive impact on the field. The book also occasionally exhibits excessive reluctance to extract principles. One example of this reticence is its treatment of the work of Chelba and Jelinek (1998); although the text hails this paper as ""the first clear demonstration of a probabilistic parser outperforming a trigram model"" (p. 457), it does not discuss what features of the algorithm lead to its superior results. Implicit in all of these comments is the belief that a mathematical foundation for statistical natural language processing can exist and will eventually develop. The authors, as cited above, maintain that this is not currently the case, and they might well be right. But in considering the contents of FSNLP, one senses that perhaps already 278 Book Reviews there is a thinner book"
J00-2011,W96-0208,0,0.122205,"Missing"
N03-1003,W99-0604,0,\N,Missing
N03-1003,C00-2172,0,\N,Missing
N03-1003,C02-1134,0,\N,Missing
N03-1003,C88-2088,0,\N,Missing
N03-1003,A94-1002,0,\N,Missing
N03-1003,P02-1040,0,\N,Missing
N03-1003,P01-1008,1,\N,Missing
N03-1003,N03-1024,0,\N,Missing
N03-1003,W02-1022,1,\N,Missing
N03-1003,P99-1044,0,\N,Missing
N03-1003,P79-1016,0,\N,Missing
N03-1003,P98-2127,0,\N,Missing
N03-1003,C98-2122,0,\N,Missing
N03-1003,P93-1024,1,\N,Missing
N04-1015,J98-3005,0,\N,Missing
N04-1015,W97-0304,0,\N,Missing
N04-1015,H01-1054,0,\N,Missing
N04-1015,A97-1004,0,\N,Missing
N04-1015,W03-1016,0,\N,Missing
N04-1015,W03-0418,0,\N,Missing
N04-1015,P94-1002,0,\N,Missing
N04-1015,N03-1003,1,\N,Missing
N04-1015,N03-1030,0,\N,Missing
N04-1015,P03-1069,0,\N,Missing
N04-1015,W90-0112,0,\N,Missing
N09-1016,W07-1401,0,0.0604948,"e know the epidemic spread quickly’ 2. ‘We doubt the epidemic spread quickly’ Introduction Making inferences based on natural-language statements is a crucial part of true natural-language understanding, and thus has many important applications. As the field of NLP has matured, there has been a resurgence of interest in creating systems capable of making such inferences, as evidenced by the activity surrounding the ongoing sequence of “Recognizing Textual Entailment” (RTE) competitions (Dagan, Glickman, and Magnini, 2006; BarHaim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, and Szpektor, 2006; Giampiccolo, Magnini, Dagan, and Dolan, 2007) and the AQUAINT knowledgebased evaluation project (Crouch, Saur´ı, and Fowler, 2005). 137 A relaxation of ‘spread quickly’ is ‘spread’; a restriction of it is ‘spread quickly via fleas’. From statement 1, we can infer the relaxed version, ‘We know the epidemic spread’, whereas the restricted version, ‘We know the epidemic spread quickly via fleas’, does not follow. But the reverse holds for statement 2: it entails the restricted version ‘We doubt the epidemic spread quickly via fleas’, but not the relaxed version. The reason is that ‘doubt’ is a downward-entailing operator;1 in other words,"
N09-1016,W07-1431,0,0.0210353,"ynonyms for “downward entailing” include downwardmonotonic and monotone decreasing. Related concepts include anti-additivity, veridicality, and one-way implicatives. Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 137–145, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics As the prevalence of these operators indicates and as van der Wouden (1997, pg. 92) states, downward entailment “plays an extremely important role in natural language” (van Benthem, 1986; Hoeksema, 1986; S´anchez Valencia, 1991; Dowty, 1994; MacCartney and Manning, 2007). Yet to date, only a few systems attempt to handle the phenomenon in a general way, i.e., to consider more than simple direct negation (Nairn, Condoravdi, and Karttunen, 2006; MacCartney and Manning, 2008; Christodoulopoulos, 2008; Bar-Haim, Berant, Dagan, Greental, Mirkin, Shnarch, and Szpektor, 2008). These systems rely on lists of items annotated with respect to their behavior in “polar” (positive or negative) environments. The lists contain a relatively small number of downward-entailing operators, at least in part because they were constructed mainly by manual inspection of verb lists (a"
N09-1016,C08-1066,0,0.352695,"nual Conference of the North American Chapter of the ACL, pages 137–145, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics As the prevalence of these operators indicates and as van der Wouden (1997, pg. 92) states, downward entailment “plays an extremely important role in natural language” (van Benthem, 1986; Hoeksema, 1986; S´anchez Valencia, 1991; Dowty, 1994; MacCartney and Manning, 2007). Yet to date, only a few systems attempt to handle the phenomenon in a general way, i.e., to consider more than simple direct negation (Nairn, Condoravdi, and Karttunen, 2006; MacCartney and Manning, 2008; Christodoulopoulos, 2008; Bar-Haim, Berant, Dagan, Greental, Mirkin, Shnarch, and Szpektor, 2008). These systems rely on lists of items annotated with respect to their behavior in “polar” (positive or negative) environments. The lists contain a relatively small number of downward-entailing operators, at least in part because they were constructed mainly by manual inspection of verb lists (although a few non-verbs are sometimes also included). We therefore propose to automatically learn downward-entailing operators2 — henceforth DE operators for short — from data; deriving more comprehensive"
N09-1016,W06-3907,0,0.564682,"Missing"
N09-1016,trawinski-soehn-2008-multilingual,0,0.031221,"Missing"
N10-1056,W09-3102,0,0.012136,"d above, a key idea in our work is to utilize SimpleEW edits. The primary difficulty in working with these modifications is that they include not only simplifications but also edits that serve other functions, such as spam removal or correction of grammar or factual content (“fixes”). We describe two main approaches to this problem: a probabilistic model that captures this mixture of different edit operations (§2.1), and the use of metadata to filter out undesirable revisions (§2.2). 2 One exception [5] changes verb tense and replaces pronouns. Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3]. 3 A type of lexical corruption, e.g., “acorn”→“eggcorn”. 365 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365–368, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2.1 Edit model We say that the k th article in a Wikipedia corresponds to (among other things) a title or topic (e.g., “Cat”) and a sequence d~k of article versions caused by successive edits. For a given lexical item or phrase A, we write A ∈ d~k if there is any version in d~k that contai"
N10-1056,W07-1007,0,0.0360269,"d above, a key idea in our work is to utilize SimpleEW edits. The primary difficulty in working with these modifications is that they include not only simplifications but also edits that serve other functions, such as spam removal or correction of grammar or factual content (“fixes”). We describe two main approaches to this problem: a probabilistic model that captures this mixture of different edit operations (§2.1), and the use of metadata to filter out undesirable revisions (§2.2). 2 One exception [5] changes verb tense and replaces pronouns. Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3]. 3 A type of lexical corruption, e.g., “acorn”→“eggcorn”. 365 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365–368, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2.1 Edit model We say that the k th article in a Wikipedia corresponds to (among other things) a title or topic (e.g., “Cat”) and a sequence d~k of article versions caused by successive edits. For a given lexical item or phrase A, we write A ∈ d~k if there is any version in d~k that contai"
N10-1056,E06-1021,0,0.20104,"Missing"
N10-1056,P09-1051,0,0.014484,"ated. However, note that ComplexEW and SimpleEW do not together constitute a clean parallel corpus, but rather an extremely noisy comparable corpus. For example, Complex/Simple same-topic document pairs are often written completely independently of each other, and even when it is possible to get good sentence alignments between them, the sentence pairs may reflect operations other than simplification, such as corrections, additions, or edit spam. Our work joins others in using Wikipedia revisions to learn interesting types of directional lexical relations, e.g, “eggcorns”3 [7] and entailments [8]. 2 Method As mentioned above, a key idea in our work is to utilize SimpleEW edits. The primary difficulty in working with these modifications is that they include not only simplifications but also edits that serve other functions, such as spam removal or correction of grammar or factual content (“fixes”). We describe two main approaches to this problem: a probabilistic model that captures this mixture of different edit operations (§2.1), and the use of metadata to filter out undesirable revisions (§2.2). 2 One exception [5] changes verb tense and replaces pronouns. Other lexical-level work fo"
N10-1056,C04-1129,0,0.0409169,"of spin-off of the well-known English Wikipedia (henceforth ComplexEW) where human editors enforce simplicity of language through rewriting. The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW. Importantly, not all the changes on SimpleEW are simplifications; we thus also make use of ComplexEW edits to filter out non-simplifications. Related work and related problems Previous work usually involves general syntactic-level trans1 http://simple.wikipedia.org formation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., “collaborate” → “work together”), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules. Simplification is strongly related to but distinct from paraphrasing and machine translation (MT). While it can be considered a directional form of the former, it differs in spirit because simplification must trade off meaning preservation (central to paraphrasing) against complexity reduction (not a consideration in paraphrasing). Simplification can also be considered"
N10-1056,P08-1040,0,0.102745,"of spin-off of the well-known English Wikipedia (henceforth ComplexEW) where human editors enforce simplicity of language through rewriting. The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW. Importantly, not all the changes on SimpleEW are simplifications; we thus also make use of ComplexEW edits to filter out non-simplifications. Related work and related problems Previous work usually involves general syntactic-level trans1 http://simple.wikipedia.org formation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., “collaborate” → “work together”), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules. Simplification is strongly related to but distinct from paraphrasing and machine translation (MT). While it can be considered a directional form of the former, it differs in spirit because simplification must trade off meaning preservation (central to paraphrasing) against complexity reduction (not a consideration in paraphrasing). Simplification can also be considered"
N18-1199,W17-7101,0,0.037529,"Missing"
N18-1199,P14-2118,0,0.670246,"s; images shared on social media are often coupled with descriptions or tags; and textbooks include illustrations, photos, and other visual elements. The ubiquity and diversity of such “text+image” material (henceforth referred to as multimodal content) suggest that, from the standpoint of sharing information, images and text are often natural complements. Lillian Lee Cornell University llee@cs.cornell.edu Ideally, machine learning algorithms that incorporate information from both text and images should have a fuller perspective than those that consider either text or images in isolation. But Hill and Korhonen (2014b) observe that for their particular multimodal architecture, the level of concreteness of a concept being represented — intuitively, the idea of a dog is more concrete than that of beauty — affects whether multimodal or single-channel representations are more effective. In their case, concreteness was derived for 766 nouns and verbs from a fixed psycholinguistic database of human ratings. In contrast, we introduce an adaptive algorithm for characterizing the visual concreteness of all the concepts indexed textually (e.g., “dog”) in a given multimodal dataset. Our approach is to leverage the g"
N18-1199,D14-1032,0,0.560888,"s; images shared on social media are often coupled with descriptions or tags; and textbooks include illustrations, photos, and other visual elements. The ubiquity and diversity of such “text+image” material (henceforth referred to as multimodal content) suggest that, from the standpoint of sharing information, images and text are often natural complements. Lillian Lee Cornell University llee@cs.cornell.edu Ideally, machine learning algorithms that incorporate information from both text and images should have a fuller perspective than those that consider either text or images in isolation. But Hill and Korhonen (2014b) observe that for their particular multimodal architecture, the level of concreteness of a concept being represented — intuitively, the idea of a dog is more concrete than that of beauty — affects whether multimodal or single-channel representations are more effective. In their case, concreteness was derived for 766 nouns and verbs from a fixed psycholinguistic database of human ratings. In contrast, we introduce an adaptive algorithm for characterizing the visual concreteness of all the concepts indexed textually (e.g., “dog”) in a given multimodal dataset. Our approach is to leverage the g"
N18-1199,Q14-1023,0,0.562174,"r cross-modal retrieval settings, images are paired with long, only loosely thematically-related documents. (Khan et al., 2009; Socher and Fei-Fei, 2010; Jia et al., 2011; Zhuang et al., 2013, inter alia). We provide experimental results on both types of data. Concreteness in datasets has been previously studied in either text-only cases (Turney et al., 2011; Hill et al., 2013) or by incorporating human judgments of perception into models (Silberer and Lapata, 2012; Hill and Korhonen, 2014a). Other work has quantified characteristics of concreteness in multimodal datasets (Young et al., 2014; Hill et al., 2014; Hill and Korhonen, 2014b; Kiela and Bottou, 2014; Jas and Parikh, 2015; Lazari2195 dou et al., 2015; Silberer et al., 2016; Lu et al., 2017; Bhaskar et al., 2017). Most related to our work is that of Kiela et al. (2014); the authors use Google image search to collect 50 images each for a variety of words and compute the average cosine similarity between vector representations of returned images. In contrast, our method can be tuned to specific datasets without reliance on an external search engine. Other algorithmic advantages of our method include that: it more readily scales than previous"
N18-1199,D14-1005,0,0.307703,"paired with long, only loosely thematically-related documents. (Khan et al., 2009; Socher and Fei-Fei, 2010; Jia et al., 2011; Zhuang et al., 2013, inter alia). We provide experimental results on both types of data. Concreteness in datasets has been previously studied in either text-only cases (Turney et al., 2011; Hill et al., 2013) or by incorporating human judgments of perception into models (Silberer and Lapata, 2012; Hill and Korhonen, 2014a). Other work has quantified characteristics of concreteness in multimodal datasets (Young et al., 2014; Hill et al., 2014; Hill and Korhonen, 2014b; Kiela and Bottou, 2014; Jas and Parikh, 2015; Lazari2195 dou et al., 2015; Silberer et al., 2016; Lu et al., 2017; Bhaskar et al., 2017). Most related to our work is that of Kiela et al. (2014); the authors use Google image search to collect 50 images each for a variety of words and compute the average cosine similarity between vector representations of returned images. In contrast, our method can be tuned to specific datasets without reliance on an external search engine. Other algorithmic advantages of our method include that: it more readily scales than previous solutions, it makes relatively few assumptions reg"
N18-1199,P14-2135,0,0.471937,"perimental results on both types of data. Concreteness in datasets has been previously studied in either text-only cases (Turney et al., 2011; Hill et al., 2013) or by incorporating human judgments of perception into models (Silberer and Lapata, 2012; Hill and Korhonen, 2014a). Other work has quantified characteristics of concreteness in multimodal datasets (Young et al., 2014; Hill et al., 2014; Hill and Korhonen, 2014b; Kiela and Bottou, 2014; Jas and Parikh, 2015; Lazari2195 dou et al., 2015; Silberer et al., 2016; Lu et al., 2017; Bhaskar et al., 2017). Most related to our work is that of Kiela et al. (2014); the authors use Google image search to collect 50 images each for a variety of words and compute the average cosine similarity between vector representations of returned images. In contrast, our method can be tuned to specific datasets without reliance on an external search engine. Other algorithmic advantages of our method include that: it more readily scales than previous solutions, it makes relatively few assumptions regarding the distribution of images/text, it normalizes for word frequency in a principled fashion, and it can produce confidence intervals. Finally, the method we propose c"
N18-1199,D12-1130,0,0.0795859,") or short, literal natural language captions (Farhadi et al., 2010; Ord´on˜ ez et al., 2011; Kulkarni et al., 2013; Fang et al., 2015, inter alia). In other cross-modal retrieval settings, images are paired with long, only loosely thematically-related documents. (Khan et al., 2009; Socher and Fei-Fei, 2010; Jia et al., 2011; Zhuang et al., 2013, inter alia). We provide experimental results on both types of data. Concreteness in datasets has been previously studied in either text-only cases (Turney et al., 2011; Hill et al., 2013) or by incorporating human judgments of perception into models (Silberer and Lapata, 2012; Hill and Korhonen, 2014a). Other work has quantified characteristics of concreteness in multimodal datasets (Young et al., 2014; Hill et al., 2014; Hill and Korhonen, 2014b; Kiela and Bottou, 2014; Jas and Parikh, 2015; Lazari2195 dou et al., 2015; Silberer et al., 2016; Lu et al., 2017; Bhaskar et al., 2017). Most related to our work is that of Kiela et al. (2014); the authors use Google image search to collect 50 images each for a variety of words and compute the average cosine similarity between vector representations of returned images. In contrast, our method can be tuned to specific da"
N18-1199,D11-1063,0,0.116143,"gs (Barnard et al., 2003; Cusano et al., 2004; Grangier and Bengio, 2008; Chen et al., 2013, inter alia) or short, literal natural language captions (Farhadi et al., 2010; Ord´on˜ ez et al., 2011; Kulkarni et al., 2013; Fang et al., 2015, inter alia). In other cross-modal retrieval settings, images are paired with long, only loosely thematically-related documents. (Khan et al., 2009; Socher and Fei-Fei, 2010; Jia et al., 2011; Zhuang et al., 2013, inter alia). We provide experimental results on both types of data. Concreteness in datasets has been previously studied in either text-only cases (Turney et al., 2011; Hill et al., 2013) or by incorporating human judgments of perception into models (Silberer and Lapata, 2012; Hill and Korhonen, 2014a). Other work has quantified characteristics of concreteness in multimodal datasets (Young et al., 2014; Hill et al., 2014; Hill and Korhonen, 2014b; Kiela and Bottou, 2014; Jas and Parikh, 2015; Lazari2195 dou et al., 2015; Silberer et al., 2016; Lu et al., 2017; Bhaskar et al., 2017). Most related to our work is that of Kiela et al. (2014); the authors use Google image search to collect 50 images each for a variety of words and compute the average cosine simi"
N18-1199,Q14-1006,0,0.0818486,"inter alia). In other cross-modal retrieval settings, images are paired with long, only loosely thematically-related documents. (Khan et al., 2009; Socher and Fei-Fei, 2010; Jia et al., 2011; Zhuang et al., 2013, inter alia). We provide experimental results on both types of data. Concreteness in datasets has been previously studied in either text-only cases (Turney et al., 2011; Hill et al., 2013) or by incorporating human judgments of perception into models (Silberer and Lapata, 2012; Hill and Korhonen, 2014a). Other work has quantified characteristics of concreteness in multimodal datasets (Young et al., 2014; Hill et al., 2014; Hill and Korhonen, 2014b; Kiela and Bottou, 2014; Jas and Parikh, 2015; Lazari2195 dou et al., 2015; Silberer et al., 2016; Lu et al., 2017; Bhaskar et al., 2017). Most related to our work is that of Kiela et al. (2014); the authors use Google image search to collect 50 images each for a variety of words and compute the average cosine similarity between vector representations of returned images. In contrast, our method can be tuned to specific datasets without reliance on an external search engine. Other algorithmic advantages of our method include that: it more readily sc"
N18-1199,N15-1016,0,\N,Missing
N18-1199,W13-2609,0,\N,Missing
N18-2067,P99-1059,0,0.141824,"stack item. As an illustration: res2 ,s0 : . 2 rh1 , i, h2 , h3 , ks rh3 , k, h4 , h5 , js rh1 , i, h2 , h5 , js rh1 , i, h2 , h4 , js . The goal of deduction is to produce the Icomputation rϵ, 0, ϵ, 0, ϵs, using the shift and reduce deduction rules starting from the axiom rϵ, 0, ϵ, 0, 1s, corresponding to the first and mandatory shift transition moving the root node from buffer to stack. ϵ stands for an empty stack or buffer. As analyzed by Cohen et al. (2011), direct tabularization for this deduction system takes Opn5 q space and Opn8 q time. With adaptation of the “hook trick” described in Eisner and Satta (1999), we can reduce the running time to Opn7 q. Each reduce rule combines two I-computations into a larger I-computation, e.g. (see Fig. 3): res0 ,s1 : rh1 , i, h2 , h3 , ks rh3 , k, h4 , h5 , js , 3.2 Our New Variants While Opn7 q or Opn10 q is not practical, the result is still impressive, since the search space is exponential. Cohen et al. (2011) were inspired by Huang and Sagae’s (2010) and Kuhlmann et al.’s (2011) dynamic-programming approach for projective systems. 3 See Cohen et al. (2011) for full description. 4 Condition (2) is used for proving completeness of the deduction system (Cohen"
N18-2067,J16-4008,1,0.576781,"plied to generalized Attardi (2006) systems; see 2 Transition-based Parsing We first introduce necessary definitions and notation. 2.1 A General Class of Transition Systems A transition system is given by a 4-tuple pC, T, cs , Cτ q, where C is a set of configurations, T is a set of transition functions between configurations, cs is an initialization function mapping an input sentence to an initial configuration, and Cτ Ă C defines a set of terminal configurations. 1 Faster exact inference algorithms have been defined for some sets of mildly non-projective trees (e.g. Pitler et al. (2013); see Gómez-Rodríguez (2016) for more), but lack an underlying transition system. Having one has the practical advantage of allowing generative models, as in Cohen et al. (2011), and transition-based scoring functions, which have yielded good projective-parsing results (Shi et al., 2017); plus the theoretical advantage of providing a single framework supporting greedy, beam-search, and exact inference. 420 Proceedings of NAACL-HLT 2018, pages 420–425 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics System Reduce Transitions Non-proj. Coverage Time Complexity Max. Degree Attardi ("
N18-2067,P10-1110,0,0.370016,"Missing"
N18-2067,P11-1068,1,0.938245,"Missing"
N18-2067,W07-2216,0,0.454627,"Missing"
N18-2067,D17-1002,1,0.863417,"Missing"
N18-2067,Q13-1002,0,0.283873,"r technique can also be applied to generalized Attardi (2006) systems; see 2 Transition-based Parsing We first introduce necessary definitions and notation. 2.1 A General Class of Transition Systems A transition system is given by a 4-tuple pC, T, cs , Cτ q, where C is a set of configurations, T is a set of transition functions between configurations, cs is an initialization function mapping an input sentence to an initial configuration, and Cτ Ă C defines a set of terminal configurations. 1 Faster exact inference algorithms have been defined for some sets of mildly non-projective trees (e.g. Pitler et al. (2013); see Gómez-Rodríguez (2016) for more), but lack an underlying transition system. Having one has the practical advantage of allowing generative models, as in Cohen et al. (2011), and transition-based scoring functions, which have yielded good projective-parsing results (Shi et al., 2017); plus the theoretical advantage of providing a single framework supporting greedy, beam-search, and exact inference. 420 Proceedings of NAACL-HLT 2018, pages 420–425 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics System Reduce Transitions Non-proj. Coverage Time Comp"
N18-2067,D11-1114,1,\N,Missing
N19-1166,W17-2905,0,0.667149,"berman, 2008; Muchnik et al., 2013; Weninger et al., 2015). Hence, we propose an early-detection approach that uses not just the content of the initiating post, but also the content and structure of the initial responding comments. In doing so, we unite streams of heretofore mostly disjoint research programs: see Figure 1. Working with over 15,000 discus1648 Proceedings of NAACL-HLT 2019, pages 1648–1659 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics No, whether a topic (or entity/hashtag/word) has been controversial [a distinction also made by Addawood et al. (2017)] (Popescu and PenIs the task to determine whether a textual item will provoke controversy? nacchiotti, 2010; Choi et al., 2010; Cao et al., 2015; Lourentzou et al., 2015; Chen et al., 2016; Addawood et al., 2017; Al-Ayyoub et al., 2017; Garimella et al., 2018) No, whether a conversation contained disagreement (Mishne and Glance, 2006; Yin et al., 2012; Allen et al., 2014; Wang and Cardie, 2014) or mapping the disagreements (Awadallah et al., 2012; Marres, 2015; Borra et al., 2015; Liu et al., 2018) No, the task is, for the given textual item, predict antisocial behavior in the ensuing discuss"
N19-1166,D14-1124,0,0.1072,"ACL-HLT 2019, pages 1648–1659 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics No, whether a topic (or entity/hashtag/word) has been controversial [a distinction also made by Addawood et al. (2017)] (Popescu and PenIs the task to determine whether a textual item will provoke controversy? nacchiotti, 2010; Choi et al., 2010; Cao et al., 2015; Lourentzou et al., 2015; Chen et al., 2016; Addawood et al., 2017; Al-Ayyoub et al., 2017; Garimella et al., 2018) No, whether a conversation contained disagreement (Mishne and Glance, 2006; Yin et al., 2012; Allen et al., 2014; Wang and Cardie, 2014) or mapping the disagreements (Awadallah et al., 2012; Marres, 2015; Borra et al., 2015; Liu et al., 2018) No, the task is, for the given textual item, predict antisocial behavior in the ensuing discussion (Zhang et al., 2018b,a), or subsequent comment volume/popularity/structure (Szabo and Huberman, 2010; Kim et al., 2011; Tatar et al., 2011; Backstrom et al., 2013; He et al., 2014; Zhang et al., 2018b), or eventual post article score (Rangwala and Jamali, 2010; Szabo and Huberman, 2010),; but all where, like us, the paradigm is early detection No, only info available"
N19-1166,C16-2057,0,0.0687804,"Missing"
N19-1166,N19-1423,0,0.0117079,"posed by Arora et al. (2017), serves as a “tough to beat” baseline for sentence representations. LSTM. We train a Bi-LSTM (Graves and Schmidhuber, 2005) over the first 128 tokens of titles + post text, followed by a mean pooling layer, and then a logistic regression layer. The LSTM’s embedding layer is initialized with the same word2vec embeddings used in W2V. Markdown formatting artifacts are discarded. BERT-LSTM. Recently, features extracted from fixed, pretrained, neural language models have resulted in high performance on a range of language tasks. Following the recommendations of §5.4 of Devlin et al. (2019), we consider representing posts by extracting BERT-Large embeddings computed for the first 128 tokens of titles + post text; we average the final 4 layers of the 24-layer, pretrained Transformer-decoder network (Vaswani et al., 2017). These token-specific vectors are then passed to a Bi-LSTM, a mean pooling layer, and a logistic classification layer. We keep markdown formatting artifacts because BERT’s token vocabulary are WordPiece subtokens (Wu et al., 2016), which are able to incorporate arbitrary punctuation without modification. BERT-MP. Instead of training a Bi-LSTM over BERT features,"
N19-1166,W18-6246,0,0.194603,"Missing"
N19-1166,W12-3710,0,0.0333568,"Proceedings of NAACL-HLT 2019, pages 1648–1659 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics No, whether a topic (or entity/hashtag/word) has been controversial [a distinction also made by Addawood et al. (2017)] (Popescu and PenIs the task to determine whether a textual item will provoke controversy? nacchiotti, 2010; Choi et al., 2010; Cao et al., 2015; Lourentzou et al., 2015; Chen et al., 2016; Addawood et al., 2017; Al-Ayyoub et al., 2017; Garimella et al., 2018) No, whether a conversation contained disagreement (Mishne and Glance, 2006; Yin et al., 2012; Allen et al., 2014; Wang and Cardie, 2014) or mapping the disagreements (Awadallah et al., 2012; Marres, 2015; Borra et al., 2015; Liu et al., 2018) No, the task is, for the given textual item, predict antisocial behavior in the ensuing discussion (Zhang et al., 2018b,a), or subsequent comment volume/popularity/structure (Szabo and Huberman, 2010; Kim et al., 2011; Tatar et al., 2011; Backstrom et al., 2013; He et al., 2014; Zhang et al., 2018b), or eventual post article score (Rangwala and Jamali, 2010; Szabo and Huberman, 2010),; but all where, like us, the paradigm is early detection No,"
N19-1166,P18-1125,0,0.064347,"pescu and PenIs the task to determine whether a textual item will provoke controversy? nacchiotti, 2010; Choi et al., 2010; Cao et al., 2015; Lourentzou et al., 2015; Chen et al., 2016; Addawood et al., 2017; Al-Ayyoub et al., 2017; Garimella et al., 2018) No, whether a conversation contained disagreement (Mishne and Glance, 2006; Yin et al., 2012; Allen et al., 2014; Wang and Cardie, 2014) or mapping the disagreements (Awadallah et al., 2012; Marres, 2015; Borra et al., 2015; Liu et al., 2018) No, the task is, for the given textual item, predict antisocial behavior in the ensuing discussion (Zhang et al., 2018b,a), or subsequent comment volume/popularity/structure (Szabo and Huberman, 2010; Kim et al., 2011; Tatar et al., 2011; Backstrom et al., 2013; He et al., 2014; Zhang et al., 2018b), or eventual post article score (Rangwala and Jamali, 2010; Szabo and Huberman, 2010),; but all where, like us, the paradigm is early detection No, only info available at the item’s creation ...using early reactions, which, recall, Salganik et al. (2006) observe to be sometimes crucial? (Dori-Hacohen and Allan, 2013; Mejova et al., 2014; Klenner et al., 2014; Dori-Hacohen et al., 2016; Jang and Allan, 2016; Jang e"
N19-1166,P14-2113,0,0.172513,"1648–1659 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics No, whether a topic (or entity/hashtag/word) has been controversial [a distinction also made by Addawood et al. (2017)] (Popescu and PenIs the task to determine whether a textual item will provoke controversy? nacchiotti, 2010; Choi et al., 2010; Cao et al., 2015; Lourentzou et al., 2015; Chen et al., 2016; Addawood et al., 2017; Al-Ayyoub et al., 2017; Garimella et al., 2018) No, whether a conversation contained disagreement (Mishne and Glance, 2006; Yin et al., 2012; Allen et al., 2014; Wang and Cardie, 2014) or mapping the disagreements (Awadallah et al., 2012; Marres, 2015; Borra et al., 2015; Liu et al., 2018) No, the task is, for the given textual item, predict antisocial behavior in the ensuing discussion (Zhang et al., 2018b,a), or subsequent comment volume/popularity/structure (Szabo and Huberman, 2010; Kim et al., 2011; Tatar et al., 2011; Backstrom et al., 2013; He et al., 2014; Zhang et al., 2018b), or eventual post article score (Rangwala and Jamali, 2010; Szabo and Huberman, 2010),; but all where, like us, the paradigm is early detection No, only info available at the item’s creation ."
P04-1035,P97-1023,0,0.121333,"such algorithms is precisely one of the strengths of our approach. But a crucial advantage specific to the utilization of a minimum-cut-based approach is that we can use maximum-flow algorithms with polynomial asymptotic running times — and near-linear running times in practice — to exactly compute the minimumcost cut(s), despite the apparent intractability of the optimization problem (Cormen, Leiserson, and Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2 In contrast, other graph-partitioning problems that have been previously used to formulate NLP classification problems3 are NP-complete (Hatzivassiloglou and McKeown, 1997; Agrawal et al., 2003; Joachims, 2003). 2 Code available at http://www.avglab.com/andrew/soft.html. Graph-based approaches to general clustering problems are too numerous to mention here. 3 3 Evaluation Framework Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons. First, as mentioned in the introduction, providing polarity information about reviews is a useful service: witness the popularity of www.rottentomatoes.com. Second, movie reviews are apparently harder to classify than reviews of other products (Turney, 2002; Dave,"
P04-1035,W02-1011,1,0.148748,": witness the popularity of www.rottentomatoes.com. Second, movie reviews are apparently harder to classify than reviews of other products (Turney, 2002; Dave, Lawrence, and Pennock, 2003). Third, the correct label can be extracted automatically from rating information (e.g., number of stars). Our data4 contains 1000 positive and 1000 negative reviews all written before 2002, with a cap of 20 reviews per author (312 authors total) per category. We refer to this corpus as the polarity dataset. Default polarity classifiers We tested support vector machines (SVMs) and Naive Bayes (NB). Following Pang et al. (2002), we use unigram-presence features: the ith coordinate of a feature vector is 1 if the corresponding unigram occurs in the input text, 0 otherwise. (For SVMs, the feature vectors are length-normalized). Each default documentlevel polarity classifier is trained and tested on the extracts formed by applying one of the sentencelevel subjectivity detectors to reviews in the polarity dataset. Subjectivity dataset To train our detectors, we need a collection of labeled sentences. Riloff and Wiebe (2003) state that “It is [very hard] to obtain collections of individual sentences that can be easily id"
P04-1035,W03-1014,0,0.173178,"Default polarity classifiers We tested support vector machines (SVMs) and Naive Bayes (NB). Following Pang et al. (2002), we use unigram-presence features: the ith coordinate of a feature vector is 1 if the corresponding unigram occurs in the input text, 0 otherwise. (For SVMs, the feature vectors are length-normalized). Each default documentlevel polarity classifier is trained and tested on the extracts formed by applying one of the sentencelevel subjectivity detectors to reviews in the polarity dataset. Subjectivity dataset To train our detectors, we need a collection of labeled sentences. Riloff and Wiebe (2003) state that “It is [very hard] to obtain collections of individual sentences that can be easily identified as subjective or objective”; the polarity-dataset sentences, for example, have not 4 Available at www.cs.cornell.edu/people/pabo/moviereview-data/ (review corpus version 2.0). been so annotated.5 Fortunately, we were able to mine the Web to create a large, automaticallylabeled sentence corpus6 . To gather subjective sentences (or phrases), we collected 5000 moviereview snippets (e.g., “bold, imaginative, and impossible to resist”) from www.rottentomatoes.com. To obtain (mostly) objective"
P04-1035,W03-0404,0,0.12256,"Missing"
P04-1035,P02-1053,0,0.199223,"ou and McKeown, 1997; Agrawal et al., 2003; Joachims, 2003). 2 Code available at http://www.avglab.com/andrew/soft.html. Graph-based approaches to general clustering problems are too numerous to mention here. 3 3 Evaluation Framework Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons. First, as mentioned in the introduction, providing polarity information about reviews is a useful service: witness the popularity of www.rottentomatoes.com. Second, movie reviews are apparently harder to classify than reviews of other products (Turney, 2002; Dave, Lawrence, and Pennock, 2003). Third, the correct label can be extracted automatically from rating information (e.g., number of stars). Our data4 contains 1000 positive and 1000 negative reviews all written before 2002, with a cap of 20 reviews per author (312 authors total) per category. We refer to this corpus as the polarity dataset. Default polarity classifiers We tested support vector machines (SVMs) and Naive Bayes (NB). Following Pang et al. (2002), we use unigram-presence features: the ith coordinate of a feature vector is 1 if the corresponding unigram occurs in the input text,"
P04-1035,J94-2004,0,0.13529,"e et al. (2004) is to reveal a document’s sentiment polarity, but they do not evaluate the polarity-classification accuracy that results. 2.2 Context and Subjectivity Detection As with document-level polarity classification, we could perform subjectivity detection on individual sentences by applying a standard classification algorithm on each sentence in isolation. However, modeling proximity relationships between sentences would enable us to leverage coherence: text spans occurring near each other (within discourse boundaries) may share the same subjectivity status, other things being equal (Wiebe, 1994). We would therefore like to supply our algorithms with pair-wise interaction information, e.g., to specify that two particular sentences should ideally receive the same subjectivity label but not state which label this should be. Incorporating such information is somewhat unnatural for classifiers whose input consists simply of individual feature vectors, such as Naive Bayes or SVMs, precisely because such classifiers label each test item in isolation. One could define synthetic features or feature vectors to attempt to overcome this obstacle. However, we propose an alternative that avoids th"
P04-1035,W03-1017,0,0.209842,"hat determines whether each sentence is subjective or not: discarding the objective ones creates an extract that should better represent a review’s subjective content to a default polarity classifier. subjective sentence? yes s2 no s3 no s4 s_n subjectivity detector s1 m−sentence extract (m&lt;=n) s1 s4 yes positive or negative review? default polarity classifier n−sentence review +/− subjectivity extraction Figure 1: Polarity classification via subjectivity detection. To our knowledge, previous work has not integrated sentence-level subjectivity detection with document-level sentiment polarity. Yu and Hatzivassiloglou (2003) provide methods for sentencelevel analysis and for determining whether a document is subjective or not, but do not combine these two types of algorithms or consider document polarity classification. The motivation behind the singlesentence selection method of Beineke et al. (2004) is to reveal a document’s sentiment polarity, but they do not evaluate the polarity-classification accuracy that results. 2.2 Context and Subjectivity Detection As with document-level polarity classification, we could perform subjectivity detection on individual sentences by applying a standard classification algori"
P05-1015,P04-1035,1,0.25046,"n than the three-class setting, since it involves more class pairs. Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classification techniques (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002). We applied the above two labeling schemes to a scale dataset3 containing four corpora of movie reviews. All reviews were automatically preprocessed to remove both explicit rating indicators and objective sentences; the motivation for the latter step is that it has previously aided positive vs. negative classification (Pang and Lee, 2004). All of the 1770, 902, 1307, or 1027 documents in a given corpus were written by the same author. This decision facilitates interpretation of the results, since it factors out the effects of different choices of methods for calibrating authors’ scales.4 We point out that but since our goal is to recover a reviewer’s “true” recommendation, reader-author agreement is more relevant. While another factor might be degree of English fluency, in an informal experiment (six subjects viewing the same three pairs), native English speakers made the only two errors. 3 Available at http://www.cs.cornell.e"
P05-1015,W02-1011,1,0.0884182,"pect to rating scales      Bo Pang and Lillian Lee (1) Department of Computer Science, Cornell University (2) Language Technologies Institute, Carnegie Mellon University (3) Computer Science Department, Carnegie Mellon University Abstract the scope of new applications enabled by the processing of subjective language. (The papers collected by Qu, Shanahan, and Wiebe (2004) form a representative sample of research in the area.) Most prior work on the specific problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative (Turney, 2002; Pang, Lee, and Vaithyanathan, 2002; Dave, Lawrence, and Pennock, 2003; Yu and Hatzivassiloglou, 2003). But it is often helpful to have more information than this binary distinction provides, especially if one is ranking items by recommendation or comparing several reviewers’ opinions: example applications include collaborative filtering and deciding which conference submissions to accept. We address the rating-inference problem, wherein rather than simply decide whether a review is “thumbs up” or “thumbs down”, as in previous sentiment analysis work, one must determine an author’s evaluation with respect to a multi-point scale"
P05-1015,W04-2408,0,0.0562415,"o apply our methods to other scale-based classification problems, and explore alternative methods. Clearly, varying the kernel in SVM regression might yield better results. Another choice is ordinal regression (McCullagh, 1980; Herbrich, Graepel, and Obermayer, 2000), which only considers the ordering on labels, rather than any explicit distances between them; this approach could work well if a good metric on labels is lacking. Also, one could use mixture models (e.g., combine “positive” and “negative” language models) to capture class relationships (McCallum, 1999; Schapire and Singer, 2000; Takamura, Matsumoto, and Yamada, 2004). We are also interested in framing multi-class but non-scale-based categorization problems as metric labeling tasks. For example, positive vs. negative vs. neutral sentiment distinctions are sometimes considered in which neutral means either objective (Engstr¨om, 2004) or a conflation of objective with a rating of mediocre (Das and Chen, 2001). (Koppel and Schler (2005) in independent work also discuss various types of neutrality.) In either case, we could apply a metric in which positive and negative are closer to objective (or objective+mediocre) than to each other. As another example, hie"
P05-1015,N04-1025,0,\N,Missing
P05-1015,W03-1017,0,\N,Missing
P05-1015,P02-1053,0,\N,Missing
P10-2046,C08-1066,0,0.0692632,"Missing"
P10-2046,W06-3907,0,0.138364,"Missing"
P10-2046,N09-1016,1,0.683431,"examples showing different constructions for anais upward entailing. The word ‘observed’ is an lyzing these operators: ‘The defendant does not own a blue example upward-entailing operator: the statement car’ 6⇒ (⇐) ‘The defendant does not own a car’; ‘They are reluctant to tango’ 6⇒ (⇐) ‘They are reluctant to dance’; ‘Police doubt Smith threatened Jones’ 6⇒ (⇐) ‘Police doubt Smith threatened Jones or Brown’; ‘You are allowed to use Mastercard’ 6⇒ (⇐) ‘You are allowed to use any credit card’. 2 The exception [2] employs the list automatically derived by Danescu-Niculescu-Mizil, Lee, and Ducott [5], described later. (i) ‘Witnesses observed opium use.’ implies (ii) ‘Witnesses observed narcotic use.’ but not vice versa (we write i ⇒ ( 6⇐) ii). That is, the truth value is preserved if we replace the 247 Proceedings of the ACL 2010 Conference Short Papers, pages 247–252, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics greater cognitive load than inferences in the opposite direction [8]. Most NLP systems for the applications mentioned above have only been deployed for a small subset of languages. A key factor is the lack of relevant resources for other lang"
P10-2046,W07-1401,0,\N,Missing
P12-1094,W11-0609,1,0.294618,"ublicly available (§2.1), a pilot study with human subjects validating IMDbbased memorability labels (§2.2), and further study of incorporating search-engine counts (§2.3). §3 details our analysis and prediction experiments, using both movie-quotes data and, as an exploration of cross-domain applicability, slogans data. §4 surveys related work across a variety of fields. §5 briefly summarizes and indicates some future directions. 2 I’m ready for my close-up. 2.1 Data To study the properties of memorable movie quotes, we need a source of movie lines and a designation of memorability. Following [8], we constructed a corpus consisting of all lines from roughly 1000 movies, varying in genre, era, and popularity; for each movie, we then extracted the list of quotes from IMDb’s Memorable Quotes page corresponding to the movie.1 A memorable quote in IMDb can appear either as an individual sentence spoken by one character, or as a multi-sentence line, or as a block of dialogue involving multiple characters. In the latter two cases, it can be hard to determine which particular portion is viewed as memorable (some involve a build-up to a punch line; others involve the follow-through after a wel"
P14-1017,N12-1074,0,0.0170236,"effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter Chenhao Tan Dept. of Computer Science Cornell University chenhao@cs.cornell.edu Lillian Lee Dept. of Computer Science Cornell University llee@cs.cornell.edu Abstract also known as adoption, sharing, spread, or virality. According to prior research, important features include characteristics of the originating author (e.g., verified Twitter user or not, author’s messages’ past success rate), the author’s social network (e.g., number of followers), message timing, and message content or topic (Artzi et al., 2012; Bakshy et al., 2011; Borghol et al., 2012; Guerini et al., 2011; Guerini et al., 2012; Hansen et al., 2011; Hong et al., 2011; Lakkaraju et al., 2013; Milkman and Berger, 2012; Ma et al., 2012; Petrovi´c et al., 2011; Romero et al., 2013; Suh et al., 2010; Sun et al., 2013; Tsur and Rappoport, 2012). Indeed, it’s not surprising that one of the most retweeted tweets of all time was from user BarackObama, with 40M followers, on November 6, 2012: “Four more years. [link to photo]”. Consider a person trying to spread an important message on a social network. He/she can spend hours trying to craf"
P14-1017,D13-1181,0,0.0216641,"ange from examining what characteristics of New York Times articles correlate with high re-sharing rates (Milkman and Berger, 2012) to looking at how differences in description affect the spread of content-controlled videos or images (Borghol et al., 2012; Lakkaraju et al., 2013). Simmons et al. (2011) examined the variation of quotes from different sources to examine how textual memes mutate as people pass them along, but did not control for author. Predicting the “success” of various texts such as novels and movie quotes has been the aim of additional prior work not already mentioned in §1 (Ashok et al., 2013; Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Pitler and Nenkova, 2008; McIntyre and Lapata, 2009). To our knowledge, there have been no large-scale studies exploring wording effects in a both topic- and author-controlled setting. Employing such controls, we find that predicting the more effective alternative wording is much harder than the previously well-studied problem of pre1 Cf. the Music Lab “multiple universes” experiment to test the randomness of popularity (Salganik et al., 2006). 2 Although hashtags have been used as coarse-grained topic labels in prior work, for o"
P14-1017,P12-1094,1,0.902086,"ew York Times articles correlate with high re-sharing rates (Milkman and Berger, 2012) to looking at how differences in description affect the spread of content-controlled videos or images (Borghol et al., 2012; Lakkaraju et al., 2013). Simmons et al. (2011) examined the variation of quotes from different sources to examine how textual memes mutate as people pass them along, but did not control for author. Predicting the “success” of various texts such as novels and movie quotes has been the aim of additional prior work not already mentioned in §1 (Ashok et al., 2013; Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Pitler and Nenkova, 2008; McIntyre and Lapata, 2009). To our knowledge, there have been no large-scale studies exploring wording effects in a both topic- and author-controlled setting. Employing such controls, we find that predicting the more effective alternative wording is much harder than the previously well-studied problem of pre1 Cf. the Music Lab “multiple universes” experiment to test the randomness of popularity (Salganik et al., 2006). 2 Although hashtags have been used as coarse-grained topic labels in prior work, for our purposes, we have no assurance that two tweets both using, s"
P14-1017,P13-1174,0,0.0078075,"e the probability that tweets containing w are retweeted more than once, denoted by rspwq. We define the rt score of a tweet as maxwPT rspwq, where T is all the words in the tweet, and the rt score of a particular POS tag z in a tweet as maxwPT &tagpwq“z rspwq. Include positive and/or negative words. (Table 8) Prior work has found that including positive or negative sentiment increases message propagation (Milkman and Berger, 2012; Godes et al., 2005; Heath et al., 2001; Hansen et al., 2011). We measured the occurrence of positive and negative words as determined by the connotation lexicon of Feng et al. (2013) (better coverage than LIWC). Measuring the occurrence of both simultaneously was inspired by Riloff et al. (2013). Refer to other people (but not your audience). (Table 9) First-person has been found useful for success before, but in the different domains of scientific abstracts (Guerini et al., 2012) and books (Ashok et al., 2013). 15 This is not an artificial restriction on our set of authors; a large follower count means (in principle) that our results draw on a large sample of decisions whether to retweet or not. 16 The tokens [at], [hashtag], [url] were ignored in the unigram-model case"
P14-1017,Q13-1028,0,0.0102141,"what characteristics of New York Times articles correlate with high re-sharing rates (Milkman and Berger, 2012) to looking at how differences in description affect the spread of content-controlled videos or images (Borghol et al., 2012; Lakkaraju et al., 2013). Simmons et al. (2011) examined the variation of quotes from different sources to examine how textual memes mutate as people pass them along, but did not control for author. Predicting the “success” of various texts such as novels and movie quotes has been the aim of additional prior work not already mentioned in §1 (Ashok et al., 2013; Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Pitler and Nenkova, 2008; McIntyre and Lapata, 2009). To our knowledge, there have been no large-scale studies exploring wording effects in a both topic- and author-controlled setting. Employing such controls, we find that predicting the more effective alternative wording is much harder than the previously well-studied problem of pre1 Cf. the Music Lab “multiple universes” experiment to test the randomness of popularity (Salganik et al., 2006). 2 Although hashtags have been used as coarse-grained topic labels in prior work, for our purposes, we have no a"
P14-1017,P11-2008,0,0.00718452,"Missing"
P14-1017,P09-1025,0,0.0188901,"kman and Berger, 2012) to looking at how differences in description affect the spread of content-controlled videos or images (Borghol et al., 2012; Lakkaraju et al., 2013). Simmons et al. (2011) examined the variation of quotes from different sources to examine how textual memes mutate as people pass them along, but did not control for author. Predicting the “success” of various texts such as novels and movie quotes has been the aim of additional prior work not already mentioned in §1 (Ashok et al., 2013; Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Pitler and Nenkova, 2008; McIntyre and Lapata, 2009). To our knowledge, there have been no large-scale studies exploring wording effects in a both topic- and author-controlled setting. Employing such controls, we find that predicting the more effective alternative wording is much harder than the previously well-studied problem of pre1 Cf. the Music Lab “multiple universes” experiment to test the randomness of popularity (Salganik et al., 2006). 2 Although hashtags have been used as coarse-grained topic labels in prior work, for our purposes, we have no assurance that two tweets both using, say, “#Tahrir” would be attempting to express the same"
P14-1017,D08-1020,0,0.00992594,"high re-sharing rates (Milkman and Berger, 2012) to looking at how differences in description affect the spread of content-controlled videos or images (Borghol et al., 2012; Lakkaraju et al., 2013). Simmons et al. (2011) examined the variation of quotes from different sources to examine how textual memes mutate as people pass them along, but did not control for author. Predicting the “success” of various texts such as novels and movie quotes has been the aim of additional prior work not already mentioned in §1 (Ashok et al., 2013; Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Pitler and Nenkova, 2008; McIntyre and Lapata, 2009). To our knowledge, there have been no large-scale studies exploring wording effects in a both topic- and author-controlled setting. Employing such controls, we find that predicting the more effective alternative wording is much harder than the previously well-studied problem of pre1 Cf. the Music Lab “multiple universes” experiment to test the randomness of popularity (Salganik et al., 2006). 2 Although hashtags have been used as coarse-grained topic labels in prior work, for our purposes, we have no assurance that two tweets both using, say, “#Tahrir” would be att"
P14-1017,D13-1066,0,0.0108307,"Missing"
P14-2066,W03-1004,0,0.0714139,"Missing"
P14-2066,E12-1036,0,0.786789,"Missing"
P14-2066,W12-3809,1,0.907755,"Missing"
P14-2066,P08-2035,0,0.505749,"Missing"
P14-2066,C12-1044,0,0.495334,"Missing"
P14-2066,N10-1056,1,0.742767,"Missing"
P14-2066,D13-1055,0,0.852036,"Missing"
P14-2066,W10-3001,0,0.0810361,"Missing"
P14-2066,E12-1054,0,0.313293,"Missing"
P14-2066,max-wisniewski-2010-mining,0,0.170426,"Missing"
P14-2066,W10-3504,0,\N,Missing
P18-1248,P16-1231,0,0.0412957,"f it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibility with rich feature models: incorporation of complex features resulted in jumps in asy"
P18-1248,W06-2922,0,0.549968,"al.’s (2017a) approach to mildly nonprojective parsing in what, to our knowledge, is the first implementation of exact decoding for a non-projective transition-based parser. As in the projective case, a mildly non1 http://universaldependencies.org/ 2664 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2664–2675 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics projective decoder has been known for several years (Cohen et al., 2011), corresponding to a variant of the transition-based parser of Attardi (2006). However, its Opn7 q runtime — or the Opn6 q of a recently introduced improvedcoverage variant (Shi et al., 2018) — is still prohibitively costly in practice. Instead, we seek a more efficient algorithm to adapt, and thus develop a transition-based interpretation of GómezRodríguez et al.’s (2011) MH 4 dynamic programming parser, which has been shown to provide very good non-projective coverage in Opn4 q time (Gómez-Rodríguez, 2016). While the MH 4 parser was originally presented as a non-projective generalization of the dynamic program that later led to the arc-hybrid transition system (Gómez"
P18-1248,P17-1193,0,0.0164891,"ex6 Corro et al. (2016) describe a parser that enforces mildly non-projective constraints (bounded block degree and wellnestedness), but it is an arc-factored model, so it is subject to the same strong independence assumptions as maximumspanning-tree parsers like McDonald et al. (2005) and does not support the greater flexibility in scoring that is the main advantage of mildly non-projective parsers over these. Instead, mild non-projectivity is exclusively used as a criterion to discard nonconforming trees. tended to graph parsing by Kurtz and Kuhlmann (2017), Kummerfeld and Klein (2017), and Cao et al. (2017a,b), with the latter providing a practical implementation of a parser for 1-EndpointCrossing, pagenumber-2 graphs. 7 Conclusion We have extended the parsing architecture of Shi et al. (2017a) to non-projective dependency parsing by implementing the MH 4 parser, a mildly non-projective Opn4 q chart parsing algorithm, using a minimal set of transition-based bi-LSTM features. For this purpose, we have established a mapping between MH 4 items and transition sequences of an underlying non-projective transition-based parser. To our knowledge, this is the first practical implementation of exact infe"
P18-1248,D17-1003,0,0.0158959,"ex6 Corro et al. (2016) describe a parser that enforces mildly non-projective constraints (bounded block degree and wellnestedness), but it is an arc-factored model, so it is subject to the same strong independence assumptions as maximumspanning-tree parsers like McDonald et al. (2005) and does not support the greater flexibility in scoring that is the main advantage of mildly non-projective parsers over these. Instead, mild non-projectivity is exclusively used as a criterion to discard nonconforming trees. tended to graph parsing by Kurtz and Kuhlmann (2017), Kummerfeld and Klein (2017), and Cao et al. (2017a,b), with the latter providing a practical implementation of a parser for 1-EndpointCrossing, pagenumber-2 graphs. 7 Conclusion We have extended the parsing architecture of Shi et al. (2017a) to non-projective dependency parsing by implementing the MH 4 parser, a mildly non-projective Opn4 q chart parsing algorithm, using a minimal set of transition-based bi-LSTM features. For this purpose, we have established a mapping between MH 4 items and transition sequences of an underlying non-projective transition-based parser. To our knowledge, this is the first practical implementation of exact infe"
P18-1248,D14-1082,0,0.177618,"troduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibility with rich feature models: incorporation of co"
P18-1248,D11-1114,1,0.895329,"us heads, such as those deriving from Attardi (2006). 3.2 The MH 4 Transition System Kuhlmann et al. (2011) show how the items of a variant of MH 3 can be given a transition-based interpretation under the “push computation” framework, yielding the arc-hybrid projective transition system. However, such a derivation has not been made for the non-projective case (k ą 3), and the known techniques used to derive previous associations between tabular and transition-based parsers do not seem to be applicable in this case. The specific issue is that the deduction systems of Kuhlmann et al. (2011) and Cohen et al. (2011) have in common that the structure of their derivations is similar to that of a Dyck (or balancedbrackets) language, where steps corresponding to shift transitions are balanced with those corresponding to reduce transitions. This makes it possible to group derivation subtrees, and the transition sequences that they yield, into “push computations” that increase the length of the stack by a constant amount. However, this does not seem possible in MH 4 . Instead, we derive a transition-based interpretation of MH 4 by a generalization of that of MH 3 that departs from push computations. To do so,"
P18-1248,P16-1034,0,0.0367949,"Missing"
P18-1248,P16-2006,0,0.0890398,"Missing"
P18-1248,K17-3002,0,0.0422677,"stantiation of the MH k parser family. This corresponds to the global version of the arc-hybrid transition system (Kuhlmann et al., 2011). We adopt the minimal feature representation ts0 , b0 u, following Shi et al. (2017a). For this model, we also implement a greedy incremental version. The edge-factored non-projective maximal spanning tree (MST) parser allows arbitrary non-projective structures. This decoding approach has been shown to be very competitive in parsing non-projective treebanks (McDonald et al., 2005), and was deployed in the top-performing system at the CoNLL 2017 shared task (Dozat et al., 2017). We score each edge individually, with the features being the bi-LSTM vectors th, mu, where h is the head, and m the modifier of the edge. The crossing-sensitive third-order 1EC parser provides a hybrid dynamic program for parsing 1-Endpoint-Crossing non-projective dependency trees with higher-order factorization (Pitler, 2014). Depending on whether an edge is crossed, we can access the modifier’s grandparent g, head h, and sibling si. We take their corresponding bi-LSTM features tg, h, m, siu for scoring each edge. This is a re-implementation of Pitler (2014) with neural scoring functions. M"
P18-1248,P15-1033,0,0.205709,"ed interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibility with rich feature models: incorporation of complex features resu"
P18-1248,C96-1058,0,0.711945,"4 -hybrid, 1EC) and combine them into an ensemble. The average result is competitive with the best CoNLL submis6 Related Work Results on Projective Languages (Table 6) For completeness, we also test our models on the 10 most projective languages that have a single treebank. MH 4 remains the most effective, but by a much smaller margin. Interestingly, MH 3 , which is strictly projective, matches the performance of 1EC; both outperform the fully nonprojective MST by half a point. Exact inference for dependency parsing can be achieved in cubic time if the model is restricted to projective trees (Eisner, 1996). However, nonprojectivity is needed for natural language parsers to satisfactorily deal with linguistic phenomena like topicalization, scrambling and extraposition, which cause crossing dependencies. In UD 2.0, 68 out of 70 treebanks were reported to contain 2670 Same Model Architecture MST MH 4 -hybrid Lan. MH 3 eu ur got hu cu da el hi de ro 78.17˘0.33 80.91˘0.10 67.10˘0.10 76.09˘0.25 71.28˘0.29 80.00˘0.15 85.89˘0.29 89.88˘0.18 76.23˘0.21 83.53˘0.35 79.90˘0.08 80.05˘0.13 67.26˘0.45 75.79˘0.36 72.18˘0.20 79.69˘0.24 85.48˘0.25 89.93˘0.12 75.99˘0.23 82.73˘0.36 80.22˘0.48 80.69˘0.19 67.92˘0.29"
P18-1248,P17-1027,1,0.899815,"Missing"
P18-1248,J16-4008,1,0.824047,"iation for Computational Linguistics projective decoder has been known for several years (Cohen et al., 2011), corresponding to a variant of the transition-based parser of Attardi (2006). However, its Opn7 q runtime — or the Opn6 q of a recently introduced improvedcoverage variant (Shi et al., 2018) — is still prohibitively costly in practice. Instead, we seek a more efficient algorithm to adapt, and thus develop a transition-based interpretation of GómezRodríguez et al.’s (2011) MH 4 dynamic programming parser, which has been shown to provide very good non-projective coverage in Opn4 q time (Gómez-Rodríguez, 2016). While the MH 4 parser was originally presented as a non-projective generalization of the dynamic program that later led to the arc-hybrid transition system (GómezRodríguez et al., 2008; Kuhlmann et al., 2011), its own relation to transition-based parsing was not known. Here, we show that MH 4 can be interpreted as exploring a subset of the search space of a transition-based parser that generalizes the arc-hybrid system, under a mapping that differs from the “push computation” paradigm used by the previously-known dynamic-programming decoders for transition systems. This allows us to extend S"
P18-1248,P08-1110,1,0.84519,"Missing"
P18-1248,J11-3004,1,0.88251,"Missing"
P18-1248,E09-1034,1,0.733544,"ency trees that allow only restricted forms of non-projectivity. A number of such sets, called mildly non-projective classes of trees, have been identified that both exhibit good empirical coverage of the non-projective phenomena found in natural languages and are known to have polynomial-time exact parsing algorithms; see Gómez-Rodríguez (2016) for a survey. However, most of these algorithms have not been implemented in practice due to their prohibitive complexity. For example, Corro et al. (2016) report an implementation of the WG1 parser, a Opn7 q mildly non-projective parser introduced in Gómez-Rodríguez et al. (2009), but it could not be run for real sentences of length greater than 20. On the other hand, Pitler et al. (2012) provide an implementation of an Opn5 q parser for a mildly non-projective class of structures called gap-minding trees, but they need to resort to aggressive pruning to make it practical, exploring only a part of the search space in Opn4 q time. To the best of our knowledge, the only practical system that actually implements exact inference for mildly non-projective parsing is the 1Endpoint-Crossing (1EC) parser of Pitler (2013; 2014), which runs in Opn4 q worst-case time like the MH"
P18-1248,P10-1110,0,0.0782498,"Missing"
P18-1248,P98-1106,0,0.435379,"Missing"
P18-1248,Q16-1023,0,0.0411972,"items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibility with rich feature models: incorporation of complex features resulted in jumps in asymptotic runtime complexity to imp"
P18-1248,P11-1068,1,0.80246,"ieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibility with rich feature models: incorporation of complex features resulted in jumps in asymptotic runtime complexity to impractical levels. However, the recent popularization of bidirectional long-short term memory networks (biLSTMs; Hochreiter and Schmidhuber, 1997) to derive feature representations for parsing, given their capacity to capture long-range information, has demonstrated that one may not need to use complex feature models to obtain good accuracy (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016)."
P18-1248,Q17-1031,0,0.0182093,"oting that 1EC has recently been ex6 Corro et al. (2016) describe a parser that enforces mildly non-projective constraints (bounded block degree and wellnestedness), but it is an arc-factored model, so it is subject to the same strong independence assumptions as maximumspanning-tree parsers like McDonald et al. (2005) and does not support the greater flexibility in scoring that is the main advantage of mildly non-projective parsers over these. Instead, mild non-projectivity is exclusively used as a criterion to discard nonconforming trees. tended to graph parsing by Kurtz and Kuhlmann (2017), Kummerfeld and Klein (2017), and Cao et al. (2017a,b), with the latter providing a practical implementation of a parser for 1-EndpointCrossing, pagenumber-2 graphs. 7 Conclusion We have extended the parsing architecture of Shi et al. (2017a) to non-projective dependency parsing by implementing the MH 4 parser, a mildly non-projective Opn4 q chart parsing algorithm, using a minimal set of transition-based bi-LSTM features. For this purpose, we have established a mapping between MH 4 items and transition sequences of an underlying non-projective transition-based parser. To our knowledge, this is the first practical implem"
P18-1248,W17-6312,0,0.175491,"usive margin. It is worth noting that 1EC has recently been ex6 Corro et al. (2016) describe a parser that enforces mildly non-projective constraints (bounded block degree and wellnestedness), but it is an arc-factored model, so it is subject to the same strong independence assumptions as maximumspanning-tree parsers like McDonald et al. (2005) and does not support the greater flexibility in scoring that is the main advantage of mildly non-projective parsers over these. Instead, mild non-projectivity is exclusively used as a criterion to discard nonconforming trees. tended to graph parsing by Kurtz and Kuhlmann (2017), Kummerfeld and Klein (2017), and Cao et al. (2017a,b), with the latter providing a practical implementation of a parser for 1-EndpointCrossing, pagenumber-2 graphs. 7 Conclusion We have extended the parsing architecture of Shi et al. (2017a) to non-projective dependency parsing by implementing the MH 4 parser, a mildly non-projective Opn4 q chart parsing algorithm, using a minimal set of transition-based bi-LSTM features. For this purpose, we have established a mapping between MH 4 items and transition sequences of an underlying non-projective transition-based parser. To our knowledge, this"
P18-1248,H05-1066,0,0.538094,"are trained with the cost-augmented large-margin loss function. The MH 3 parser is the projective instantiation of the MH k parser family. This corresponds to the global version of the arc-hybrid transition system (Kuhlmann et al., 2011). We adopt the minimal feature representation ts0 , b0 u, following Shi et al. (2017a). For this model, we also implement a greedy incremental version. The edge-factored non-projective maximal spanning tree (MST) parser allows arbitrary non-projective structures. This decoding approach has been shown to be very competitive in parsing non-projective treebanks (McDonald et al., 2005), and was deployed in the top-performing system at the CoNLL 2017 shared task (Dozat et al., 2017). We score each edge individually, with the features being the bi-LSTM vectors th, mu, where h is the head, and m the modifier of the edge. The crossing-sensitive third-order 1EC parser provides a hybrid dynamic program for parsing 1-Endpoint-Crossing non-projective dependency trees with higher-order factorization (Pitler, 2014). Depending on whether an edge is crossed, we can access the modifier’s grandparent g, head h, and sibling si. We take their corresponding bi-LSTM features tg, h, m, siu fo"
P18-1248,W07-2216,0,0.0933426,"Missing"
P18-1248,E06-1011,0,0.160259,"languages (sorted by projective ratio; ja (Japanese) is fully projective). non-projectivity (Wang et al., 2017). However, exact inference has been shown to be intractable for models that support arbitrary nonprojectivity, except under strong independence assumptions (McDonald and Satta, 2007). Thus, exact inference parsers that support unrestricted non-projectivity are limited to edge-factored models (McDonald et al., 2005; Dozat et al., 2017). Alternatives include treebank transformation and pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005), approximate inference (e.g. McDonald and Pereira (2006); Attardi (2006); Nivre (2009); Fernández-González and Gómez-Rodríguez (2017)) or focusing on sets of dependency trees that allow only restricted forms of non-projectivity. A number of such sets, called mildly non-projective classes of trees, have been identified that both exhibit good empirical coverage of the non-projective phenomena found in natural languages and are known to have polynomial-time exact parsing algorithms; see Gómez-Rodríguez (2016) for a survey. However, most of these algorithms have not been implemented in practice due to their prohibitive complexity. For example, Corro et"
P18-1248,P09-1040,0,0.082074,"panese) is fully projective). non-projectivity (Wang et al., 2017). However, exact inference has been shown to be intractable for models that support arbitrary nonprojectivity, except under strong independence assumptions (McDonald and Satta, 2007). Thus, exact inference parsers that support unrestricted non-projectivity are limited to edge-factored models (McDonald et al., 2005; Dozat et al., 2017). Alternatives include treebank transformation and pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005), approximate inference (e.g. McDonald and Pereira (2006); Attardi (2006); Nivre (2009); Fernández-González and Gómez-Rodríguez (2017)) or focusing on sets of dependency trees that allow only restricted forms of non-projectivity. A number of such sets, called mildly non-projective classes of trees, have been identified that both exhibit good empirical coverage of the non-projective phenomena found in natural languages and are known to have polynomial-time exact parsing algorithms; see Gómez-Rodríguez (2016) for a survey. However, most of these algorithms have not been implemented in practice due to their prohibitive complexity. For example, Corro et al. (2016) report an implemen"
P18-1248,P05-1013,0,0.148399,"Missing"
P18-1248,C04-1010,0,0.0771663,"th minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibilit"
P18-1248,Q14-1004,0,0.562421,"ers from the “push computation” paradigm used by the previously-known dynamic-programming decoders for transition systems. This allows us to extend Shi et al. (2017a)’s work to non-projective parsing, by implementing MH 4 with a minimal set of transition-based features. Experimental results show that our approach outperforms the projective approach of Shi et al. (2017a) and maximum-spanning-tree nonprojective parsing on the most highly nonprojective languages in the CoNLL 2017 sharedtask data that have a single treebank. We also compare with the third-order 1-Endpoint-Crossing (1EC) parser of Pitler (2014), the only other practical implementation of an exact mildly nonprojective decoder that we know of, which also runs in Opn4 q but without a transition-based interpretation. We obtain comparable results for these two algorithms, in spite of the fact that the MH 4 algorithm is notably simpler than 1EC. The MH 4 parser remains effective in parsing projective treebanks, while our baseline parser, the fully non-projective maximum spanning tree algorithm, falls behind due to its unnecessarily large search space in parsing these languages. Our code, including our re-implementation of the third-order"
P18-1248,D12-1044,0,0.0208706,"sses of trees, have been identified that both exhibit good empirical coverage of the non-projective phenomena found in natural languages and are known to have polynomial-time exact parsing algorithms; see Gómez-Rodríguez (2016) for a survey. However, most of these algorithms have not been implemented in practice due to their prohibitive complexity. For example, Corro et al. (2016) report an implementation of the WG1 parser, a Opn7 q mildly non-projective parser introduced in Gómez-Rodríguez et al. (2009), but it could not be run for real sentences of length greater than 20. On the other hand, Pitler et al. (2012) provide an implementation of an Opn5 q parser for a mildly non-projective class of structures called gap-minding trees, but they need to resort to aggressive pruning to make it practical, exploring only a part of the search space in Opn4 q time. To the best of our knowledge, the only practical system that actually implements exact inference for mildly non-projective parsing is the 1Endpoint-Crossing (1EC) parser of Pitler (2013; 2014), which runs in Opn4 q worst-case time like the MH 4 algorithm used in this paper. Thus, the system presented here is the second practical implementation of exac"
P18-1248,Q13-1002,0,0.104846,"Missing"
P18-1248,N15-1068,0,0.107247,"set of trees covered by MH 4 has not been characterized with a non-operational definition, while the set of 1-Endpoint-Crossing trees can be simply defined. However, it also has the following advantages: (+1) It can be given a transition-based interpretation, allowing us to use transition-based scoring functions and to implement the analogous algorithm with greedy or beam search apart from exact inference. No transition-based interpretation is known for 1EC. While a transition-based algorithm has been defined for a strict subset of 1-Endpoint-Crossing trees, called 2-Crossing Interval trees (Pitler and McDonald, 2015), this is a separate algorithm with no known mapping or relation to 1EC or any other dynamic programming model. Thus, we provide the first exact inference algorithm for a non-projective transitionbased parser with practical complexity. (+2) It is conceptually much simpler, with one kind of item and two deduction steps, while the 1-EndpointCrossing parser has five classes of items and several dozen distinct deduction steps. It is also a purely bottom-up parser, whereas the 1-EndpointCrossing parser does not have the bottom-up property. This property is necessary for models that involve composit"
P18-1248,N18-2067,1,0.894022,"of exact decoding for a non-projective transition-based parser. As in the projective case, a mildly non1 http://universaldependencies.org/ 2664 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2664–2675 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics projective decoder has been known for several years (Cohen et al., 2011), corresponding to a variant of the transition-based parser of Attardi (2006). However, its Opn7 q runtime — or the Opn6 q of a recently introduced improvedcoverage variant (Shi et al., 2018) — is still prohibitively costly in practice. Instead, we seek a more efficient algorithm to adapt, and thus develop a transition-based interpretation of GómezRodríguez et al.’s (2011) MH 4 dynamic programming parser, which has been shown to provide very good non-projective coverage in Opn4 q time (Gómez-Rodríguez, 2016). While the MH 4 parser was originally presented as a non-projective generalization of the dynamic program that later led to the arc-hybrid transition system (GómezRodríguez et al., 2008; Kuhlmann et al., 2011), its own relation to transition-based parsing was not known. Here,"
P18-1248,D17-1002,1,0.905144,"Missing"
P18-1248,K17-3003,1,0.634683,"). While the MH 4 parser was originally presented as a non-projective generalization of the dynamic program that later led to the arc-hybrid transition system (GómezRodríguez et al., 2008; Kuhlmann et al., 2011), its own relation to transition-based parsing was not known. Here, we show that MH 4 can be interpreted as exploring a subset of the search space of a transition-based parser that generalizes the arc-hybrid system, under a mapping that differs from the “push computation” paradigm used by the previously-known dynamic-programming decoders for transition systems. This allows us to extend Shi et al. (2017a)’s work to non-projective parsing, by implementing MH 4 with a minimal set of transition-based features. Experimental results show that our approach outperforms the projective approach of Shi et al. (2017a) and maximum-spanning-tree nonprojective parsing on the most highly nonprojective languages in the CoNLL 2017 sharedtask data that have a single treebank. We also compare with the third-order 1-Endpoint-Crossing (1EC) parser of Pitler (2014), the only other practical implementation of an exact mildly nonprojective decoder that we know of, which also runs in Opn4 q but without a transition-"
P18-1248,L16-1680,0,0.0139431,"5 Experiments Data and Evaluation We experiment with the Universal Dependencies (UD) 2.0 dataset used for the CoNLL 2017 shared task (Zeman et al., 2017). We restrict our choice of languages to be those with only one training treebank, for a better comparison with the shared task results.5 Among these languages, we pick the top 10 most non-projective languages. Their basic statistics are listed in Table 3. For all development-set results, we assume gold-standard tokenization and sentence delimitation. When comparing to the shared task results on test sets, we use the provided baseline UDPipe (Straka et al., 2016) segmentation. Our models do not use part-of-speech tags or morphological tags as features, but rather leverage such information via stack propagation (Zhang and Weiss, 2016), i.e., we learn to predict them as a secondary training objective. We report unlabeled attachment F1scores (UAS) on the development sets for better focus on comparing our (unlabeled) parsing modules. We report its labeled variant (LAS), the main metric of the shared task, on the test sets. For each experiment setting, we ran the model with 5 different random initializations, and report the mean and standard deviation. We"
P18-1248,K17-3020,0,0.0222026,"Missing"
P18-1248,W03-3023,0,0.328263,". To make MH 4 compatible with minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due"
P18-1248,P16-1147,0,0.0903915,"Missing"
P18-1248,P11-2033,0,0.0505462,"sed feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibility with rich feature mod"
P93-1024,A88-1019,0,0.0266565,"between them, in our experiments the relation between a transitive main verb and the head noun of its direct object. Our raw knowledge about the relation consists of the frequencies f~n of occurrence of particular pairs (v,n) in the required configuration in a training corpus. Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire text automatically parsed by 183 Hindle&apos;s parser Fidditch (Hindle, 1993). More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992). We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to filter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like ""say"". We will consider here only the problem of classifying nouns according to their distribution as direct objects of verbs; the converse problem is formally similar. More generally, the theoretical bas"
P93-1024,P93-1022,0,0.0456394,"e models reconstruct missing d a t a in the 1 awe use unweighted averages because we are interested her on how well the noun distributions are approximated by the cluster model. If we were interested on the total information loss of using the asymmetric model to encode a test corpus, we would instead use the weighted average ~,~e~t fnD(t,~ll~,,) where f,, is the relative frequency of n in the test set. 189 verb distribution for n from the cluster centroids close to n. The d a t a for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993). 104 noun-verb pairs with a fairly frequent verb (between 500 and 5000 occurrences) were randomly picked, and all occurrences of each pair in the training set were deleted. The resulting training set was used to build a sequence of cluster models as before. Each model was used to decide which of two verbs v and v ~ are more likely to appear with a noun n where the (v, n) d a t a was deleted from the training set, and the decisions were compared with the corresponding ones derived from the original event frequencies in the initial d a t a set. The error rate for each model is simply the propor"
P93-1024,P90-1034,0,0.658019,"s proposed by a grammar. It is well known that a simple tabulation of frequencies of certain words participating in certain configurations, for example of frequencies of pairs of a transitive main verb and the head noun of its direct object, cannot be reliably used for comparing the likelihoods of different alternative configurations. The problemis that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities. Hindle (1990) proposed dealing with the Problem Setting In what follows, we will consider two major word classes, 12 and Af, for the verbs and nouns in our experiments, and a single relation between them, in our experiments the relation between a transitive main verb and the head noun of its direct object. Our raw knowledge about the relation consists of the frequencies f~n of occurrence of particular pairs (v,n) in the required configuration in a training corpus. Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire"
P93-1024,C92-2066,0,\N,Missing
P93-1024,J92-4003,0,\N,Missing
P94-1038,J92-4003,0,0.0475503,"Missing"
P94-1038,P93-1005,0,0.00927013,"Finally, the similarity-based model m a y be applied to configurations other than bigrams. For trigrams, it is necessary to measure similarity between different conditioning bigrams. This can be done directly, 276 by measuring the distance between distributions of the form P(w31wl, w2), corresponding to different bigrams (wl, w~). Alternatively, and more practically, it would be possible to define a similarity measure between bigrams as a function of similarities between corresponding words in them. Other types of conditional cooccurrence probabilities have been used in probabilistic parsing (Black et al., 1993). If the configuration in question includes only two words, such as P(objectlverb), then it is possible to use the model we have used for bigrams. If the configuration includes more elements, it is necessary to adjust the method, along the lines discussed above for trigrams. for help with his baseline back-off model, and Andre Ljolje and Michael Riley for providing the word lattices for our experiments. Conclusions Similarity-based models suggest an appealing approach for dealing with data sparseness. Based on corpus statistics, they provide analogies between words that often agree with our li"
P94-1038,P93-1022,1,0.847024,"Missing"
P94-1038,H92-1021,0,0.4253,"not possible to estimate probabilities from observed frequencies, a n d s o m e other estimation scheme has to be used. We focus here on a particular kind of configuration, word cooccurrence. Examples of such cooccurrences include relationships between head words in syntactic constructions (verb-object or adjective-noun, for example) and word sequences (n-grams). In commonly used models, the probability estimate for a previously unseen cooccurrence is a function of the probability esti272 Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss (1992), derived from work on acoustic model smoothing by Sugawara et al. (1985). We present a different method that takes as starting point the back-off scheme of Katz (1987). We first allocate an appropriate probability mass for unseen cooccurrences following the back-off method. Then we redistribute that mass to unseen cooccurrences according to an averaged cooccurrence distribution of a set of most similar conditioning words, using relative entropy as our similarity measure. This second step replaces the use of the independence assumption in the original back-off model. We applied our method to e"
P94-1038,H93-1050,0,0.0786007,"Missing"
P94-1038,H91-1055,0,0.0409352,"Missing"
P94-1038,P93-1024,1,0.855009,"Missing"
P94-1038,H92-1027,0,0.0497973,"Missing"
P94-1038,C92-2066,0,\N,Missing
P97-1002,P81-1022,0,0.221406,"w can be used to multiply m x m Boolean matrices in time O(m3-e/3). In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang. Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist. 1 Introduction The context-free grammar (CFG) formalism was developed during the birth of the field of computational linguistics. The standard methods for CFG parsing are the CKY algorithm (Kasami, 1965; Younger, 1967) and Earley&apos;s algorithm (Earley, 1970), both of which have a worst-case running time of O(gN 3) for a CFG (in Chomsky normal form) of size g and a string of length N. Graham et al. (1980) give a variant of Earley&apos;s algorithm which runs in time O(gN3/log N). Valiant&apos;s parsing method is the asymptotically fastest known (Valiant, 1975). It uses Boolean matrix multiplication (BMM) to speed up the dynamic programming in the CKY algorithm: its worst-case running time is O(gM(N)), where M(rn) is the time it takes to multiply two m x m Boolean matrices together. The standard method for multiplying matrices takes time O(m3). There exist ma"
P97-1002,J94-2002,0,0.353109,"ix multiplication, has worst-case running time O(mS/logm). for all possible parses of a string (after all, the recovery of structural information is what distinguishes parsing algorithms from recognition algorithms); such information is very useful for applications like natural language understanding, where multiple interpretations for a sentence may result from different constituent structures. Therefore, practical parsers should keep track of c-derivations. Secondly, a parser should create an output structure from which information about constituents can be retrieved in an efficient way - - Satta (1994) points out an observation of Lang to the effect that one can consider the input string itself to be a retrievalinefficient representation of parse information. In short, we require practical parsers to output a representation of the parse forest for a string that allows efficient retrieval of parse information. Lang in fact argues that parsing means exactly the production of a shared forest structure &quot;from which any specific parse can be extracted in time linear with the size of the extracted parse tree&quot; (Lang, 1994, pg. 487), and Satta (1994) makes this assumption as well. These notions lead"
P97-1008,J92-4003,0,0.0417534,"Missing"
P97-1008,A88-1019,0,0.0192643,"Missing"
P97-1008,P94-1038,1,0.513543,"e unigram probability (1 KL divergence Kullback-Leibler (KL) divergence is a standard information-theoretic measure of the dissimilarity between two probability mass functions (Cover and Thomas, 1991). We can apply it to the conditional distribution P(.[wl) induced by Wl on words in V2: ~V(Wl, W~) ~ ~s(~1 ) P,.(w2lwl) = 7P(w2) + Measures of Similarity T o t a l d i v e r g e n c e to t h e a v e r a g e A related measure is based on the total KL divergence to the average of the two distributions: A(wx, W11)= D (w, wl + ) + D (w~[ wl + w~) 2 7)PsiM(W2lWl), where 7 is determined experimentally (Dagan, Pereira, and Lee, 1994). This represents, in effect, a linear combination of the similarity es58 where tion (Wl ÷ w~)/2 shorthand for the distribu2.3.4 Essen and Steinbiss (1992) introduced confusion probability 2, which estimates the probabil(P(.IwJ + P(.Iw~)) ity that word w~ can be substituted for word Wl: Since D('II-) > O, A(Wl,W~) >_O. Furthermore, letting p(w2) = P(w2[wJ, p'(w2) = P(w2lw~) and C : {w2 : p(w2) > O,p'(w2) > O}, it is straightforward to show by grouping terms appropriately that Pc(w lWl) = w(wl, = ~ , P(wllw2)P(w~[w2)P(w2) w2 P(Wl) A(wi,wb= Unlike the measures described above, wl may not neces"
P97-1008,H92-1021,0,0.12642,"defined it must be the case that P(w2]w~l) > 0 whenever P(w21wl) > 0. Unfortunately, this will not in general be the case for MLEs based on samples, so we would need smoothed estimates of P(w2]w~) that redistribute some probability mass to zerofrequency events. However, using smoothed estimates for P(w2[wl) as well requires a sum over all w2 6 172, which is expensive ['or the large vocabularies under consideration. Given the smoothed denominator distribution, we set Considerable latitude is allowed in defining the set $(Wx), as is evidenced by previous work that can be put in the above form. Essen and Steinbiss (1992) and Karov and Edelman (1996) (implicitly) set 8(wl) = V1. However, it may be desirable to restrict ,5(wl) in some fashion, especially if 1/1 is large. For instance, Dagan. Pereira, and Lee (1994) use the closest k or fewer words w~ such that the dissimilarity between wl and w~ is less than a threshold value t; k and t are tuned experimentally. Now, we could directly replace P,.(w2[wl) in the back-off equation (2) with PSIM(W21Wl). However, other variations a r e possible, such as interpolating with the unigram probability (1 KL divergence Kullback-Leibler (KL) divergence is a standard informa"
P97-1008,W96-0104,0,0.0133278,"at P(w2]w~l) > 0 whenever P(w21wl) > 0. Unfortunately, this will not in general be the case for MLEs based on samples, so we would need smoothed estimates of P(w2]w~) that redistribute some probability mass to zerofrequency events. However, using smoothed estimates for P(w2[wl) as well requires a sum over all w2 6 172, which is expensive ['or the large vocabularies under consideration. Given the smoothed denominator distribution, we set Considerable latitude is allowed in defining the set $(Wx), as is evidenced by previous work that can be put in the above form. Essen and Steinbiss (1992) and Karov and Edelman (1996) (implicitly) set 8(wl) = V1. However, it may be desirable to restrict ,5(wl) in some fashion, especially if 1/1 is large. For instance, Dagan. Pereira, and Lee (1994) use the closest k or fewer words w~ such that the dissimilarity between wl and w~ is less than a threshold value t; k and t are tuned experimentally. Now, we could directly replace P,.(w2[wl) in the back-off equation (2) with PSIM(W21Wl). However, other variations a r e possible, such as interpolating with the unigram probability (1 KL divergence Kullback-Leibler (KL) divergence is a standard information-theoretic measure of the"
P97-1008,P93-1024,1,0.599038,"ge model consists of three parts: a scheme for deciding which word pairs require a similarity-based estimate, a method for combining information from similar words, and, of course, a function measuring the similarity between words. We give the details of each of these three parts in the following three sections. We will only be concerned with similarity between words in V1. 1To the best of our ""knowledge, this is the first use of this particular distribution dissimilarity function in statistical language processing. The function itself is implicit in earlier work on distributional clustering (Pereira, Tishby, and Lee, 1993}, has been used by Tishby (p.e.) in other distributional similarity work, and, as suggested by Yoav Freund (p.c.), it is related to results of Hoeffding (1965) on the probability that a given sample was drawn from a given joint distribution. 57 2.1 Discounting and Redistribution Data sparseness makes the maximum likelihood estimate (MLE) for word pair probabilities unreliable. The MLE for the probability of a word pair (Wl, w2), conditional on the appearance of word wl, is simply PML(W2[wl) -- c(wl, w2) c( i) (1) where c(wl, w2) is the frequency of (wl, w2) in the training corpus and c(wl) is"
P97-1008,P92-1053,0,\N,Missing
P99-1004,H92-1021,0,0.0608025,"Missing"
P99-1004,P93-1023,0,0.00803945,"Missing"
P99-1004,P90-1034,0,0.852591,"0. The intuition behind Kendall&apos;s T is as follows. Assume all verbs have distinct conditional probabilities. If sorting the verbs by the likelihoods assigned by q yields exactly the same ordering as that which results from ranking them according to r, then T(q, r) = 1; if it yields exactly the opposite ordering, then T(q, r) -- - 1 . We treat a value of - 1 as indicating extreme dissimilarity. 3 It is worth noting at this point that there are several well-known measures from the NLP literature that we have omitted from our experiments. Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a). It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(VIn ) and P(VIm)) , but rather the similarity between a joint distribution P(X1,X2) and the corresponding product distribution P(X1)P(X2). Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature Values that are probabilities. Variations of the value d"
P99-1004,J98-1002,0,0.0178255,"Missing"
P99-1004,P97-1009,0,0.0682199,"efficient does take this type of information into account. Our explicit division of V-space into various support regions has been implicitly considered in other work. Smadja et al. (1996) observe that for two potential mutual translations X and Y, the fact that X occurs with translation Y indicates association; X &apos; s occurring with a translation other t h a n Y decreases one&apos;s belief in their association; but the absence of both X and Y yields no information. In essence, Smadja et al. argue that information from the union of supports, rather t h a n the just the intersection, is important. D. Lin (1997; 1998a) takes an axiomatic approach to determining the characteristics of a good similarity measure. Starting with a formalization (based on certain assumptions) of the intuition that the similarity between two events depends on both their commonality and their differences, he derives a unique similarity function schema. T h e The Skew Divergence Based on the results just described, it appears that it is desirable to have a similarity function that focuses on the verbs that cooccur with both of the nouns being compared. However, we can make a further observation: with the exception of the con"
P99-1004,P95-1025,0,0.01209,"all verbs have distinct conditional probabilities. If sorting the verbs by the likelihoods assigned by q yields exactly the same ordering as that which results from ranking them according to r, then T(q, r) = 1; if it yields exactly the opposite ordering, then T(q, r) -- - 1 . We treat a value of - 1 as indicating extreme dissimilarity. 3 It is worth noting at this point that there are several well-known measures from the NLP literature that we have omitted from our experiments. Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a). It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(VIn ) and P(VIm)) , but rather the similarity between a joint distribution P(X1,X2) and the corresponding product distribution P(X1)P(X2). Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature Values that are probabilities. Variations of the value difference metric (Stanfill and Waltz, 1986) have been e"
P99-1004,P96-1006,0,0.0107449,"Missing"
P99-1004,W97-0323,0,0.0179004,"ause it does not measure the similarity between two arbitrary probability distributions (in our case, P(VIn ) and P(VIm)) , but rather the similarity between a joint distribution P(X1,X2) and the corresponding product distribution P(X1)P(X2). Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature Values that are probabilities. Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for supervised disambiguation (Ng and H.B. Lee, 1996; Ng, 1997); but it is not reasonable in language modeling to expect training data tagged with correct probabilities. The Dice coej~cient (Smadja et al., 1996; D. Lin, 1998a, 1998b) is monotonic in Jaccard&apos;s coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant. Finally, we did not use the KL divergence because it requires a smoothed base language model. SZero would also be a reasonable choice, since it indicates zero correlation between q and r. However, it would then not be clear how to average in the estimates of negatively correlated words in equation (1). 27 3 be"
P99-1004,J96-1001,0,0.0406058,"e similarity between a joint distribution P(X1,X2) and the corresponding product distribution P(X1)P(X2). Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature Values that are probabilities. Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for supervised disambiguation (Ng and H.B. Lee, 1996; Ng, 1997); but it is not reasonable in language modeling to expect training data tagged with correct probabilities. The Dice coej~cient (Smadja et al., 1996; D. Lin, 1998a, 1998b) is monotonic in Jaccard&apos;s coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant. Finally, we did not use the KL divergence because it requires a smoothed base language model. SZero would also be a reasonable choice, since it indicates zero correlation between q and r. However, it would then not be clear how to average in the estimates of negatively correlated words in equation (1). 27 3 behavior. Because we have a binary decision task, Ef,k(n, vl) simply counts the number of k nearest neighbors to n that make the right decision. If w"
P99-1004,P97-1056,0,0.0119353,"rth noting at this point that there are several well-known measures from the NLP literature that we have omitted from our experiments. Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a). It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(VIn ) and P(VIm)) , but rather the similarity between a joint distribution P(X1,X2) and the corresponding product distribution P(X1)P(X2). Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature Values that are probabilities. Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for supervised disambiguation (Ng and H.B. Lee, 1996; Ng, 1997); but it is not reasonable in language modeling to expect training data tagged with correct probabilities. The Dice coej~cient (Smadja et al., 1996; D. Lin, 1998a, 1998b) is monotonic in Jaccard&apos;s coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant. Finally,"
P99-1004,J90-1003,0,\N,Missing
P99-1004,H93-1050,0,\N,Missing
P99-1004,A88-1019,0,\N,Missing
P99-1004,J93-1006,0,\N,Missing
P99-1004,P98-2127,0,\N,Missing
P99-1004,C98-2122,0,\N,Missing
P99-1005,J92-4003,0,0.080162,"Missing"
P99-1005,J98-1001,0,0.0417837,"Missing"
P99-1005,P99-1004,1,0.760101,"pression, or equivalently the amount of generalization, produced by the two models. Clusterp(vln) = ~-]p(vlc)p(cln) c • nearest-cluster estimate: ~(vln) -- p(vlc*), where c* maximizes p(c*ln). 2.2 N e a r e s t - n e i g h b o r s a v e r a g i n g As noted earlier, the nearest-neighbors averaging method is an alternative to clustering for estimating the probabilities of unseen cooccurfences. Given an unseen pair (n, v), we calculate an estimate 15(vln ) as an appropriate average of p(vln I) where n I is distributionally similar to n. Many distributional similarity measures can be considered (Lee, 1999). In this paper, we focus on the one that gave the best results in our earlier work (Dagan et al., 1999), the Jensen-Shannon divergence (Rao, 1982; Lin, 1991). The Jensen-Shannon divergence of two discrete distributions p and q over the same domain is defined as 1 gS(p, q) = ~ It is easy to see that JS(p, q) is always defined. In previous work, we used the estimate ~5(vln) = 1 ~ p(vln,)exp(_Zj(n,n,)), (In nlES(n,k) where J(n,n') = JS (p(VIn),p(Yln')), Z and k are tunable parameters, S(n, k) is the set of k nouns with the smallest Jensen-Shannon divergence to n, and an is a normalization term."
P99-1005,P93-1024,1,0.917998,"r concreteness and experimental evaluation, we focus in this paper on a particular type of cooccurrence, t h a t of a main verb and the head noun of its direct object in English text. Our main goal is to obtain estimates ~(vln ) of the conditional probability of a main verb v given a direct object head noun n, which can t h e n be used in particular prediction tasks. In previous work, we and our co-authors have proposed two different probability estimation methods that incorporate word similarity information: distributional clustering and nearestneighbors averaging. Distributional clustering (Pereira et al., 1993) assigns to each word a probability distribution over clusters to which it m a y belong, and characterizes each cluster by a centroid, which is an average of cooccurrence distributions of words weighted according to cluster membership probabilities. Cooccurrence probabilities can then be derived from either a membership-weighted average of the clusters to which the words in the cooccurrence belong, or just from the highest-probability cluster. In contrast, nearest-neighbors averaging 1 (Dagan et al., 1999) does not explicitly cluster words. Rather, a given cooccurrence probability is estimated"
W02-0105,J86-3001,0,0.0627532,"for constituents. To describe this structure, we formally defined context-free grammars. We then showed how (a tiny fragment of) X-bar theory can be modeled by a context-free grammar and, using its structural assignments and the notion of heads of constituents, accounted for some of the ambiguities and non-ambiguities in the linguistic examples we previously examined. The discussion of context-free grammars naturally led us to pushdown automata (which provided a nice contrast to the Turing machines we studied earlier in the course). And, having thus introduced stacks, we then investigated the Grosz and Sidner (1986) stack-based theory of discourse structure, showing that language structures exist at granularities beyond the sentence level. Statistical language processing [6 lectures] We began this unit by considering word frequency distributions, and in particular, Zipf’s law — note that our having studied power-law distributions in the Web unit greatly facilitated this discussion. In fact, because we had previously investigated generative models for the Web, it was natural to consider Miller’s (1957) “monkeys” model which demonstrates that very simple generative models can account for Zipf’s law. Next,"
W02-0105,A00-2032,1,\N,Missing
W02-1011,J96-1002,0,0.148119,"ssumption clearly does not hold in real-world situations, Naive Bayes-based text categorization still tends to perform surprisingly well (Lewis, 1998); indeed, Domingos and Pazzani (1997) show that Naive Bayes is optimal for certain problem classes with highly dependent features. On the other hand, more sophisticated algorithms might (and often do) yield better results; we examine two such algorithms next. 5.2 Maximum Entropy Maximum entropy classification (MaxEnt, or ME, for short) is an alternative technique which has proven effective in a number of natural language processing applications (Berger et al., 1996). Nigam et al. (1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classification. Its estimate of P (c |d) takes the following exponential form: Ã ! X 1 exp λi,c Fi,c (d, c) , PME (c |d) := Z(d) i where Z(d) is a normalization function. Fi,c is a feature/class function for feature fi and class c, defined as follows:6 n 0 Fi,c (d, c0 ) := 1, ni (d) &gt; 0 and c = c . 0 otherwise For instance, a particular feature/class function might fire if and only if the bigram “still hate” appears and the document’s sentiment is hypothesized to be negative.7 Importantly, un"
W02-1011,P97-1023,0,0.148141,"ndicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our specific classification task of determining what that opinion actually is. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002). Past work on sentiment-based categorization of entire documents has often involved either the use of models inspired by cognitive linguistics (Hearst, 1992; Sack, 1994) or the manual or semi-manual construction of discriminant-word lexicons (Huettner and Subasic, 2000; Das and Chen, 2001; Tong, 2001). Interestingly, our baseline experiments, described in Section 4, show that humans may not always have the best intuition for choosing discriminating words. Turney’s (2002) work on classification of reviews is perhaps the closest to ours.2 He applied a specific unsuper"
W02-1011,C00-1044,0,0.148153,"Times vs. The Daily News), native-language background, and “brow” (e.g., high-brow vs. “popular”, or low-brow) (Mosteller and Wallace, 1984; Argamon-Engelson et 1 http://www.mindfuleye.com/about/lexant.htm al., 1998; Tomokiyo and Jones, 2001; Kessler et al., 1997). Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002). Other work explicitly attempts to find features indicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our specific classification task of determining what that opinion actually is. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002). Past work on sentim"
W02-1011,C94-2174,0,0.148116,"source or source style, with statistically-detected stylistic variation (Biber, 1988) serving as an important cue. Examples include author, publisher (e.g., the New York Times vs. The Daily News), native-language background, and “brow” (e.g., high-brow vs. “popular”, or low-brow) (Mosteller and Wallace, 1984; Argamon-Engelson et 1 http://www.mindfuleye.com/about/lexant.htm al., 1998; Tomokiyo and Jones, 2001; Kessler et al., 1997). Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002). Other work explicitly attempts to find features indicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our specific classification task of determining what that opinion actually is. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of i"
W02-1011,P97-1005,0,0.148116,"ult it is. 2 Previous Work This section briefly surveys previous work on nontopic-based text categorization. One area of research concentrates on classifying documents according to their source or source style, with statistically-detected stylistic variation (Biber, 1988) serving as an important cue. Examples include author, publisher (e.g., the New York Times vs. The Daily News), native-language background, and “brow” (e.g., high-brow vs. “popular”, or low-brow) (Mosteller and Wallace, 1984; Argamon-Engelson et 1 http://www.mindfuleye.com/about/lexant.htm al., 1998; Tomokiyo and Jones, 2001; Kessler et al., 1997). Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002). Other work explicitly attempts to find features indicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our specific classification task of determining what that o"
W02-1011,N01-1011,0,0.148116,"rely not conditionally independent, meaning that the feature set they comprise violates Naive Bayes’ conditional-independence assumptions; on the other hand, recall that this does not imply that Naive Bayes will necessarily do poorly (Domingos and Pazzani, 1997). Line (3) of the results table shows that bigram information does not improve performance beyond that of unigram presence, although adding in the bigrams does not seriously impact the results, even for Naive Bayes. This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be effective features for word sense disambiguation. However, comparing line (4) to line (2) shows that relying just on bigrams causes accuracy to decline by as much as 5.8 percentage points. Hence, if context is in fact important, as our intuitions suggest, bigrams are not effective at capturing it in our setting. 11 Alternatively, we could have tried integrating frequency information into MaxEnt. However, feature/class functions are traditionally defined as binary (Berger et al., 1996); hence, explicitly incorporating frequencies would require different function"
W02-1011,N01-1031,0,0.148122,"nderstanding of how difficult it is. 2 Previous Work This section briefly surveys previous work on nontopic-based text categorization. One area of research concentrates on classifying documents according to their source or source style, with statistically-detected stylistic variation (Biber, 1988) serving as an important cue. Examples include author, publisher (e.g., the New York Times vs. The Daily News), native-language background, and “brow” (e.g., high-brow vs. “popular”, or low-brow) (Mosteller and Wallace, 1984; Argamon-Engelson et 1 http://www.mindfuleye.com/about/lexant.htm al., 1998; Tomokiyo and Jones, 2001; Kessler et al., 1997). Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002). Other work explicitly attempts to find features indicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our specific classification task of"
W02-1011,P02-1053,0,0.14813,"12 This serves as a crude form of word sense disambiguation (Wilks and Stevenson, 1998): for example, it would distinguish the different usages of “love” in “I love this movie” (indicating sentiment orientation) versus “This is a love story” (neutral with respect to sentiment). However, the effect of this information seems to be a wash: as depicted in line (5) of Figure 3, the accuracy improves slightly for Naive Bayes but declines for SVMs, and the performance of MaxEnt is unchanged. Since adjectives have been a focus of previous work in sentiment detection (Hatzivassiloglou and Wiebe, 2000; Turney, 2002)13 , we looked at the performance of using adjectives alone. Intuitively, we might expect that adjectives carry a great deal of information regarding a document’s sentiment; indeed, the human-produced lists from Section 4 contain almost no other parts of speech. Yet, the results, shown in line (6) of Figure 3, are relatively poor: the 2633 adjectives provide less useful information than unigram presence. Indeed, line (7) shows that simply using the 2633 most frequent unigrams is a better choice, yielding performance comparable to that of using (the presence of) all 16165 (line (2)). This may i"
W02-1022,C00-1007,0,0.0767618,"Missing"
W02-1022,P01-1008,1,0.671208,"Missing"
W02-1022,J93-2003,0,0.00300923,"Missing"
W02-1022,P98-1116,0,0.091567,"Missing"
W02-1022,J00-2004,0,0.0216146,"Missing"
W02-1022,P00-1056,0,0.0659232,"Missing"
W02-1022,W00-0306,0,0.0156982,"Missing"
W02-1022,P02-1040,0,0.113723,"Missing"
W02-1022,A00-2026,0,0.016196,"Missing"
W02-1022,W99-0602,0,0.0133339,"Missing"
W02-1022,H94-1019,0,\N,Missing
W02-1022,C98-1112,0,\N,Missing
W06-1639,H05-1042,0,0.00429577,"t segments can be modeled as positive-strength links. Here we discuss two types of constraints that are considered in this work. between s,s0 where c(s) is the “opposite” class from c(s). A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. Same-speaker constraints: In Congressional debates and in general social-discourse contexts, a single speaker may make a number of comments regarding a topic. It is reasonable to expect that in many sett"
W06-1639,daelemans-hoste-2002-evaluation,0,0.00990146,"ure (rather than frequency-of-feature) vectors. The ind value 6 We are attempting to determine whether a speech segment represents support or not. This differs from the problem of determining what the speaker’s actual opinion is, a problem that, as an anonymous reviewer put it, is complicated by “grandstanding, backroom deals, or, more innocently, plain change of mind (‘I voted for it before I voted against it’)”. 5 SVMlight is available at svmlight.joachims.org. Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al., 2005). 330 Agreement classifier (“reference⇒agreement?”) majority baseline Train: no amdmts; θagr = 0 Train: with amdmts; θagr = 0 both hard and soft constraints in a principled fashion as an advantage of our approach. Different-speaker agreements In House discourse, it is common for one speaker to make reference to another in the context of an agreement or disagreement over the topic of discussion. The systematic identification of instances of agreement can, as we have discussed, be a powerful tool for the development of intelligently selected weights for links between speech"
W06-1639,P04-1085,0,0.289656,"t the most important observation we can make from Table 5 is that once again, the addition of agreement information leads to substantial improvements in accuracy. we believe, is our most important finding. 4.4 “Hard” agreement constraints 4.5 On the development/test set split Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. We have seen several cases in which the method that performs best on the development set does not yield the best test-set performance. However, we felt that it would be illegitimate to change the train/development/tes"
W06-1639,W06-3808,0,0.166034,"aphy). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement beQualitative summary of results The above difficulties underscore the importance of enhancing standard classification techniques with new information sources that promise to improve accuracy, such as inter-document relationships between the documents to be labeled. In this paper, we demonstrate that the incorporation of agreement modeling can provide substantial impr"
W06-1639,N03-2012,0,0.175548,"on overall classification accuracy is less clear: the development set favors samespeaker links over concatenation, while the test set does not. But we stress that the most important observation we can make from Table 5 is that once again, the addition of agreement information leads to substantial improvements in accuracy. we believe, is our most important finding. 4.4 “Hard” agreement constraints 4.5 On the development/test set split Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. We have seen several cases in which the method that perfo"
W06-1639,H05-1068,0,0.00855045,"of-feature) vectors. The ind value 6 We are attempting to determine whether a speech segment represents support or not. This differs from the problem of determining what the speaker’s actual opinion is, a problem that, as an anonymous reviewer put it, is complicated by “grandstanding, backroom deals, or, more innocently, plain change of mind (‘I voted for it before I voted against it’)”. 5 SVMlight is available at svmlight.joachims.org. Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al., 2005). 330 Agreement classifier (“reference⇒agreement?”) majority baseline Train: no amdmts; θagr = 0 Train: with amdmts; θagr = 0 both hard and soft constraints in a principled fashion as an advantage of our approach. Different-speaker agreements In House discourse, it is common for one speaker to make reference to another in the context of an agreement or disagreement over the topic of discussion. The systematic identification of instances of agreement can, as we have discussed, be a powerful tool for the development of intelligently selected weights for links between speech segments. The problem"
W06-1639,P04-1035,1,0.0511309,"ch segments A wide range of relationships between text segments can be modeled as positive-strength links. Here we discuss two types of constraints that are considered in this work. between s,s0 where c(s) is the “opposite” class from c(s). A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes. As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision. Same-speaker constraints: In Congressional debates and in general social-discourse contexts, a single speaker may make a number of comments re"
W06-1639,P05-1015,1,0.458992,"r an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement beQualitative summary of results The above difficulties underscore the importance of enhancing standard classification techniques with new information sources that promise to improve accuracy, such as inter-document relationships between the documents to be labeled. In this paper, we demonstrate that the incorporation of agreement modeling can"
W06-1639,W02-1011,1,0.039219,"aker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement beQualitative summary of results The above d"
W06-1639,J00-3003,0,0.153985,"Missing"
W06-1639,J02-4002,0,0.0541503,"4 “Hard” agreement constraints 4.5 On the development/test set split Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement. More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004). Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002). We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work. We have seen several cases in which the method that performs best on the development set does not yield the best test-set performance. However, we felt that it would be illegitimate to change the train/development/test sets in a post hoc fashion, that is, after seeing the experimental results. Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach ra"
W06-1639,P02-1053,0,0.0067143,"posal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006). Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual4 evidence of a high likelihood of agreement beQualitative summary of results The above difficulties un"
W06-1639,P88-1016,0,0.238883,"supportive; but if the bill under discussion is aimed at rescinding an existing flag-burning ban, the speech may represent opposition to the legislation. Given the current state of the art in sentiment analysis, it is doubtful that one could determine the (probably topic-specific) relationship between presented evidence and speaker opinion. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity"
W06-1639,J94-2004,0,0.169309,"escinding an existing flag-burning ban, the speech may represent opposition to the legislation. Given the current state of the art in sentiment analysis, it is doubtful that one could determine the (probably topic-specific) relationship between presented evidence and speaker opinion. Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography). In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003). Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and"
W11-0609,P11-1078,0,0.035223,"depends on the trigger family considered. The variation among families is in line with the previous empirical results on the multimodality of convergence in real conversations, as discussed in §4. be used to further automatic controversy detection (Mishne and Glance, 2006; Gómez et al., 2008). Moreover, if we succeeded in linking our results on narrative importance to relative social status, we might further the development of systems that can infer social relationships in online social networks when conversational data is present but other, more explicit cues are absent (Wyatt et al., 2008; Bramsen et al., 2011). Such systems could be valuable to the rapidly expanding field of analyzing social networks. 7 Summary and future work We provide some insight into the causal mechanism behind convergence, a topic that has generated substantial scrutiny and debate for over 40 years (Ireland et al., 2011; Branigan et al., 2010). Our work, along with Elson and McKeown (2010), advocates for the value of fictional sources in the study of linguistic and social phenomena. To stimulate such studies, we render our metadata-rich corpus of movie dialog public. In §1, we described some practical applications of a better"
W11-0609,C00-1027,0,0.0463923,"ture triggers the appearance of that feature in the respondent’s immediate reply. 79 (1) (2) Comparison with correlation: the importance of asymmetry10 Why do we employ ConvA,B , Equation (1), instead of the well-known correlation coefficient? One reason is that correlation fails to 9 We use “initiating” and “reply” loosely: in our terminology, the conversation hA: “Hi.” B: “Eaten?” A: “Nope.”i has two exchanges, one initiated by A’s “Hi”, the other by B’s “Eaten?”. 10 Other asymmetric measures based on conditional probability of occurrence have been proposed for adaptation within monologues (Church, 2000) and between conversations (Stenchikova and Stent, 2007). Since our focus is different, we control for different factors. 0.80 0.70 6 Experimental results 6.1 Convergence exists in fictional dialogs For each ordered pair of characters (A, B) and for each feature family t, we estimate equation (1) in a straightforward manner: the fraction of B’s replies to t-manifesting A utterances that themselves exhibit t, minus the fraction of all replies of B to A that exhibit t.13 Fig. 1 compares the average values of these two fractions (as a way of putting convergence values into context), showing posit"
W11-0609,P10-1015,0,0.0124249,"al., 2007), gender categorization (Koppel et al., 2002; Mukherjee and Liu, 2010; Herring and Paolillo, 2006), identification of interactional style (Jurafsky et al., 2009; Ranganath et al., 2009), and recognizing deceptive language (Hancock et al., 2008; Mihalcea and Strapparava, 2009). Imagined conversations There has been work in the NLP community applying computational techniques to fiction, scripts, and other types of text containing imagined conversations. For example, one recent project identifies conversational networks in novels, with the goal of evaluating various literary theories (Elson et al., 2010; Elson and McKeown, 2010). Movie scripts were used as word-sensedisambiguation evaluation data as part of an effort to generate computer animation from the scripts (Ye and Baldwin, 2006). Sonderegger (2010) employed a corpus of English poetry to study the relationship between pronunciation and network structure. Rayson et al. (2001) computed part-of-speech frequencies for imaginative writing in the British National Corpus, finding a typology gradient progressing from conversation to imaginative writing (e.g., novels) to task-oriented speech to informative writing. The data analyzed by Oberlan"
W11-0609,N09-1072,0,0.00714269,"e a computer (Nass and Lee, 2000; Branigan et al., 2010). 2 Related work not already mentioned Linguistic style and human characteristics Using stylistic (i.e., non-topical) elements like arti78 cles and prepositions to characterize the utterer in some way has a long history, including in authorship attribution (Mosteller and Wallace, 1984; Juola, 2008), personality-type classification (Argamon et al., 2005; Oberlander and Gill, 2006; Mairesse et al., 2007), gender categorization (Koppel et al., 2002; Mukherjee and Liu, 2010; Herring and Paolillo, 2006), identification of interactional style (Jurafsky et al., 2009; Ranganath et al., 2009), and recognizing deceptive language (Hancock et al., 2008; Mihalcea and Strapparava, 2009). Imagined conversations There has been work in the NLP community applying computational techniques to fiction, scripts, and other types of text containing imagined conversations. For example, one recent project identifies conversational networks in novels, with the goal of evaluating various literary theories (Elson et al., 2010; Elson and McKeown, 2010). Movie scripts were used as word-sensedisambiguation evaluation data as part of an effort to generate computer animation from"
W11-0609,W04-3250,0,0.0207287,"tion, P (bt,→a = 1|at = 1) (see eq. 1). Figure 1: Implicit depiction of convergence for each trigger family t, illustrated as the difference between the means of P (bt,→a = 1|at = 1) (right/light-blue bars) and P (bt,→a = 1) (left/dark-blue bars). (This implicit representation allows one to see the magnitude of the two components making up our definition of convergence.) The trigger families are ordered by decreasing convergence. All differences are statistically significant (paired t-test). In all figures in this paper, error bars represent standard error, estimated via bootstrap resampling (Koehn, 2004). (Here, the error bars, in red, are very tight.) Movies vs. Twitter One can ask how our results on movie dialogs correspond to those for real-life conversations. To study this, we utilize the results of Danescu-Niculescu-Mizil et al. (2011) on a largescale collection of Twitter exchanges as data on real conversational exchanges. Figure 2 depicts the comparison, revealing two interesting effects. First, Twitter users coordinate more than movie characters on all the trigger families we considered, which does show that the convergence effect is stronger in actual interchanges. On the other hand,"
W11-0609,P09-2078,0,0.00875635,"style and human characteristics Using stylistic (i.e., non-topical) elements like arti78 cles and prepositions to characterize the utterer in some way has a long history, including in authorship attribution (Mosteller and Wallace, 1984; Juola, 2008), personality-type classification (Argamon et al., 2005; Oberlander and Gill, 2006; Mairesse et al., 2007), gender categorization (Koppel et al., 2002; Mukherjee and Liu, 2010; Herring and Paolillo, 2006), identification of interactional style (Jurafsky et al., 2009; Ranganath et al., 2009), and recognizing deceptive language (Hancock et al., 2008; Mihalcea and Strapparava, 2009). Imagined conversations There has been work in the NLP community applying computational techniques to fiction, scripts, and other types of text containing imagined conversations. For example, one recent project identifies conversational networks in novels, with the goal of evaluating various literary theories (Elson et al., 2010; Elson and McKeown, 2010). Movie scripts were used as word-sensedisambiguation evaluation data as part of an effort to generate computer animation from the scripts (Ye and Baldwin, 2006). Sonderegger (2010) employed a corpus of English poetry to study the relationship"
W11-0609,D10-1021,0,0.00909866,"1988; van Baaren et al., 2003), even when the other conversational participant is known to be a computer (Nass and Lee, 2000; Branigan et al., 2010). 2 Related work not already mentioned Linguistic style and human characteristics Using stylistic (i.e., non-topical) elements like arti78 cles and prepositions to characterize the utterer in some way has a long history, including in authorship attribution (Mosteller and Wallace, 1984; Juola, 2008), personality-type classification (Argamon et al., 2005; Oberlander and Gill, 2006; Mairesse et al., 2007), gender categorization (Koppel et al., 2002; Mukherjee and Liu, 2010; Herring and Paolillo, 2006), identification of interactional style (Jurafsky et al., 2009; Ranganath et al., 2009), and recognizing deceptive language (Hancock et al., 2008; Mihalcea and Strapparava, 2009). Imagined conversations There has been work in the NLP community applying computational techniques to fiction, scripts, and other types of text containing imagined conversations. For example, one recent project identifies conversational networks in novels, with the goal of evaluating various literary theories (Elson et al., 2010; Elson and McKeown, 2010). Movie scripts were used as word-se"
W11-0609,D09-1035,0,0.0125395,"Lee, 2000; Branigan et al., 2010). 2 Related work not already mentioned Linguistic style and human characteristics Using stylistic (i.e., non-topical) elements like arti78 cles and prepositions to characterize the utterer in some way has a long history, including in authorship attribution (Mosteller and Wallace, 1984; Juola, 2008), personality-type classification (Argamon et al., 2005; Oberlander and Gill, 2006; Mairesse et al., 2007), gender categorization (Koppel et al., 2002; Mukherjee and Liu, 2010; Herring and Paolillo, 2006), identification of interactional style (Jurafsky et al., 2009; Ranganath et al., 2009), and recognizing deceptive language (Hancock et al., 2008; Mihalcea and Strapparava, 2009). Imagined conversations There has been work in the NLP community applying computational techniques to fiction, scripts, and other types of text containing imagined conversations. For example, one recent project identifies conversational networks in novels, with the goal of evaluating various literary theories (Elson et al., 2010; Elson and McKeown, 2010). Movie scripts were used as word-sensedisambiguation evaluation data as part of an effort to generate computer animation from the scripts (Ye and Baldw"
W11-0609,2007.sigdial-1.29,0,0.0307635,"ure in the respondent’s immediate reply. 79 (1) (2) Comparison with correlation: the importance of asymmetry10 Why do we employ ConvA,B , Equation (1), instead of the well-known correlation coefficient? One reason is that correlation fails to 9 We use “initiating” and “reply” loosely: in our terminology, the conversation hA: “Hi.” B: “Eaten?” A: “Nope.”i has two exchanges, one initiated by A’s “Hi”, the other by B’s “Eaten?”. 10 Other asymmetric measures based on conditional probability of occurrence have been proposed for adaptation within monologues (Church, 2000) and between conversations (Stenchikova and Stent, 2007). Since our focus is different, we control for different factors. 0.80 0.70 6 Experimental results 6.1 Convergence exists in fictional dialogs For each ordered pair of characters (A, B) and for each feature family t, we estimate equation (1) in a straightforward manner: the fraction of B’s replies to t-manifesting A utterances that themselves exhibit t, minus the fraction of all replies of B to A that exhibit t.13 Fig. 1 compares the average values of these two fractions (as a way of putting convergence values into context), showing positive differences for all of the considered families of fe"
W11-0609,U06-1020,0,0.00904544,"t al., 2009), and recognizing deceptive language (Hancock et al., 2008; Mihalcea and Strapparava, 2009). Imagined conversations There has been work in the NLP community applying computational techniques to fiction, scripts, and other types of text containing imagined conversations. For example, one recent project identifies conversational networks in novels, with the goal of evaluating various literary theories (Elson et al., 2010; Elson and McKeown, 2010). Movie scripts were used as word-sensedisambiguation evaluation data as part of an effort to generate computer animation from the scripts (Ye and Baldwin, 2006). Sonderegger (2010) employed a corpus of English poetry to study the relationship between pronunciation and network structure. Rayson et al. (2001) computed part-of-speech frequencies for imaginative writing in the British National Corpus, finding a typology gradient progressing from conversation to imaginative writing (e.g., novels) to task-oriented speech to informative writing. The data analyzed by Oberlander and Gill (2006) consisted of emails that participants were instructed to write by imagining that they were going to update a good friend on their current goings-on. 3 Movie dialogs co"
W12-3809,D10-1111,0,0.0549839,"re, is correlated with these different document classes, because of the previous research attention that has been devoted to hedging in particular and because of hedging being one of the topics of this workshop. The point of this paper is 4 However, this presumption that more hedges characterize a more scientific discourse has been contested. See section 2 for discussion and section 4.2 for our empirical investigation. 5 Several other groups have addressed the problem of trying to identify different sides or perspectives (Lin et al., 2006; Hardisty et al., 2010; Beigman Klebanov et al., 2010; Ahmed and Xing, 2010). thus not to compare the efficacy of hedging features with other types, such as bag-of-words features. Of course, to do so is an important and interesting direction for future work. In the end, we were not able to achieve satisfactory results even with respect to our narrowed goal. However, we believe that other researchers may be able to follow the plan of attack we outline below, and perhaps use the data we are releasing, in order to achieve our goal. We would welcome hearing the results of other people’s efforts. 2 How should we test whether hedging distinguishes scientific text? One very"
W12-3809,P10-2047,0,0.0482654,"dered as a single feature, is correlated with these different document classes, because of the previous research attention that has been devoted to hedging in particular and because of hedging being one of the topics of this workshop. The point of this paper is 4 However, this presumption that more hedges characterize a more scientific discourse has been contested. See section 2 for discussion and section 4.2 for our empirical investigation. 5 Several other groups have addressed the problem of trying to identify different sides or perspectives (Lin et al., 2006; Hardisty et al., 2010; Beigman Klebanov et al., 2010; Ahmed and Xing, 2010). thus not to compare the efficacy of hedging features with other types, such as bag-of-words features. Of course, to do so is an important and interesting direction for future work. In the end, we were not able to achieve satisfactory results even with respect to our narrowed goal. However, we believe that other researchers may be able to follow the plan of attack we outline below, and perhaps use the data we are releasing, in order to achieve our goal. We would welcome hearing the results of other people’s efforts. 2 How should we test whether hedging distinguishes sci"
W12-3809,W10-3001,0,0.0929357,"Missing"
W12-3809,D10-1028,0,0.104385,"her hedging specifically, considered as a single feature, is correlated with these different document classes, because of the previous research attention that has been devoted to hedging in particular and because of hedging being one of the topics of this workshop. The point of this paper is 4 However, this presumption that more hedges characterize a more scientific discourse has been contested. See section 2 for discussion and section 4.2 for our empirical investigation. 5 Several other groups have addressed the problem of trying to identify different sides or perspectives (Lin et al., 2006; Hardisty et al., 2010; Beigman Klebanov et al., 2010; Ahmed and Xing, 2010). thus not to compare the efficacy of hedging features with other types, such as bag-of-words features. Of course, to do so is an important and interesting direction for future work. In the end, we were not able to achieve satisfactory results even with respect to our narrowed goal. However, we believe that other researchers may be able to follow the plan of attack we outline below, and perhaps use the data we are releasing, in order to achieve our goal. We would welcome hearing the results of other people’s efforts. 2 How should we test wh"
W12-3809,W06-2915,0,0.0725569,"r focus is on whether hedging specifically, considered as a single feature, is correlated with these different document classes, because of the previous research attention that has been devoted to hedging in particular and because of hedging being one of the topics of this workshop. The point of this paper is 4 However, this presumption that more hedges characterize a more scientific discourse has been contested. See section 2 for discussion and section 4.2 for our empirical investigation. 5 Several other groups have addressed the problem of trying to identify different sides or perspectives (Lin et al., 2006; Hardisty et al., 2010; Beigman Klebanov et al., 2010; Ahmed and Xing, 2010). thus not to compare the efficacy of hedging features with other types, such as bag-of-words features. Of course, to do so is an important and interesting direction for future work. In the end, we were not able to achieve satisfactory results even with respect to our narrowed goal. However, we believe that other researchers may be able to follow the plan of attack we outline below, and perhaps use the data we are releasing, in order to achieve our goal. We would welcome hearing the results of other people’s efforts."
W12-3809,W10-3004,0,\N,Missing
