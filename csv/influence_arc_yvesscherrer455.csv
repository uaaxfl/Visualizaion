2008.jeptalnrecital-recital.8,J93-2003,0,0.0111513,"Missing"
2008.jeptalnrecital-recital.8,N01-1015,0,0.0607457,"Missing"
2008.jeptalnrecital-recital.8,N07-1047,0,0.0620571,"Missing"
2008.jeptalnrecital-recital.8,N01-1020,0,0.0759984,"Missing"
2008.jeptalnrecital-recital.8,P99-1067,0,0.12586,"Missing"
2008.jeptalnrecital-recital.8,P07-3010,1,0.810382,"Missing"
2009.jeptalnrecital-court.44,E06-1047,0,0.048483,"Missing"
2009.jeptalnrecital-court.44,2005.eamt-1.12,0,0.0693959,"Missing"
2009.jeptalnrecital-court.44,2003.mtsummit-papers.21,0,0.0837921,"Missing"
2009.jeptalnrecital-court.44,W06-1107,0,0.0285519,"Missing"
2009.jeptalnrecital-court.44,N01-1020,0,0.0993082,"Missing"
2009.jeptalnrecital-court.44,P07-3010,1,0.872367,"Missing"
2009.jeptalnrecital-court.44,W07-1216,0,0.0577623,"Missing"
2010.jeptalnrecital-demonstration.9,2009.jeptalnrecital-court.44,1,0.531218,"Missing"
2011.jeptalnrecital-court.43,2010.iwslt-papers.10,0,0.0430639,"Missing"
2011.jeptalnrecital-court.43,J94-4002,0,0.259549,"Missing"
2011.jeptalnrecital-court.43,W07-1216,1,0.779922,"Missing"
2011.jeptalnrecital-court.44,D08-1078,0,0.0589666,"Missing"
2011.jeptalnrecital-court.44,2010.iwslt-papers.10,0,0.0344141,"Missing"
2011.jeptalnrecital-court.44,P86-1040,0,0.266575,"Missing"
2011.jeptalnrecital-court.44,J94-4002,0,0.0968705,"Missing"
2011.jeptalnrecital-court.44,W10-1737,0,0.0535835,"Missing"
2011.jeptalnrecital-court.44,W07-1216,1,0.808368,"Missing"
2011.jeptalnrecital-court.44,W09-0415,1,0.868101,"Missing"
2016.jeptalnrecital-jep.14,1989.mtsummit-1.5,0,0.657647,"Missing"
2020.findings-emnlp.49,P19-1470,0,0.255107,"to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others contribute very little or nothing at all. Similarly, Michel et al. (2019) raise the question whether 16 attention heads are really necessary to obtain competitive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer architecture with attention operations that reduce time complexity (Shen et al., 2018; Wu et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Dai et al., 2019; Indurthi et al., 2019; Kitaev et al., 2020). This study falls into the third category and is motivated by the observation that most self-attention patterns learned by the Transformer architecture merely re"
2020.findings-emnlp.49,2013.iwslt-evaluation.1,0,0.0167238,"Missing"
2020.findings-emnlp.49,2014.iwslt-evaluation.1,0,0.019893,"Missing"
2020.findings-emnlp.49,D14-1179,0,0.0593278,"Missing"
2020.findings-emnlp.49,W19-4828,0,0.038033,"rt architecture for NMT, and more recently for language modeling (Radford et al., 2018) and other downstream tasks (Strubell et al., 2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation"
2020.findings-emnlp.49,D19-1223,0,0.0592143,"Missing"
2020.findings-emnlp.49,D19-5622,0,0.0709276,"uld be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For instance, Shaw et al. (2018) replace the sinusoidal position encoding in the Transformer with relative position encoding to improve the focus on local positional patterns. Several studies modify the attention formula to bias the attention weights towards local areas (Yang et al., 2018; Xu et al., 2019; Fonollosa et al., 2019). Wu et al. (2019) and Yang et al. (2019) use convolutional modules to replace parts of self-attention, making the overall networks computationally more efficient. Cui et al. (2019) mask out certain tokens when computing attention, which favors local attention patterns and prevents redun557 dancy in the different attention heads. All these contributions have shown the importance of localness, and the possibility to use lightweight convolutional networks to reduce the number of parameters while yielding competitive results (Wu et al., 2019). In this respect, our work is orthogonal to previous research: we focus only on the original Transformer architecture and investigate the replacement of learnable encoder self-attention by fixed, non-learnable attentive patterns. Recen"
2020.findings-emnlp.49,W19-5203,0,0.0370687,"dies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020). A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others contribute very little or nothing at all. Similarly, Michel et al. (2019) raise the question whether 16 attention heads are really necessary to obtain competitive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer"
2020.findings-emnlp.49,P19-1285,0,0.0608932,"Missing"
2020.findings-emnlp.49,R19-1028,0,0.0494014,"r lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020). A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others contribute very little or nothing at all. Similarly, Michel et al. (2019) raise the question whether 16 attention heads are really necessary to obtain competitive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer architecture with atten"
2020.findings-emnlp.49,N19-1423,0,0.256847,"d of each encoder layer with simple fixed – non-learnable – attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios. 1 Introduction Models based on the Transformer architecture (Vaswani et al., 2017) have led to tremendous performance increases in a wide range of downstream tasks (Devlin et al., 2019; Radford et al., 2019), including Machine Translation (MT) (Vaswani et al., 2017; Ott et al., 2018). One main component of the architecture is the multi-headed attention mechanism that allows the model to capture longrange contextual information. Despite these successes, the impact of the suggested parametrization choices, in particular the self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in p"
2020.findings-emnlp.49,D19-1453,0,0.0283687,"self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020). A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others contribute very little or nothing at all. Similarly, Michel et al. (2019) raise the question whether 16 attention heads are really necessary to obtain competitive"
2020.findings-emnlp.49,D19-1082,0,0.0180082,"2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For ins"
2020.findings-emnlp.49,D19-1424,0,0.0364636,"2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For ins"
2020.findings-emnlp.49,P19-1290,0,0.0201375,"s ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others contribute very little or nothing at all. Similarly, Michel et al. (2019) raise the question whether 16 attention heads are really necessary to obtain competitive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer architecture with attention operations that reduce time complexity (Shen et al., 2018; Wu et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Dai et al., 2019; Indurthi et al., 2019; Kitaev et al., 2020). This study falls into the third category and is motivated by the observation that most self-attention patterns learned by the Transformer architecture merely reflect positional encoding of contextual information (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Voita et al., 2019a; Correia et al., 2019). From this standpoint, we argue that most attentive connections in the encoder do not need to be learned at all, but can be replaced by simple predefined patterns. To this end, we design, analyze and experimentally compare intuitive and simple fixed attention pattern"
2020.findings-emnlp.49,P19-1356,0,0.0291859,"2018) and other downstream tasks (Strubell et al., 2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the trans"
2020.findings-emnlp.49,P17-4012,0,0.0365928,"omary in NMT to split words into subword units, and there is evidence that self-attention treats split words differently than non-split ones (Correia et al., 2019). Therefore, we propose a second variant of the predefined patterns that assigns the same attention values to all parts of the same word (see lower row of Figure 1). 4 We perform a series of experiments to evaluate the fixed attentive encoder patterns, starting with a standard German ↔ English translation setup (Section 4.1) and then extending the scope to lowresource and high-resource scenarios (Section 4.2). We use the OpenNMT-py (Klein et al., 2017) library for training, the base version of Transformer as hyper-parameters (Vaswani et al., 2017), and compare against the reference using sacreBLEU (Papineni et al., 2002; Post, 2018) .3 4.1 j=0 (j Results: Standard scenario To assess the general viability of the proposed approach and to quantify the effects of different numbers of encoder and decoder layers, we train models on a mid-sized dataset of 2.9M training sentences from the German ↔ English WMT19 news translation task (Barrault et al., 2019), using newstest2013 and newstest2014 as development and test data, respectively. We learn tru"
2020.findings-emnlp.49,W04-3250,0,0.0785459,"en +1L 7Fword +1L 1L 25.66 24.90* 25.03* 23.76* 27.28 27.01 26.72* 25.75* 27.88 26.84* 27.38* 26.96* 28.62 28.09* 27.78* 27.34* 28.71 28.43 27.82* 27.44* 29.31 28.61* 28.40* 27.56* 30.99 30.61 30.69 30.17* Table 1: BLEU scores for the German ↔ English (DE ↔ EN) standard scenario, for different configurations of learnable (L) and fixed (F) attention heads. Scores marked in gray with * are significantly lower than the respective 8L model scores, at p &lt; 0.05. Statistical significance is computed using the compare-mt tool (Neubig et al., 2019) with paired bootstrap resampling with 1000 resamples (Koehn, 2004). coding (BPE) segmentation (Sennrich et al., 2016) on the training corpus, using 35 000 merge operations. We train four Transformer models: • 8L: all 8 attention heads in each layer are learnable, • 7Ftoken +1L: 7 fixed token-based attention heads and 1 learnable head per encoder layer, • 7Fword +1L: 7 fixed word-based attention patterns and 1 learnable head per encoder layer, • 1L: a single learnable attention head per encoder layer. Each model is trained in 7 configurations: 6 encoder layers with 6 decoder layers, and 1 to 6 encoder layers coupled to 1 decoder layer. BLEU scores are shown i"
2020.findings-emnlp.49,D19-1445,0,0.154105,"titive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer architecture with attention operations that reduce time complexity (Shen et al., 2018; Wu et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Dai et al., 2019; Indurthi et al., 2019; Kitaev et al., 2020). This study falls into the third category and is motivated by the observation that most self-attention patterns learned by the Transformer architecture merely reflect positional encoding of contextual information (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Voita et al., 2019a; Correia et al., 2019). From this standpoint, we argue that most attentive connections in the encoder do not need to be learned at all, but can be replaced by simple predefined patterns. To this end, we design, analyze and experimentally compare intuitive and simple fixed attention patterns. The proposed patterns are solely based on positional information and do not require any learnable parameters nor external knowledge. The fixed patterns reflect the importance of locality and pose the question whether encoder self-attention needs to be learned 556 Findings of the Assoc"
2020.findings-emnlp.49,P18-1007,0,0.0409789,"Missing"
2020.findings-emnlp.49,W19-4825,0,0.0950801,"Missing"
2020.findings-emnlp.49,Q16-1037,0,0.0364869,"heads on the encoder side is negligible. Moreover, we also note that as we replace attention heads with non-learnable ones, our configurations reduce the number of parameters without degrading translation quality. 5 Analysis To further analyze the fixed attentive encoder patterns, we perform three targeted evaluations: i) on the sentence length, ii) on the subject-verb agreement task, and iii) on the Word Sense Disambiguation (WSD) task. The length analysis inspects the translation quality by sentence length. The subjectverb agreement task is commonly used to evaluate long-range dependencies (Linzen et al., 2016; Tran et al., 2018; Tang et al., 2018), while the WSD task addresses lexical ambiguity phenomena, i.e., words of the source language that have multiple translations in the target language representing different meanings (Marvin and Koehn, 2018; Liu 8L 7Ftoken +1L 7Ftoken (H8 disabled) &lt;10 [10,20)[20,30)[30,40)[40,50)[50,60) ≥60 Figure 2: BLEU scores for different ranges of sentence lengths. et al., 2018; Pu et al., 2018; Tang et al., 2019). For both tasks, we use contrastive test suites (Sennrich, 2017; Popovi´c and Castilho, 2019) that rely on the ability of NMT systems to score given transl"
2020.findings-emnlp.49,N18-1121,0,0.0527275,"Missing"
2020.findings-emnlp.49,D15-1166,0,0.0881284,"ay hamper the extraction of global semantic features beneficial for the word sense disambiguation capability of the MT model. Keeping one learnable head in the encoder compensates for degradations, but this trade-off needs to be carefully assessed. • Position-wise attentive patterns play a key role in low-resource scenarios, both for related (German ↔ English) and unrelated (Vietnamese ↔ English) languages. 2 Related work Attention mechanisms in Neural Machine Translation (NMT) were first introduced in combination with Recurrent Neural Networks (RNNs) (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), between the encoder and decoder. The Transformer architecture extended the mechanism by introducing the so-called self-attention to replace the RNNs in the encoder and decoder, and by using multiple attention heads (Vaswani et al., 2017). This architecture rapidly became the de facto state-of-the-art architecture for NMT, and more recently for language modeling (Radford et al., 2018) and other downstream tasks (Strubell et al., 2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intui"
2020.findings-emnlp.49,W19-4827,0,0.0866727,"Missing"
2020.findings-emnlp.49,W18-1812,0,0.0199672,"ze the fixed attentive encoder patterns, we perform three targeted evaluations: i) on the sentence length, ii) on the subject-verb agreement task, and iii) on the Word Sense Disambiguation (WSD) task. The length analysis inspects the translation quality by sentence length. The subjectverb agreement task is commonly used to evaluate long-range dependencies (Linzen et al., 2016; Tran et al., 2018; Tang et al., 2018), while the WSD task addresses lexical ambiguity phenomena, i.e., words of the source language that have multiple translations in the target language representing different meanings (Marvin and Koehn, 2018; Liu 8L 7Ftoken +1L 7Ftoken (H8 disabled) &lt;10 [10,20)[20,30)[30,40)[40,50)[50,60) ≥60 Figure 2: BLEU scores for different ranges of sentence lengths. et al., 2018; Pu et al., 2018; Tang et al., 2019). For both tasks, we use contrastive test suites (Sennrich, 2017; Popovi´c and Castilho, 2019) that rely on the ability of NMT systems to score given translations. Broadly speaking, a sentence containing the linguistic phenomenon of interest is paired with the correct reference translation and with a modified translation with a specific type of error. A contrast is considered successfully detected"
2020.findings-emnlp.49,N19-4007,0,0.0434251,"74 22.63* 23.36 23.07 22.88* 22.29* 25.02 24.63 24.85 23.87* DE–EN 8L 7Ftoken +1L 7Fword +1L 1L 25.66 24.90* 25.03* 23.76* 27.28 27.01 26.72* 25.75* 27.88 26.84* 27.38* 26.96* 28.62 28.09* 27.78* 27.34* 28.71 28.43 27.82* 27.44* 29.31 28.61* 28.40* 27.56* 30.99 30.61 30.69 30.17* Table 1: BLEU scores for the German ↔ English (DE ↔ EN) standard scenario, for different configurations of learnable (L) and fixed (F) attention heads. Scores marked in gray with * are significantly lower than the respective 8L model scores, at p &lt; 0.05. Statistical significance is computed using the compare-mt tool (Neubig et al., 2019) with paired bootstrap resampling with 1000 resamples (Koehn, 2004). coding (BPE) segmentation (Sennrich et al., 2016) on the training corpus, using 35 000 merge operations. We train four Transformer models: • 8L: all 8 attention heads in each layer are learnable, • 7Ftoken +1L: 7 fixed token-based attention heads and 1 learnable head per encoder layer, • 7Fword +1L: 7 fixed word-based attention patterns and 1 learnable head per encoder layer, • 1L: a single learnable attention head per encoder layer. Each model is trained in 7 configurations: 6 encoder layers with 6 decoder layers, and 1 to 6"
2020.findings-emnlp.49,W18-6301,0,0.0232455,"n position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios. 1 Introduction Models based on the Transformer architecture (Vaswani et al., 2017) have led to tremendous performance increases in a wide range of downstream tasks (Devlin et al., 2019; Radford et al., 2019), including Machine Translation (MT) (Vaswani et al., 2017; Ott et al., 2018). One main component of the architecture is the multi-headed attention mechanism that allows the model to capture longrange contextual information. Despite these successes, the impact of the suggested parametrization choices, in particular the self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connec"
2020.findings-emnlp.49,P02-1040,0,0.107248,"efore, we propose a second variant of the predefined patterns that assigns the same attention values to all parts of the same word (see lower row of Figure 1). 4 We perform a series of experiments to evaluate the fixed attentive encoder patterns, starting with a standard German ↔ English translation setup (Section 4.1) and then extending the scope to lowresource and high-resource scenarios (Section 4.2). We use the OpenNMT-py (Klein et al., 2017) library for training, the base version of Transformer as hyper-parameters (Vaswani et al., 2017), and compare against the reference using sacreBLEU (Papineni et al., 2002; Post, 2018) .3 4.1 j=0 (j Results: Standard scenario To assess the general viability of the proposed approach and to quantify the effects of different numbers of encoder and decoder layers, we train models on a mid-sized dataset of 2.9M training sentences from the German ↔ English WMT19 news translation task (Barrault et al., 2019), using newstest2013 and newstest2014 as development and test data, respectively. We learn truecasers and Byte-Pair En1 Pattern 5 and 7 are flipped versions of pattern 4 and 6, respectively. 2 In Section 4.4, we present a contrastive system in which the eighth head"
2020.findings-emnlp.49,Y16-2002,0,0.0203337,"os since intuitive properties of self-attention are directly encoded within the model, which may be hard to learn from small training datasets. We empirically test this assumption on four translation tasks: • German → English (DE→EN), using the data from the IWSLT 2014 shared task (Cettolo et al., 2014). As prior work (Ranzato et al., 2016; Sennrich and Zhang, 2019), we report BLEU score on the concatenated dev sets: tst2010, tst2011, tst2012, dev2010, dev2012 (159 000 training sentences, 7 282 for development, and 6 750 for testing). • Korean → English (KO→EN), using the dataset described in Park et al. (2016) (90 000 training sentences, 1 000 for development, and 2 000 for testing).4 • Vietnamese ↔ English (VI↔EN), using the data from the IWSLT 2015 shared task (Cettolo et al., 2015), using tst2012 and tst2013 for development and testing, respectively (133 000 training sentences, 1 553 for development and 1 268 per testing). 560 4 https://github.com/jungyeul/ korean-parallel-corpora Disabled head Enc. heads DE–EN KO–EN EN–VI VI–EN 8L 7Ftoken +1L 7Fword +1L 1L Prior work 30.86 32.95 32.56 30.22 † 33.60 6.67 8.43 8.70 6.14 † 10.37 29.85 31.05 31.15 28.67 ] 27.71 1 Current word 2 Previous word 3 Next"
2020.findings-emnlp.49,W19-7602,0,0.0356148,"Missing"
2020.findings-emnlp.49,W18-6319,0,0.0208893,"ond variant of the predefined patterns that assigns the same attention values to all parts of the same word (see lower row of Figure 1). 4 We perform a series of experiments to evaluate the fixed attentive encoder patterns, starting with a standard German ↔ English translation setup (Section 4.1) and then extending the scope to lowresource and high-resource scenarios (Section 4.2). We use the OpenNMT-py (Klein et al., 2017) library for training, the base version of Transformer as hyper-parameters (Vaswani et al., 2017), and compare against the reference using sacreBLEU (Papineni et al., 2002; Post, 2018) .3 4.1 j=0 (j Results: Standard scenario To assess the general viability of the proposed approach and to quantify the effects of different numbers of encoder and decoder layers, we train models on a mid-sized dataset of 2.9M training sentences from the German ↔ English WMT19 news translation task (Barrault et al., 2019), using newstest2013 and newstest2014 as development and test data, respectively. We learn truecasers and Byte-Pair En1 Pattern 5 and 7 are flipped versions of pattern 4 and 6, respectively. 2 In Section 4.4, we present a contrastive system in which the eighth head is fixed as"
2020.findings-emnlp.49,Q18-1044,0,0.0129872,"n (WSD) task. The length analysis inspects the translation quality by sentence length. The subjectverb agreement task is commonly used to evaluate long-range dependencies (Linzen et al., 2016; Tran et al., 2018; Tang et al., 2018), while the WSD task addresses lexical ambiguity phenomena, i.e., words of the source language that have multiple translations in the target language representing different meanings (Marvin and Koehn, 2018; Liu 8L 7Ftoken +1L 7Ftoken (H8 disabled) &lt;10 [10,20)[20,30)[30,40)[40,50)[50,60) ≥60 Figure 2: BLEU scores for different ranges of sentence lengths. et al., 2018; Pu et al., 2018; Tang et al., 2019). For both tasks, we use contrastive test suites (Sennrich, 2017; Popovi´c and Castilho, 2019) that rely on the ability of NMT systems to score given translations. Broadly speaking, a sentence containing the linguistic phenomenon of interest is paired with the correct reference translation and with a modified translation with a specific type of error. A contrast is considered successfully detected if the reference translation obtains a higher score than the artificially modified translation. The evaluation metric corresponds to the accuracy over all decisions. We conduct th"
2020.findings-emnlp.49,W19-5354,1,0.845427,"learnable attention head (7Ftoken H8 disabled), performance drops consistently in both test suites, showing that the learnable head plays a key role for WSD, specializing in semantic feature extraction. Word sense disambiguation It has been shown that the encoder of Transformerbased MT models includes semantic information beneficial for WSD (Tang et al., 2018, 2019). In this respect, a model with predefined fixed patterns may struggle to encode global semantic features. To this end, we evaluate our models on two German–English WSD test suites, ContraWSD (Rios Gonzales et al., 2017) and MuCoW (Raganato et al., 2019).9 Table 6 shows the performance of our models on the WSD benchmarks. Overall, the model with 6 decoder layers and fixed attentive patterns 9 As MuCoW is automatically built using various parallel corpora, we discarded those ones included in our training. We only report the average result from the TED (Cettolo et al., 2013) and Tatoeba (Tiedemann, 2012) sources. 563 6 Conclusion In this work, we propose to simplify encoder selfattention of Transformer-based NMT models by replacing all but one attention heads with fixed positional attentive patterns that require neither training nor external kn"
2020.findings-emnlp.49,W18-5431,1,0.940897,"ain component of the architecture is the multi-headed attention mechanism that allows the model to capture longrange contextual information. Despite these successes, the impact of the suggested parametrization choices, in particular the self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020). A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show t"
2020.findings-emnlp.49,W17-4702,0,0.0179277,"ingly enough, when we disable the learnable attention head (7Ftoken H8 disabled), performance drops consistently in both test suites, showing that the learnable head plays a key role for WSD, specializing in semantic feature extraction. Word sense disambiguation It has been shown that the encoder of Transformerbased MT models includes semantic information beneficial for WSD (Tang et al., 2018, 2019). In this respect, a model with predefined fixed patterns may struggle to encode global semantic features. To this end, we evaluate our models on two German–English WSD test suites, ContraWSD (Rios Gonzales et al., 2017) and MuCoW (Raganato et al., 2019).9 Table 6 shows the performance of our models on the WSD benchmarks. Overall, the model with 6 decoder layers and fixed attentive patterns 9 As MuCoW is automatically built using various parallel corpora, we discarded those ones included in our training. We only report the average result from the TED (Cettolo et al., 2013) and Tatoeba (Tiedemann, 2012) sources. 563 6 Conclusion In this work, we propose to simplify encoder selfattention of Transformer-based NMT models by replacing all but one attention heads with fixed positional attentive patterns that requir"
2020.findings-emnlp.49,2020.tacl-1.54,0,0.0195092,"., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For instance, Shaw et al. (201"
2020.findings-emnlp.49,D19-1592,0,0.0620283,"Missing"
2020.findings-emnlp.49,E17-2060,0,0.124097,". The subjectverb agreement task is commonly used to evaluate long-range dependencies (Linzen et al., 2016; Tran et al., 2018; Tang et al., 2018), while the WSD task addresses lexical ambiguity phenomena, i.e., words of the source language that have multiple translations in the target language representing different meanings (Marvin and Koehn, 2018; Liu 8L 7Ftoken +1L 7Ftoken (H8 disabled) &lt;10 [10,20)[20,30)[30,40)[40,50)[50,60) ≥60 Figure 2: BLEU scores for different ranges of sentence lengths. et al., 2018; Pu et al., 2018; Tang et al., 2019). For both tasks, we use contrastive test suites (Sennrich, 2017; Popovi´c and Castilho, 2019) that rely on the ability of NMT systems to score given translations. Broadly speaking, a sentence containing the linguistic phenomenon of interest is paired with the correct reference translation and with a modified translation with a specific type of error. A contrast is considered successfully detected if the reference translation obtains a higher score than the artificially modified translation. The evaluation metric corresponds to the accuracy over all decisions. We conduct the analyses using the DE–EN models from Section 4.1, i.e., 8L, 7Ftoken +1L, and 7Ftok"
2020.findings-emnlp.49,P16-1162,0,0.126617,"23.76* 27.28 27.01 26.72* 25.75* 27.88 26.84* 27.38* 26.96* 28.62 28.09* 27.78* 27.34* 28.71 28.43 27.82* 27.44* 29.31 28.61* 28.40* 27.56* 30.99 30.61 30.69 30.17* Table 1: BLEU scores for the German ↔ English (DE ↔ EN) standard scenario, for different configurations of learnable (L) and fixed (F) attention heads. Scores marked in gray with * are significantly lower than the respective 8L model scores, at p &lt; 0.05. Statistical significance is computed using the compare-mt tool (Neubig et al., 2019) with paired bootstrap resampling with 1000 resamples (Koehn, 2004). coding (BPE) segmentation (Sennrich et al., 2016) on the training corpus, using 35 000 merge operations. We train four Transformer models: • 8L: all 8 attention heads in each layer are learnable, • 7Ftoken +1L: 7 fixed token-based attention heads and 1 learnable head per encoder layer, • 7Fword +1L: 7 fixed word-based attention patterns and 1 learnable head per encoder layer, • 1L: a single learnable attention head per encoder layer. Each model is trained in 7 configurations: 6 encoder layers with 6 decoder layers, and 1 to 6 encoder layers coupled to 1 decoder layer. BLEU scores are shown in Table 1. Results for the most powerful model (6+6"
2020.findings-emnlp.49,P19-1021,0,0.123147,"orm the word-based one, but with higher numbers of decoder layers the two variants are statistically equivalent. 4.2 Results: Low-resource and high-resource scenarios We hypothesize that fixed attentive patterns are especially useful in low-resource scenarios since intuitive properties of self-attention are directly encoded within the model, which may be hard to learn from small training datasets. We empirically test this assumption on four translation tasks: • German → English (DE→EN), using the data from the IWSLT 2014 shared task (Cettolo et al., 2014). As prior work (Ranzato et al., 2016; Sennrich and Zhang, 2019), we report BLEU score on the concatenated dev sets: tst2010, tst2011, tst2012, dev2010, dev2012 (159 000 training sentences, 7 282 for development, and 6 750 for testing). • Korean → English (KO→EN), using the dataset described in Park et al. (2016) (90 000 training sentences, 1 000 for development, and 2 000 for testing).4 • Vietnamese ↔ English (VI↔EN), using the data from the IWSLT 2015 shared task (Cettolo et al., 2015), using tst2012 and tst2013 for development and testing, respectively (133 000 training sentences, 1 553 for development and 1 268 per testing). 560 4 https://github.com/ju"
2020.findings-emnlp.49,N18-2074,0,0.0327278,"rs et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For instance, Shaw et al. (2018) replace the sinusoidal position encoding in the Transformer with relative position encoding to improve the focus on local positional patterns. Several studies modify the attention formula to bias the attention weights towards local areas (Yang et al., 2018; Xu et al., 2019; Fonollosa et al., 2019). Wu et al. (2019) and Yang et al. (2019) use convolutional modules to replace parts of self-attention, making the overall networks computationally more efficient. Cui et al. (2019) mask out certain tokens when computing attention, which favors local attention patterns and prevents redun557 dancy in"
2020.findings-emnlp.49,D18-1548,0,0.0620527,"Missing"
2020.findings-emnlp.49,P19-1452,0,0.0242263,"for language modeling (Radford et al., 2018) and other downstream tasks (Strubell et al., 2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within"
2020.findings-emnlp.49,tiedemann-2012-parallel,1,0.656296,", 2019). In this respect, a model with predefined fixed patterns may struggle to encode global semantic features. To this end, we evaluate our models on two German–English WSD test suites, ContraWSD (Rios Gonzales et al., 2017) and MuCoW (Raganato et al., 2019).9 Table 6 shows the performance of our models on the WSD benchmarks. Overall, the model with 6 decoder layers and fixed attentive patterns 9 As MuCoW is automatically built using various parallel corpora, we discarded those ones included in our training. We only report the average result from the TED (Cettolo et al., 2013) and Tatoeba (Tiedemann, 2012) sources. 563 6 Conclusion In this work, we propose to simplify encoder selfattention of Transformer-based NMT models by replacing all but one attention heads with fixed positional attentive patterns that require neither training nor external knowledge. We train NMT models on different data sizes and language directions with the proposed fixed patterns, showing that the encoder self-attention can be simplified drastically, reducing parameter footprint at training time without degradation in translation quality. In low-resource scenarios, translation quality is even improved. Our extensive anal"
2020.findings-emnlp.49,D18-1503,0,0.0461458,"Missing"
2020.findings-emnlp.49,W19-4808,0,0.021559,"de facto state-of-the-art architecture for NMT, and more recently for language modeling (Radford et al., 2018) and other downstream tasks (Strubell et al., 2018; Devlin et al., 2019; Bosselut et al., 2019). The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020). As regards MT, recent work (Voita et al., 2019b) suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the li"
2020.findings-emnlp.49,P19-1032,0,0.0245622,"nt information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others contribute very little or nothing at all. Similarly, Michel et al. (2019) raise the question whether 16 attention heads are really necessary to obtain competitive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer architecture with attention operations that reduce time complexity (Shen et al., 2018; Wu et al., 2019; Child et al., 2019; Sukhbaatar et al., 2019; Dai et al., 2019; Indurthi et al., 2019; Kitaev et al., 2020). This study falls into the third category and is motivated by the observation that most self-attention patterns learned by the Transformer architecture merely reflect positional encoding of contextual information (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Voita et al., 2019a; Correia et al., 2019). From this standpoint, we argue that most attentive connections in the encoder do not need to be learned at all, but can be replaced by simple predefined patterns. To this end, we design, analyze and experimentally compare int"
2020.findings-emnlp.49,D19-1448,0,0.27398,"that allows the model to capture longrange contextual information. Despite these successes, the impact of the suggested parametrization choices, in particular the self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020). A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others"
2020.findings-emnlp.49,D18-1458,0,0.031139,"Missing"
2020.findings-emnlp.49,P19-1580,0,0.478925,"that allows the model to capture longrange contextual information. Despite these successes, the impact of the suggested parametrization choices, in particular the self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections (Raganato and Tiedemann, 2018; Tang et al., 2018; Mareˇcek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020). A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives (Garg et al., 2019), or improving the representation through external information such as syntactic supervision (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019). The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways (Sanh et al., 2019). For example, Voita et al. (2019b) show that a few attention heads do the “heavy lifting” whereas others"
2020.findings-emnlp.49,D19-1149,0,0.0338734,"Missing"
2020.findings-emnlp.49,P19-1295,0,0.0438248,"while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For instance, Shaw et al. (2018) replace the sinusoidal position encoding in the Transformer with relative position encoding to improve the focus on local positional patterns. Several studies modify the attention formula to bias the attention weights towards local areas (Yang et al., 2018; Xu et al., 2019; Fonollosa et al., 2019). Wu et al. (2019) and Yang et al. (2019) use convolutional modules to replace parts of self-attention, making the overall networks computationally more efficient. Cui et al. (2019) mask out certain tokens when computing attention, which favors local attention patterns and prevents redun557 dancy in the different attention heads. All these contributions have shown the importance of localness, and the possibility to use lightweight convolutional networks to reduce the number of parameters while yielding competitive results (Wu et al., 2019). In this respect, our work is"
2020.findings-emnlp.49,D18-1475,0,0.0238611,"lation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For instance, Shaw et al. (2018) replace the sinusoidal position encoding in the Transformer with relative position encoding to improve the focus on local positional patterns. Several studies modify the attention formula to bias the attention weights towards local areas (Yang et al., 2018; Xu et al., 2019; Fonollosa et al., 2019). Wu et al. (2019) and Yang et al. (2019) use convolutional modules to replace parts of self-attention, making the overall networks computationally more efficient. Cui et al. (2019) mask out certain tokens when computing attention, which favors local attention patterns and prevents redun557 dancy in the different attention heads. All these contributions have shown the importance of localness, and the possibility to use lightweight convolutional networks to reduce the number of parameters while yielding competitive results (Wu et al., 2019). In this res"
2020.findings-emnlp.49,N19-1407,0,0.0330641,"research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases (Hao et al., 2019a). For instance, Shaw et al. (2018) replace the sinusoidal position encoding in the Transformer with relative position encoding to improve the focus on local positional patterns. Several studies modify the attention formula to bias the attention weights towards local areas (Yang et al., 2018; Xu et al., 2019; Fonollosa et al., 2019). Wu et al. (2019) and Yang et al. (2019) use convolutional modules to replace parts of self-attention, making the overall networks computationally more efficient. Cui et al. (2019) mask out certain tokens when computing attention, which favors local attention patterns and prevents redun557 dancy in the different attention heads. All these contributions have shown the importance of localness, and the possibility to use lightweight convolutional networks to reduce the number of parameters while yielding competitive results (Wu et al., 2019). In this respect, our work is orthogonal to previous research: we focus only on the original Tr"
2020.findings-emnlp.49,2020.acl-main.687,0,0.0709417,"pes of heads: positional heads point to an adjacent token, syntactic heads point to tokens in a specific syntactic relation, and rare word heads point to the least frequent tokens in a sentence. Correia et al. (2019) identify two additional types of heads: BPE-merging heads spread weight over adjacent tokens that are part of the same BPE cluster or hyphenated words, and interrogation heads point to question marks at the end of the sentence. In line with these findings, we design our fixed attention patterns and train NMT models without the need of learning them. In concurrent work, You et al. (2020) propose to replace learnable attention weights in Transformerbased NMT with hard-coded Gaussian distributions. This paper is complementary and differs in several respects: while You et al. (2020) consider three fixed patterns across the encoder-decoder architecture, we focus only on the encoder selfattention but present seven fixed patterns that cover additional known properties of self-attention. We study the relative impact of each of them and analyze their performance with respect to different numbers of encoder-decoder layers, and as semantic feature extractor for lexical ambiguity phenom"
2020.findings-emnlp.49,2015.iwslt-evaluation.1,0,\N,Missing
2020.lrec-1.224,P05-1074,0,0.188523,"pproach uses machine translation models to generate paraphrases via backtranslation (Mallinson et al., 2017; Suzuki et al., 2017; Wieting and Gimpel, 2018), where a sentence is first translated into one or more target languages and then back into the source language. In addition to machine translation models, rule-based systems (Meteer and Shaked, 1988) and methods based on lexical substitution (Kauchak and Barzilay, 2006) have previously been used for paraphrase generation. There are other approaches to finding paraphrases beside paraphrase generation. Pivot methods rely on parallel corpora (Bannard and Callison-Burch, 2005) whereas paraphrase detection and extraction can be based on sentence embeddings (Wieting and Gimpel, 2018; Wieting and Gimpel, 2017; Sjöblom et al., 2018) or semantic matching models (Lan and Xu, 2018). With arbitrary input sentences, these methods are intuitively unappealing because they search for a closest match in the available data, and no amount of data guarantees that a correct paraphrase is found for any given input. For this reason, we adopt the generation approach. Naturally, generation models are still dependent on suitable training data, and generalization outside the training set"
2020.lrec-1.224,P11-1020,0,0.327434,"movies and TV shows. The English subset of Opusparcus has been previously used in paraphrase generation (Ampomah et al., 2019; Hämäläinen and Alnajjar, 2019), but to our knowledge, no previous work has used all six languages in the corpus. We perform systematic evaluation and analysis of paraphrase generation. To assess semantic adequacy of the generated paraphrases, we compute scores from manual annotations, which we compare to BLEU (Papineni et al., 2002) and a recently proposed, so-called BERTScore (Zhang et al., 2020). Furthermore, we quantify the novelty of the phrases using PINC scores (Chen and Dolan, 2011). Major contributions of the present work are experiments in data selection to understand the trade-off between semantic adequacy and novelty in paraphrase generation, as well as the validation of BERTScore in the colloquial domain using manual evaluation. In contrast to much of the previous work, we also perform experiments in multiple languages. 2. Data Opusparcus (Creutz, 2018) is a sentential paraphrase corpus consisting of pairs of sentences extracted automatically from the OpenSubtitles corpus (Lison and Tiedemann, 2016). Opusparcus is publicly available1 and contains training, developme"
2020.lrec-1.224,L18-1218,1,0.921039,"in contrast to many of the common paraphrase data sets such as the Microsoft Research Paraphrase Cor1814 pus consisting of news text (Dolan et al., 2004; Dolan and Brockett, 2005), PPDB (Ganitkevitch et al., 2013), which covers many formal domains, in addition to some more colloquial data, or Quora Question Pairs (Iyer et al., 2017), which covers a variety of topics but is limited to questions. In addition, most of the work on paraphrase generation has been for English, while we are interested in broadening the work to multiple languages. We focus on the Opusparcus corpus for our experiments (Creutz, 2018). Opusparcus consists of sentential paraphrases in six languages extracted from subtitles of movies and TV shows. The English subset of Opusparcus has been previously used in paraphrase generation (Ampomah et al., 2019; Hämäläinen and Alnajjar, 2019), but to our knowledge, no previous work has used all six languages in the corpus. We perform systematic evaluation and analysis of paraphrase generation. To assess semantic adequacy of the generated paraphrases, we compute scores from manual annotations, which we compare to BLEU (Papineni et al., 2002) and a recently proposed, so-called BERTScore"
2020.lrec-1.224,I05-5002,0,0.12163,"Missing"
2020.lrec-1.224,C04-1051,0,0.454567,"Missing"
2020.lrec-1.224,N13-1092,0,0.0968169,"Missing"
2020.lrec-1.224,W16-4207,0,0.0169198,"paper, we focus on paraphrase generation using neural machine translation methods. In paraphrase generation we are interested in models that take in an arbitrary input sentence and generate an output with the same meaning but different surface form. We apply traditional recurrent encoder-decoder networks with attention (Luong et al., 2015) as well as Transformer based models (Vaswani et al., 2017), which are the state of the art of modern machine translation. Previous work has already addressed paraphrase generation through machine translation trained on monolingual data (Quirk et al., 2004; Hasan et al., 2016; Prakash et al., 2016). Variants integrating variational autoencoders into the models (Gupta et al., 2018) or different learning schemes based on reinforcement learning (Li et al., 2017) have also been proposed. Roy and Grangier (2019) propose a method based on variational autoencoders and unlabeled monolingual data. A closely related approach uses machine translation models to generate paraphrases via backtranslation (Mallinson et al., 2017; Suzuki et al., 2017; Wieting and Gimpel, 2018), where a sentence is first translated into one or more target languages and then back into the source lan"
2020.lrec-1.224,N06-1058,0,0.079541,"based on reinforcement learning (Li et al., 2017) have also been proposed. Roy and Grangier (2019) propose a method based on variational autoencoders and unlabeled monolingual data. A closely related approach uses machine translation models to generate paraphrases via backtranslation (Mallinson et al., 2017; Suzuki et al., 2017; Wieting and Gimpel, 2018), where a sentence is first translated into one or more target languages and then back into the source language. In addition to machine translation models, rule-based systems (Meteer and Shaked, 1988) and methods based on lexical substitution (Kauchak and Barzilay, 2006) have previously been used for paraphrase generation. There are other approaches to finding paraphrases beside paraphrase generation. Pivot methods rely on parallel corpora (Bannard and Callison-Burch, 2005) whereas paraphrase detection and extraction can be based on sentence embeddings (Wieting and Gimpel, 2018; Wieting and Gimpel, 2017; Sjöblom et al., 2018) or semantic matching models (Lan and Xu, 2018). With arbitrary input sentences, these methods are intuitively unappealing because they search for a closest match in the available data, and no amount of data guarantees that a correct para"
2020.lrec-1.224,P17-4012,0,0.0134294,"sformer is based purely on self-attention within the encoder and the decoder, as well as attention between the encoder and the decoder. We use 6 layers in both the encoder and the decoder, with hidden state and word embedding dimensionalities of 512, separate word embeddings for encoder and decoder, 8 attention heads, and a feedforward dimensionality of 2048 within the layers. The total number of parameters in the model is approximately 90 million. A dropout of 0.1 is used between layers. These and the rest of the hyperparameters for the Transformer follow the recommended setup of OpenNMT-py (Klein et al., 2017), which we use for all experiments. All models are trained for 400k steps or until convergence, with a validation score as the convergence criterion. The data is preprocessed using byte pair encoding (BPE) (Sennrich et al., 2016) with 30k operations to avoid out-ofvocabulary tokens. We use the Adam optimizer (Kingma and Ba, 2014) to train both models with a learning rate of 0.0001. For the RNN model, the learning rate is halved every 40k steps starting after 200k steps, and for the Transformer, the recommended noam decay is used. We use a batch size of 256 samples for the RNN model and a token"
2020.lrec-1.224,C18-1328,0,0.0205467,"languages and then back into the source language. In addition to machine translation models, rule-based systems (Meteer and Shaked, 1988) and methods based on lexical substitution (Kauchak and Barzilay, 2006) have previously been used for paraphrase generation. There are other approaches to finding paraphrases beside paraphrase generation. Pivot methods rely on parallel corpora (Bannard and Callison-Burch, 2005) whereas paraphrase detection and extraction can be based on sentence embeddings (Wieting and Gimpel, 2018; Wieting and Gimpel, 2017; Sjöblom et al., 2018) or semantic matching models (Lan and Xu, 2018). With arbitrary input sentences, these methods are intuitively unappealing because they search for a closest match in the available data, and no amount of data guarantees that a correct paraphrase is found for any given input. For this reason, we adopt the generation approach. Naturally, generation models are still dependent on suitable training data, and generalization outside the training set is a problem that needs to be tackled. However, massive data sets beyond the training data are not needed in this approach. We are interested in paraphrase generation specifically in less formal, collo"
2020.lrec-1.224,L16-1147,0,0.0721853,"Missing"
2020.lrec-1.224,D15-1166,0,0.10034,"Missing"
2020.lrec-1.224,E17-1083,0,0.0175289,"n machine translation. Previous work has already addressed paraphrase generation through machine translation trained on monolingual data (Quirk et al., 2004; Hasan et al., 2016; Prakash et al., 2016). Variants integrating variational autoencoders into the models (Gupta et al., 2018) or different learning schemes based on reinforcement learning (Li et al., 2017) have also been proposed. Roy and Grangier (2019) propose a method based on variational autoencoders and unlabeled monolingual data. A closely related approach uses machine translation models to generate paraphrases via backtranslation (Mallinson et al., 2017; Suzuki et al., 2017; Wieting and Gimpel, 2018), where a sentence is first translated into one or more target languages and then back into the source language. In addition to machine translation models, rule-based systems (Meteer and Shaked, 1988) and methods based on lexical substitution (Kauchak and Barzilay, 2006) have previously been used for paraphrase generation. There are other approaches to finding paraphrases beside paraphrase generation. Pivot methods rely on parallel corpora (Bannard and Callison-Burch, 2005) whereas paraphrase detection and extraction can be based on sentence embe"
2020.lrec-1.224,C88-2088,0,0.76885,"into the models (Gupta et al., 2018) or different learning schemes based on reinforcement learning (Li et al., 2017) have also been proposed. Roy and Grangier (2019) propose a method based on variational autoencoders and unlabeled monolingual data. A closely related approach uses machine translation models to generate paraphrases via backtranslation (Mallinson et al., 2017; Suzuki et al., 2017; Wieting and Gimpel, 2018), where a sentence is first translated into one or more target languages and then back into the source language. In addition to machine translation models, rule-based systems (Meteer and Shaked, 1988) and methods based on lexical substitution (Kauchak and Barzilay, 2006) have previously been used for paraphrase generation. There are other approaches to finding paraphrases beside paraphrase generation. Pivot methods rely on parallel corpora (Bannard and Callison-Burch, 2005) whereas paraphrase detection and extraction can be based on sentence embeddings (Wieting and Gimpel, 2018; Wieting and Gimpel, 2017; Sjöblom et al., 2018) or semantic matching models (Lan and Xu, 2018). With arbitrary input sentences, these methods are intuitively unappealing because they search for a closest match in t"
2020.lrec-1.224,C18-1102,0,0.0223572,"syntactic structures may be completely different. These two expressions could have an identical or similar underlying semantic representation or there could be a mapping that transforms one surface form to another. On the other hand, there are practical applications of paraphrase models. Such models can be useful in information retrieval or data mining for discovering expressions with the intended meaning but with totally different surface realization than the original query (Riezler et al., 2007). In addition, paraphrasing is used in abstractive summarization as part of summarization models (Nayeem et al., 2018), as well as for evaluation (Vadapalli et al., 2017). Paraphrases can also be used for proofing or grammar checking, producing suggested corrections. Similarly, someone perfecting their skills in a second language, or someone looking for alternate, possibly more idiomatic, expressions may benefit from paraphrase models. For instance, to pick one word, to corroborate, in a few contexts, we can find the following paraphrase pairs: “She’ll corroborate my story.” → “She’ll back me up.”, “Can you corroborate that?” → “I need proofs.”, “Will people corroborate your account?” → “Is there anybody who"
2020.lrec-1.224,P02-1040,0,0.110251,"We focus on the Opusparcus corpus for our experiments (Creutz, 2018). Opusparcus consists of sentential paraphrases in six languages extracted from subtitles of movies and TV shows. The English subset of Opusparcus has been previously used in paraphrase generation (Ampomah et al., 2019; Hämäläinen and Alnajjar, 2019), but to our knowledge, no previous work has used all six languages in the corpus. We perform systematic evaluation and analysis of paraphrase generation. To assess semantic adequacy of the generated paraphrases, we compute scores from manual annotations, which we compare to BLEU (Papineni et al., 2002) and a recently proposed, so-called BERTScore (Zhang et al., 2020). Furthermore, we quantify the novelty of the phrases using PINC scores (Chen and Dolan, 2011). Major contributions of the present work are experiments in data selection to understand the trade-off between semantic adequacy and novelty in paraphrase generation, as well as the validation of BERTScore in the colloquial domain using manual evaluation. In contrast to much of the previous work, we also perform experiments in multiple languages. 2. Data Opusparcus (Creutz, 2018) is a sentential paraphrase corpus consisting of pairs of"
2020.lrec-1.224,C16-1275,0,0.0189341,"paraphrase generation using neural machine translation methods. In paraphrase generation we are interested in models that take in an arbitrary input sentence and generate an output with the same meaning but different surface form. We apply traditional recurrent encoder-decoder networks with attention (Luong et al., 2015) as well as Transformer based models (Vaswani et al., 2017), which are the state of the art of modern machine translation. Previous work has already addressed paraphrase generation through machine translation trained on monolingual data (Quirk et al., 2004; Hasan et al., 2016; Prakash et al., 2016). Variants integrating variational autoencoders into the models (Gupta et al., 2018) or different learning schemes based on reinforcement learning (Li et al., 2017) have also been proposed. Roy and Grangier (2019) propose a method based on variational autoencoders and unlabeled monolingual data. A closely related approach uses machine translation models to generate paraphrases via backtranslation (Mallinson et al., 2017; Suzuki et al., 2017; Wieting and Gimpel, 2018), where a sentence is first translated into one or more target languages and then back into the source language. In addition to m"
2020.lrec-1.224,W04-3219,0,0.153845,"h for that?” In this paper, we focus on paraphrase generation using neural machine translation methods. In paraphrase generation we are interested in models that take in an arbitrary input sentence and generate an output with the same meaning but different surface form. We apply traditional recurrent encoder-decoder networks with attention (Luong et al., 2015) as well as Transformer based models (Vaswani et al., 2017), which are the state of the art of modern machine translation. Previous work has already addressed paraphrase generation through machine translation trained on monolingual data (Quirk et al., 2004; Hasan et al., 2016; Prakash et al., 2016). Variants integrating variational autoencoders into the models (Gupta et al., 2018) or different learning schemes based on reinforcement learning (Li et al., 2017) have also been proposed. Roy and Grangier (2019) propose a method based on variational autoencoders and unlabeled monolingual data. A closely related approach uses machine translation models to generate paraphrases via backtranslation (Mallinson et al., 2017; Suzuki et al., 2017; Wieting and Gimpel, 2018), where a sentence is first translated into one or more target languages and then back"
2020.lrec-1.224,P07-1059,0,0.066496,"face-level features. Two expressions may carry the same meaning, although they may not contain the same words or their syntactic structures may be completely different. These two expressions could have an identical or similar underlying semantic representation or there could be a mapping that transforms one surface form to another. On the other hand, there are practical applications of paraphrase models. Such models can be useful in information retrieval or data mining for discovering expressions with the intended meaning but with totally different surface realization than the original query (Riezler et al., 2007). In addition, paraphrasing is used in abstractive summarization as part of summarization models (Nayeem et al., 2018), as well as for evaluation (Vadapalli et al., 2017). Paraphrases can also be used for proofing or grammar checking, producing suggested corrections. Similarly, someone perfecting their skills in a second language, or someone looking for alternate, possibly more idiomatic, expressions may benefit from paraphrase models. For instance, to pick one word, to corroborate, in a few contexts, we can find the following paraphrase pairs: “She’ll corroborate my story.” → “She’ll back me"
2020.lrec-1.224,P19-1605,0,0.0147792,"erent surface form. We apply traditional recurrent encoder-decoder networks with attention (Luong et al., 2015) as well as Transformer based models (Vaswani et al., 2017), which are the state of the art of modern machine translation. Previous work has already addressed paraphrase generation through machine translation trained on monolingual data (Quirk et al., 2004; Hasan et al., 2016; Prakash et al., 2016). Variants integrating variational autoencoders into the models (Gupta et al., 2018) or different learning schemes based on reinforcement learning (Li et al., 2017) have also been proposed. Roy and Grangier (2019) propose a method based on variational autoencoders and unlabeled monolingual data. A closely related approach uses machine translation models to generate paraphrases via backtranslation (Mallinson et al., 2017; Suzuki et al., 2017; Wieting and Gimpel, 2018), where a sentence is first translated into one or more target languages and then back into the source language. In addition to machine translation models, rule-based systems (Meteer and Shaked, 1988) and methods based on lexical substitution (Kauchak and Barzilay, 2006) have previously been used for paraphrase generation. There are other a"
2020.lrec-1.224,P16-1162,0,0.00791093,"dimensionalities of 512, separate word embeddings for encoder and decoder, 8 attention heads, and a feedforward dimensionality of 2048 within the layers. The total number of parameters in the model is approximately 90 million. A dropout of 0.1 is used between layers. These and the rest of the hyperparameters for the Transformer follow the recommended setup of OpenNMT-py (Klein et al., 2017), which we use for all experiments. All models are trained for 400k steps or until convergence, with a validation score as the convergence criterion. The data is preprocessed using byte pair encoding (BPE) (Sennrich et al., 2016) with 30k operations to avoid out-ofvocabulary tokens. We use the Adam optimizer (Kingma and Ba, 2014) to train both models with a learning rate of 0.0001. For the RNN model, the learning rate is halved every 40k steps starting after 200k steps, and for the Transformer, the recommended noam decay is used. We use a batch size of 256 samples for the RNN model and a token batch size of 4096 for the Transformer. At inference time we ensemble the last three checkpoints to produce the outputs and use beam search with beam size 10. 1815 Large Unidirectional Edit distance de 12.0 6.0 6.2 en 40.0 20.0"
2020.lrec-1.224,W18-6109,1,0.846834,"ntence is first translated into one or more target languages and then back into the source language. In addition to machine translation models, rule-based systems (Meteer and Shaked, 1988) and methods based on lexical substitution (Kauchak and Barzilay, 2006) have previously been used for paraphrase generation. There are other approaches to finding paraphrases beside paraphrase generation. Pivot methods rely on parallel corpora (Bannard and Callison-Burch, 2005) whereas paraphrase detection and extraction can be based on sentence embeddings (Wieting and Gimpel, 2018; Wieting and Gimpel, 2017; Sjöblom et al., 2018) or semantic matching models (Lan and Xu, 2018). With arbitrary input sentences, these methods are intuitively unappealing because they search for a closest match in the available data, and no amount of data guarantees that a correct paraphrase is found for any given input. For this reason, we adopt the generation approach. Naturally, generation models are still dependent on suitable training data, and generalization outside the training set is a problem that needs to be tackled. However, massive data sets beyond the training data are not needed in this approach. We are interested in paraphras"
2020.lrec-1.224,P17-3007,0,0.0163179,"revious work has already addressed paraphrase generation through machine translation trained on monolingual data (Quirk et al., 2004; Hasan et al., 2016; Prakash et al., 2016). Variants integrating variational autoencoders into the models (Gupta et al., 2018) or different learning schemes based on reinforcement learning (Li et al., 2017) have also been proposed. Roy and Grangier (2019) propose a method based on variational autoencoders and unlabeled monolingual data. A closely related approach uses machine translation models to generate paraphrases via backtranslation (Mallinson et al., 2017; Suzuki et al., 2017; Wieting and Gimpel, 2018), where a sentence is first translated into one or more target languages and then back into the source language. In addition to machine translation models, rule-based systems (Meteer and Shaked, 1988) and methods based on lexical substitution (Kauchak and Barzilay, 2006) have previously been used for paraphrase generation. There are other approaches to finding paraphrases beside paraphrase generation. Pivot methods rely on parallel corpora (Bannard and Callison-Burch, 2005) whereas paraphrase detection and extraction can be based on sentence embeddings (Wieting and G"
2020.lrec-1.224,I17-2034,0,0.0258164,"These two expressions could have an identical or similar underlying semantic representation or there could be a mapping that transforms one surface form to another. On the other hand, there are practical applications of paraphrase models. Such models can be useful in information retrieval or data mining for discovering expressions with the intended meaning but with totally different surface realization than the original query (Riezler et al., 2007). In addition, paraphrasing is used in abstractive summarization as part of summarization models (Nayeem et al., 2018), as well as for evaluation (Vadapalli et al., 2017). Paraphrases can also be used for proofing or grammar checking, producing suggested corrections. Similarly, someone perfecting their skills in a second language, or someone looking for alternate, possibly more idiomatic, expressions may benefit from paraphrase models. For instance, to pick one word, to corroborate, in a few contexts, we can find the following paraphrase pairs: “She’ll corroborate my story.” → “She’ll back me up.”, “Can you corroborate that?” → “I need proofs.”, “Will people corroborate your account?” → “Is there anybody who can vouch for that?” In this paper, we focus on para"
2020.lrec-1.224,P17-1190,0,0.0186124,"Gimpel, 2018), where a sentence is first translated into one or more target languages and then back into the source language. In addition to machine translation models, rule-based systems (Meteer and Shaked, 1988) and methods based on lexical substitution (Kauchak and Barzilay, 2006) have previously been used for paraphrase generation. There are other approaches to finding paraphrases beside paraphrase generation. Pivot methods rely on parallel corpora (Bannard and Callison-Burch, 2005) whereas paraphrase detection and extraction can be based on sentence embeddings (Wieting and Gimpel, 2018; Wieting and Gimpel, 2017; Sjöblom et al., 2018) or semantic matching models (Lan and Xu, 2018). With arbitrary input sentences, these methods are intuitively unappealing because they search for a closest match in the available data, and no amount of data guarantees that a correct paraphrase is found for any given input. For this reason, we adopt the generation approach. Naturally, generation models are still dependent on suitable training data, and generalization outside the training set is a problem that needs to be tackled. However, massive data sets beyond the training data are not needed in this approach. We are"
2020.lrec-1.224,P18-1042,0,0.0125559,"ady addressed paraphrase generation through machine translation trained on monolingual data (Quirk et al., 2004; Hasan et al., 2016; Prakash et al., 2016). Variants integrating variational autoencoders into the models (Gupta et al., 2018) or different learning schemes based on reinforcement learning (Li et al., 2017) have also been proposed. Roy and Grangier (2019) propose a method based on variational autoencoders and unlabeled monolingual data. A closely related approach uses machine translation models to generate paraphrases via backtranslation (Mallinson et al., 2017; Suzuki et al., 2017; Wieting and Gimpel, 2018), where a sentence is first translated into one or more target languages and then back into the source language. In addition to machine translation models, rule-based systems (Meteer and Shaked, 1988) and methods based on lexical substitution (Kauchak and Barzilay, 2006) have previously been used for paraphrase generation. There are other approaches to finding paraphrases beside paraphrase generation. Pivot methods rely on parallel corpora (Bannard and Callison-Burch, 2005) whereas paraphrase detection and extraction can be based on sentence embeddings (Wieting and Gimpel, 2018; Wieting and Gi"
2020.lrec-1.452,Q15-1038,0,0.0127666,"TurkuNLP, and the OPUS collection of translated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide speci"
2020.lrec-1.452,N19-1423,0,0.00878062,"bserved for that language pair. Figure 3 (right) shows that the expected effect holds for some language pairs (typically with English as target language), whereas the contrary effect is observed for other pairs (typically with English as source language). Experiments with a larger number of bins have shown similarly inconclusive outcomes. This outcome could hint at shortcomings of the sense embeddings used in this work (Mancini et al., 2017). In future work, we plan to evaluate more recent sense embedding approaches, for instance an approach based on big pre-trained language models like BERT (Devlin et al., 2019; Scarlini et al., 2020). Another way to assess sense distinctiveness is by including it directly in the evaluation metric, as proposed with the weighted precision score in Section 3. A comparison between the standard and the weighted precision scores is shown in Table 6. The weighted precision scores are generally higher, and the difference is proportional to the average similarity between senses (see Table 3). 5. Conclusion In this paper, we presented an extended version of M U C OW, an automatically built evaluation benchmark for measuring WSD capabilities of machine translation systems, av"
2020.lrec-1.452,P16-1191,0,0.0136942,"ated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambi"
2020.lrec-1.452,K18-2013,0,0.0234719,"20 examples were found are discarded. Unambiguous source nouns, i.e., those associated with only one sense id, are discarded as well. We call each pairing of a source noun with a sense id sense cluster. 2.2. 2. Methodology: Building M U C OW The gist of our approach lies in the combination of different resources and tools: the wide-coverage multilingual sense inventory of BabelNet (Navigli and Ponzetto, 2012) and its associated sense embeddings (Mancini et al., 2017), the OPUS collection of translated texts from the web (Tiedemann, 2012), and the multilingual neural parsing pipeline TurkuNLP (Kanerva et al., 2018). In the following, we describe the three steps needed to create a M U C OW test suite. 2.1. It is known that lexical resources such as BabelNet tend to suffer from overly fine granularity of their sense inventory (Navigli, 2006; Palmer et al., 2007). We therefore introduce two additional merging steps: 1. We merge those sense clusters that share at least one common target word in BabelNet. 2. We merge sense clusters with similar meanings, as defined by their sense embeddings (Mancini et al., 2017). Concretely, following earlier work (Raganato et al., 2019), we merge senses whose cosine simila"
2020.lrec-1.452,P19-1568,0,0.0178262,"comprising training data with known sense distributions. Our approach for the construction of the benchmark builds upon the wide-coverage multilingual sense inventory of BabelNet, the multilingual neural parsing pipeline TurkuNLP, and the OPUS collection of translated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific lingu"
2020.lrec-1.452,N18-1121,0,0.0207783,"oureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambiguity of nouns, to our knowledge, two major test suites exist: ContraWSD (Rios et al., 2017; Rios et al., 2018) and M U C OW (Raganato et al., 2019). Both test suites are available in two variants: scoring and translation. The first variant relies on the ability of neural machine translation systems"
2020.lrec-1.452,P19-1569,0,0.0154071,"ion for 10 language pairs, comprising training data with known sense distributions. Our approach for the construction of the benchmark builds upon the wide-coverage multilingual sense inventory of BabelNet, the multilingual neural parsing pipeline TurkuNLP, and the OPUS collection of translated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To m"
2020.lrec-1.452,K17-1012,0,0.410241,"In the second case (Quelle), the first and second clusters are merged because their similarity is above the threshold. Sense ids for which less than 20 examples were found are discarded. Unambiguous source nouns, i.e., those associated with only one sense id, are discarded as well. We call each pairing of a source noun with a sense id sense cluster. 2.2. 2. Methodology: Building M U C OW The gist of our approach lies in the combination of different resources and tools: the wide-coverage multilingual sense inventory of BabelNet (Navigli and Ponzetto, 2012) and its associated sense embeddings (Mancini et al., 2017), the OPUS collection of translated texts from the web (Tiedemann, 2012), and the multilingual neural parsing pipeline TurkuNLP (Kanerva et al., 2018). In the following, we describe the three steps needed to create a M U C OW test suite. 2.1. It is known that lexical resources such as BabelNet tend to suffer from overly fine granularity of their sense inventory (Navigli, 2006; Palmer et al., 2007). We therefore introduce two additional merging steps: 1. We merge those sense clusters that share at least one common target word in BabelNet. 2. We merge sense clusters with similar meanings, as def"
2020.lrec-1.452,P06-1014,0,0.0818786,"C OW The gist of our approach lies in the combination of different resources and tools: the wide-coverage multilingual sense inventory of BabelNet (Navigli and Ponzetto, 2012) and its associated sense embeddings (Mancini et al., 2017), the OPUS collection of translated texts from the web (Tiedemann, 2012), and the multilingual neural parsing pipeline TurkuNLP (Kanerva et al., 2018). In the following, we describe the three steps needed to create a M U C OW test suite. 2.1. It is known that lexical resources such as BabelNet tend to suffer from overly fine granularity of their sense inventory (Navigli, 2006; Palmer et al., 2007). We therefore introduce two additional merging steps: 1. We merge those sense clusters that share at least one common target word in BabelNet. 2. We merge sense clusters with similar meanings, as defined by their sense embeddings (Mancini et al., 2017). Concretely, following earlier work (Raganato et al., 2019), we merge senses whose cosine similarity is higher than 0.3. Step 1: Identify ambiguous source words and their translations For each language pair, we determine a set of parallel text sources mainly from the OPUS collection (see Table 1).1 Based on our previous ex"
2020.lrec-1.452,P02-1040,0,0.108776,"Missing"
2020.lrec-1.452,P17-1170,0,0.022727,"test suite is available at http://github.com/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambiguity of nouns, to our"
2020.lrec-1.452,W19-7602,0,0.0872284,"Missing"
2020.lrec-1.452,W18-6319,0,0.0354383,"Missing"
2020.lrec-1.452,Q18-1044,0,0.0925728,"o et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambiguity of nouns, to our knowledge, two major test suites exist: ContraWSD (Rios et al., 2017; Rios et al., 2018) and M U C OW (Raganato et al., 2019). Both test suites are available in two variants: scoring and translation. The first variant relies on the ability of neural machine t"
2020.lrec-1.452,D17-1120,1,0.887901,"Missing"
2020.lrec-1.452,P16-1162,0,0.0418738,"0.4626 0.4651 0.5193 0.4142 0.4428 0.6752 0.7047 0.6977 0.7070 0.6982 0.7871 0.8383 0.7651 0.8008 0.8281 0.8054 0.8352 0.8315 0.8704 0.8811 0.9168 0.9311 0.8896 0.9433 0.9001 0.5437 0.4701 0.5444 0.3888 0.4947 0.7320 0.7120 0.7159 0.6968 0.7546 0.8202 0.8525 0.7984 0.7998 0.8516 0.8370 0.8465 0.8476 0.8869 0.8911 0.9296 0.9364 0.9099 0.9480 0.9239 Table 5: F1-scores of the baseline models, broken down by frequency bins. Bolded values indicate an improvement of at least 0.03 absolute compared to the other (Small or Big) model. pair. Sentences are encoded using Truecaser and Byte-Pair Encoding (Sennrich et al., 2016), with 32 000 merge operations for each language, learned on each training corpus separately. Note that these models are not specifically adapted towards good sense disambiguation performance. They merely show how well off-the-shelf NMT architectures perform on lexical ambiguities. It is expected that architectures specifically adapted to WSD (Pu et al., 2018, for instance) would show substantially higher scores. Table 4 summarizes the results of our experiments. The first column indicates general translation quality, as measured by BLEU7 score on the Newstest corpora.8 The Big training corpus"
2020.lrec-1.452,P19-1105,0,0.0178471,"/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambiguity of nouns, to our knowledge, two major test suites exist: Contr"
2020.lrec-1.452,W18-6304,0,0.0165359,"containing an ambiguous source word, an evaluation script checks whether any of the correct or incorrect target senses can be identified in the translation output. While both variants have different pros and cons, the translation one allows an evaluation directly on the output of a system, avoiding the need for a function for scoring a translation, which is typically not available for online systems or unsupervised MT approaches. Recent works suggest that the state-of-the-art Transformer architecture (Vaswani et al., 2017) for neural MT (NMT) is able to deal with lexical ambiguity quite well (Tang et al., 2018; Tang et al., 2019), learning to distinguish between senses during translation with high precision. Prior works have shown that NMT models based on Recurrent Neural Networks (RNNs) struggle when dealing with rare word senses (Rios et al., 2017), but it is not fully clear how well the more recent Transformer architecture performs under different sense frequencies, size of training corpora and across different language pairs. In earlier work (Raganato et al., 2019), we have presented M U C OW, a language-independent and fully automatic method for building a test suite for lexically ambiguous no"
2020.lrec-1.452,D19-1149,0,0.0136627,"uous source word, an evaluation script checks whether any of the correct or incorrect target senses can be identified in the translation output. While both variants have different pros and cons, the translation one allows an evaluation directly on the output of a system, avoiding the need for a function for scoring a translation, which is typically not available for online systems or unsupervised MT approaches. Recent works suggest that the state-of-the-art Transformer architecture (Vaswani et al., 2017) for neural MT (NMT) is able to deal with lexical ambiguity quite well (Tang et al., 2018; Tang et al., 2019), learning to distinguish between senses during translation with high precision. Prior works have shown that NMT models based on Recurrent Neural Networks (RNNs) struggle when dealing with rare word senses (Rios et al., 2017), but it is not fully clear how well the more recent Transformer architecture performs under different sense frequencies, size of training corpora and across different language pairs. In earlier work (Raganato et al., 2019), we have presented M U C OW, a language-independent and fully automatic method for building a test suite for lexically ambiguous nouns. Here, we report"
2020.lrec-1.452,P12-1029,0,0.0251856,"al neural parsing pipeline TurkuNLP, and the OPUS collection of translated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW. Keywords: machine translation, word sense disambiguation, evaluation benchmark, test suite 1. Introduction In recent years, several advances have been made in Word Sense Disambiguation (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic p"
2020.lrec-1.452,2013.iwslt-evaluation.1,0,0.0874185,"Missing"
2020.lrec-1.452,eisele-chen-2010-multiun,0,0.0194206,"Missing"
2020.lrec-1.452,2005.mtsummit-papers.11,0,0.396432,"Missing"
2020.lrec-1.452,W19-5354,1,0.938821,"T), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambiguity of nouns, to our knowledge, two major test suites exist: ContraWSD (Rios et al., 2017; Rios et al., 2018) and M U C OW (Raganato et al., 2019). Both test suites are available in two variants: scoring and translation. The first variant relies on the ability of neural machine translation systems to score given translations: a sentence containing an ambiguous source word is paired with the correct reference translation and with a modified translation in which the ambiguous word has been replaced by a word of a different sense. A contrast is considered successfully detected if the reference translation obtains a higher score than the artificially modified translation. The second variant relies directly on the translation produced by the"
2020.lrec-1.452,W17-4702,0,0.114354,"tion (WSD) (Raganato et al., 2017; Loureiro and Jorge, 2019; Kumar et al., 2019). WSD models that tackle lexical ambiguity effectively bring numerous benefits to a variety of downstream tasks and applications, such as information retrieval and extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and text categorization (Flekova and Gurevych, 2016; Pilehvar et al., 2017; Sinoara et al., 2019; Shimura et al., 2019). Another downstream application is machine translation (MT), where word sense disambiguation plays a crucial role to select the correct translation sense for each ambiguous word (Rios et al., 2017; Pu et al., 2018; Liu et al., 2018). To measure specific linguistic phenomena in machine translation, several test suites – or challenge sets – have emerged (Popovi´c and Castilho, 2019). They are evaluation benchmarks that focus on particular linguistic phenomena and provide specific evaluation criteria or metrics. For lexical ambiguity of nouns, to our knowledge, two major test suites exist: ContraWSD (Rios et al., 2017; Rios et al., 2018) and M U C OW (Raganato et al., 2019). Both test suites are available in two variants: scoring and translation. The first variant relies on the ability of"
2020.lrec-1.452,W18-6437,0,0.0323486,"Missing"
2020.lrec-1.452,tiedemann-2012-parallel,1,0.711047,"e their similarity is above the threshold. Sense ids for which less than 20 examples were found are discarded. Unambiguous source nouns, i.e., those associated with only one sense id, are discarded as well. We call each pairing of a source noun with a sense id sense cluster. 2.2. 2. Methodology: Building M U C OW The gist of our approach lies in the combination of different resources and tools: the wide-coverage multilingual sense inventory of BabelNet (Navigli and Ponzetto, 2012) and its associated sense embeddings (Mancini et al., 2017), the OPUS collection of translated texts from the web (Tiedemann, 2012), and the multilingual neural parsing pipeline TurkuNLP (Kanerva et al., 2018). In the following, we describe the three steps needed to create a M U C OW test suite. 2.1. It is known that lexical resources such as BabelNet tend to suffer from overly fine granularity of their sense inventory (Navigli, 2006; Palmer et al., 2007). We therefore introduce two additional merging steps: 1. We merge those sense clusters that share at least one common target word in BabelNet. 2. We merge sense clusters with similar meanings, as defined by their sense embeddings (Mancini et al., 2017). Concretely, follo"
2020.lrec-1.848,P05-1074,0,0.564779,"a single language – in most cases English – or a small number of languages (see Section 4.). Furthermore, some paraphrase datasets focus on lexical and phrasal rather than sentential paraphrases, while others are created (semi-)automatically using machine translation. This paper describes the creation of a paraphrase corpus for 97 languages with a total of 1.9 million sentences. It consists of entire sentences produced by crowdsourcing within the Tatoeba project (Section 2.). The paraphrase matching process is entirely automatic and is based on the multilingual pivoting approach introduced by Bannard and Callison-Burch (2005). The number of sentences per language ranges from 200 to 250 000, which makes the dataset more suitable for fine-tuning and evaluation purposes than for training. It covers languages for which, to our knowledge, no other paraphrase dataset exists. In contrast to some previous work, we organize paraphrases as sets rather than pairs: all sentences in a paraphrase set are considered paraphrases of each other. This representation is especially well-suited for multi-reference evaluation of paraphrase generation models, as there is generally not a single correct way of paraphrasing a given input se"
2020.lrec-1.848,J13-3001,0,0.157875,"quarters of inferred paraphrases are correct and that most remaining ones are either correct but trivial, or near-paraphrases that neutralize a morphological distinction. The corpus contains a total of 1.9 million sentences, with 200 – 250 000 sentences per language. It covers a range of languages for which, to our knowledge, no other paraphrase dataset exists. The dataset is available at https://doi.org/10.5281/zenodo.3707949. Keywords: Multilingual corpus, Paraphrases, Crowdsourcing 1. Introduction Paraphrases are different textual realizations of the same meaning within a single language (Bhagat and Hovy, 2013; Ganitkevitch and Callison-Burch, 2014). Paraphrase detection and generation have become popular tasks in NLP and are increasingly integrated into a wide variety of common downstream tasks such as machine translation, information retrieval, question answering, and semantic parsing (Federmann et al., 2019). Although the availability of large datasets for training and evaluation has facilitated research on paraphrase detection and generation, most of these datasets cover only a single language – in most cases English – or a small number of languages (see Section 4.). Furthermore, some paraphras"
2020.lrec-1.848,P02-1040,0,0.108312,"Missing"
2020.lrec-1.848,I05-5002,0,0.304027,"ges are abbreviated using ISO 639-1 and 639-2 codes. Subcorpora with more than 40 000 paraphrase sets or 100 000 sentences are highlighted in bold. consists of one tab-separated file per language containing one sentence per row. Sentences with the same paraphrase set id are considered paraphrases.10 Figure 3 shows an excerpt of the English file. 4. Related resources A large number of paraphrase corpora have been proposed over the last years. In this section, we enumerate the most relevant corpora and compare them with TaPaCo. MSRPC The Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Dolan and Brockett, 2005) consists of 10 The paraphrase set ids are kept constant across languages. For example, the English sentences of set 40 are translations of the French sentences of set 40. 6870 1313 297738 He isn’t my cousin. 907;4000;7360;7417 1313 1892 574175 2218079 He’s not my cousin. You’re among friends. 1892 2218509 You’re with friends. 1892 24158 24158 24158 842729 842729 842770 842770 5013251 1481280 3174733 3174734 906575 906578 906758 906785 You guys are among friends. Cat got your tongue? Did the cat get your tongue? Has the cat got your tongue? It is nevertheless a good sentence. It’s a good sente"
2020.lrec-1.848,C04-1051,0,0.228264,"per language. Languages are abbreviated using ISO 639-1 and 639-2 codes. Subcorpora with more than 40 000 paraphrase sets or 100 000 sentences are highlighted in bold. consists of one tab-separated file per language containing one sentence per row. Sentences with the same paraphrase set id are considered paraphrases.10 Figure 3 shows an excerpt of the English file. 4. Related resources A large number of paraphrase corpora have been proposed over the last years. In this section, we enumerate the most relevant corpora and compare them with TaPaCo. MSRPC The Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Dolan and Brockett, 2005) consists of 10 The paraphrase set ids are kept constant across languages. For example, the English sentences of set 40 are translations of the French sentences of set 40. 6870 1313 297738 He isn’t my cousin. 907;4000;7360;7417 1313 1892 574175 2218079 He’s not my cousin. You’re among friends. 1892 2218509 You’re with friends. 1892 24158 24158 24158 842729 842729 842770 842770 5013251 1481280 3174733 3174734 906575 906578 906758 906785 You guys are among friends. Cat got your tongue? Did the cat get your tongue? Has the cat got your tongue? It is nevertheless a good"
2020.lrec-1.848,D19-5503,0,0.0499583,"hich, to our knowledge, no other paraphrase dataset exists. The dataset is available at https://doi.org/10.5281/zenodo.3707949. Keywords: Multilingual corpus, Paraphrases, Crowdsourcing 1. Introduction Paraphrases are different textual realizations of the same meaning within a single language (Bhagat and Hovy, 2013; Ganitkevitch and Callison-Burch, 2014). Paraphrase detection and generation have become popular tasks in NLP and are increasingly integrated into a wide variety of common downstream tasks such as machine translation, information retrieval, question answering, and semantic parsing (Federmann et al., 2019). Although the availability of large datasets for training and evaluation has facilitated research on paraphrase detection and generation, most of these datasets cover only a single language – in most cases English – or a small number of languages (see Section 4.). Furthermore, some paraphrase datasets focus on lexical and phrasal rather than sentential paraphrases, while others are created (semi-)automatically using machine translation. This paper describes the creation of a paraphrase corpus for 97 languages with a total of 1.9 million sentences. It consists of entire sentences produced by c"
2020.lrec-1.848,ganitkevitch-callison-burch-2014-multilingual,0,0.359684,"araphrases are correct and that most remaining ones are either correct but trivial, or near-paraphrases that neutralize a morphological distinction. The corpus contains a total of 1.9 million sentences, with 200 – 250 000 sentences per language. It covers a range of languages for which, to our knowledge, no other paraphrase dataset exists. The dataset is available at https://doi.org/10.5281/zenodo.3707949. Keywords: Multilingual corpus, Paraphrases, Crowdsourcing 1. Introduction Paraphrases are different textual realizations of the same meaning within a single language (Bhagat and Hovy, 2013; Ganitkevitch and Callison-Burch, 2014). Paraphrase detection and generation have become popular tasks in NLP and are increasingly integrated into a wide variety of common downstream tasks such as machine translation, information retrieval, question answering, and semantic parsing (Federmann et al., 2019). Although the availability of large datasets for training and evaluation has facilitated research on paraphrase detection and generation, most of these datasets cover only a single language – in most cases English – or a small number of languages (see Section 4.). Furthermore, some paraphrase datasets focus on lexical and phrasal"
2020.lrec-1.848,N13-1092,0,0.0885657,"Missing"
2020.lrec-1.848,P15-2070,0,0.0578314,"Missing"
2020.lrec-1.848,tiedemann-2012-parallel,0,0.0519129,"same language are common, and we exploit these alternative translations as paraphrases. Furthermore, contributors can add sentences to lists2 and annotate them with tags.3 The Tatoeba project was started in 2006 with the intention to provide example sentences for particular linguistic constructions and words to help language learners.4 Hence, the material provided by Tatoeba consists mainly of simple, short sentences in colloquial style. All data published on Tatoeba is released under CC-BY 2.0 FR.5 As a starting point, we use the Tatoeba dataset made available as part of the OPUS collection (Tiedemann, 2012).6 OPUS provides sentence alignments for all available language pairs. We do not currently use other annotations provided by OPUS (tokenization, word alignment, parsing). The OPUS version of Tatoeba covers 338 languages and contains a total of 7.8 million sentences. Sentence alignment information is available for 1679 language pairs. 1 https://tatoeba.org/eng/about For example, to specify the original source of the data; cf. https://tatoeba.org/eng/sentences_lists/ index. 3 For example, to indicate morphological or phonological properties of the sentence; cf. https://tatoeba.org/eng/ tags/view"
2020.lrec-1.848,P18-1042,0,0.0134581,"aining the most reliable paraphrase pairs. QQP The Quora Question Pairs corpus (Iyer et al., 2017) contains pairs of equivalent questions extracted from the Quora website. The corpus covers a variety topics but is limited to questions in English. Opusparcus The Open Subtitles Paraphrase Corpus (Creutz, 2018) contains ranked sentential paraphrase pairs extracted from movie subtitles. Its content is thus less formal and more colloquial in style. The corpus is available for six languages, which partially overlap with the largest subcorpora of TaPaCo. ParaNMT-50M The particularity of this corpus (Wieting and Gimpel, 2018) is its construction: instead of matching existing sentences, this corpus pairs English sentences written by humans with English sentences produced by machine translation from Czech. It covers a wide variety of textual domains and is also one of the biggest datasets (50M sentences), thanks to its automatic creation. Multilingual whispers (Federmann et al., 2019) focuses on informal English data (casual online conversations and e-mails). Its creation procedure is most similar to ParaNMT-50M in that paraphrases are explicitly created using a range of techniques (manual translation or paraphras11"
2020.vardial-1.1,2020.vardial-1.24,0,0.254959,"Missing"
2020.vardial-1.1,2020.vardial-1.26,0,0.535455,"y stages of the COVID-19 pandemic. Lock downs and restrictive measures in many countries during this period have impacted universities and research centers worldwide causing significant disruption. We believe that this situation is very likely to have discouraged more teams to participate in this year’s evaluation campaign. 2 Team RDI Akanksha Anumit¸i CUBoulder-UBC Phlyers HeLju NRC Piyush Mishra SUKI The Linguistadors T¨ubingen UAIC UnibucKernel UPB ZHAW-InIT Total SMG ULI System Description Paper X X (Popa and S, tef˘anescu, 2020) * (Ceolin and Zhang, 2020) (Scherrer and Ljubeˇsi´c, 2020) (Bernier-Colborne and Goutte, 2020) (Mishra, 2020) (Jauhiainen et al., 2020a) X X X X X X X X X X X (C¸o¨ ltekin, 2020) (Rebeja and Cristea, 2020) (G˘aman and Ionescu, 2020a) (Zaharia et al., 2020) (Benites et al., 2020) X X X 8 7 1 11 Table 1: The teams that participated in the VarDial Evaluation Campaign 2020 along with their system description papers. *The system description paper by team CUBoulder-UBC does not appear in the VarDial workshop proceedings. CUBoulder-UBC reused a system described in Hulden et al. (2015). 4 Romanian Dialect Identification RDI 4.1 Dataset The training data is composed of news articles from the Mo"
2020.vardial-1.1,W19-1402,0,0.3829,"Missing"
2020.vardial-1.1,W18-3909,1,0.890114,"Missing"
2020.vardial-1.1,P19-1068,1,0.744799,"Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. 1 https://sites.google.com/view/vardial2020/evaluation-campaign 2 For recent surveys on these topics see Zampieri et al. (2020) and Jauhiainen et al. (2019c). License details: http: 1 Proceedings of the 7th VarDial Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14 Barcelona, Spain (Online), December 13, 2020 2 Shared Tasks at VarDial 2020 Romanian Dialect Identification (RDI): In the Romanian Dialect Identification (RDI) shared task, we provided participants with the MOROCO data set (Butnaru and Ionescu, 2019) for training, which contains Moldavian (MD) and Romanian (RO) samples of text collected from the news domain. The task was a binary classification by dialect, in which a classification model is required to discriminate between the Moldavian (MD) and the Romanian (RO) dialects. The task was closed, therefore, participants are not allowed to use external data to train their models. The test set contained newly collected text samples from a different domain, not previously included in MOROCO, resulting in a cross-domain dialect identification task. Social Media Variety Geolocation (SMG): In cont"
2020.vardial-1.1,2020.vardial-1.25,0,0.269727,"Missing"
2020.vardial-1.1,2020.vardial-1.17,0,0.072284,"Missing"
2020.vardial-1.1,N19-1423,0,0.024395,"ainen et al. (2020b). 6.2 Participants and Approaches Unfortunately, the ULI shared task had only one team submitting results to the tracks. The NRC team submitted three runs for each of the shared task tracks. All the runs used BERT-related deep neural networks taking sequences of characters as input similar to what the NRC team used when they won the CLI shared task (Jauhiainen et al., 2019b) in the previous VarDial Evaluation Campaign (BernierColborne et al., 2019). The encoders of the networks were pre-trained on masked language modeling (MLM) and sentence pair classification (SPC) tasks (Devlin et al., 2019). The third run on each track was using only the information on the training set as opposed to the second run, in which the MLM was also done on the unlabeled test set in order to adapt the model. The first run on each track was a plurality voting ensemble of the six models used in the second and third runs of all the tracks. 6.3 Results For the baseline, we used an implementation of the HeLI method equal to the one we used when evaluating language identification methods for 285 languages (Jauhiainen et al., 2017). The baseline and the NRC teams results are listed in Tables 8, 9, and 10. Rank"
2020.vardial-1.1,2020.findings-emnlp.387,0,0.274164,"Missing"
2020.vardial-1.1,goldhahn-etal-2012-building,0,0.217587,"same data format and evaluation methodology. Both constrained and unconstrained submissions were allowed, but only one participating team made use of the latter. Uralic Language Identification (ULI): This shared task focused on discriminating between endangered languages of the Uralic group. In addition to 29 Uralic minority languages, the shared task also featured 149 non-relevant languages. For training, we provided texts from the Wanca 2016 corpora (Jauhiainen et al., 2019a) for the relevant languages while the texts for the non-relevant languages came from the Leipzig corpora collection (Goldhahn et al., 2012). The test set for the relevant languages included sentences from the forthcoming Wanca 2017 corpora (Jauhiainen et al., 2020b) that were not present in the Wanca 2016 corpora. The sentences for the non-relevant languages were from the Leipzig corpora collection. The ULI shared task was divided into three separate tracks using the same training and test data. The difference between the tracks was based on how the submissions were scored: track 1 focused on macro-averaged F-score for the 29 relevant languages, track 2 on micro-averaged F-score for the relevant languages, and track 3 on macro-av"
2020.vardial-1.1,L16-1284,1,0.891978,"Missing"
2020.vardial-1.1,2020.vardial-1.23,1,0.814174,"Missing"
2020.vardial-1.1,D18-1469,1,0.402749,"e SMG task is split into three subtasks covering different language areas: the BCMS subtask is focused on geolocated tweets published in the area of Croatia, Bosnia and Herzegovina, Montenegro and Serbia in the HBS macro-language (Ljubeˇsi´c et al., 2016); the DE-AT subtask focuses on conversations from the microblogging platform Jodel initiated in Germany and Austria, which are written in standard German but commonly contain regional and dialectal forms; the CH subtask is based on Jodel conversations initiated in Switzerland, which were found to be held majoritarily in Swiss German dialects (Hovy and Purschke, 2018). All three subtasks used the same data format and evaluation methodology. Both constrained and unconstrained submissions were allowed, but only one participating team made use of the latter. Uralic Language Identification (ULI): This shared task focused on discriminating between endangered languages of the Uralic group. In addition to 29 Uralic minority languages, the shared task also featured 149 non-relevant languages. For training, we provided texts from the Wanca 2016 corpora (Jauhiainen et al., 2019a) for the relevant languages while the texts for the non-relevant languages came from the"
2020.vardial-1.1,W17-1225,1,0.926999,"Missing"
2020.vardial-1.1,J16-3005,1,0.860747,"h quantile loss. SUKI. This approach divides each geographic area into a fixed grid with 81 areas and uses a n-gram language model to predict the most likely area (Jauhiainen et al., 2020a). The Linguistadors. These submissions are based on classic regression methods (linear regression, lasso regression, and ridge regression) and rely on TF-IDF weighted input features. UnibucKernel. The UnibucKernel team (G˘aman and Ionescu, 2020a) submitted two single systems, a character-level CNN (Zhang et al., 2015) with double regression output, and a Nu-SVR model trained on top of n-gram string kernels (Ionescu et al., 2016). The third system is an ensemble approach based on XGBoost, trained on the predictions provided by the two previously mentioned systems and an LSTMbased one. The LSTM is trained on top of fine-tuned German BERT embeddings. ZHAW-InIT. The ZHAW-InIT team (Benites et al., 2020) uses unsupervised k-means clustering to infer a set of dialect classes which are then used in a classification architecture. Their systems are based 7 either on SVMs with TF-IDF weighted word and character n-gram features, or on the HELI language modeling architecture (ZHAW-InIT (HELI)). The SVM submission to the CH subta"
2020.vardial-1.1,W17-0221,1,0.877702,"Missing"
2020.vardial-1.1,W19-1409,1,0.856366,"Missing"
2020.vardial-1.1,2020.vardial-1.21,1,0.758565,"Missing"
2020.vardial-1.1,W16-4801,1,0.731395,"Missing"
2020.vardial-1.1,2020.vardial-1.27,0,0.363938,"ock downs and restrictive measures in many countries during this period have impacted universities and research centers worldwide causing significant disruption. We believe that this situation is very likely to have discouraged more teams to participate in this year’s evaluation campaign. 2 Team RDI Akanksha Anumit¸i CUBoulder-UBC Phlyers HeLju NRC Piyush Mishra SUKI The Linguistadors T¨ubingen UAIC UnibucKernel UPB ZHAW-InIT Total SMG ULI System Description Paper X X (Popa and S, tef˘anescu, 2020) * (Ceolin and Zhang, 2020) (Scherrer and Ljubeˇsi´c, 2020) (Bernier-Colborne and Goutte, 2020) (Mishra, 2020) (Jauhiainen et al., 2020a) X X X X X X X X X X X (C¸o¨ ltekin, 2020) (Rebeja and Cristea, 2020) (G˘aman and Ionescu, 2020a) (Zaharia et al., 2020) (Benites et al., 2020) X X X 8 7 1 11 Table 1: The teams that participated in the VarDial Evaluation Campaign 2020 along with their system description papers. *The system description paper by team CUBoulder-UBC does not appear in the VarDial workshop proceedings. CUBoulder-UBC reused a system described in Hulden et al. (2015). 4 Romanian Dialect Identification RDI 4.1 Dataset The training data is composed of news articles from the Moldavian and Rom"
2020.vardial-1.1,2020.vardial-1.18,0,0.531349,"Missing"
2020.vardial-1.1,2020.vardial-1.20,0,0.189489,"Missing"
2020.vardial-1.1,L16-1641,1,0.889566,"Missing"
2020.vardial-1.1,2020.vardial-1.19,1,0.750864,"Missing"
2020.vardial-1.1,2020.vardial-1.22,0,0.515168,"Missing"
2020.vardial-1.1,W17-1201,1,0.773425,"Missing"
2020.vardial-1.19,W19-1402,0,0.207892,"Missing"
2020.vardial-1.19,2020.acl-main.747,0,0.0924702,"Missing"
2020.vardial-1.19,N19-1423,0,0.0497397,"d TF-IDF-weighted n-grams of length 3 to 6 occurring at least 5 times in the training corpus. For this and all other experiments presented in the paper, we train and test our systems on lower-cased data, as we found no evidence that casing information would be relevant for geolocation. No further pre-processing was applied to the data. Table 2 (second row) shows that this approach easily beats the centroid baseline for all three subtasks. 4.2 Neural machine learning approaches In recent years, pre-trained language representations have become very successful for various downstream tasks. BERT (Devlin et al., 2019) is currently one of the most popular pre-trained language representation frameworks and is based on the Transformer neural network architecture (Vaswani et al., 2017). Typically, a BERT model is created in two phases. In the pre-training phase, a Transformer is trained from scratch using a masked language modeling task. This task only requires unlabeled data. A 1 We used the MultiOutputRegressor class of the Scikit-Learn toolkit (Pedregosa et al., 2011) to combine the two models. 203 Median distance (km) BCMS DEAT CH Model Centroid baseline SVR with TF-IDF character n-grams Constrained BERT M"
2020.vardial-1.19,C12-1064,0,0.117972,"jana) submission to the SMG task. Our motivation was to investigate how existing classification and regression approaches can be adapted to a double regression task. Most of our work is based on the BERT sentence classification architecture in both constrained and unconstrained settings. We experiment with various pre-trained models, different types of coordinate encoding and other hyperparameters. Finally, we manually analyze the development set predictions made with our best-performing models. 2 Related work One of the first works focusing on predicting geolocation from social media text is Han et al. (2012). The authors investigate feature (token) selection methods for location prediction, showing that traditional predictive algorithms yield significantly better results if feature selection is performed. There has been already a shared task on geolocation prediction at WNUT 2016 (Han et al., 2016). The task focused not only on predicting geolocation from text, but also from various user metadata. The best performing systems were combining the available information via feedforward networks or ensembles. Thomas and Hennig (2018) report significant improvements over the winner of the WNUT-16 shared"
2020.vardial-1.19,W16-3928,0,0.143136,"riment with various pre-trained models, different types of coordinate encoding and other hyperparameters. Finally, we manually analyze the development set predictions made with our best-performing models. 2 Related work One of the first works focusing on predicting geolocation from social media text is Han et al. (2012). The authors investigate feature (token) selection methods for location prediction, showing that traditional predictive algorithms yield significantly better results if feature selection is performed. There has been already a shared task on geolocation prediction at WNUT 2016 (Han et al., 2016). The task focused not only on predicting geolocation from text, but also from various user metadata. The best performing systems were combining the available information via feedforward networks or ensembles. Thomas and Hennig (2018) report significant improvements over the winner of the WNUT-16 shared task by learning separately text and metadata embeddings via different neural network architectures This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 202 Proceedings of the 7th VarDial Workshop on"
2020.vardial-1.19,2020.lrec-1.329,0,0.220743,"et al., 2019). 3 For all BERT experiments reported in this section, we use the hyperparameters specified in Sections 4.3 and 4.4, but report numbers from single runs. 4 We pre-trained the models for 50 000 steps with a batch size of 32, for all three models. For each model, a WordPiece tokenizer with vocabulary size 30 000 is trained on the same data. 5 https://huggingface.co/bert-base-multilingual-uncased 6 https://huggingface.co/EMBEDDIA/crosloengual-bert 7 https://huggingface.co/dbmdz/bert-base-german-uncased 204 and continued pre-training for another five epochs on the SwissCrawl corpus (Linder et al., 2020). For all three tasks, we rely on the provided tokenizers without modifications. This third setup further improves geolocation results (see last line of Table 2). Our final submissions are based on the first BERT setup for the constrained setting, and on the third BERT setup for the unconstrained setting. 4.3 Hyperparameter tuning During our initial experiments, we found the BERT models to be quite sensitive to some hyperparameter settings. We found that the optimal batch sizes for the fine-tuning step depended on the amount of available training data. We obtained the best results with batch s"
2020.vardial-1.3,I11-1062,0,0.0194345,"differ in both the way the language data is represented and their general architecture. 4.1 Approaches to automatic language identification There is a vast amount of literature on language identification and most approaches base their predictions on character n-gram statistics and language model features such as estimated token probabilities. A comprehensive overview of approaches is provided by Jauhiainen et al. (2018). There are also recent approaches based on neural models and representation learning. In our work we focus on two popular tools representing purely statistical models (langID, Lui and Baldwin (2011)) and neural models (fastText, Joulin et al. (2016)). While fastText is also used for language identification, it is designed as a general text classification model, which is used for tasks like sentiment analysis as well. This orientation towards general text classification presumably is the reason for choosing a language representation based on bag of words and bag of word n-grams. If Low Saxon had a unified orthography, one would expect such a model to be more useful for dialect identification, which in this case would have to rely more on differences in lexicon and syntax. The basic struct"
2020.vardial-1.3,tiedemann-2012-parallel,1,0.604228,"daily life (Kornai, 2013). This is especially true for unstandardised languages like Low Saxon, where the lack of a written norm poses challenges for the development of modern NLP applications, which typically rely on large amounts of, ideally, orthographically uniform data. While a reference corpus exists for Middle Low Saxon (ReN-Team, 2019), the few datasets of modern Low Saxon available so far tend to either be very restricted content-wise (e.g. the DSA data (Wrede et al., 1927–1956)) or only represent a fraction of the language area without indication of the dialect (e.g. the OPUS data (Tiedemann, 2012)). In addition, both the DSA data and most of the OPUS data consist of content translated into Low Saxon instead of original texts, which will affect the naturalness of the language. The aim of this dataset for Low Saxon is thus to provide open and for the most part original data in Low Saxon covering nearly the whole language area in order to foster research and facilitate the development of NLP tools. The composition of this dataset and testing the suitability of language recognition tools is a first step in our larger research project on processing Low Saxon data and modelling the historica"
2020.wmt-1.134,2020.lrec-1.467,1,0.814685,"ade available on the WMT website. In this section, we present the parallel and monolingual resources that we used for our systems. 2.1 Inuktitut–English Training data The training resources for the Inuktitut–English tasks are summarized in Table 1. Two allowed parallel resources are provided, the training part of the Nunavut Hansard (NH) corpus (Joanis et al., 2020) and the small WikiTitles corpus. Since the NH training corpus contained a significant proportion of duplicates and preliminary experiments suggested a slight adverse effect of duplicates, we removed them with the OpusFilter tools (Aulamo et al., 2020). We also cleaned the WikiTitles corpus, removing Inuktitut entries not in syllabic script and identical entries. The Inuktitut side of both training corpora was also used to create a parallel corpus for the romanized ↔ syllabic transliteration task. The romanized version was converted from the syllabic one using the uniconv + iconv pipeline proposed by the corpus providers. The NH corpus contains a large amount of unaligned data, which we used as additional monolingual corpora. We removed all sentences that were already covered by one of the parallel NH datasets. The English and Inuktitut par"
2020.wmt-1.134,W18-1207,0,0.0139709,"out-ofvocabulary words, and heavy computational costs due to large vocabularies. Moreover, such wordlevel modeling does not allow the productive recombination of morphemes and is thus unsuitable for morphologically rich languages such as Inuktitut or Sorbian. In recent years, a consensus has emerged that NMT vocabularies should consist of subwords of variable size. Various unsupervised word segmentation algorithms have been proposed, among which byte-pair encoding (BPE) (Sennrich et al., 2016), SentencePiece (Kudo and Richardson, 2018), and several variants of Morfessor (Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Gr¨onroos et al., 2018, 2020). Besides the actual word segmentation algorithm, various parameters influence the quality of the resulting translation system: 4 We use Europarl, NewsCommentary, Taoeba and WMTNews as Czech monolingual corpora, and Training, Sorbian Institute and Witaj as Sorbian monolingual corpora. 1132 • Separate word segmentation models for each language or one joint vocabulary for all languages. The joint approach scales better to multilingual models, and enables consistent segmentation of named entities and cognate Segmentation model Translation model Algorithm Training da"
2020.wmt-1.134,W17-3203,0,0.0157694,".4M CS 708k / 1931k 708k / 1931k 61.90 61.56 55.06 55.04 62.41 62.16 53.78 53.83 Table 4: Segmentation model experiments for German–Upper Sorbian. All segmentation models are joint models with 20 000 units, but trained on variable amounts of data. All translation models are OpenNMT-py Transformers with default settings with active subword sampling, trained either without (1–6) or with (7–8) additional backtranslations. words across languages, assuming they are written in the same script. • The chosen vocabulary size and the amount of training data from which the segmentation model is learned. Denkowski and Neubig (2017) recommend a vocabulary size of 32k units, trained jointly on all languages, for normal-sized datasets. In contrast, Ding et al. (2019) obtain the best results with small vocabularies of only 500 units in low-resource scenarios. Optimal vocabulary size varies thus depending on the size of the parallel and monolingual data. • If the segmentation algorithm is based on a probabilistic model (such as SentencePiece or Morfessor), it can be used to sample different segmentations for any given word. This technique is known as subword regularization (Kudo, 2018) and has been shown to improve the robus"
2020.wmt-1.134,N19-1423,0,0.00664495,"tchOut (Wang et al., 2018), and subword regularization (Kudo, 2018). Sampling fresh noise for each minibatch is important, especially in low-resource conditions where the small data set is reused for many epochs. The denoising sequence autoencoder has previously been applied to language model pretraining in BART (Lewis et al., 2019). Typical noise models for denoising sequence autoencoder apply small changes to the input side of the corpus: local reordering (Lample et al., 2018), deletions (Iyyer et al., 2015), insertions (Vaibhav et al., 2019), substitutions (Wang et al., 2018), and masking (Devlin et al., 2019). Of these, our method applies local reordering and token deletion. Taboo sampling segmentation task. Gr¨onroos et al. (2020) propose taboo sampling, a noise model extending the subword regularization idea specifically for monolingual data. It takes in monolingual text and generates two maximally different segmentations, e.g. dys + functional on the source side and dysfunction + al on the target side. During taboo sampling, all multi-character subwords used in the first segmentation have their probability temporarily set to zero, to ensure that they are not used in the second segmentation. Tra"
2020.wmt-1.134,W19-6620,0,0.0147578,"n. All segmentation models are joint models with 20 000 units, but trained on variable amounts of data. All translation models are OpenNMT-py Transformers with default settings with active subword sampling, trained either without (1–6) or with (7–8) additional backtranslations. words across languages, assuming they are written in the same script. • The chosen vocabulary size and the amount of training data from which the segmentation model is learned. Denkowski and Neubig (2017) recommend a vocabulary size of 32k units, trained jointly on all languages, for normal-sized datasets. In contrast, Ding et al. (2019) obtain the best results with small vocabularies of only 500 units in low-resource scenarios. Optimal vocabulary size varies thus depending on the size of the parallel and monolingual data. • If the segmentation algorithm is based on a probabilistic model (such as SentencePiece or Morfessor), it can be used to sample different segmentations for any given word. This technique is known as subword regularization (Kudo, 2018) and has been shown to improve the robustness of translation models. Gr¨onroos et al. (2020) tested various segmentation model configurations on a multilingual translation tas"
2020.wmt-1.134,W18-6410,1,0.895101,"Missing"
2020.wmt-1.134,2020.lrec-1.486,1,0.885032,"Missing"
2020.wmt-1.134,2020.lrec-1.312,0,0.16699,"n– German tasks can be qualified as low-resource settings, with less than 800K (deduplicated) parallel training instances for the former and 60K for the latter. For both tasks, we follow the constrained setting, which limits the allowed data to those made available on the WMT website. In this section, we present the parallel and monolingual resources that we used for our systems. 2.1 Inuktitut–English Training data The training resources for the Inuktitut–English tasks are summarized in Table 1. Two allowed parallel resources are provided, the training part of the Nunavut Hansard (NH) corpus (Joanis et al., 2020) and the small WikiTitles corpus. Since the NH training corpus contained a significant proportion of duplicates and preliminary experiments suggested a slight adverse effect of duplicates, we removed them with the OpusFilter tools (Aulamo et al., 2020). We also cleaned the WikiTitles corpus, removing Inuktitut entries not in syllabic script and identical entries. The Inuktitut side of both training corpora was also used to create a parallel corpus for the romanized ↔ syllabic transliteration task. The romanized version was converted from the syllabic one using the uniconv + iconv pipeline prop"
2020.wmt-1.134,Q18-1017,0,0.0241358,"ents presented in Tables 3 and 4 already confirmed the positive impact of subword regularization and backtranslation. Row 1 of Tables 5 and 6 provide baseline results with these two techniques. Backtranslated training instances are marked with a special token. Scheduled multi-task learning As row 2 in Table 6 shows, the mere inclusion of a German↔Czech task with language labels but without any task scheduling already increases BLEU scores by 1.5 points. However, simple transfer learning setups such as this are prone to catastrophic forgetting, especially in low-resource settings such as ours. Kiperwasser and Ballesteros (2018) propose a general strategy called scheduled multi-task learning, in which different tasks are mixed according to a task-mix schedule. Gr¨onroos et al. (2020) propose a partwise constant task-mix schedule with an arbitrary number of steps, any of which can be mixing multiple tasks. This flexibility is useful when training with a large number of heterogeneous tasks: multiple language pairs with different amounts of data, data from different domains (oversampling the in-domain data), natural vs synthetic (e.g. back-translated) data, and auxiliary tasks (e.g. autoencoder). A training schedule wit"
2020.wmt-1.134,P17-4012,0,0.0172715,"nroos et al. (2020). The Transformer contains 8 encoder and 8 decoder layers with 16 attention heads each. The hidden layer size is 1024, the filter size 4096. The minibatch varies between 7200 and 9200 tokens, depending on the task, and gradients are accumulated over 4 minibatches. All models were trained for 200 000 steps, which corresponded to 5–7 days training time on a single V100 GPU. The best savepoint was selected on the basis of development set accuracy; this measure turned out to be more stable than development set BLEU score. We use the dynamicdata branch of the OpenNMT-py toolkit (Klein et al., 2017) for our experiments.6 This branch provides the necesWitaj and Sorbian Institute corpora. We added an equivalent amount of German data from NewsCommentary and WMTNews. The Czech data also stems from NewsCommentary and WMT-News and is complemented by a subset of Czech Europarl. 6 https://github.com/Waino/OpenNMT-py The functionality of the dynamicdata branch is included by sary adaptations for the techniques introduced by Gr¨onroos et al. (2020): scheduled multi-task learning requires the ability to adjust the task mix during training, whereas subword regularization and the denoising sentence a"
2020.wmt-1.134,P18-1007,0,0.158466,"urce target language (HRL), Finnish. Among the WMT 2020 shared tasks, the German → Upper Sorbian low-resource translation task exactly corresponds to this setup, with Czech being a high-resource language closely related to Upper Sorbian. We adapt the approach proposed by Gr¨onroos et al. (2020) also to three slightly different scenarios: in the Upper Sorbian → German task, the low-resource language is on the source side, but can be complemented with Czech in the same way; for the English → Inuktitut task, no related high-resource language is available; and for Subword regularization Following Kudo (2018), each time a word is used during training, a new segmentation into subwords is sampled from the probabilistic segmentation model. Monolingual tasks In order to benefit from more easily available monolingual data and to make the model more robust to noise, they propose to include denoising sequence autoencoder tasks. A first variant applies small changes to the input side of the corpus (e.g. word deletions, substitutions and reorderings). A second variant, called taboo sampling, relies on the subword regularization idea and generates two maximally different segmentations of the source and targ"
2020.wmt-1.134,D18-2012,0,0.0118493,"are represented as atomic vocabulary items, leads to sparse statistics, issues with out-ofvocabulary words, and heavy computational costs due to large vocabularies. Moreover, such wordlevel modeling does not allow the productive recombination of morphemes and is thus unsuitable for morphologically rich languages such as Inuktitut or Sorbian. In recent years, a consensus has emerged that NMT vocabularies should consist of subwords of variable size. Various unsupervised word segmentation algorithms have been proposed, among which byte-pair encoding (BPE) (Sennrich et al., 2016), SentencePiece (Kudo and Richardson, 2018), and several variants of Morfessor (Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Gr¨onroos et al., 2018, 2020). Besides the actual word segmentation algorithm, various parameters influence the quality of the resulting translation system: 4 We use Europarl, NewsCommentary, Taoeba and WMTNews as Czech monolingual corpora, and Training, Sorbian Institute and Witaj as Sorbian monolingual corpora. 1132 • Separate word segmentation models for each language or one joint vocabulary for all languages. The joint approach scales better to multilingual models, and enables consistent segmentatio"
2020.wmt-1.134,N16-1162,0,0.0255245,"Missing"
2020.wmt-1.134,P16-1162,0,0.0245308,"t solution however, in which word forms are represented as atomic vocabulary items, leads to sparse statistics, issues with out-ofvocabulary words, and heavy computational costs due to large vocabularies. Moreover, such wordlevel modeling does not allow the productive recombination of morphemes and is thus unsuitable for morphologically rich languages such as Inuktitut or Sorbian. In recent years, a consensus has emerged that NMT vocabularies should consist of subwords of variable size. Various unsupervised word segmentation algorithms have been proposed, among which byte-pair encoding (BPE) (Sennrich et al., 2016), SentencePiece (Kudo and Richardson, 2018), and several variants of Morfessor (Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Gr¨onroos et al., 2018, 2020). Besides the actual word segmentation algorithm, various parameters influence the quality of the resulting translation system: 4 We use Europarl, NewsCommentary, Taoeba and WMTNews as Czech monolingual corpora, and Training, Sorbian Institute and Witaj as Sorbian monolingual corpora. 1132 • Separate word segmentation models for each language or one joint vocabulary for all languages. The joint approach scales better to multilingual"
2020.wmt-1.134,P15-1162,0,0.0601642,"Missing"
2020.wmt-1.134,N18-2074,0,0.0284829,"ween the transliteration and taboo tasks. For Sorbian, the monolingual tasks only help when translating towards German, but not when translating towards Sorbian. One reason for this somewhat surprising finding could be that the Sorbian monolingual data is identical with the Sorbian target of the backtranslations, so that no additional data is added with the monolingual tasks. 5 Submissions and results For the best-performing configurations, we trained two models each, one (“basic”) with the hyperparameters listed above, and an alternative one with relative position distance clipping at 4 (see Shaw et al., 2018). However, this setting did not yield any consistent accuracy gains or losses. For the Inuktitut task, we submitted single systems of settings 2 and 3 for both directions. For EN→IU, the alternative model of setting 2 obtained the best scores on the test set (10.1 BLEU / 0.301 chrF), whereas for IU→EN, the basic model of setting 3 obtained the best scores on the test set (23.0 BLEU / 0.455 chrF). Among the 11 primary submissions in both translation directions, our sub1135 EN→IU IU→EN Training steps 0–200k 0–200k Bilingual Backtranslation Noise EN Noise IU Taboo IU / Translit. rom.→syll. Taboo"
2020.wmt-1.134,tiedemann-2012-parallel,0,0.115013,"ntences with an average character cross-entropy higher than 30 on either side were removed. 2.2 Upper Sorbian–German Training data The training data for the Upper Sorbian–German tasks are summarized in Table 2. The organizers provide a parallel German–Sorbian corpus of 60k sentence pairs that we use without further filtering or processing. Moreover, we use four sources of parallel German–Czech data for both directions: the Europarl and JW300 corpora provided on OPUS, as suggested by the organizers, and additionally the Tatoeba and NewsCommentary corpora, which are also available through OPUS (Tiedemann, 2012). The German side of three datasets2 is backtranslated to Upper Sorbian using a baseline system. The Czech side of the four datasets is backtranslated to Upper Sorbian using an unsupervised character-level translation system (see below). Length filters are applied to all data from external resources (see below). The organizers provide three monolingual Sorbian corpora: Sorbian Institute, a Sorbian Web Crawl, and Witaj. All corpora are backtranslated to German using a baseline system and filtered. As monolingual German and Czech resources, we selected the NewsCommentary corpus and the 2018 and"
2020.wmt-1.134,N19-1190,0,0.0156437,"(Srivastava et al., 2014), label smoothing (Szegedy et al., 2016), SwitchOut (Wang et al., 2018), and subword regularization (Kudo, 2018). Sampling fresh noise for each minibatch is important, especially in low-resource conditions where the small data set is reused for many epochs. The denoising sequence autoencoder has previously been applied to language model pretraining in BART (Lewis et al., 2019). Typical noise models for denoising sequence autoencoder apply small changes to the input side of the corpus: local reordering (Lample et al., 2018), deletions (Iyyer et al., 2015), insertions (Vaibhav et al., 2019), substitutions (Wang et al., 2018), and masking (Devlin et al., 2019). Of these, our method applies local reordering and token deletion. Taboo sampling segmentation task. Gr¨onroos et al. (2020) propose taboo sampling, a noise model extending the subword regularization idea specifically for monolingual data. It takes in monolingual text and generates two maximally different segmentations, e.g. dys + functional on the source side and dysfunction + al on the target side. During taboo sampling, all multi-character subwords used in the first segmentation have their probability temporarily set to"
2020.wmt-1.134,D18-1100,0,0.0211107,"oder task. In the denoising autoencoder (Vincent et al., 2008; Hill et al., 2016) clean text is corrupted by sampling from a noise model, and fed in as a pseudo-source. The target is a reconstruction of the clean input. The goal of the autoencoder tasks is to use monolingual data to strengthen target language modeling in the decoder and source language understanding in the encoder. In addition, the autoencoder task acts as regularization. Noise has been used as a regularizer in many NLP techniques, including dropout (Srivastava et al., 2014), label smoothing (Szegedy et al., 2016), SwitchOut (Wang et al., 2018), and subword regularization (Kudo, 2018). Sampling fresh noise for each minibatch is important, especially in low-resource conditions where the small data set is reused for many epochs. The denoising sequence autoencoder has previously been applied to language model pretraining in BART (Lewis et al., 2019). Typical noise models for denoising sequence autoencoder apply small changes to the input side of the corpus: local reordering (Lample et al., 2018), deletions (Iyyer et al., 2015), insertions (Vaibhav et al., 2019), substitutions (Wang et al., 2018), and masking (Devlin et al., 2019). Of t"
2020.wmt-1.40,D15-1124,0,0.0620164,"Missing"
2020.wmt-1.40,K17-1012,0,0.0195405,"mbiguous source words, we could not identify any substantial progress – at least to the extent that it is measurable by the M U C OW method – in that area over the last year. 1 The M U C OW test suite 1. Identify ambiguous source nouns and their translations, using word-aligned and tagged parallel corpora from the OPUS collection (Tiedemann, 2012). 2. Cluster the translations into senses. First, we query BabelNet (Navigli and Ponzetto, 2012), a wide-coverage multilingual encyclopedic dictionary, to assign senses (synsets) to words. Second, we refine the results with the SW2V sense embeddings (Mancini et al., 2017). Introduction At WMT 2019, we introduced the M U C OW (multilingual contrastive word sense disambiguation) test suite (Raganato et al., 2019) and evaluated the news task submissions of nine translation directions with it.1 We observed that systems generally performed quite well on word sense disambiguation, but found a big gap between indomain and out-of-domain disambiguation performance for some translation directions, in particular with constrained systems. For WMT 2020, we reuse the same test suite for the same language pairs. This gives us the opportunity to measure the advancement of mac"
2020.wmt-1.40,W19-5354,1,0.879826,"in that area over the last year. 1 The M U C OW test suite 1. Identify ambiguous source nouns and their translations, using word-aligned and tagged parallel corpora from the OPUS collection (Tiedemann, 2012). 2. Cluster the translations into senses. First, we query BabelNet (Navigli and Ponzetto, 2012), a wide-coverage multilingual encyclopedic dictionary, to assign senses (synsets) to words. Second, we refine the results with the SW2V sense embeddings (Mancini et al., 2017). Introduction At WMT 2019, we introduced the M U C OW (multilingual contrastive word sense disambiguation) test suite (Raganato et al., 2019) and evaluated the news task submissions of nine translation directions with it.1 We observed that systems generally performed quite well on word sense disambiguation, but found a big gap between indomain and out-of-domain disambiguation performance for some translation directions, in particular with constrained systems. For WMT 2020, we reuse the same test suite for the same language pairs. This gives us the opportunity to measure the advancement of machine translation within a year. We expect the larger training data sets and the model improvements to have a small but positive impact on tran"
2020.wmt-1.40,2020.lrec-1.452,1,0.774552,"st of incorrect target words (the incorrect target synset), and information about the domain of the synsets. The participants only see the source sentences, not the metadata. Table 1 shows a few example sentences taken from the English–German test suite. The main statistics of the test suites used for WMT 2020 are reported in Table 2. Language Source Target In-dom Out-dom Senpair words synsets synsets synsets tences EN–CS EN–DE DE–EN EN–RU 98 176 217 97 200 362 461 199 29 220 329 40 171 142 132 163 1843 3337 4268 1814 Table 2: Sizes of the M U C OW data sets compiled for WMT 2019 and 2020. In Raganato et al. (2020), we report on an extended version of M U C OW that covers the following aspects: • The selection of data sources is improved to reduce noise and domain effects. 3 • The sense inference process is streamlined and relies on lemmatization instead of word alignment, leading to better coverage especially for morphologically rich languages. Evaluation and Results The source language sentences were sent to the WMT participants as part of the test set, and we received the translations in the target language for evaluation. We then checked if any of the correct or incorrect target words listed in the"
2020.wmt-1.40,W18-6456,0,0.0460527,"Missing"
2020.wmt-1.40,tiedemann-2012-parallel,1,0.554765,"n process is also entirely automated. We evaluate all participating systems of the language pairs English → Czech, English ↔ German, and English → Russian and compare the results with those obtained at WMT 2019. While current NMT systems are fairly good at handling ambiguous source words, we could not identify any substantial progress – at least to the extent that it is measurable by the M U C OW method – in that area over the last year. 1 The M U C OW test suite 1. Identify ambiguous source nouns and their translations, using word-aligned and tagged parallel corpora from the OPUS collection (Tiedemann, 2012). 2. Cluster the translations into senses. First, we query BabelNet (Navigli and Ponzetto, 2012), a wide-coverage multilingual encyclopedic dictionary, to assign senses (synsets) to words. Second, we refine the results with the SW2V sense embeddings (Mancini et al., 2017). Introduction At WMT 2019, we introduced the M U C OW (multilingual contrastive word sense disambiguation) test suite (Raganato et al., 2019) and evaluated the news task submissions of nine translation directions with it.1 We observed that systems generally performed quite well on word sense disambiguation, but found a big ga"
2021.americasnlp-1.29,P19-1310,0,0.0459725,"Missing"
2021.americasnlp-1.29,2020.acl-demos.20,1,0.740639,"directed to finding shared task is aimed at developing machine transla- relevant corpora that could help with the translation tion (MT) systems for indigenous languages of the tasks, as well as to make the best out of the data Americas, all of them paired with Spanish (Mager provided by the organizers. In order to have an efet al., 2021). Needless to say, these language pairs ficient procedure to maintain and process the data pose big challenges since none of them benefits sets for all the ten languages, we utilized the Opusfrom large quantities of parallel data and there is Filter toolbox2 (Aulamo et al., 2020). It provides limited monolingual data. For our participation, both ready-made and extensible methods for comwe focused our efforts mainly on three aspects: (1) bining, cleaning, and filtering parallel and monogathering additional parallel and monolingual data lingual corpora. OpusFilter uses a configuration for each language, taking advantage in particular file that lists all the steps for processing the data; of the OPUS corpus collection (Tiedemann, 2012), in order to make quick changes and extensions prothe JHU Bible corpus (McCarthy et al., 2020) and grammatically, we generated the config"
2021.americasnlp-1.29,2020.lrec-1.356,0,0.0833266,"Missing"
2021.americasnlp-1.29,2020.lrec-1.320,0,0.232022,"Missing"
2021.americasnlp-1.29,2020.coling-main.351,0,0.459236,"Missing"
2021.americasnlp-1.29,2020.lrec-1.352,0,0.702383,"parallel data and there is Filter toolbox2 (Aulamo et al., 2020). It provides limited monolingual data. For our participation, both ready-made and extensible methods for comwe focused our efforts mainly on three aspects: (1) bining, cleaning, and filtering parallel and monogathering additional parallel and monolingual data lingual corpora. OpusFilter uses a configuration for each language, taking advantage in particular file that lists all the steps for processing the data; of the OPUS corpus collection (Tiedemann, 2012), in order to make quick changes and extensions prothe JHU Bible corpus (McCarthy et al., 2020) and grammatically, we generated the configuration file translations of political constitutions of various with a Python script. Latin American countries, (2) cleaning and filterFigure 1 shows a part of the applied OpusFiling the corpora to maximize their quality with the ter workflow for a single language pair, Spanish– OpusFilter toolbox (Aulamo et al., 2020), and (3) Raramuri, and restricted to the primary training contrasting different training techniques that could data. The provided training set and (concatenated) take advantage of the scarce data available. 1 https://github.com/Helsinki"
2021.americasnlp-1.29,tiedemann-2012-parallel,1,0.926026,"hem benefits sets for all the ten languages, we utilized the Opusfrom large quantities of parallel data and there is Filter toolbox2 (Aulamo et al., 2020). It provides limited monolingual data. For our participation, both ready-made and extensible methods for comwe focused our efforts mainly on three aspects: (1) bining, cleaning, and filtering parallel and monogathering additional parallel and monolingual data lingual corpora. OpusFilter uses a configuration for each language, taking advantage in particular file that lists all the steps for processing the data; of the OPUS corpus collection (Tiedemann, 2012), in order to make quick changes and extensions prothe JHU Bible corpus (McCarthy et al., 2020) and grammatically, we generated the configuration file translations of political constitutions of various with a Python script. Latin American countries, (2) cleaning and filterFigure 1 shows a part of the applied OpusFiling the corpora to maximize their quality with the ter workflow for a single language pair, Spanish– OpusFilter toolbox (Aulamo et al., 2020), and (3) Raramuri, and restricted to the primary training contrasting different training techniques that could data. The provided training se"
2021.americasnlp-1.29,W19-5441,1,0.88874,"Missing"
2021.americasnlp-1.29,W19-6804,0,0.422544,"Missing"
2021.americasnlp-1.29,2020.loresmt-1.1,0,0.313054,"Missing"
2021.americasnlp-1.29,L16-1144,0,0.334949,"Missing"
2021.americasnlp-1.29,2020.wmt-1.134,1,0.835483,"Missing"
2021.americasnlp-1.29,2020.wmt-1.139,1,0.845344,"Missing"
2021.konvens-1.25,L18-1622,0,0.0631718,"Missing"
2021.konvens-1.25,2020.vardial-1.3,1,0.766582,"esent a dataset for Low Saxon,1 a Germanic minority language spoken by roughly five million people in Northern Central Europe (Moseley, 2010). Despite its relatively large number of speakers, there are hardly any annotated corpora for this language, hampering corpus-based research into more modern varieties and causing a lack of wellfunctioning NLP tools. The dataset is part of our research into the diachronic development of the internal variation in Low Saxon and builds upon the Reference Corpus Middle Low German/Low Rhenish (1200-1650) (ReN-Team, 2019) (henceforth ReN) and the LSDC dataset (Siewert et al., 2020) attempting to fill the gap between them. Therefore, it covers both historical and contemporary Low Saxon dialects from the Veluwe in the western corner of the language area to the Lower-Prussian dialects in the east. Our ultimate goal with this new dataset is to perform analyses of the internal variation within Low Saxon and its change over time. Questions 1 Also called Low German, referring here to the varieties protected under the European Charter for Regional and Minority Languages as Nedersaksisch in the Netherlands and Niederdeutsch in Germany as well as extinct eastern varieties. Figure"
2021.konvens-1.25,2021.findings-acl.433,0,0.0949783,"Missing"
2021.nodalida-main.37,2020.acl-demos.20,1,0.842361,"Missing"
2021.nodalida-main.37,W18-6315,0,0.0187219,"tion from Northern S´ami to Finnish (Pirinen et al., 2017) within the Apertium framework (Forcada et al., 2011). We also combine both methods to further augment the data. Our experiments demonstrate the positive effects of both strategies and the possibility of obtaining complementary information from different backtranslation engines. 2 Related work Using backtranslations from different sources as training data has been shown to be beneficial for improving machine translation quality. In addition to proposing training data augmentation methods that do not require reverse translation systems, Burlot and Yvon (2018) compare the effects of using statistical machine translation (SMT) and NMT based backtranslations for English→French and English→German translations. They show that both types of backtranslations improve translation quality, NMT slightly more than SMT. Poncelas et al. (2019) also produce backtranslations with SMT and NMT. They show that the translation quality of a German→English NMT system is improved when including either type of backtranslations in the training data. The greatest improvement is observed when both types of backtranslations are used. Augmenting training data with RBMT backtr"
2021.nodalida-main.37,D14-1179,0,0.0250003,"Missing"
2021.nodalida-main.37,W19-6908,0,0.0266878,"T based backtranslations for English→French and English→German translations. They show that both types of backtranslations improve translation quality, NMT slightly more than SMT. Poncelas et al. (2019) also produce backtranslations with SMT and NMT. They show that the translation quality of a German→English NMT system is improved when including either type of backtranslations in the training data. The greatest improvement is observed when both types of backtranslations are used. Augmenting training data with RBMT backtranslations has also proven to be useful for boosting translation quality. Dowling et al. (2019) use RBMT backtranslations to improve statistical machine translation performance for Scottish Gaelic→English translations. The authors show that backtranslations can be beneficial even in cases where the translation quality of the MT system used to produce the backtranslations is low. Soto et al. (2019) study the performance of NMT systems trained with augmented training data backtranslated using RBMT, SMT and NMT. They experiment with Basque→Spanish translations and show that the translation performance improves when using each type of augmented training data individually. Soto et al. (2020)"
2021.nodalida-main.37,2020.findings-emnlp.352,0,0.0350446,"as the backtranslation model. For Transformers, we use the example hyperparameters from MarianNMT 4 which replicate the setup 4 https://github.com/marian-nmt/ marian-examples/tree/master/transformer NMT RBMT UiT 19.4 12.3 YLE 4.5 10.0 Table 1: Reverse translation model (sme-fin) quality in BLEU points evaluated with the UiT test set and the YLE test set. from Vaswani et al. (2017). For subword segmentation, we use the SentencePiece tokenizer (Kudo and Richardson, 2018) with vocabulary size 8000, which has been shown to produce the best results with the data set sizes that we are dealing with (Gowda and May, 2020; Gr¨onroos et al., 2021). We train the models until the cross-entropy of the validation set does not improve for 10 consecutive validation steps. For the RBMT backtranslations, we use Apertium with the sme-fin model by Pirinen et al. (2017). This system implements a shallow transfer-based translation engine consisting of modules for morphological analysis, disambiguation and generation, modules for lexical translation based on context rules, and a module for syntactic transformation operations. Table 1 shows the quality of the sme-fin translation models used for backtranslations in BLEU point"
2021.nodalida-main.37,P18-4020,0,0.029041,"Missing"
2021.nodalida-main.37,D18-2012,0,0.0240889,"rection. All models using additional backtranslated training sets are trained with both RNNs and Transformers. All RNN models have the same architecture as the backtranslation model. For Transformers, we use the example hyperparameters from MarianNMT 4 which replicate the setup 4 https://github.com/marian-nmt/ marian-examples/tree/master/transformer NMT RBMT UiT 19.4 12.3 YLE 4.5 10.0 Table 1: Reverse translation model (sme-fin) quality in BLEU points evaluated with the UiT test set and the YLE test set. from Vaswani et al. (2017). For subword segmentation, we use the SentencePiece tokenizer (Kudo and Richardson, 2018) with vocabulary size 8000, which has been shown to produce the best results with the data set sizes that we are dealing with (Gowda and May, 2020; Gr¨onroos et al., 2021). We train the models until the cross-entropy of the validation set does not improve for 10 consecutive validation steps. For the RBMT backtranslations, we use Apertium with the sme-fin model by Pirinen et al. (2017). This system implements a shallow transfer-based translation engine consisting of modules for morphological analysis, disambiguation and generation, modules for lexical translation based on context rules, and a m"
2021.nodalida-main.37,P12-3005,0,0.0489923,"tences are empty or longer than 100 words, • The ratio of the sentence lengths in words is greater than 3, • The sentence pair contains words longer than 40 characters, • The sentence pair contains HTML elements, • The sentences have dissimilar numerals based on the “Non-zero numerals score” (V´azquez et al., 2019), • The sentences have dissimilar punctuation based on the “Terminal punctuation score” (V´azquez et al., 2019), • The sentence pair contains characters outside of the Latin script, • The sentences are not recognized to be their correct language by the langid.py language identifier (Lui and Baldwin, 2012). After filtering, 29,106 clean sentence pairs remain in the parallel data set. From this clean set, 2000 pairs are randomly selected to form a validation set and another 2000 pairs to form a test set, leaving 25,106 pairs for training. Note that all subsets are disjoint due to the initial deduplication. The additional test set consists of two news articles describing S´ami culture in Finland available in both Finnish and Northern S´ami on YLE News. It was extracted from the web and manually aligned to create a clean reference set. This test set 2 https://yle.fi/uutiset/osasto/sapmi/ https://g"
2021.nodalida-main.37,P02-1040,0,0.113675,"r¨onroos et al., 2021). We train the models until the cross-entropy of the validation set does not improve for 10 consecutive validation steps. For the RBMT backtranslations, we use Apertium with the sme-fin model by Pirinen et al. (2017). This system implements a shallow transfer-based translation engine consisting of modules for morphological analysis, disambiguation and generation, modules for lexical translation based on context rules, and a module for syntactic transformation operations. Table 1 shows the quality of the sme-fin translation models used for backtranslations in BLEU points (Papineni et al., 2002). The NMT model performs much better with UiT test data than with the YLE test data, which shows that the NMT system is strongly adapted to the UiT data, while the RBMT system has similar performance with both test sets. 4.1 Backtranslations All the 462,803 sentences of the cleaned monolingual data are translated with the sme-fin NMT and RBMT models. As the quality of the source side of the backtranslations is not as important as the quality of the target side (Sennrich et al., 2016), we keep an unfiltered version of both backtranslation data sets. To see the effect of filtering the augmented"
2021.nodalida-main.37,W17-0214,0,0.13371,"driven translation systems with automatically created synthetic data, e.g. backtranslation (Sennrich et al., 2016). In this paper, we combine both strategies in the context of neural machine translation (NMT) from Finnish to Northern S´ami. In particular, we investigate the impact of RBMT in data augmentation in comparison to standard NMT-based backtranslation. Northern S´ami is a Uralic minority language spoken in Norway, Sweden and Finland. Historically, most of the work on machine translation from and to S´ami languages is based on RBMT (Trosterud and Unhammer, 2012; Antonsen et al., 2017; Pirinen et al., 2017). Data-driven approaches such as NMT are generally more competitive, but require large amounts of training data in the form of parallel translated sentences. For minority languages, finding parallel data sets is usually more difficult than collecting monolingual data, which is also the case for Northern S´ami. A common way of leveraging monolingual data for NMT is the above mentioned backtranslation strategy, a method where monolingual data of the target language is translated automatically to the source language to create additional parallel training data. In this work, we use two reverse tra"
2021.nodalida-main.37,R19-1107,0,0.0350046,"Missing"
2021.nodalida-main.37,P16-1009,0,0.274972,"es the RBMT approach only for the in-domain test set. This suggests that the RBMT system provides general-domain knowledge that cannot be found from the relative small parallel training data. 1 Introduction Machine translation from and to minority languages is challenging because large parallel corpora are typically hard to obtain. Two strategies have proven most successful to eliminate this bottleneck: using rule-based machine translation (RBMT) systems that do not rely on large data, or training data-driven translation systems with automatically created synthetic data, e.g. backtranslation (Sennrich et al., 2016). In this paper, we combine both strategies in the context of neural machine translation (NMT) from Finnish to Northern S´ami. In particular, we investigate the impact of RBMT in data augmentation in comparison to standard NMT-based backtranslation. Northern S´ami is a Uralic minority language spoken in Norway, Sweden and Finland. Historically, most of the work on machine translation from and to S´ami languages is based on RBMT (Trosterud and Unhammer, 2012; Antonsen et al., 2017; Pirinen et al., 2017). Data-driven approaches such as NMT are generally more competitive, but require large amount"
2021.nodalida-main.37,W19-7102,0,0.0294514,"Missing"
2021.nodalida-main.37,2020.acl-main.359,0,0.0180743,"ling et al. (2019) use RBMT backtranslations to improve statistical machine translation performance for Scottish Gaelic→English translations. The authors show that backtranslations can be beneficial even in cases where the translation quality of the MT system used to produce the backtranslations is low. Soto et al. (2019) study the performance of NMT systems trained with augmented training data backtranslated using RBMT, SMT and NMT. They experiment with Basque→Spanish translations and show that the translation performance improves when using each type of augmented training data individually. Soto et al. (2020) also analyze the effects of using augmented training data backtranslated with the three different paradigms. They focus on two language pairs: a low-resource language pair, Basque→Spanish, and a high-resource language pair, German→English. In addition to showing similar results as Soto et al. (2019), they show further improvement in translation performance when all types of augmented training data are combined. 3 Data The UiT freecorpus1 contains a Finnish - Northern S´ami (fin-sme) parallel corpus with 110k sentence pairs and a distinct set of 868k monolingual Northern S´ami sentences. The U"
2021.nodalida-main.37,2012.freeopmt-1.3,0,0.116304,"Missing"
2021.nodalida-main.37,W19-5441,1,0.894614,"Missing"
2021.vardial-1.1,2020.vardial-1.26,0,0.0398438,"Missing"
2021.vardial-1.1,W19-1402,0,0.103765,"Missing"
2021.vardial-1.1,2021.vardial-1.15,0,0.0589664,"Missing"
2021.vardial-1.1,P19-1068,1,0.833848,"ntification (RDI): The 2021 Romanian Dialect Identification shared task is at the third iteration, following the 2019 Moldavian vs. Romanian Cross-Dialect Topic identification (MRC) (Zampieri et al., 2019) and the 2020 Romanian Dialect Identification (RDI) (G˘aman et al., 2020) shared tasks. The 2021 RDI shared task is formulated as a cross-domain binary classification by dialect problem, in which a classification model is required to discriminate between the Moldavian (MD) and the Romanian (RO) subdialects. This year, we provided participants with an augmented version of the MOROCO data set (Butnaru and Ionescu, 2019) for training, which contains Moldavian and Romanian samples of text collected from the news domain. Last year’s test set of tweets (G˘aman and Ionescu, 2020b) is used for validation. A new set of tweets has been collected for the 2021 shared task. The task has two formats, open and closed. In the closed format, participants are not allowed to use external data to train their models. In the open format, participants are allowed to use external resources such as unlabeled corpora, lexicons and pre-trained embeddings (e.g. BERT), but the use of additional labeled data is still not allowed. Urali"
2021.vardial-1.1,2021.vardial-1.12,0,0.478027,"nnada) grammar with English lexicon or English grammar with south Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mixing. The language tag of the comment were given. The challenge of the task was to identify the language of the given comment. It was 2 http://urn.fi/urn:nbn:fi: lb-2020102201 2 Team DLI RDI SMG ULI HeLju HWR LAST NAYEL NRC Phlyers SUKI UnibucKernel UPB System Description Papers (Scherrer and Ljubeˇsi´c, 2021) (Jauhiainen et al., 2021b) (Bestgen, 2021) (Bernier-Colborne et al., 2021) (Ceolin, 2021) (Jauhiainen et al., 2021a) (G˘aman et al., 2021) (Zaharia et al., 2021) Table 1: The teams that participated in the VarDial Evaluation Campaign 2021. a challenging task, since Tamil, Malayalam and Kannada are closely related languages, some of the words being common in all these languages. The participants had to train a system to identify the language of each comment. Our dataset size is 16,672 comments for training and 4,588 for testing. There were three language tags such as Tamil, Malayalam and Kannada. A new category Not in intended language was added to include comments written in a lan"
2021.vardial-1.1,2020.vardial-1.25,0,0.247563,"opment data for training and (ii) the idea of adapting the language model to the test set. The team that was ranked in the second place is UPB. Their best submission is an ensemble that comprises several deep models, including a Romanian BERT. Different from their last year’s participation (Zaharia et al., 2020), they carefully split the training set into sentences. This idea was borrowed from top-ranked teams of the 2020 RDI shared task. Phlyers ranked on the third place in the 2021 ranking, without significant differences in terms of performance with respect to their previous participation (Ceolin and Zhang, 2020). Despite having access to significantly more in-domain data compared with the previous RDI shared task, the participants were not able to report significant performance gains. Indeed, the top scoring team (C ¸ o¨ ltekin, 2020) in 2020 reached a macro F1 score of 0.7876, while the top scoring team in 2021 achieved a macro F1 score of 0.7772. Although the test sets are not identical, we 6 Social Media Variety Geolocation (SMG) 6.1 Dataset The SMG task is based on three datasets from two Social Media platforms, Jodel and Twitter. Since its first edition in 2020, the datasets have been expanded."
2021.vardial-1.1,2020.sltu-1.25,1,0.720696,"format and evaluation methodology. 4 3 Participating Teams A total of nine teams submitted runs to one or more shared tasks in this year’s VarDial evaluation campaign. In Table 1, we list the teams that participated in the shared tasks, including references to the 8 system description papers which will be published as parts of the VarDial workshop proceedings. Detailed information about the submissions in each respective task is included in the following sections of this report. 4.1 Dravidian Language Identification (DLI) Dataset The DLI task is based on three datasets from YouTube comments (Chakravarthi et al., 2020b,a; Hande et al., 2020). In the 2021 (DLI) shared task, participants have to train a model on comments written in Roman script. Our corpora contains all the three types of code-mixed sentences: InterSentential switch, Intra-Sentential switch and Tag switching. All comments were written in Roman script (Non-native script) with either one of the south Dravidian (Tamil, Malayalam, and Kannada) grammar with English lexicon or English grammar with south Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mi"
2021.vardial-1.1,2020.sltu-1.28,1,0.713401,"format and evaluation methodology. 4 3 Participating Teams A total of nine teams submitted runs to one or more shared tasks in this year’s VarDial evaluation campaign. In Table 1, we list the teams that participated in the shared tasks, including references to the 8 system description papers which will be published as parts of the VarDial workshop proceedings. Detailed information about the submissions in each respective task is included in the following sections of this report. 4.1 Dravidian Language Identification (DLI) Dataset The DLI task is based on three datasets from YouTube comments (Chakravarthi et al., 2020b,a; Hande et al., 2020). In the 2021 (DLI) shared task, participants have to train a model on comments written in Roman script. Our corpora contains all the three types of code-mixed sentences: InterSentential switch, Intra-Sentential switch and Tag switching. All comments were written in Roman script (Non-native script) with either one of the south Dravidian (Tamil, Malayalam, and Kannada) grammar with English lexicon or English grammar with south Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mi"
2021.vardial-1.1,W19-1409,1,0.900062,"Missing"
2021.vardial-1.1,W18-3929,1,0.901892,"Missing"
2021.vardial-1.1,W14-5316,0,0.049676,"Missing"
2021.vardial-1.1,2021.vardial-1.10,1,0.840728,"Missing"
2021.vardial-1.1,2020.vardial-1.21,1,0.873523,"Missing"
2021.vardial-1.1,2020.vardial-1.1,1,0.844419,"Missing"
2021.vardial-1.1,2021.vardial-1.9,1,0.849095,"Missing"
2021.vardial-1.1,2020.vardial-1.23,1,0.84553,"Missing"
2021.vardial-1.1,W17-0221,1,0.89467,"Missing"
2021.vardial-1.1,2020.peoples-1.6,1,0.765867,"logy. 4 3 Participating Teams A total of nine teams submitted runs to one or more shared tasks in this year’s VarDial evaluation campaign. In Table 1, we list the teams that participated in the shared tasks, including references to the 8 system description papers which will be published as parts of the VarDial workshop proceedings. Detailed information about the submissions in each respective task is included in the following sections of this report. 4.1 Dravidian Language Identification (DLI) Dataset The DLI task is based on three datasets from YouTube comments (Chakravarthi et al., 2020b,a; Hande et al., 2020). In the 2021 (DLI) shared task, participants have to train a model on comments written in Roman script. Our corpora contains all the three types of code-mixed sentences: InterSentential switch, Intra-Sentential switch and Tag switching. All comments were written in Roman script (Non-native script) with either one of the south Dravidian (Tamil, Malayalam, and Kannada) grammar with English lexicon or English grammar with south Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mixing. The language tag o"
2021.vardial-1.1,W19-1419,1,0.865245,"Missing"
2021.vardial-1.1,2021.vardial-1.14,1,0.79889,"poro movie la Enna irukunu baki ellam. 4.2 Results 4 Participants and Approaches Due to the short time between the announcement of the shared task and the submission deadline, the participation was lower than we expected. Four teams submitted results to the shared task. Bestgen (2021) proposed a logistic regression model based on n-grams of characters with maximum length as features to classify the comments. The authors achieved a high score with simple techniques. The authors also analyzed the results in detail. For more information, the reader should look at the working notes of the author. Jauhiainen et al. (2021b) submitted results using two models, a Na¨ıve Bayes (NB) classifier with adaptive language models, which was shown to obtain competitive performance in many language and dialect identification tasks, and a transformerbased model, which is widely regarded as the stateof-the-art in a number of NLP tasks. Their first Table 2: The results of all entries by the four team participating in the DLI shared task in terms of Macro-F1. Given the difficulty of the DLI 2021 task, the level of performance achieved by the systems is appreciable. Identifying the Other-language category was particularly diffi"
2021.vardial-1.1,2021.vardial-1.13,0,0.185158,"h Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mixing. The language tag of the comment were given. The challenge of the task was to identify the language of the given comment. It was 2 http://urn.fi/urn:nbn:fi: lb-2020102201 2 Team DLI RDI SMG ULI HeLju HWR LAST NAYEL NRC Phlyers SUKI UnibucKernel UPB System Description Papers (Scherrer and Ljubeˇsi´c, 2021) (Jauhiainen et al., 2021b) (Bestgen, 2021) (Bernier-Colborne et al., 2021) (Ceolin, 2021) (Jauhiainen et al., 2021a) (G˘aman et al., 2021) (Zaharia et al., 2021) Table 1: The teams that participated in the VarDial Evaluation Campaign 2021. a challenging task, since Tamil, Malayalam and Kannada are closely related languages, some of the words being common in all these languages. The participants had to train a system to identify the language of each comment. Our dataset size is 16,672 comments for training and 4,588 for testing. There were three language tags such as Tamil, Malayalam and Kannada. A new category Not in intended language was added to include comments written in a language other than the Dravidian languages. A sample comment from our data"
2021.vardial-1.1,W17-1201,1,0.533591,"Missing"
2021.vardial-1.1,C16-1322,1,0.889269,"Missing"
2021.vardial-1.1,W18-3901,1,0.749269,"Missing"
2021.vardial-1.1,2021.vardial-1.16,1,0.843333,"Missing"
2021.vardial-1.1,W14-5307,1,0.728758,"Missing"
2021.vardial-1.16,2020.vardial-1.24,0,0.0617826,"Missing"
2021.vardial-1.16,N19-1423,0,0.0391256,"ects, pages 135–140 April 20, 2021 ©2021 Association for Computational Linguistics Instances Training Development Test BCMS CH DE-AT Task 353 953 38 013 4 189 25 261 2 416 2 438 318 487 29 122 31 515 Table 1: Data characteristics. the unconstrained one. We will inspect last year’s submissions in more detail in Section 4.2. 3 4 Experiments Due to lack of time (the two evaluation campaigns were held just a few months apart due to the *ACL conference bidding procedure), all our experiments are based on our successful 2020 submissions (Scherrer and Ljubeˇsi´c, 2020): we use the BERT architecture (Devlin et al., 2019) with a fully connected layer on top of the CLS token. This fully connected layer implements double regression with a two-dimensional output vector and Mean Absolute Error loss. We provide both constrained submissions, where Median distance Mean distance Eps. BCMS 3k 30k 92.74 59.93 129.14 109.51 28 23 CH 3k 30k 22.94 21.20 33.01 30.60 11 9 DE-AT 3k 30k 182.76 160.67 205.90 186.35 4 2 Table 2: Effect of different vocabulary sizes on constrained model performance, evaluated on the development set. Eps. refers to the number of fine-tuning epochs to reach minimum median distance. Data The VarDial"
2021.vardial-1.16,2020.vardial-1.1,1,0.922113,"Missing"
2021.vardial-1.16,C12-1064,0,0.0295633,"itter in the case of BCMS (Ljubeˇsi´c et al., 2016) and Jodel in the case of DEAT and CH (Hovy and Purschke, 2018). This paper describes the HeLju (Helsinki– Ljubljana) submission to the SMG task. Following our successful participation in 2020 (Scherrer and Ljubeˇsi´c, 2020), we again propose systems based on the BERT architecture in both constrained and unconstrained settings. We report experiments with different tokenization parameters and with newly available pre-trained models. Furthermore, we Related work One of the first works focusing on predicting geolocation from social media text is Han et al. (2012). The authors investigate feature (token) selection methods for location prediction, showing that traditional predictive algorithms yield significantly better results if feature selection is performed. There has been already a shared task on geolocation prediction at WNUT 2016 (Han et al., 2016). The task focused not only on predicting geolocation from text, but also from various user metadata. The best performing systems combined the available information via feedforward networks or ensembles. Thomas and Hennig (2018) report significant improvements over the winner of the WNUT-16 shared task"
2021.vardial-1.16,W16-3928,0,0.0188952,"stems based on the BERT architecture in both constrained and unconstrained settings. We report experiments with different tokenization parameters and with newly available pre-trained models. Furthermore, we Related work One of the first works focusing on predicting geolocation from social media text is Han et al. (2012). The authors investigate feature (token) selection methods for location prediction, showing that traditional predictive algorithms yield significantly better results if feature selection is performed. There has been already a shared task on geolocation prediction at WNUT 2016 (Han et al., 2016). The task focused not only on predicting geolocation from text, but also from various user metadata. The best performing systems combined the available information via feedforward networks or ensembles. Thomas and Hennig (2018) report significant improvements over the winner of the WNUT-16 shared task by separately learning text and metadata embeddings via different neural network architectures (LSTM, feedforward), merging those embeddings and performing the final classification via a softmax layer. During the last iteration of the VarDial social media geolocation shared task (Gaman et al., 2"
2021.vardial-1.16,D18-1469,0,0.0179281,"-longitude coordinate pairs. This contrasts with most other VarDial tasks, in which the goal is to choose from a finite set of variety labels. The second edition of the SMG task is run at VarDial 2021 (Chakravarthi et al., 2021), with the same three language areas as in the previous year: the BosnianCroatian-Montenegrin-Serbian (BCMS) language area, the German language area comprised of Germany and Austria (DE-AT), and German-speaking Switzerland (CH). All three datasets are based on social media data, Twitter in the case of BCMS (Ljubeˇsi´c et al., 2016) and Jodel in the case of DEAT and CH (Hovy and Purschke, 2018). This paper describes the HeLju (Helsinki– Ljubljana) submission to the SMG task. Following our successful participation in 2020 (Scherrer and Ljubeˇsi´c, 2020), we again propose systems based on the BERT architecture in both constrained and unconstrained settings. We report experiments with different tokenization parameters and with newly available pre-trained models. Furthermore, we Related work One of the first works focusing on predicting geolocation from social media text is Han et al. (2012). The authors investigate feature (token) selection methods for location prediction, showing that"
2021.vardial-1.16,2020.vardial-1.21,0,0.247774,"Missing"
2021.vardial-1.16,2020.lrec-1.329,0,0.0250875,"Missing"
2021.vardial-1.16,2021.bsnlp-1.5,1,0.835998,"Missing"
2021.vardial-1.16,C16-1322,1,0.288624,"Missing"
2021.vardial-1.16,2020.vardial-1.19,1,0.846959,"Missing"
2021.wnut-1.52,2020.coling-main.583,0,0.153727,"Missing"
2021.wnut-1.52,P17-4012,0,0.0504319,"Missing"
2021.wnut-1.52,P16-1009,0,0.0406311,"have a positive effect on all languages but Danish and Indonesian–English.6 In general, the accuracies of the BERT constraints lie about halfway between the unconstrained and the oracle ones. 6 Including synthetic training data from back-translation The results of the language-independent models of Section 4 suggest that the provided training data is of insufficient size to train reliable translation models, especially neural ones. A well-known strategy to augment the training data in MT is backtranslation, where target language data is translated to the source language by an auxiliary model (Sennrich et al., 2016). The resulting parallel data (a standard target side, and a noisy source side) is then included in the training data of the main model. In the normalization setting, this amounts to finding “clean” data and running it through a model that produces a noisy version of it. To this end, we used filtered subsets of the monolingual OpenSubtitles corpora from OPUS7 (Tiedemann, 2012) as input data for producing back-translations. We filtered the OPUS data using the OpusFilter package (Aulamo et al., 2020) and the following filters: • The length of the line lies between 5 and 25 words (this correspond"
2021.wnut-1.52,C18-1112,0,0.0260908,"der Goot et al., human supervision is low (Zupan et al., 2019). 2021). The main motivation behind lexical norWhile neural approaches have almost entirely malization is to minimize the variability of the lin- replaced statistical ones in “standard” translation guistic signal, either for computational usage or settings (translating between distinct languages), human consumption. Accordingly, the shared task recent studies have shown that SMT-based apsubmissions are evaluated both intrinsically and proaches remain competitive for normalization extrinsically (on a dependency parsing task). tasks (Tang et al., 2018; Bollmann, 2019). The need for lexical normalization for computaNormalization systems not based on translation tional usage is diminishing these days, given the architectures have also been proposed. For examend-to-end methodology that is becoming more ple, MoNoise (van der Goot, 2019) generates a list and more popular, where the systems are robust of normalization candidates for each token and then 465 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 465–472 November 11, 2021. ©2021 Association for Computational Linguistics Code Language"
2021.wnut-1.52,2020.lrec-1.769,0,0.15916,"Missing"
C16-2019,P16-1231,0,0.0263933,"Missing"
C16-2019,Q13-1034,0,0.0212259,"Missing"
C16-2019,D14-1082,0,0.0124588,"mmar-based constituency parser using both attachment rules (to build phrase-structure representations) and specific procedures This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 The parsing quality is not identical for all languages. The best results are achieved with English and French, then German, Spanish, Italian, then Portuguese and Greek, and finally Romanian. 2 For instance, the Stanford parser (Klein & Manning, 2003; Chen & Manning, 2014), the MaltParser (Nivre et al. 2007), TreeTagger (Schmidt, 1995), Mate Tools (Bohnet et al., 2013), SyntaxNet (Andor et al, 2016), Marmot (Mueller et al, 2013). 3 The Sketch engine (Kilgarriff et al., 2014), mwetoolkit (Ramisch, 2015). 89 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 89–92, Osaka, Japan, December 11-17 2016. to compute properties such as long-distance dependencies, argument-structure building, coordination structures, and so on. It uses an information-rich lexical database containing inflected words, le"
C16-2019,D13-1032,0,0.0532091,"Missing"
C16-2019,petrov-etal-2012-universal,0,0.0224736,"Missing"
C16-2019,W07-1216,1,0.768863,"the scripts described below is quite easy. Furthermore, by default, the parser handles collocations and other MWEs, as well as anaphora resolution (limited to 3rd person personal pronouns). When used in the tagger mode, it can be set to display grammatical functions and collocations (see below for details). The following sections give a short description of the Fips parser, which is at the core of all the tools, some specific details and descriptions of the parser/tagger tool, and finally a description of the collocation extraction tool. 2 The Fips parser/tagger The Fips multilingual parser (Wehrli, 2007; Wehrli & Nerima, 2015) is a grammar-based constituency parser using both attachment rules (to build phrase-structure representations) and specific procedures This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 The parsing quality is not identical for all languages. The best results are achieved with English and French, then German, Spanish, Italian, then Portuguese and Greek, and finally Romanian. 2 For instance, the Stanford parse"
D10-1112,W09-0807,0,0.0986682,"Missing"
D10-1112,hughes-etal-2006-reconsidering,0,0.0902646,"Missing"
D10-1112,N01-1020,0,0.0441157,"German dialects has been taken by the ChochichästliOrakel.1 By specifying the pronunciation of ten predefined words, the web site creates a probability map that shows the likelihood of these pronunciations in the Swiss German dialect area. Our model is heavily inspired by this work, but extends the set of cues to the entire lexicon. As mentioned, the ID model is based on a large Swiss German lexicon. Its derivation from a Standard German lexicon can be viewed as a case of lexicon induction. Lexicon induction methods for closely related languages using phonetic similarity have been proposed by Mann and Yarowsky (2001) and Schafer and Yarowsky (2002), and applied to Swiss German data by Scherrer (2007). The extraction of digital data from hand-drawn dialectological maps is a time-consuming task. Therefore, the data should be made available for different uses. Our Swiss German raw data is accessible 1 http://dialects.from.ch 1152 on an interactive web page (Scherrer, 2010), and we have proposed ideas for reusing this data for machine translation and dialect parsing (Scherrer and Rambow, 2010). An overview of digital dialectological maps for other languages is available on http://www.ericwheeler.ca/atlaslist."
D10-1112,W02-2026,0,0.0235376,"n by the ChochichästliOrakel.1 By specifying the pronunciation of ten predefined words, the web site creates a probability map that shows the likelihood of these pronunciations in the Swiss German dialect area. Our model is heavily inspired by this work, but extends the set of cues to the entire lexicon. As mentioned, the ID model is based on a large Swiss German lexicon. Its derivation from a Standard German lexicon can be viewed as a case of lexicon induction. Lexicon induction methods for closely related languages using phonetic similarity have been proposed by Mann and Yarowsky (2001) and Schafer and Yarowsky (2002), and applied to Swiss German data by Scherrer (2007). The extraction of digital data from hand-drawn dialectological maps is a time-consuming task. Therefore, the data should be made available for different uses. Our Swiss German raw data is accessible 1 http://dialects.from.ch 1152 on an interactive web page (Scherrer, 2010), and we have proposed ideas for reusing this data for machine translation and dialect parsing (Scherrer and Rambow, 2010). An overview of digital dialectological maps for other languages is available on http://www.ericwheeler.ca/atlaslist. 3 Swiss German dialects The Ger"
D10-1112,P07-3010,1,0.843,"ten predefined words, the web site creates a probability map that shows the likelihood of these pronunciations in the Swiss German dialect area. Our model is heavily inspired by this work, but extends the set of cues to the entire lexicon. As mentioned, the ID model is based on a large Swiss German lexicon. Its derivation from a Standard German lexicon can be viewed as a case of lexicon induction. Lexicon induction methods for closely related languages using phonetic similarity have been proposed by Mann and Yarowsky (2001) and Schafer and Yarowsky (2002), and applied to Swiss German data by Scherrer (2007). The extraction of digital data from hand-drawn dialectological maps is a time-consuming task. Therefore, the data should be made available for different uses. Our Swiss German raw data is accessible 1 http://dialects.from.ch 1152 on an interactive web page (Scherrer, 2010), and we have proposed ideas for reusing this data for machine translation and dialect parsing (Scherrer and Rambow, 2010). An overview of digital dialectological maps for other languages is available on http://www.ericwheeler.ca/atlaslist. 3 Swiss German dialects The German-speaking area of Switzerland encompasses the Nort"
D10-1112,2010.jeptalnrecital-demonstration.9,1,0.729486,"d on a large Swiss German lexicon. Its derivation from a Standard German lexicon can be viewed as a case of lexicon induction. Lexicon induction methods for closely related languages using phonetic similarity have been proposed by Mann and Yarowsky (2001) and Schafer and Yarowsky (2002), and applied to Swiss German data by Scherrer (2007). The extraction of digital data from hand-drawn dialectological maps is a time-consuming task. Therefore, the data should be made available for different uses. Our Swiss German raw data is accessible 1 http://dialects.from.ch 1152 on an interactive web page (Scherrer, 2010), and we have proposed ideas for reusing this data for machine translation and dialect parsing (Scherrer and Rambow, 2010). An overview of digital dialectological maps for other languages is available on http://www.ericwheeler.ca/atlaslist. 3 Swiss German dialects The German-speaking area of Switzerland encompasses the Northeastern two thirds of the Swiss territory, and about two thirds of the Swiss population define (any variety of) German as their first language. In German-speaking Switzerland, dialects are used in speech, while Standard German is used nearly exclusively in written contexts"
D19-6506,N18-1118,0,0.11533,"2013; • we provide a thorough analysis of two popular machine translation datasets in terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics recent work explores the feasability of extending NMT models to make them context-aware. A common approach is to use additional encoders for the context sentence(s) with a modified attention mechanism (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). Another technique (Miculicich et al., 2018; Maruf et al., 2019) explores the integration of context through a hierarchical architecture which models the contextual information in a structured manner using word-level and sentencelevel abstractions. The different models have been evaluated on different language pairs and different datasets. In this paper, we focus on a single language pair, English–German (in both directions), and on two textual domains: news translation and movie subtitles translation. For the news translation task (denoted as WMT) we rely on the establis"
D19-6506,W12-3156,0,0.0319356,"translation. Given the overall training data sizes, NMT models should thus be able to pick up this signal. Overall, the examined discourse-level features show consistent patterns across the training, validation and test sets. This was not necessarily expected for the WMT corpus, whose training set stems from a wide variety of sources.9 Three other discourse-level features could have been analyzed as well: We did not include verbal tenses, as we do not expect them to be particularly problematic for the German–English language pair. Likewise, we did not include measures for lexical consistency (Carpuat and Simard, 2012), as this was already reported to be handled well in SMT. Finally, we did not include ellipsis (Voita et al., 2019) as we found it difficult to detect and not very relevant for German. Negations: We establish a list of sentential and nominal negation words for both languages (cf. Table 7) and count the number of sentences that contain at least one negation word. We also count negation discrepancies, i.e. aligned sentence pairs where a negation was identified in one language but not in the other. While the overall frequencies of negations are similar in both corpora, there are significantly mor"
D19-6506,2012.eamt-1.60,0,0.0133131,"sCommentary v14, and Rapid2019 collections. We select the Newstest2015 and Newstest2016 corpora as our validation set and the Newstest2018 corpus as our test set. General properties of this dataset can be found in Table 1. Table 1 shows that the two datasets are comparable in terms of sentence numbers.4 However, the documents in OST are up to 50 times larger than those in WMT (cf. column Sents/Doc). On the other hand, WMT sentences are more than twice as long than OST sentences (cf. column Tokens/Sent), which is in line with our expectations. A third dataset based on transcripts of TED talks (Cettolo et al., 2012), has also been used for document-level translation (Agrawal et al., 2018). We do not consider this dataset for training due to its smaller size, but use the PROTEST test suite, which is based on this corpus, for evaluation (Guillou and Hardmeier, 2016; Guillou et al., 2018). 3.1 whereas pronouns are three to four times rarer in the WMT corpus.6 This divergence is to be expected, as OST consists mainly of dialogues. Not all pronouns are intrinsically hard to translate. Therefore, we also examine how many ambiguous pronouns occur in the corpora. To this end, the English and German corpora are w"
D19-6506,P11-2031,0,0.0483706,"-correlation and lexical matches using hunalign (Varga et al., 2005) to link the system output to the reference translations. The reported results from the fixed-size models are based on this approach. Evaluation Each system is evaluated on the respective test set using the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. In particular, we evaluate each of them on three variants of the test set: 5.1 Generic translation metrics We report BLEU and METEOR scores for all our experiments in Tables 4 and 5. The results and significance tests were computed using MultEval (Clark et al., 2011). By and large, the concatenation models are able to exploit contextual information: BLEU as well as METEOR scores decrease by statistically significant amounts if the context is inconsistent or absent. However, it is difficult to distinguish a winning configuration. In particular, the system that obtains the highest absolute scores is not necessarily the one that learns most from contextual information. The 1Prev+Curr → 1Prev+Curr system obtains the highest absolute scores among sliding window systems in all four tasks, but is not particularly affected by context inconsistencies. On the other"
D19-6506,W15-1003,0,0.0142582,"as been looked at (LapshinovaKoltunski and Hardmeier, 2017; Voita et al., 2018; Lapshinova-Koltunski et al., 2019). Other devices studied are verbal tenses (Gong et al., 2012; Lo´aiciga et al., 2014; Ramm and Fraser, 2016) and connectives (Meyer et al., 2012; Meyer and Popescu-Belis, 2012), although not using neural models. Motivated by approximating the ability of systems to grasp more abstract properties related to coherence, ambiguous words have also been targeted (Rios Gonzales et al., 2017; Bawden et al., 2018; Rios et al., 2018), as well as ellipsis (Voita et al., 2019). Last, negation (Fancellu and Webber, 2015) is a rather understudied phenomenon, but like pronouns and their antecedents, the scope of the negation can be in a different sentence. In this paper we investigate these features in the training data and assess translation using standard automatic metrics and a data scrambling strategy. 2.2 Context-aware NMT Tiedemann and Scherrer (2017) present a simple approach to context-aware NMT: instead of training the model on pairs of single source and target sentences, they add sentences from the left context to the sentence to be translated, either only on the source side or both on source and targ"
D19-6506,D12-1026,0,0.0235191,"titles (referred to as OST), we use data from the OpenSubtitles corpus released on OPUS3 with our own split into training, development and test data. More details about the data and our setup will be given in the following section. Guillou and Hardmeier, 2016; M¨uller et al., 2018; Guillou et al., 2018) have been largely at the center of attention, and more recently the translation of pronouns in the context of their coreferential chains has been looked at (LapshinovaKoltunski and Hardmeier, 2017; Voita et al., 2018; Lapshinova-Koltunski et al., 2019). Other devices studied are verbal tenses (Gong et al., 2012; Lo´aiciga et al., 2014; Ramm and Fraser, 2016) and connectives (Meyer et al., 2012; Meyer and Popescu-Belis, 2012), although not using neural models. Motivated by approximating the ability of systems to grasp more abstract properties related to coherence, ambiguous words have also been targeted (Rios Gonzales et al., 2017; Bawden et al., 2018; Rios et al., 2018), as well as ellipsis (Voita et al., 2019). Last, negation (Fancellu and Webber, 2015) is a rather understudied phenomenon, but like pronouns and their antecedents, the scope of the negation can be in a different sentence. In this pap"
D19-6506,W19-5321,0,0.244825,"r only on the source side or both on source and target sides. These models are evaluated on a German–English corpus extracted from OpenSubtitles, and the best results are obtained with two source sentences and one target sentence. Agrawal et al. (2018) extend these experiments by considering additional contexts. They evaluate their work on the IWSLT 2017 dataset for English–Italian, which consists of transcripts of TED talks. In 2019, the WMT conference featured for the first time a document-level translation task for English–German (Barrault et al., 2019). One of the best-performing systems (Junczys-Dowmunt, 2019) is based on a similar idea: all sentences of a document are concatenated and translated as a whole. Documents whose length exceeds the maximum sequence length defined by the model are simply split. The approaches outlined above, which we refer to as “concatenation models”, do not require any change to the NMT model architecture. Other 3 Two datasets for English–German document-level translation Different text genres and types exhibit different types of discourse-level properties. The choice of training corpus therefore determines what features a NMT model can potentially learn, and the choice"
D19-6506,E12-3001,0,0.029018,"paper, we investigate the discourse-related biases in data. Our contributions are twofold: 2 2.1 Related work Discourse Research about discourse and MT has shifted from explicitly enhancing systems with discourse knowledge to evaluating how much the systems have learned specific discourse features through different resources, test suites being a popular one (cf. Sim Smith, 2017; Popescu-Belis, 2019). Throughout, however, particular discourse phenomena are consistently targeted, as they are indeed indicators of globally good, cohesive and coherent texts. Pronouns (Hardmeier and Federico, 2010; Guillou, 2012; Hardmeier et al., 2013; • we provide a thorough analysis of two popular machine translation datasets in terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics recent work explores the feasability of extending NMT models to make them context-aware. A common approach is to use additional encoders for the context sentence(s) with a modified attention mechanism"
D19-6506,P18-4020,0,0.0671872,"Missing"
D19-6506,L16-1100,0,0.104266,"anguage pairs and different datasets. In this paper, we focus on a single language pair, English–German (in both directions), and on two textual domains: news translation and movie subtitles translation. For the news translation task (denoted as WMT) we rely on the established setup of WMT 20192 with the Newstest2018 data as our dedicated test set. For the movie subtitles (referred to as OST), we use data from the OpenSubtitles corpus released on OPUS3 with our own split into training, development and test data. More details about the data and our setup will be given in the following section. Guillou and Hardmeier, 2016; M¨uller et al., 2018; Guillou et al., 2018) have been largely at the center of attention, and more recently the translation of pronouns in the context of their coreferential chains has been looked at (LapshinovaKoltunski and Hardmeier, 2017; Voita et al., 2018; Lapshinova-Koltunski et al., 2019). Other devices studied are verbal tenses (Gong et al., 2012; Lo´aiciga et al., 2014; Ramm and Fraser, 2016) and connectives (Meyer et al., 2012; Meyer and Popescu-Belis, 2012), although not using neural models. Motivated by approximating the ability of systems to grasp more abstract properties relate"
D19-6506,W17-4810,0,0.0389785,"Missing"
D19-6506,W18-6435,1,0.893655,"Missing"
D19-6506,W19-2805,1,0.878249,"Missing"
D19-6506,P14-1065,0,0.0609459,"Missing"
D19-6506,L16-1147,1,0.83002,"nge to the NMT model architecture. Other 3 Two datasets for English–German document-level translation Different text genres and types exhibit different types of discourse-level properties. The choice of training corpus therefore determines what features a NMT model can potentially learn, and the choice of test corpus determines which features can be reliably evaluated. Our experiments are based on two datasets that cover the same language pair, but very different textual characteristics. The OST dataset is built from the English– German part of the publicly available OpenSubtitles2016 corpus (Lison and Tiedemann, 2016). Of the 16,910 movies and TV series in the collection, 16,510 are used for training, and 4 each are held out for development and testing purposes. Each movie is considered a single document. It corresponds to the dataset used in Tiedemann and Scherrer (2017). General properties of this dataset can be found in Table 1. The WMT dataset comprises the subset of corpora allowed at the WMT 2019 news translation 2 See http://www.statmt.org/wmt19/ translation-task.html. 3 http://opus.nlpl.eu/ OpenSubtitles2016.php 52 Corpus Documents Sentences Sents/Doc Tokens DE Tokens EN Tokens/Sent OST Train OST V"
D19-6506,2010.iwslt-papers.10,0,0.172,"Missing"
D19-6506,loaiciga-etal-2014-english,1,0.889712,"Missing"
D19-6506,W16-3418,0,0.0171114,"anaphor and the antecedent mismatch in features (they-singular, it/they group) are very poorly handled. Test suite metrics Discourse-specific metrics such as Guzm´an et al. (2014) would be welcome to assess the translation quality on specific discourse-level features such as those discussed in Section 3.1. However, they have the disadvantage of relying on a discourse parser, which we do not have for German. At 6 Conclusion We have presented two English–German document-level translation datasets and shown that they represent different text genres with 13 We used the provided tool described in Hardmeier and Guillou (2016). the differences are not significant. 58 different distributions of discourse-level features. The context-aware NMT models on these datasets show performance differences that are to some extent indicative of the underlying textual characteristics: the longer sentences in the news dataset make it harder to find differences between training configurations or evaluation setups. Fixed-window approaches show surprisingly good results on the movie subtitles dataset, but the impact of the realignment process remains to be investigated further. The general performance of a document-level MT system ca"
D19-6506,P14-5010,0,0.00242307,"ignificantly more discrepancies in the OST dataset. These can be ascribed to two factors: free translation (a negation can be paraphrased with expressions such as fail to, doubt if, etc.), and sentence alignment errors. Coreference chains: We assume that a large amount of pronouns, connectives and negations do not require access to large contexts for their correct translation, either because they are unambiguous or because the current sentence is sufficient for their disambiguation. To corroborate this assumption, we annotate the English corpora with the Stanford CoreNLP coreference resolver (Manning et al., 2014; Clark and Manning, 2016) and the German corpora with the CorZu coreference resolver (Tuggener, 2016).7 We first report the numbers of coreference chains identified by the resolvers. These numbers are hard to compare across languages due to different performance levels of the two resolvers, and translationese factors such as explicitation. However, they confirm the intuition that news text contains more referring entities than movie dialogues.8 4 Context-aware MT models In this paper, our main focus lies on concatenation models as one of the most straightforward and successful approaches to d"
D19-6506,D13-1037,1,0.817438,"tigate the discourse-related biases in data. Our contributions are twofold: 2 2.1 Related work Discourse Research about discourse and MT has shifted from explicitly enhancing systems with discourse knowledge to evaluating how much the systems have learned specific discourse features through different resources, test suites being a popular one (cf. Sim Smith, 2017; Popescu-Belis, 2019). Throughout, however, particular discourse phenomena are consistently targeted, as they are indeed indicators of globally good, cohesive and coherent texts. Pronouns (Hardmeier and Federico, 2010; Guillou, 2012; Hardmeier et al., 2013; • we provide a thorough analysis of two popular machine translation datasets in terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics recent work explores the feasability of extending NMT models to make them context-aware. A common approach is to use additional encoders for the context sentence(s) with a modified attention mechanism (Jean et al., 2017; Bawd"
D19-6506,N19-1313,0,0.192639,"terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics recent work explores the feasability of extending NMT models to make them context-aware. A common approach is to use additional encoders for the context sentence(s) with a modified attention mechanism (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). Another technique (Miculicich et al., 2018; Maruf et al., 2019) explores the integration of context through a hierarchical architecture which models the contextual information in a structured manner using word-level and sentencelevel abstractions. The different models have been evaluated on different language pairs and different datasets. In this paper, we focus on a single language pair, English–German (in both directions), and on two textual domains: news translation and movie subtitles translation. For the news translation task (denoted as WMT) we rely on the established setup of WMT 20192 with the Newstest2018 data as our dedicated test set. For the m"
D19-6506,W12-0117,0,0.0313976,"plit into training, development and test data. More details about the data and our setup will be given in the following section. Guillou and Hardmeier, 2016; M¨uller et al., 2018; Guillou et al., 2018) have been largely at the center of attention, and more recently the translation of pronouns in the context of their coreferential chains has been looked at (LapshinovaKoltunski and Hardmeier, 2017; Voita et al., 2018; Lapshinova-Koltunski et al., 2019). Other devices studied are verbal tenses (Gong et al., 2012; Lo´aiciga et al., 2014; Ramm and Fraser, 2016) and connectives (Meyer et al., 2012; Meyer and Popescu-Belis, 2012), although not using neural models. Motivated by approximating the ability of systems to grasp more abstract properties related to coherence, ambiguous words have also been targeted (Rios Gonzales et al., 2017; Bawden et al., 2018; Rios et al., 2018), as well as ellipsis (Voita et al., 2019). Last, negation (Fancellu and Webber, 2015) is a rather understudied phenomenon, but like pronouns and their antecedents, the scope of the negation can be in a different sentence. In this paper we investigate these features in the training data and assess translation using standard automatic metrics and a"
D19-6506,N16-1005,0,0.06311,"Missing"
D19-6506,2012.amta-papers.20,0,0.0252849,"OPUS3 with our own split into training, development and test data. More details about the data and our setup will be given in the following section. Guillou and Hardmeier, 2016; M¨uller et al., 2018; Guillou et al., 2018) have been largely at the center of attention, and more recently the translation of pronouns in the context of their coreferential chains has been looked at (LapshinovaKoltunski and Hardmeier, 2017; Voita et al., 2018; Lapshinova-Koltunski et al., 2019). Other devices studied are verbal tenses (Gong et al., 2012; Lo´aiciga et al., 2014; Ramm and Fraser, 2016) and connectives (Meyer et al., 2012; Meyer and Popescu-Belis, 2012), although not using neural models. Motivated by approximating the ability of systems to grasp more abstract properties related to coherence, ambiguous words have also been targeted (Rios Gonzales et al., 2017; Bawden et al., 2018; Rios et al., 2018), as well as ellipsis (Voita et al., 2019). Last, negation (Fancellu and Webber, 2015) is a rather understudied phenomenon, but like pronouns and their antecedents, the scope of the negation can be in a different sentence. In this paper we investigate these features in the training data and assess translation using s"
D19-6506,W17-4814,0,0.0187737,"analyzed. On one side, it is difficult to pinpoint particular contextual features that neural MT (NMT) models are picking up. On the other, it is difficult to judge good translations purely in terms of discourse features. In this paper, we investigate the discourse-related biases in data. Our contributions are twofold: 2 2.1 Related work Discourse Research about discourse and MT has shifted from explicitly enhancing systems with discourse knowledge to evaluating how much the systems have learned specific discourse features through different resources, test suites being a popular one (cf. Sim Smith, 2017; Popescu-Belis, 2019). Throughout, however, particular discourse phenomena are consistently targeted, as they are indeed indicators of globally good, cohesive and coherent texts. Pronouns (Hardmeier and Federico, 2010; Guillou, 2012; Hardmeier et al., 2013; • we provide a thorough analysis of two popular machine translation datasets in terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Associatio"
D19-6506,D18-1325,0,0.220364,"translation datasets in terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics recent work explores the feasability of extending NMT models to make them context-aware. A common approach is to use additional encoders for the context sentence(s) with a modified attention mechanism (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). Another technique (Miculicich et al., 2018; Maruf et al., 2019) explores the integration of context through a hierarchical architecture which models the contextual information in a structured manner using word-level and sentencelevel abstractions. The different models have been evaluated on different language pairs and different datasets. In this paper, we focus on a single language pair, English–German (in both directions), and on two textual domains: news translation and movie subtitles translation. For the news translation task (denoted as WMT) we rely on the established setup of WMT 20192 with the Newstest2018 data as our dedicate"
D19-6506,W17-4811,1,0.942765,"approximating the ability of systems to grasp more abstract properties related to coherence, ambiguous words have also been targeted (Rios Gonzales et al., 2017; Bawden et al., 2018; Rios et al., 2018), as well as ellipsis (Voita et al., 2019). Last, negation (Fancellu and Webber, 2015) is a rather understudied phenomenon, but like pronouns and their antecedents, the scope of the negation can be in a different sentence. In this paper we investigate these features in the training data and assess translation using standard automatic metrics and a data scrambling strategy. 2.2 Context-aware NMT Tiedemann and Scherrer (2017) present a simple approach to context-aware NMT: instead of training the model on pairs of single source and target sentences, they add sentences from the left context to the sentence to be translated, either only on the source side or both on source and target sides. These models are evaluated on a German–English corpus extracted from OpenSubtitles, and the best results are obtained with two source sentences and one target sentence. Agrawal et al. (2018) extend these experiments by considering additional contexts. They evaluate their work on the IWSLT 2017 dataset for English–Italian, which c"
D19-6506,W18-6307,0,0.0515987,"Missing"
D19-6506,P19-1116,0,0.121894,"Missing"
D19-6506,P02-1040,0,0.10417,"lized that they do not necessarily match with the segment boundaries in the reference data even though the original paper suggests that this should be rather stable (JunczysDowmunt, 2019). This is especially fatal if the number of segments does not match. Therefore, we apply standard sentence alignment based on length-correlation and lexical matches using hunalign (Varga et al., 2005) to link the system output to the reference translations. The reported results from the fixed-size models are based on this approach. Evaluation Each system is evaluated on the respective test set using the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. In particular, we evaluate each of them on three variants of the test set: 5.1 Generic translation metrics We report BLEU and METEOR scores for all our experiments in Tables 4 and 5. The results and significance tests were computed using MultEval (Clark et al., 2011). By and large, the concatenation models are able to exploit contextual information: BLEU as well as METEOR scores decrease by statistically significant amounts if the context is inconsistent or absent. However, it is difficult to distinguish a winning configuration. In particular, t"
D19-6506,P18-1117,0,0.0725072,"thorough analysis of two popular machine translation datasets in terms of document-level features, 1 Context here refers to text outside of the sentence to be translated. 51 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 51–61 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics recent work explores the feasability of extending NMT models to make them context-aware. A common approach is to use additional encoders for the context sentence(s) with a modified attention mechanism (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). Another technique (Miculicich et al., 2018; Maruf et al., 2019) explores the integration of context through a hierarchical architecture which models the contextual information in a structured manner using word-level and sentencelevel abstractions. The different models have been evaluated on different language pairs and different datasets. In this paper, we focus on a single language pair, English–German (in both directions), and on two textual domains: news translation and movie subtitles translation. For the news translation task (denoted as WMT) we rely on the established setup of WMT 2019"
D19-6506,W16-2203,0,0.0134477,"om the OpenSubtitles corpus released on OPUS3 with our own split into training, development and test data. More details about the data and our setup will be given in the following section. Guillou and Hardmeier, 2016; M¨uller et al., 2018; Guillou et al., 2018) have been largely at the center of attention, and more recently the translation of pronouns in the context of their coreferential chains has been looked at (LapshinovaKoltunski and Hardmeier, 2017; Voita et al., 2018; Lapshinova-Koltunski et al., 2019). Other devices studied are verbal tenses (Gong et al., 2012; Lo´aiciga et al., 2014; Ramm and Fraser, 2016) and connectives (Meyer et al., 2012; Meyer and Popescu-Belis, 2012), although not using neural models. Motivated by approximating the ability of systems to grasp more abstract properties related to coherence, ambiguous words have also been targeted (Rios Gonzales et al., 2017; Bawden et al., 2018; Rios et al., 2018), as well as ellipsis (Voita et al., 2019). Last, negation (Fancellu and Webber, 2015) is a rather understudied phenomenon, but like pronouns and their antecedents, the scope of the negation can be in a different sentence. In this paper we investigate these features in the training"
D19-6506,W18-6437,0,0.0598349,"Missing"
L16-1641,P12-2072,1,0.853191,"Missing"
L16-1641,W14-5310,0,0.466095,"ular channels) and in an increased interest in automatic processing. As opposed to other, more or less digitised, sources of Swiss German data (Hotzenk¨ocherle et al., 1962-1997; Staub et al., 1881- ; Scherrer and Rambow, 2010; Kolly and Leemann, 2015), which consist of isolated word types, this corpus is intended to represent continuous speech, that is, the words as they are actually used in texts. The main difference between the corpus presented in this paper and the other two existing corpora of Swiss German, a corpus of SMS messages (Stark et al., 2009-2015) and a corpus of written texts (Hollenstein and Aepli, 2014), is the fact that this is the only corpus of transcribed spoken language. Another corpus based on transcription of spoken language is under development; it contains a smaller sample of old recordings of shorter texts (more information is available at http://www.phonogrammarchiv. uzh.ch/en.html). Data source and the size of the corpus The corpus contains transcriptions of video recordings collected by the ArchiMob association (see http://www. archimob.ch) in the period 1999-2001. This collection contains 555 recordings. Each recording is produced with one informant using a semi-directive techn"
L18-1527,W16-4801,0,0.0282045,"n the DFS platform and to supplement the existing surveys with additional data points to continuously refine the accuracy of the geolocalisation task. At the same time, the semi-automatic selection of items for the task provided an interesting application of various methods of data analysis and machine learning. There have been two major approaches to speaker geolocalisation (or dialect identification) in the literature: the corpus-based approach predicts the dialect of any text fragment extracted from a corpus; this approach has been followed by the VarDial shared tasks in recent years (e.g. Malmasi et al. 2016, Zampieri et al. 2017), but also by Scherrer & Rambow (2010) or Rahimi et al. (2017), for example. The dialectological approach tries to identify a small set of distinguishing dialectal features, which are then elicited interactively from the user in order to identify their dialect (Leemann et al. 2016, 2018a, 2018b). The task proposed here follows the dialectological approach. We rely on data from two surveys on regionalisms in European French (France, Belgium and Switzerland), which were carried out in 2015-2016 as a part of the project Français de nos régions (Avanzi et al. 2016); key info"
L18-1527,1989.mtsummit-1.5,0,0.449703,"Missing"
L18-1527,W12-0211,0,0.0320745,"t for Switzerland and Belgium (and are successfully identified, as shown above for the Swiss case), the inferred clusters within France are much less clearly correlated with single linguistic variants, but rather emerge through the combination of a large number of gradual linguistic differences. Due to these problems, we did not pursue this approach any further. 4.4 Feature elimination 4.3 Clustering and shibboleth detection This approach consisted of two steps: we first determined an optimal areal partition using hierarchical clustering, and then applied the shibboleth detection algorithm of Prokić et al. (2012) to find the most characteristic set of questions for each area. Figure 5 shows an example of hierarchical clustering solutions using Ward’s method and 10 target clusters, obtained using the complete dataset (6463 participants, 130 questions). It is worth pointing out that the aggregated data clusters nicely into geographically coherent and linguistically sensible regions, suggesting that the quality of the survey data is good. In a second approach, we did not fix a geographical partition in advance, but kept the 109 areas as defined above while finding the optimal set of questions. For this,"
L18-1527,D17-1016,0,0.0146591,"s to continuously refine the accuracy of the geolocalisation task. At the same time, the semi-automatic selection of items for the task provided an interesting application of various methods of data analysis and machine learning. There have been two major approaches to speaker geolocalisation (or dialect identification) in the literature: the corpus-based approach predicts the dialect of any text fragment extracted from a corpus; this approach has been followed by the VarDial shared tasks in recent years (e.g. Malmasi et al. 2016, Zampieri et al. 2017), but also by Scherrer & Rambow (2010) or Rahimi et al. (2017), for example. The dialectological approach tries to identify a small set of distinguishing dialectal features, which are then elicited interactively from the user in order to identify their dialect (Leemann et al. 2016, 2018a, 2018b). The task proposed here follows the dialectological approach. We rely on data from two surveys on regionalisms in European French (France, Belgium and Switzerland), which were carried out in 2015-2016 as a part of the project Français de nos régions (Avanzi et al. 2016); key information regarding the surveys is shown in Table 1. Survey 1 May 2015- May 2016 40 que"
L18-1527,D10-1112,1,0.567624,"s with additional data points to continuously refine the accuracy of the geolocalisation task. At the same time, the semi-automatic selection of items for the task provided an interesting application of various methods of data analysis and machine learning. There have been two major approaches to speaker geolocalisation (or dialect identification) in the literature: the corpus-based approach predicts the dialect of any text fragment extracted from a corpus; this approach has been followed by the VarDial shared tasks in recent years (e.g. Malmasi et al. 2016, Zampieri et al. 2017), but also by Scherrer & Rambow (2010) or Rahimi et al. (2017), for example. The dialectological approach tries to identify a small set of distinguishing dialectal features, which are then elicited interactively from the user in order to identify their dialect (Leemann et al. 2016, 2018a, 2018b). The task proposed here follows the dialectological approach. We rely on data from two surveys on regionalisms in European French (France, Belgium and Switzerland), which were carried out in 2015-2016 as a part of the project Français de nos régions (Avanzi et al. 2016); key information regarding the surveys is shown in Table 1. Survey 1 M"
P07-3010,J93-2003,0,0.0130219,"ex. We argue that this point alleviates the restrictions imposed by the scarcity of the resources. In particular, we claim that if two languages are close, even if one of them is scarcely documented, we can successfully use techniques that require training. Finding lexical mappings amounts to finding word pairs that are maximally similar, with respect to a particular definition of similarity. Similarity measures can be based on any level of linguistic analysis: semantic similarity relies on context vectors (Rapp, 1999), while syntactic similarity is based on the alignment of parallel corpora (Brown et al., 1993). Our work is based on the assumption that phonetic (or rather graphemic, as we use written data) similarity measures are the most appropriate in the given language context because they require less sophisticated training data than semantic or syntactic similarity models. However, phonetic similarity measures can only be used for cognate language pairs, i.e. language pairs that can be traced back to a common historical origin and that possess highly similar linguistic (in particular, phonological and morphological) characteristics. Moreover, we can only expect phonetic similarity measures to i"
P07-3010,W06-1108,0,0.0908306,", which performed worst in Mann and Yarowsky’s study. The originality of their approach is that they apply models used for speech processing to cognate word pair induction. In particular, they refer to a previous study by Ristad and Yianilos (1998). Ristad and Yianilos showed how a stochastic transducer can be trained in a non-supervised manner using the EM algorithm and successfully applied their model to the problem of pronunciation recognition (soundto-letter conversion). Jansche (2003) reviews their work in some detail, correcting thereby some errors in the presentation of the algorithms. Heeringa et al. (2006) present several modifications of the Levenshtein distance that approximate linguistic intuitions better. These models are presented in the framework of dialectometry, i.e. they provide numerical measures for the classification of dialects. However, some of their models can be adapted to be used in a lexicon induction task. Kondrak and Sherif (2006) use phonetic similarity models for cognate word identification. Other studies deal with lexicon induction for cognate language pairs and for scarce resource languages. Rapp (1999) extends an existing bilingual lexicon with the help of non-parallel"
P07-3010,2006.amta-papers.9,0,0.0250727,"Missing"
P07-3010,W06-1107,0,0.0351209,"sing the EM algorithm and successfully applied their model to the problem of pronunciation recognition (soundto-letter conversion). Jansche (2003) reviews their work in some detail, correcting thereby some errors in the presentation of the algorithms. Heeringa et al. (2006) present several modifications of the Levenshtein distance that approximate linguistic intuitions better. These models are presented in the framework of dialectometry, i.e. they provide numerical measures for the classification of dialects. However, some of their models can be adapted to be used in a lexicon induction task. Kondrak and Sherif (2006) use phonetic similarity models for cognate word identification. Other studies deal with lexicon induction for cognate language pairs and for scarce resource languages. Rapp (1999) extends an existing bilingual lexicon with the help of non-parallel corpora, assuming that corresponding words share cooccurrence patterns. His method has been used by Hwa et al. (2006) to induce a dictionary between Modern Standard Arabic and the Levantine Arabic dialect. Although this work involves two closely re56 lated language varieties, graphemic similarity measures are not used at all. Nevertheless, Schafer a"
P07-3010,N01-1020,0,0.834725,"airs that can be traced back to a common historical origin and that possess highly similar linguistic (in particular, phonological and morphological) characteristics. Moreover, we can only expect phonetic similarity measures to induce cognate word pairs, i.e. word pairs whose forms and significations are similar, as a result of a historical relationship. We will present different models of phonetic similarity that are adapted to the given language pair. In particular, attention has been paid to develop techniques requiring little manually annotated data. 2 Related Work Our work is inspired by Mann and Yarowsky (2001). They induce translation lexicons between a resource-rich language (typically English) and a scarce resource language of another language family (for example, Portuguese) by using a resource55 Proceedings of the ACL 2007 Student Research Workshop, pages 55–60, c Prague, June 2007. 2007 Association for Computational Linguistics rich bridge language of the same family (for example, Spanish). While they rely on existing translation lexicons for the source-to-bridge step (English-Spanish), they use string distance models (called cognate models) for the bridge-to-target step (Spanish-Portuguese)."
P07-3010,P99-1067,0,0.260446,"e two languages are closely related, the lexical relations to be induced are less complex. We argue that this point alleviates the restrictions imposed by the scarcity of the resources. In particular, we claim that if two languages are close, even if one of them is scarcely documented, we can successfully use techniques that require training. Finding lexical mappings amounts to finding word pairs that are maximally similar, with respect to a particular definition of similarity. Similarity measures can be based on any level of linguistic analysis: semantic similarity relies on context vectors (Rapp, 1999), while syntactic similarity is based on the alignment of parallel corpora (Brown et al., 1993). Our work is based on the assumption that phonetic (or rather graphemic, as we use written data) similarity measures are the most appropriate in the given language context because they require less sophisticated training data than semantic or syntactic similarity models. However, phonetic similarity measures can only be used for cognate language pairs, i.e. language pairs that can be traced back to a common historical origin and that possess highly similar linguistic (in particular, phonological and"
P07-3010,W02-2026,0,0.12104,"if (2006) use phonetic similarity models for cognate word identification. Other studies deal with lexicon induction for cognate language pairs and for scarce resource languages. Rapp (1999) extends an existing bilingual lexicon with the help of non-parallel corpora, assuming that corresponding words share cooccurrence patterns. His method has been used by Hwa et al. (2006) to induce a dictionary between Modern Standard Arabic and the Levantine Arabic dialect. Although this work involves two closely re56 lated language varieties, graphemic similarity measures are not used at all. Nevertheless, Schafer and Yarowsky (2002) have shown that these two techniques can be combined efficiently. They use Rapp’s co-occurrence vectors in combination with Mann and Yarowsky’s EM-trained transducer. 3 Two-Stage Models of Lexical Induction Following the standard statistical machine translation architecture, we represent the lexicon induction task as a two-stage model. In the first stage, we use the source word to generate a fixed number of candidate translation strings, according to a transducer which represents a particular similarity measure. In the second stage, these candidate strings are filtered through a lexicon of th"
scherrer-cartoni-2012-trilingual,nerima-wehrli-2008-generating,0,\N,Missing
scherrer-cartoni-2012-trilingual,J93-1004,0,\N,Missing
scherrer-cartoni-2012-trilingual,J93-2003,0,\N,Missing
scherrer-cartoni-2012-trilingual,P07-1108,0,\N,Missing
scherrer-cartoni-2012-trilingual,N07-1061,0,\N,Missing
scherrer-cartoni-2012-trilingual,P07-2045,0,\N,Missing
scherrer-cartoni-2012-trilingual,P06-4018,0,\N,Missing
scherrer-cartoni-2012-trilingual,J03-1002,0,\N,Missing
scherrer-cartoni-2012-trilingual,2005.mtsummit-papers.11,0,\N,Missing
scherrer-cartoni-2012-trilingual,lardilleux-etal-2010-bilingual,0,\N,Missing
scherrer-etal-2014-swissadmin,W07-1216,1,\N,Missing
scherrer-etal-2014-swissadmin,P07-2045,0,\N,Missing
scherrer-etal-2014-swissadmin,petrov-etal-2012-universal,0,\N,Missing
scherrer-etal-2014-swissadmin,2005.mtsummit-papers.11,0,\N,Missing
scherrer-etal-2014-swissadmin,W10-3705,1,\N,Missing
scherrer-etal-2014-swissadmin,scherrer-cartoni-2012-trilingual,1,\N,Missing
scherrer-sagot-2014-language,W13-5306,1,\N,Missing
scherrer-sagot-2014-language,C04-1137,0,\N,Missing
scherrer-sagot-2014-language,H01-1035,0,\N,Missing
scherrer-sagot-2014-language,N01-1020,0,\N,Missing
scherrer-sagot-2014-language,D11-1119,0,\N,Missing
scherrer-sagot-2014-language,P99-1067,0,\N,Missing
scherrer-sagot-2014-language,W07-0705,0,\N,Missing
scherrer-sagot-2014-language,W02-0902,0,\N,Missing
scherrer-sagot-2014-language,P07-2045,0,\N,Missing
scherrer-sagot-2014-language,2009.eamt-1.3,0,\N,Missing
scherrer-sagot-2014-language,J03-1002,0,\N,Missing
scherrer-sagot-2014-language,R11-1018,0,\N,Missing
scherrer-sagot-2014-language,I13-1112,0,\N,Missing
scherrer-sagot-2014-language,feldman-etal-2006-cross,0,\N,Missing
W08-1003,H91-1060,0,0.0908138,"Missing"
W08-1003,D07-1066,0,0.0227577,"Missing"
W08-1003,A97-1014,0,0.0537598,"Missing"
W08-1003,W07-1216,0,0.030056,"organized as follows. In Section 2, we present the Fips framework. In Section 3, we recall the main characteristics of the TIGER treebank, explain the adaptations we applied to the Fips tagger and give some information about the evaluation setup. We go on to report the results for the three main tasks: Part-of-Speech tagging (Section 4), lemma identification (Section 5), and morphological analysis (Section 6). Section 7 compares our work to statistical POS tagging and to parser evaluation. We conclude by giving an overview of the benefits of quantitative evaluation. 2 The Fips framework Fips (Wehrli, 2007) is a deep symbolic parser developed at the University of Geneva. It currently supports six languages, and others are under development. The parser is based on an adaption of generative linguistics, borrowing concepts from the Minimalist model (Chomsky, 1995), from the Simpler 16 Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 16–23, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics CP DP C TP deutete VP D NP Der N AdvP AdvP Minister PP PP DP P DP P DP für D bei NP Zuzahlungen Part D NP eine N an Obergrenze N Kuren Figure 1: Example out"
W08-1003,A00-1031,0,\N,Missing
W09-0415,W06-2006,0,0.0132214,"Gamallo, 2007). We plan to use semi-automatic generation to build the 6 remaining dictionaries. For this purpose we will derive a bilingual lexicon by transitivity, using two existing ones. For instance, if we have bilingual correspondences for language pair 1 This attribute takes the form of an integer between 6 (preferred) and 0 (lowest). 92 A ! B and B ! C, we can obtain A ! C. We will see below how the correspondences are validated. The idea of using a pivot language for deriving bilingual lexicons from existing ones is not new. The reader can find related approaches in (Paik & al. 2004, Ahn & Frampton 2006, Zhang & al. 2007) . The specificity of our approach is that the initial resources are manually made, i.e. non noisy, lexicons. The derivation process goes as follows: and German-French lexicons. For the checking of the validity of the correspondences (point 4 of the process) we used the parallel corpus of the debates of the European Parliament during the period 1996 to 2001 (Koehn, 2005). Figure 2 summarizes the results of the four steps of the derivation process: Step 1 2 3 4 1. Take two bilingual tables for language pairs (A, B) and (B, C) and perform a relational equi-join. Perform a filt"
W09-0415,W04-2204,0,0.0228175,"Missing"
W09-0415,2007.mtsummit-papers.4,0,0.0769589,"Missing"
W09-0415,2001.mtsummit-road.1,0,0.0739494,"Missing"
W09-0415,W07-1216,1,\N,Missing
W09-0415,P06-1120,1,\N,Missing
W09-0415,2005.mtsummit-papers.11,0,\N,Missing
W09-0415,W02-1705,0,\N,Missing
W09-0415,2007.mtsummit-papers.73,0,\N,Missing
W11-2604,W06-2920,0,0.0125127,"t structures for different dialects. One of the three variants is identical to the Standard German structure produced above. In a second variant, the positions of the article and the adverb are exchanged without modifying the dependency links: Transformation rules 4.1 The Standard German corpus ADV The transformation rules require morphosyntactically annotated Standard German input data. Therefore, we had to choose a specific annotation format and a specific corpus to test the rules on. We selected the Standard German TIGER treebank (Brants et al., 2002), in the CoNLL-style dependency format (Buchholz and Marsi, 2006; K¨ubler, 2008).8 This format allows a compact representation of the syntactic structure. Figure 2 shows a sample sentence, annotated in this format. While we use the TIGER corpus for test and evaluation purposes in this paper, the rules are aimed to be sufficiently generic so that they apply correctly to any other corpus annotated according to the same guidelines. 4.2 ART 8 Thanks to Yannick Versley for making this version available to us. 9 X symbolizes any type of node that possesses an article and an adjective as dependents. In practice, X usually is a noun. 34 ADJA X This transformation"
W11-2604,E06-1047,0,0.0833308,"ailable for many dialect areas. This paper explores how dialect syntax fieldwork can guide the development of multidialectal natural language processing tools. Our goal is to transform Standard German sentence structures so that they become syntactically valid in Swiss German dialects.1 1 Here, we do not take into account the phonetic, morphological and lexical changes involved in generating the actual Swiss German word forms. For such a model, see for example Scherrer and Rambow (2010a). Related work One line of research in natural language processing deals with parsing methods for dialects. Chiang et al. (2006) argue that it is often easier to manually create resources that relate a dialect to a standard language than it is to manually create syntactically annotated resources for the dialect itself. They investigate three approaches for parsing the Levantine dialect of Arabic, one of which consists of transducing a Standard Arabic treebank into Levantine with the help of hand-crafted rules. We agree with this point of view: we devise transformation rules that relate Swiss German dialects to Standard German. In the case of closely related languages,2 different 2 In any case, it is difficult to establ"
W11-2604,D10-1112,1,0.830779,"y, since the 1990s, that syntax has gained the attraction of dialectologists. As a result, syntactic data from field studies are now available for many dialect areas. This paper explores how dialect syntax fieldwork can guide the development of multidialectal natural language processing tools. Our goal is to transform Standard German sentence structures so that they become syntactically valid in Swiss German dialects.1 1 Here, we do not take into account the phonetic, morphological and lexical changes involved in generating the actual Swiss German word forms. For such a model, see for example Scherrer and Rambow (2010a). Related work One line of research in natural language processing deals with parsing methods for dialects. Chiang et al. (2006) argue that it is often easier to manually create resources that relate a dialect to a standard language than it is to manually create syntactically annotated resources for the dialect itself. They investigate three approaches for parsing the Levantine dialect of Arabic, one of which consists of transducing a Standard Arabic treebank into Levantine with the help of hand-crafted rules. We agree with this point of view: we devise transformation rules that relate Swiss"
W11-2604,2009.eamt-1.3,0,0.0189969,"d that minor syntactic differences are dealt with explicitly. Corb´ıBellot et al. (2005) present a shallow-transfer system for the different Romance languages of Spain. Structural transfer rules account for gender change and word reorderings. Another system (Homola and Kuboˇn, 2005) covers several Slavonic languages of Eastern Europe and confirms the necessity of shallow parsing except for the most similar language pair (Czech-Slovak). In contrast, statistical machine translation systems have been proposed to translate closely related languages on a letter-by-letter basis (Vilar et al., 2007; Tiedemann, 2009). However, the word reordering capabilities of a common phrase-based model are still required to obtain reasonable performances. There are two main types of syntactic differences between Swiss German dialects and Standard German. Some of the differences are representative of the mainly spoken use of Swiss German. They do not show much interdialectal variation, and they are also encountered in other spoken varieties of German. Other differences are dialectological in nature, in the sense that they are specific to some subgroups of Swiss German dialects and usually do not occur outside of the Al"
W11-2604,W08-2321,0,0.0278003,"view: we devise transformation rules that relate Swiss German dialects to Standard German. In the case of closely related languages,2 different 2 In any case, it is difficult to establish strict linguistic criteria 30 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 30–38, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics types of annotation projection have been proposed to facilitate the creation of treebanks. See Volk and Samuelsson (2004) for an overview of the problem. In a rather different approach, Vaillant (2008) presents a hand-crafted multi-dialect grammar that conceives of a dialect as some kind of “agreement feature”. This allows to share identical rules across dialects and differentiate them only where necessary. We follow a similar approach by linking the transformation rules to geographical data from recent dialectological fieldwork. Another line of research is oriented towards machine translation models for closely related languages. It is common in this field that minor syntactic differences are dealt with explicitly. Corb´ıBellot et al. (2005) present a shallow-transfer system for the differ"
W11-2604,W07-0705,0,0.0325438,"common in this field that minor syntactic differences are dealt with explicitly. Corb´ıBellot et al. (2005) present a shallow-transfer system for the different Romance languages of Spain. Structural transfer rules account for gender change and word reorderings. Another system (Homola and Kuboˇn, 2005) covers several Slavonic languages of Eastern Europe and confirms the necessity of shallow parsing except for the most similar language pair (Czech-Slovak). In contrast, statistical machine translation systems have been proposed to translate closely related languages on a letter-by-letter basis (Vilar et al., 2007; Tiedemann, 2009). However, the word reordering capabilities of a common phrase-based model are still required to obtain reasonable performances. There are two main types of syntactic differences between Swiss German dialects and Standard German. Some of the differences are representative of the mainly spoken use of Swiss German. They do not show much interdialectal variation, and they are also encountered in other spoken varieties of German. Other differences are dialectological in nature, in the sense that they are specific to some subgroups of Swiss German dialects and usually do not occur"
W11-2604,W04-1910,0,0.047118,"c treebank into Levantine with the help of hand-crafted rules. We agree with this point of view: we devise transformation rules that relate Swiss German dialects to Standard German. In the case of closely related languages,2 different 2 In any case, it is difficult to establish strict linguistic criteria 30 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 30–38, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics types of annotation projection have been proposed to facilitate the creation of treebanks. See Volk and Samuelsson (2004) for an overview of the problem. In a rather different approach, Vaillant (2008) presents a hand-crafted multi-dialect grammar that conceives of a dialect as some kind of “agreement feature”. This allows to share identical rules across dialects and differentiate them only where necessary. We follow a similar approach by linking the transformation rules to geographical data from recent dialectological fieldwork. Another line of research is oriented towards machine translation models for closely related languages. It is common in this field that minor syntactic differences are dealt with explici"
W11-2604,W08-1008,0,\N,Missing
W11-2604,2005.eamt-1.12,0,\N,Missing
W12-0210,N01-1020,0,0.016162,"ow such an effect. Region-specific toponyms are rare. The results suggest that a more fine-grained variant of Levenshtein distance might be useful. In the following paragraphs, we present several improvements for future work. The results suggest that a more fine-grained variant of Levenshtein distance might improve the precision and recall of the cognate detection algorithm. Notably, it has been found that vowels change more readily than consonant in closely related language varieties. In consequence, changing one vowel by another should be penalized less than changing a vowel by a consonant (Mann and Yarowsky, 2001). The same holds for accented vs. non-accented characters. Complex graphemes representing a single phoneme appear rather frequently in the Dieth transcription system (e.g. for long vowels) and should also be treated separately. We should also mention that the proposed method likely faces a problem of scale. Indeed, each word of each text has to be compared with each word of each text. This is only manageable with a small corpus like ours. We conclude by pointing out a limitation of this approach: the automatic alignment process based on the concept of cognate pairs obviously only works for pho"
W12-0210,J03-1002,0,0.00607441,"to elicit the local words or pronunciations of a given concept. The mere fact that two responses are linked to the same question number of the questionnaire suffices to guarantee that they refer to the same concept. This property leads us to consider dialectological survey data as word-aligned by design. In contrast, the Archimob corpus is not aligned. Again, algorithms for aligning words in parallel and comparable corpora have been proposed in the field of machine translation. For large parallel corpora, distributional alignment methods based solely on cooccurrence statistics are sufficient (Och and Ney, 2003; Koehn et al., 2007). For comparable corpora, the order and frequency of occurrence of the words cannot be used as alignment cues. Instead, the phonetic and orthographic structures are used to match similar word pairs (Simard et al., 1992; Koehn and Knight, 2002; Kondrak and Sherif, 2006). Obviously, this approach only works for cognate word pairs – word pairs with a common etymology and similar surface forms. This task is known as cognate identification. In the next section, we detail how cognate identification is used to compute the distance between different dialect versions of a comparabl"
W12-0210,1992.tmi-1.7,0,0.402097,"s to consider dialectological survey data as word-aligned by design. In contrast, the Archimob corpus is not aligned. Again, algorithms for aligning words in parallel and comparable corpora have been proposed in the field of machine translation. For large parallel corpora, distributional alignment methods based solely on cooccurrence statistics are sufficient (Och and Ney, 2003; Koehn et al., 2007). For comparable corpora, the order and frequency of occurrence of the words cannot be used as alignment cues. Instead, the phonetic and orthographic structures are used to match similar word pairs (Simard et al., 1992; Koehn and Knight, 2002; Kondrak and Sherif, 2006). Obviously, this approach only works for cognate word pairs – word pairs with a common etymology and similar surface forms. This task is known as cognate identification. In the next section, we detail how cognate identification is used to compute the distance between different dialect versions of a comparable corpus. 4 (1) es scht`oo` t n`ıd (2) wil si n`ıd schtoot Intuitively, two cognate word pairs can be found in the texts (1) and (2): hscht`oo` t, schtooti and hn`ıd, n`ıdi.4 The words es, wil, si do not have cognate equivalents in the oth"
W12-0210,W06-1108,0,0.551541,"texts (1) and (2): hscht`oo` t, schtooti and hn`ıd, n`ıdi.4 The words es, wil, si do not have cognate equivalents in the other text. As a result, the two texts have a similarity of 21 , one of the two cognate pairs consisting of identical words. In the example above, we have assumed informal meanings of cognate word pair and identical word pair. In the following sections, we define these concepts more precisely. 4.1 Identifying cognate word pairs Most recently proposed cognate identification algorithms are based on variants of Levenshtein distance, or string edit distance (Levenshtein, 1966; Heeringa et al., 2006; Kondrak and Sherif, 2006). Levenshtein distance is defined as the smallest number of insertion, deletion and substitution operations required to transform one string into another. Computing the linguistic similarity of two comparable texts (3) The hypothesis put forward in this paper is that the linguistic similarity of two comparable texts can be approximated by the degree of similarity b b 0 i i 0 i 1 s s 0 c c 0 h h 0 p p 0 i i 0 i 1 u l 1 4 Accented and unaccented characters are considered as different. See footnote 5. 65 Example (3) shows two words and the associated operation costs. Th"
W12-0210,W02-0902,0,0.0186981,"ological survey data as word-aligned by design. In contrast, the Archimob corpus is not aligned. Again, algorithms for aligning words in parallel and comparable corpora have been proposed in the field of machine translation. For large parallel corpora, distributional alignment methods based solely on cooccurrence statistics are sufficient (Och and Ney, 2003; Koehn et al., 2007). For comparable corpora, the order and frequency of occurrence of the words cannot be used as alignment cues. Instead, the phonetic and orthographic structures are used to match similar word pairs (Simard et al., 1992; Koehn and Knight, 2002; Kondrak and Sherif, 2006). Obviously, this approach only works for cognate word pairs – word pairs with a common etymology and similar surface forms. This task is known as cognate identification. In the next section, we detail how cognate identification is used to compute the distance between different dialect versions of a comparable corpus. 4 (1) es scht`oo` t n`ıd (2) wil si n`ıd schtoot Intuitively, two cognate word pairs can be found in the texts (1) and (2): hscht`oo` t, schtooti and hn`ıd, n`ıdi.4 The words es, wil, si do not have cognate equivalents in the other text. As a result, th"
W12-0210,P07-2045,0,0.00546926,"Missing"
W12-0210,W06-1107,0,0.0200518,"word-aligned by design. In contrast, the Archimob corpus is not aligned. Again, algorithms for aligning words in parallel and comparable corpora have been proposed in the field of machine translation. For large parallel corpora, distributional alignment methods based solely on cooccurrence statistics are sufficient (Och and Ney, 2003; Koehn et al., 2007). For comparable corpora, the order and frequency of occurrence of the words cannot be used as alignment cues. Instead, the phonetic and orthographic structures are used to match similar word pairs (Simard et al., 1992; Koehn and Knight, 2002; Kondrak and Sherif, 2006). Obviously, this approach only works for cognate word pairs – word pairs with a common etymology and similar surface forms. This task is known as cognate identification. In the next section, we detail how cognate identification is used to compute the distance between different dialect versions of a comparable corpus. 4 (1) es scht`oo` t n`ıd (2) wil si n`ıd schtoot Intuitively, two cognate word pairs can be found in the texts (1) and (2): hscht`oo` t, schtooti and hn`ıd, n`ıdi.4 The words es, wil, si do not have cognate equivalents in the other text. As a result, the two texts have a similari"
W13-2409,J03-1002,0,0.0060212,"0 Pairs 3,199 3,638 10,033 16,029 Ident 493 1,708 8,281 9,834 Diff 2,706 1,930 1,752 6,195 4.1 Table 4: Size of Lf oo lexicon. SMT models consist of two main components: the translation model, which is trained on bilingual data, and the language model, which is trained on monolingual data of the target language. We use the word pairs from Lgoo to train the translation model, and the modern Slovene words from Lgoo to train the language model.7 As said above, we test the model on the word pairs of Lf oo . The experiments have been carried out with the tools of the standard SMT pipeline: GIZA++ (Och and Ney, 2003) for alignment, Moses (Koehn et al., 2007) for phrase extraction and decoding, and IRSTLM (Federico et al., 2008) for language modelling. After preliminary experimentation, we settled on the following parameter settings: mentioned, contains no hnform, mformi pairs already appearing in Lgoo . This setting simulates the task of an existing system receiving a new text to modernize. The lexicons used in the experiment contain entries with nform, mform, and the per-slice frequencies of the pair in the corpus from which the lexicon was derived, as illustrated in the example below: benetkah benetkah"
W13-2409,erjavec-2012-goo300k,1,0.847567,"con, Lf oo is derived from the foo3M corpus and, as 5 The corpora used in our experiments are slightly smaller than the originals: the text from two books and one newspaper issue has been removed, as the former contain highly idiosyncratic ways of spelling words, not seen elsewhere, and the latter contains a mixture of the Bohoriˇc and contemporary alphabet, causing problems for word form normalization. The texts older than 1750 have also been removed from goo300k, as such texts do not occur in foo3M, which is used for testing our approach. 6 A previous version of this corpus is described in (Erjavec, 2012). 2 The dataset used in this paper is available under the CC-BY-NC-SA license from http://nl.ijs.si/imp/ experiments/bsnlp-2013/. 3 Sloleks is encoded in LMF and available under the CCBY-NC-SA license from http://www.slovenscina. eu/. 4 The data for historical Slovene comes from the IMP resources, see http://nl.ijs.si/imp/. 59 Period 18B 19A 19B Σ Pairs 6,305 18,733 30,874 45,810 Ident 2,635 12,223 24,597 31,160 Diff 3,670 6,510 6,277 14,650 OOV 703 2,117 4,759 7,369 create three different models for the three time periods of old Slovene (18B, 19A, 19B). The first experiment follows a supervis"
W13-2409,W11-0415,0,0.142797,"dge. We present the relevant lexicons and two experiments. In one, we use a lexicon of historical word– contemporary word pairs and a list of contemporary words; in the other, we only use a list of historical words and one of contemporary ones. We show that both methods produce significantly better results than the baseline. 1 2 Related Work The most common approach to modernizing historical words uses (semi-) hand-constructed transcription rules, which are then applied to historical words, and the results filtered against a contemporary lexicon (Baron and Rayson, 2008; Scheible et al., 2010; Scheible et al., 2011); such rules are often encoded and used as (extended) finite state automata (Reffle, 2011). An alternative to such deductive approaches is the automatic induction of mappings. For example, Kestemont et al. (2010) use machine learning to convert 12th century Middle Dutch word forms to contemporary lemmas. Word modernization can be viewed as a special case of transforming cognate words from one language to a closely related one. This task has traditionally been performed with stochastic transducers or HMMs trained on a set of cognate word pairs (Mann and Yarowsky, 2001). More recently, character"
W13-2409,R11-1018,0,0.0365609,"Missing"
W13-2409,W02-0902,0,0.039742,"ion (C-SMT) (Vilar et al., 2007; Tiedemann, 2009) has been proposed as an alternative approach to translating words between closely related languages and has been shown to outperform stochastic transducers on the task of name transliteration (Tiedemann and Nabende, 2009). For the related task of matching cognate pairs in bilingual non-parallel corpora, various languageindependent similarity measures have been proposed on the basis of string edit distance (Kondrak and Dorr, 2004). Cognate word matching has been shown to facilitate the extraction of translation lexicons from comparable corpora (Koehn and Knight, 2002; Kondrak et al., 2003; Fišer and Ljubeši´c, 2011). Introduction A lot of recent work deals with detecting and matching cognate words in corpora of closely related language varieties. This approach is also useful for processing historical language (Piotrowski, 2012), where historical word forms are matched against contemporary forms, thus normalizing the varied and changing spelling of words over time. Such normalization has a number of applications: it enables better full-text search in cultural heritage digital libraries, makes old texts more understandable to today’s readers and significant"
W13-2409,2009.eamt-1.3,0,0.0391946,"utomata (Reffle, 2011). An alternative to such deductive approaches is the automatic induction of mappings. For example, Kestemont et al. (2010) use machine learning to convert 12th century Middle Dutch word forms to contemporary lemmas. Word modernization can be viewed as a special case of transforming cognate words from one language to a closely related one. This task has traditionally been performed with stochastic transducers or HMMs trained on a set of cognate word pairs (Mann and Yarowsky, 2001). More recently, character-based statistical machine translation (C-SMT) (Vilar et al., 2007; Tiedemann, 2009) has been proposed as an alternative approach to translating words between closely related languages and has been shown to outperform stochastic transducers on the task of name transliteration (Tiedemann and Nabende, 2009). For the related task of matching cognate pairs in bilingual non-parallel corpora, various languageindependent similarity measures have been proposed on the basis of string edit distance (Kondrak and Dorr, 2004). Cognate word matching has been shown to facilitate the extraction of translation lexicons from comparable corpora (Koehn and Knight, 2002; Kondrak et al., 2003; Fiš"
W13-2409,C04-1137,0,0.0389962,"ducers or HMMs trained on a set of cognate word pairs (Mann and Yarowsky, 2001). More recently, character-based statistical machine translation (C-SMT) (Vilar et al., 2007; Tiedemann, 2009) has been proposed as an alternative approach to translating words between closely related languages and has been shown to outperform stochastic transducers on the task of name transliteration (Tiedemann and Nabende, 2009). For the related task of matching cognate pairs in bilingual non-parallel corpora, various languageindependent similarity measures have been proposed on the basis of string edit distance (Kondrak and Dorr, 2004). Cognate word matching has been shown to facilitate the extraction of translation lexicons from comparable corpora (Koehn and Knight, 2002; Kondrak et al., 2003; Fišer and Ljubeši´c, 2011). Introduction A lot of recent work deals with detecting and matching cognate words in corpora of closely related language varieties. This approach is also useful for processing historical language (Piotrowski, 2012), where historical word forms are matched against contemporary forms, thus normalizing the varied and changing spelling of words over time. Such normalization has a number of applications: it ena"
W13-2409,N03-2016,0,0.0396247,"., 2007; Tiedemann, 2009) has been proposed as an alternative approach to translating words between closely related languages and has been shown to outperform stochastic transducers on the task of name transliteration (Tiedemann and Nabende, 2009). For the related task of matching cognate pairs in bilingual non-parallel corpora, various languageindependent similarity measures have been proposed on the basis of string edit distance (Kondrak and Dorr, 2004). Cognate word matching has been shown to facilitate the extraction of translation lexicons from comparable corpora (Koehn and Knight, 2002; Kondrak et al., 2003; Fišer and Ljubeši´c, 2011). Introduction A lot of recent work deals with detecting and matching cognate words in corpora of closely related language varieties. This approach is also useful for processing historical language (Piotrowski, 2012), where historical word forms are matched against contemporary forms, thus normalizing the varied and changing spelling of words over time. Such normalization has a number of applications: it enables better full-text search in cultural heritage digital libraries, makes old texts more understandable to today’s readers and significantly improves further te"
W13-2409,N01-1020,0,\N,Missing
W13-2409,W07-0705,0,\N,Missing
W13-2409,P07-2045,0,\N,Missing
W13-5306,N03-1017,0,0.00421059,"rm better than language-independent methods with an accurately chosen threshold. Cognate extraction by formal similarity (4.1.1) Training of the C-SMT model (4.1.2) Application of the C-SMT model, frequency and confidence filtering (4.1.3) Inferring word pairs with combined contextual and formal similarity (4.2.1) 2.2 Bilingual lexicon induction: hwRL , wNRL i pairs The principle underlying statistical machine translation (SMT) consists in learning alignments between pairs of words co-occurring in a parallel corpus. In phrase-based SMT, words may be grouped together to form so-called phrases (Koehn et al., 2003). Recently, a variant of this model has been proposed: character-based SMT, or henceforth C-SMT (Vilar et al., 2007; Tiedemann, 2009). In this paradigm, instead of aligning words (or word phrases) in a corpus consisting of sentences, one aligns characters (or segments of characters) in a corpus consisting of words. Of course, character alignments are well defined only for cognate pairs. Thus, it has been applied to translation between closely related languages (Vilar et al., 2007; Tiedemann, 2009) and to transliteration (Tiedemann and Nabende, 2009). Whereas in the existing C-SMT literature tr"
W13-5306,C04-1137,0,0.805659,"are regular. In closely related languages, cognates account for a large part of the lexicon. Mann and Yarowsky (2001) aim to detect cognate pairs in order to induce a translation lexicon. They evaluate different measures of phonetic or graphemic distance on this task. In particular, they distinguish static measures (independent of the language pair) from adaptive measures (adapted to the language pair by machine learning). Unsurprisingly, the authors observe better performances with the adaptive measures. However, they require a bilingual training corpus which we do not have at our disposal. Kondrak and Dorr (2004) present a large number of language-independent distance measures in order to predict whether two drug names are confusable or not. Among the graphemic measures (they also propose measures operating on phonetic transcriptions), the BI-SIM algorithm (see Section 4.1.1) yields the best results. Inkpen et al. (2005) apply these measures to the task of cognate identification in related languages (English– 2.3 Context similarity Exploiting context similarity is a promising approach for the induction of translation pairs from comparable corpora, whether the languages are closely related or not. The"
W13-5306,feldman-etal-2006-cross,0,0.466697,"706 431 884 456 197 515 193 3 451 532 2 252 337 Spanish Portuguese Transfer of morphosyntactic annotations Table 1: Wikipedia corpora The most straightforward idea for annotating a text from a non-resourced language consists in using a word-aligned parallel corpus, annotating the resourced side of it, and transferring the annotations to the aligned words in the other language. Yarowsky et al. (2001) successfully apply this approach to POS tagging, noun phrase chunking, named entity classification and even morphological analysis induction. Another approach to this problem has been proposed by Feldman et al. (2006). They train a tagger on the resourced language and apply it to the non-resourced language, after some modifications to the tagging model. Such a tagger is bound to have a high OOV rate, and Feldman et al. (2006) propose two strategies to reduce it. First, they use a basic morphological analyzer for the nonresourced language to predict potential tags. Second, they extract a list of cognate pairs in order to transfer tags from one language to the other. While this approach looks promising, we chose to avoid the manual creation of a morphological analyzer, thus keeping our approach fully automat"
W13-5306,N01-1020,0,0.20443,"ech tags (5.1) Tagging of non-tagged words by suffix analogy (5.2) Creation of morphological lexicon: hwNRL ,ti pairs Figure 1: Flowchart of the proposed approach. 2.1 Character-based statistical machine translation Cognate detection Hauer and Kondrak (2011) define cognates as words of different languages that share a common linguistic origin. Two words form a cognate pair if they are (1) phonetically or graphemically similar, (2) semantically similar, and (3) if the phonetic or graphemic similarities are regular. In closely related languages, cognates account for a large part of the lexicon. Mann and Yarowsky (2001) aim to detect cognate pairs in order to induce a translation lexicon. They evaluate different measures of phonetic or graphemic distance on this task. In particular, they distinguish static measures (independent of the language pair) from adaptive measures (adapted to the language pair by machine learning). Unsurprisingly, the authors observe better performances with the adaptive measures. However, they require a bilingual training corpus which we do not have at our disposal. Kondrak and Dorr (2004) present a large number of language-independent distance measures in order to predict whether t"
W13-5306,R11-1018,0,0.438587,"Missing"
W13-5306,J03-1002,0,0.00480808,"we consider the NRL as the source language and the RL as the target language. In particular, this allows us to match different wNRL with the same wRL and thus to take into account orthographic variation in the NRL. Such variation is less expected in the RL, which is assumed to have standardized spelling. Moreover, the classic SMT architecture puts the resource-intensive language model on the target language side, which is an additional argument in favour of the chosen translation direction. 4.1.2 Training of the C-SMT model Our C-SMT model relies on the standard pipeline consisting of GIZA++ (Och and Ney, 2003) for character alignment, IRSTLM (Federico et al., 2008) for language modelling, and Moses (Koehn et al., 2007) for phrase extraction and decoding. These tools may be configured in various ways; we have tested a large set of parameter configurations in preliminary experiments, but due to space restrictions, we just mention the parameter settings that we finally retained. • We add special symbols to the beginning and the end of each word. • We train a character 10-gram language model on the target language words. We removed words appearing less than 10 times in the corpus; each word is repeated"
W13-5306,P99-1067,0,0.467575,"of language-independent distance measures in order to predict whether two drug names are confusable or not. Among the graphemic measures (they also propose measures operating on phonetic transcriptions), the BI-SIM algorithm (see Section 4.1.1) yields the best results. Inkpen et al. (2005) apply these measures to the task of cognate identification in related languages (English– 2.3 Context similarity Exploiting context similarity is a promising approach for the induction of translation pairs from comparable corpora, whether the languages are closely related or not. The main idea (Fung, 1998; Rapp, 1999) is to extract word n-grams (or alternatively, bags of words) from both languages and induce word pairs that co-occur in the neighbourhood (context) of already known word pairs. For example, a French word appearing in the context of the word école is likely to be translated by an English word appearing in the context of the word school. This method requires a seed word lexicon (e.g., containing the pair hécole, schooli), as well as large corpora in both languages in order to build sufficiently large similarity vectors. Fišer and Ljubeši´c (2011) adapt this method to closely related languages:"
W13-5306,P01-1058,0,0.0377259,"data and etymological distance, making them a good testing ground for our methods. Moreover, we use subsets of varying size of Catalan–Spanish to assess the impact of the data size (see Table 1). We evaluate all five language pairs on the lexicon induction task on the basis of the dictionaries made available through the Apertium project (Forcada et al., 2011) (see Table 2). The Spanish tag dictionary is extracted from the AnCora-ES corpus (Taulé et al., 2008).1 It contains 42 part-of-speech tags and covers 40 148 words. The Portuguese tag dictionary is extracted from the CETEMPúblico corpus (Santos and Rocha, 2001).2 It contains 117 part-of-speech tags (of which 48 are combinations of two tags) and covers 107 235 words. Data Our approach relies on three types of data: 1. A raw text of the NRL. From this text we extract word lists for cognate induction, frequency information by word-type as well as morphosyntactic contexts. 2. A raw text of the RL, from which we extract the same information. 3. A tag dictionary which associates RL words with their part-of-speech tags. We extract this dictionary from an annotated RL corpus; note however that tag dictionaries may be obtained from other sources, in which ca"
W13-5306,taule-etal-2008-ancora,0,0.0349568,"ician–Spanish and Galician– Portuguese, using raw text extracted from the respective Wikipedias. These language pairs vary widely in terms of available raw data and etymological distance, making them a good testing ground for our methods. Moreover, we use subsets of varying size of Catalan–Spanish to assess the impact of the data size (see Table 1). We evaluate all five language pairs on the lexicon induction task on the basis of the dictionaries made available through the Apertium project (Forcada et al., 2011) (see Table 2). The Spanish tag dictionary is extracted from the AnCora-ES corpus (Taulé et al., 2008).1 It contains 42 part-of-speech tags and covers 40 148 words. The Portuguese tag dictionary is extracted from the CETEMPúblico corpus (Santos and Rocha, 2001).2 It contains 117 part-of-speech tags (of which 48 are combinations of two tags) and covers 107 235 words. Data Our approach relies on three types of data: 1. A raw text of the NRL. From this text we extract word lists for cognate induction, frequency information by word-type as well as morphosyntactic contexts. 2. A raw text of the RL, from which we extract the same information. 3. A tag dictionary which associates RL words with their"
W13-5306,I11-1097,0,0.0235239,"tion (Tiedemann and Nabende, 2009). Whereas in the existing C-SMT literature training data is extracted from parallel corpora, we propose to create a (noisy) training corpus from monolingual corpora using cognate detection. Inferring high-frequency word pairs with contextual similarity (4.2.2) Addition of formally identical word pairs (4.3) Transfer of part-of-speech tags (5.1) Tagging of non-tagged words by suffix analogy (5.2) Creation of morphological lexicon: hwNRL ,ti pairs Figure 1: Flowchart of the proposed approach. 2.1 Character-based statistical machine translation Cognate detection Hauer and Kondrak (2011) define cognates as words of different languages that share a common linguistic origin. Two words form a cognate pair if they are (1) phonetically or graphemically similar, (2) semantically similar, and (3) if the phonetic or graphemic similarities are regular. In closely related languages, cognates account for a large part of the lexicon. Mann and Yarowsky (2001) aim to detect cognate pairs in order to induce a translation lexicon. They evaluate different measures of phonetic or graphemic distance on this task. In particular, they distinguish static measures (independent of the language pair)"
W13-5306,2009.eamt-1.3,0,0.398913,"g of the C-SMT model (4.1.2) Application of the C-SMT model, frequency and confidence filtering (4.1.3) Inferring word pairs with combined contextual and formal similarity (4.2.1) 2.2 Bilingual lexicon induction: hwRL , wNRL i pairs The principle underlying statistical machine translation (SMT) consists in learning alignments between pairs of words co-occurring in a parallel corpus. In phrase-based SMT, words may be grouped together to form so-called phrases (Koehn et al., 2003). Recently, a variant of this model has been proposed: character-based SMT, or henceforth C-SMT (Vilar et al., 2007; Tiedemann, 2009). In this paradigm, instead of aligning words (or word phrases) in a corpus consisting of sentences, one aligns characters (or segments of characters) in a corpus consisting of words. Of course, character alignments are well defined only for cognate pairs. Thus, it has been applied to translation between closely related languages (Vilar et al., 2007; Tiedemann, 2009) and to transliteration (Tiedemann and Nabende, 2009). Whereas in the existing C-SMT literature training data is extracted from parallel corpora, we propose to create a (noisy) training corpus from monolingual corpora using cognate"
W13-5306,W02-0902,0,0.754796,"gible and demand for translation is low. In this paper, we present a generic approach for the transfer of part-of-speech (POS) annotations from a resourced language (RL) towards an etymologically closely related non-resourced language (NRL), without using any bilingual (i.e., parallel) data. We rely on two hypotheses. First, on the lexical level, the two languages share a lot of cognates, i.e., word pairs that are formally similar and that are translations of each other. Second, on the structural level, we admit that the word order of both languages is similar, and that the set 2 Related work Koehn and Knight (2002) propose various methods for inferring translation lexicons using only monolingual data. They consider several clues, including the identity or formal similarity of words (i.e., borrowings and cognates), similarity of the contexts of occurrence, and similarity of the frequency of words. They evaluate their method on English–German noun pairs. Our work is partly inspired by this paper, but uses different combinations of clues as well as updated methods and algorithms, and extends the task to POS tagging. We shall now describe in more detail the three major types of clues used in the literature."
W13-5306,W07-0705,0,0.289818,"rity (4.1.1) Training of the C-SMT model (4.1.2) Application of the C-SMT model, frequency and confidence filtering (4.1.3) Inferring word pairs with combined contextual and formal similarity (4.2.1) 2.2 Bilingual lexicon induction: hwRL , wNRL i pairs The principle underlying statistical machine translation (SMT) consists in learning alignments between pairs of words co-occurring in a parallel corpus. In phrase-based SMT, words may be grouped together to form so-called phrases (Koehn et al., 2003). Recently, a variant of this model has been proposed: character-based SMT, or henceforth C-SMT (Vilar et al., 2007; Tiedemann, 2009). In this paradigm, instead of aligning words (or word phrases) in a corpus consisting of sentences, one aligns characters (or segments of characters) in a corpus consisting of words. Of course, character alignments are well defined only for cognate pairs. Thus, it has been applied to translation between closely related languages (Vilar et al., 2007; Tiedemann, 2009) and to transliteration (Tiedemann and Nabende, 2009). Whereas in the existing C-SMT literature training data is extracted from parallel corpora, we propose to create a (noisy) training corpus from monolingual cor"
W13-5306,D11-1119,0,0.0997769,"Missing"
W13-5306,H01-1035,0,0.183135,"22 876 44 502 487 945 2 699 006 7 939 544 5 478 092 3 600 117 32 240 505 200 011 499 978 999 948 9 999 857 49 999 543 139 160 258 215 809 201 417 674 848 23 230 41 908 62 772 267 786 882 842 1 712 078 23 381 287 12 611 706 431 884 456 197 515 193 3 451 532 2 252 337 Spanish Portuguese Transfer of morphosyntactic annotations Table 1: Wikipedia corpora The most straightforward idea for annotating a text from a non-resourced language consists in using a word-aligned parallel corpus, annotating the resourced side of it, and transferring the annotations to the aligned words in the other language. Yarowsky et al. (2001) successfully apply this approach to POS tagging, noun phrase chunking, named entity classification and even morphological analysis induction. Another approach to this problem has been proposed by Feldman et al. (2006). They train a tagger on the resourced language and apply it to the non-resourced language, after some modifications to the tagging model. Such a tagger is bound to have a high OOV rate, and Feldman et al. (2006) propose two strategies to reduce it. First, they use a basic morphological analyzer for the nonresourced language to predict potential tags. Second, they extract a list"
W13-5306,P07-2045,0,\N,Missing
W14-5304,A00-1031,0,0.12705,"e independence assumption between transitions and emissions: crucially, the emission probability of a word only depends on its tag; it does not depend on previous words or on previous tags. Assuming, as stated in the introduction, that the word order is similar and the tag sets identical between the RL and the NRL, we argue that the transition probabilities estimated on RL data are also valid for NRL. Only the emission probabilities have to be adapted since RL words are formally different from NRL words. Following earlier work (Feldman et al., 2006; Duong et al., 2013), we use the TnT tagger (Brants, 2000), an implementation of a trigram HMM tagger that includes smoothing and handling of unknown words. In contrast to other implementations that use inaccessible binary files, TnT stores the estimated parameters in easily modifiable plain text files. 3.1 Adapting emission counts The goal of this work is to adapt an existing RL HMM tagger for a closely related NRL by replacing the RL words in the emission parameters by the corresponding NRL words. Let us explain this process with an example, using Spanish as RL and Catalan as NRL. The TnT tagger creates an emission parameter file that contains, for"
W14-5304,P11-1061,0,0.0404087,"2 Related work The task of creating part-of-speech taggers (and other NLP tools) for new languages without resorting to manually annotated corpora has inspired a lot of recent research. The most popular line of work, initiated by Yarowsky et al. (2001), draws on parallel corpora. They tag the source side of a parallel corpus with an existing tagger, and then project the tags along the word alignment links onto the target side of the parallel corpus. A new tagger is then trained on the target side, using aggressive smoothing to reduce the noise caused by alignment errors. In a similar setting, Das and Petrov (2011) use a more sophisticated graph-based projection algorithm with label propagation to obtain high-precision tags for the target words. Follow-up work by Li et al. (2012) uses tag dictionaries extracted from Wiktionary instead of parallel corpora, and Täckström et al. (2013) attempt to combine these two data sources: the Wiktionary data provides constraints on word types, whereas the parallel data is used to filter these constraints on the token level, depending on the context of a given word occurrence. Duong et al. (2013) show that the original approach of Das and Petrov (2011) can be simplifi"
W14-5304,P13-2112,0,0.249121,"to reduce the noise caused by alignment errors. In a similar setting, Das and Petrov (2011) use a more sophisticated graph-based projection algorithm with label propagation to obtain high-precision tags for the target words. Follow-up work by Li et al. (2012) uses tag dictionaries extracted from Wiktionary instead of parallel corpora, and Täckström et al. (2013) attempt to combine these two data sources: the Wiktionary data provides constraints on word types, whereas the parallel data is used to filter these constraints on the token level, depending on the context of a given word occurrence. Duong et al. (2013) show that the original approach of Das and Petrov (2011) can be simplified by focusing on high-confidence alignment links, thus achieving equivalent performance without resorting to graph-based projection. The research based on parallel corpora does not assume any particular etymological relationship between the two languages, but Duong et al. (2013) note that their approach works best when the source and target languages are closely related. Other approaches explicity model the case of two closely related languages, such as Feldman et al. (2006). They train a tagger on the source language wi"
W14-5304,feldman-etal-2006-cross,0,0.653858,"written resources are required if the RL and the NRL are closely related. We propose a generic method for tagger adaptation that relies on three assumptions which generally hold for closely related language varieties. First, we assume that the two languages share a lot of cognates, i.e., word pairs that are formally similar and that are translations of each other. Second, we suppose that the word order of both languages is similar. Third, we assume that the set of POS tags is identical. Under these assumptions, we can avoid the requirements of parallel data and of manual annotation. Following Feldman et al. (2006), the reasoning behind our method is that a Hidden Markov Model (HMM) tagger trained in a supervised way on RL data can be adapted to the NRL by translating the RL words in its parameter files to the NRL. This requires a bilingual dictionary between RL words and NRL words. In this paper, we create different HMM taggers using the bilingual dictionaries obtained with the unsupervised lexicon induction methods presented in our earlier work (Scherrer and Sagot, 2014). The paper is organized as follows. In Section 2, we present related work on tagger adaptation and lexicon induction. In Section 3,"
W14-5304,R11-1018,0,0.0288415,"Missing"
W14-5304,N13-1014,0,0.0343455,"s are closely related. Other approaches explicity model the case of two closely related languages, such as Feldman et al. (2006). They train a tagger on the source language with standard tools and resources, and then adapt the parameter files of that tagger to the target language using a hand-written morphological analyzer and a list of cognate word pairs. Bernhard and Ligozat (2013) use a similar approach to adapt a German tagger to Alsatian; they show that manually annotating a small list of closed-class words leads to considerable gains in tagging accuracy. In a slightly different setting, Garrette and Baldridge (2013) show that taggers for low-resource languages can be built from scratch with only two hours of manual annotation work. Even though recent work on closely related and low-resource languages presupposes manually annotated data to some extent, we believe that it is possible to create a tagger for such languages fully automatically. We adopt the general model proposed by Feldman et al. (2006), but use automatically induced bilingual dictionaries to translate the source language words in the tagger parameter files. The bilingual dictionaries are obtained with our unsupervised lexicon induction pipe"
W14-5304,W02-0902,0,0.0580907,"annotation work. Even though recent work on closely related and low-resource languages presupposes manually annotated data to some extent, we believe that it is possible to create a tagger for such languages fully automatically. We adopt the general model proposed by Feldman et al. (2006), but use automatically induced bilingual dictionaries to translate the source language words in the tagger parameter files. The bilingual dictionaries are obtained with our unsupervised lexicon induction pipeline (Scherrer and Sagot, 2013; Scherrer and Sagot, 2014). This pipeline is inspired by early work by Koehn and Knight (2002), who propose various methods for inferring translation lexicons using monolingual data. Our lexicon induction pipeline is composed of three main steps. First, a list of formally similar word pairs (cognate pairs) is extracted from monolingual corpora using the BI-SIM score (Kondrak and Dorr, 2004). Second, regularities occurring in these word pairs are learned by training and applying a characterlevel statistical machine translation (CSMT) system (Vilar et al., 2007; Tiedemann, 2009). Third, crosslingual contextual similarity measures are used to induce additional word pairs. The main idea is"
W14-5304,C04-1137,0,0.0151458,"automatically induced bilingual dictionaries to translate the source language words in the tagger parameter files. The bilingual dictionaries are obtained with our unsupervised lexicon induction pipeline (Scherrer and Sagot, 2013; Scherrer and Sagot, 2014). This pipeline is inspired by early work by Koehn and Knight (2002), who propose various methods for inferring translation lexicons using monolingual data. Our lexicon induction pipeline is composed of three main steps. First, a list of formally similar word pairs (cognate pairs) is extracted from monolingual corpora using the BI-SIM score (Kondrak and Dorr, 2004). Second, regularities occurring in these word pairs are learned by training and applying a characterlevel statistical machine translation (CSMT) system (Vilar et al., 2007; Tiedemann, 2009). Third, crosslingual contextual similarity measures are used to induce additional word pairs. The main idea is to extract word n-grams from comparable corpora of both languages and induce word pairs that co-occur in the context of already known word pairs (Fung, 1998; Rapp, 1999; Fišer and Ljubeši´c, 2011). In our pipeline, the already known word pairs are those induced with CSMT. In this paper, we extend"
W14-5304,D12-1127,0,0.0492484,"nt research. The most popular line of work, initiated by Yarowsky et al. (2001), draws on parallel corpora. They tag the source side of a parallel corpus with an existing tagger, and then project the tags along the word alignment links onto the target side of the parallel corpus. A new tagger is then trained on the target side, using aggressive smoothing to reduce the noise caused by alignment errors. In a similar setting, Das and Petrov (2011) use a more sophisticated graph-based projection algorithm with label propagation to obtain high-precision tags for the target words. Follow-up work by Li et al. (2012) uses tag dictionaries extracted from Wiktionary instead of parallel corpora, and Täckström et al. (2013) attempt to combine these two data sources: the Wiktionary data provides constraints on word types, whereas the parallel data is used to filter these constraints on the token level, depending on the context of a given word occurrence. Duong et al. (2013) show that the original approach of Das and Petrov (2011) can be simplified by focusing on high-confidence alignment links, thus achieving equivalent performance without resorting to graph-based projection. The research based on parallel cor"
W14-5304,petrov-etal-2012-universal,0,0.0278411,"ulé et al., 2008), which contains about 500 000 words. The AnCora morphosyntactic annotation includes the main category (e.g. noun), the subcategory (e.g. proper noun), and several morphological categories (e.g., gender, number, person, tense, mode), yielding about 280 distinct labels. Since we are mainly interested in part-of-speech information, we simplified these labels by taking into account the two first characters of each label, corresponding to the main category and the subcategory. This simplified tagset contains 42 distinct labels, which is still considerably more than the 12 tags of Petrov et al. (2012) commonly used in comparable settings. All taggers need to be evaluated on a Catalan gold standard that shares the same tagset as Spanish. For this purpose, we use the Catalan part of AnCora, which also contains about 500 000 words. We simplified the tags in the same way as above. The Catalan part of AnCora is also used to train the supervised models presented in Sections 4.3 and 4.4. Finally, the lexicon induction algorithms require data on their own, which we present here for completeness. As in Scherrer and Sagot (2013), we use Wikipedia dumps consisting of 140M words for Catalan and 430M w"
W14-5304,P99-1067,0,0.181518,"a list of formally similar word pairs (cognate pairs) is extracted from monolingual corpora using the BI-SIM score (Kondrak and Dorr, 2004). Second, regularities occurring in these word pairs are learned by training and applying a characterlevel statistical machine translation (CSMT) system (Vilar et al., 2007; Tiedemann, 2009). Third, crosslingual contextual similarity measures are used to induce additional word pairs. The main idea is to extract word n-grams from comparable corpora of both languages and induce word pairs that co-occur in the context of already known word pairs (Fung, 1998; Rapp, 1999; Fišer and Ljubeši´c, 2011). In our pipeline, the already known word pairs are those induced with CSMT. In this paper, we extend our previous work (Scherrer and Sagot, 2014) in two aspects. First, we use a more powerful HMM tagging model instead of the simple unigram tagger that insufficiently accounts for the ambiguity in language. Second, we assess the impact of each lexicon induction step separately rather than merely evaluating the final result of the pipeline. 3 HMM tagging Hidden Markov Models (HMMs) are a simple yet powerful formal device frequently used for part-ofspeech tagging. A HM"
W14-5304,W13-5306,1,0.737542,"that taggers for low-resource languages can be built from scratch with only two hours of manual annotation work. Even though recent work on closely related and low-resource languages presupposes manually annotated data to some extent, we believe that it is possible to create a tagger for such languages fully automatically. We adopt the general model proposed by Feldman et al. (2006), but use automatically induced bilingual dictionaries to translate the source language words in the tagger parameter files. The bilingual dictionaries are obtained with our unsupervised lexicon induction pipeline (Scherrer and Sagot, 2013; Scherrer and Sagot, 2014). This pipeline is inspired by early work by Koehn and Knight (2002), who propose various methods for inferring translation lexicons using monolingual data. Our lexicon induction pipeline is composed of three main steps. First, a list of formally similar word pairs (cognate pairs) is extracted from monolingual corpora using the BI-SIM score (Kondrak and Dorr, 2004). Second, regularities occurring in these word pairs are learned by training and applying a characterlevel statistical machine translation (CSMT) system (Vilar et al., 2007; Tiedemann, 2009). Third, crossli"
W14-5304,scherrer-sagot-2014-language,1,0.90854,"et of POS tags is identical. Under these assumptions, we can avoid the requirements of parallel data and of manual annotation. Following Feldman et al. (2006), the reasoning behind our method is that a Hidden Markov Model (HMM) tagger trained in a supervised way on RL data can be adapted to the NRL by translating the RL words in its parameter files to the NRL. This requires a bilingual dictionary between RL words and NRL words. In this paper, we create different HMM taggers using the bilingual dictionaries obtained with the unsupervised lexicon induction methods presented in our earlier work (Scherrer and Sagot, 2014). The paper is organized as follows. In Section 2, we present related work on tagger adaptation and lexicon induction. In Section 3, we review Hidden Markov Models and their relevance for tagging and for our method of tagger adaptation. Section 4 presents a set of different taggers in some detail and evaluates them on Catalan, using Spanish as RL. In Section 5, we demonstrate the validity of the proposed approach by performing small-scale evaluations on a number of Romance, Germanic and Slavic languages: we transfer part-of-speech tags from Spanish to Aragonese, from Czech to Slovak and Sorbia"
W14-5304,Q13-1001,0,0.0299912,"corpora. They tag the source side of a parallel corpus with an existing tagger, and then project the tags along the word alignment links onto the target side of the parallel corpus. A new tagger is then trained on the target side, using aggressive smoothing to reduce the noise caused by alignment errors. In a similar setting, Das and Petrov (2011) use a more sophisticated graph-based projection algorithm with label propagation to obtain high-precision tags for the target words. Follow-up work by Li et al. (2012) uses tag dictionaries extracted from Wiktionary instead of parallel corpora, and Täckström et al. (2013) attempt to combine these two data sources: the Wiktionary data provides constraints on word types, whereas the parallel data is used to filter these constraints on the token level, depending on the context of a given word occurrence. Duong et al. (2013) show that the original approach of Das and Petrov (2011) can be simplified by focusing on high-confidence alignment links, thus achieving equivalent performance without resorting to graph-based projection. The research based on parallel corpora does not assume any particular etymological relationship between the two languages, but Duong et al."
W14-5304,taule-etal-2008-ancora,0,0.0354594,"of them (Sections 4.2 to 4.4) are supervised taggers and serve as baseline taggers and as upper bounds. The four remaining taggers (Sections 4.6 to 4.9) are taggers created by adaptation from a Spanish tagger, using the method presented in Section 3.1; 32 they differ in the lexicons used to translate the emission counts. These four taggers represent the main contribution of this paper. We start by listing the data used in our experiments. 4.1 Data Most taggers presented below are initially trained on a part-of-speech annotated corpus of Spanish. We use the Spanish part of the AnCora treebank (Taulé et al., 2008), which contains about 500 000 words. The AnCora morphosyntactic annotation includes the main category (e.g. noun), the subcategory (e.g. proper noun), and several morphological categories (e.g., gender, number, person, tense, mode), yielding about 280 distinct labels. Since we are mainly interested in part-of-speech information, we simplified these labels by taking into account the two first characters of each label, corresponding to the main category and the subcategory. This simplified tagset contains 42 distinct labels, which is still considerably more than the 12 tags of Petrov et al. (20"
W14-5304,2009.eamt-1.3,0,0.0155116,"peline (Scherrer and Sagot, 2013; Scherrer and Sagot, 2014). This pipeline is inspired by early work by Koehn and Knight (2002), who propose various methods for inferring translation lexicons using monolingual data. Our lexicon induction pipeline is composed of three main steps. First, a list of formally similar word pairs (cognate pairs) is extracted from monolingual corpora using the BI-SIM score (Kondrak and Dorr, 2004). Second, regularities occurring in these word pairs are learned by training and applying a characterlevel statistical machine translation (CSMT) system (Vilar et al., 2007; Tiedemann, 2009). Third, crosslingual contextual similarity measures are used to induce additional word pairs. The main idea is to extract word n-grams from comparable corpora of both languages and induce word pairs that co-occur in the context of already known word pairs (Fung, 1998; Rapp, 1999; Fišer and Ljubeši´c, 2011). In our pipeline, the already known word pairs are those induced with CSMT. In this paper, we extend our previous work (Scherrer and Sagot, 2014) in two aspects. First, we use a more powerful HMM tagging model instead of the simple unigram tagger that insufficiently accounts for the ambigui"
W14-5304,W07-0705,0,0.0353776,"lexicon induction pipeline (Scherrer and Sagot, 2013; Scherrer and Sagot, 2014). This pipeline is inspired by early work by Koehn and Knight (2002), who propose various methods for inferring translation lexicons using monolingual data. Our lexicon induction pipeline is composed of three main steps. First, a list of formally similar word pairs (cognate pairs) is extracted from monolingual corpora using the BI-SIM score (Kondrak and Dorr, 2004). Second, regularities occurring in these word pairs are learned by training and applying a characterlevel statistical machine translation (CSMT) system (Vilar et al., 2007; Tiedemann, 2009). Third, crosslingual contextual similarity measures are used to induce additional word pairs. The main idea is to extract word n-grams from comparable corpora of both languages and induce word pairs that co-occur in the context of already known word pairs (Fung, 1998; Rapp, 1999; Fišer and Ljubeši´c, 2011). In our pipeline, the already known word pairs are those induced with CSMT. In this paper, we extend our previous work (Scherrer and Sagot, 2014) in two aspects. First, we use a more powerful HMM tagging model instead of the simple unigram tagger that insufficiently accoun"
W14-5304,H01-1035,0,0.17464,"ion 6. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 30 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 30–38, Dublin, Ireland, August 23 2014. 2 Related work The task of creating part-of-speech taggers (and other NLP tools) for new languages without resorting to manually annotated corpora has inspired a lot of recent research. The most popular line of work, initiated by Yarowsky et al. (2001), draws on parallel corpora. They tag the source side of a parallel corpus with an existing tagger, and then project the tags along the word alignment links onto the target side of the parallel corpus. A new tagger is then trained on the target side, using aggressive smoothing to reduce the noise caused by alignment errors. In a similar setting, Das and Petrov (2011) use a more sophisticated graph-based projection algorithm with label propagation to obtain high-precision tags for the target words. Follow-up work by Li et al. (2012) uses tag dictionaries extracted from Wiktionary instead of par"
W17-1201,W16-4802,0,0.127115,"character ngrams and a Na”ive Bayes classifier. The system followed the work of the system submitted to the DSL 2016 by Barbaresi (2016). • CECL: The system uses a two-step approach as in (Goutte et al., 2014). The first step identifies the language group using an SVM classifier with a linear kernel trained on character n-grams (1-4) that occur at least 100 times in the dataset weighted by Okapi BM25 (Robertson et al., 1995). The second step discriminates between each language within the group using a set of SVM classifiers trained • tubasfs: Following the success of tubasfs at DSL 2016 (C¸o¨ ltekin and Rama, 2016), which was ranked first in the closed training track, this year’s tubasfs submission used a linear SVM classifier. The system used both characters and words as features, and carefully optimized hyperparameters: n-gram size and margin/regularization parameter for SVM. 5 In 2016 ADI and DSL were organized under the name DSL shared task, and ADI was run as a sub-task. 4 • gauge: This team submitted a total of three runs. Run 1 used an SVM classifier with character n-grams (2–6), run 2 (their best run) used logistic regression trained using character n-grams (1–6), and run 3 used hard voting of t"
W17-1201,W17-1221,0,0.532877,"of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent pu"
W17-1201,W17-1215,0,0.0474398,"Missing"
W17-1201,W17-1213,0,0.0702486,"pairs include a triple of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar langua"
W17-1201,W13-1728,1,0.0339111,"rovided lexical features. This year, we added a multi-model aspect to the task by further providing acoustic features. The system description paper of CECL (Bestgen, 2017) provides some interesting insights about the DSL task. First, they found out that BM25 weighting, which was previously applied to native language identification (NLI) (Wang et al., 2016), worked better than using TF.IDF. They further highlighted the similarity between similar language identification and NLI as evidenced by a number of entries in the DSL task that are adaptations of systems used for NLI (Goutte et al., 2013; Gebre et al., 2013; Jarvis et al., 2013). We observe that the variation in performance among the top ten teams is less than four percentage points. The team ranked last (eleventh) approached the task using LSTM and achieved an F1 score of 0.202. Unfortunately, they did not submit a system description paper, and thus we do not have much detail about their system. However, in the DSL 2016 task (Malmasi et al., 2016), neural network-based approaches already proved not to be very competitive for the task. See (Medvedeva et al., 2017) for a comparison between the performance of an SVM and an RNN approach for the DSL"
W17-1201,W17-1217,0,0.0300428,"Missing"
W17-1201,W15-5413,0,0.101469,"Missing"
W17-1201,W13-1712,0,0.199423,"Missing"
W17-1201,W14-5316,0,0.160565,"Missing"
W17-1201,U13-1003,0,0.160089,"est set A: in the closed training setting, where the systems were trained only using the training data provided by the DSL organizers, their accuracy dropped from 95.54 to 94.01, from 95.24 to 92.78, from 95.24 to 93.01, and from 94.67 to 93.02, respectively.3 Finally, inspired by recent work on language identification of user-generated content (Ljubeˇsi´c and Kranjˇci´c, 2015; Abainia et al., 2016), in the DSL 2016 task (Malmasi et al., 2016), we looked at how systems perform on discriminating between similar languages and language varieties across different domains, an aspect highlighted by Lui and Cook (2013) and Lui (2014). For this purpose, we provided an out-of-domain test set containing manually annotated microblog posts written in Bosnian, Croatian, Serbian, Brazilian and European Portuguese. 2.1 2.2 Dataset The DSLCC v4.04 contains 22,000 short excerpts of news texts for each language or language variety divided into 20,000 texts for training (18,000 texts) and development (2,000 texts), and 2,000 texts for testing. It contains a total of 8.6 million tokens for training and over half a million tokens for testing. The fourteen languages included in the v4.0 grouped by similarity are Bosnian,"
W17-1201,L16-1284,1,0.900242,"Missing"
W17-1201,W16-3928,0,0.0176574,"ranging from 3 for CLP to 11 for DSL. Below we describe the individual tasks. 2 Discriminating between Similar Languages (DSL) Discriminating between similar languages is one of the main challenges faced by language identification systems. Since 2014 the DSL shared task has been organized every year providing scholars and developers with an opportunity to evaluate language identification methods using a standard dataset and evaluation methodology. Albeit related to other shared tasks such as the 2014 TweetLID challenge (Zubiaga et al., 2014) and the 2016 shared task on Geolocation Prediction (Han et al., 2016), the DSL shared task continues to be the only shared task focusing on the discrimination between similar languages and language varieties. 1 The MAZA team submitted two separate papers: one for each task they participated in. 2 This number does not include the submissions to the Arabic Dialect Identification subtask of DSL in 2016. 2 At DSL 2015, the four best systems, MAC (Malmasi and Dras, 2015b), MMS (Zampieri et al., 2015a), NRC (Goutte and L´eger, 2015), and SUKI (Jauhiainen et al., 2015) performed similarly on test set B compared to test set A: in the closed training setting, where the"
W17-1201,W15-5407,1,0.88226,"ods using a standard dataset and evaluation methodology. Albeit related to other shared tasks such as the 2014 TweetLID challenge (Zubiaga et al., 2014) and the 2016 shared task on Geolocation Prediction (Han et al., 2016), the DSL shared task continues to be the only shared task focusing on the discrimination between similar languages and language varieties. 1 The MAZA team submitted two separate papers: one for each task they participated in. 2 This number does not include the submissions to the Arabic Dialect Identification subtask of DSL in 2016. 2 At DSL 2015, the four best systems, MAC (Malmasi and Dras, 2015b), MMS (Zampieri et al., 2015a), NRC (Goutte and L´eger, 2015), and SUKI (Jauhiainen et al., 2015) performed similarly on test set B compared to test set A: in the closed training setting, where the systems were trained only using the training data provided by the DSL organizers, their accuracy dropped from 95.54 to 94.01, from 95.24 to 92.78, from 95.24 to 93.01, and from 94.67 to 93.02, respectively.3 Finally, inspired by recent work on language identification of user-generated content (Ljubeˇsi´c and Kranjˇci´c, 2015; Abainia et al., 2016), in the DSL 2016 task (Malmasi et al., 2016), we l"
W17-1201,W17-1222,1,0.800029,"milar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al.,"
W17-1201,W17-1211,0,0.333885,"Danish, and Norwegian (TL) – Swedish (SL). Note that the latter two pairs include a triple of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the r"
W17-1201,W17-1220,1,0.878577,"milar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al.,"
W17-1201,W16-4801,1,0.679876,"Missing"
W17-1201,W17-1225,0,0.428199,"us Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al., 2016). We also saw the number of system submissions to the DSL challenge grow from 8 in 2014 to 10 in 2015 and then to 17 in 2016.2 The 2015 and the 2016 editions of the DSL"
W17-1201,W13-1714,0,0.148384,"ures. This year, we added a multi-model aspect to the task by further providing acoustic features. The system description paper of CECL (Bestgen, 2017) provides some interesting insights about the DSL task. First, they found out that BM25 weighting, which was previously applied to native language identification (NLI) (Wang et al., 2016), worked better than using TF.IDF. They further highlighted the similarity between similar language identification and NLI as evidenced by a number of entries in the DSL task that are adaptations of systems used for NLI (Goutte et al., 2013; Gebre et al., 2013; Jarvis et al., 2013). We observe that the variation in performance among the top ten teams is less than four percentage points. The team ranked last (eleventh) approached the task using LSTM and achieved an F1 score of 0.202. Unfortunately, they did not submit a system description paper, and thus we do not have much detail about their system. However, in the DSL 2016 task (Malmasi et al., 2016), neural network-based approaches already proved not to be very competitive for the task. See (Medvedeva et al., 2017) for a comparison between the performance of an SVM and an RNN approach for the DSL task. 2.5 Arabic Dial"
W17-1201,W17-1219,0,0.132166,"cia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al., 2016). We also saw the number of system submissions to th"
W17-1201,W15-5408,0,0.243101,"Missing"
W17-1201,W16-4820,0,0.425079,"Missing"
W17-1201,W17-1212,0,0.183999,"Missing"
W17-1201,W17-1226,0,0.0868204,"Missing"
W17-1201,L16-1641,1,0.367123,"Missing"
W17-1201,N15-1010,0,0.0146702,"Missing"
W17-1201,W15-3040,0,0.0163982,"ams (1–6), and run 3 used hard voting of three systems: SVM, Logistic Regression, and Na”ive Bayes and character ngrams (2–6) as features. • bayesline: This team participated with a Multinomial Na¨ıve Bayes (MNB) classifier similar to that of Tan et al. (2014), with no special parameter tuning, as this system was initially intended to serve as an intelligent baseline for the task (but now it has matured into a competitive system). In their bestperforming run 1, they relied primarily on character 4-grams as features. The feature sets they used were selected by a search strategy as proposed in (Scarton et al., 2015). • cic ualg: This team submitted three runs. Runs 1 and 2 first predict the language group, and then discriminate between the languages within that group. The first step uses an SVM classifier with a combination of character 3– 5-grams, typed character 3-grams, applying the character n-gram categories introduced by Sapkota et al. (2015), and word unigrams using TF-weighting. The second step uses the same features and different classifiers: SVMs + Multinominal Na¨ıve Bayes (MNB) in run 1, and MNB in run 2 (which works best). Run 3 uses a single MNB classifier to discriminate between all fourte"
W17-1201,D10-1112,1,0.910362,"CN i-vector (as in Run 2) with (ii) an SVM model trained on count bag of characters 2–4-grams, which yielded an F1 of 0.612. This year, we introduced a new dialectal area, which focused on German dialects of Switzerland. Indeed, the German-speaking part of Switzerland is characterized by the widespread use of dialects in everyday communication, and by a large number of different dialects and dialectal areas. There have been two major approaches to Swiss German dialect identification in the literature. The corpus-based approach predicts the dialect of any text fragment extracted from a corpus (Scherrer and Rambow, 2010; Hollenstein and Aepli, 2015). The dialectological approach tries to identify a small set of distinguishing dialectal features, which are then elicited interactively from the user in order to identify his or her dialect (Leemann et al., 2016). In this task, we adopt a corpus-based approach, and we develop a new dataset for this. • deepCybErNet: This team submitted two runs. Run 1 adopted a Bi-LSTM architecture using the lexical features, and achieved an F1 score of 0.208, while run 2 used the i-vector features and achieved an F1 of 0.574. 3.3 Results Table 5 shows the evaluation results for t"
W17-1201,W14-5307,1,0.307051,"Missing"
W17-1201,W15-5411,1,0.900323,"Missing"
W17-1201,L16-1680,0,0.0188911,"Missing"
W17-1201,N12-1052,0,0.0102303,"Missing"
W17-1201,W14-1614,1,0.904663,"Missing"
W17-1201,tiedemann-2012-parallel,1,0.0255189,"LP task: parallel training data. Participants were asked not to use the development data with their gold standard annotation of dependency relations for any training purposes. The purpose of the development datasets is entirely for testing model performance during system development. All the knowledge used for parsing should origin in the provided source language data. Other sources (except for target language sources) could also be used in unconstrained submissions, but none of the participants chose that option. For the constrained setup, we also provided parallel datasets coming from OPUS (Tiedemann, 2012) that could be used for training cross-lingual parsers in any way. The datasets included translated movie subtitles and contained quite a bit of noise in terms of alignment, encoding, and translation quality. They were also from a very different domain, which made the setup quite realistic considering that one would used whatever could be found for the task. The sizes of the parallel datasets are given in Table 8. In the setup of the shared task, we also provided simple baselines and an “upper bound” of a model trained on annotated target language data. The cross-lingual baselines included del"
W17-1201,C14-1175,1,0.927054,"nd without any optimization of the hyper parameters. The size of the source language data is given in Table 5. We can see that for Czech we have by far the largest corpus, which will also be reflected in the results we obtain. VarDial 2017 featured for the first time a crosslingual parsing task for closely related languages.7 Transfer learning and annotation projection are popular approaches in this field and various techniques and models have been proposed in the literature in particular in connection with dependency parsing (Hwa et al., 2005; McDonald et al., 2013; T¨ackstr¨om et al., 2012; Tiedemann, 2014). The motivation for cross-lingual models is the attempt to bootstrap tools for languages that do not have annotated resources, which are typically necessary for supervised data-driven techniques, using data and resources from other languages. This is especially successful for closely related languages with similar syntactic structures and strong lexical overlap (Agi´c et al., 2012). With this background, it is a natural extension for our shared task to consider cross-lingual parsing as well. We do so by simulating the resource-poor situation by selecting language pairs from the Universal Depe"
W17-1201,W15-2137,1,0.514813,"ad of around 0.7. 4.5 Summary This first edition of the GDI task was a success, given the short time between the 2016 and 2017 editions. In the future, we would like to better control transcriber effects, either by a more thorough selection of training and test data, or by adding transcriber-independent features such as acoustic features, as has been done in the ADI task this year. Further dialectal areas could also be added. 10 5 Cross-lingual Dependency Parsing (CLP) Avoiding gold labels is important here in order to avoid exaggerated results that blur the picture of a more realistic setup (Tiedemann, 2015). The tagger models are trained on the original target language treebanks using UDpipe (Straka et al., 2016) with standard settings and without any optimization of the hyper parameters. The size of the source language data is given in Table 5. We can see that for Czech we have by far the largest corpus, which will also be reflected in the results we obtain. VarDial 2017 featured for the first time a crosslingual parsing task for closely related languages.7 Transfer learning and annotation projection are popular approaches in this field and various techniques and models have been proposed in th"
W17-1201,W17-1216,1,0.921592,"shop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and G"
W17-1210,W14-5309,0,0.0694165,"Missing"
W17-1210,P15-2044,0,0.0122799,"fferent extent, depending on both the dialect region and the impact of the respective umbrella language. In order to get an idea of the similarities and differences of the Slavic languages involved, compare the different versions of John 1:1 in Figure 1. 3 rate projection algorithms (Das and Petrov, 2011; Duong et al., 2013), the integration of external lexicon sources (Li et al., 2012; Täckström et al., 2013), the extension from part-of-speech tagging to full morphological tagging (Buys and Botha, 2016), and the investigation of truly low-resource settings by resorting to Bible translations (Agić et al., 2015). A related approach (Aepli et al., 2014) uses majority voting to disambiguate tags proposed by several source languages. However, these projection approaches are not adapted to our setting as no parallel corpora – not even the Bible2 – are electronically available for Rusyn. Another approach consists in training a model for one language and applying it to another, closely related language. In this process, the model is trained not to focus on the exact shape of the words, but on more generic, language-independent cues, such as part-of-speech tags for parsing (Zeman and Resnik, 2008), or word"
W17-1210,J92-4003,0,0.390927,"Missing"
W17-1210,P16-1184,0,0.0196877,"s sense, because the Rusyn dialect continuum features both West Slavic and East Slavic linguistic traits to a different extent, depending on both the dialect region and the impact of the respective umbrella language. In order to get an idea of the similarities and differences of the Slavic languages involved, compare the different versions of John 1:1 in Figure 1. 3 rate projection algorithms (Das and Petrov, 2011; Duong et al., 2013), the integration of external lexicon sources (Li et al., 2012; Täckström et al., 2013), the extension from part-of-speech tagging to full morphological tagging (Buys and Botha, 2016), and the investigation of truly low-resource settings by resorting to Bible translations (Agić et al., 2015). A related approach (Aepli et al., 2014) uses majority voting to disambiguate tags proposed by several source languages. However, these projection approaches are not adapted to our setting as no parallel corpora – not even the Bible2 – are electronically available for Rusyn. Another approach consists in training a model for one language and applying it to another, closely related language. In this process, the model is trained not to focus on the exact shape of the words, but on more g"
W17-1210,P11-1061,0,0.0348326,"eral related languages – namely, the East Slavic languages Ukrainian and Russian and the West Slavic languages Polish and Slovak – and combining and adapting them to Rusyn. This multi-source setting makes sense, because the Rusyn dialect continuum features both West Slavic and East Slavic linguistic traits to a different extent, depending on both the dialect region and the impact of the respective umbrella language. In order to get an idea of the similarities and differences of the Slavic languages involved, compare the different versions of John 1:1 in Figure 1. 3 rate projection algorithms (Das and Petrov, 2011; Duong et al., 2013), the integration of external lexicon sources (Li et al., 2012; Täckström et al., 2013), the extension from part-of-speech tagging to full morphological tagging (Buys and Botha, 2016), and the investigation of truly low-resource settings by resorting to Bible translations (Agić et al., 2015). A related approach (Aepli et al., 2014) uses majority voting to disambiguate tags proposed by several source languages. However, these projection approaches are not adapted to our setting as no parallel corpora – not even the Bible2 – are electronically available for Rusyn. Another ap"
W17-1210,P13-2112,0,0.0179125,"– namely, the East Slavic languages Ukrainian and Russian and the West Slavic languages Polish and Slovak – and combining and adapting them to Rusyn. This multi-source setting makes sense, because the Rusyn dialect continuum features both West Slavic and East Slavic linguistic traits to a different extent, depending on both the dialect region and the impact of the respective umbrella language. In order to get an idea of the similarities and differences of the Slavic languages involved, compare the different versions of John 1:1 in Figure 1. 3 rate projection algorithms (Das and Petrov, 2011; Duong et al., 2013), the integration of external lexicon sources (Li et al., 2012; Täckström et al., 2013), the extension from part-of-speech tagging to full morphological tagging (Buys and Botha, 2016), and the investigation of truly low-resource settings by resorting to Bible translations (Agić et al., 2015). A related approach (Aepli et al., 2014) uses majority voting to disambiguate tags proposed by several source languages. However, these projection approaches are not adapted to our setting as no parallel corpora – not even the Bible2 – are electronically available for Rusyn. Another approach consists in tr"
W17-1210,feldman-etal-2006-cross,0,0.103367,"Missing"
W17-1210,N13-1039,0,0.0607235,"Missing"
W17-1210,I05-1075,0,0.0292168,"ted work The task of creating taggers for languages lacking manually annotated training corpora has inspired a lot of recent research. The most popular line of work, initiated by Yarowsky and Ngai (2001), draws on parallel corpora. They annotate the source side of a parallel corpus with an existing tagger, and then project the tags along the word alignment links onto the target side of the parallel corpus. A new tagger is then trained on the target side, with some smoothing to reduce the noise caused by alignment errors. Follow-up work has focused on the inclusion of several source languages (Fossum and Abney, 2005), more accu4 Training data While morphosyntactically annotated corpora exist for all four source languages, e.g. in the form 2 We did not find any Rusyn material in the sources given by Christodouloupoulos and Steedman (2015) and Mayer and Cysouw (2014). The sentence cited in Figure 1 has been taken from the printed edition Krajnjak and Kudzej (transl.) (2009). 85 ID Origin PL RU1 RU2 SK UD 1.4 Polish train / dev UD 1.4 Russian train / dev UD 1.4 SynTagRus train / dev UD 1.4 Slovak train / dev UD 1.4 Ukrainian train / dev+test Additional data UK Training data Sentences Tokens Test data Tags Se"
W17-1210,P14-2095,0,0.0202385,"2014) uses majority voting to disambiguate tags proposed by several source languages. However, these projection approaches are not adapted to our setting as no parallel corpora – not even the Bible2 – are electronically available for Rusyn. Another approach consists in training a model for one language and applying it to another, closely related language. In this process, the model is trained not to focus on the exact shape of the words, but on more generic, language-independent cues, such as part-of-speech tags for parsing (Zeman and Resnik, 2008), or word clusters for partof-speech tagging (Kozhevnikov and Titov, 2014). A related idea consists in translating the words of the model to the target language, either using a hand-written morphological analyzer and a list of cognate word pairs (Feldman et al., 2006), or using bilingual dictionaries extracted from parallel corpora (Zeman and Resnik, 2008) or induced from monolingual corpora (Scherrer, 2014). Our work mostly follows the second approach: we train taggers on four resource-rich Slavic languages and adapt them to Rusyn using a variety of techniques. Related work The task of creating taggers for languages lacking manually annotated training corpora has i"
W17-1210,W17-1405,1,0.811671,"Missing"
W17-1210,D12-1127,0,0.0258536,"West Slavic languages Polish and Slovak – and combining and adapting them to Rusyn. This multi-source setting makes sense, because the Rusyn dialect continuum features both West Slavic and East Slavic linguistic traits to a different extent, depending on both the dialect region and the impact of the respective umbrella language. In order to get an idea of the similarities and differences of the Slavic languages involved, compare the different versions of John 1:1 in Figure 1. 3 rate projection algorithms (Das and Petrov, 2011; Duong et al., 2013), the integration of external lexicon sources (Li et al., 2012; Täckström et al., 2013), the extension from part-of-speech tagging to full morphological tagging (Buys and Botha, 2016), and the investigation of truly low-resource settings by resorting to Bible translations (Agić et al., 2015). A related approach (Aepli et al., 2014) uses majority voting to disambiguate tags proposed by several source languages. However, these projection approaches are not adapted to our setting as no parallel corpora – not even the Bible2 – are electronically available for Rusyn. Another approach consists in training a model for one language and applying it to another, cl"
W17-1210,W14-5304,1,0.853936,"nguage. In this process, the model is trained not to focus on the exact shape of the words, but on more generic, language-independent cues, such as part-of-speech tags for parsing (Zeman and Resnik, 2008), or word clusters for partof-speech tagging (Kozhevnikov and Titov, 2014). A related idea consists in translating the words of the model to the target language, either using a hand-written morphological analyzer and a list of cognate word pairs (Feldman et al., 2006), or using bilingual dictionaries extracted from parallel corpora (Zeman and Resnik, 2008) or induced from monolingual corpora (Scherrer, 2014). Our work mostly follows the second approach: we train taggers on four resource-rich Slavic languages and adapt them to Rusyn using a variety of techniques. Related work The task of creating taggers for languages lacking manually annotated training corpora has inspired a lot of recent research. The most popular line of work, initiated by Yarowsky and Ngai (2001), draws on parallel corpora. They annotate the source side of a parallel corpus with an existing tagger, and then project the tags along the word alignment links onto the target side of the parallel corpus. A new tagger is then trained"
W17-1210,mayer-cysouw-2014-creating,0,0.0281133,"rce side of a parallel corpus with an existing tagger, and then project the tags along the word alignment links onto the target side of the parallel corpus. A new tagger is then trained on the target side, with some smoothing to reduce the noise caused by alignment errors. Follow-up work has focused on the inclusion of several source languages (Fossum and Abney, 2005), more accu4 Training data While morphosyntactically annotated corpora exist for all four source languages, e.g. in the form 2 We did not find any Rusyn material in the sources given by Christodouloupoulos and Steedman (2015) and Mayer and Cysouw (2014). The sentence cited in Figure 1 has been taken from the printed edition Krajnjak and Kudzej (transl.) (2009). 85 ID Origin PL RU1 RU2 SK UD 1.4 Polish train / dev UD 1.4 Russian train / dev UD 1.4 SynTagRus train / dev UD 1.4 Slovak train / dev UD 1.4 Ukrainian train / dev+test Additional data UK Training data Sentences Tokens Test data Tags Sentences Tokens Tags 6 800 69 499 920 4 029 79 772 704 48 171 850 689 580 8 483 80 575 657 200 1 281 1 040 3 962 70 299 RUE1 Manually annotated gold standard RUE2 Corpus of Spoken Rusyn 700 6 887 502 10 044 6 250 109 694 1 060 12 440 55 395 104 5 922 1 0"
W17-1210,Q13-1001,0,0.020777,"uages Polish and Slovak – and combining and adapting them to Rusyn. This multi-source setting makes sense, because the Rusyn dialect continuum features both West Slavic and East Slavic linguistic traits to a different extent, depending on both the dialect region and the impact of the respective umbrella language. In order to get an idea of the similarities and differences of the Slavic languages involved, compare the different versions of John 1:1 in Figure 1. 3 rate projection algorithms (Das and Petrov, 2011; Duong et al., 2013), the integration of external lexicon sources (Li et al., 2012; Täckström et al., 2013), the extension from part-of-speech tagging to full morphological tagging (Buys and Botha, 2016), and the investigation of truly low-resource settings by resorting to Bible translations (Agić et al., 2015). A related approach (Aepli et al., 2014) uses majority voting to disambiguate tags proposed by several source languages. However, these projection approaches are not adapted to our setting as no parallel corpora – not even the Bible2 – are electronically available for Rusyn. Another approach consists in training a model for one language and applying it to another, closely related language. I"
W17-1210,D13-1032,0,0.0332804,"he derived lexicons discussed in Section 5.5). We evaluate our methods on a small handannotated sample of Rusyn containing 104 sentences and 1 050 tokens and 96 distinct tags (henceforth RUE1). At the time of conducting the experiments, the Corpus of Spoken Rusyn (RUE2), which we aim to annotate with the presented methods, contains 5 922 sentences with 75 201 tokens. We also report OOV rates on the latter and use it as additional unlabeled data for some of the adaptation processes described below. 5 Experiments 5.1 The MarMoT tagger We use the MarMoT tagger for all of our experiments. MarMoT (Mueller et al., 2013) is a stateof-the-art toolkit for morphological tagging based on Conditional Random Fields (CRFs). It has been shown to work well on full morphological tagging with hundreds of tags (as opposed to part-ofspeech tagging, which typically only uses a few dozen tags), thanks to pruning and coarse-to-fine decoding. Unless stated otherwise, we use the default parameters for morphological tagging. We evaluate the different models on the development sets of the five source corpora as well as on RUE1. A token is considered correctly tagged if its part-of-speech tag is correct and if all morphological f"
W17-1210,W14-4901,0,0.0685209,"Missing"
W17-1210,N01-1026,0,0.149968,"language, either using a hand-written morphological analyzer and a list of cognate word pairs (Feldman et al., 2006), or using bilingual dictionaries extracted from parallel corpora (Zeman and Resnik, 2008) or induced from monolingual corpora (Scherrer, 2014). Our work mostly follows the second approach: we train taggers on four resource-rich Slavic languages and adapt them to Rusyn using a variety of techniques. Related work The task of creating taggers for languages lacking manually annotated training corpora has inspired a lot of recent research. The most popular line of work, initiated by Yarowsky and Ngai (2001), draws on parallel corpora. They annotate the source side of a parallel corpus with an existing tagger, and then project the tags along the word alignment links onto the target side of the parallel corpus. A new tagger is then trained on the target side, with some smoothing to reduce the noise caused by alignment errors. Follow-up work has focused on the inclusion of several source languages (Fossum and Abney, 2005), more accu4 Training data While morphosyntactically annotated corpora exist for all four source languages, e.g. in the form 2 We did not find any Rusyn material in the sources giv"
W17-1210,I08-3008,0,0.0417014,"translations (Agić et al., 2015). A related approach (Aepli et al., 2014) uses majority voting to disambiguate tags proposed by several source languages. However, these projection approaches are not adapted to our setting as no parallel corpora – not even the Bible2 – are electronically available for Rusyn. Another approach consists in training a model for one language and applying it to another, closely related language. In this process, the model is trained not to focus on the exact shape of the words, but on more generic, language-independent cues, such as part-of-speech tags for parsing (Zeman and Resnik, 2008), or word clusters for partof-speech tagging (Kozhevnikov and Titov, 2014). A related idea consists in translating the words of the model to the target language, either using a hand-written morphological analyzer and a list of cognate word pairs (Feldman et al., 2006), or using bilingual dictionaries extracted from parallel corpora (Zeman and Resnik, 2008) or induced from monolingual corpora (Scherrer, 2014). Our work mostly follows the second approach: we train taggers on four resource-rich Slavic languages and adapt them to Rusyn using a variety of techniques. Related work The task of creati"
W17-1405,W17-1210,1,0.811671,"Missing"
W17-1405,E09-2008,0,0.0601141,"Missing"
W17-1405,N01-1020,0,0.0484437,"ribed speech data. It goes without saying that the different types of variation present in our data significantly complicate the task of developing NLP resources. 3 Lexicon Induction We propose to build a morphosyntactic dictionary for Rusyn, using existing resources from etymologically related languages. The idea is that if we know that a Rusyn word X corresponds to the Ukrainian word Y , and that Y is linked to the morphosyntactic descriptions M1 , M2 , Mn , we can create an entry in the Rusyn dictionary consisting of X and M1 , M2 , Mn . The proposed approach is inspired by earlier work by Mann and Yarowsky (2001), who aim to detect cognate word pairs in order to induce a translation lexicon. They evaluate different measures of phonetic or graphemic distance on this task. While they show that distance measures adapted to the language pair by machine learning work best, we are not able to use them as we do not have the required bilingual training corpus at our disposal. Scherrer and Sagot (2014) use such distance measures as a first step of a pipeline for transferring morphosyntactic annotations from a resourced language (RL) towards an etymologically related non-resourced language (NRL). Due to the hig"
W17-1405,scherrer-sagot-2014-language,1,0.910201,"Missing"
W17-1405,sharoff-etal-2008-designing,0,0.0390785,"nian NLP resources exist, e.g., the Ukrainian National Corpus.4 However, these resources cannot easily be used to train taggers or parsers. UGtag (Kotsyba et al., 2011) is a tagger specifically developed for Ukrainian; it is essentially a morphological dictionary with a simple disambiguation component. Its underlying dictionary is rather large and can be easily converted to text format, making it a good addition to the small MULTEXTEast Ukrainian dictionary. For Russian, we complemented the small MULTEXT-East dictionary with the TnT lexicon file based on data from the Russian National Corpus (Sharoff et al., 2008). We also harmonized the MSD tags (morphosyntactic descriptions) across all languages and data 4 www.mova.info sources. Table 1 sums up the used resources. Our NRL data consist of 10 361 unique tokens extracted from the Corpus of Spoken Rusyn (which currently contains a total of 75 000 running words). In addition, we were able to obtain a small sample of morphosyntactically annotated Rusyn, amounting to 1 047 tokens; the induction methods are evaluated on this sample. 3.2 Exact Matches As a baseline, we checked how many Rusyn word forms could be retrieved by exact match in the four RL lexicons"
W17-4733,W11-2123,0,0.0291972,", hyphen retokenization (HR), direction (forward or backward). The best result was submitted for manual evaluation, where it ranked #1 (tied with one unconstrained system). tical MT. Both techniques are popular in neural MT but their impact on statistical MT has not been evaluated properly before. Therefore, we started a systematic comparison of different setups including various types of segmentations and data collections. All systems are based on Moses (Koehn et al., 2007) and we use standard configurations for training non-factored phrase-based SMT models using KenLM for language modeling (Heafield, 2011) and BLEU-based MERT for tuning. The only difference to the standard pipeline is ¨ the use of efmaral (Ostling and Tiedemann, 2016), an efficient implementation of fertility-based IBM word alignment models with a Bayesian extension and Gibbs sampling.3 Table 5 summarizes the results of our SMT experiments during development. The first observation is that BPE (and also supervised morphological segmentation) is not very helpful. This is somewhat surprising as we expect a similar problem as with neural MT in the sense that the productive and rich morphology in Finnish causes problems due to data"
W17-4733,P07-2045,0,0.00799708,"17 WMT+back opus+osm Table 4: Submitted HNMT systems with official results. They vary with respect to decoder type, input normalization (IN), hyphen retokenization (HR), direction (forward or backward). The best result was submitted for manual evaluation, where it ranked #1 (tied with one unconstrained system). tical MT. Both techniques are popular in neural MT but their impact on statistical MT has not been evaluated properly before. Therefore, we started a systematic comparison of different setups including various types of segmentations and data collections. All systems are based on Moses (Koehn et al., 2007) and we use standard configurations for training non-factored phrase-based SMT models using KenLM for language modeling (Heafield, 2011) and BLEU-based MERT for tuning. The only difference to the standard pipeline is ¨ the use of efmaral (Ostling and Tiedemann, 2016), an efficient implementation of fertility-based IBM word alignment models with a Bayesian extension and Gibbs sampling.3 Table 5 summarizes the results of our SMT experiments during development. The first observation is that BPE (and also supervised morphological segmentation) is not very helpful. This is somewhat surprising as we"
W17-4733,P16-1100,0,0.062073,"Missing"
W17-4733,C16-1172,0,0.0398962,"Missing"
W17-4733,P10-2016,0,0.0239383,"Missing"
W17-4733,E06-1032,0,0.0313954,"and a standard phrase-based SMT model to translate parts of 2014-2016 news data. The statistics of the backtranslations are given in Table 6. sional translator. The impression of the reviewer was that the perceived quality of NMT far exceeds that of SMT, mainly due to the superior fluency of NMT. The BLEU scores of the systems also indicate a significant quality difference in favor of NMT. However, single-reference BLEU scores are known to be unreliable indicators of quality for morphologically complex languages (Bojar et al., 2010), and they are also known to favor SMT over other MT methods (Callison-Burch et al., 2006). Due to this, it is possible that the BLEU scores, impressive as they are, do not reflect the real qualitative impact of NMT for English–Finnish MT. To explore whether single-reference evaluation underestimates NMT quality, a sample of 68 sentences was extracted from the test set. Both SMT and NMT translations of the sample were postedited with minimal changes to the same quality level as the reference translation. The minimally edited MT was then used as a TER reference to obtain a more reliable estimate of the MT quality. The sample was chosen from sentences where SMT has a sentence-level T"
W17-4733,W16-2322,0,0.0179058,"on the development set, particularly early during training, the BLEU and chrF3 evaluations on development data made us decide against the slower context gates in the final run. 2.4 Ensembling HNMT supports two general modes of ensembling, as well as their combination: Gal and Ghahramani (2016) proposed a method for regularization of recurrent neural networks. This has also been implemented in HNMT, but preliminary experiments on Finnish did not indicate any improvement over the baseline system. While Sennrich et al. (2016a) reported large improvements for the Romanian news translation task at WMT 2016, the amount of training data is lower than what is available for Finnish, which should explain some of the difference. They also apply dropout on the word level, whereas the HNMT application currently only drops recurrent states. 2.3 Forward-Backward reranking 3 English–Finnish In our experiments, we used all English–Finnish parallel data sets provided by WMT except the Wiki headlines, which is a small and rather noisy data set that did not contribute anything in our experiments from last year. We also added substantial amounts of backtranslated data that has Coverage decoder Wu et al. (2016)"
W17-4733,W16-2323,0,0.100318,"ance to tune the balance between adequacy and fluency. While we obtained better cross-entropy on the development set, particularly early during training, the BLEU and chrF3 evaluations on development data made us decide against the slower context gates in the final run. 2.4 Ensembling HNMT supports two general modes of ensembling, as well as their combination: Gal and Ghahramani (2016) proposed a method for regularization of recurrent neural networks. This has also been implemented in HNMT, but preliminary experiments on Finnish did not indicate any improvement over the baseline system. While Sennrich et al. (2016a) reported large improvements for the Romanian news translation task at WMT 2016, the amount of training data is lower than what is available for Finnish, which should explain some of the difference. They also apply dropout on the word level, whereas the HNMT application currently only drops recurrent states. 2.3 Forward-Backward reranking 3 English–Finnish In our experiments, we used all English–Finnish parallel data sets provided by WMT except the Wiki headlines, which is a small and rather noisy data set that did not contribute anything in our experiments from last year. We also added subs"
W17-4733,P16-1009,0,0.106442,"ance to tune the balance between adequacy and fluency. While we obtained better cross-entropy on the development set, particularly early during training, the BLEU and chrF3 evaluations on development data made us decide against the slower context gates in the final run. 2.4 Ensembling HNMT supports two general modes of ensembling, as well as their combination: Gal and Ghahramani (2016) proposed a method for regularization of recurrent neural networks. This has also been implemented in HNMT, but preliminary experiments on Finnish did not indicate any improvement over the baseline system. While Sennrich et al. (2016a) reported large improvements for the Romanian news translation task at WMT 2016, the amount of training data is lower than what is available for Finnish, which should explain some of the difference. They also apply dropout on the word level, whereas the HNMT application currently only drops recurrent states. 2.3 Forward-Backward reranking 3 English–Finnish In our experiments, we used all English–Finnish parallel data sets provided by WMT except the Wiki headlines, which is a small and rather noisy data set that did not contribute anything in our experiments from last year. We also added subs"
W17-4733,P16-1162,0,0.42351,"ance to tune the balance between adequacy and fluency. While we obtained better cross-entropy on the development set, particularly early during training, the BLEU and chrF3 evaluations on development data made us decide against the slower context gates in the final run. 2.4 Ensembling HNMT supports two general modes of ensembling, as well as their combination: Gal and Ghahramani (2016) proposed a method for regularization of recurrent neural networks. This has also been implemented in HNMT, but preliminary experiments on Finnish did not indicate any improvement over the baseline system. While Sennrich et al. (2016a) reported large improvements for the Romanian news translation task at WMT 2016, the amount of training data is lower than what is available for Finnish, which should explain some of the difference. They also apply dropout on the word level, whereas the HNMT application currently only drops recurrent states. 2.3 Forward-Backward reranking 3 English–Finnish In our experiments, we used all English–Finnish parallel data sets provided by WMT except the Wiki headlines, which is a small and rather noisy data set that did not contribute anything in our experiments from last year. We also added subs"
W17-4733,W16-2326,1,0.868028,"Missing"
W17-4733,W15-3021,1,0.848314,"Missing"
W17-4733,Q17-1007,0,0.0227089,"s only works if the different θm are relatively similar, typically because they were saved at different points during the same training process. The overhead for proper ensembling is linear in the number of ensembled systems, both for training (assuming one is building an ensemble of separately trained models) and inference, while parameter averaging is essentially free. HNMT allows proper ensembling of groups of models where the parameters are averaged within each group. This flexible structure allows a number of setups, which are explored further in Section 3.2. Context gates Context gates (Tu et al., 2017) introduce an explicit model for selecting to which extent the target sentence generation should focus on the source sentence or the target context, giving the network a chance to tune the balance between adequacy and fluency. While we obtained better cross-entropy on the development set, particularly early during training, the BLEU and chrF3 evaluations on development data made us decide against the slower context gates in the final run. 2.4 Ensembling HNMT supports two general modes of ensembling, as well as their combination: Gal and Ghahramani (2016) proposed a method for regularization of"
W17-4733,J11-1005,0,0.0160451,"oints at 6, 7.5 and 11.5 days. Translating Chinese into English Chinese is a language without word boundaries, so word segmentation is necessary before using our hybrid encoder with Chinese source sentences. There are different segmentation methods at different granularities, and they will lead to different translations. In the work of Su et al. (2017), they proposed a lattice-based recurrent encoder which applied three segmentations at different granularities (from the CTB, PKU and MSRA corpora). In our model, we just tried two segmentations: One is a fine-grained method implemented in Zpar (Zhang and Clark, 2011), the other is a coarse-granularity method by THULAC (Sun et al., 2016). The model with THULAC segmen6 Conclusions This paper introduces the Helsinki Neural Machine Translation system (HNMT) and its succesful application to the news translation task in WMT 2017. The models we trained handle well the translation into morphologically complex lan345 guages such as Finnish and our submission scored best among the participants in the English–Finnish task. The evaluations show that the neural models are superior to the strong SMT baselines that exploit the same tricks such as backtranslated data and"
W17-4811,P13-2068,0,0.0608001,"Missing"
W17-4811,W09-2404,0,0.337422,"Missing"
W17-4811,W12-3156,0,0.0476943,"context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases. 1 Introduction Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hard"
W17-4811,W10-1737,0,0.0691336,"tion consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Modest improvements could, for example, be shown for the translation of pronouns (Le Nagard and Koehn, 2010; Hardmeier and Fed2 Attention-Based NMT Encoder-decoder models with attention have been proposed by Bahdanau et al. (2014) and have become the de-facto standard in neural machine translation. The model is based on recurrent neural network layers that encode a given sentence in the source language into a distributed vector representation that will be decoded into the target language by another recurrent network. The attention 82 Proceedings of the Third Workshop on Discourse in Machine Translation, pages 82–92, c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Lingui"
W17-4811,L16-1147,1,0.8448,"ed to other genres such as newspaper texts or other edited written material. Utterances are even shortened substantially for space limitations. This property supports our experiments in which we want to include context beyond sentence boundaries. Similar to statistical MT, neural MT also struggles most with long sequences and, therefore, it is important to keep the segments short. On average there are about 8 tokens per language in each aligned translation unit (which may cover one or more sentences or sentence fragments). In particular, we use the publicly available OpenSubtitles2016 corpus (Lison and Tiedemann, 2016) for German and English2 and reserve 400 randomly selected movies for development and testing purposes. In total, there are 16,910 movies and TV series in the collection. We tokenized and truecased the data sets using standard tools from the Moses toolbox (Koehn et al., 2007). The final corpus comprises 13.9 million translation units with about 107 million tokens in German and 115 million tokens in English. The training data includes 13.5 million training instances and we selected the 5,000 first translation units of the test set for automatic evaluation. Note that we trust the alignment and d"
W17-4811,E12-3001,0,0.0786392,"s that improve textual coherence in translation at least in some selected cases. 1 Introduction Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Modest improvements could, for example, be shown for the translation of pronouns (Le Nagard and Koehn, 2010; Hardmeier and Fed2 Attention-Based NMT Encoder-decoder models with attention have been proposed by Bahdanau et al. (2014) and h"
W17-4811,loaiciga-etal-2014-english,0,0.0176773,"tences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Modest improvements could, for example, be shown for the translation of pronouns (Le Nagard and Koehn, 2010; Hardmeier and Fed2 Attention-Based NMT Encoder-decoder models with attention have been proposed by Bahdanau et al. (2014) and have become the de-facto standard in neural machine translation. The model is based on recurrent neural network layers that encode a given sentence in the"
W17-4811,2010.iwslt-papers.10,0,0.54578,"Missing"
W17-4811,P16-1100,0,0.0245296,"out limiting the capacity of the internal representation. Previous work has shown that NMT models can successfully learn attention distributions that explain intuitively plausible connections between source and target language. This framework is very well suited for the study we conduct in this paper as we emphasise the capabilities of NMT to pick up contextual dependencies from wider context across sentence boundaries. In our work, we rely on the freely avail¨ able Helsinki NMT system (HNMT) (Ostling 1 et al., 2017) that implements a hybrid bidirectional encoder with character-level backoff (Luong and Manning, 2016) using recurrent LSTM units (Hochreiter and Schmidhuber, 1997). The system also features layer normalisation (Ba et al., 2016), variational dropout (Gal and Ghahramani, 2016), coverage penalties (Wu et al., 2016), beam search decoding and straightforward model ensembling. The backbone is Theano, which enables efficient GPU-based training and decoding with mini-batches. 3 least helps to incorporate more knowledge about the situation and in consequence leads to better translations, also stylistically. The final advantage of subtitles is the size of the translation units. Sentences (and sentence"
W17-4811,P13-4033,1,0.867193,"ual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases. 1 Introduction Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Mo"
W17-4811,P11-1124,0,0.0626374,"Missing"
W17-4811,D13-1037,1,0.875891,"ual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases. 1 Introduction Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Mo"
W17-4811,2012.amta-papers.20,0,0.0668273,"Missing"
W17-4811,W17-4733,1,0.86296,"Missing"
W17-4811,E12-3010,0,0.183068,"Missing"
W17-4811,P16-1162,0,0.318378,"in learning the influence of specific context word sequences on the translation of the focus sentence. An example is the ambiguous pronoun “sie” that could be a feminine singular or a plural third person pronoun. The use of grammatical gender in German also makes it possible to refer to an inanimate antecedent. Discourse-level information is needed to make correct decisions. The question is whether our model can actually pick this up and whether attention patterns can show the relevant connections. The NMT models that we train rely on subwordunits. We apply standard byte-pair encoding (BPE) (Sennrich et al., 2016) for splitting words into segments. For the extended source context models, we set a vocabulary size of 30,000 when training BPE codes and apply a vocabulary size of 60,000 when training the models (context words double the vocabulary because of their cc prefix). For the 2+2 model, we train BPE codes from both languages together (with a size of 60,000) and we set a vocabulary threshold of 50 when applying BPE to the data. 5 Experiments and Results We train attention-based models using the Helsinki NMT system with similar parameters but different training data to see the effect of contextual in"
W17-4811,W10-2602,1,0.922449,"Missing"
W17-4811,N12-1046,0,0.102001,"Missing"
W17-4811,W12-2503,0,0.0453393,"translation at least in some selected cases. 1 Introduction Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Modest improvements could, for example, be shown for the translation of pronouns (Le Nagard and Koehn, 2010; Hardmeier and Fed2 Attention-Based NMT Encoder-decoder models with attention have been proposed by Bahdanau et al. (2014) and have become the de-facto standard in neural mac"
W17-4811,D13-1163,0,0.0812776,"ome selected cases. 1 Introduction Typical models of machine translation handle sentences in isolation and discard any information beyond sentence boundaries. Efforts in making statistical MT aware of discourse-level phenomena appeared to be difficult (Hardmeier, 2012; Carpuat and Simard, 2012; Hardmeier et al., 2013a). Various studies have been published that consider textual coherence, document-wide translation consistency, the proper handling of referential elements such as pronominal anaphora and other discourse-level phenomena (Guillou, 2012; Russo et al., 2012; Voigt and Jurafsky, 2012; Xiong et al., 2013a; Ben et al., 2013; Xiong and Zhang, 2013; Xiong et al., 2013b; Loaiciga et al., 2014). The typical approach in the literature focuses on the development of task-specific components that are often tested as standalone modules that need to be integrated with MT decoders (Hardmeier et al., 2013b). Modest improvements could, for example, be shown for the translation of pronouns (Le Nagard and Koehn, 2010; Hardmeier and Fed2 Attention-Based NMT Encoder-decoder models with attention have been proposed by Bahdanau et al. (2014) and have become the de-facto standard in neural machine translation. Th"
W17-4811,P07-2045,0,\N,Missing
W18-3901,W18-3913,0,0.217545,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3919,0,0.254393,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3932,0,0.129693,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W17-1223,0,0.131933,"data to reduce the transcriber effects seen last year. 7 • The LaMa system is a blend (weighted vote) of eight classifiers being stochastic gradient descent (hinge and modified Huber), multinomial Na¨ıve Bayes, both counts and tf-idf, FastText, and modified Kneser-Ney smoothing. The classifiers were trained using word n-grams (1-6) and character n-grams (1-8). The hyperparameters were determined with cross-validation and searching on the development set. • XAC system is a refined version of the n-gram-based Bayesline system described in last year’s XAC submission to the VarDial shared tasks (Barbaresi, 2017), and previously used as a baseline for the DSL shared task (Tan et al., 2014). The XAC team achieved their best results using a Na¨ıve Bayes classifier. • The GDI classification system is based on an ensemble of multiple SVM classifiers. The system was trained on various word- and character-level features. • The dkosmajac system is based on a normalized Euclidean distance measure. The distances are calculated between a sample and each class profile. The class profiles are generated by selecting the most frequent features for each class, which results in profiles that are of the same length fo"
W18-3901,W18-3918,0,0.0590619,"Missing"
W18-3901,W18-3925,0,0.142653,"Missing"
W18-3901,W17-1214,0,0.138486,"was part of the first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions gen"
W18-3901,W18-3909,0,0.15091,"Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive evaluation exercise with four shared tasks in 2017. This year, the VarDial workshop featured the second edition of the VarDial evaluation campaign w"
W18-3901,W18-3933,1,0.889149,"Missing"
W18-3901,W18-3920,1,0.880795,"Missing"
W18-3901,W18-3926,0,0.0553879,"Missing"
W18-3901,W18-3921,0,0.054375,"Missing"
W18-3901,W17-1225,0,0.134424,"r of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layers, which represents the prob"
W18-3901,W16-4818,0,0.048944,", duration (Dur.), in number of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layer"
W18-3901,W18-3915,0,0.0733204,"Missing"
W18-3901,W18-3929,0,0.411409,"Missing"
W18-3901,W18-3907,0,0.178349,"Missing"
W18-3901,W18-3922,0,0.0589219,"Missing"
W18-3901,W18-3928,0,0.0556119,"Missing"
W18-3901,kumar-2012-challenges,1,0.824307,"ed printed stories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Osl"
W18-3901,kumar-2014-developing,1,0.832345,"ories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Oslo team submit"
W18-3901,W14-0405,1,0.912975,"Missing"
W18-3901,W18-3917,1,0.897664,"Missing"
W18-3901,L16-1242,1,0.902855,"Missing"
W18-3901,L16-1676,1,0.914666,"Missing"
W18-3901,W16-4814,1,0.86288,"-Oslo system (C ¸ o¨ ltekin et al., 2018) is trained on word and character n-grams using a single SVM classifier, which is fine-tuned using cross-validation. It is similar to the submissions by the same authors to previous VarDial shared tasks (C¸o¨ ltekin and Rama, 2017; C¸o¨ ltekin and Rama, 2016). They also tried an approach based on RNN, which worked worse. • Arabic Identification system is based on an ensemble of SVM classifiers trained on character and word n-grams. The approach is similar to the systems ranked second and first in the previous two ADI tasks (Malmasi and Zampieri, 2017a; Malmasi and Zampieri, 2016). 5 5.3 Results Six teams submitted runs for the ADI shared task and the results are shown in Table 3. The best result, an F1 score of 0.589, was achieved by UnibucKernel,1 followed by safina, with an F1 score of 0.575. The following three teams are tied for the third place as they are not statistically different. Rank 1 2 3 3 3 4 Team F1 (Macro) UnibucKernel safina BZU SYSTRAN T¨ubingen-Oslo Arabic Identification 0.589 0.576 0.534 0.529 0.514 0.500 Table 3: ADI results: ranked taking statistical significance into account. 5.4 Summary We introduced multi-phoneme representation for the dialecta"
W18-3901,W17-1222,1,0.833986,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W17-1220,1,0.80558,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W16-4801,1,0.733081,"Missing"
W18-3901,W18-3927,0,0.0554913,"Missing"
W18-3901,W18-3914,0,0.175754,"ES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive eva"
W18-3901,W18-3924,0,0.0833746,"and the number of submissions varied widely across the tasks, ranging from 6 entries for ADI and MTT to 12 entries for DFS. Table 1 lists the participating teams, the shared tasks they took part in, and a reference to the system description paper. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participate"
W18-3901,L16-1641,1,0.509642,"Missing"
W18-3901,W17-1224,1,0.744067,"Missing"
W18-3901,W18-3923,1,0.879634,"Missing"
W18-3901,W14-5307,1,0.857122,"Missing"
W18-3901,W15-5401,1,0.855743,"Missing"
W18-3901,W17-1201,1,0.607978,"Missing"
W18-6425,P18-4020,0,0.0395573,"Missing"
W18-6425,P17-4012,0,0.0367393,"primary submissions. While the main focus of our work lay on the English-to-Finnish translation direction, we also participated in the Finnishto-English, English-to-Estonian and Estonian-toEnglish translation directions. In 2017, the University of Helsinki participated in WMT with an in-house implementation of an attentional encoder-decoder architecture based on the ¨ Theano framework, called HNMT (Ostling et al., 2017). Since then, the development of Theano has stopped, and various open-source Neural Machine Translation (NMT) toolkits based on alternative frameworks have been made available (Klein et al., 2017; Junczys-Dowmunt et al., 2018, inter alia). In parallel, a novel neural network architecture for machine translation, called Transformer, has been introduced (Vaswani et al., 2017). The Transformer follows the encoder-decoder paradigm, but does not use any recurrent layers. Instead, its architecture relies primarily on attention mechanisms, stacking on each layer multiple attention components. Preliminary experiments with the Transformer architecture and its implementation in OpenNMT-py (Klein et al., 2017) showed consistent performance improvements compared to our 2017 architecture. Conseque"
W18-6425,2005.mtsummit-papers.11,0,0.0177992,"son et al., 2016; Tars and Fishel, 2018) we added a domain label to each input sentence, according to the data source. For example, each sentence from the Europarl corpus was prepended with the hEUROPARLi label. The overall idea of domain labelling is that data coming from different sources are of different quality and represent different genres and writing styles. In this way, the translation model can be informed of the data source without increasing the number of parameters. 2 English→Finnish 2.1 NMT models We trained our systems on almost all parallel data made available by WMT: Europarl (Koehn, 2005), ParaCrawl1 , Rapid, as well as the WMT 2015 test and development sets. We did not use WikiHeadlines. For development and tuning of the system 488 1 https://paracrawl.eu/ Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 488–495 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64052 parameters, we used the WMT 2016 and 2017 test sets. A common strategy is to create synthetic training data by back-translation (Sennrich et al., 2016a). For our WMT 2017 submissio"
W18-6425,W17-4733,1,0.930303,"ch to our best neural model, analyzing the output and highlighting future research. 1 Introduction The University of Helsinki participated in the WMT 2018 shared task on news translation with seven primary submissions. While the main focus of our work lay on the English-to-Finnish translation direction, we also participated in the Finnishto-English, English-to-Estonian and Estonian-toEnglish translation directions. In 2017, the University of Helsinki participated in WMT with an in-house implementation of an attentional encoder-decoder architecture based on the ¨ Theano framework, called HNMT (Ostling et al., 2017). Since then, the development of Theano has stopped, and various open-source Neural Machine Translation (NMT) toolkits based on alternative frameworks have been made available (Klein et al., 2017; Junczys-Dowmunt et al., 2018, inter alia). In parallel, a novel neural network architecture for machine translation, called Transformer, has been introduced (Vaswani et al., 2017). The Transformer follows the encoder-decoder paradigm, but does not use any recurrent layers. Instead, its architecture relies primarily on attention mechanisms, stacking on each layer multiple attention components. Prelimi"
W18-6425,P16-1009,0,0.0362795,"data made available by WMT: Europarl (Koehn, 2005), ParaCrawl1 , Rapid, as well as the WMT 2015 test and development sets. We did not use WikiHeadlines. For development and tuning of the system 488 1 https://paracrawl.eu/ Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 488–495 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64052 parameters, we used the WMT 2016 and 2017 test sets. A common strategy is to create synthetic training data by back-translation (Sennrich et al., 2016a). For our WMT 2017 submission, we already used SMT to create 5.5M sentences of back-translated data from the Finnish news2014 and news2016 corpora. This year, we created another 5.5M sentences of back-translation from the Finnish news2014news2017 corpora using our previous NMT system ¨ (Ostling et al., 2017). The final submissions make use of both resources. We applied the standard preprocessing pipeline consisting of tokenization,2 normalization,3 truecasing and byte-pair encoding (Sennrich et al., 2016b). Following Vaswani et al. (2017), we have used a joint BPE vocabulary of 37 000 units."
W18-6425,P16-1162,0,0.0924233,"data made available by WMT: Europarl (Koehn, 2005), ParaCrawl1 , Rapid, as well as the WMT 2015 test and development sets. We did not use WikiHeadlines. For development and tuning of the system 488 1 https://paracrawl.eu/ Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 488–495 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64052 parameters, we used the WMT 2016 and 2017 test sets. A common strategy is to create synthetic training data by back-translation (Sennrich et al., 2016a). For our WMT 2017 submission, we already used SMT to create 5.5M sentences of back-translated data from the Finnish news2014 and news2016 corpora. This year, we created another 5.5M sentences of back-translation from the Finnish news2014news2017 corpora using our previous NMT system ¨ (Ostling et al., 2017). The final submissions make use of both resources. We applied the standard preprocessing pipeline consisting of tokenization,2 normalization,3 truecasing and byte-pair encoding (Sennrich et al., 2016b). Following Vaswani et al. (2017), we have used a joint BPE vocabulary of 37 000 units."
W18-6425,skadins-etal-2014-billions,1,0.88596,"Missing"
W18-6425,W17-4731,1,0.827394,"x than simple concatenation of subword units. First the BPE tags are joined and then the surface forms are generated using the FINTWOL generation functionality, which takes as input lemmas and morphological tags and output all compatible surface forms. The default tags are automatically added for lemmas which do not have explicit tags. Heuristics are used to select a surface form if several possibilities are generated. The submitted model was trained on the bilingual and back-translated data, as adding the backtranslated data greatly improved the quality of the translations. 2.3 Rule-based MT Hurskainen and Tiedemann (2017) propose a rulebased machine translation system for English– Finnish. During the past year, the rule-based MT system has been developed in several ways. In addition to the usual debugging and rule testing, also some major structural changes have been made. Below we will discuss the latter type of problems. Translating locative expressions: While English uses prepositions for marking location, Finnish uses locative cases. English has a bewildering number of prepositions for this purpose. At least the following preposition are used: in, on, at, with, by, to, into, for, of, from, over, through, a"
W18-6425,W17-4704,0,0.0426224,"Missing"
W18-6425,J82-2005,0,0.701555,"Missing"
W18-6425,E12-1015,1,0.798906,"t on each specific language pair. 5.1 for 20 epochs, evaluating each of them on the development set after every epoch, taking the best iteration as final model. As hyper-parameters, we used the base version of the Transformer architecture, following the suggestion of the OpenNMT-py tool,5 including a shared word embedding space between encoder and decoder among others. Unlike last year, we did not include any averaging or ensembling techniques. Synthetic data Another way to take advantage of the close etymological relationship between Estonian and Finnish is to create synthetic training data (Tiedemann, 2012). We explored this option in the following setup: 1. Train a character-level seq2seq system for Finnish-to-Estonian, using the Europarl and EUbookshop (Skadin¸sˇ et al., 2014) corpora. 2. Translate the Finnish side of the parallel English–Finnish corpus to Estonian. 3. Combine the Estonian and English parts of the corpus and use this dataset as backtranslations to train the final system. We were able to process 1.5M sentences using this approach. These sentences complemented the other training data, consisting of parallel data and direct English–Estonian back-translations. 6 Experiments In thi"
W18-6425,W16-2326,1,0.895598,"Missing"
W18-6425,D16-1163,0,0.025825,"l +Back +Synth HY-NMT Multilingual +Label Table 1: Number of training sentences, with and without back-translation (Back) and synthetic data (Synth). Et→En 21.6 20.3 26.5 25.4 26.5* 25.0 – 26.4 En→Et 16.7 17.6 – 21.8* – 21.0 – 20.8 Table 2: BLEU-cased scores on newstest2018 for the English–Estonian language pair in various configurations using domain labels (Label), backtranslated data (Back), or synthetic data (Synth). Our primary submissions are marked with *. two. In this way, even though Estonian has no parallel news data, the model will see the news label in the Finnish data. Inspired by Zoph et al. (2016), we first train the multilingual model with all languages in all directions, and then fine-tune it on each specific language pair. 5.1 for 20 epochs, evaluating each of them on the development set after every epoch, taking the best iteration as final model. As hyper-parameters, we used the base version of the Transformer architecture, following the suggestion of the OpenNMT-py tool,5 including a shared word embedding space between encoder and decoder among others. Unlike last year, we did not include any averaging or ensembling techniques. Synthetic data Another way to take advantage of the c"
W18-6433,W15-1844,0,0.0116739,". We collected sentences containing a coreference link involving a personal pronoun (it) or a relative pronoun (that, which, who, whom, whose). The base sentence remains unchanged. In order to generate the variant, the antecedent noun of the pronoun is then changed to a synonym using WordNet (Miller, 1995): • Czech: MorphoDiTa (Strakov´a et al., 2014) • German: SMOR (Schmid et al., 2004) • Personal pronoun: This cat is cute and I love it. → This dog is cute and I love it. • Finnish: The finnish-analyze-words script7 provided by the Language Bank of Finland8 and based on the Omorfi morphology (Pirinen, 2015) and the HFST toolkit (Lind´en et al., 2011) • Relative pronoun: The woman who left was angry. → The man who left was angry. In the output of the MT system, we are then able to locate the antecedent of the pronoun by looking for the only noun that differs between the base and variant translations (namely, the translation of cat/woman in the base and dog/man in the variant). Finally, we check whether the noun and personal • English: MorphoDiTa (Strakov´a et al., 2014) 7 http://urn.fi/urn:nbn:fi: lb-2018041701 8 https://www.kielipankki.fi/ 549 pronoun bear the same gender.9 We also check number"
W18-6433,W17-4705,1,0.757878,".fi francois.yvon@limsi.fr Abstract about systems performance than just one overall number (even if it correlates well with human judgement). Evaluation metrics that focus on various aspects of the translation, such as syntax or morphology, rather than on general translation quality, have thus seen renewed interest. This interest has spurred the inclusion of additional test suites into the WMT 2018 news translation task. Progress in the quality of machine translation output calls for new automatic evaluation procedures and metrics. In this paper, we extend the Morpheval protocol introduced by Burlot and Yvon (2017) for the English-toCzech and English-to-Latvian translation directions to three additional language pairs, and report its use to analyze the results of WMT 2018’s participants for these language pairs. Considering additional, typologically varied source and target languages also enables us to draw some generalizations regarding this morphology-oriented evaluation procedure. 1 Burlot and Yvon (2017, B&Y in the following) present a test suite for evaluating the morphological competence of machine translation systems. They provide a set of sentence pairs in the source language that differ by one"
W18-6433,schmid-etal-2004-smor,0,0.0605379,"Missing"
W18-6433,P16-1162,0,0.017792,"th feature types, the variants are created through some type of transformation that is supposed to be invariant with respect to target morphology. For the consistency features, this transformation is semantic (based on the hyponymy relation), whereas it is morphological for the stability features. Rare word features In the early days of NMT, translation of out-ofvocabulary words was virtually impossible and hampered the performance when compared with SMT. In recent years however, most systems have adopted an approach in which rare words are split into “subwords” during preprocessing (see e.g. Sennrich et al., 2016), such that any unknown word can be composed of various subword chunks during test time. Several subword chunking algorithms with various parameter settings can be used, but their respective performance differences are hard to assess as they typically concern low-frequency words with low impact on general translation quality. Therefore, we introduce two features that specifically deal with low-frequency items. These features are language-independent and do not require the use of a morphological analyzer. For the first feature, we identify large numbers (at least 3 digits) in the English source"
W18-6433,P05-1045,0,0.0339138,"set of contrasts that can be triggered in the source language and evaluated in the target language; As source corpora, we use the English News2007 and 2008 corpora (for EN-CS and ENDE), the English News-2007 corpus (for ENFI), and SETIMES2 (for TR-EN). In order to detect the source features, the corpora are annotated using TreeTagger (Schmid, 1994) and/or CoreNLP (Manning et al., 2014) (for English), or an Apertium (Forcada et al., 2011) morphological analyser (for Turkish). For the named entities feature used in EN-FI, we additionally annotate the source corpora with the Stanford NER tagger (Finkel et al., 2005). • a procedure to generate contrast pairs from a monolingual source language corpus; • and a procedure to score the target language translations of the contrast pairs. B&Y describe three types of contrasts. Type A contrasts resemble paradigm completion tasks, in which one single morphological feature (number, gender, tense, etc.) is evaluated. The two sentences of a contrast pair only differ in one word (or phrase) and across one feature at a time. Type B contrasts contain somewhat more complicated substitutions that are mainly evaluated in terms of agreement. For example, a contrast pair con"
W18-6433,P14-5003,0,0.0577653,"Missing"
W18-6433,tiedemann-2012-parallel,0,0.0291093,"For instance, the English expression apple juice in the base translates into the German compound Apfelsaft. We modify the word apple and obtain orange juice, which translates into Orangensaft. In the MT output we finally compare both compounds Apfelsaft and Orangensaft and report a success if they have at least one morpheme in common. Here, the common morpheme is -saft. For the test suite generation, we needed a translation dictionary containing compounds on the German side and multi-word expressions on the English side. We gathered all the English-German parallel data we could find on OPUS (Tiedemann, 2012) and removed the data available at the WMT18 News Translation shared task. This resulted in nearly 40M parallel sentences. We obtained a phrase table out of this data using the Moses toolkit (Koehn et al., 2007). We finally extracted from this phrase table a dictionary containing a compound on the German side and several multi-word expressions on the English side (removing punctuation and other noisy tokens). Verb position The test suite is generated by locating complex sentences where (a) the principal clause can be omitted and (b) the subordinate clause leads to a German translation where th"
W18-6433,P07-2045,1,0.0112429,"ut we finally compare both compounds Apfelsaft and Orangensaft and report a success if they have at least one morpheme in common. Here, the common morpheme is -saft. For the test suite generation, we needed a translation dictionary containing compounds on the German side and multi-word expressions on the English side. We gathered all the English-German parallel data we could find on OPUS (Tiedemann, 2012) and removed the data available at the WMT18 News Translation shared task. This resulted in nearly 40M parallel sentences. We obtained a phrase table out of this data using the Moses toolkit (Koehn et al., 2007). We finally extracted from this phrase table a dictionary containing a compound on the German side and several multi-word expressions on the English side (removing punctuation and other noisy tokens). Verb position The test suite is generated by locating complex sentences where (a) the principal clause can be omitted and (b) the subordinate clause leads to a German translation where the verb should be located at the end of the clause. Using CoreNLP annotations, we focus on specific English conjunctions that lead to a verb shift in German, like that → dass, because → weil, etc. In order to gen"
W18-6433,P14-5010,0,0.0375358,"generation: The Morpheval test suites 1. Collect a large number of short sentences (length &lt; 15 words) containing a source feature of interest. A Morpheval test suite according to B&Y consists of three aspects: • the definition of a set of contrasts that can be triggered in the source language and evaluated in the target language; As source corpora, we use the English News2007 and 2008 corpora (for EN-CS and ENDE), the English News-2007 corpus (for ENFI), and SETIMES2 (for TR-EN). In order to detect the source features, the corpora are annotated using TreeTagger (Schmid, 1994) and/or CoreNLP (Manning et al., 2014) (for English), or an Apertium (Forcada et al., 2011) morphological analyser (for Turkish). For the named entities feature used in EN-FI, we additionally annotate the source corpora with the Stanford NER tagger (Finkel et al., 2005). • a procedure to generate contrast pairs from a monolingual source language corpus; • and a procedure to score the target language translations of the contrast pairs. B&Y describe three types of contrasts. Type A contrasts resemble paradigm completion tasks, in which one single morphological feature (number, gender, tense, etc.) is evaluated. The two sentences of"
W19-1401,W19-1402,0,0.268136,"Missing"
W19-1401,P19-1068,1,0.913629,"guages and asked to predict the valid morphological analyses for a seventh, unseen language. In the “Semi-Closed” track, the process was the same, only participants were provided with additional raw data by the organisers. This was in the form of raw text Wikipedia dumps, bilingual dictionaries from the Apertium project and any treebanks available in the known languages from the Universal Dependencies project. Moldavian vs. Romanian Cross-dialect Topic identification (MRC): In the Moldavian vs. Romanian Cross-topic Identification shared task, we provided participants with the MOROCO data set (Butnaru and Ionescu, 2019) which contains Moldavian and Romanian samples of text collected from the news domain. The samples belong to one of the following six topics: culture, finance, politics, science, sports, and tech. The samples are pre-processed in order to eliminate named entities. For each sample, the data set provides corresponding dialectal and category labels. To this end, we proposed three subtasks for the 2019 VarDial Evaluation Campaign. The first sub-task was a binary classification by dialect task, in which a classification model is required to discriminate between the Moldavian and the Romanian dialec"
W19-1401,W18-3929,1,0.894916,"Missing"
W19-1401,W19-1413,1,0.836296,"Missing"
W19-1401,W19-1416,0,0.056458,"Missing"
W19-1401,W18-3907,1,0.895659,"Missing"
W19-1401,Y96-1018,1,0.197287,"k. 5.5 Summary Three teams participated in this first iteration of the cross-lingual analysis task. Two of the teams employed variations of neural encoderdecoder systems. Apart from lemmatization performance, it proved to be difficult to attain consistent improvements over the neural baseline systems. However, the suffix stripping approach used by the HSE team did deliver clear improvements in lemmatization for both Turkic and Romance languages. 6 6.1 Dataset Texts to distinguish between the two variations were compiled from the two existing corpora of news: Sinica Corpus for Taiwan Mandarin (Chen et al., 1996) and LCMC (The Lancaster Corpus of Mandarin Chinese, (McEnery and Xiao, 2003)) for Mainland Mandarin. Both corpora are segmented and tokenized. We remove the punctuation and unify the orthography used to eliminate orthographic cues. Since both corpora are balanced corpora, our initial thought was to provide genre-aware classification. However, inspection of both corpora suggested the genres were not defined in the same way and are not distributed homogeneously. In the next edition this idea may be exploited by using some additional resources as genre vs. regional variations which is an importa"
W19-1401,W19-1419,1,0.847943,"Missing"
W19-1401,W19-1414,0,0.0601978,"Missing"
W19-1401,W16-4801,1,0.6692,"Missing"
W19-1401,W19-1420,0,0.0913091,"ces from newspapers for each Mandarin variety. The main task is to determine if a sentence is written in the Mandarin Cuneiform Language Identification (CLI): This shared task focused on discriminating between languages and dialects originally written using the cuneiform script. The task included 2 dif2 Team Adaptcenter BAM dkosmajac DTeam SharifCL ghpaetzold gretelliz92 ekh IUCL HSE itsalexyang lonewolf MineriaUNAM NRC-CNRC R2I LIS PZ SC-UPB situx SUKI tearsofjoy T¨ubingenOslo Twist Bytes Total GDI CMA DMT X MRC CLI X X System Description Papers (Butnaru, 2019) X X X X X X (Tudoreanu, 2019) (Doostmohammadi and Nassajian, 2019) X X (Hu et al., 2019) (Mikhailov et al., 2019) (Yang and Xiang, 2019) X X X X X X X X (Bernier-Colborne et al., 2019) (Chifu, 2019) (Paetzold and Zampieri, 2019) (Onose and Cercel, 2019) X X X X X X X 7 5 X 8 X X 6 3 (Jauhiainen et al., 2019b) (Wu et al., 2019) (C¸o¨ ltekin and Barnes, 2019) (Benites et al., 2019) 14 Table 1: The teams that participated in the Third VarDial Evaluation Campaign. took part in, and a reference to each of the 14 system description papers published in the VarDial workshop proceedings. ferent languages: Sumerian and Akkadian. Furthermore, the Akkadian language was"
W19-1401,W19-1415,0,0.0347692,"Missing"
W19-1401,L18-1550,0,0.0287077,"sed on a majority voting scheme applied on five classification models: kNearest Neighbors, Logistic Regression, Support Vector Machines, Neural Networks and Random Forests. For the first and the third runs, the models are trained on both training and development sets. For the second run, the model is trained only on the training set. SC-UPB. The SC-UPB team first cleaned the dataset by removing stopwords as well as special characters. The first run submitted to each of the three subtasks is based on a model that represents text as the mean of word vectors given by a pretrained FastText model (Grave et al., 2018). The representation is provided as input to a Recurrent Neural Network with gated recurrent units, which is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. The second run submitted to each of the three subtasks is based on a hierarchical attention network introduced by Yang et al. (2016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting"
W19-1401,W18-4802,0,0.0115539,"zers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then removed any form whic"
W19-1401,W19-1417,0,0.0619352,"Missing"
W19-1401,E17-2034,0,0.0280356,"e-art for this task, however, developing rule-based analyzers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjective"
W19-1401,D18-1135,1,0.824726,"016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting scheme. Their model’s parameters are tuned independently for each subtask, using random search and 5-fold crossvalidation. The tearsofjoy team also tried a transductive learning approach which is based on retraining the model by adding confident predictions from the test set to the training set, an idea previously studied in (Ionescu and Butnaru, 2018). • Binary classification by dialect (subtask 1) – the task is to discriminate between the Moldavian and the Romanian dialects. • MD→RO cross-dialect multi-class categorization by topic (subtask 2) – the task is to classify the samples written in the Romanian dialect into six topics, using a model trained on samples written in the Moldavian dialect. • RO→MD cross-dialect multi-class categorization by topic (subtask 3) – the task is to classify the samples written in the Moldavian dialect into six topics, using a model trained on samples written in the Romanian dialect. 7.2 Participants and App"
W19-1401,W19-1409,1,0.877823,"Missing"
W19-1401,W19-1418,0,0.0615736,"Missing"
W19-1401,N16-1174,0,0.0228023,"t. SC-UPB. The SC-UPB team first cleaned the dataset by removing stopwords as well as special characters. The first run submitted to each of the three subtasks is based on a model that represents text as the mean of word vectors given by a pretrained FastText model (Grave et al., 2018). The representation is provided as input to a Recurrent Neural Network with gated recurrent units, which is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. The second run submitted to each of the three subtasks is based on a hierarchical attention network introduced by Yang et al. (2016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting scheme. Their model’s parameters are tuned independently for each subtask, using random search and 5-fold crossvalidation. The tearsofjoy team also tried a transductive learning approach which is based on retraining the model by adding confident predictions from the test set to the training set, an idea previously studied in (Ione"
W19-1401,W19-1423,1,0.825557,"e ranking for subtask 3, as shown in Table 8. 7.4 Dataset Summary We proposed three MRC subtasks for VarDial 2019. Three participants submitted runs for all three subtasks, and another two participants submitted runs only for subtask 1. Two teams (DTeam 5 12 http://oracc.museum.upenn.edu Language or Dialect Sumerian Old Babylonian Middle Babylonian peripheral Standard Babylonian Neo-Babylonian Late Babylonian Neo-Assyrian two systems in more detail. The PZ team used a SVM metaclassifier ensemble of several linear SVM classifiers trained using character n-gram and character skip-gram features. Paetzold and Zampieri (2019) give further details. The SharifCL team submitted three runs and their best performing system was an ensemble of a SVM and a NB classifier (Doostmohammadi and Nassajian, 2019). The ghpaetzold team submitted only one run using 2-layer compositional recurrent neural network that learns numerical representations of sentences based on their words, and of words based on their characters. Their system is described in more detail by Paetzold and Zampieri (2019). The ekh team used a sum of relative frequencies of character bigrams together with a penalty value for those bigrams or unigrams that were"
W19-1401,W17-1201,1,0.803354,"Missing"
W19-1401,L16-1641,1,0.880892,"Missing"
W19-1401,W14-5307,1,0.821127,"Missing"
W19-1401,W18-0209,1,0.707638,"r, developing rule-based analyzers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then"
W19-1401,W19-0301,1,0.916428,"task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then removed any form which did not include at least one"
W19-1401,E14-2006,0,0.0437089,"Missing"
W19-1401,W19-1422,0,0.0730531,"Missing"
W19-1401,W19-1412,0,0.0741523,"Missing"
W19-2005,P13-1158,0,0.0334914,"r to languageindependent meaning representations than bilingual models do. Hence, our hypothesis is that 35 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 35–42 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics Language English French Afrikaans Albanian Breton German Greek Frisian Hindi Italian Dutch Ossetian Polish Russian Slovene Spanish Serbian Swedish paraphrase generation using machine translation (Quirk et al., 2004; Finch et al., 2004; Prakash et al., 2016) based on parallel monolingual corpora (Lin et al., 2014; Fader et al., 2013), pivot-based translation (Bannard and CallisonBurch, 2005; Mallinson et al., 2017) and paraphrase databased extracted from parallel corpora (Ganitkevitch et al., 2013). Related work on multilingual sentence representation (Artetxe and Schwenk, 2018; Schwenk and Douze, 2017; Lample and Conneau, 2019) has focused on fixedsize vector representations that can be used in natural language inference (Conneau et al., 2018; Eriguchi et al., 2018) or other downstream tasks such as bitext mining (Artetxe and Schwenk, 2018) or (cross-lingual) document classification (Schwenk and Li, 2018). 2 Transl. 19 1"
W19-2005,N13-1092,0,0.194932,"Missing"
W19-2005,Q17-1024,0,0.043325,"Missing"
W19-2005,P18-4020,0,0.0253946,"Missing"
W19-2005,P05-1074,0,0.529478,"Missing"
W19-2005,E17-1083,0,0.0276671,"e, our hypothesis is that 35 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 35–42 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics Language English French Afrikaans Albanian Breton German Greek Frisian Hindi Italian Dutch Ossetian Polish Russian Slovene Spanish Serbian Swedish paraphrase generation using machine translation (Quirk et al., 2004; Finch et al., 2004; Prakash et al., 2016) based on parallel monolingual corpora (Lin et al., 2014; Fader et al., 2013), pivot-based translation (Bannard and CallisonBurch, 2005; Mallinson et al., 2017) and paraphrase databased extracted from parallel corpora (Ganitkevitch et al., 2013). Related work on multilingual sentence representation (Artetxe and Schwenk, 2018; Schwenk and Douze, 2017; Lample and Conneau, 2019) has focused on fixedsize vector representations that can be used in natural language inference (Conneau et al., 2018; Eriguchi et al., 2018) or other downstream tasks such as bitext mining (Artetxe and Schwenk, 2018) or (cross-lingual) document classification (Schwenk and Li, 2018). 2 Transl. 19 14 5 2 1 24 7 1 4 5 3 2 5 5 1 8 2 1 Verses 234,173 369,910 75,974 58,192 1,781 499,8"
W19-2005,P11-1020,0,0.119638,"Missing"
W19-2005,mayer-cysouw-2014-creating,0,0.0213217,"avoid unknown words and to improve generalisations. Note that in our setup we need to ensure that subword-level segmentations are consistent for each language involved in several translation tasks. We opted for languagedependent BPE models with 10,000 merge operations for each code table. The total vocabulary size then depends on the combination of languages that we use in training but the vocabulary stays exactly the same for each language involved in all experiments. 2.1 Training data and configurations The main data we use for our experiments comes from a collection of Bible translations (Mayer and Cysouw, 2014) that includes over a thousand languages. For high-density languages like English and French, various alternatives are available (see Table 1). Using the Bible makes it possible to easily extend our work with additional languages representing a wide range of linguistic variation, while at the same time keeping genre and content constant across languages. For the sake of discussion, we selected English 36 100 60 80 40 60 20 40 20 En g +B –Fra +S reton +O loven s e +S setian erb +S ian pa +G nish r +H eek +F indi +S risi +A wed an frik ish +P aans o +It lish +R alian +Gussia er n + ma +A Dut n l"
W19-2005,P02-1040,0,0.10355,"Missing"
W19-2005,C16-1275,0,0.0537101,"er multilingual machine translation models learn representations that are closer to languageindependent meaning representations than bilingual models do. Hence, our hypothesis is that 35 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 35–42 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics Language English French Afrikaans Albanian Breton German Greek Frisian Hindi Italian Dutch Ossetian Polish Russian Slovene Spanish Serbian Swedish paraphrase generation using machine translation (Quirk et al., 2004; Finch et al., 2004; Prakash et al., 2016) based on parallel monolingual corpora (Lin et al., 2014; Fader et al., 2013), pivot-based translation (Bannard and CallisonBurch, 2005; Mallinson et al., 2017) and paraphrase databased extracted from parallel corpora (Ganitkevitch et al., 2013). Related work on multilingual sentence representation (Artetxe and Schwenk, 2018; Schwenk and Douze, 2017; Lample and Conneau, 2019) has focused on fixedsize vector representations that can be used in natural language inference (Conneau et al., 2018; Eriguchi et al., 2018) or other downstream tasks such as bitext mining (Artetxe and Schwenk, 2018) or ("
W19-2005,W04-3219,0,0.635583,"n particular, we would like to see whether multilingual machine translation models learn representations that are closer to languageindependent meaning representations than bilingual models do. Hence, our hypothesis is that 35 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 35–42 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics Language English French Afrikaans Albanian Breton German Greek Frisian Hindi Italian Dutch Ossetian Polish Russian Slovene Spanish Serbian Swedish paraphrase generation using machine translation (Quirk et al., 2004; Finch et al., 2004; Prakash et al., 2016) based on parallel monolingual corpora (Lin et al., 2014; Fader et al., 2013), pivot-based translation (Bannard and CallisonBurch, 2005; Mallinson et al., 2017) and paraphrase databased extracted from parallel corpora (Ganitkevitch et al., 2013). Related work on multilingual sentence representation (Artetxe and Schwenk, 2018; Schwenk and Douze, 2017; Lample and Conneau, 2019) has focused on fixedsize vector representations that can be used in natural language inference (Conneau et al., 2018; Eriguchi et al., 2018) or other downstream tasks such as bit"
W19-2005,W17-2619,0,0.0422909,"al Linguistics Language English French Afrikaans Albanian Breton German Greek Frisian Hindi Italian Dutch Ossetian Polish Russian Slovene Spanish Serbian Swedish paraphrase generation using machine translation (Quirk et al., 2004; Finch et al., 2004; Prakash et al., 2016) based on parallel monolingual corpora (Lin et al., 2014; Fader et al., 2013), pivot-based translation (Bannard and CallisonBurch, 2005; Mallinson et al., 2017) and paraphrase databased extracted from parallel corpora (Ganitkevitch et al., 2013). Related work on multilingual sentence representation (Artetxe and Schwenk, 2018; Schwenk and Douze, 2017; Lample and Conneau, 2019) has focused on fixedsize vector representations that can be used in natural language inference (Conneau et al., 2018; Eriguchi et al., 2018) or other downstream tasks such as bitext mining (Artetxe and Schwenk, 2018) or (cross-lingual) document classification (Schwenk and Li, 2018). 2 Transl. 19 14 5 2 1 24 7 1 4 5 3 2 5 5 1 8 2 1 Verses 234,173 369,910 75,974 58,192 1,781 499,844 87,218 29,173 93,242 122,363 87,460 37,807 52,668 75,904 29,088 236,830 35,019 29,088 Tokens 6,750,869 10,529,929 2,329,773 1,648,242 44,316 13,712,459 2,357,095 852,582 2,829,274 3,429,18"
W19-2005,L18-1560,0,0.0181527,"Lin et al., 2014; Fader et al., 2013), pivot-based translation (Bannard and CallisonBurch, 2005; Mallinson et al., 2017) and paraphrase databased extracted from parallel corpora (Ganitkevitch et al., 2013). Related work on multilingual sentence representation (Artetxe and Schwenk, 2018; Schwenk and Douze, 2017; Lample and Conneau, 2019) has focused on fixedsize vector representations that can be used in natural language inference (Conneau et al., 2018; Eriguchi et al., 2018) or other downstream tasks such as bitext mining (Artetxe and Schwenk, 2018) or (cross-lingual) document classification (Schwenk and Li, 2018). 2 Transl. 19 14 5 2 1 24 7 1 4 5 3 2 5 5 1 8 2 1 Verses 234,173 369,910 75,974 58,192 1,781 499,844 87,218 29,173 93,242 122,363 87,460 37,807 52,668 75,904 29,088 236,830 35,019 29,088 Tokens 6,750,869 10,529,929 2,329,773 1,648,242 44,316 13,712,459 2,357,095 852,582 2,829,274 3,429,182 2,596,298 936,533 1,248,108 1,727,536 748,367 6,607,932 844,299 833,983 Table 1: Statistics about the Bible data in our collection: number of individual Bible translations, number of verses and number of tokens per language in the training data sets. Experimental Setup For our experiments, we apply a standa"
W19-2005,E17-3017,0,0.0777581,"Missing"
W19-2005,P16-1162,0,0.0385519,"g criterion, which is set to five subsequent failures of improving the validation score. The translations are done with a beam search decoder of size 12. The validation frequency is set to run each 2,500 mini-batches. For the multilingual setup, we follow Johnson et al. (2016) by adding target language flags to the source text placing them as pseudo tokens in the beginning of each input sentence. We always train models in both directions enabling the model to read and generate the same language without explicitly training that task (i.e. paraphrasing is modeled as zero-shot translation). BPE (Sennrich et al., 2016) is used to avoid unknown words and to improve generalisations. Note that in our setup we need to ensure that subword-level segmentations are consistent for each language involved in several translation tasks. We opted for languagedependent BPE models with 10,000 merge operations for each code table. The total vocabulary size then depends on the combination of languages that we use in training but the vocabulary stays exactly the same for each language involved in all experiments. 2.1 Training data and configurations The main data we use for our experiments comes from a collection of Bible tra"
W19-5347,W18-6410,1,0.861487,"Missing"
W19-5347,P18-4020,0,0.0236348,"Missing"
W19-5347,P17-4012,0,0.0294692,"tence-level approaches In this section we describe our sentencelevel translation models and the experiments in the English-to-German, English-to-Finnish and Finnish-to-English translation directions. Table 3: Percentage of lines rejected by each filter for English–Finnish data sets. The strict version is the same as for English–German, and the relax version applies relaxed thresholds. 3.1 Model architectures We experimented with both NMT and rule-based systems. All of our neural sentence-level models are based on the transformer architecture (Vaswani et al., 2017). We used both the OpenNMTpy (Klein et al., 2017) and MarianNMT (Junczysplete sentences that end with proper final punctuation marks, and the filter might remove quite a bit of the useful data examples. However, our fi415 Dowmunt et al., 2018) frameworks. Our experiments focused on the following: BLEU news2018 Single model 5 save-points 5 save-points + 4 fine-tuned • Ensemble models: using ensembles with a combination of independent runs and savepoints from a single training run. 44.61 46.65 47.45 • Left-to-right and right-to-left models: Transformer models with decoding of the output in left-to-right and right-to-left order. Table 4: Englis"
W19-5347,D18-2012,0,0.0289023,"d BPE algorithm is run over the Omorfi-segmented text in order to split low-frequency morphemes. In this experiment, we compare two models for each translation direction: • One model segmented with the standard BPE algorithm (joint vocabulary size of 50 000, vocabulary frequency threshold of 50). English–Finnish and Finnish–English The problem of open-vocabulary translation is particularly acute for morphologically rich languages like Finnish. In recent NMT research, the standard approach consists of applying a word segmentation algorithm such as BPE (Sennrich et al., 2016b) or SentencePiece (Kudo and Richardson, 2018) during pre-processing. In recent WMT editions, various alternative segmentation approaches were examined for Finnish: hybrid models that back off to character-level rep¨ resentations (Ostling et al., 2017), and variants of the Morfessor unsupervised morphology algorithm (Gr¨onroos et al., 2018). This year, we exper• One model where the Finnish side is presegmented with Omorfi, and both the Omorfisegmented Finnish side and the English side are segmented with BPE (same parameters as above). All models are trained on filtered versions of Europarl, ParaCrawl, Rapid, Wikititles, newsdev2015 and ne"
W19-5347,P16-1009,0,0.234209,"sh-to-German (Section 4), and a comparison of different word segmentation approaches for Finnish (Section 3.3). The final submitted NMT systems are summarized in Section 5, while the rule-based machine translation system is described in Section 3.4. 2 Pre-processing • removing non-printing characters, • normalizing punctuation, • tokenization. In addition to these steps, we replaced a number of English contractions with the full form, e.g. “They’re” → “They are”. After the above steps, we applied a Moses truecaser model trained for individual languages, and finally a byte-pair encoding (BPE) (Sennrich et al., 2016b) segmentation using a set of codes for either language pair. For English–German, we initially pre-processed the data using only punctuation normalization and tokenization. We subsequently trained an English truecaser model using all monolingual English data as well as the English side of all parallel English–German datasets except the Rapid corpus (in which non-English characters were missing from a substantial portion of the German sentences). We also repeated the same for German. Afterwards, we used a heuristic cleanup script1 in Pre-processing, data filtering and back-translation It is we"
W19-5347,P16-1162,0,0.441043,"sh-to-German (Section 4), and a comparison of different word segmentation approaches for Finnish (Section 3.3). The final submitted NMT systems are summarized in Section 5, while the rule-based machine translation system is described in Section 3.4. 2 Pre-processing • removing non-printing characters, • normalizing punctuation, • tokenization. In addition to these steps, we replaced a number of English contractions with the full form, e.g. “They’re” → “They are”. After the above steps, we applied a Moses truecaser model trained for individual languages, and finally a byte-pair encoding (BPE) (Sennrich et al., 2016b) segmentation using a set of codes for either language pair. For English–German, we initially pre-processed the data using only punctuation normalization and tokenization. We subsequently trained an English truecaser model using all monolingual English data as well as the English side of all parallel English–German datasets except the Rapid corpus (in which non-English characters were missing from a substantial portion of the German sentences). We also repeated the same for German. Afterwards, we used a heuristic cleanup script1 in Pre-processing, data filtering and back-translation It is we"
W19-5347,P12-3005,0,0.0262377,"rovided parallel training data. This is especially true for the ParaCrawl and Rapid data sets. This is rather unexpected as a basic language identifier certainly must be part of the crawling and extraction pipeline. Nevertheless, after some random inspection of the data, we found it necessary to apply off-the-shelf language identifiers to the data for removing additional erroneous text from the training data. In particular, we applied the Compact Language Detector version 2 (CLD2) from the Google Chrome project (using the Python interface from pycld22 ), and the widely used langid.py package (Lui and Baldwin, 2012) to classify each sentence in the ParaCrawl, CommonCrawl, Rapid and Wikititles data sets. We removed all sentence pairs in which the language of one of the aligned sentences was not reliably detected. For this, we required the correct language ID from both classifiers, the reliable-flag set to “True” by CLD2 with a reliability score of 90 or more, and the detection probability of langid.py to be at least 0.9. Data filtering For data filtering we applied four types of filters: (i) rule-based heuristics, (ii) filters based on language identification, (iii) filters based on word alignment models,"
W19-5347,N19-1313,0,0.0317811,"for 1 epoch. The results for the NMT-HAN model are disappointing. The document-level model performs significantly worse than the sentence-level model. Hierarchical attention models A number of approaches have been developed to utilize the attention mechanism to capture extended context for document-level translation. We experimented with the two following models: • NMT-HAN: Sentence-level transformer model with a hierarchical attention network to capture the document-level context (Miculicich et al., 2018). • selectAttn: Selective attention model for context-aware neural machine translation (Maruf et al., 2019). For testing the selectAttn model, we used the same data with document-level information as we applied in the concatenation models. For NMTHAN we had to use a smaller training set due to lack of resources and due to the implementation not supporting data shards. For NMT-HAN we used only Europarl, NewsCommentary and Rapid for training. Table 11 summarizes the results on the development test data. Both of the tested models need to be trained on sentence-level first, before tuning the document-level components. 5 Model NMT-HAN selectAttn Sentence-level Document-level 35.03 35.26 31.73 34.75 Resu"
W19-5347,D18-1325,0,0.0246673,"uggested in the documentation with respect to optimizers, learning rates and dropout. Unfortunately, the results do not look very promising as we can see in Table 11. The document-level model does not even reach the performance of the sentence-level model even though we trained until convergence on development data with patience of 10 reporting steps, which is quite disappointing. Overall, the scores are below the standard transformer models of the other experiments, and hence, we did not try to further optimize the results using that model. For the NMT-HAN model we used the implementation of Miculicich et al. (2018) with the recommended hyperparameter values and settings. The system is based on the OpenNMT-py implementation of the transformer. The model includes 6 hidden layers on both the encoder and decoder side with a dimensionality of 512 and the multihead attention has 8 attention heads. We applied a sublayer and attention dropout of 0.1. The target and source vocabulary size is 30K. We trained the sentence-level model for 20 epochs after which we further fine-tuned the encoder side hierarchical attention for 1 epoch and the joint encoderdecoder hierarchical attention for 1 epoch. The results for th"
W19-5347,W17-4733,1,0.860954,"BPE algorithm (joint vocabulary size of 50 000, vocabulary frequency threshold of 50). English–Finnish and Finnish–English The problem of open-vocabulary translation is particularly acute for morphologically rich languages like Finnish. In recent NMT research, the standard approach consists of applying a word segmentation algorithm such as BPE (Sennrich et al., 2016b) or SentencePiece (Kudo and Richardson, 2018) during pre-processing. In recent WMT editions, various alternative segmentation approaches were examined for Finnish: hybrid models that back off to character-level rep¨ resentations (Ostling et al., 2017), and variants of the Morfessor unsupervised morphology algorithm (Gr¨onroos et al., 2018). This year, we exper• One model where the Finnish side is presegmented with Omorfi, and both the Omorfisegmented Finnish side and the English side are segmented with BPE (same parameters as above). All models are trained on filtered versions of Europarl, ParaCrawl, Rapid, Wikititles, newsdev2015 and newstest2015 as well as backtranslations. Following our experiments at WMT 5 https://flammie.github.io/ omorfi/pages/usage-examples.html# morphological-segmentation 417 2018 (Raganato et al., 2018), we also u"
W19-5347,W18-6427,0,0.0413463,"Missing"
W19-5347,W17-4811,1,0.87915,"2018. We then test our systems on both the original test set with coherent test data divided into short news documents and the shuffled test set with broken coherence. 4.1 System Baseline 2+1 3+1a 3+1b 1t+1s+1 2+2 BLEU news2018 Shuffled Coherent 38.96 36.62 33.90 34.14 36.82 38.53 38.96 37.17 34.30 34.39 37.24 39.08 Table 10: Comparison of concatenation approaches for English–German document-level translation. Concatenation models Some of the previously published approaches use concatenation of multiple source-side sentences in order to extend the context of the currently translated sentence (Tiedemann and Scherrer, 2017). In addition to the source-side concatenation model, we also tested an approach where we concatenate The results overall are rather disappointing. All but one of the concatenation models underperform and cannot beat the sentence-level baseline. Note that the concat-target model (1t+1s+1) even refers to an oracle experiment in which the reference 420 translation of the previous sentence is fed into the translation model for translating the current source sentence. As this is not very successful, we did not even try to run a proper evaluation with system output provided as target context during"
W19-5347,W15-1844,0,0.0159073,"scored n-best lists. The positive effect of beam search is further illustrated in Figure 1. All previous models were run with a beam size of 12. As we can see, the general trend is that larger beams lead to improved performance, at least until the limit of 64 in our experiments. Beam size 4 is an exception in the left-to-right models. 46.5 46.0 45.5 45.0 44.5 44.0 43.5 43.0 L2R R2L 1 2 4 8 16 Beam size 32 64 Figure 1: The effect of beam size on translation performance. All results use model ensembles and the scores are case-sensitive. imented with rule-based word segmentation based on Omorfi (Pirinen, 2015). Omorfi is a morphological analyzer for Finnish with a large-coverage lexicon. Its segmentation tool5 splits a word form into morphemes as defined by the morphological rules. In particular, it distinguishes prefixes, infixes and suffixes through different segmentation markers: Intia→ ←n ja Japani→ ←n p¨aa¨ → ←ministeri→ India GEN and Japan GEN prime minister ←t tapaa→ ←vat Tokio→ ←ssa PL meet 3 PL Tokyo INE While Omorfi provides word segmentation based on morphological principles, it does not rely on any frequency cues. Therefore, the standard BPE algorithm is run over the Omorfi-segmented te"
W19-5354,W18-6436,0,0.0245897,"th online systems (which do not provide an API for scoring) or with rule-based systems. Second, it is unclear to what extent the score of an MT system reflects its quality, as it might never have generated that particular sentence. Third, it requires the explicit construction of contrastive sentences, which is not trivial, especially for morphologically rich languages. For these reasons, the WMT test suite calls focus on translation test suites, where the participants are asked to produce translations of the source sentence instead of scoring given hypotheses. Following Rios et al. (2018) and Macketanz et al. (2018), who proposed small-scale translation test suites targeting WSD, we participated at WMT with modified versions of M U C OW. The modifications only concern step (3). As a result, we make available two variants of M U C OW, a multilingual contrastive word sense disambiguation test suite for machine translation. The scoring variant covers 11 language pairs with a total of almost 240 000 sentence pairs. The translation variant covers 9 language pairs with a total of 15 600 sentences. The data and scoring scripts are available at https://github. com/Helsinki-NLP/MuCoW. 2 that were aligned at least"
W19-5354,D17-1263,0,0.0781231,"Missing"
W19-5354,K17-1012,0,0.149717,"Missing"
W19-5354,W18-1812,0,0.0539437,"disambiguation test sets for machine translation Alessandro Raganato∗ † , Yves Scherrer∗ and J¨org Tiedemann∗ ∗ University of Helsinki † Basement AI {name.surname}@helsinki.fi Abstract tactic divergences between source and target language (Burchardt et al., 2017; Burlot and Yvon, 2017; Isabelle et al., 2017; Sennrich, 2017; Burlot et al., 2018; Macketanz et al., 2018) or on discourse phenomena (Guillou and Hardmeier, 2016; Bawden et al., 2018; M¨uller et al., 2018; Guillou et al., 2018). Another linguistic phenomenon that is challenging for translation is lexical ambiguity (Liu et al., 2018; Marvin and Koehn, 2018), i.e., words of the source language that have multiple translations in the target language representing different meanings. Recently, Rios Gonzales et al. (2017) introduced a lexical ambiguity benchmark called ContraWSD that is based on contrastive translation pairs: a sentence containing an ambiguous source word is paired with the correct reference translation and with a modified translation in which the ambiguous word has been replaced by a word of a different sense. Contrastive evaluation makes use of the ability of NMT systems to score given translations: a contrast is considered successf"
W19-5354,2005.mtsummit-papers.11,0,0.283026,"Missing"
W19-5354,D18-1512,0,0.0320985,"Missing"
W19-5354,W18-6307,0,0.0773078,"Missing"
W19-5354,L16-1147,1,0.893774,"Missing"
W19-5354,P06-1014,0,0.106146,"lds. An additional manual evaluation was carried out with 50 random German words3 and four settings that obtained high F1 or F0.5 scores. As shown in Table 3, the SW2V method with a threshold set at 0.3 obtained the highest precision value by a large margin and therefore also the best F0.5 score. We chose this setting for all languages. Source words that end up with a single synset as a result of this step are discarded. Step 2b: Refine sense clusters with sense embeddings It is known that lexical resources such as BabelNet tend to suffer from overly fine granularity of their sense inventory (Navigli, 2006; Palmer et al., 2007). We therefore introduce an additional merging step: i) we associate each Babel synset with an embedding, ii) compute pairwise cosine similarities between synsets, iii) and merge them if their embedding similarity is higher than a threshold γ. Choosing a good Babel synset embedding and an optimal threshold is a difficult task. We evaluated three Babel synset vector representations, using the existing German→English ContraWSD test suite as gold standard: Nasari (Camacho-Collados et al., 2016) is a vector representation built by combining the knowledge from Wikipedia and Wo"
W19-5354,N18-1121,0,0.113634,"rastive word sense disambiguation test sets for machine translation Alessandro Raganato∗ † , Yves Scherrer∗ and J¨org Tiedemann∗ ∗ University of Helsinki † Basement AI {name.surname}@helsinki.fi Abstract tactic divergences between source and target language (Burchardt et al., 2017; Burlot and Yvon, 2017; Isabelle et al., 2017; Sennrich, 2017; Burlot et al., 2018; Macketanz et al., 2018) or on discourse phenomena (Guillou and Hardmeier, 2016; Bawden et al., 2018; M¨uller et al., 2018; Guillou et al., 2018). Another linguistic phenomenon that is challenging for translation is lexical ambiguity (Liu et al., 2018; Marvin and Koehn, 2018), i.e., words of the source language that have multiple translations in the target language representing different meanings. Recently, Rios Gonzales et al. (2017) introduced a lexical ambiguity benchmark called ContraWSD that is based on contrastive translation pairs: a sentence containing an ambiguous source word is paired with the correct reference translation and with a modified translation in which the ambiguous word has been replaced by a word of a different sense. Contrastive evaluation makes use of the ability of NMT systems to score given translations: a contra"
W19-5354,P16-1162,0,0.351179,"rget sense of ambiguous source words. Here, we give some baseline results obtained with supervised NMT systems. Following Rios Gonzales et al. (2017), we score both reference and contrastive translations with the same NMT system. A correct decision is detected when the score of the reference is higher than the scores from all contrastive translations. The final test suite score corresponds to the accuracy over all decisions. Three models are examined for German→English: a 6-layer bi-LSTM model and a Transformer model4 trained on the provided training data from WMT17 plus backtranslations from Sennrich et al. (2016b), and the University of Edinburgh’s WMT17 submission, a deep LSTM model with additional synthetic data trained with Nematus (Sennrich et al., 2017b).5 The upper half of Table 5 reports ContraWSD Statistics We apply the three steps presented above to all toEnglish translation directions that were part of the Conference of Machine Translation (WMT) news translation task over the last years. Table 4 summarizes the statistics of these resources. The average number of senses per source word ranges between 2.0 and 2.11 (2.36–2.4 for ContraWSD). The lexicons for the Baltic languages are small due t"
W19-5354,W18-6456,0,0.0397964,"nd contrastive translations with pretrained NMT models, and as translation test suite for the WMT19 news shared task. We find that state-of-the-art and fine-tuned NMT systems still present some drawbacks on handling ambiguous words, especially when evaluated on out-of-domain data and when the encoder has to deal with a morphologically rich language. It will be particularly instructive to see how well the WSD test suite results correlate with human evaluation scores and with recently proposed evaluation metrics that are based on semantic representations of the translations (Gupta et al., 2015; Shimanaka et al., 2018). As future work we plan to further extend the test suite including more languages and parallel data, and make use of the contrastive sentences as adversarial examples during training. The Prague Bulletin of Mathematical Linguistics, 108:159–170. Franck Burlot, Yves Scherrer, Vinit Ravishankar, Ondˇrej Bojar, Stig-Arne Gr¨onroos, Maarit Koponen, Tommi Nieminen, and Franc¸ois Yvon. 2018. The WMT’18 morpheval test suites for English-Czech, English-German, English-Finnish and Turkish-English. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 546–560, Belgium"
W19-5354,W18-6437,0,0.115004,"Missing"
W19-5354,K17-3009,0,0.0299513,"Missing"
W19-5354,W17-4702,0,0.577585,"}@helsinki.fi Abstract tactic divergences between source and target language (Burchardt et al., 2017; Burlot and Yvon, 2017; Isabelle et al., 2017; Sennrich, 2017; Burlot et al., 2018; Macketanz et al., 2018) or on discourse phenomena (Guillou and Hardmeier, 2016; Bawden et al., 2018; M¨uller et al., 2018; Guillou et al., 2018). Another linguistic phenomenon that is challenging for translation is lexical ambiguity (Liu et al., 2018; Marvin and Koehn, 2018), i.e., words of the source language that have multiple translations in the target language representing different meanings. Recently, Rios Gonzales et al. (2017) introduced a lexical ambiguity benchmark called ContraWSD that is based on contrastive translation pairs: a sentence containing an ambiguous source word is paired with the correct reference translation and with a modified translation in which the ambiguous word has been replaced by a word of a different sense. Contrastive evaluation makes use of the ability of NMT systems to score given translations: a contrast is considered successfully detected if the reference translation obtains a higher score than an artificially modified translation. However, all these test suites require significant am"
W19-5354,W18-6304,0,0.0758876,"ransformer, we use the base version (Vaswani et al., 2017). 5 data.statmt.org/wmt17_systems/ Measuring machine translation WSD capability with M U C OW The aim of M U C OW is to examine the ability of current machine translation systems to choose the 473 and M U C OW accuracy scores as well as BLEU scores computed on the WMT17 test set. The ranking of the three models is consistent across the three tasks. Interestingly, the Transformer model (trained on far less data than the Nematus model) scores much better on the two test suites than the BLEU score would suggest, confirming the findings by Tang et al. (2018). The University of Edinburgh also makes available their NMT models for other WMT16 and WMT17 language pairs.6 M U C OW accuracy scores of these models are shown in the lower half of Table 5 together with the WMT test set BLEU scores reported by the authors (Sennrich et al., 2016a, 2017a). Even though we only assess the confidence of an NMT system in detecting the right sense of a single word within a sentence, the results show that WSD is still an issue in MT – even in stateof-the-art-systems – that requires further study. 4 Language Source Target In-dom Out-dom Senpair words synsets synsets"
W19-5354,E17-2060,0,0.0888622,"Missing"
W19-5354,tiedemann-2012-parallel,1,0.749947,"sions of M U C OW. The modifications only concern step (3). As a result, we make available two variants of M U C OW, a multilingual contrastive word sense disambiguation test suite for machine translation. The scoring variant covers 11 language pairs with a total of almost 240 000 sentence pairs. The translation variant covers 9 language pairs with a total of 15 600 sentences. The data and scoring scripts are available at https://github. com/Helsinki-NLP/MuCoW. 2 that were aligned at least 10 times each with at least two distinct target words. We use parallel corpora from the OPUS collection (Tiedemann, 2012),1 counting only one-to-one word alignment links. Table 1 provides an example. 2.2 Step 2a: Cluster target words via BabelNet For each source word of the previous step, those target words that potentially share the same meaning (for example synonyms) are clustered together. To this end, we exploit BabelNet (Navigli and Ponzetto, 2012), a wide-coverage multilingual encyclopedic dictionary obtained automatically from various resources (WordNet and Wikipedia, among others). BabelNet 4.0 covers 284 languages with almost 16 million entries, called Babel synsets. Each entry represents a given meanin"
W19-5354,W18-6312,0,0.0173874,"uage pairs presented in the WMT19 news shared translation task, plus on other 5 language pairs using pretrained NMT models. The M U C OW test suite is available at http://github. com/Helsinki-NLP/MuCoW. 1 Introduction Neural Machine Translation (NMT) has provided impressive advances in translation quality, leading to a discussion whether translations produced by professional human translators can still be distinguished from the output of NMT systems, and to what extent automatic evaluation measures can reliably account for these differences (Hassan Awadalla et al., 2018; L¨aubli et al., 2018; Toral et al., 2018). One answer to this question lies in the development of so-called test suites (Burchardt et al., 2017) or challenge sets (Isabelle et al., 2017) that focus on particular linguistic phenomena that are known to be difficult to evaluate with simple reference-based metrics such as BLEU. Existing test suites focus e.g. on morphosyntactic and syn470 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 470–480 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 177 50 29 28 27 26 input typing entering entry load"
W19-5354,E17-3017,0,0.063791,"Missing"
W19-5354,W16-2323,0,0.112409,"rget sense of ambiguous source words. Here, we give some baseline results obtained with supervised NMT systems. Following Rios Gonzales et al. (2017), we score both reference and contrastive translations with the same NMT system. A correct decision is detected when the score of the reference is higher than the scores from all contrastive translations. The final test suite score corresponds to the accuracy over all decisions. Three models are examined for German→English: a 6-layer bi-LSTM model and a Transformer model4 trained on the provided training data from WMT17 plus backtranslations from Sennrich et al. (2016b), and the University of Edinburgh’s WMT17 submission, a deep LSTM model with additional synthetic data trained with Nematus (Sennrich et al., 2017b).5 The upper half of Table 5 reports ContraWSD Statistics We apply the three steps presented above to all toEnglish translation directions that were part of the Conference of Machine Translation (WMT) news translation task over the last years. Table 4 summarizes the statistics of these resources. The average number of senses per source word ranges between 2.0 and 2.11 (2.36–2.4 for ContraWSD). The lexicons for the Baltic languages are small due t"
W19-5354,P16-1009,0,0.265498,"rget sense of ambiguous source words. Here, we give some baseline results obtained with supervised NMT systems. Following Rios Gonzales et al. (2017), we score both reference and contrastive translations with the same NMT system. A correct decision is detected when the score of the reference is higher than the scores from all contrastive translations. The final test suite score corresponds to the accuracy over all decisions. Three models are examined for German→English: a 6-layer bi-LSTM model and a Transformer model4 trained on the provided training data from WMT17 plus backtranslations from Sennrich et al. (2016b), and the University of Edinburgh’s WMT17 submission, a deep LSTM model with additional synthetic data trained with Nematus (Sennrich et al., 2017b).5 The upper half of Table 5 reports ContraWSD Statistics We apply the three steps presented above to all toEnglish translation directions that were part of the Conference of Machine Translation (WMT) news translation task over the last years. Table 4 summarizes the statistics of these resources. The average number of senses per source word ranges between 2.0 and 2.11 (2.36–2.4 for ContraWSD). The lexicons for the Baltic languages are small due t"
W19-5432,P11-2031,0,0.0380532,"g model architecture. The two Morfessor systems can be considered equivalent, as no clear winner emerges. The two official evaluation metrics BLEU and TER do not rank the systems consistently. Character-level metrics were not provided by the organizers, but follow-up experiments showed that chrF2 yields the same rankings as BLEU, whereas CharacTer deviates from BLEU and TER. The results of our submissions – and of many competitors in this shared task – lie very closely together. Before drawing any conclusions, it would therefore be useful to perform statistical significance testing. MultEval (Clark et al., 2011) provides significance scores through bootstrap resampling, but requires the output from multiple training runs of the same translation system. Unfortunately, we were not able to complete multiple training runs of our models due to time constraints. 6 Acknowledgments We would like to thank Stig-Arne Gr¨onroos for the help with Cognate Morfessor. The authors gratefully acknowledge the support of the Academy of Finland through project 314062 from the ICT 2023 call on Computation, Machine Learning and Artificial Intelligence. The authors also acknowledge CSC – IT Center for Science, Finland, for"
W19-5432,W17-4123,0,0.0483261,"Missing"
W19-5432,W10-2210,1,0.786398,"el NMT models tend to be slow due to the greater length of the sequences. 2.2 Morfessor Morfessor (Creutz and Lagus, 2002, 2007) is a method for unsupervised morphological segmentation. In contrast to the byte-pair encoding (BPE) algorithm widely adopted in neural machine translation (Sennrich et al., 2016), Morfessor defines a proper statistical model and applies maximum a posteriori estimation for the model parameters. The granularity of the segmentation (and thus size of the subword lexicon) is tunable by inserting a hyperparameter for varying the balance between prior and data likelihood (Kohonen et al., 2010). The prior can be considered as a encoding cost for the subword lexicon, and the likelihood as encoding cost for the corpus given the lexicon. In the first Morfessor variant, Morfessor Baseline (Creutz and Lagus, 2002; Virpioja et al., 2013), the statistical model is a unigram language model, i.e., the subword units are assumed to occur independently in words. Under this assumption, the probability of a sequence of tokens is simplified to be the product of the subword occurrence probabilities, which enables an efficient training algorithm. The Morfessor Baseline method has been widely tested"
W19-5432,P16-2058,0,0.0616344,"Missing"
W19-5432,P18-1007,0,0.116203,"(2017) report significant improvements over BPE segmentation for Turkish. 2.3 2.4 SentencePiece unigram model As discussed in Section 2.2, Morfessor Baseline defines a unigram language model and determines the size of its lexicon by using a prior probability for the lexicon parameters. A more straightforward approach, first proposed by Varjokallio et al. (2013) for application in ASR, is to fix the lexicon size beforehand and try to find the set of units such that they maximize likelihood of the data for a unigram model. Another heuristic search algorithm for this problem has been proposed by Kudo (2018). In addition, he proposes a subword regularization method for NMT: The unigram language model can be used to generate multiple candidate segmentations to emulate noise and segmentation errors in the data, and thus improve the Cognate Morfessor Cognate Morfessor (Gr¨onroos et al., 2018) is a variant of Morfessor designed to optimize subword segmentation for two related languages so that segmentations are consistent especially for cognates, i.e., word pairs that are similar orthographically, semantically, and distributionally. Cognate Morfessor extends the cost function of 237 ES ↔ PT CS ↔ PL E"
W19-5432,D18-2012,0,0.216413,"s reduces the amount of effort needed for a machine translation system to be able to generalize (Pourdamghani and Knight, 2017). Nevertheless, and especially since the languages offered in this shared task are to some extent morphologically complex, we assume that proper subword segmentation will be beneficial for neural machine translation (NMT) performance. In particular, we aim at consistent segmentation across both related languages. While generic subword segmentation methods such as BPE (Sennrich et al., 2016), Morfessor (Creutz and Lagus, 2007; Gr¨onroos et al., 2014), or SentencePiece (Kudo and Richardson, 2018) yield improved consistency by concatenat2.1 Character segmentation For similar languages, a commonly used segmentation scheme is character-level segmentation, where every character, including the space character, is considered independently. The idea of character-level machine translation for similar languages dates back to SMT times (e.g. Tiedemann, 2009). More recently, character-level NMT has shown promising results for distant languages (Costa-juss`a and Fonollosa, 2016; Lee et al., 2017) as well as for similar ones (Costa-juss`a et al., 2017). The advantage of character-level models is t"
W19-5432,W02-0603,0,0.130135,"con for them. For example, consider that some common inflection produces a slightly different suffix for the two languages. A joint lexicon is likely to have both suffixes as subword units. Then the suffix for language A may interfere with the segmentation of stems of language B that happen to contain the same string, and vice versa. Cognate Morfessor can avoid such problems by keeping the suffixes in separate lexicons. the segmentation algorithm is free of hyperparameters. However, character-level NMT models tend to be slow due to the greater length of the sequences. 2.2 Morfessor Morfessor (Creutz and Lagus, 2002, 2007) is a method for unsupervised morphological segmentation. In contrast to the byte-pair encoding (BPE) algorithm widely adopted in neural machine translation (Sennrich et al., 2016), Morfessor defines a proper statistical model and applies maximum a posteriori estimation for the model parameters. The granularity of the segmentation (and thus size of the subword lexicon) is tunable by inserting a hyperparameter for varying the balance between prior and data likelihood (Kohonen et al., 2010). The prior can be considered as a encoding cost for the subword lexicon, and the likelihood as enco"
W19-5432,W17-4727,1,0.906874,"Missing"
W19-5432,Q17-1026,0,0.0290544,"et al., 2016), Morfessor (Creutz and Lagus, 2007; Gr¨onroos et al., 2014), or SentencePiece (Kudo and Richardson, 2018) yield improved consistency by concatenat2.1 Character segmentation For similar languages, a commonly used segmentation scheme is character-level segmentation, where every character, including the space character, is considered independently. The idea of character-level machine translation for similar languages dates back to SMT times (e.g. Tiedemann, 2009). More recently, character-level NMT has shown promising results for distant languages (Costa-juss`a and Fonollosa, 2016; Lee et al., 2017) as well as for similar ones (Costa-juss`a et al., 2017). The advantage of character-level models is that they do not require any other type of preprocessing such as tokenization or truecasing, and that 236 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 236–244 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics Morfessor Baseline (consisting of a lexicon and corpus coding costs) by three lexicon and corpus costs: one for each language, and one for edit operations that transform the cognate forms bet"
W19-5432,W18-6410,1,0.875519,"Missing"
W19-5432,P02-1040,0,0.107805,"tion toolkit – OpenNMT-py (Klein et al., 2017) –, use the same model architecture – the Transformer (Vaswani et al., 2017) –, and the same hyperparameters4 . Training data are shuffled beforehand. We set a threshold in terms of epochs for each translation direction, after which we stop model training.5 This allows us to compare models fairly, as they have all seen the same amount of training data, which is not guaranteed when relying on training time or number of batches. Results on the development set are shown in Table 3 and discussed in detail below. We report two word-level metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), as well as two character-level metrics, CharacTer (Wang et al., 2016) and chrF (Popovi´c, 2016). BLEU and chrF are computed with SacreBLEU (Post, 2018).6 In order to quantify the impact of preand post-processing, we compute BLEU scores with the unprocessed reference as well as with an additional reference that has been normalized, robustness of the translation. The unigram method by Kudo (2018) is implemented in the SentencePiece software (Kudo and Richardson, 2018). 2.5 Byte pair encoding In Sennrich et al. (2016) the authors adapt the byte pair encoding (BPE)"
W19-5432,C14-1111,1,0.901066,"Missing"
W19-5432,W16-2341,0,0.0699623,"Missing"
W19-5432,W18-6319,0,0.0135154,"d. We set a threshold in terms of epochs for each translation direction, after which we stop model training.5 This allows us to compare models fairly, as they have all seen the same amount of training data, which is not guaranteed when relying on training time or number of batches. Results on the development set are shown in Table 3 and discussed in detail below. We report two word-level metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), as well as two character-level metrics, CharacTer (Wang et al., 2016) and chrF (Popovi´c, 2016). BLEU and chrF are computed with SacreBLEU (Post, 2018).6 In order to quantify the impact of preand post-processing, we compute BLEU scores with the unprocessed reference as well as with an additional reference that has been normalized, robustness of the translation. The unigram method by Kudo (2018) is implemented in the SentencePiece software (Kudo and Richardson, 2018). 2.5 Byte pair encoding In Sennrich et al. (2016) the authors adapt the byte pair encoding (BPE) data compression algorithm (Gage, 1994) to the task of word segmentation. They use the idea of the original algorithm, iteratively replacing the most frequent pair of bytes in a seque"
W19-5432,D17-1266,0,0.0259837,"of the parameter space. Character-level models proved to be competitive for translation between Spanish and Portuguese, but they are slower in training and decoding. 1 2 Introduction Subword segmentation Our experiments focused on four subword segmentation methods, which are summarized shortly in this section. Machine translation between closely related languages is, in principle, less challenging than translation between distantly related ones. Sharing large parts of their grammars and vocabularies reduces the amount of effort needed for a machine translation system to be able to generalize (Pourdamghani and Knight, 2017). Nevertheless, and especially since the languages offered in this shared task are to some extent morphologically complex, we assume that proper subword segmentation will be beneficial for neural machine translation (NMT) performance. In particular, we aim at consistent segmentation across both related languages. While generic subword segmentation methods such as BPE (Sennrich et al., 2016), Morfessor (Creutz and Lagus, 2007; Gr¨onroos et al., 2014), or SentencePiece (Kudo and Richardson, 2018) yield improved consistency by concatenat2.1 Character segmentation For similar languages, a commonly"
W19-5432,W16-2342,0,0.0155825,"ani et al., 2017) –, and the same hyperparameters4 . Training data are shuffled beforehand. We set a threshold in terms of epochs for each translation direction, after which we stop model training.5 This allows us to compare models fairly, as they have all seen the same amount of training data, which is not guaranteed when relying on training time or number of batches. Results on the development set are shown in Table 3 and discussed in detail below. We report two word-level metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), as well as two character-level metrics, CharacTer (Wang et al., 2016) and chrF (Popovi´c, 2016). BLEU and chrF are computed with SacreBLEU (Post, 2018).6 In order to quantify the impact of preand post-processing, we compute BLEU scores with the unprocessed reference as well as with an additional reference that has been normalized, robustness of the translation. The unigram method by Kudo (2018) is implemented in the SentencePiece software (Kudo and Richardson, 2018). 2.5 Byte pair encoding In Sennrich et al. (2016) the authors adapt the byte pair encoding (BPE) data compression algorithm (Gage, 1994) to the task of word segmentation. They use the idea of the or"
W19-5432,W18-6425,1,0.853179,"aseline (α = 0.05) Cognate Morfessor (α = 0.01) 52.8 51.0 52.0 28.6 33.1 29.4 PT → ES Characters Morfessor Baseline (α = 0.05) Cognate Morfessor (α = 0.01) 59.1 58.6 58.4 25.5 25.1 25.3 CS → PL Characters Morfessor Baseline (α = 0.05) Cognate Morfessor (α = 0.01) 5.9 7.0 7.1 88.4 87.3 87.4 PL → CS Characters Morfessor Baseline (α = 0.05) Cognate Morfessor (α = 0.01) 6.6 7.2 7.0 80.2 79.6 79.4 4.4 SentencePiece unigram models We trained the segmentation models only on the available parallel datasets for each language pair, following the findings of our submission to the WMT18 translation task (Raganato et al., 2018). We specified a vocabulary size of 5,000 tokens for each language and we took advantage from the tokenizer integrated in the SentencePiece implementation (Kudo and Richardson, 2018) by training the models on non-tokenized data. We applied the same truecasing models as before. Results reported in Table 3 show that the models trained on SentencePiece-encoded data are consistently behind the Morfessor Baseline and Cognate Morfessor ones, except for the Spanish– Portuguese translation direction. This might be caused by the choice of vocabulary size used and the selected epoch in the table. These"
W19-5432,P16-1162,0,0.774633,"than translation between distantly related ones. Sharing large parts of their grammars and vocabularies reduces the amount of effort needed for a machine translation system to be able to generalize (Pourdamghani and Knight, 2017). Nevertheless, and especially since the languages offered in this shared task are to some extent morphologically complex, we assume that proper subword segmentation will be beneficial for neural machine translation (NMT) performance. In particular, we aim at consistent segmentation across both related languages. While generic subword segmentation methods such as BPE (Sennrich et al., 2016), Morfessor (Creutz and Lagus, 2007; Gr¨onroos et al., 2014), or SentencePiece (Kudo and Richardson, 2018) yield improved consistency by concatenat2.1 Character segmentation For similar languages, a commonly used segmentation scheme is character-level segmentation, where every character, including the space character, is considered independently. The idea of character-level machine translation for similar languages dates back to SMT times (e.g. Tiedemann, 2009). More recently, character-level NMT has shown promising results for distant languages (Costa-juss`a and Fonollosa, 2016; Lee et al., 2"
W19-5432,2006.amta-papers.25,0,0.115959,"et al., 2017) –, use the same model architecture – the Transformer (Vaswani et al., 2017) –, and the same hyperparameters4 . Training data are shuffled beforehand. We set a threshold in terms of epochs for each translation direction, after which we stop model training.5 This allows us to compare models fairly, as they have all seen the same amount of training data, which is not guaranteed when relying on training time or number of batches. Results on the development set are shown in Table 3 and discussed in detail below. We report two word-level metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), as well as two character-level metrics, CharacTer (Wang et al., 2016) and chrF (Popovi´c, 2016). BLEU and chrF are computed with SacreBLEU (Post, 2018).6 In order to quantify the impact of preand post-processing, we compute BLEU scores with the unprocessed reference as well as with an additional reference that has been normalized, robustness of the translation. The unigram method by Kudo (2018) is implemented in the SentencePiece software (Kudo and Richardson, 2018). 2.5 Byte pair encoding In Sennrich et al. (2016) the authors adapt the byte pair encoding (BPE) data compression algorithm (Ga"
W19-5432,C18-1112,0,0.0206243,"results with the two references may vary by up to 2 points. Despite the large amounts of available training data, we chose hyperparameters resulting in rather small vocabulary sizes for all subword splitting schemes, ranging between 2800 and 8900 units per language pair. This choice was guided by three reasons: (1) the competitive performance of character-level models, (2) the desire to force the models to split words across languages, and to do so not only for rare words, and (3) the competitive performance of small vocabulary sizes in related problems such as historical text normalization (Tang et al., 2018). A general finding, shared by the other participants, is that the scores on the Slavic language pair are much lower than on the Romance language pair. We assume that the Spanish–Portuguese development and test sets are built by translating directly from one language to the other, whereas the Czech–Polish development and test sets had been translated from English independently of each other, leading to much freer translations. If this hypothesis is correct, the automatic evaluation scores for Czech–Polish may in fact underestimate the real translation quality. 4.1 4.3 The Cognate Morfessor tra"
W19-5432,2009.eamt-1.3,0,0.0287233,"icular, we aim at consistent segmentation across both related languages. While generic subword segmentation methods such as BPE (Sennrich et al., 2016), Morfessor (Creutz and Lagus, 2007; Gr¨onroos et al., 2014), or SentencePiece (Kudo and Richardson, 2018) yield improved consistency by concatenat2.1 Character segmentation For similar languages, a commonly used segmentation scheme is character-level segmentation, where every character, including the space character, is considered independently. The idea of character-level machine translation for similar languages dates back to SMT times (e.g. Tiedemann, 2009). More recently, character-level NMT has shown promising results for distant languages (Costa-juss`a and Fonollosa, 2016; Lee et al., 2017) as well as for similar ones (Costa-juss`a et al., 2017). The advantage of character-level models is that they do not require any other type of preprocessing such as tokenization or truecasing, and that 236 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 236–244 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics Morfessor Baseline (consisting of a lexicon and corp"
W19-5432,P12-3005,0,\N,Missing
W19-5432,P17-4012,0,\N,Missing
W19-5432,W17-4733,1,\N,Missing
W19-5432,P16-1009,0,\N,Missing
W19-5432,W15-1844,0,\N,Missing
