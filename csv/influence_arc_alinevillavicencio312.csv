2020.cogalex-1.9,N19-4010,0,0.0184949,"ks well for MWEs that are either idiomatic or literal but falls short for idiomaticity identification at the token level when the MWE is ambiguous. Nandakumar et al. (2019) used different types of contextualized and non-contextualized word embeddings from character-level to word-level models to investigate the capability of such models in detecting nuances of non-compositionality in MWEs. When evaluating the models, they considered the MWEs out of their context which is problematic especially in case of utilizing CWEs as the reason behind the success (Peters et al., 2018; Devlin et al., 2019; Akbik et al., 2019) of these models is in their ability to produce context-specific embeddings for each token. The main drawback of above-mentioned works is that they do not take the context of each individual expression into account when classifying them. However, there have been some attempts to detect 73 idiomaticity in MWEs in context (at token level) using Distributional Models. Peng et al. (2015) exploited contextual information captured in word embeddings to automatically recognize idiomatic tokens. They calculate the inner product of the embeddings of the context words with the embedding of target expres"
2020.cogalex-1.9,N10-1029,0,0.0436694,"oit the capacity of CWEs, an idiom should be treated as a single token both in training and testing. This hypothesis is inspired by the evidence from psycholinguistic studies which support the idea that the idiomatic expressions are stored and retrieved as a whole from memory at the time of use (Siyanova-Chanturia and Martinez, 2014). It is also rooted in the idea that different types of information are captured in vectors depending on the type of input, i.e. word, character, phrase to the model (Schick and Sch¨utze, 2019). Moreover, this method proved successful in other tasks. For instance, Carpuat and Diab (2010) conducted a study for integrating MWEs in Statistical Machine Translation (SMT) and improved the BLEU score by treating MWEs as single tokens in training and testing. Contribution: We show that CWEs can be utilized directly to detect idiomaticity in potentially idiomatic expressions due to their nature of providing distinct vectors for the same expression depending on its context. We also apply the Idiom Principle (Sinclair et al., 1991) when training the models which improves the results as expected and supports our hypothesis that an MWE should be treated as a single token both in training"
2020.cogalex-1.9,J17-4005,0,0.307033,"Missing"
2020.cogalex-1.9,W07-1106,0,0.0680707,"antic Models (DSM) are computational models based on the Distributional Hypothesis (Harris, 1954) and the idea that words occurring in similar contexts tend to have a similar meaning. Recently, two flavours of Distributional Models have been introduced and utilized which are known as contextualized and non-contextualized embedding models. The former produces different embeddings for a word depending on the context and the latter offers only one embedding for a word regardless of the context. Researchers have leveraged DSMs along with linguistic knowledge to deal with identifying MWEs at type (Cook et al., 2007; Cordeiro et al., 2016; Nandakumar et al., 2019) and token level (King and Cook, 2018; Rohanian et al., 2020). For instance, the degree of linguistic fixedness was used as the basis for Fazly and Stevenson (2006) to apply an unsupervised method to distinguish between idiomatic and literal tokens of verb-noun combinations (VNCs). They argue that idiomatic VNCs come in fixed syntactic forms in terms of passivation, determiner, and noun pluralization. They extracted these forms using known idiomatic/literal VNC patterns and among all variations they determined which were the canonical form(s). T"
2020.cogalex-1.9,P16-1187,1,0.939944,"capable of producing suitable substitutes for ambiguous expressions in context which is promising for downstream tasks like text simplification. 1 Introduction The task of determining whether a sequence of words (a Multiword Expression - MWE) is idiomatic has received lots of attention (Fazly and Stevenson, 2006; Cook et al., 2007). Especially for MWE type idiomaticity identification (Constant et al., 2017), where the goal is to decide if an MWE can be idiomatic regardless of context, high agreement with human judgments has been achieved, for instance, for compound nouns (Reddy et al., 2011; Cordeiro et al., 2016). However, as this task does not take context into account, these techniques have limited success in the case of ambiguous MWEs where the same expression can be literal or idiomatic depending on a particular context. For example, such models would always classify hit the road as idiomatic (or conversely always as literal) while the expression could be idiomatic in one context and literal in another. As a consequence, for practical NLP tasks, especially Machine Translation and Information Retrieval, token idiomaticity identification is needed, with the classification of a potential idioms as li"
2020.cogalex-1.9,E06-1043,0,0.416003,"rs of Distributional Models have been introduced and utilized which are known as contextualized and non-contextualized embedding models. The former produces different embeddings for a word depending on the context and the latter offers only one embedding for a word regardless of the context. Researchers have leveraged DSMs along with linguistic knowledge to deal with identifying MWEs at type (Cook et al., 2007; Cordeiro et al., 2016; Nandakumar et al., 2019) and token level (King and Cook, 2018; Rohanian et al., 2020). For instance, the degree of linguistic fixedness was used as the basis for Fazly and Stevenson (2006) to apply an unsupervised method to distinguish between idiomatic and literal tokens of verb-noun combinations (VNCs). They argue that idiomatic VNCs come in fixed syntactic forms in terms of passivation, determiner, and noun pluralization. They extracted these forms using known idiomatic/literal VNC patterns and among all variations they determined which were the canonical form(s). Then they classified new tokens as idiomatic if they appeared in their canonical forms. Cook et al. (2007) leveraged the idea of canonical forms and the Distributional Hypothesis and built co-occurrence vectors rep"
2020.cogalex-1.9,D18-1060,0,0.024434,"e context and showed competitive results. King and Cook (2018) proposed a model based on distributed representations, non-CWE to classify VNC usages as idiomatic/literal. First, they represented the context as the average embeddings of context words and trained a Support Vector Machines (SVM) classifier on top of that. They further showed that incorporating the information about the expressions canonical forms boosted the performance of their model. A related task of metaphor token detection has seen successful results with the combination of CWEs and non-CWEs, along with linguistic features (Gao et al., 2018; Mao et al., 2019). For instance, Gao et al. (2018) used Word2Vec and ELMo (Peters et al., 2018) as embeddings, with a bidirectional LSTM to encode sentences, and a feed-forward neural network for classifying them as literal or metaphoric. Rohanian et al. (2020) presented a neural model and BERT, to classify metaphorical verbs in their sentential context using information from the dependency parse tree and annotations for verbal MWEs. They showed that incorporating the knowledge of MWEs can enhance the performance of a metaphor classification model. We follow the intuition that CWEs can be di"
2020.cogalex-1.9,D19-1533,0,0.0204847,"reverse”, and “The Ulster Society are about to hit the road on one of their magical history tours” (Burnard, 2000). We argue that successful classification of potentially idiomatic expressions as idiomatic/literal is not possible without taking the context into account. Recently introduced Contextualized Word Embeddings (CWEs) are ideal for this task as they can provide different embeddings for each instance of the same word type. CWEs such as Context2Vec (Melamud et al., 2016) and BERT (Devlin et al., 2019) proved successful in the task of Word Sense Disambiguation (WSD) (Huang et al., 2019; Hadiwinoto et al., 2019). We also argue that disambiguation of potentially idiomatic expressions is analogous to WSD in a sense that it also tries to assign the most appropriate sense to an idiom, i.e. literal, or idiomatic depending on its respective context. ∗ This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 72 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 72–80 Barcelona, Spain (Online), December 12, 2020 Moreover, we hypothesize that in order to fully exploit the capacity of CWEs, an idiom s"
2020.cogalex-1.9,D15-1162,0,0.0115575,"(Figure 2) models. 75 4 Experimental Setup To test the hypothesis, we build 6 different context representations using three embedding models: Word2Vec, Context2Vec and BERT in two different settings: 1- Their original models where each expression is not treated as a single token 2- Our own models, which we call Idiom-Principle-Inspired models, where each expression is treated as a single token. For the first setting, we use the original pre-trained models, Word2Vec, Context2Vec and BERT-baseuncased. For the second setting, we use BNC corpus (Burnard, 2000) and first lemmatize it using spaCy (Honnibal and Johnson, 2015). Then, we tokenize it where each MWE is treated as a single token with an underline between the first and the second part (e.g. blow the whistle is mapped to blow whistle). Finally, we build three semantic spaces, using Word2Vec, Context2Vec, and BERT. Our goal is to determine the correct sense of an MWE in context, based on a manually tagged dataset, VNC (Cook et al., 2008). Following Melamud et al. (2016), we use the simple non-parametric version of the kNN classification algorithm (Cover and Hart, 1967) with k = 1 and for the distance measure, we rely on cosine distance of the vectors. As"
2020.cogalex-1.9,D19-1355,0,0.0214182,"than I was able to reverse”, and “The Ulster Society are about to hit the road on one of their magical history tours” (Burnard, 2000). We argue that successful classification of potentially idiomatic expressions as idiomatic/literal is not possible without taking the context into account. Recently introduced Contextualized Word Embeddings (CWEs) are ideal for this task as they can provide different embeddings for each instance of the same word type. CWEs such as Context2Vec (Melamud et al., 2016) and BERT (Devlin et al., 2019) proved successful in the task of Word Sense Disambiguation (WSD) (Huang et al., 2019; Hadiwinoto et al., 2019). We also argue that disambiguation of potentially idiomatic expressions is analogous to WSD in a sense that it also tries to assign the most appropriate sense to an idiom, i.e. literal, or idiomatic depending on its respective context. ∗ This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 72 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 72–80 Barcelona, Spain (Online), December 12, 2020 Moreover, we hypothesize that in order to fully exploit the ca"
2020.cogalex-1.9,P19-1378,0,0.012804,"ed competitive results. King and Cook (2018) proposed a model based on distributed representations, non-CWE to classify VNC usages as idiomatic/literal. First, they represented the context as the average embeddings of context words and trained a Support Vector Machines (SVM) classifier on top of that. They further showed that incorporating the information about the expressions canonical forms boosted the performance of their model. A related task of metaphor token detection has seen successful results with the combination of CWEs and non-CWEs, along with linguistic features (Gao et al., 2018; Mao et al., 2019). For instance, Gao et al. (2018) used Word2Vec and ELMo (Peters et al., 2018) as embeddings, with a bidirectional LSTM to encode sentences, and a feed-forward neural network for classifying them as literal or metaphoric. Rohanian et al. (2020) presented a neural model and BERT, to classify metaphorical verbs in their sentential context using information from the dependency parse tree and annotations for verbal MWEs. They showed that incorporating the knowledge of MWEs can enhance the performance of a metaphor classification model. We follow the intuition that CWEs can be directly used for the"
2020.cogalex-1.9,K16-1006,0,0.335581,"ust be translated differently in “The bullets were hitting the road and I could see them coming towards me a lot faster than I was able to reverse”, and “The Ulster Society are about to hit the road on one of their magical history tours” (Burnard, 2000). We argue that successful classification of potentially idiomatic expressions as idiomatic/literal is not possible without taking the context into account. Recently introduced Contextualized Word Embeddings (CWEs) are ideal for this task as they can provide different embeddings for each instance of the same word type. CWEs such as Context2Vec (Melamud et al., 2016) and BERT (Devlin et al., 2019) proved successful in the task of Word Sense Disambiguation (WSD) (Huang et al., 2019; Hadiwinoto et al., 2019). We also argue that disambiguation of potentially idiomatic expressions is analogous to WSD in a sense that it also tries to assign the most appropriate sense to an idiom, i.e. literal, or idiomatic depending on its respective context. ∗ This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 72 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pag"
2020.cogalex-1.9,W19-2004,0,0.0856826,"s based on the Distributional Hypothesis (Harris, 1954) and the idea that words occurring in similar contexts tend to have a similar meaning. Recently, two flavours of Distributional Models have been introduced and utilized which are known as contextualized and non-contextualized embedding models. The former produces different embeddings for a word depending on the context and the latter offers only one embedding for a word regardless of the context. Researchers have leveraged DSMs along with linguistic knowledge to deal with identifying MWEs at type (Cook et al., 2007; Cordeiro et al., 2016; Nandakumar et al., 2019) and token level (King and Cook, 2018; Rohanian et al., 2020). For instance, the degree of linguistic fixedness was used as the basis for Fazly and Stevenson (2006) to apply an unsupervised method to distinguish between idiomatic and literal tokens of verb-noun combinations (VNCs). They argue that idiomatic VNCs come in fixed syntactic forms in terms of passivation, determiner, and noun pluralization. They extracted these forms using known idiomatic/literal VNC patterns and among all variations they determined which were the canonical form(s). Then they classified new tokens as idiomatic if th"
2020.cogalex-1.9,R15-1066,0,0.0204122,"WEs. When evaluating the models, they considered the MWEs out of their context which is problematic especially in case of utilizing CWEs as the reason behind the success (Peters et al., 2018; Devlin et al., 2019; Akbik et al., 2019) of these models is in their ability to produce context-specific embeddings for each token. The main drawback of above-mentioned works is that they do not take the context of each individual expression into account when classifying them. However, there have been some attempts to detect 73 idiomaticity in MWEs in context (at token level) using Distributional Models. Peng et al. (2015) exploited contextual information captured in word embeddings to automatically recognize idiomatic tokens. They calculate the inner product of the embeddings of the context words with the embedding of target expression. They argue that since the literal forms can predict the local context better, their inner product with context words is larger than that of idiomatic ones, hence they tell apart literals from idiomatic forms. Salton et al. (2016) exploited Skip-Thought Vectors (Kiros et al., 2015) to represent the sentential context of an MWE and used SVM and K-Nearest Neighbours to classify MW"
2020.cogalex-1.9,N18-1202,0,0.136358,"identification model at the type level works well for MWEs that are either idiomatic or literal but falls short for idiomaticity identification at the token level when the MWE is ambiguous. Nandakumar et al. (2019) used different types of contextualized and non-contextualized word embeddings from character-level to word-level models to investigate the capability of such models in detecting nuances of non-compositionality in MWEs. When evaluating the models, they considered the MWEs out of their context which is problematic especially in case of utilizing CWEs as the reason behind the success (Peters et al., 2018; Devlin et al., 2019; Akbik et al., 2019) of these models is in their ability to produce context-specific embeddings for each token. The main drawback of above-mentioned works is that they do not take the context of each individual expression into account when classifying them. However, there have been some attempts to detect 73 idiomaticity in MWEs in context (at token level) using Distributional Models. Peng et al. (2015) exploited contextual information captured in word embeddings to automatically recognize idiomatic tokens. They calculate the inner product of the embeddings of the context"
2020.cogalex-1.9,I11-1024,0,0.0224936,"y. The model is also capable of producing suitable substitutes for ambiguous expressions in context which is promising for downstream tasks like text simplification. 1 Introduction The task of determining whether a sequence of words (a Multiword Expression - MWE) is idiomatic has received lots of attention (Fazly and Stevenson, 2006; Cook et al., 2007). Especially for MWE type idiomaticity identification (Constant et al., 2017), where the goal is to decide if an MWE can be idiomatic regardless of context, high agreement with human judgments has been achieved, for instance, for compound nouns (Reddy et al., 2011; Cordeiro et al., 2016). However, as this task does not take context into account, these techniques have limited success in the case of ambiguous MWEs where the same expression can be literal or idiomatic depending on a particular context. For example, such models would always classify hit the road as idiomatic (or conversely always as literal) while the expression could be idiomatic in one context and literal in another. As a consequence, for practical NLP tasks, especially Machine Translation and Information Retrieval, token idiomaticity identification is needed, with the classification of"
2020.cogalex-1.9,2020.acl-main.259,0,0.164592,"idea that words occurring in similar contexts tend to have a similar meaning. Recently, two flavours of Distributional Models have been introduced and utilized which are known as contextualized and non-contextualized embedding models. The former produces different embeddings for a word depending on the context and the latter offers only one embedding for a word regardless of the context. Researchers have leveraged DSMs along with linguistic knowledge to deal with identifying MWEs at type (Cook et al., 2007; Cordeiro et al., 2016; Nandakumar et al., 2019) and token level (King and Cook, 2018; Rohanian et al., 2020). For instance, the degree of linguistic fixedness was used as the basis for Fazly and Stevenson (2006) to apply an unsupervised method to distinguish between idiomatic and literal tokens of verb-noun combinations (VNCs). They argue that idiomatic VNCs come in fixed syntactic forms in terms of passivation, determiner, and noun pluralization. They extracted these forms using known idiomatic/literal VNC patterns and among all variations they determined which were the canonical form(s). Then they classified new tokens as idiomatic if they appeared in their canonical forms. Cook et al. (2007) leve"
2020.cogalex-1.9,P16-1019,0,0.179764,"when classifying them. However, there have been some attempts to detect 73 idiomaticity in MWEs in context (at token level) using Distributional Models. Peng et al. (2015) exploited contextual information captured in word embeddings to automatically recognize idiomatic tokens. They calculate the inner product of the embeddings of the context words with the embedding of target expression. They argue that since the literal forms can predict the local context better, their inner product with context words is larger than that of idiomatic ones, hence they tell apart literals from idiomatic forms. Salton et al. (2016) exploited Skip-Thought Vectors (Kiros et al., 2015) to represent the sentential context of an MWE and used SVM and K-Nearest Neighbours to classify MWEs as idiomatic or literal in their context. They compared their work against a topic model representation that include the full paragraph as the context and showed competitive results. King and Cook (2018) proposed a model based on distributed representations, non-CWE to classify VNC usages as idiomatic/literal. First, they represented the context as the average embeddings of context words and trained a Support Vector Machines (SVM) classifier"
2020.sltu-1.11,N16-1109,0,0.0691211,"Missing"
2020.sltu-1.11,P06-1085,0,0.0751927,"can be used for word segmentation and morphological analysis, being known as very robust in low-resource settings (Godard et al., 2016; Goldwater et al., 2009a). In these monolingual models, words are generated by a uni or bigram model over a non-finite inventory, through the use of a Dirichlet process. Although providing reliable segmentation in low-resource settings, these monolingual models are incapable of automatically producing alignments with a foreign language, and therefore the discovered pseudoword segments can be seen as “meaningless”. Godard et al. (2018) also showed that dpseg2 (Goldwater et al., 2006; Goldwater et al., 2009a) behaves poorly on pseudophone units discovered from speech, which limits its application. Here, we investigate its use as an intermediate monolingual-rooted segmentation system, whose discovered boundaries are used as clues by bilingual models. 3. Experimental Settings Multilingual Dataset: For our experiments we use the MaSS dataset (Boito et al., 2020), a fully aligned and multilingual dataset containing 8,130 sentences extracted 2 Available at http://homepages.inf.ed.ac.uk/ sgwater/resources.html 80 Figure 1: An illustration of the hybrid pipeline for the EN&gt;RO la"
2020.sltu-1.11,N09-1036,0,0.0449523,"the dpseg output boundaries. In this augmented input representation, illustrated in Figure 1, a boundary is denoted by a special token which separates the words identified by dpseg. We call this soft-boundary insertion, since the dpseg boundaries inserted into the phoneme sequence can be ignored by the NMT model, and new boundaries can be inserted as well. For instance, in Figure 1 aintrat becomes a intrat (boundary insertion), and urat debine becomes uratdebine (soft-boundary removal). Models for Word Segmentation Monolingual Bayesian Approach Non-parametric Bayesian models (Goldwater, 2007; Johnson and Goldwater, 2009) are statistical approaches that can be used for word segmentation and morphological analysis, being known as very robust in low-resource settings (Godard et al., 2016; Goldwater et al., 2009a). In these monolingual models, words are generated by a uni or bigram model over a non-finite inventory, through the use of a Dirichlet process. Although providing reliable segmentation in low-resource settings, these monolingual models are incapable of automatically producing alignments with a foreign language, and therefore the discovered pseudoword segments can be seen as “meaningless”. Godard et al."
2020.sltu-1.11,W10-2912,0,0.0352998,"Missing"
2020.sltu-1.11,2020.lrec-1.799,1,0.824529,"Missing"
2020.sltu-1.11,P02-1040,0,0.10942,"Missing"
2020.sltu-1.11,strunk-etal-2014-untrained,0,0.0772016,"Missing"
2021.acl-long.212,N19-4010,0,0.0550351,"Missing"
2021.acl-long.212,2020.cl-1.4,1,0.834217,"en idiomaticity for NCs in two languages, (2) evaluating to what extent models are able to detect idiomaticity at type and token level, analysing different levels of contextualisation and (3) proposing two new measures of idiomaticity. Moreover, the paraphrases provided for each NC at type and token level make NCTTI a useful resource for enhancing paraphrase datasets (e.g. PPDB (Ganitkevitch et al., 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020). Such paraphrases may also be useful for improving the task of machine translation, avoiding the need for parallel MWE corpora (Zaninello and Birch, 2020). Section 2 gives an overview of existing idiomaticity datasets. Section 3 presents the NCTTI dataset and the annotations, and section 4 discusses the evaluation of the performance of different word embeddings in detecting idiomaticity. We used as basis the English and Portuguese subsets of the NC Compositionality dataset (Cordeiro et al., 2019), which contain compositionality scores for 280 two-word NCs in English (90 of which came from Red"
2021.acl-long.212,N19-1050,0,0.0201961,"most 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and Dagan (2019) train various classifiers initialised with static and contextualised embeddings for different compositional tasks, achieving the best results with BERT embeddings. Yu and Ettinger (2020), using partially idiomatic expressions of the BiRD dataset (Asaadi et al., 2019), show that contextualised embeddings from language models heavily rely on word content, missing additional information provided by compositional operations. In this paper we take advantage of the NCTTI dataset to observe whether vector representations obtained with different strategies correlate with human annotations at both type and token levels. 3 The Noun Compound Type and Token Idiomaticity dataset This section describes the procedure to create the NCTTI dataset and its main characteristics.2 3.1 2731 Source data compositional) after reading various sentences with this NC. To obtain more"
2021.acl-long.212,J90-1003,0,0.334862,"of context and of the collocated component. It is implemented as the sum of the individual vectors of the NC components, where each NC component is fed individually to the model as a sentence, referred to as NC outComp . On each case, we calculate two Spearman correlations with human judgments: at token level, using all the sentences for each language; and at type level, comparing the average cosine similarities of each NC with their compositionality scores at type level. We also compute correlations between the similarities and frequency-based data, namely the NC raw frequency, and the PPMI (Church and Hanks, 1990) between its component words, to verify whether they have any impact in these measures of idiomaticity. The frequency data were obtained from ukWaC, with 2.25B tokens in English (Baroni et al., 2009), and brWaC, containing 2.7B tokens in Portuguese (Wagner Filho et al., 2018). The results by Cordeiro et al. (2019) suggested that if the two components of an NC are processed as a single token unit (for instance, by explic9 This representation equivalent to the Avg Phrase used by Yu and Ettinger (2020). itly linking them with an underscore) the resulting static representation captures the NC idio"
2021.acl-long.212,J19-1001,1,0.069189,"evel, to determine the potential of an MWE to be idiomatic in general. Some of these approaches are based on the assumption that the * Equal contribution. distance between the representation of an MWE as a unit and the representation of the compositional combination of its components is an indication of the degree of idiomaticity: they are closer if the MWE is more compositional. Good performances are obtained even with non-contextualised word embeddings like word2vec (Mikolov et al., 2013), and vector operations like addition and multiplication (Mitchell and Lapata, 2010; Reddy et al., 2011; Cordeiro et al., 2019). Additionally, for some MWEs, there is a potential ambiguity between an idiomatic and a literal sense, like in the potentially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occ"
2021.acl-long.212,N19-1423,0,0.118084,"tially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016). Contextualised embedding models, like BERT (Devlin et al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and their combination with contextualised models could improve results (e.g. for metaphor identification (Mao et al."
2021.acl-long.212,W15-0904,0,0.0253873,"dataset (Cordeiro et al., 2019), which contain compositionality scores for 280 two-word NCs in English (90 of which came from Reddy et al. (2011)), and 180 in Portuguese, all of them labeled at type level: i.e., the annotators provided a compositionality value for a compound (from 0 –fully idiomatic– to 5, fully 1 Type level annotations come from Cordeiro et al. (2019), the dataset used as source for the NCTTI. 2 The NCCTI dataset can be downloaded from the following url: https://github.com/marcospln/nctti. 2 Related Work Datasets with type-level annotations are available for NCs in English (Farahmand et al., 2015; Reddy et al., 2011; Ramisch et al., 2016; Kruszewski and Baroni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better"
2021.acl-long.212,N13-1092,0,0.0790478,"Missing"
2021.acl-long.212,D18-1060,0,0.0195993,"whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016). Contextualised embedding models, like BERT (Devlin et al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and their combination with contextualised models could improve results (e.g. for metaphor identification (Mao et al., 2019)). This complementarity between non-contextualised and contex2730 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2730–2741 August 1–6, 2021. ©20"
2021.acl-long.212,W17-6615,0,0.0552581,"Missing"
2021.acl-long.212,D19-1630,0,0.0466687,"Missing"
2021.acl-long.212,P18-2055,0,0.0160492,"re is a potential ambiguity between an idiomatic and a literal sense, like in the potentially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016). Contextualised embedding models, like BERT (Devlin et al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and their combination with c"
2021.acl-long.212,S14-1021,0,0.0313475,"cores for 280 two-word NCs in English (90 of which came from Reddy et al. (2011)), and 180 in Portuguese, all of them labeled at type level: i.e., the annotators provided a compositionality value for a compound (from 0 –fully idiomatic– to 5, fully 1 Type level annotations come from Cordeiro et al. (2019), the dataset used as source for the NCTTI. 2 The NCCTI dataset can be downloaded from the following url: https://github.com/marcospln/nctti. 2 Related Work Datasets with type-level annotations are available for NCs in English (Farahmand et al., 2015; Reddy et al., 2011; Ramisch et al., 2016; Kruszewski and Baroni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and"
2021.acl-long.212,P19-1378,0,0.0129047,"al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and their combination with contextualised models could improve results (e.g. for metaphor identification (Mao et al., 2019)). This complementarity between non-contextualised and contex2730 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2730–2741 August 1–6, 2021. ©2021 Association for Computational Linguistics tualised models may be an indication that enough core idiomatic information may already be available at type level. Moreover, type-based compositionality prediction measures that perform well with static embeddings may also perform well for token-based prediction with contextualised mode"
2021.acl-long.212,S07-1009,0,0.0963826,"from the performance obtained per token. Our contributions can be summarised as: (1) building the NCTTI dataset with information about type and token idiomaticity for NCs in two languages, (2) evaluating to what extent models are able to detect idiomaticity at type and token level, analysing different levels of contextualisation and (3) proposing two new measures of idiomaticity. Moreover, the paraphrases provided for each NC at type and token level make NCTTI a useful resource for enhancing paraphrase datasets (e.g. PPDB (Ganitkevitch et al., 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020). Such paraphrases may also be useful for improving the task of machine translation, avoiding the need for parallel MWE corpora (Zaninello and Birch, 2020). Section 2 gives an overview of existing idiomaticity datasets. Section 3 presents the NCTTI dataset and the annotations, and section 4 discusses the evaluation of the performance of different word embeddings in detecting idiomaticity. We used as basis the English and Portuguese subsets of the N"
2021.acl-long.212,S10-1002,0,0.0483478,"d per token. Our contributions can be summarised as: (1) building the NCTTI dataset with information about type and token idiomaticity for NCs in two languages, (2) evaluating to what extent models are able to detect idiomaticity at type and token level, analysing different levels of contextualisation and (3) proposing two new measures of idiomaticity. Moreover, the paraphrases provided for each NC at type and token level make NCTTI a useful resource for enhancing paraphrase datasets (e.g. PPDB (Ganitkevitch et al., 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020). Such paraphrases may also be useful for improving the task of machine translation, avoiding the need for parallel MWE corpora (Zaninello and Birch, 2020). Section 2 gives an overview of existing idiomaticity datasets. Section 3 presents the NCTTI dataset and the annotations, and section 4 discusses the evaluation of the performance of different word embeddings in detecting idiomaticity. We used as basis the English and Portuguese subsets of the NC Compositionality datas"
2021.acl-long.212,W19-2004,0,0.198183,"ithub.com/marcospln/nctti. 2 Related Work Datasets with type-level annotations are available for NCs in English (Farahmand et al., 2015; Reddy et al., 2011; Ramisch et al., 2016; Kruszewski and Baroni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and Dagan (2019) train various classifiers initialised with static and contextualised embeddings for different compositional tasks, achieving the best results with BERT embeddings. Yu and Ettinger (2020), using partially idiomatic expressions of the BiRD dataset (Asaadi et al., 2019), show that contextualised embeddings from language models heavily rely on word content, missing additional information provided by comp"
2021.acl-long.212,C16-1069,0,0.0133153,"out type and token idiomaticity for NCs in two languages, (2) evaluating to what extent models are able to detect idiomaticity at type and token level, analysing different levels of contextualisation and (3) proposing two new measures of idiomaticity. Moreover, the paraphrases provided for each NC at type and token level make NCTTI a useful resource for enhancing paraphrase datasets (e.g. PPDB (Ganitkevitch et al., 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020). Such paraphrases may also be useful for improving the task of machine translation, avoiding the need for parallel MWE corpora (Zaninello and Birch, 2020). Section 2 gives an overview of existing idiomaticity datasets. Section 3 presents the NCTTI dataset and the annotations, and section 4 discusses the evaluation of the performance of different word embeddings in detecting idiomaticity. We used as basis the English and Portuguese subsets of the NC Compositionality dataset (Cordeiro et al., 2019), which contain compositionality scores for 280 two-word NCs in Engli"
2021.acl-long.212,D14-1162,0,0.0850405,"he ELMo small model provided by Peters et al. (2018), BERT-Large uncased (Devlin et al., 2019), DistilBERT (Sanh et al., 2019), based on BERT-Base and distilled on SQuAD dataset, and SentenceBERT (Reimers and Gurevych, 2019), trained on BERT-Large and both MultiNLI and SNLI.8 For Portuguese we selected the ELMo pre-trained weights provided by Quinta de Castro et al. (2018) and the multilingual versions of the models used for English, namely mBERT (base cased), and both multilingual DistilBERT and Sentence-BERT (Reimers and Gurevych, 2020). As a static noncontextualised baseline we used GloVe (Pennington et al., 2014) (the English official models with 300 dimensions and trained on 840 billion tokens, and the equivalent Portuguese model released by Hartmann et al. (2017)). The vector representations were obtained with the flairNLP framework (Akbik et al., 2019) using the models provided by the transformers library (Wolf et al., 2020). The representations of NCs (and their sentences) were obtained by averaging the word (or subword, if adopted by the model) embeddings. We used the concatenation of the three layers for ELMo and of 8 https://www.nyu.edu/projects/bowman/ multinli/ https://nlp.stanford.edu/projec"
2021.acl-long.212,P16-1019,0,0.0242545,"iguity between an idiomatic and a literal sense, like in the potentially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016). Contextualised embedding models, like BERT (Devlin et al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and their combination with contextualised models c"
2021.acl-long.212,N18-1202,0,0.0511458,"exts to observe how the interpretation of the NCs varies across sentences, and whether this correlates with the contextualised representations produced by various models. More specifically, we assume that, if models adequately incorporate contextual information, the standard deviations of the similarities between the NCs in different contexts should be correlated with those of the human annotators. 4.1 Models We evaluate four contextualised models: three BERT variants, based on the Transformers architecture (Vaswani et al., 2017), and ELMo, which learns word vectors using bidirectional LSTMs (Peters et al., 2018). For English we used the ELMo small model provided by Peters et al. (2018), BERT-Large uncased (Devlin et al., 2019), DistilBERT (Sanh et al., 2019), based on BERT-Base and distilled on SQuAD dataset, and SentenceBERT (Reimers and Gurevych, 2019), trained on BERT-Large and both MultiNLI and SNLI.8 For Portuguese we selected the ELMo pre-trained weights provided by Quinta de Castro et al. (2018) and the multilingual versions of the models used for English, namely mBERT (base cased), and both multilingual DistilBERT and Sentence-BERT (Reimers and Gurevych, 2020). As a static noncontextualised b"
2021.acl-long.212,P16-2026,1,0.842874,"ain compositionality scores for 280 two-word NCs in English (90 of which came from Reddy et al. (2011)), and 180 in Portuguese, all of them labeled at type level: i.e., the annotators provided a compositionality value for a compound (from 0 –fully idiomatic– to 5, fully 1 Type level annotations come from Cordeiro et al. (2019), the dataset used as source for the NCTTI. 2 The NCCTI dataset can be downloaded from the following url: https://github.com/marcospln/nctti. 2 Related Work Datasets with type-level annotations are available for NCs in English (Farahmand et al., 2015; Reddy et al., 2011; Ramisch et al., 2016; Kruszewski and Baroni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individ"
2021.acl-long.212,I11-1024,0,0.0984062,"vestigated at type level, to determine the potential of an MWE to be idiomatic in general. Some of these approaches are based on the assumption that the * Equal contribution. distance between the representation of an MWE as a unit and the representation of the compositional combination of its components is an indication of the degree of idiomaticity: they are closer if the MWE is more compositional. Good performances are obtained even with non-contextualised word embeddings like word2vec (Mikolov et al., 2013), and vector operations like addition and multiplication (Mitchell and Lapata, 2010; Reddy et al., 2011; Cordeiro et al., 2019). Additionally, for some MWEs, there is a potential ambiguity between an idiomatic and a literal sense, like in the potentially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish id"
2021.acl-long.212,D19-1410,0,0.0204799,"ontextual information, the standard deviations of the similarities between the NCs in different contexts should be correlated with those of the human annotators. 4.1 Models We evaluate four contextualised models: three BERT variants, based on the Transformers architecture (Vaswani et al., 2017), and ELMo, which learns word vectors using bidirectional LSTMs (Peters et al., 2018). For English we used the ELMo small model provided by Peters et al. (2018), BERT-Large uncased (Devlin et al., 2019), DistilBERT (Sanh et al., 2019), based on BERT-Base and distilled on SQuAD dataset, and SentenceBERT (Reimers and Gurevych, 2019), trained on BERT-Large and both MultiNLI and SNLI.8 For Portuguese we selected the ELMo pre-trained weights provided by Quinta de Castro et al. (2018) and the multilingual versions of the models used for English, namely mBERT (base cased), and both multilingual DistilBERT and Sentence-BERT (Reimers and Gurevych, 2020). As a static noncontextualised baseline we used GloVe (Pennington et al., 2014) (the English official models with 300 dimensions and trained on 840 billion tokens, and the equivalent Portuguese model released by Hartmann et al. (2017)). The vector representations were obtained w"
2021.acl-long.212,2020.emnlp-main.365,0,0.0255985,"word vectors using bidirectional LSTMs (Peters et al., 2018). For English we used the ELMo small model provided by Peters et al. (2018), BERT-Large uncased (Devlin et al., 2019), DistilBERT (Sanh et al., 2019), based on BERT-Base and distilled on SQuAD dataset, and SentenceBERT (Reimers and Gurevych, 2019), trained on BERT-Large and both MultiNLI and SNLI.8 For Portuguese we selected the ELMo pre-trained weights provided by Quinta de Castro et al. (2018) and the multilingual versions of the models used for English, namely mBERT (base cased), and both multilingual DistilBERT and Sentence-BERT (Reimers and Gurevych, 2020). As a static noncontextualised baseline we used GloVe (Pennington et al., 2014) (the English official models with 300 dimensions and trained on 840 billion tokens, and the equivalent Portuguese model released by Hartmann et al. (2017)). The vector representations were obtained with the flairNLP framework (Akbik et al., 2019) using the models provided by the transformers library (Wolf et al., 2020). The representations of NCs (and their sentences) were obtained by averaging the word (or subword, if adopted by the model) embeddings. We used the concatenation of the three layers for ELMo and of"
2021.acl-long.212,N19-1048,0,0.0612801,"Missing"
2021.acl-long.212,2020.acl-main.368,0,0.034856,"Missing"
2021.acl-long.212,L16-1362,0,0.0563567,"Missing"
2021.acl-long.212,Q19-1027,0,0.0191584,"roni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and Dagan (2019) train various classifiers initialised with static and contextualised embeddings for different compositional tasks, achieving the best results with BERT embeddings. Yu and Ettinger (2020), using partially idiomatic expressions of the BiRD dataset (Asaadi et al., 2019), show that contextualised embeddings from language models heavily rely on word content, missing additional information provided by compositional operations. In this paper we take advantage of the NCTTI dataset to observe whether vector representations obtained with different strategies correlate with human annotations at both typ"
2021.acl-long.212,W13-1005,0,0.0299096,"(90 of which came from Reddy et al. (2011)), and 180 in Portuguese, all of them labeled at type level: i.e., the annotators provided a compositionality value for a compound (from 0 –fully idiomatic– to 5, fully 1 Type level annotations come from Cordeiro et al. (2019), the dataset used as source for the NCTTI. 2 The NCCTI dataset can be downloaded from the following url: https://github.com/marcospln/nctti. 2 Related Work Datasets with type-level annotations are available for NCs in English (Farahmand et al., 2015; Reddy et al., 2011; Ramisch et al., 2016; Kruszewski and Baroni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and Dagan (2019) train various c"
2021.acl-long.212,E09-1086,0,0.0413724,"ally, for some MWEs, there is a potential ambiguity between an idiomatic and a literal sense, like in the potentially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016). Contextualised embedding models, like BERT (Devlin et al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and the"
2021.acl-long.212,L18-1686,1,0.834341,"uman judgments: at token level, using all the sentences for each language; and at type level, comparing the average cosine similarities of each NC with their compositionality scores at type level. We also compute correlations between the similarities and frequency-based data, namely the NC raw frequency, and the PPMI (Church and Hanks, 1990) between its component words, to verify whether they have any impact in these measures of idiomaticity. The frequency data were obtained from ukWaC, with 2.25B tokens in English (Baroni et al., 2009), and brWaC, containing 2.7B tokens in Portuguese (Wagner Filho et al., 2018). The results by Cordeiro et al. (2019) suggested that if the two components of an NC are processed as a single token unit (for instance, by explic9 This representation equivalent to the Avg Phrase used by Yu and Ettinger (2020). itly linking them with an underscore) the resulting static representation captures the NC idiomatic meaning. This is not surprising since by linking the two components we create a new word that would be treated by the model as completely independent of the preexisting component words. But such preprocessing may not be desirable or even feasible. In this sense the cont"
2021.acl-long.212,2020.emnlp-main.397,0,0.443606,"at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and Dagan (2019) train various classifiers initialised with static and contextualised embeddings for different compositional tasks, achieving the best results with BERT embeddings. Yu and Ettinger (2020), using partially idiomatic expressions of the BiRD dataset (Asaadi et al., 2019), show that contextualised embeddings from language models heavily rely on word content, missing additional information provided by compositional operations. In this paper we take advantage of the NCTTI dataset to observe whether vector representations obtained with different strategies correlate with human annotations at both type and token levels. 3 The Noun Compound Type and Token Idiomaticity dataset This section describes the procedure to create the NCTTI dataset and its main characteristics.2 3.1 2731 Source"
2021.acl-long.212,2020.lrec-1.471,0,0.018533,"levels of contextualisation and (3) proposing two new measures of idiomaticity. Moreover, the paraphrases provided for each NC at type and token level make NCTTI a useful resource for enhancing paraphrase datasets (e.g. PPDB (Ganitkevitch et al., 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020). Such paraphrases may also be useful for improving the task of machine translation, avoiding the need for parallel MWE corpora (Zaninello and Birch, 2020). Section 2 gives an overview of existing idiomaticity datasets. Section 3 presents the NCTTI dataset and the annotations, and section 4 discusses the evaluation of the performance of different word embeddings in detecting idiomaticity. We used as basis the English and Portuguese subsets of the NC Compositionality dataset (Cordeiro et al., 2019), which contain compositionality scores for 280 two-word NCs in English (90 of which came from Reddy et al. (2011)), and 180 in Portuguese, all of them labeled at type level: i.e., the annotators provided a compositionality value for a compound (from 0"
2021.case-1.1,2021.case-1.4,1,0.808064,"Missing"
2021.case-1.1,2021.case-1.23,1,0.586051,"ned issues in sociopolitical and crisis event studies have been studied by numerous scholars for decades to date, there are still no answers or solutions to them (Wang et al., 2016; Lorenzini et al., 2016; Schrodt, 2020; Raleigh, 2020; Eck, 2021; Boschee, 2021). Therefore, we aim at contributing to the understanding and resolution of event database creation via quantifying performance of the state-of-the-art text processing systems in the shared task Socio-political and Crisis Events Detection. 7 The shared task consists of three tasks that are on collection (Task 1), classification (Task 2) (Haneczok et al., 2021), and evaluation (Task 3) of event databases. Shared task and submission details are reported in the overview papers of the tasks (H¨urriyeto˘glu et al., 2021; Haneczok et al., 2021; Giorgi et al., 2021) and the system description papers in this proceedings respectively. We provide a summary of the tasks and the findings in the following subsections. Radford (2021) presents a study on geocoding and a new data set. Geocoding is an important subtask of event detection, in which the goal is to find the geographic coordinates associated with event descriptions. The paper presents an “end-to-end pr"
2021.case-1.1,D19-2004,0,0.0230739,"tions for situation awareness, using various branches of artificial intelligence (AI), natural language processing (NLP), machine learning (ML), and advanced statistical methods. ing (ML) and NLP methods to deal better with the vast amount and variety of data in this domain (H¨urriyeto˘glu et al., 2021). Nonetheless, automated approaches suffer from major issues like bias, low generalizability, class imbalance, training data limitations, ethical issues, and lack of recall quantification which affect the quality of the results and their use drastically (Leins et al., 2020; Bhatia et al., 2020; Chang et al., 2019; Y¨or¨uk et al., 2021). Moreover, the results of the automated systems for socio-political event information collection may not be comparable to each other or not of sufficient quality (Wang et al., 2016; Schrodt, 2020). Socio-political events are varied and nuanced. Both the political context and the local language used may affect whether and how they are reported. Therefore, all steps of information collection (event definition, language resources, and manual or algorithmic steps) may need to be constantly updated. This leads us to a series of challenging questions such as: Do events relate"
2021.case-1.1,2021.case-1.16,0,0.401507,"sifier, which identifies whether a pair 7 https://github.com/emerging-welfare/ case-2021-shared-task, accessed on June 9, 2021. 4 1 and 2, sentences that are about the same event are identified in subtask 3, and event trigger and its arguments are extracted in subtask 4. 13 teams have submitted 238 submissions for the evaluation scenarios specified with subtask and language combinations. The best submissions utilized deep learning approaches that combine the training data in various languages, utilize large models, further re-train the models, and create ensemble models (Awasthy et al., 2021; Hettiarachchi et al., 2021; Re et al., 2021; Hu and Stoehr, 2021; Tan et al., 2021). Although training data was limited in Portuguese and Spanish and not available in Hindi, the best performing participants managed to deliver predictions that are between 77.27 and 93.03 F1macro in subtasks 1, 2, and 3 for all languages. The performance of the best system for subtask 4 for all languages was between 66.20 and 78.11 for all languages and 4-5 F1-macro points ahead of all other teams in all languages. 3.2 from the test data and inclusion of some unseen classes the top results obtained can be considered good, however, there"
2021.case-1.1,2021.case-1.20,0,0.462297,"ps://github.com/emerging-welfare/ case-2021-shared-task, accessed on June 9, 2021. 4 1 and 2, sentences that are about the same event are identified in subtask 3, and event trigger and its arguments are extracted in subtask 4. 13 teams have submitted 238 submissions for the evaluation scenarios specified with subtask and language combinations. The best submissions utilized deep learning approaches that combine the training data in various languages, utilize large models, further re-train the models, and create ensemble models (Awasthy et al., 2021; Hettiarachchi et al., 2021; Re et al., 2021; Hu and Stoehr, 2021; Tan et al., 2021). Although training data was limited in Portuguese and Spanish and not available in Hindi, the best performing participants managed to deliver predictions that are between 77.27 and 93.03 F1macro in subtasks 1, 2, and 3 for all languages. The performance of the best system for subtask 4 for all languages was between 66.20 and 78.11 for all languages and 4-5 F1-macro points ahead of all other teams in all languages. 3.2 from the test data and inclusion of some unseen classes the top results obtained can be considered good, however, there is place for improvement. 3.3 Task 3:"
2021.case-1.1,2021.case-1.11,1,0.396514,"Missing"
2021.case-1.1,2021.case-1.3,0,0.0306167,"nt database creation comprises of three steps that are collecting events, classifying them, and measuring utility of the system output, which is an event database, against ground-truth. Each of these steps contains pitfalls and subject to limitations. For instance, the data source utilized maybe biased or a ground-truth may not be available. Although aforementioned issues in sociopolitical and crisis event studies have been studied by numerous scholars for decades to date, there are still no answers or solutions to them (Wang et al., 2016; Lorenzini et al., 2016; Schrodt, 2020; Raleigh, 2020; Eck, 2021; Boschee, 2021). Therefore, we aim at contributing to the understanding and resolution of event database creation via quantifying performance of the state-of-the-art text processing systems in the shared task Socio-political and Crisis Events Detection. 7 The shared task consists of three tasks that are on collection (Task 1), classification (Task 2) (Haneczok et al., 2021), and evaluation (Task 3) of event databases. Shared task and submission details are reported in the overview papers of the tasks (H¨urriyeto˘glu et al., 2021; Haneczok et al., 2021; Giorgi et al., 2021) and the system desc"
2021.case-1.1,2021.findings-acl.371,0,0.0374574,"Missing"
2021.case-1.1,2021.case-1.5,0,0.0130576,"n Hong Kong sources when the Movement emerged. 3.1 Task 1: Multilingual protest news detection The task is designed to be both multilingual (having both training and test data in English, Portuguese, and Spanish) and cross-lingual (having data in Hindi only for test). There are four subtasks that are document classification (subtask 1), sentence classification (subtask 2), event sentence classification (subtask 3), and event extraction (subtask 4). Event information is at the center of all of the subtasks, i.e. documents and sentences are classified as containing event information in subtasks Kar et al. (2021) describe an algorithm for event argument detection and aggregation. The paper reports on document level aggregation of the following argument types: Time, Place, Casualties, After-Effect, Reason, and Participant. The ArgFuse algorithm is based on a BERT based active learning classifier, which identifies whether a pair 7 https://github.com/emerging-welfare/ case-2021-shared-task, accessed on June 9, 2021. 4 1 and 2, sentences that are about the same event are identified in subtask 3, and event trigger and its arguments are extracted in subtask 4. 13 teams have submitted 238 submissions for the"
2021.case-1.1,2021.case-1.10,0,0.0428229,"Missing"
2021.case-1.1,2020.acl-main.261,0,0.0473763,"Missing"
2021.case-1.1,2021.case-1.22,0,0.170981,"ther a pair 7 https://github.com/emerging-welfare/ case-2021-shared-task, accessed on June 9, 2021. 4 1 and 2, sentences that are about the same event are identified in subtask 3, and event trigger and its arguments are extracted in subtask 4. 13 teams have submitted 238 submissions for the evaluation scenarios specified with subtask and language combinations. The best submissions utilized deep learning approaches that combine the training data in various languages, utilize large models, further re-train the models, and create ensemble models (Awasthy et al., 2021; Hettiarachchi et al., 2021; Re et al., 2021; Hu and Stoehr, 2021; Tan et al., 2021). Although training data was limited in Portuguese and Spanish and not available in Hindi, the best performing participants managed to deliver predictions that are between 77.27 and 93.03 F1macro in subtasks 1, 2, and 3 for all languages. The performance of the best system for subtask 4 for all languages was between 66.20 and 78.11 for all languages and 4-5 F1-macro points ahead of all other teams in all languages. 3.2 from the test data and inclusion of some unseen classes the top results obtained can be considered good, however, there is place for impr"
2021.case-1.1,2021.case-1.7,0,0.0366005,"tasks and the findings in the following subsections. Radford (2021) presents a study on geocoding and a new data set. Geocoding is an important subtask of event detection, in which the goal is to find the geographic coordinates associated with event descriptions. The paper presents an “end-to-end probabilistic model” for geocoding from text data. A novel data set has been created for evaluating the performance of geocoding systems. The output of the new model is compared with a state-of-the-art model, called Mordecai. The comparison clearly shows an improvement provided by the proposed model. Scharf et al. (2021) report on a study on the political bias in Hong Kong published news reporting about protest events. The paper reports on lexical differences between home and Western news sources about protests happening in Hong Kong in the period 1998-2020. Experiments on topic modeling, sentiment analysis, lexical distribution and comparative lexical analysis between Westernand Hong Kong-based sources reveal a bias in the reporting from the Hong Kong press. The evaluation reveals that during the Anti-Extradition Law Amendment Bill Movement reports from Hong Kong made fewer references to police violence comp"
2021.case-1.1,2020.aespen-1.3,0,0.0664894,"the vast amount and variety of data in this domain (H¨urriyeto˘glu et al., 2021). Nonetheless, automated approaches suffer from major issues like bias, low generalizability, class imbalance, training data limitations, ethical issues, and lack of recall quantification which affect the quality of the results and their use drastically (Leins et al., 2020; Bhatia et al., 2020; Chang et al., 2019; Y¨or¨uk et al., 2021). Moreover, the results of the automated systems for socio-political event information collection may not be comparable to each other or not of sufficient quality (Wang et al., 2016; Schrodt, 2020). Socio-political events are varied and nuanced. Both the political context and the local language used may affect whether and how they are reported. Therefore, all steps of information collection (event definition, language resources, and manual or algorithmic steps) may need to be constantly updated. This leads us to a series of challenging questions such as: Do events related to minority groups are represented well? Are new types of events covered? Are the event definitions and their operationalization comparable across systems? We organize the workshop on Challenges and Applications of Aut"
2021.case-1.1,2021.case-1.8,0,0.0985759,"performance of the state-of-the-art text processing systems in the shared task Socio-political and Crisis Events Detection. 7 The shared task consists of three tasks that are on collection (Task 1), classification (Task 2) (Haneczok et al., 2021), and evaluation (Task 3) of event databases. Shared task and submission details are reported in the overview papers of the tasks (H¨urriyeto˘glu et al., 2021; Haneczok et al., 2021; Giorgi et al., 2021) and the system description papers in this proceedings respectively. We provide a summary of the tasks and the findings in the following subsections. Radford (2021) presents a study on geocoding and a new data set. Geocoding is an important subtask of event detection, in which the goal is to find the geographic coordinates associated with event descriptions. The paper presents an “end-to-end probabilistic model” for geocoding from text data. A novel data set has been created for evaluating the performance of geocoding systems. The output of the new model is compared with a state-of-the-art model, called Mordecai. The comparison clearly shows an improvement provided by the proposed model. Scharf et al. (2021) report on a study on the political bias in Hon"
2021.case-1.1,2020.aespen-1.2,0,0.0325023,"The work on event database creation comprises of three steps that are collecting events, classifying them, and measuring utility of the system output, which is an event database, against ground-truth. Each of these steps contains pitfalls and subject to limitations. For instance, the data source utilized maybe biased or a ground-truth may not be available. Although aforementioned issues in sociopolitical and crisis event studies have been studied by numerous scholars for decades to date, there are still no answers or solutions to them (Wang et al., 2016; Lorenzini et al., 2016; Schrodt, 2020; Raleigh, 2020; Eck, 2021; Boschee, 2021). Therefore, we aim at contributing to the understanding and resolution of event database creation via quantifying performance of the state-of-the-art text processing systems in the shared task Socio-political and Crisis Events Detection. 7 The shared task consists of three tasks that are on collection (Task 1), classification (Task 2) (Haneczok et al., 2021), and evaluation (Task 3) of event databases. Shared task and submission details are reported in the overview papers of the tasks (H¨urriyeto˘glu et al., 2021; Haneczok et al., 2021; Giorgi et al., 2021) and the"
2021.case-1.1,2021.findings-acl.314,0,0.0197891,"(ERC) Starting Grant 714868 awarded to Dr. Erdem Y¨or¨uk for his project Emerging Welfare. Halterman and Radford (2021) show utility of “upsampling” coarse document labels to finegrained labels or spans for protest size detection; and References Parul Awasthy, Jian Ni, Ken Barker, and Radu Florian. 2021. IBM MNLP IE at CASE 2021 task 1: Multi-granular and multilingual event detection on protest news. In Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021), online. Association for Computational Linguistics (ACL). Tsarapatsanis and Aletras (2021) discuss the importance of academic freedom, the diversity of legal and ethical norms, and the threat of moralism in the computational law field. 6 Conclusion This workshop is the fourth event from a series of workshops on automatic extraction of sociopolitical events from news, organized by the Emerging Market Welfare Project, with the support of the Joint Research Centre of the European Commission, with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development in the area of event extraction of socio-politica"
2021.case-1.1,2021.case-1.6,0,0.0619007,"Missing"
2021.case-1.1,2021.findings-acl.186,0,0.0387072,"a modified version of BERT. Most papers use BERT embeddings as features in their models and one paper discusses an algorithm, which uses a full syntactic parser. Sentiment analysis is used in one paper, which studies the political bias of the news. The shared task results shed light on critical aspects of the automated socio-political extraction and evaluation methodology. 8 Invited Talks The workshop contains an invited talks session as well. The authors of the papers published in Findings of ACL and related to workshop theme are invited to present their work in this session. The papers are Zhou et al. (2021) propose an event-driven trading strategy that predicts stock movements by detecting corporate events from news articles; Halterman et al. (2021) introduce the IndiaPoliceEvents Corpus—all 21,391 sentences from 1,257 Times of India articles about events in the state of Gujarat during March 2002; Acknowledgments The authors from Koc University were funded by the European Research Council (ERC) Starting Grant 714868 awarded to Dr. Erdem Y¨or¨uk for his project Emerging Welfare. Halterman and Radford (2021) show utility of “upsampling” coarse document labels to finegrained labels or spans for pro"
2021.cmcl-1.16,P19-1283,0,0.0259098,"uage Models (PLMs) outperform feature based models in ways that do not correlate with identifiable cognitive processes (Sood et al., 2020). Since many PLMs evolved from the study of human cognitive processes (Vaswani et al., 2017) but now perform in ways that do not correlate with human cognition, we wished to investigate how merging cognitively inspired features with PLMs may impact predictive behaviour. We felt this was a particularly pertinent question given that PLMs have been shown to contain information about crucial features for predicting eye tracking patterns such as parts of speech (Chrupała and Alishahi, 2019; Tenney et al., 2019) and sentence length (Jawahar et al., 2019). We therefore had the goals of providing a competitive Shared Task entry, and investigating the following hypotheses: A) Does linguistic/cognitive information that can be predicted by eye-tracking features prove useful for predicting eye-tracking features? B) Can adding cognitively inspired features to a model based on PLMs improve performance in predicting eye tracking features? 125 Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 125–133 Online Event, June 10, 2021. ©2021 Association for C"
2021.cmcl-1.16,J19-1001,1,0.82479,"raged these values again at the type (word) level. For words present in the task training data but not the GECO data, we estimated the values using means for words in the GECO data of a similar frequency (according to the wordfreq). 4.4 ZuCo reading material (Hollenstein et al., 2020). We produced two indicator features for the presence of MWEs: a binary indicator, and a categorical variable summarising the syntactic pattern of the MWE, motivated by Yaneva et al.’s evidence that MWEs of different syntactic patterns display different eye-tracking characteristics (2017). Following the method of Cordeiro et al. (2019), we joined component words of MWEs in Wikitext103 using underscores (i.e. climate change became climate_change) and then generated Skipgram word embeddings (Mikolov et al., 2013) for all single words and MWEs identified in Wikitext-103. Using the feat_comp function in mwetoolkit (Ramisch, 2012), these MWE embeddings were used to compute compositionality scores and weights (Cordeiro et al., 2019). 2 MWEs identified in the training data were assigned MWE embeddings and compositionality information as features, and non-MWEs were assigned single word embeddings and zero values for compositionalit"
2021.cmcl-1.16,2020.conll-1.2,0,0.233031,"dels, our first model tests ‘simple’ features that are informed by up-to-date expert linguistic knowledge. In particular, we investigate information about multi-word expressions (MWEs) as eye-tracking information has been used to detect MWEs in context (Rohanian et al., 2017; Yaneva et al., 2017), and empirically MWEs appear have processing advantages over non-formulaic language (Siyanova-Chanturia et al., 2017). Our second model is motivated by evidence that Pre-trained Language Models (PLMs) outperform feature based models in ways that do not correlate with identifiable cognitive processes (Sood et al., 2020). Since many PLMs evolved from the study of human cognitive processes (Vaswani et al., 2017) but now perform in ways that do not correlate with human cognition, we wished to investigate how merging cognitively inspired features with PLMs may impact predictive behaviour. We felt this was a particularly pertinent question given that PLMs have been shown to contain information about crucial features for predicting eye tracking patterns such as parts of speech (Chrupała and Alishahi, 2019; Tenney et al., 2019) and sentence length (Jawahar et al., 2019). We therefore had the goals of providing a co"
2021.eacl-main.310,Q16-1037,0,0.0344375,"event-related potential (ERP) data showed that idiomatic expressions, especially those with a salient meaning (Giora, 1999), have processing advantages (Laurent et al., 2006; Rommers et al., 2013). In NLP, probing tasks have been useful in revealing to what extent contextualised models are capable of learning different linguistic properties (Conneau et al., 2018). They allow for more controlled settings, removing obvious biases and potentially confounding factors from evaluations, and allowing both the use of artificially constructed but controlled sentences and naturally occurring sentences (Linzen et al., 2016; Gulordava et al., 2018). In priming tasks, related stimuli are easier to process than unrelated ones. One assumption is that, for models, related stimuli would achieve greater similarity than unrelated stimuli. These tasks have been used, for instance, to evaluate how neural language models represent syntax (van Schijndel and Linzen, 2018; Prasad et al., 2019), and the preferences that they may display, such as the use of mainly lexical information in a lexical substitution task even if contextual information is available (Aina et al., 2019). Concerning pre-trained neural language models, wh"
2021.eacl-main.310,N19-1112,0,0.0193904,"imuli would achieve greater similarity than unrelated stimuli. These tasks have been used, for instance, to evaluate how neural language models represent syntax (van Schijndel and Linzen, 2018; Prasad et al., 2019), and the preferences that they may display, such as the use of mainly lexical information in a lexical substitution task even if contextual information is available (Aina et al., 2019). Concerning pre-trained neural language models, which produce contextualised word representations, analyses about their abilities have shown, for instance, that they can encode syntactic information (Liu et al., 2019) including long-distance subject– verb agreement (Goldberg, 2019). Regarding semantic knowledge, the results of various experiments suggest that BERT can somewhat represent semantic roles (Ettinger, 2020). However, its improvements appear mainly in core roles that may be predicted from syntactic representations (Tenney et al., 2019). Moreover, from the representations generated by BERT, ELMo and Flair (Akbik et al., 2018) for word sense disambiguation, only the clusters of BERT vectors seem to be related to word senses (Wiedemann et al., 2019), although in crosslingual alignment of ELMo embedd"
2021.eacl-main.310,W19-2004,0,0.0968498,"ave also been observed (Schuster et al., 2019). The use of contextualised models for representing MWEs has been reported with mixed results. Shwartz and Dagan (2019) evaluated different classifiers initialised with contextualised and non-contextualised embeddings in five tasks related to lexical composition (including the literality of NCs) and found that contextualised models, especially BERT, obtained better performance across all tasks. However, for capturing idiomaticity in MWEs, static models like word2vec (Mikolov et al., 2013) seem to have better performance than contextualised models (Nandakumar et al., 2019; King and Cook, 2018). These mixed results suggest that a controlled evaluation setup is needed to obtain comparable results across models and languages. Therefore, we have carefully designed probing tasks to assess the representation of NCs in vector space models. As the same word can have different representations even in related paraphrased contexts (Shi et al., 2019), we adopt paraphrases with minimal modifications to compare the idiomatic and literal representations of a given NC. 3 3.1 Materials and Methods Noun Compound Senses Dataset The Noun Compound Senses (NCS) dataset is based on"
2021.eacl-main.310,D14-1162,0,0.0917283,"ructed the Noun Compound Senses Dataset, which contains noun compounds and their paraphrases, in context neutral and context informative naturalistic sentences, in two languages: English and Portuguese. Results obtained using four types of probing measures with models like ELMo, BERT and some of its variants, indicate that idiomaticity is not yet accurately represented by contextualised models. 1 Introduction Contextualised word representation models, like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), seem to represent words more accurately than static word embeddings like GloVe (Pennington et al., 2014), as they can encode different usages of a word. In fact, representations of a word in several contexts can be grouped in different clusters, which seem to be related to the various senses of the word (Schuster et al., 2019), and they can be used to match polysemous words in context to specific sense definitions (Chang and Chen, 2019). However, multiword expressions (MWEs) fall into a continuum of idiomaticity1 (Sag et al., 2002; Fazly 1 We understand idiomaticity as semantic opacity and its continuum as different degrees of opacity (Cruse, 1986). In this paper, we propose a set of probing mea"
2021.eacl-main.310,N18-1202,0,0.277947,"cal choice, are readily available in some standard and widely used representations. For that, we constructed the Noun Compound Senses Dataset, which contains noun compounds and their paraphrases, in context neutral and context informative naturalistic sentences, in two languages: English and Portuguese. Results obtained using four types of probing measures with models like ELMo, BERT and some of its variants, indicate that idiomaticity is not yet accurately represented by contextualised models. 1 Introduction Contextualised word representation models, like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), seem to represent words more accurately than static word embeddings like GloVe (Pennington et al., 2014), as they can encode different usages of a word. In fact, representations of a word in several contexts can be grouped in different clusters, which seem to be related to the various senses of the word (Schuster et al., 2019), and they can be used to match polysemous words in context to specific sense definitions (Chang and Chen, 2019). However, multiword expressions (MWEs) fall into a continuum of idiomaticity1 (Sag et al., 2002; Fazly 1 We understand idiomaticity as semantic opacity and i"
2021.eacl-main.310,K19-1007,0,0.0335974,"Missing"
2021.eacl-main.310,I11-1024,0,0.0274059,"olled evaluation setup is needed to obtain comparable results across models and languages. Therefore, we have carefully designed probing tasks to assess the representation of NCs in vector space models. As the same word can have different representations even in related paraphrased contexts (Shi et al., 2019), we adopt paraphrases with minimal modifications to compare the idiomatic and literal representations of a given NC. 3 3.1 Materials and Methods Noun Compound Senses Dataset The Noun Compound Senses (NCS) dataset is based on the NC Compositionality dataset, which contains NCs in English (Reddy et al., 2011), Portuguese and French (Cordeiro et al., 2019). Using the protocol by Reddy et al. (2011), human judgments were collected about the interpretation of each NC in 3 naturalistic corpus sentences. The task was to judge, for each NC, how literal the contributions of its component were for its meaning (e.g., “Is climate change truly/literally a change in climate?”). Each NC got a score, which was the average of the human judgments with a Likert scale from 0 (non-literal/idiomatic) to 5 (lit3552 eral/compositional).2 3.2 For the NCS dataset, a set of probing sentences for the 280 NCs in English and"
2021.eacl-main.310,D19-1410,0,0.0864267,"etermine how much for a given model an NC in context differs from the same NC out of context we measure sim(P4) in-out = cos(NC ⊂ S , NC ). We expect similarity scores to be higher in the NEU condition, given their semantically vague context, than for the NAT condition. 3.3 Calculating Embeddings We use as a baseline the static non-contextualised GloVe model (Pennington et al., 2014) and, for contextualised embeddings, four widely adopted models: ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and two BERT variants, DistilBERT (DistilB) (Sanh et al., 2019) and Sentence-BERT (SBERT) (Reimers and Gurevych, 2019b). For all the contextualised models, we use their pre-trained weights publicly available through the Flair implementation5 . For GloVe, the English and Portuguese models described in Pennington et al. (2014) and Hartmann et al. (2017). For ELMo, we use the small model provided by Peters et al. (2018), and for Portuguese we adopt the weights provided by Quinta de Castro et al. (2018). For all BERT-based models, we used the multilingual models for both English and Portuguese.6 To have a single embedding for the whole sentence or its parts, e.g., the NC representation, we use the standard proce"
2021.eacl-main.310,N19-1048,0,0.0588836,"Missing"
2021.eacl-main.310,2020.acl-main.368,0,0.0421216,"Missing"
2021.eacl-main.310,D18-1499,0,0.0202023,"Missing"
2021.eacl-main.310,N19-1162,0,0.152769,"four types of probing measures with models like ELMo, BERT and some of its variants, indicate that idiomaticity is not yet accurately represented by contextualised models. 1 Introduction Contextualised word representation models, like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), seem to represent words more accurately than static word embeddings like GloVe (Pennington et al., 2014), as they can encode different usages of a word. In fact, representations of a word in several contexts can be grouped in different clusters, which seem to be related to the various senses of the word (Schuster et al., 2019), and they can be used to match polysemous words in context to specific sense definitions (Chang and Chen, 2019). However, multiword expressions (MWEs) fall into a continuum of idiomaticity1 (Sag et al., 2002; Fazly 1 We understand idiomaticity as semantic opacity and its continuum as different degrees of opacity (Cruse, 1986). In this paper, we propose a set of probing measures to examine how accurately idiomaticity in MWEs, particularly in noun compounds (NCs), is captured in vector space models, focusing on some widely used representations. Inspired by the semantic priming paradigm (Neely e"
2021.eacl-main.310,D19-1113,0,0.0202599,"especially BERT, obtained better performance across all tasks. However, for capturing idiomaticity in MWEs, static models like word2vec (Mikolov et al., 2013) seem to have better performance than contextualised models (Nandakumar et al., 2019; King and Cook, 2018). These mixed results suggest that a controlled evaluation setup is needed to obtain comparable results across models and languages. Therefore, we have carefully designed probing tasks to assess the representation of NCs in vector space models. As the same word can have different representations even in related paraphrased contexts (Shi et al., 2019), we adopt paraphrases with minimal modifications to compare the idiomatic and literal representations of a given NC. 3 3.1 Materials and Methods Noun Compound Senses Dataset The Noun Compound Senses (NCS) dataset is based on the NC Compositionality dataset, which contains NCs in English (Reddy et al., 2011), Portuguese and French (Cordeiro et al., 2019). Using the protocol by Reddy et al. (2011), human judgments were collected about the interpretation of each NC in 3 naturalistic corpus sentences. The task was to judge, for each NC, how literal the contributions of its component were for its"
2021.eacl-main.310,Q19-1027,0,0.11139,"However, its improvements appear mainly in core roles that may be predicted from syntactic representations (Tenney et al., 2019). Moreover, from the representations generated by BERT, ELMo and Flair (Akbik et al., 2018) for word sense disambiguation, only the clusters of BERT vectors seem to be related to word senses (Wiedemann et al., 2019), although in crosslingual alignment of ELMo embeddings, clusters of polysemous words related to different senses have also been observed (Schuster et al., 2019). The use of contextualised models for representing MWEs has been reported with mixed results. Shwartz and Dagan (2019) evaluated different classifiers initialised with contextualised and non-contextualised embeddings in five tasks related to lexical composition (including the literality of NCs) and found that contextualised models, especially BERT, obtained better performance across all tasks. However, for capturing idiomaticity in MWEs, static models like word2vec (Mikolov et al., 2013) seem to have better performance than contextualised models (Nandakumar et al., 2019; King and Cook, 2018). These mixed results suggest that a controlled evaluation setup is needed to obtain comparable results across models an"
2021.eacl-main.310,L18-1686,1,0.827727,"averaging to obtain the NC embedding, as it is the standard procedure to represent not only MWEs but also out-of-vocabulary words, which are split into sub-tokens in contextualised models (Nandakumar et al., 2019; Wiedemann et al., 2019). However, we have also explored other methods to represent NCs in a single vector. First, we have incorporated type-level vectors of the NCs into a BERT model, inspired by compositionality prediction methods (Baldwin et al., 2003; Cordeiro et al., 2019). To do so, we annotated the target NCs in large English and Portuguese corpora (Baroni et al., 2009; Wagner Filho et al., 2018) and used attentive mimicking with onetoken-approximation (Schick and Sch¨utze, 2019, 2020b) to learn up to 500 contexts for each NC. These new vectors encode each NC in a single representation, therefore avoiding possible biases produced by the compositional operations. Then, we used BERTRAM (Schick and Sch¨utze, 2020a) to inject these type-level vectors in the BERT multilingual model. As expected, learning the vectors 3558 of the NCs as single tokens improved the representation of idiomatic expressions (see BERTRAM in Tables 2 and 4), decreasing the correlation with idiomaticity in P1 (e.g.,"
2021.findings-emnlp.294,J17-4005,0,0.0648311,"Missing"
2021.findings-emnlp.294,J19-1001,1,0.932177,"ls, program code to the meaning of the MWE (Venkatapathy and and associated processing scripts, including hyper- Joshi, 2005), or both (Reddy et al., 2011; Cordeiro parameters, publicly available in the interest of et al., 2019; Schulte im Walde et al., 2016). reproducibility and for subsequent reuse1 . While most of these target only English, some inThis paper is organised as follows: Section 2 clude scores for other languages such as German presents a discussion of related work. We then (Schulte im Walde et al., 2016), and French and present AStitchInLanguageModels consisting of Portuguese (Cordeiro et al., 2019). the novel MWE dataset and the two associated Existing datasets of compositionality that intasks in Section 3. We discuss our experiments clude context often add context automatically by and results for these two tasks in Section 4, before first selecting MWEs that are either only compresenting a discussion of the more interesting elepositional or only idiomatic. For instance, the ments of our findings in Section 5. We present our VNC-Tokens Dataset (Cook et al., 2008) consists conclusions and possible avenues of future work in of 53 English MWEs each with a maximum of Section 6. 100 sentence"
2021.findings-emnlp.294,N19-1423,0,0.149479,"y well in the one-shot and few-shot scenarios, but that there is significant scope for improvement in the zero-shot scenario. On the task of representing idiomaticity, we find that pre-training is not always effective, while finetuning could provide a sample efficient method of learning representations of sentences containing MWEs. 1 phrases are explicitly designed to be compositional both in non-contextual (Mitchell and Lapata, 2010; Mikolov et al., 2013b) and contextual embedding models. Pre-trained language models in particular exploit compositionality at both the word and sub-word levels (Devlin et al., 2019) to reduce the size of their vocabulary, which makes representing idiomatic phrases particularly challenging. The effective representation of idiomatic MWEs is critical for them to be correctly interpreted in downstream tasks. Such an improvement will benefit both classification-based problems (e.g. sentiment analysis) and sequence-to-sequence tasks (e.g. machine translation). To this end, we present a dataset consisting of naturally occurring sentences containing potentially idiomatic MWEs and two tasks aimed at evaluating language models’ ability to effectively detect and represent idiomatic"
2021.findings-emnlp.294,W15-0904,0,0.0603424,"Missing"
2021.findings-emnlp.294,2021.eacl-main.310,1,0.907158,"ability to effectively detect and represent idiomaticity. The primary contributions of this work are: 1. A novel dataset consisting of: Introduction and Motivation Pre-trained language models such as BERT (De- 2. vlin et al., 2019) and XLNet (Yang et al., 2019) have been widely used in a variety of Natural Language Processing tasks. Despite their success in multiple downstream applications, such as sentence classification (Zhang et al., 2019) and reading comprehension (Raffel et al., 2019), they are unable to effectively represent idiomatic multiword expressions (MWEs) (Yu and Ettinger, 2020; Garcia et al., 2021). Capturing idiomaticity is particularly challenging as the representations of words and 3464 (a) naturally occurring sentences (and two surrounding sentences) containing potentially idiomatic MWEs annotated with a finegrained set of meanings: compositional meaning, idiomatic meaning(s), proper noun and “meta usage”; (b) paraphrases for each meaning of each MWE; Two tasks aiming at evaluating i) a model’s ability to detect idiomatic usage, and ii) the effectiveness of sentence embeddings in representing idiomaticity. Table 1 provides details of these tasks and associated subtasks, each designe"
2021.findings-emnlp.294,2020.cogalex-1.9,1,0.689067,"ends be proportional to the number of instances of a token, representations of MWEs are often lacking. The second is that non-contextual type level representations are inherently limited as MWEs often have multiple meanings, as detailed in Section 2.1. While contextual embeddings can handle polysemy, they fail to fully capture the meaning of MWEs as discussed earlier. How contextual embeddings fair in comparison to their non-contextual predecessors is not entirely clear as Nandakumar et al. (2019) found that they do worse on some tasks while Shwartz and Dagan (2019) found that they do better. Hashempour and Villavicencio (2020) adopted the idiom principle (MWE as a single token) with contextual language models (specifically BERT), and found that this method does not benefit transformer-based pre-trained models. However, they did not introduce a new token to represent each MWE as is required during the training of non-contextual models built on the idiom principle, but instead replaced MWEs with a single token in the input and rely on BERT’s word-piece tokenizer. To the best of our knowledge this work is the first to introduce new tokens for MWEs into a contextual pre-trained language model (see Section 4.2). The tas"
2021.findings-emnlp.294,P16-1020,0,0.0203551,"mples with a fine-grained set of meangle units in learning embeddings (Mikolov et al., ings associated with each usage. We restrict our attention to noun compounds, a subset of idiomatic 2013b). This method was improved upon by use of an explicit disambiguation step prior to com- MWEs, sourced from the Noun Compound Senses (NCS) dataset (Cordeiro et al., 2019), which exposition (Kartsaklis et al., 2014), and by the joint learning of compositional and idiomatic embed- tends the dataset by Reddy et al. (2011). dings using a “compositionality scoring” func3.1 Data Collection and Annotation tion (Hashimoto and Tsuruoka, 2016). This “single token” method has the advantage of being rooted in A total of 12 judges were asked to collect examthe linguistic idiom principle (Sinclair et al., 1991), ples containing a list of MWEs occurring natuwhich postulates that humans process idioms by rally in context, in both English and Portuguese. treating them as a “single independent token”. For each MWE, judges were instructed to obtain 3466 7 to 10 examples of each meaning ( “Idiomatic”, “Non-Idiomatic”, “Proper Noun” and “Meta Usage”) where possible, with between 20 and 30 total examples for each MWE. We define “Meta Usage” to"
2021.findings-emnlp.294,S13-2025,0,0.0268761,"MWEs. Dagan (2019) showed, using six tasks, that conDespite the importance of the context surround1 ing an MWE, where available, context, in the form https://github.com/H-TayyarMadabushi /AStitchInLanguageModels of sentences containing MWEs, is available only 3465 Subtask B for those MWEs that are either idiomatic or compositional. This significant shortcoming makes it impossible to train models to learn to differentiate between the compositional and idiomatic usage of the same MWE. Finally, while existing datasets also provide paraphrases for the compositional and idiomatic meanings of MWEs (Hendrickx et al., 2013; Garcia et al., 2021), they are limited to having exactly one compositional and one idiomatic meaning, which is not always the case as is exemplified by the phrase “head hunter” which, while not having a literal usage, has multiple idiomatic meanings (i.e recruiter, baseball pitcher who aims for the head, and hunter). AStitchInLanguageModels is designed to alleviate these shortcomings, specifically: a) the lack of context sentences, b) the need for fine grained classification of MWEs, and a more complete set of paraphrases for all possible meanings of MWEs (Section 3). 2.2 Methods Despite bei"
2021.findings-emnlp.294,P14-2035,0,0.0303782,"ral distribu- usage in naturally occurring sentences along with the two surrounding sentences. We then annotated tional semantic models such as word2vec (Mikolov et al., 2013a) wherein MWEs were taken as sin- these examples with a fine-grained set of meangle units in learning embeddings (Mikolov et al., ings associated with each usage. We restrict our attention to noun compounds, a subset of idiomatic 2013b). This method was improved upon by use of an explicit disambiguation step prior to com- MWEs, sourced from the Noun Compound Senses (NCS) dataset (Cordeiro et al., 2019), which exposition (Kartsaklis et al., 2014), and by the joint learning of compositional and idiomatic embed- tends the dataset by Reddy et al. (2011). dings using a “compositionality scoring” func3.1 Data Collection and Annotation tion (Hashimoto and Tsuruoka, 2016). This “single token” method has the advantage of being rooted in A total of 12 judges were asked to collect examthe linguistic idiom principle (Sinclair et al., 1991), ples containing a list of MWEs occurring natuwhich postulates that humans process idioms by rally in context, in both English and Portuguese. treating them as a “single independent token”. For each MWE, judge"
2021.findings-emnlp.294,W06-1203,0,0.105845,"izer. To the best of our knowledge this work is the first to introduce new tokens for MWEs into a contextual pre-trained language model (see Section 4.2). The task of identifying idiomaticity in sentences was initially addressed by use of symbolic methods (Baldwin and Villavicencio, 2002; Sag et al., 2002), statistical properties of text such as mutual information (Lin, 1999), and latent semantic analysis (Baldwin et al., 2003). The subsequent adoption of distributional semantics led to the use of constituent word embeddings to determine the compositionality of phrases, such as in the work by Katz and Giesbrecht (2006) who 3 AStitchInLanguageModels: Dataset made use of the semantic similarity between the and Tasks distributional vectors associated with an MWE as a whole and those associated with its parts to de- To create a dataset and tasks aimed at improving language models’ ability to identify and capture termine compositionality. This is achieved by use idiomaticity, we first collected examples of MWE of a single token to represent an MWE. This trend continued with the introduction of neural distribu- usage in naturally occurring sentences along with the two surrounding sentences. We then annotated tion"
2021.findings-emnlp.294,P99-1041,0,0.439374,"not introduce a new token to represent each MWE as is required during the training of non-contextual models built on the idiom principle, but instead replaced MWEs with a single token in the input and rely on BERT’s word-piece tokenizer. To the best of our knowledge this work is the first to introduce new tokens for MWEs into a contextual pre-trained language model (see Section 4.2). The task of identifying idiomaticity in sentences was initially addressed by use of symbolic methods (Baldwin and Villavicencio, 2002; Sag et al., 2002), statistical properties of text such as mutual information (Lin, 1999), and latent semantic analysis (Baldwin et al., 2003). The subsequent adoption of distributional semantics led to the use of constituent word embeddings to determine the compositionality of phrases, such as in the work by Katz and Giesbrecht (2006) who 3 AStitchInLanguageModels: Dataset made use of the semantic similarity between the and Tasks distributional vectors associated with an MWE as a whole and those associated with its parts to de- To create a dataset and tasks aimed at improving language models’ ability to identify and capture termine compositionality. This is achieved by use idioma"
2021.findings-emnlp.294,schneider-etal-2014-comprehensive,0,0.0214862,"ntences using both pre-training and fine-tuning. textual pre-trained language models, capable of handling polysemy, continued to be unable to effectively handle idiomatic MWEs, although they tend to do better than their non-contextual predecessors. Further experiments with probing pre-trained language models across multiple languages have also confirmed this result (Yu and Ettinger, 2020; Garcia et al., 2021). 2.1 Existing Datasets Datasets of MWE annotated corpora include that associated with the PARSEME shared task (Savary et al., 2017) which focuses on verbal MWEs and the STREUSLE dataset (Schneider et al., 2014; Table 1: AStitchInLanguageModels Tasks: The two Schneider and Smith, 2015; Schneider et al., 2016) tasks and associated subtasks. which includes noun, verb, prepositional and posThis dataset and associated tasks have the po- sessive expressions including “semantic supertential to catalyse research into representing more senses”. However, most existing datasets associcomplex elements of language beginning with id- ated with compositionality of MWEs consist of iomaticity, thus ensuring a timely stitch in language isolated phrases, labelled with overall composimodels. We call this dataset and a"
2021.findings-emnlp.294,N15-1177,0,0.0255888,"guage models, capable of handling polysemy, continued to be unable to effectively handle idiomatic MWEs, although they tend to do better than their non-contextual predecessors. Further experiments with probing pre-trained language models across multiple languages have also confirmed this result (Yu and Ettinger, 2020; Garcia et al., 2021). 2.1 Existing Datasets Datasets of MWE annotated corpora include that associated with the PARSEME shared task (Savary et al., 2017) which focuses on verbal MWEs and the STREUSLE dataset (Schneider et al., 2014; Table 1: AStitchInLanguageModels Tasks: The two Schneider and Smith, 2015; Schneider et al., 2016) tasks and associated subtasks. which includes noun, verb, prepositional and posThis dataset and associated tasks have the po- sessive expressions including “semantic supertential to catalyse research into representing more senses”. However, most existing datasets associcomplex elements of language beginning with id- ated with compositionality of MWEs consist of iomaticity, thus ensuring a timely stitch in language isolated phrases, labelled with overall composimodels. We call this dataset and associated tasks tionality scores (Venkatapathy and Joshi, 2005; AStitchInLa"
2021.findings-emnlp.294,L16-1362,0,0.0206534,", labelled with overall composimodels. We call this dataset and associated tasks tionality scores (Venkatapathy and Joshi, 2005; AStitchInLanguageModels, and make the dataset, Biemann and Giesbrecht, 2011; Farahmand et al., the associated splits for each task, pre-training data, 2015), scores of how individual words contribute pre-trained and fine-tuned models, program code to the meaning of the MWE (Venkatapathy and and associated processing scripts, including hyper- Joshi, 2005), or both (Reddy et al., 2011; Cordeiro parameters, publicly available in the interest of et al., 2019; Schulte im Walde et al., 2016). reproducibility and for subsequent reuse1 . While most of these target only English, some inThis paper is organised as follows: Section 2 clude scores for other languages such as German presents a discussion of related work. We then (Schulte im Walde et al., 2016), and French and present AStitchInLanguageModels consisting of Portuguese (Cordeiro et al., 2019). the novel MWE dataset and the two associated Existing datasets of compositionality that intasks in Section 3. We discuss our experiments clude context often add context automatically by and results for these two tasks in Section 4, bef"
2021.findings-emnlp.294,Q19-1027,0,0.098411,"o tasks in Section 4, before first selecting MWEs that are either only compresenting a discussion of the more interesting elepositional or only idiomatic. For instance, the ments of our findings in Section 5. We present our VNC-Tokens Dataset (Cook et al., 2008) consists conclusions and possible avenues of future work in of 53 English MWEs each with a maximum of Section 6. 100 sentences extracted from the BNC, while Tu and Roth (2012) collected 1,348 sentences associ2 Related work ated with 23 verb phrases annotated as composiThe problems posed by MWEs to NLP models have tional and idiomatic. Shwartz and Dagan (2019) been known for some time (Sag et al., 2002; Con- focused on a subset of noun compounds that are stant et al., 2017; Shwartz and Dagan, 2019). For only compositional or idiomatic from the dataset instance, Sag et al. (2002) refer to the idiomatic- provided by Reddy et al. (2011) and automatically ity problem and place the need for effective pro- added sentences from Wikipedia. Finally, the NCS cessing of MWEs on par with that for word sense Dataset (Garcia et al., 2021) consists of 280 Endisambiguation to be able to effectively process glish and 180 Portuguese MWEs, annotated with text. While"
2021.findings-emnlp.294,S12-1010,0,0.0248869,"two associated Existing datasets of compositionality that intasks in Section 3. We discuss our experiments clude context often add context automatically by and results for these two tasks in Section 4, before first selecting MWEs that are either only compresenting a discussion of the more interesting elepositional or only idiomatic. For instance, the ments of our findings in Section 5. We present our VNC-Tokens Dataset (Cook et al., 2008) consists conclusions and possible avenues of future work in of 53 English MWEs each with a maximum of Section 6. 100 sentences extracted from the BNC, while Tu and Roth (2012) collected 1,348 sentences associ2 Related work ated with 23 verb phrases annotated as composiThe problems posed by MWEs to NLP models have tional and idiomatic. Shwartz and Dagan (2019) been known for some time (Sag et al., 2002; Con- focused on a subset of noun compounds that are stant et al., 2017; Shwartz and Dagan, 2019). For only compositional or idiomatic from the dataset instance, Sag et al. (2002) refer to the idiomatic- provided by Reddy et al. (2011) and automatically ity problem and place the need for effective pro- added sentences from Wikipedia. Finally, the NCS cessing of MWEs o"
boos-etal-2014-identification,pearce-2002-comparative,0,\N,Missing
boos-etal-2014-identification,W12-3311,0,\N,Missing
boos-etal-2014-identification,C10-3015,1,\N,Missing
boos-etal-2014-identification,J93-1007,0,\N,Missing
boos-etal-2014-identification,N09-1059,0,\N,Missing
boos-etal-2014-identification,D07-1110,1,\N,Missing
boos-etal-2014-identification,C10-2144,0,\N,Missing
boos-etal-2014-identification,D07-1090,0,\N,Missing
boos-etal-2014-identification,calzolari-etal-2002-towards,0,\N,Missing
boos-etal-2014-identification,J03-2004,0,\N,Missing
boos-etal-2014-identification,W13-1001,0,\N,Missing
boos-etal-2014-identification,J10-4006,0,\N,Missing
boos-etal-2014-identification,P01-1005,0,\N,Missing
boos-etal-2014-identification,W12-3904,1,\N,Missing
boos-etal-2014-identification,P98-2127,0,\N,Missing
boos-etal-2014-identification,C98-2122,0,\N,Missing
boos-etal-2014-identification,C12-1013,0,\N,Missing
boos-etal-2014-identification,W08-2107,1,\N,Missing
boos-etal-2014-identification,W13-1014,0,\N,Missing
boos-etal-2014-identification,laranjeira-etal-2014-comparing,1,\N,Missing
boos-etal-2014-identification,W12-3301,1,\N,Missing
boos-etal-2014-identification,W13-1013,0,\N,Missing
boos-etal-2014-identification,W13-1002,0,\N,Missing
boos-etal-2014-identification,W13-1017,0,\N,Missing
boos-etal-2014-identification,korhonen-etal-2006-large,0,\N,Missing
C10-2120,1999.tc-1.8,0,0.307592,"Missing"
C10-2120,J03-3005,0,0.0982174,"Missing"
C10-2120,J03-3001,0,0.204183,"Missing"
C10-2120,2005.mtsummit-papers.11,0,0.0656565,"Missing"
C10-2120,W06-1208,0,0.260743,"Missing"
C10-2120,ramisch-etal-2010-mwetoolkit,1,0.585795,"ate techniques for the combination of n-gram counts from heterogeneous sources. Therefore, we will use the insights about the vocabulary differences presented in the previous section. In this evaluation, we measure the impact of the suggested techniques in the identification of noun–noun compounds in corpora. Noun compounds are very frequent in general-purpose and specialised texts (e.g. bus stop, European Union and gene activation). We extract them automatically from ep and from genia using a standard method based on POS patterns and association measures (Evert and Krenn, 2005; Pecina, 2008; Ramisch et al., 2010). 5.1 Experimental setup The evaluation task consists of, given a corpus of N words, extract all occurrences of adjacent pairs of nouns8 and then rank them using a standard statistical measure that estimates the association strength between the two nouns. Analogously to the formalism adopted in section 4.2, we assume that, for each corpus, we generate a set NN containing n-grams v1...n ∈ NN 9 for which we obtain n-gram counts from four sources. The elements in NN are generated by comparing the POS pattern noun–noun against all the bigrams in the corpus and keeping only those pairs of adjacent"
C10-2120,D07-1110,1,0.9292,"Missing"
C10-2120,J01-1001,0,0.12675,"percase words at the beginning of sentences were lowercased; • other words were not modified. 3 Genia contains manual POS tag annotation. Europarl was tagged using the TreeTagger (www.ims.uni-stuttgart.de/projekte/ corplex/TreeTagger). 1043 This lowercasing algorithm helps to deal with the massive use of abbreviations, acronyms, named entities, and formulae found in specialised corpora, such as those containing biomedical (and other specialised) scientific articles. For calculating arbitrary-sized n-grams in large textual corpora efficiently, we implemented a structure based on suffix arrays (Yamamoto and Church, 2001). While suffix trees are often used in LM tools, where n-grams have a fixed size, they are not fit for arbitrary length n-gram searches and can consume quite large amounts of memory to store all the node pointers. Suffix arrays, on the other hand, allow for arbitrary length n-grams to be counted in a time that is proportional to log(N), where N is the number of words (which is equivalent to the number of suffixes) in the corpus. Suffix arrays use a constant amount of memory proportional to N. In our implementation, where every word and every word position in the corpus are encoded as a 4-byte"
C10-3006,magnini-etal-2006-multilingual,0,0.0696992,"Missing"
C10-3015,ramisch-etal-2010-mwetoolkit,1,0.700251,"rds can be considered synonyms to transmit the idea of completeness? This is an example of a collocation, i.e. a sequence of words that tend to occur together and whose interpretation generally crosses the boundaries between words (Smadja, 1993). More generally, collocations are a frequent type of multiword expression (MWE), a sequence of words that presents some lexical, syntactic, semantic, pragmatic or statistical idiosyncrasies (Sag et al., 2002). The definition of MWE also includes a wide range of constructions like phrasal verbs (go 2 1 The first version of the toolkit was presented in (Ramisch et al., 2010b), where we described a language- and type-independent methodology. Inside the black box MWE identification is composed of two phases: first, we automatically generate a list of candi57 Coling 2010: Demonstration Volume, pages 57–60, Beijing, August 2010 c(w1 . . . wn ) N n × c(w1 . . . wn ) dice = ∑ni=1 c(wi ) c(w1 . . . wn ) pmi = log2 E(w1 . . . wn ) c(w1 . . . wn ) − E(w1 . . . wn ) p t-score = c(w1 . . . wn ) mle candidate = fEP fgoogle class status quo 137 US navy 4 International Cooperation 2 Cooperation Agreement 188 Panama Canal 2 security institution 5 lending institution 4 human ri"
C10-3015,C10-2120,1,0.605893,"rds can be considered synonyms to transmit the idea of completeness? This is an example of a collocation, i.e. a sequence of words that tend to occur together and whose interpretation generally crosses the boundaries between words (Smadja, 1993). More generally, collocations are a frequent type of multiword expression (MWE), a sequence of words that presents some lexical, syntactic, semantic, pragmatic or statistical idiosyncrasies (Sag et al., 2002). The definition of MWE also includes a wide range of constructions like phrasal verbs (go 2 1 The first version of the toolkit was presented in (Ramisch et al., 2010b), where we described a language- and type-independent methodology. Inside the black box MWE identification is composed of two phases: first, we automatically generate a list of candi57 Coling 2010: Demonstration Volume, pages 57–60, Beijing, August 2010 c(w1 . . . wn ) N n × c(w1 . . . wn ) dice = ∑ni=1 c(wi ) c(w1 . . . wn ) pmi = log2 E(w1 . . . wn ) c(w1 . . . wn ) − E(w1 . . . wn ) p t-score = c(w1 . . . wn ) mle candidate = fEP fgoogle class status quo 137 US navy 4 International Cooperation 2 Cooperation Agreement 188 Panama Canal 2 security institution 5 lending institution 4 human ri"
C10-3015,J93-1007,0,0.652141,"t of selected examples, comparing it with related work on MWE extraction. 1 MWEs in a nutshell One of the factors that makes Natural Language Processing (NLP) a challenging area is the fact that some linguistic phenomena are not entirely compositional or predictable. For instance, why do we prefer to say full moon instead of total moon or entire moon if all these words can be considered synonyms to transmit the idea of completeness? This is an example of a collocation, i.e. a sequence of words that tend to occur together and whose interpretation generally crosses the boundaries between words (Smadja, 1993). More generally, collocations are a frequent type of multiword expression (MWE), a sequence of words that presents some lexical, syntactic, semantic, pragmatic or statistical idiosyncrasies (Sag et al., 2002). The definition of MWE also includes a wide range of constructions like phrasal verbs (go 2 1 The first version of the toolkit was presented in (Ramisch et al., 2010b), where we described a language- and type-independent methodology. Inside the black box MWE identification is composed of two phases: first, we automatically generate a list of candi57 Coling 2010: Demonstration Volume, pag"
copestake-etal-2002-multiword,W98-0707,0,\N,Missing
copestake-etal-2002-multiword,P97-1018,1,\N,Missing
D07-1110,W02-2001,1,0.844682,"Missing"
D07-1110,baldwin-etal-2004-road,0,0.0343885,"Missing"
D07-1110,A97-1052,0,0.0539433,"Missing"
D07-1110,C02-2025,0,0.0124496,"wrong head words. However, the lexical type predictor of Zhang and Kordoni (2006) that we used in our experiments did not generate interesting new entries for them in the subsequent steps, and they were thus discarded, as discussed below. With the 30 MWE candidates, we extracted a sub-corpus from the BNC with 674 sentences which included at least one of these MWEs. The lexical acquisition technique described in Zhang and Kordoni (2006) was used with this subcorpus in order to acquire new lexical entries for the head words. The lexical acquisition model was trained with the Redwoods treebank (Oepen et al., 2002), following Zhang et al. (2006). The lexical prediction model predicted for each occurrence of the head words a most plausible lexical type in that context. Only those predictions that occurred 5 times or more were taken into consideration for the generation of the new lexical entries. As a result, we obtained 21 new lexical entries. These new lexical entries were later merged into the ERG lexicon. To evaluate the grammar performance with and without these new lexical entries, we 1. parsed the sub-corpus with/without new lexical entries and compared the grammar coverage; 2. inspected the parse"
D07-1110,pearce-2002-comparative,0,0.705957,"ring Aline Villavicencio♣♠ , Valia Kordoni♦ , Yi Zhang♦ , Marco Idiart♥ and Carlos Ramisch♣ ♣ Institute of Informatics, Federal University of Rio Grande do Sul (Brazil) ♠ Department of Computer Sciences, Bath University (UK) ♦ Department of Computational Linguistics, Saarland University, and DFKI GmbH (Germany) ♥ Institute of Physics, Federal University of Rio Grande do Sul (Brazil) avillavicencio@inf.ufrgs.br, {yzhang,kordoni}@coli.uni-sb.de idiart@if.ufrgs.br, ceramisch@inf.ufrgs.br Abstract Another difficulty for work on MWE identification is that of the evaluation of the results obtained (Pearce, 2002; Evert and Krenn, 2005), starting from the lack of consensus about a precise definition for MWEs (Villavicencio et al., 2005). This paper focuses on the evaluation of methods for the automatic acquisition of Multiword Expressions (MWEs) for robust grammar engineering. First we investigate the hypothesis that MWEs can be detected by the distinct statistical properties of their component words, regardless of their type, comparing 3 statistical measures: mutual information (MI), χ2 and permutation entropy (PE). Our overall conclusion is that at least two measures, MI and PE, seem to differentiat"
D07-1110,zhang-kordoni-2006-automated,1,0.854157,"candidates are used in 2 The combination of the “word with space” approach of Zhang et al. (2006) with the constructional approach we propose here is an interesting topic that we want to investigate in future research. 1040 this experiment. We used simple heuristics in order to extract the head words from these MWEs: • the n-grams are POS-tagged with an automatic tagger; • finite verbs in the n-grams are extracted as head words; • nouns are also extracted if there is no verb in the n-gram. Occasionally, the tagger errors might introduce wrong head words. However, the lexical type predictor of Zhang and Kordoni (2006) that we used in our experiments did not generate interesting new entries for them in the subsequent steps, and they were thus discarded, as discussed below. With the 30 MWE candidates, we extracted a sub-corpus from the BNC with 674 sentences which included at least one of these MWEs. The lexical acquisition technique described in Zhang and Kordoni (2006) was used with this subcorpus in order to acquire new lexical entries for the head words. The lexical acquisition model was trained with the Redwoods treebank (Oepen et al., 2002), following Zhang et al. (2006). The lexical prediction model p"
D07-1110,W06-1206,1,0.248015,"se corpora are probably compensated by their size. Finally, we show a qualitative evaluation of the results of automatically adding extracted MWEs to existing linguistic resources. We argue that such a process improves qualitatively, if a more compositional approach to grammar/lexicon automated extension is adopted. 1 Introduction The task of automatically identifying Multiword Expressions (MWEs) like phrasal verbs (break down) and compound nouns (coffee machine) using statistical measures has been the focus of considerable investigative effort, (e.g. Pearce (2002), Evert and Krenn (2005) and Zhang et al. (2006)). Given the heterogeneousness of the different phenomena that are considered to be MWEs, there is no consensus about which method is best suited for which type of MWE, and if there is a single method that can be successfully used for any kind of MWE. In this paper we investigate some of the issues involved in the evaluation of automatically extracted MWEs, from their extraction to their subsequent use in an NLP task. In order to do that, we present a discussion of different statistical measures, and the influence that the size and quality of different data sources have. We then perform a comp"
D07-1110,P04-1057,0,\N,Missing
D07-1110,W06-1208,0,\N,Missing
D14-1047,W02-0908,0,0.508083,"2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010), identifying and demoting bad neighbors (Ferret, 2013), or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013). For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of cont"
D14-1047,P10-2017,0,0.0803912,"like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, c October 25-29, 2014, Doha, Qatar. 2014"
D14-1047,ferret-2010-testing,0,0.071418,"an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010), identifying and demoting bad neighbors (Ferret, 2013), or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013). For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the ta"
D14-1047,P13-1055,0,0.551824,"emantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010), identifying and demoting bad neighbors (Ferret, 2013), or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013). For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an inf"
D14-1047,W05-0604,0,0.923337,"ts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, c Oct"
D14-1047,P98-2127,0,0.89512,"nclusions and discussion of future work. Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri. We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion. For evaluation, we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions. Results illustrate the sensitivity of distributional thesauri to filters. 1 Introduction Large-scale distributional thesauri created automatically from corpora (Grefenstette, 1994; Lin, 1998; Weeds et al., 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measur"
D14-1047,W03-1810,0,0.648621,"2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 1998; Curran and Moens, 2002). For instance, for intrinsic ev"
D14-1047,Q13-1029,0,0.017365,"endencies generated by RASP (Briscoe et al., 2006), using nouns (heads of NPs) which have subject and direct object relations with the target verb. Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject). The thesauri were constructed using Lin’s (1998) method. Lin’s version of the distributional hypothesis states that two words (verbs v1 and v2 in our case) are similar if they share a large proportion of contexts weighted by their information content, assessed with PMI (Bansal et al., 2012; Turney, 2013). In the literature, little attention is paid to context filters. To investigate their impact, we compare two kinds of filters, and before calculating similarity using Lin’s measure, we apply them to remove 4 Results Figure 1 shows average WordNet similarities for thesauri built filtering by frequency threshold th and by p most frequent contexts. Table 1 summarizes the parametrization leading to the best WordNet similarity for each kind of filter. In all cases we show the results obtained for different frequency ranges2 as well as the results when averaging over all verbs. 1 Even though larger"
D14-1047,W09-0211,0,0.0811251,"Missing"
D14-1047,C04-1146,0,0.0807586,"Missing"
D14-1047,N12-1095,0,\N,Missing
D14-1047,P06-4020,0,\N,Missing
D14-1047,C98-2122,0,\N,Missing
E99-1039,J92-2004,0,\N,Missing
E99-1039,J99-1002,0,\N,Missing
E99-1039,J92-2003,0,\N,Missing
J09-2001,J07-4002,0,0.0149345,"used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the quality of English–Japanese machine translation improved signiﬁcantly as a result. ¨ More recently, Atterer and Schutze (2007) challenged the real-world utility of methods based on the RRR data set, on the grounds that it is based on the availability of a gold-standard parse tree for a given input. They proposed that, instead, PP attachment be evaluated as a means of post-processing over the raw output of an actual parser, and produced results to indicate: (a) that a state-of-the-art parser (Bikel 2004) does remarkably well at PP attachment without a dedicated PP attachment module; but also (b) that post-processing based on a range of methods developed over the RRR data set (Collins and Brooks 1995; Toutanova, Mannin"
J09-2001,P98-1013,0,0.0554213,"Missing"
J09-2001,baldwin-etal-2004-road,1,0.714541,"eposition PVs allow limited coordination of PP objects (e.g., refer to the book and to the DVD), and the NP object of the selected preposition can be passivized (e.g., the book they referred to). Even subtler are the syntactic effects observed for determinerless PPs. The singular noun in the PP–D is often strictly countable (e.g., off screen, on break), resulting in syntactic markedness as, without a determiner, the noun does not constitute a saturated NP. This in turn dictates the need for a dedicated analysis in a linguistically motivated grammar in order to be able to avoid parse failures (Baldwin et al. 2004; van der Beek 2005). Additionally, there is considerable variation in the internal modiﬁability of determinerless PPs, with some not permitting any internal modiﬁcation (e.g., of course) and others allowing optional internal modiﬁcation (e.g., at considerable length). There are also, however, cases of obligatory internal modiﬁcation (e.g., at considerable/great expense vs. *at expense) and highly restricted internal modiﬁcation (e.g., at long last vs. *at great/short last). Balancing up these different possibilities in terms of over- and undergeneration in a grammar is far from trivial (Baldw"
J09-2001,W02-2001,1,0.68867,"Missing"
J09-2001,J04-4004,0,0.00886265,"ation. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the quality of English–Japanese machine translation improved signiﬁcantly as a result. ¨ More recently, Atterer and Schutze (2007) challenged the real-world utility of methods based on the RRR data set, on the grounds that it is based on the availability of a gold-standard parse tree for a given input. They proposed that, instead, PP attachment be evaluated as a means of post-processing over the raw output of an actual parser, and produced results to indicate: (a) that a state-of-the-art parser (Bikel 2004) does remarkably well at PP attachment without a dedicated PP attachment module; but also (b) that post-processing based on a range of methods developed over the RRR data set (Collins and Brooks 1995; Toutanova, Manning, and Ng 2004; Olteanu and Moldovan 2005) generally improves parser accuracy. In addition, they developed a variant of the RRR data set (RRR-sent) which contains full sentential contexts of possible PP attachment ambiguity. Others who have successfully built PP re-attachment models for speciﬁc parsers are Olteanu (2004) and Foth and Menzel (2006). Agirre, Baldwin, ¨ and Martinez"
J09-2001,W07-1604,0,0.0628822,"Missing"
J09-2001,copestake-etal-2002-multiword,1,0.504905,"Missing"
J09-2001,J01-1005,0,0.00769076,"for text classiﬁcation, and demonstrated that dependency tuples incorporating prepositions are a more effective document representation than simple words. In a direct challenge to the prevalent “stop word” perception of prepositions in information retrieval, Hansen (2005) and Lassen (2006) placed emphasis on not only prepositions but preposition semantics in a music retrieval system and ontology-based text search system, respectively. Information extraction is one application where prepositions are uncontroversially crucial to system accuracy, in terms of the role they play in named entities (Cucchiarelli and Velardi 2001; Toral 2005; Kozareva 2006) and in IE patterns, in linking the elements in a text (Appelt et al. 1993; Muslea 1999; Ono et al. 2001; Leroy and Chen 2002). Benamara (2005) used preposition semantics in a cooperative question answering system. In the context of cross-language question answering (CLQA), Hartrumpf, Helbig, and Osswald (2006) used MultiNet to interpret the semantics of German prepositions, and demonstrated that in instances where the answer passage contained a different preposition to that included in the original question, preposition semantics boosted the performance of their CL"
J09-2001,W07-1607,0,0.0580473,"Missing"
J09-2001,J09-1005,0,0.0399341,"Missing"
J09-2001,W07-1603,0,0.0220521,"1987), which is in turn derived from Wood (1979). The preposition sense inventory was used as the basis for extending VerbNet and led to signiﬁcant redevelopment of the verb class set (Kipper, Snyder, and Palmer 2004), in a poignant illustration of how preposition semantics impinges on verb semantics. In other work, Sablayrolles (1995) classiﬁed 199 simple and complex spatial prepositions into 16 classes. Lersundi and Agirre (2003) applied a similar methodology to Dorr and Habash (2002) in developing a multilingual sense inventory for Basque postpositions and English and Spanish prepositions. Fort and Guillaume (2007) developed a syntactico-semantic lexicon of French prepositions, partly based on PrepNet; their particular interest was in enhancing parsing performance. Old (2003) analyzed Roget’s Thesaurus and arrived at the conclusion that it was not a good source of standalone preposition semantics. Beavers (2003) analyzed the aspectual and path properties of goal-marking postpositions in Japanese, and proposed an analysis based on predicate and event restrictions. Boonthum, Toida, and Levinstein (2005, 2006) deﬁned a generalpurpose sense inventory of seven prepositions (but purportedly applicable to all"
J09-2001,P06-2029,0,0.0121475,"repositional MWE types that cause the greatest syntactic problems in English, in terms of their relative frequency and tendency for syntactic variation, are: 1. verb-particle constructions (VPCs), where the verb selects for an intransitive preposition (e.g., chicken out or hand in: Deh´e et al. [2002]); 2. prepositional verbs (PVs), where the verb selects for a transitive preposition (e.g., rely on or refer to: Huddleston and Pullum [2002]); 3. determinerless PPs (PP–Ds), where a PP is made up of a preposition and singular noun without a determiner (e.g., at school, off screen: Baldwin et al. [2006]). All three MWE types undergo limited syntactic variation (Sag et al. 2002). For example, transitive verb particle constructions generally undergo the particle alternation, 125 Computational Linguistics Volume 35, Number 2 whereby the particle may occur either adjacent to the verb (e.g., tear up the letter), or be separated from the verb by the NP complement (e.g., tear the letter up). Some VPCs readily occur with both orders (like tear up), while others have a strong preference for a particular order (e.g., take off —under the interpretation of having the day off—tends to occur in the partic"
J09-2001,I08-1059,0,0.00951014,"Missing"
J09-2001,W99-0614,0,0.022322,"uple is eatv , pizzan1 , with p , chopsticksn2 . The high degree of interest in PP attachment stems from it being a common phenomenon when parsing languages such as English, and hence a major cause of parser errors (Lin 2003). As such, it has implications for any task requiring full syntactic analysis or a notion of constituency, such as prosodic phrasing (van Herwijnen et al. 2003). Languages other than English with PP attachment ambiguity which have been the target of research include Dutch (van Herwijnen et al. 2003), French (Gaussier and Cancedda 2001; Gala and Lafourcade 2005), German (Hartrumpf 1999; Volk 2001, 2003; Foth and Menzel 2006), Spanish (Calvo, Gelbukh, and Kilgarriff 2005), and Swedish (Kokkinakis 2000; Aasa 2004; Volk 2006). PP attachment research has undergone a number of signiﬁcant paradigm shifts over the course of the last three decades, and been the target of interest of theoretical syntax, AI, psycholinguistics, statistical NLP, and statistical parsing. Early research on PP attachment focused on the development of heuristics intended to model human processing strategies, based on analysis of the competing parse trees ¨ independent of lexical or discourse context (Frazi"
J09-2001,W06-2105,0,0.127742,"Missing"
J09-2001,W07-1608,0,0.045722,"Missing"
J09-2001,W07-1601,0,0.0955897,"tual reality, in the context of interpreting the spatial information of prepositions. For example, Xu and Badler (2000) developed a geometric deﬁnition of the motion trajectories of prepositions, whereas Tokunaga, Koyama, and Saito (2005) use potential functions to estimate the spatial extent of Japanese spatial nouns (which combine with postpositions to have a similar syntactic and semantic proﬁle to English spatial prepositions). Kelleher and van Genabith (2003) proposed a method for interpreting in front of and behind in a virtual reality environment based on different frames of reference. Hying (2007) carried out an analysis of preposition semantics in the HRCR Map Task corpus, and used it to evaluate two models of projective prepositions. Kelleher and Kruijff (2005) developed a model for grounding spatial expressions in visual perception and also for modeling proximity, and Reichelt and Verleih (2005) developed the B3D system for generating a computational representation of prepositions in geospatial applications. Furlan, Baldwin, and Klippel (2007) used preposition occurrence in Web data as a means of classifying landmarks for use in route directions. Finally, Kelleher and Costello (2009"
J09-2001,isahara-etal-2008-development,0,0.0127084,"tions in MT, and automatic error correction of preposition usage in non-native speaker text. Following the lead of Saint-Dizier (2006b), Jørgensen and Lønning (2009), and others, we also hope to see more crosslinguistic and typological research on the lexical semantics of prepositions. Although there has been a steady proliferation of WordNets for different languages, linked variously to English WordNet (e.g., EuroWordNet for several European languages [Vossen 1998], BALKANET for Balkan languages [Stamou et al. 2002], HowNet for Chinese [Dong and Dong 2006], and Japanese WordNet for Japanese [Isahara et al. 2008]), they have tended to follow the lead of English WordNet and focus exclusively on content words. Given the increasing maturity of resources such as the Preposition Project and PrepNet, the time seems right to develop preposition sense inventories for more languages, linked back to English. On the basis of currently available resources and future efforts such as these, we believe there will be a steady lowering of the barrier to including a more systematic handling of prepositions in NLP applications. 137 Computational Linguistics Volume 35, Number 2 The purpose of this article has been to hi"
J09-2001,W04-2604,0,0.162756,"Missing"
J09-2001,P03-1054,0,0.0059965,"en a latent feature of all work on part of speech (POS) tagging and parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive prepositions (RP), and unselected intransitive prepositions (RB, along with a range of other adverbials).2 There have been only isolated cases of research where a dedicated approach has been used to distinguish between these three sub-usages, in the interests of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005). Two large areas of research on the syntactic aspects of prepositions are (a) PPattachment and (b) prepositions in multiword expressions, which are discussed in the following sections. Although this article will focus largely on the syntax of prepositions in English, prepositions in other languages naturally have their own challenges. Notable examples which have been the target of research in computational linguistics ¨ are adpositions in Estonian (Muischnek, Mu¨ urisep, and Puolakainen 2005), adpositions in Finnish (Lestrade 2006), short and long prepositions in"
J09-2001,W06-2102,0,0.0282577,"Missing"
J09-2001,W02-0802,0,0.135776,"S representation for the directional sense of up (as in up the stairs) is: (4) (toward Loc (nil 2) (UP Loc (nil 2) (∗ Thing 6))) where the numbers indicate the logical arguments of the predicates. This representation indicates that the logical subject of the PP (indexed by “2”; e.g., the piano in move the piano up the stairs) is relocated up in the direction of the logical argument (indexed by “6”; e.g., the stairs in our example), which is in turn a concrete thing. The LCS Lexicon was developed from a theoretical point of view and isn’t directly tied to corpus usage. The Preposition Project (Litkowski 2002; Litkowski and Hargraves 2005, 2006) is an attempt to develop a comprehensive semantic database for English prepositions, intended for NLP applications. The project took the New Oxford Dictionary of English (Pearsall 1998) as its source of preposition sense deﬁnitions, which it then ﬁne-tuned based on cross-comparison with both functionally tagged prepositions in FrameNet (Baker, Fillmore, and Lowe 1998) and the account of preposition semantics in a descriptive grammar of English (Quirk et al. 1985); it also draws partially on Dorr’s LCS deﬁnitions of prepositions. Importantly, the Prepositio"
J09-2001,W06-2106,0,0.216037,"Missing"
J09-2001,S07-1005,0,0.462645,"a decision tree classiﬁer based on a set of contextual features similar to those used in WSD systems. O’Hara and Wiebe (2009) is an updated version of this original research, using a broader range of resources. Ye and Baldwin (2006) also built on the earlier research, in attempting to enhance the accuracy of semantic role labeling with dedicated PP disambiguation. 131 Computational Linguistics Volume 35, Number 2 They demonstrated the potential for accurate preposition labeling to contribute to largescale improvements in overall semantic role labeling performance. As mentioned in Section 3.2, Litkowski and Hargraves (2007) ran a task on the WSD of prepositions at SemEval 2007, as a spinoff of the Preposition Project. The task focused on 34 prepositions, with a combined total of 332 senses. Similarly to a lexical sample WSD task, participants were required to disambiguate token instances of each preposition relative to the provided discrete sense inventory. Three teams participated in the task (Popescu, Tonelli, and Pianta 2007; Ye and Baldwin 2007; Yuret 2007), with all systems outperforming two baselines over both ﬁne- and coarse-grained sense inventories, through various combinations of lexical, syntactic, an"
J09-2001,U05-1008,1,0.776051,"ll work on part of speech (POS) tagging and parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive prepositions (RP), and unselected intransitive prepositions (RB, along with a range of other adverbials).2 There have been only isolated cases of research where a dedicated approach has been used to distinguish between these three sub-usages, in the interests of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005). Two large areas of research on the syntactic aspects of prepositions are (a) PPattachment and (b) prepositions in multiword expressions, which are discussed in the following sections. Although this article will focus largely on the syntax of prepositions in English, prepositions in other languages naturally have their own challenges. Notable examples which have been the target of research in computational linguistics ¨ are adpositions in Estonian (Muischnek, Mu¨ urisep, and Puolakainen 2005), adpositions in Finnish (Lestrade 2006), short and long prepositions in Polish (Tseng 2004), and the"
J09-2001,J93-2004,0,0.0342879,"Missing"
J09-2001,W03-1810,0,0.0601999,"Missing"
J09-2001,E03-1079,0,0.011156,"relative to a baseline parser. As part of this effort, they developed a standardized data set for exploration of the interaction between lexical semantics and parsing/PP attachment accuracy. 3 Because it was automatically extracted, the RRR data set is notoriously noisy. For instance, Pantel and Lin (2000) observed that 133 tuples contain the as either n1 or n2 . 124 Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications One signiﬁcant variation on the classic binary PP attachment task which attempts to generate a richer semantic characterisation of the PP is the work of Merlo (2003) and Merlo and Esteve Ferrer (2006), who included classiﬁcation of the PP as an argument or adjunct, making for a four-way classiﬁcation task. In this context, they found that PP attachment resolution for argument PPs is considerably easier than is the case for adjunct PPs. Returning to our observation that PP attachment can occur in multiple syntactic conﬁgurations, Merlo, Crocker, and Berthouzoz (1997) applied backed-off estimation to the problem of multiple PP attachment, in the form of 14 discrete syntactic conﬁgurations. Unsurprisingly, they found the task considerably harder than the bas"
J09-2001,W97-0317,0,0.0285464,"Missing"
J09-2001,J06-3002,0,0.0586485,"Missing"
J09-2001,odijk-2004-reusable,0,0.0197692,"Missing"
J09-2001,W03-0411,0,0.483873,"Missing"
J09-2001,J09-2002,0,0.213341,"Missing"
J09-2001,H05-1035,0,0.0667112,"P parser (Briscoe, Carroll, and Watson 2006) is highly effective at VPC identiﬁcation, and (b) that the incorporation of lexicalized models of selectional preferences can lead to modest improvements in parser accuracy. In terms of English PV extraction, Baldwin (2005b) proposed a method based on a combination of statistical measures and linguistic diagnostics, and demonstrated that the combination of statistics with linguistic diagnostics achieved the best extraction performance. Research on prepositional MWEs in languages other than English includes Krenn and Evert (2001) and Evert and Krenn (2005) on the extraction of German PP–verb collocations (which are similar to verbal idioms/verb–noun combinations in English [Fazly, Cook, and Stevenson 2009]) based on a range of lexical association measures. Pecina (2008) further extended this work using a much broader set of lexical association ¨ measures and classiﬁer combination. Looking at German, Domges et al. (2007) analyzed the productivity of PP–Ds headed by unter, and used their results to motivate a syntactic analysis of the phenomenon. For Dutch, van der Beek (2005) worked on the extraction of PP–Ds from the output of a parser, once ag"
J09-2001,P00-1014,0,0.0776453,"), log-linear models (Franz 1996), maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees (Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, and Moliner 1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the"
J09-2001,S07-1040,0,0.127872,"Missing"
J09-2001,W02-0312,0,0.0166734,"Missing"
J09-2001,P05-1034,0,0.00722033,"Missing"
J09-2001,H94-1048,0,0.238432,"Missing"
J09-2001,W06-2109,0,0.0511325,"Missing"
J09-2001,W93-0307,0,0.114962,"f PP attachment (e.g., cases of n1 being a pronoun [high attachment], or the PP post-modifying n1 in subject position [low attachment]) to derive smoothed estimates of Prhigh ( p|v), Prhigh ( NULL|n) (i.e., the probability of n not being post-modiﬁed by a PP), and Prlow ( p|n), which then form the basis of Prhigh ( p|v, n) and Prlow ( p|v, n), respectively. The proposed method was signiﬁcant in demonstrating the effectiveness of simple co-occurrence probabilities, without explicit semantics or discourse processing, and also in its ability to operate without explicitly annotated training data. Resnik and Hearst (1993) observed that PP attachment preferences are also conditioned on the semantics of the noun object of the preposition in the PP, as can be seen in our earlier example of Kim eats pizza with chopsticks/anchovies where chopsticks leads to verb attachment and anchovies to noun attachment. Although they were unable to come up with a model which was empirically superior to existing methods which did not represent the semantics of the noun object, this paved the way for a new wave of research using the full 4-tuple of v, n1 , p, n2 . 123 Computational Linguistics Volume 35, Number 2 The ﬁrst to suc"
J09-2001,E95-1040,0,0.0174849,", classiﬁed into ﬁve categories. The hierarchy is 6 See Levin and Rappaport-Hovav (in press) for a recent review of this style of semantics. 130 Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications derived from pioneering work by Sp¨arck Jones and Boguraev (1987), which is in turn derived from Wood (1979). The preposition sense inventory was used as the basis for extending VerbNet and led to signiﬁcant redevelopment of the verb class set (Kipper, Snyder, and Palmer 2004), in a poignant illustration of how preposition semantics impinges on verb semantics. In other work, Sablayrolles (1995) classiﬁed 199 simple and complex spatial prepositions into 16 classes. Lersundi and Agirre (2003) applied a similar methodology to Dorr and Habash (2002) in developing a multilingual sense inventory for Basque postpositions and English and Spanish prepositions. Fort and Guillaume (2007) developed a syntactico-semantic lexicon of French prepositions, partly based on PrepNet; their particular interest was in enhancing parsing performance. Old (2003) analyzed Roget’s Thesaurus and arrived at the conclusion that it was not a good source of standalone preposition semantics. Beavers (2003) analyzed"
J09-2001,saint-dizier-2006-prepnet,0,0.389946,"s an attempt to develop a compositional account of preposition semantics which interfaces with the semantics of the predicate (e.g., verb or predicative noun). Similarly to the English LCS Lexicon, it uses LCS as the descriptive language, in conjunction with typed λ-calculus and underspeciﬁed representations. Noteworthy elements of PrepNet are that it attempts to capture selectional constraints, metaphorical sense extension, and complex arguments. PrepNet was originally developed over French prepositions, but has since been applied to the analysis of instrumentals across a range of languages (Saint-Dizier 2006b). VerbNet (Kipper, Dang, and Palmer 2000; Kipper Schuler 2005) contains a shallow hierarchy of 50 spatial prepositions, classiﬁed into ﬁve categories. The hierarchy is 6 See Levin and Rappaport-Hovav (in press) for a recent review of this style of semantics. 130 Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications derived from pioneering work by Sp¨arck Jones and Boguraev (1987), which is in turn derived from Wood (1979). The preposition sense inventory was used as the basis for extending VerbNet and led to signiﬁcant redevelopment of the verb class set (Kipper, Snyde"
J09-2001,I08-2106,0,0.148755,"Missing"
J09-2001,W04-2118,0,0.060867,"Missing"
J09-2001,W06-3312,0,0.0198173,"ete syntactic conﬁgurations. Unsurprisingly, they found the task considerably harder than the basic V NP PP case, due to increased ambiguity and data sparseness. Mitchell (2004) similarly performed an extensive analysis of the Penn Treebank to investigate the different contexts PP attachment ambiguities occur in, and the relative ability of different PP attachment methods to disambiguate each. There have also been domain-speciﬁc methods proposed for PP attachment, for example, in the area of biomedicine (Hahn, Romacker, and Schulz 2002; Pustejovsky et al. 2002; Leroy, Chen, and Martinez 2003; Schuman and Bergler 2006). 2.2 The Syntax of Prepositional Multiword Expressions Prepositions are also often found as part of multiword expressions (MWEs), such as verb-particle constructions (break down), prepositional verbs (rely on), determinerless PPs (in hospital), complex prepositions (by means of ) and compound nominals (affairs of state). MWEs are lexical items which are composed of more than one word and are lexically, syntactically, semantically, pragmatically, and/or statistically idiosyncratic in some way (Sag et al. 2002). In this section, we present a brief overview of the syntax of the key prepositional"
J09-2001,2003.mtsummit-papers.44,0,0.0189971,"Missing"
J09-2001,P93-1042,0,0.0400489,"exposure in the NLP literature but has been a latent feature of all work on part of speech (POS) tagging and parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive prepositions (RP), and unselected intransitive prepositions (RB, along with a range of other adverbials).2 There have been only isolated cases of research where a dedicated approach has been used to distinguish between these three sub-usages, in the interests of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005). Two large areas of research on the syntactic aspects of prepositions are (a) PPattachment and (b) prepositions in multiword expressions, which are discussed in the following sections. Although this article will focus largely on the syntax of prepositions in English, prepositions in other languages naturally have their own challenges. Notable examples which have been the target of research in computational linguistics ¨ are adpositions in Estonian (Muischnek, Mu¨ urisep, and Puolakainen 2005), adpositions in Finn"
J09-2001,P98-2201,0,0.0394995,"Missing"
J09-2001,J87-1007,0,0.395324,"Missing"
J09-2001,A00-1034,0,0.133018,"Missing"
J09-2001,W97-0109,0,0.0185413,"elemans, and Veenstra 1997; Zhao and Lin 2004), log-linear models (Franz 1996), maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees (Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, and Moliner 1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in"
J09-2001,W08-1205,0,0.0228208,"Missing"
J09-2001,C08-1109,0,0.0128719,"Missing"
J09-2001,W00-1308,0,0.0182022,"ure but has been a latent feature of all work on part of speech (POS) tagging and parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive prepositions (RP), and unselected intransitive prepositions (RB, along with a range of other adverbials).2 There have been only isolated cases of research where a dedicated approach has been used to distinguish between these three sub-usages, in the interests of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005). Two large areas of research on the syntactic aspects of prepositions are (a) PPattachment and (b) prepositions in multiword expressions, which are discussed in the following sections. Although this article will focus largely on the syntax of prepositions in English, prepositions in other languages naturally have their own challenges. Notable examples which have been the target of research in computational linguistics ¨ are adpositions in Estonian (Muischnek, Mu¨ urisep, and Puolakainen 2005), adpositions in Finnish (Lestrade 2006), short a"
J09-2001,N07-1007,0,0.059034,"Missing"
J09-2001,W06-2103,0,0.0494386,"Missing"
J09-2001,E03-1051,0,0.0846401,"(Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the quality of English–Japanese machine translation improved signiﬁcantly as a result. ¨ More recently, Atterer and Schutze (2007) challenged the real-world utility of methods based on the RRR data set, on the grounds that it is based on the ava"
J09-2001,C02-1004,0,0.0113591,"(Franz 1996), maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees (Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, and Moliner 1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the quality of"
J09-2001,W06-2112,0,0.0587411,"sing languages such as English, and hence a major cause of parser errors (Lin 2003). As such, it has implications for any task requiring full syntactic analysis or a notion of constituency, such as prosodic phrasing (van Herwijnen et al. 2003). Languages other than English with PP attachment ambiguity which have been the target of research include Dutch (van Herwijnen et al. 2003), French (Gaussier and Cancedda 2001; Gala and Lafourcade 2005), German (Hartrumpf 1999; Volk 2001, 2003; Foth and Menzel 2006), Spanish (Calvo, Gelbukh, and Kilgarriff 2005), and Swedish (Kokkinakis 2000; Aasa 2004; Volk 2006). PP attachment research has undergone a number of signiﬁcant paradigm shifts over the course of the last three decades, and been the target of interest of theoretical syntax, AI, psycholinguistics, statistical NLP, and statistical parsing. Early research on PP attachment focused on the development of heuristics intended to model human processing strategies, based on analysis of the competing parse trees ¨ independent of lexical or discourse context (Frazier 1979; Schutze 1995). For example, Minimal Attachment was the strategy of choosing the attachment site which “minimizes” the parse tree, a"
J09-2001,P90-1004,0,0.024064,"this would be unable to disambiguate between Examples (2) and (3) as they both contain the same number of nodes. Late Attachment, on the other hand, was the strategy of attaching “low” in the parse tree, corresponding to Example (2). Ford, Bresnan, and Kaplan (1982) proposed an alternative heuristic strategy, based on the existence of p in a subcategorization frame for v. In later research, Pereira (1985) described a method for incorporating Right Association and Minimal Attachment 122 Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications into a shift-reduce parser, and Whittemore and Ferrara (1990) developed a rule-based algorithm to combine various attachment preferences on the basis of empirical evaluation of the predictive power of each. Syntactic preferences were of course a blunt instrument in dealing with PP attachment, and largely ineffectual in predicting the difference in PP attachment between Kim eats pizza with chopsticks (verb attachment) and Kim eats pizza with anchovies (noun attachment), for example. This led to a shift away from syntactic methods in the 1980s towards AI-inspired techniques which used world knowledge to resolve PP attachment ambiguity. In the case of Exam"
J09-2001,S07-1051,1,0.50376,"tial for accurate preposition labeling to contribute to largescale improvements in overall semantic role labeling performance. As mentioned in Section 3.2, Litkowski and Hargraves (2007) ran a task on the WSD of prepositions at SemEval 2007, as a spinoff of the Preposition Project. The task focused on 34 prepositions, with a combined total of 332 senses. Similarly to a lexical sample WSD task, participants were required to disambiguate token instances of each preposition relative to the provided discrete sense inventory. Three teams participated in the task (Popescu, Tonelli, and Pianta 2007; Ye and Baldwin 2007; Yuret 2007), with all systems outperforming two baselines over both ﬁne- and coarse-grained sense inventories, through various combinations of lexical, syntactic, and semantic features. The bestperforming system achieved F-scores of 0.818 and 0.861 over ﬁne- and coarse-grained senses, respectively (Ye and Baldwin 2007). In other papers dedicated to prepositional WSD, Boonthum, Toida, and Levinstein (2005, 2006) proposed a semantic collocation-based approach to preposition interpretation, and demonstrated the import of the method in a paraphrase recognition task. Alam (2003, 2004) used decisi"
J09-2001,P98-2234,0,0.0307679,"97; Zhao and Lin 2004), log-linear models (Franz 1996), maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees (Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, and Moliner 1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and d"
J09-2001,S07-1044,0,0.00886363,"position labeling to contribute to largescale improvements in overall semantic role labeling performance. As mentioned in Section 3.2, Litkowski and Hargraves (2007) ran a task on the WSD of prepositions at SemEval 2007, as a spinoff of the Preposition Project. The task focused on 34 prepositions, with a combined total of 332 senses. Similarly to a lexical sample WSD task, participants were required to disambiguate token instances of each preposition relative to the provided discrete sense inventory. Three teams participated in the task (Popescu, Tonelli, and Pianta 2007; Ye and Baldwin 2007; Yuret 2007), with all systems outperforming two baselines over both ﬁne- and coarse-grained sense inventories, through various combinations of lexical, syntactic, and semantic features. The bestperforming system achieved F-scores of 0.818 and 0.861 over ﬁne- and coarse-grained senses, respectively (Ye and Baldwin 2007). In other papers dedicated to prepositional WSD, Boonthum, Toida, and Levinstein (2005, 2006) proposed a semantic collocation-based approach to preposition interpretation, and demonstrated the import of the method in a paraphrase recognition task. Alam (2003, 2004) used decision trees to d"
J09-2001,W97-1016,0,0.0754138,"Missing"
J09-2001,W99-0606,0,\N,Missing
J09-2001,J82-3004,0,\N,Missing
J09-2001,J87-3005,0,\N,Missing
J09-2001,J93-1005,0,\N,Missing
J09-2001,W99-0628,0,\N,Missing
J09-2001,E03-1044,1,\N,Missing
J09-2001,W06-2110,1,\N,Missing
J09-2001,W07-1605,0,\N,Missing
J09-2001,W06-1207,0,\N,Missing
J09-2001,W01-0707,0,\N,Missing
J09-2001,W06-2107,0,\N,Missing
J09-2001,W04-2608,0,\N,Missing
J09-2001,W06-2113,0,\N,Missing
J09-2001,A97-1052,0,\N,Missing
J09-2001,W04-0403,0,\N,Missing
J09-2001,J09-2003,0,\N,Missing
J09-2001,P08-1037,1,\N,Missing
J09-2001,calzolari-etal-2002-towards,0,\N,Missing
J09-2001,C98-1013,0,\N,Missing
J09-2001,C98-2229,0,\N,Missing
J09-2001,P03-2026,0,\N,Missing
J09-2001,P93-1032,0,\N,Missing
J09-2001,C98-2196,0,\N,Missing
J09-2001,W03-1812,1,\N,Missing
J09-2001,J10-4006,0,\N,Missing
J09-2001,P06-4020,0,\N,Missing
J09-2001,J09-2004,0,\N,Missing
J09-2001,J09-2005,0,\N,Missing
J09-2001,E06-3004,0,\N,Missing
J09-2001,P03-1065,0,\N,Missing
J09-2001,W07-1602,1,\N,Missing
J09-2001,W02-0804,0,\N,Missing
J09-2001,P98-2127,0,\N,Missing
J09-2001,C98-2122,0,\N,Missing
J09-2001,P07-1072,0,\N,Missing
J09-2001,W06-2104,0,\N,Missing
J19-1001,N09-1003,0,0.063104,"Missing"
J19-1001,J08-4004,0,0.212898,"ositional or fully idiomatic) for all scores. These findings can be partly explained by end-of-scale effects, that result in greater variability for the intermediate scores in the Likert scale (from 1 to 4) that correspond to the partly compositional cases. Hence, we expect that it will be easier to predict the compositionality of idiomatic/compositional compounds than of partly compositional ones. Inter-Annotator Agreement (α). To measure inter-annotator agreement of multiple participants, taking into account the distance between the ordinal ratings of the Likert scale, we adopt the α score (Artstein and Poesio 2008). The α score is more appropriate for ordinal data than traditional agreement scores for categorical data, such as Cohen’s and Fleiss’ κ (Cohen 1960; Fleiss and Cohen 1973). However, due to the use of crowdsourcing, most participants rated only a small number of compounds with very limited chance of overlap among them: the average number of answers per participant is 13.6 for EN-comp90 , 10.2 for EN-compExt , 33.7 for FR-comp, and 53.5 for PT-comp. Because the 11 Participants with negative correlation with the mean, and answers farther than ±1.5 from the mean. 12 Only FR-comp is shown as the o"
J19-1001,W03-1809,0,0.184601,"Missing"
J19-1001,P14-1023,0,0.105379,"Missing"
J19-1001,J10-4006,0,0.309322,"2005; Camacho-Collados, Pilehvar, and Navigli 2015; Lapesa and Evert 2017) and for modeling syntactic and semantic analogies between word pairs (Mikolov, Yih, and Zweig 2013). These representations for individual words can also be combined to create representations for larger units such as phrases, sentences, and even whole documents, using simple additive and multiplicative vector operations (Mitchell and Lapata 2010; Reddy, McCarthy, and Manandhar 2011; Mikolov et al. 2013; Salehi, Cook, and Baldwin 2015), syntax-based lexical functions (Socher et al. 2012), or matrix and tensor operations (Baroni and Lenci 2010; Bride, Van de Cruys, and Asher 2015). However, it is not clear to what extent this approach is adequate in the case of idiomatic multiword expressions (MWEs). MWEs fall into a wide spectrum of compositionality; that is, some MWEs are more compositional (e.g., olive oil) while others are more idiomatic (Sag et al. 2002; Baldwin and Kim 2010). In the latter case, the meaning of the MWE may not be straightforwardly related to the meanings of its parts, creating a challenge for the principle of compositionality (e.g., snake oil as a product of questionable benefit, not necessarily an oil and cer"
J19-1001,boos-etal-2014-identification,1,0.883664,"Missing"
J19-1001,P15-1028,0,0.036196,"Missing"
J19-1001,P15-2001,0,0.0314647,"Missing"
J19-1001,W15-0903,0,0.108306,"n that MWEs are frequent in languages (Sag et al. 2002), identifying idiomaticity and producing accurate semantic representations for compositional and idiomatic cases is of relevance to NLP tasks and applications that involve some form of semantic processing, including semantic parsing (Hwang et al. 2010; Jagfeld and van der Plas 2015), word sense disambiguation (Finlayson and Kulkarni 2011; Schneider et al. 2016), 1 Attributed to Frege (1892/1960). 2 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds and machine translation (Ren et al. 2009; Carpuat and Diab 2010; Cap et al. 2015; Salehi et al. 2015). Moreover, the evaluation of DSMs on tasks involving MWEs, such as compositionality prediction, has the potential to drive their development towards new directions. The main hypothesis of our work is that, if the meaning of a compositional nominal compound can be derived from a combination of its parts, this translates in DSMs as similar vectors for a compositional nominal compound and for the combination of the vectors of its parts using some vector operation, that we refer to as composition function. Conversely we can use the lack of similarity between the nominal compo"
J19-1001,N10-1029,0,0.0596242,"Missing"
J19-1001,J90-1003,0,0.608336,"hat are more frequent tend to be assigned higher compositionality scores. However, frequency alone is not enough to predict compositionality, and further investigation is needed to determine if compositionality and frequency are also correlated with other factors. 14 r2arith and r2geom are .91 and .96 in PT-comp, .90 and .96 in EN-comp90 , and .92 and .95 in EN-compExt . 13 Computational Linguistics Volume 45, Number 1 We also analyzed the correlation between compositionality and conventionalization to determine if more idiomatic compounds correspond to more conventionalized ones. We use PMI (Church and Hanks 1990) as a measure of conventionalization, as it indicates the strength of association between the components (Farahmand, Smith, and Nivre 2015). We found no statistically significant correlation between compositionality and PMI. 4. Compositionality Prediction Framework We propose a compositionality prediction framework15 including the following elements: a DSM, created from corpora using existing state-of-the-art models that generate corpus-derived vectors16 for compounds w1 w2 and for their components w1 and w2 ; a composition function; and a set of predicted compositionality scores (pc). The fra"
J19-1001,J17-4005,1,0.911229,"Missing"
J19-1001,P16-1187,1,0.856526,"Missing"
J19-1001,L16-1194,1,0.854555,"tion (Section 2.4). 2 This article significantly extends and updates previous publications: 1. We consolidate the description of the data sets introduced in Ramisch et al. (2016) and Ramisch, Cordeiro, and Villavicencio (2016) by adding details about data collection, filtering, and results of a thorough analysis studying the correlation between compositionality and related variables. 2. We extend the compositionality prediction framework described in Cordeiro, Ramisch, and Villavicencio (2016) by adding and evaluating new composition functions and DSMs. 3. We extend the evaluation reported in Cordeiro et al. (2016) not only by adding Portuguese, but also by evaluating additional parameters: corpus size, composition functions, and new DSMs. 3 Computational Linguistics Volume 45, Number 1 2.1 Distributional Semantic Models Distributional semantic models (DSMs) use context information to represent the meaning of lexical units as vectors. These vectors are built assuming the distributional hypothesis, whose central idea is that the meaning of a word can be learned based on the contexts where it appears—or, as popularized by Firth (1957), “you shall know a word by the company it keeps.” Formally, a DSM attem"
J19-1001,P02-1030,0,0.120372,"e characteristics of the training corpus such 3 After dimensionality reduction, nowadays word vectors are often called word embeddings. 4 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds as size (Mikolov, Yih, and Zweig 2013) as well as frequency thresholds and filters (Ferret 2013; Padro´ et al. 2014b), genre (Lapesa and Evert 2014), preprocessing (Pado´ and Lapata 2003, 2007), and type of context (window vs. syntactic dependencies) (Agirre et al. 2009; Lapesa and Evert 2017). Characteristics of the model include the choice of association and similarity measures (Curran and Moens 2002), dimensionality reduction strategies (Van de Cruys et al. 2012), and the use of subsampling and negative sampling techniques (Mikolov, Yih, and Zweig 2013). However, the particular impact of these factors on the quality of the resulting DSM may be heterogeneous and depends on the task and model (Lapesa and Evert 2014). Because there is no consensus about a single optimal model that works for all tasks, we compare a variety of models (Section 5) to determine which are best suited for our compositionality prediction framework. 2.2 Compositionality Prediction Before adopting the principle of com"
J19-1001,W15-0904,0,0.275318,"Missing"
J19-1001,J09-1005,0,0.0990759,"Missing"
J19-1001,P13-1055,0,0.0187504,"solve the prediction task. The weight parameters that connect the unity representing wi with the d-dimensional hidden layer are taken as its vector representation v(wi ). There are a number of factors that may influence the ability of a DSM to accurately learn a semantic representation. These include characteristics of the training corpus such 3 After dimensionality reduction, nowadays word vectors are often called word embeddings. 4 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds as size (Mikolov, Yih, and Zweig 2013) as well as frequency thresholds and filters (Ferret 2013; Padro´ et al. 2014b), genre (Lapesa and Evert 2014), preprocessing (Pado´ and Lapata 2003, 2007), and type of context (window vs. syntactic dependencies) (Agirre et al. 2009; Lapesa and Evert 2017). Characteristics of the model include the choice of association and similarity measures (Curran and Moens 2002), dimensionality reduction strategies (Van de Cruys et al. 2012), and the use of subsampling and negative sampling techniques (Mikolov, Yih, and Zweig 2013). However, the particular impact of these factors on the quality of the resulting DSM may be heterogeneous and depends on the task an"
J19-1001,W11-0805,0,0.0272043,"er MWE categories by addressing their variability in future work. Furthermore, to determine to what extent these approaches are also adequate cross-lingually, we evaluate them in three languages: English, French, and Portuguese. Given that MWEs are frequent in languages (Sag et al. 2002), identifying idiomaticity and producing accurate semantic representations for compositional and idiomatic cases is of relevance to NLP tasks and applications that involve some form of semantic processing, including semantic parsing (Hwang et al. 2010; Jagfeld and van der Plas 2015), word sense disambiguation (Finlayson and Kulkarni 2011; Schneider et al. 2016), 1 Attributed to Frege (1892/1960). 2 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds and machine translation (Ren et al. 2009; Carpuat and Diab 2010; Cap et al. 2015; Salehi et al. 2015). Moreover, the evaluation of DSMs on tasks involving MWEs, such as compositionality prediction, has the potential to drive their development towards new directions. The main hypothesis of our work is that, if the meaning of a compositional nominal compound can be derived from a combination of its parts, this translates in DSMs as similar vectors for a com"
J19-1001,W05-0604,0,0.0251484,"is’ distributional hypothesis that the meaning of a word can be inferred from the context in which it occurs (Harris 1954; Firth 1957). In DSMs, words are usually represented as vectors that, to some extent, capture cooccurrence patterns in corpora (Lin 1998; Landauer, Foltz, and Laham 1998; Mikolov et al. 2013; Baroni, Dinu, and Kruszewski 2014). Evaluation of DSMs has focused on obtaining accurate semantic representations for words, and state-of-the-art models are already capable of obtaining a high level of agreement with human judgments for predicting synonymy or similarity between words (Freitag et al. 2005; Camacho-Collados, Pilehvar, and Navigli 2015; Lapesa and Evert 2017) and for modeling syntactic and semantic analogies between word pairs (Mikolov, Yih, and Zweig 2013). These representations for individual words can also be combined to create representations for larger units such as phrases, sentences, and even whole documents, using simple additive and multiplicative vector operations (Mitchell and Lapata 2010; Reddy, McCarthy, and Manandhar 2011; Mikolov et al. 2013; Salehi, Cook, and Baldwin 2015), syntax-based lexical functions (Socher et al. 2012), or matrix and tensor operations (Baro"
J19-1001,W11-0115,0,0.0207817,", McCarthy, and Manandhar 2011; Schulte ¨ im Walde, Muller, and Roller 2013; Salehi, Cook, and Baldwin 2015). These weights can capture the asymmetric contribution of each of the components to the semantics of the whole phrase (Bannard, Baldwin, and Lascarides 2003; Reddy, McCarthy, and Manandhar 2011). For example, in flea market, it is the head (market) that has a clear contribution to the overall meaning, whereas in couch potato it is the modifier (couch). The additive model can be generalized to use a matrix of multiplicative coefficients, which can be estimated through linear regression (Guevara 2011). This model can be 4 The task of determining whether a phrase is compositional is closely related to MWE discovery (Constant et al. 2017), which aims to automatically extract MWE lists from corpora. 5 Computational Linguistics Volume 45, Number 1 further modified to learn polynomial projections of higher degree, with quadratic projections yielding particularly promising results (Yazdani, Farahmand, and Henderson 2015). These models come with the caveat of being supervised, requiring some amount of pre-annotated data in the target language. Because of these requirements, our study focuses on u"
J19-1001,E17-1006,0,0.0188226,"ments, our study focuses on unsupervised compositionality prediction methods only, based exclusively on automatically POS-tagged and lemmatized monolingual corpora. Alternatives to the additive model include the multiplicative model and its variants (Mitchell and Lapata 2008). However, results suggest that this representation is inferior to the one obtained through the additive model (Reddy, McCarthy, and Manandhar 2011; Salehi, Cook, and Baldwin 2015). Recent work on predicting intracompound semantics also supports that additive models tend to yield better results than multiplicative models (Hartung et al. 2017). The third ingredient is the measure of similarity between the compositionally constructed vector and its actual corpus-based representation. Cosine similarity is the most commonly used measure for compositionality prediction in the literature (Schone ¨ and Jurafsky 2001; Reddy, McCarthy, and Manandhar 2011; Schulte im Walde, Muller, and Roller 2013; Salehi, Cook, and Baldwin 2015). Alternatively, one can calculate the overlap between the distributional neighbors of the whole phrase and those of the component words (McCarthy, Keller, and Carroll 2003), or the number of single-word distributio"
J19-1001,S13-2025,0,0.276868,"Missing"
J19-1001,W10-1810,0,0.0249222,"Missing"
J19-1001,N15-2005,0,0.060798,"Missing"
J19-1001,W14-1503,0,0.12443,"0 dimensions. • W ORD F ORM: One of the four word-form and stopword removal variants used to represent a corpus, in Section 5.1: surface+ , surface, lemma, and lemmaPoS . They represent different levels of specificity in the informational content of the tokens, and may have a language-dependent impact on the performance of compositionality prediction. 30 https://github.com/alexandres/lexvec 31 This is in line with the authors’ threshold suggestions (Salle, Villavicencio, and Idiart 2016). 32 Common window sizes are between 1+1 and 10+10, but a few works adopt larger sizes like 16+16 or 20+20 (Kiela and Clark 2014; Lapesa and Evert 2014). 18 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds 5.3 Evaluation Metrics To evaluate a compositionality prediction configuration, we calculate Spearman’s ρ rank correlation between the predicted compositionality scores (pc)s and the human compositionality scores (hc)s for the compounds that appear in the evaluation data set. We mostly use the rank correlation instead of linear correlation (Pearson) because we are interested in the framework’s ability to order compounds from least to most compositional, regardless of the actual predicted"
J19-1001,N16-1039,0,0.0487032,"Missing"
J19-1001,S14-1021,0,0.246147,"nguistics Volume 45, Number 1 • Farahmand, Smith, and Nivre (2015) collected judgments for 1,042 English noun–noun compounds. Each compound has binary judgments regarding non-compositionality and conventionalization given by four expert annotators (both native and non-native speakers). A hard threshold is applied so that compounds are considered as noncompositional if at least two annotators say so (Yazdani, Farahmand, and Henderson 2015), and the total compositionality score is given by the sum of the four binary judgments. This data set will be referred to as Farahmand in our experiments. • Kruszewski and Baroni (2014) built the Norwegian Blue Parrot data set, containing judgments for modifier-head phrases in English. The judgments consider whether the phrase is (1) an instance of the concept denoted by the head (e.g., dead parrot and parrot) and (2) a member of the more general concept that includes the head (e.g., dead parrot and pet), along with typicality ratings, with 5,849 judgments in total. • Roller, Schulte im Walde, and Scheible (2013) collected judgments for a set of 244 German noun–noun compounds, each compound with an average of around 30 judgments on a compositionality scale from 1 to 7, obtai"
J19-1001,Q14-1041,0,0.142602,"meters that connect the unity representing wi with the d-dimensional hidden layer are taken as its vector representation v(wi ). There are a number of factors that may influence the ability of a DSM to accurately learn a semantic representation. These include characteristics of the training corpus such 3 After dimensionality reduction, nowadays word vectors are often called word embeddings. 4 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds as size (Mikolov, Yih, and Zweig 2013) as well as frequency thresholds and filters (Ferret 2013; Padro´ et al. 2014b), genre (Lapesa and Evert 2014), preprocessing (Pado´ and Lapata 2003, 2007), and type of context (window vs. syntactic dependencies) (Agirre et al. 2009; Lapesa and Evert 2017). Characteristics of the model include the choice of association and similarity measures (Curran and Moens 2002), dimensionality reduction strategies (Van de Cruys et al. 2012), and the use of subsampling and negative sampling techniques (Mikolov, Yih, and Zweig 2013). However, the particular impact of these factors on the quality of the resulting DSM may be heterogeneous and depends on the task and model (Lapesa and Evert 2014). Because there is no"
J19-1001,E17-2063,0,0.0459986,"Missing"
J19-1001,Q15-1016,0,0.120596,"Missing"
J19-1001,P98-2127,0,0.361882,"processing (NLP), this is an attractive way of linearly deriving the meaning of larger units from their components, performing the semantic interpretation of any text. For representing the meaning of individual words and their combinations in computational systems, distributional semantic models (DSMs) have been widely used. DSMs are based on Harris’ distributional hypothesis that the meaning of a word can be inferred from the context in which it occurs (Harris 1954; Firth 1957). In DSMs, words are usually represented as vectors that, to some extent, capture cooccurrence patterns in corpora (Lin 1998; Landauer, Foltz, and Laham 1998; Mikolov et al. 2013; Baroni, Dinu, and Kruszewski 2014). Evaluation of DSMs has focused on obtaining accurate semantic representations for words, and state-of-the-art models are already capable of obtaining a high level of agreement with human judgments for predicting synonymy or similarity between words (Freitag et al. 2005; Camacho-Collados, Pilehvar, and Navigli 2015; Lapesa and Evert 2017) and for modeling syntactic and semantic analogies between word pairs (Mikolov, Yih, and Zweig 2013). These representations for individual words can also be combined to"
J19-1001,P99-1041,0,0.435749,"Missing"
J19-1001,W03-1810,0,0.420844,"Missing"
J19-1001,N13-1090,0,0.708849,"y of linearly deriving the meaning of larger units from their components, performing the semantic interpretation of any text. For representing the meaning of individual words and their combinations in computational systems, distributional semantic models (DSMs) have been widely used. DSMs are based on Harris’ distributional hypothesis that the meaning of a word can be inferred from the context in which it occurs (Harris 1954; Firth 1957). In DSMs, words are usually represented as vectors that, to some extent, capture cooccurrence patterns in corpora (Lin 1998; Landauer, Foltz, and Laham 1998; Mikolov et al. 2013; Baroni, Dinu, and Kruszewski 2014). Evaluation of DSMs has focused on obtaining accurate semantic representations for words, and state-of-the-art models are already capable of obtaining a high level of agreement with human judgments for predicting synonymy or similarity between words (Freitag et al. 2005; Camacho-Collados, Pilehvar, and Navigli 2015; Lapesa and Evert 2017) and for modeling syntactic and semantic analogies between word pairs (Mikolov, Yih, and Zweig 2013). These representations for individual words can also be combined to create representations for larger units such as phrase"
J19-1001,P08-1028,0,0.859792,"he compositional meaning of a phrase is calculated from the meanings of its parts. Third, we need the compositionality measure itself, which estimates the similarity between the compositionally constructed meaning of a phrase and its observed meaning, derived from corpora. There are a number of alternatives for each of the ingredients, and throughout this article we call a specific choice of the three ingredients a compositionality prediction configuration. Regarding the second ingredient, that is, the mathematical model of compositional meaning, the most natural choice is the additive model (Mitchell and Lapata 2008). In the additive model, the compositional meaning of a phrase P w1 w2 . . . wn is calculated as a linear combination of the word vectors of its components: i βi v(wi ), where v(wi ) is a d-dimensional vector for each word wi , and the βi coefficients assign different weights to the representation of each word (Reddy, McCarthy, and Manandhar 2011; Schulte ¨ im Walde, Muller, and Roller 2013; Salehi, Cook, and Baldwin 2015). These weights can capture the asymmetric contribution of each of the components to the semantics of the whole phrase (Bannard, Baldwin, and Lascarides 2003; Reddy, McCarthy"
J19-1001,nivre-etal-2006-maltparser,0,0.0915977,"Missing"
J19-1001,P03-1017,0,0.224052,"Missing"
J19-1001,J07-2002,0,0.190255,"Missing"
J19-1001,padro-etal-2014-comparing,1,0.907159,"Missing"
J19-1001,D14-1047,1,0.898559,"Missing"
J19-1001,D14-1162,0,0.0803558,"Missing"
J19-1001,P16-2026,1,0.883726,"de only a brief introduction here, underlining their most relevant characteristics to our framework (Section 2.1). Then, we define compositionality prediction and discuss existing approaches, focusing on distributional techniques for multiword expressions (Section 2.2). Our framework is evaluated on nominal compounds, and we discuss their relevant properties (Section 2.3) along with existing data sets for evaluating compositionality prediction (Section 2.4). 2 This article significantly extends and updates previous publications: 1. We consolidate the description of the data sets introduced in Ramisch et al. (2016) and Ramisch, Cordeiro, and Villavicencio (2016) by adding details about data collection, filtering, and results of a thorough analysis studying the correlation between compositionality and related variables. 2. We extend the compositionality prediction framework described in Cordeiro, Ramisch, and Villavicencio (2016) by adding and evaluating new composition functions and DSMs. 3. We extend the evaluation reported in Cordeiro et al. (2016) not only by adding Portuguese, but also by evaluating additional parameters: corpus size, composition functions, and new DSMs. 3 Computational Linguistics"
J19-1001,W16-1804,1,0.852714,"de only a brief introduction here, underlining their most relevant characteristics to our framework (Section 2.1). Then, we define compositionality prediction and discuss existing approaches, focusing on distributional techniques for multiword expressions (Section 2.2). Our framework is evaluated on nominal compounds, and we discuss their relevant properties (Section 2.3) along with existing data sets for evaluating compositionality prediction (Section 2.4). 2 This article significantly extends and updates previous publications: 1. We consolidate the description of the data sets introduced in Ramisch et al. (2016) and Ramisch, Cordeiro, and Villavicencio (2016) by adding details about data collection, filtering, and results of a thorough analysis studying the correlation between compositionality and related variables. 2. We extend the compositionality prediction framework described in Cordeiro, Ramisch, and Villavicencio (2016) by adding and evaluating new composition functions and DSMs. 3. We extend the evaluation reported in Cordeiro et al. (2016) not only by adding Portuguese, but also by evaluating additional parameters: corpus size, composition functions, and new DSMs. 3 Computational Linguistics"
J19-1001,I11-1024,0,0.139794,"Missing"
J19-1001,W09-2907,0,0.0942899,"Missing"
J19-1001,D15-1290,0,0.0208105,"Missing"
J19-1001,W14-0818,0,0.0654706,"Missing"
J19-1001,W13-1005,0,0.257755,"Missing"
J19-1001,E14-1050,0,0.103733,"Missing"
J19-1001,N15-1099,0,0.258868,"requent in languages (Sag et al. 2002), identifying idiomaticity and producing accurate semantic representations for compositional and idiomatic cases is of relevance to NLP tasks and applications that involve some form of semantic processing, including semantic parsing (Hwang et al. 2010; Jagfeld and van der Plas 2015), word sense disambiguation (Finlayson and Kulkarni 2011; Schneider et al. 2016), 1 Attributed to Frege (1892/1960). 2 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds and machine translation (Ren et al. 2009; Carpuat and Diab 2010; Cap et al. 2015; Salehi et al. 2015). Moreover, the evaluation of DSMs on tasks involving MWEs, such as compositionality prediction, has the potential to drive their development towards new directions. The main hypothesis of our work is that, if the meaning of a compositional nominal compound can be derived from a combination of its parts, this translates in DSMs as similar vectors for a compositional nominal compound and for the combination of the vectors of its parts using some vector operation, that we refer to as composition function. Conversely we can use the lack of similarity between the nominal compound vector representa"
J19-1001,W15-0909,0,0.117951,"requent in languages (Sag et al. 2002), identifying idiomaticity and producing accurate semantic representations for compositional and idiomatic cases is of relevance to NLP tasks and applications that involve some form of semantic processing, including semantic parsing (Hwang et al. 2010; Jagfeld and van der Plas 2015), word sense disambiguation (Finlayson and Kulkarni 2011; Schneider et al. 2016), 1 Attributed to Frege (1892/1960). 2 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds and machine translation (Ren et al. 2009; Carpuat and Diab 2010; Cap et al. 2015; Salehi et al. 2015). Moreover, the evaluation of DSMs on tasks involving MWEs, such as compositionality prediction, has the potential to drive their development towards new directions. The main hypothesis of our work is that, if the meaning of a compositional nominal compound can be derived from a combination of its parts, this translates in DSMs as similar vectors for a compositional nominal compound and for the combination of the vectors of its parts using some vector operation, that we refer to as composition function. Conversely we can use the lack of similarity between the nominal compound vector representa"
J19-1001,P16-2068,1,0.899605,"Missing"
J19-1001,S16-1084,0,0.507599,"introduction here, underlining their most relevant characteristics to our framework (Section 2.1). Then, we define compositionality prediction and discuss existing approaches, focusing on distributional techniques for multiword expressions (Section 2.2). Our framework is evaluated on nominal compounds, and we discuss their relevant properties (Section 2.3) along with existing data sets for evaluating compositionality prediction (Section 2.4). 2 This article significantly extends and updates previous publications: 1. We consolidate the description of the data sets introduced in Ramisch et al. (2016) and Ramisch, Cordeiro, and Villavicencio (2016) by adding details about data collection, filtering, and results of a thorough analysis studying the correlation between compositionality and related variables. 2. We extend the compositionality prediction framework described in Cordeiro, Ramisch, and Villavicencio (2016) by adding and evaluating new composition functions and DSMs. 3. We extend the evaluation reported in Cordeiro et al. (2016) not only by adding Portuguese, but also by evaluating additional parameters: corpus size, composition functions, and new DSMs. 3 Computational Linguistics"
J19-1001,W01-0513,0,0.0987547,"Missing"
J19-1001,L16-1362,0,0.115467,"ether the phrase is (1) an instance of the concept denoted by the head (e.g., dead parrot and parrot) and (2) a member of the more general concept that includes the head (e.g., dead parrot and pet), along with typicality ratings, with 5,849 judgments in total. • Roller, Schulte im Walde, and Scheible (2013) collected judgments for a set of 244 German noun–noun compounds, each compound with an average of around 30 judgments on a compositionality scale from 1 to 7, obtained through crowdsourcing. The resource was later enriched with feature norms (Roller and Schulte im Walde 2014). • Schulte im Walde et al. (2016) collected judgments for a set of 868 German noun–noun compounds, including human judgments of compositionality on a scale of 1 to 6. Compounds are judged by multiple annotators, and the final compositionality score is the average across annotators. The data set is also annotated for in-corpus frequency, productivity, and ambiguity, and a subset of 180 compounds has been selected for balancing these variables. The annotations were performed by the authors, linguists, and through crowdsourcing. For the balanced subset of 180 compounds, compositionality annotations were performed by experts only"
J19-1001,S13-1038,0,0.0425081,"Missing"
J19-1001,D12-1110,0,0.265383,"Missing"
J19-1001,J13-4009,0,0.0755415,"Missing"
J19-1001,C12-1165,0,0.0732893,"Missing"
J19-1001,D15-1201,0,0.116679,"Missing"
L16-1194,P13-4006,0,0.0285428,"; Ramisch, 2015). Meaning composition for MWEs requires accurate meaning representation of single words. To date, many models have been proposed for representing the lexical semantics of single words. We focus on distributional models, based on Harris’ distributional hypothesis. DSMs have 1221 1 http://mwetoolkit.sf.net been around for a while (Landauer and Dumais, 1997). However, the recent enthusiasm about neural networks and word embeddings has made DSMs more accurate and faster to build using very large corpora. Many tools are nowadays available for building word embeddings, like Dissect (Dinu et al., 2013), minimantics2 , word2vec3 (Mikolov et al., 2013) and Glove4 (Pennington et al., 2014). Modeling semantic compositionality in DSMs is a hot topic in NLP. As word meanings can be represented as vectors, composition can be effectively modeled through simple operations like vector addition and multiplication (Mitchell and Lapata, 2010). Some authors have proposed models for estimating the degree of semantic idiomaticity of MWEs, focusing on noun compounds. Reddy et al. (2011) suggest a compositionality measure which is the cosine similarity between the MWE vector and the sum of the vectors of the"
L16-1194,W15-0904,0,0.493606,"the mwetoolkit that estimates semantic compositionality scores for multiword expressions (MWEs) based on word embeddings. First, we describe our implementation of vector-space operations working on distributional vectors. The compositionality score is based on the cosine distance between the MWE vector and the composition of the vectors of its member words. Our generic system can handle several types of word embeddings and MWE lists, and may combine individual word representations using several composition techniques. We evaluate our implementation on a dataset of 1042 English noun compounds (Farahmand et al., 2015), comparing different configurations of the underlying word embeddings and word-composition models. We show that our vector-based scores model non-compositionality better than standard association measures such as log-likelihood. Keywords: Lexical semantics, multiword expressions, compositionality, word embeddings. 1. Introduction Multiword expressions (MWEs) are often defined as word combinations “whose exact and unambiguous meaning or connotation cannot be derived directly from the meaning or connotation of its components” (Choueka, 1988). A broader definition of MWEs considers that, beyond"
L16-1194,W13-1017,0,0.0271844,"on measures used in the literature to compare the model predictions with human judgments. • Spearman’s Rho (ρ): measures correlation between the ranks provided by the model predictions and by human judgments. • Normalized Discounted Cumulative Gain (NDCG): a precision measure that penalizes more intensely wrong predictions at the top of the ranking. • Best F-score (F1 ): the highest F-score considering the first k predictions, for all values of k. • Precision at k (P@k): precision for the top k predictions. • Average precision (AP): average of precision calculated at each relevant prediction (Gurrutxaga and Alegria, 2013). Our dataset contains four binary judgments per compound. For ρ, we use the sum of the binary judgments to rank the compounds. For NDCG, F1 , P@k and AP, a compound is considered relevant (i.e. conventional or noncompositional) if at least two judges consider it relevant (Yazdani et al., 2015). 5. Results Table 1 presents the results when evaluating the predictive ability of different models concerning conventionality. Except for the baseline, all of the other models use a 5 Since our model predicts compositionality, and the human judges annotated non-compositionality, we inverted the model p"
L16-1194,Q15-1016,0,0.077608,"tional models, as well as different word combination weights. We train an instance of each of these distributional semantic models: minimantics, word2vec (cbow) and GloVe. For training, we feed an MWE-annotated corpus where MWEs are joined as a single token as in bounty_hunter), as performed by Ferret (2014). We fix the following parameters: • Corpus: UKWaC, containing 2G words of English texts crawled from the web (Baroni et al., 2009); • Context window: lemma of each content word 8 words to the left/right of the target;   • Context weight decay: linear, that is, 88 , 78 , 68 , . . . , 18 (Levy et al., 2015); • Dimensions per embedding: 250. We train an additional model, minimanticsB , with a window of size 1 and dimension of 500, to verify the impact of the parameters. We compare our model for compositionality prediction with a simple baseline that uses the loglikelihood (LL) association score. LL compares an MWE frequency with the frequency of each component word. We implemented several evaluation measures used in the literature to compare the model predictions with human judgments. • Spearman’s Rho (ρ): measures correlation between the ranks provided by the model predictions and by human judgm"
L16-1194,D14-1162,0,0.102269,"ntation of single words. To date, many models have been proposed for representing the lexical semantics of single words. We focus on distributional models, based on Harris’ distributional hypothesis. DSMs have 1221 1 http://mwetoolkit.sf.net been around for a while (Landauer and Dumais, 1997). However, the recent enthusiasm about neural networks and word embeddings has made DSMs more accurate and faster to build using very large corpora. Many tools are nowadays available for building word embeddings, like Dissect (Dinu et al., 2013), minimantics2 , word2vec3 (Mikolov et al., 2013) and Glove4 (Pennington et al., 2014). Modeling semantic compositionality in DSMs is a hot topic in NLP. As word meanings can be represented as vectors, composition can be effectively modeled through simple operations like vector addition and multiplication (Mitchell and Lapata, 2010). Some authors have proposed models for estimating the degree of semantic idiomaticity of MWEs, focusing on noun compounds. Reddy et al. (2011) suggest a compositionality measure which is the cosine similarity between the MWE vector and the sum of the vectors of the component words. This model was also used by Salehi et al. (2015), in combination wit"
L16-1194,I11-1024,0,0.0490926,"and faster to build using very large corpora. Many tools are nowadays available for building word embeddings, like Dissect (Dinu et al., 2013), minimantics2 , word2vec3 (Mikolov et al., 2013) and Glove4 (Pennington et al., 2014). Modeling semantic compositionality in DSMs is a hot topic in NLP. As word meanings can be represented as vectors, composition can be effectively modeled through simple operations like vector addition and multiplication (Mitchell and Lapata, 2010). Some authors have proposed models for estimating the degree of semantic idiomaticity of MWEs, focusing on noun compounds. Reddy et al. (2011) suggest a compositionality measure which is the cosine similarity between the MWE vector and the sum of the vectors of the component words. This model was also used by Salehi et al. (2015), in combination with word translation information coming from parallel corpora. Yazdani et al. (2015) propose and evaluate more sophisticated composition functions, based on linear, non-linear and neural network projections. The mwetoolkit+sem framework is based on vector addition and cosine similarity. 3. The mwetoolkit+sem Framework For the semantic processing, we developed a new tool, called feat_composi"
L16-1194,N15-1099,0,0.155378,"13) and Glove4 (Pennington et al., 2014). Modeling semantic compositionality in DSMs is a hot topic in NLP. As word meanings can be represented as vectors, composition can be effectively modeled through simple operations like vector addition and multiplication (Mitchell and Lapata, 2010). Some authors have proposed models for estimating the degree of semantic idiomaticity of MWEs, focusing on noun compounds. Reddy et al. (2011) suggest a compositionality measure which is the cosine similarity between the MWE vector and the sum of the vectors of the component words. This model was also used by Salehi et al. (2015), in combination with word translation information coming from parallel corpora. Yazdani et al. (2015) propose and evaluate more sophisticated composition functions, based on linear, non-linear and neural network projections. The mwetoolkit+sem framework is based on vector addition and cosine similarity. 3. The mwetoolkit+sem Framework For the semantic processing, we developed a new tool, called feat_compositionality, described in Figure 1. It outputs a compositionality score for each MWE in a list of input candidates, based on an input word-embeddings file. Figure 1: Overview of feat_composit"
L16-1194,D15-1201,0,0.247861,"n NLP. As word meanings can be represented as vectors, composition can be effectively modeled through simple operations like vector addition and multiplication (Mitchell and Lapata, 2010). Some authors have proposed models for estimating the degree of semantic idiomaticity of MWEs, focusing on noun compounds. Reddy et al. (2011) suggest a compositionality measure which is the cosine similarity between the MWE vector and the sum of the vectors of the component words. This model was also used by Salehi et al. (2015), in combination with word translation information coming from parallel corpora. Yazdani et al. (2015) propose and evaluate more sophisticated composition functions, based on linear, non-linear and neural network projections. The mwetoolkit+sem framework is based on vector addition and cosine similarity. 3. The mwetoolkit+sem Framework For the semantic processing, we developed a new tool, called feat_compositionality, described in Figure 1. It outputs a compositionality score for each MWE in a list of input candidates, based on an input word-embeddings file. Figure 1: Overview of feat_compositionality → representing each The first step combines the vectors − w i word wi in an MWE. The appropri"
L16-1365,W07-1107,0,0.0378695,"This paper is structured as follows: in §2 we describe some related work on MWEs and child language; and in §3 we present CHILDES-MWE and describe the annotation pro1 CHILDES-MWE is freely available at html://www.inf. ufrgs.br/pln/resource/CHILDESMWE.zip. cess. The materials and methods used for this work are in §4, along with an analysis of the results obtained. We finish with conclusions and future work in §5. 2. Learning Multiword Expressions In the psycholinguistic literature there has been considerable interest in questions about how people acquire, represent and process MWEs (Bod, 2001; Dahlmann and Adolphs, 2007). Compounding in particular can be seen as a way of introducing new words into the lexicon (Gagn´e and Spalding, 2006), and as being at the interface between morphology and syntax. As a consequence, to understand and produce compounds children need to learn to combine information at different levels of linguistic description. This includes how to order the elements of a compound, where the head is, how to do pluralization in a compound, which combinations are frozen and idiomatic and what their meaning is (Berman, 2011). For the latter in particular, there is a wide range of variation, since w"
L16-1365,P06-2064,0,0.033405,"ibution and compositionality in child-directed sentences. Keywords: Multiword Expressions, Compound nouns, compositionality, Language Acquisition 1. Introduction The increasing availability of psycholinguistic, lexical and ontological resources and of more precise and robust natural language processing tools has enabled the automatic annotation of language acquisition corpora with additional sources of information. In particular, among them resources like WordNet (Miller, 1995) and specialised datasets contain information about Multiword Expressions (MWEs) such as noun compounds (police car) (Kim and Baldwin, 2006; Nakov, 2008a), phrasal verbs (break down) (McCarthy et al., 2003) and collocations (e.g. salt and pepper) (Seretan, 2011; Eryi˘git et al., 2011). These resources can be used as basis for annotating MWE occurrence in corpora such as the English portion of the Child Language Data Exchange System (CHILDES) (MacWhinney, 1995), which contains transcriptions of child language data. In this paper we introduce the resulting resource, CHILDESMWE, which contains almost 350,000 English sentences annotated with more than 70,000 distinct MWEs of various types, including compound nouns and phrasal verbs.1"
L16-1365,W11-0818,0,0.0294343,",845,264 sentences. We extended the data from the CHILDES Verb Construction Database (Villavicencio et al., 2012b) and annotated them with MWE information. The database contains morphological and syntactic information, along with psycholinguistic and distributional information including verb semantic classes, age of acquisition and familiarity. Details about CHILDESMWE are in Table 1, focusing on children of up to 7 years of age, given the relevance of this period for language acquisition, and their linguistic input (CD) and output (CP). To automatically annotate MWEs in corpora we used jMWE (Kulkarni and Finlayson, 2011), defining a detector that finds all occurrences of MWEs, as specified in a list of MWE types. We prioritize precision only looking # MWE Types 59,439 9,813 1,790 846 71,888 Table 2: MWE Types per Size 4. Compound Nouns in CD and CP sentences To investigate the relation between MWEs in the linguistic input and output of children, focusing on compound nouns (CNs), we examined the following hypotheses: H1 CNs in child-produced sentences follow the distribution found in child-directed sentences across ages. 2308 2 Listed for recurrent sequences like you know, and I mean. CD age 13-24 25-48 49-60"
L16-1365,W03-1810,0,0.0501943,": Multiword Expressions, Compound nouns, compositionality, Language Acquisition 1. Introduction The increasing availability of psycholinguistic, lexical and ontological resources and of more precise and robust natural language processing tools has enabled the automatic annotation of language acquisition corpora with additional sources of information. In particular, among them resources like WordNet (Miller, 1995) and specialised datasets contain information about Multiword Expressions (MWEs) such as noun compounds (police car) (Kim and Baldwin, 2006; Nakov, 2008a), phrasal verbs (break down) (McCarthy et al., 2003) and collocations (e.g. salt and pepper) (Seretan, 2011; Eryi˘git et al., 2011). These resources can be used as basis for annotating MWE occurrence in corpora such as the English portion of the Child Language Data Exchange System (CHILDES) (MacWhinney, 1995), which contains transcriptions of child language data. In this paper we introduce the resulting resource, CHILDESMWE, which contains almost 350,000 English sentences annotated with more than 70,000 distinct MWEs of various types, including compound nouns and phrasal verbs.1 The resulting annotated resource can be used as basis for language"
L16-1365,W11-0807,0,0.0259213,"87 distinct words from which 69,719 are verbal (3,096), nominal (62,410), adverbial (827) and adjectival (3,386) MWEs. CE Cranberry expressions dataset (Trawinski et al., 2008) containing MWEs whose components cannot be found outside the MWE (e.g. sandboy as happy as a sandboy). NC Noun Compounds datasets by Kim and Baldwin (2008) and Nakov (2008b) containing sequences of nouns (e.g. cheese knife). CN Compound Nominalizations (Nicholson and Baldwin, 2008) which are a subclass of compound nouns in which the head noun is deverbal (e.g. product replacement). LVC Light-Verb Constructions dataset (Tu and Roth, 2011) containing expressions where the verb has a light or supporting role and the meaning is mainly derived from the direct object noun like take a walk. VPC Verb-Particle Constructions dataset (Baldwin, 2008) with combinations of verbs and prepositional, adverbial or adjectival particles (e.g. break down). Cases of to <verb> (e.g. to come, and to break) and <pronoun> <verb>.2 were not included. The final list contains 71,888 MWE types characterized as in Tables 2 and 3. CHILDES-MWE contains 347,391 sentences annotated with MWEs, and details about a subset of these corpora are shown in Table 1 and"
L16-1365,W12-0911,1,0.888452,"Missing"
L16-1365,villavicencio-etal-2012-large,1,0.881223,"break down). Cases of to <verb> (e.g. to come, and to break) and <pronoun> <verb>.2 were not included. The final list contains 71,888 MWE types characterized as in Tables 2 and 3. CHILDES-MWE contains 347,391 sentences annotated with MWEs, and details about a subset of these corpora are shown in Table 1 and in Figure 1 for children of ages 1 to 7. Size 2 3 4 5 Total CHILDES-MWE The English CHILDES contains 60 subcorpora (12 from British English and 48 form North American English), with 895,130 word types in 4,845,264 sentences. We extended the data from the CHILDES Verb Construction Database (Villavicencio et al., 2012b) and annotated them with MWE information. The database contains morphological and syntactic information, along with psycholinguistic and distributional information including verb semantic classes, age of acquisition and familiarity. Details about CHILDESMWE are in Table 1, focusing on children of up to 7 years of age, given the relevance of this period for language acquisition, and their linguistic input (CD) and output (CP). To automatically annotate MWEs in corpora we used jMWE (Kulkarni and Finlayson, 2011), defining a detector that finds all occurrences of MWEs, as specified in a list of"
L16-1365,W11-3806,0,\N,Missing
L16-1422,P98-1013,0,0.404484,"epends on the semantic of the word it relates to (in this case, the verb). For Portuguese, there are only a few initiatives that were dedicated to developing resource with this type of semantic information: PropBank.Br (Duran et al., 2011; Duran and Alu´ısio, 2012), VerbNet.Br (Scarton, 2013) and FrameNet Brasil (Salom˜ao, 2009) are the most prominent, and all of them were directly derived from their English counterparts, as their names clearly point out. VerbLexPor1 is a lexical resource with semantic role annotation for Portuguese that portrays the semantic function of The FrameNet project (Baker et al., 1998) uses semantic roles that are specific to a domain or frame. For instance, in the semantic structure of the frame judgment there are four core semantic roles (COGNIZER , EVALUEE , EXPRES SOR , and REASON ), but the roles are completely different when another frame, such as perception active, is taken into account, even if the verb in question is the same. For Portuguese, FrameNet Brasil (Salom˜ao, 2009) uses this approach and has already annotated various frames for generic and domain-specific vocabulary. The PropBank project (Palmer et al., 2005) builds on an idea similar to FrameNet, but ins"
L16-1422,dias-da-silva-etal-2008-automatic,0,0.0432018,"Missing"
L16-1422,duran-aluisio-2012-propbank,0,0.0374652,"Missing"
L16-1422,W11-4519,0,0.100484,"Missing"
L16-1422,C12-1053,0,0.0156462,"re annotated separately, so that it is possible to access sentences either from the Cardiology or from the newspaper corpus. The full resource presents more than seven thousand semantically annotated sentences, containing 192 different verbs and more than 15 thousand individual arguments and adjuncts. Keywords: Semantic Role Labeling, Lexical Resource, Corpus 1. Introduction Lexical resources with semantic role information are of great use in NLP applications, for example, in anaphora resolution (Kong and Zhou, 2012), automatic summarization (Yoshikawa et al., 2012) and automatic translation (Feng et al., 2012; Jones et al., 2012). Semantic roles are labels that describe the semantic function of an argument in relation to a determined word class (usually, a verb). For instance, in Sentence A, we have a simple clause, in which the subject (Roger) carried out the action (represented by the verb), while the direct object (the vase) is being modified by the event. Hence, we have the semantic roles of AGENT and PA TIENT. On the other hand, in Sentence B, although we still have a subject (the vase), it is no longer the performer of the action, but the one who suffers a change of state (from being in one"
L16-1422,C12-1083,0,0.0135249,"tely, so that it is possible to access sentences either from the Cardiology or from the newspaper corpus. The full resource presents more than seven thousand semantically annotated sentences, containing 192 different verbs and more than 15 thousand individual arguments and adjuncts. Keywords: Semantic Role Labeling, Lexical Resource, Corpus 1. Introduction Lexical resources with semantic role information are of great use in NLP applications, for example, in anaphora resolution (Kong and Zhou, 2012), automatic summarization (Yoshikawa et al., 2012) and automatic translation (Feng et al., 2012; Jones et al., 2012). Semantic roles are labels that describe the semantic function of an argument in relation to a determined word class (usually, a verb). For instance, in Sentence A, we have a simple clause, in which the subject (Roger) carried out the action (represented by the verb), while the direct object (the vase) is being modified by the event. Hence, we have the semantic roles of AGENT and PA TIENT. On the other hand, in Sentence B, although we still have a subject (the vase), it is no longer the performer of the action, but the one who suffers a change of state (from being in one piece to being in mor"
L16-1422,C12-1090,0,0.0129216,"annotated data, and this could be useful for other researchers. The sentences from both corpora were annotated separately, so that it is possible to access sentences either from the Cardiology or from the newspaper corpus. The full resource presents more than seven thousand semantically annotated sentences, containing 192 different verbs and more than 15 thousand individual arguments and adjuncts. Keywords: Semantic Role Labeling, Lexical Resource, Corpus 1. Introduction Lexical resources with semantic role information are of great use in NLP applications, for example, in anaphora resolution (Kong and Zhou, 2012), automatic summarization (Yoshikawa et al., 2012) and automatic translation (Feng et al., 2012; Jones et al., 2012). Semantic roles are labels that describe the semantic function of an argument in relation to a determined word class (usually, a verb). For instance, in Sentence A, we have a simple clause, in which the subject (Roger) carried out the action (represented by the verb), while the direct object (the vase) is being modified by the event. Hence, we have the semantic roles of AGENT and PA TIENT. On the other hand, in Sentence B, although we still have a subject (the vase), it is no lo"
L16-1422,mangeot-chalvin-2006-dictionary,0,0.0404904,"ocabulary. The PropBank project (Palmer et al., 2005) builds on an idea similar to FrameNet, but instead of using descriptive roles (such as SOURCE , GOAL, and THEME) it uses numbered roles (such as ARG 0, ARG 1, and ARG 2). For Portuguese, the PropBank.Br project (Duran et al., 2011; Du1 VerbLexPor is readily available for download in XML and SQL formats: http://cameleon.imag.fr/xwiki/ bin/view/Main/Semantic%20role%20labels %20corpus%20-%20Brazilian%20Portuguese. It is also possible to look up for syntactic and semantic information on verbs of the resource accessing the Jibiki platform(Mangeot-Nagata, 2006): http: //jibiki.univ-savoie.fr/jibiki/Home.po. • Sentence A: [Roger] broke [the vase]. • Sentence B: [The vase] broke. 2656 ran and Alu´ısio, 2012) has annotated 5.537 instances2 with ARG 0 to ARG 5. In addition to these numbered roles, PropBank (as does FrameNet) also has roles for adjuncts (such as ARG - TMP, for adjuncts that express time). The VerbNet project (Schuler, 2005) uses descriptive roles, such as the ones in FrameNet, but instead of having different roles for each frame, it has a single set of roles that applies to all verbs and arguments. For Portuguese, the VerbNet.Br project"
L16-1422,J05-1004,0,0.158446,"the semantic function of The FrameNet project (Baker et al., 1998) uses semantic roles that are specific to a domain or frame. For instance, in the semantic structure of the frame judgment there are four core semantic roles (COGNIZER , EVALUEE , EXPRES SOR , and REASON ), but the roles are completely different when another frame, such as perception active, is taken into account, even if the verb in question is the same. For Portuguese, FrameNet Brasil (Salom˜ao, 2009) uses this approach and has already annotated various frames for generic and domain-specific vocabulary. The PropBank project (Palmer et al., 2005) builds on an idea similar to FrameNet, but instead of using descriptive roles (such as SOURCE , GOAL, and THEME) it uses numbered roles (such as ARG 0, ARG 1, and ARG 2). For Portuguese, the PropBank.Br project (Duran et al., 2011; Du1 VerbLexPor is readily available for download in XML and SQL formats: http://cameleon.imag.fr/xwiki/ bin/view/Main/Semantic%20role%20labels %20corpus%20-%20Brazilian%20Portuguese. It is also possible to look up for syntactic and semantic information on verbs of the resource accessing the Jibiki platform(Mangeot-Nagata, 2006): http: //jibiki.univ-savoie.fr/"
L16-1422,P12-2068,0,0.0171429,"her researchers. The sentences from both corpora were annotated separately, so that it is possible to access sentences either from the Cardiology or from the newspaper corpus. The full resource presents more than seven thousand semantically annotated sentences, containing 192 different verbs and more than 15 thousand individual arguments and adjuncts. Keywords: Semantic Role Labeling, Lexical Resource, Corpus 1. Introduction Lexical resources with semantic role information are of great use in NLP applications, for example, in anaphora resolution (Kong and Zhou, 2012), automatic summarization (Yoshikawa et al., 2012) and automatic translation (Feng et al., 2012; Jones et al., 2012). Semantic roles are labels that describe the semantic function of an argument in relation to a determined word class (usually, a verb). For instance, in Sentence A, we have a simple clause, in which the subject (Roger) carried out the action (represented by the verb), while the direct object (the vase) is being modified by the event. Hence, we have the semantic roles of AGENT and PA TIENT. On the other hand, in Sentence B, although we still have a subject (the vase), it is no longer the performer of the action, but the one who"
L16-1422,C98-1013,0,\N,Missing
L16-1580,C12-3044,0,0.0490122,"Missing"
L16-1580,dias-da-silva-etal-2008-automatic,0,0.0808221,"Missing"
L16-1580,P14-5004,0,0.0131509,"These selected words were randomly divided in groups of 3 words, and we then selected the group with closest mean frequency and mean number of senses in regard to the target word. For English, there are several datasets for the evaluation of distributional thesauri, such as: • 65 noun pairs (Rubenstein and Goodenough, 1965) • 80 test items in TOEFL (Landauer and Dumais, 1997) • 353 noun pairs (WordSim (Finkelstein et al., 2001)) • 2003 pairs and context sentences (SCWS (Huang et al., 2012)) • 3,000 pairs (Bruni et al., 2014) Many of these are available as part of Word Vector Evaluation suite (Faruqui and Dyer, 2014).9 The TOEFL dataset in particular presents, for each target word, four alternatives, and the task is to select which among them is the more semantically related to the target word than the others. Other examples of TOEFL-like tasks include the WordNet-Based Synonymy Test (WBST) (Freitag et al., 2005), which is an extension of the TOEFL test that was automatically generated from WordNet. The dataset presented here adapts the methodology of the latter to BabelNet, selecting 4 alternatives for each target word to create test items automatically for resource limited languages like Portuguese. 3."
L16-1580,W05-0604,0,0.225649,"resources with information about the similarity between words. Moreover, due to the large scale of the resulting thesauri a manual evaluation by human judges is prohibitively expensive and would consume too much time. An alternative is the extrinsic evaluation of the quality of a thesaurus, where performance on a particular task would indirectly reflect the quality of a resource. For instance, we can approximate the concept of similarity by presenting an explicit semantic relation between words, as done in the TOEFL test (Landauer and Dumais, 1997) and the WordNet-Based Synonymy Test (WBST) (Freitag et al., 2005) for English. For Portuguese, automatically generated ontologies include BabelNet (Navigli and Ponzetto, 2010) and Onto.PT (Gonc¸alo Oliveira and Gomes, 2010), but there are no specific gold standards for the evaluation of distributional thesauri. Having this in mind, we developed the BabeNet-Based Semantic Gold Standard (B2 SG) for Portuguese, based on the WBST.8 The main difference between them is that B2 SG is based on an automatically generated resource, while WBST is based on a manually constructed one, WordNet. In this paper we discuss related work in Section 2, and the methodology used"
L16-1580,W10-2302,0,0.187404,"tter. Keywords: distributional thesauri, gold standard, lexical resource 1. Introduction The importance of resources such as WordNet (Fellbaum, 1998), that represent semantic relations between words, can be measured by the number of initiatives dedicated to (re)producing them in other languages, such as the EuroWordNet1 (Vossen, 1998) and the Global WordNet Association2 (Bond and Paik, 2012). These resources have been windely used in numerous NLP applications, such as systems for Q&A, text simplification, and sentiment analysis. For Portuguese, some such initiatives include Onto.PT3 (Gonc¸alo Oliveira and Gomes, 2010), OpenWN-PT4 (de Paiva et al., 2012), MultiWordnet of Portuguese5 , WordNet.PT6 (Marrafa, 2002), WordNet.Br7 (Dias-da-Silva et al., 2008). However, the manual construction of this type of resource is costly and much time-consuming, in addition to having low coverage and being applicable to only one domain. Moreover, its availability is limited or non-existent for many languages. A popular alternative is the automatic construction of distributional thesauri from corpora, resulting in a resource with semantic association among words. These techniques are language independent and applicable to an"
L16-1580,P12-1092,0,0.0121396,"st produced in Step 2, but, for each target word, only words without an explicit relation to it were selected as candidates. These selected words were randomly divided in groups of 3 words, and we then selected the group with closest mean frequency and mean number of senses in regard to the target word. For English, there are several datasets for the evaluation of distributional thesauri, such as: • 65 noun pairs (Rubenstein and Goodenough, 1965) • 80 test items in TOEFL (Landauer and Dumais, 1997) • 353 noun pairs (WordSim (Finkelstein et al., 2001)) • 2003 pairs and context sentences (SCWS (Huang et al., 2012)) • 3,000 pairs (Bruni et al., 2014) Many of these are available as part of Word Vector Evaluation suite (Faruqui and Dyer, 2014).9 The TOEFL dataset in particular presents, for each target word, four alternatives, and the task is to select which among them is the more semantically related to the target word than the others. Other examples of TOEFL-like tasks include the WordNet-Based Synonymy Test (WBST) (Freitag et al., 2005), which is an extension of the TOEFL test that was automatically generated from WordNet. The dataset presented here adapts the methodology of the latter to BabelNet, sel"
L16-1580,P98-2127,0,0.33965,"T4 (de Paiva et al., 2012), MultiWordnet of Portuguese5 , WordNet.PT6 (Marrafa, 2002), WordNet.Br7 (Dias-da-Silva et al., 2008). However, the manual construction of this type of resource is costly and much time-consuming, in addition to having low coverage and being applicable to only one domain. Moreover, its availability is limited or non-existent for many languages. A popular alternative is the automatic construction of distributional thesauri from corpora, resulting in a resource with semantic association among words. These techniques are language independent and applicable to any domain (Lin, 1998). As a consequence there is much attention being devoted to the systematic construction, evaluation and enhancement of distributional thesauri. The automatic evaluation of such thesauri, in particular, is a complex task, because of the lack of resources with information about the similarity between words. Moreover, due to the large scale of the resulting thesauri a manual evaluation by human judges is prohibitively expensive and would consume too much time. An alternative is the extrinsic evaluation of the quality of a thesaurus, where performance on a particular task would indirectly reflect"
L16-1580,P10-1023,0,0.279409,"resulting thesauri a manual evaluation by human judges is prohibitively expensive and would consume too much time. An alternative is the extrinsic evaluation of the quality of a thesaurus, where performance on a particular task would indirectly reflect the quality of a resource. For instance, we can approximate the concept of similarity by presenting an explicit semantic relation between words, as done in the TOEFL test (Landauer and Dumais, 1997) and the WordNet-Based Synonymy Test (WBST) (Freitag et al., 2005) for English. For Portuguese, automatically generated ontologies include BabelNet (Navigli and Ponzetto, 2010) and Onto.PT (Gonc¸alo Oliveira and Gomes, 2010), but there are no specific gold standards for the evaluation of distributional thesauri. Having this in mind, we developed the BabeNet-Based Semantic Gold Standard (B2 SG) for Portuguese, based on the WBST.8 The main difference between them is that B2 SG is based on an automatically generated resource, while WBST is based on a manually constructed one, WordNet. In this paper we discuss related work in Section 2, and the methodology used for developing B2 SG in Section 3. We describe an intrinsic evaluation of the quality of the test items propos"
L18-1686,W07-1208,0,0.00950467,"ently, corpora for 31 different languages are available, including a 4 billion tokens corpus for Portuguese (ptTenTen). However, this corpus is not openly available, a key aspect of our research motivation. Moreover, it includes content from both the European and Brazilian variants of Portuguese, not being directly comparable to our proposal. Focusing on characteristics of the written language, while there is a large core that is shared among different Portuguese variants, there are also lexical and syntactic characteristics of Brazilian Portuguese that are marked in comparison to the others (Branco and Costa, 2007). For instance, comparing these two variants, there is a strong difference in the preference for the use of clitics. Subsequent analyses that use these corpora or models constructed from them may result in an amalgamation of different variants that does not reflect accurately the characteristics of any particular variant. This may create problems for downstream applications, like text simplification, for which the simplicity and naturalness of a text are linked to the language usage for that particular community of speakers. With this work we aim to produce a large corpus that targets Brazilia"
L18-1686,J13-3008,0,0.0489767,"Missing"
L18-1686,Q15-1016,0,0.0304373,"revious work (Wagner Filho et al., 2016) and aim to construct a large and freely available Web corpus for Brazilian Portuguese, compatible with the state of the art in other languages. An example of the need for new, large corpora in this language was shown by Rodrigues et al. (2016), who had to combine 19 different corpora to obtain a 1.7 billion tokens corpus and create a distributional semantics model comparable to those available for English. Besides language models, the corpus presented here can also be used, for example, in dictionary creation (Kilgarriff et al., 2008), word similarity (Levy et al., 2015) and word sense induction (Navigli and Velardi, 2010; Di Marco and Navigli, 2013). A widely adopted approach for the construction of large corpora is the WaCky (Web-As-Corpus Kool Yinitiative) methodology (Bernardini et al., 2006; Baroni et al., 2009), which has been used to provide corpora in the scale of billions of tokens for multiple languages, extracting text content from the Web. It also enables the construction of corpora without domain biases, considering that a corpus will automatically get balanced after reaching a substantial size (Xiao, 2010). When constructing these corpora a real"
L18-1686,P10-1134,0,0.0276637,"aim to construct a large and freely available Web corpus for Brazilian Portuguese, compatible with the state of the art in other languages. An example of the need for new, large corpora in this language was shown by Rodrigues et al. (2016), who had to combine 19 different corpora to obtain a 1.7 billion tokens corpus and create a distributional semantics model comparable to those available for English. Besides language models, the corpus presented here can also be used, for example, in dictionary creation (Kilgarriff et al., 2008), word similarity (Levy et al., 2015) and word sense induction (Navigli and Velardi, 2010; Di Marco and Navigli, 2013). A widely adopted approach for the construction of large corpora is the WaCky (Web-As-Corpus Kool Yinitiative) methodology (Bernardini et al., 2006; Baroni et al., 2009), which has been used to provide corpora in the scale of billions of tokens for multiple languages, extracting text content from the Web. It also enables the construction of corpora without domain biases, considering that a corpus will automatically get balanced after reaching a substantial size (Xiao, 2010). When constructing these corpora a real concern is to gather as representative and diverse"
L18-1686,D14-1162,0,0.0855033,"Missing"
L18-1686,E06-1030,0,0.0521653,"compass 2.7 billion tokens. This result is discussed in terms of corpus size, domain diversity and content originality (Section 4). Final remarks are presented in Section 5. The resulting language resource, named brWaC, is freely available for research purposes, both for querying and downloading1 . 2. Related Work With growing content availability in the Web, it became natural for researchers to look at it as a source to complement their traditional text repositories. Among notable examples to adopt this idea are the Terabyte corpus (Clarke et al., 2002) (53bi tokens) and the Web Text corpus (Liu and Curran, 2006) (10bi tokens). Nonetheless, these early re4339 1 www.inf.ufrgs.br/pln/brwac sources often included large amounts of material that is of limited relevance (such as computer code) and duplicated materials. Moreover, content quality was not controlled. Recently, therefore, the focus in Web corpora construction shifted, from downloading large volumes of text to attaining corpus quality trough efficient post processing. In this context, the WaCky (Web-As-Corpus Kool Yinitiative) methodology was proposed (Bernardini et al., 2006; Baroni et al., 2009). It includes four steps: (1) identification of s"
L18-1686,W16-4119,1,0.895628,"wnloading, in the expectation of aiding in new advances for the processing of Brazilian Portuguese. Keywords: Web as Corpus, large corpus, Brazilian Portuguese 1. Introduction In recent years, initiatives for the construction of large corpora have attained ever-growing interest in the NLP community. They are especially relevant for applications which demand large volumes of data, such as neural methods (Pennington et al., 2014; Bahdanau et al., 2014; Johnson et al., 2016), and also tend to produce more reliable resources for statistical models. In this paper, we build on previous work (Wagner Filho et al., 2016) and aim to construct a large and freely available Web corpus for Brazilian Portuguese, compatible with the state of the art in other languages. An example of the need for new, large corpora in this language was shown by Rodrigues et al. (2016), who had to combine 19 different corpora to obtain a 1.7 billion tokens corpus and create a distributional semantics model comparable to those available for English. Besides language models, the corpus presented here can also be used, for example, in dictionary creation (Kilgarriff et al., 2008), word similarity (Levy et al., 2015) and word sense induct"
laranjeira-etal-2014-comparing,P02-1040,0,\N,Missing
laranjeira-etal-2014-comparing,P07-2045,0,\N,Missing
laranjeira-etal-2014-comparing,J10-4006,0,\N,Missing
laranjeira-etal-2014-comparing,W12-3904,1,\N,Missing
laranjeira-etal-2014-comparing,D13-1147,0,\N,Missing
N18-2037,J90-1003,0,0.269966,"on for Computational Linguistics 10 times 10-fold-cross validation.3 To determine the effectiveness of different types of similarity measures for switch identification we examine semantic similarity from a manually constructed resource, as well as two measures derived from corpora: word association strength, and semantic relatedness. Semantic similarity is determined from the shortest path that connects two words according to the WordNet (Fellbaum, 1998; Perkins, 2010) hypernym taxonomy. The association strength is calculated using the positive value of the Pointwise Mutual Information (PMI) (Church and Hanks, 1990), and the semantic relatedness using the cosine similarity between two GloVe word embeddings (Pennington et al., 2014).‘ WordNet provides a high quality manual resource but is not available for all languages. In this work we translated the SVF responses from Brazilian Portuguese to English.4 Similarity using association strength and semantic relatedness can be constructed from raw corpora, which makes them an attractive alternative for low-resourced languages like Portuguese. In this work we used a corpus built from the Portuguese Wikipedia5 , which was lemmatized and had high frequency functi"
N18-2037,W17-6926,0,0.0392705,"Missing"
N18-2037,W15-1215,0,0.0707257,"Missing"
N18-2037,J91-1002,0,0.0940211,"ficant differences between members of the same group. Additionally, we also considered a fifth group, the Cognitively Impaired (CI) group, that includes randomly selected participants from the three clinical groups. The responses of each participant are annotated following the guidelines adopted by Troyer et al. (1998); Bertola et al. (2014b). 3.2 Switch identification In this paper we explore different types of similarity for detecting switches in SVF. An SVF can be divided in semantic chains, which we define as sequences of consecutive words whose similarity falls above a certain threshold (Morris and Hirst, 1991; Pakhomov and Hemmy, 2014). Different semantic chains are separated by switches2 . Switches form the basis for training classifiers to distinguish control from clinical cases in the SVF dataset (Bertola et al., 2014a). We use Random Forest classifiers (Breiman, 2001) trained with the following features: the number of switches, n; the largest chain size, cmax = max(ca ); the average chain n+1 P 1 ca ; the fraction of occurrence of length, c¯ = n+1 where H(x) = 1 if x ≥ 0 and H(x) = 0 otherwise. Detection based in the local mean. The threshold is given by the average similarity of the last k pa"
P13-1130,D10-1119,0,0.0261647,"Brown and Hanlon, 1970; Ingram, 1989). 3 One alternative solution to Baker’s paradox that has been widely discussed at least since Chomsky (1981) is the use of indirect negative evidence. On the indirect negative evidence model, if a verb is not found where it would be expected to occur, the learner may conclude it is not part of the adult grammar. Crucially, the indirect evidence model is inherently statistical. Different formalizations of indirect negative evidence have been incorporated in several computational learning models for learning e.g. grammars (Briscoe, 1997; Villavicencio, 2002; Kwiatkowski et al., 2010); dative verbs (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010); and multiword verbs (Nematzadeh, Fazly, and Stevenson, 2013). Since a number of closely related 3.1 Materials and Methods Dative Corpora To emulate a child language acquisition environment we use naturalistic longitudinal child-directed data, from the Brown corpus in CHILDES, for one child (Adam) for a subset of 19 verbs in the DOD and PD verb frames, figure 1. This dataset was originally reported in Perfors, Tenenbaum, and Wonnacott (2010), and longitudinal and incremental aspects to acquisition are approximated b"
P13-1130,W08-2112,0,0.0273025,"keeping it simple Aline Villavicencio♣ , Marco Idiart♥ Robert Berwick♦ , Igor Malioutov♠ ♣ Institute of Informatics, Federal University of Rio Grande do Sul (Brazil) ♥ Institute of Physics, Federal University of Rio Grande do Sul (Brazil) ♦ LIDS, Dept. of EECS, Massachusetts Institute of Technology (USA) ♠ CSAIL, Dept. of EECS, Massachusetts Institute of Technology (USA) avillavicencio@inf.ufrgs.br, marco.idiart@if.ufrgs.br berwick@csail.mit.edu, igorm@mit.edu Abstract bination of structured information and incomplete knowledge, (Perfors, Tenenbaum, and Wonnacott, 2010; Hsu and Chater, 2010; Parisien, Fazly, and Stevenson, 2008; Parisien and Stevenson, 2010) as they offer several advantages in this domain. They can readily handle the evident noise and ambiguity of acquisition input, while at the same time providing efficiency via priors that mirror known pre-existing language biases. Further, hierarchical Bayesian Models (HBMs) can combine distinct abstraction levels of linguistic knowledge, from variation at the level of individual lexical items, to cross-item variation, using hyper-parameters to capture observed patterns of both under- and over-generalization as in the acquisition of e.g. dative alternations in En"
P13-1130,D09-1067,0,0.0789919,"Missing"
P13-1130,P97-1054,0,\N,Missing
P16-1187,W03-1812,0,0.0436116,"ment adopted by Farahmand et al. (2015), for a dataset of English nominal compounds. There has been much interest in creating semantic representations of larger units, such as phrases (Mikolov et al., 2013b), sentences and 1 In French, one can also use a preposition and optional determiner, like cancer du poumon (lung cancer). 2 It refers to an initiative that provides money to many people without much effort. documents (Le and Mikolov, 2014), and in examining whether it is possible to accurately derive the semantics of a compound or multiword expression from its parts (McCarthy et al., 2003; Baldwin et al., 2003; Tratz and Hovy, 2010; Reddy et al., 2011). For the latter, proposals include using additive and multiplicative functions to combine vector representations of component words (Mitchell and Lapata, 2008; Reddy et al., 2011), calculating the overlap between the components and the expression (McCarthy et al., 2003) and looking at the literality of translations into multiple languages (Salehi et al., 2014). Other proposals to explicitly represent the semantics of nominal compounds include the use of paraphrases (Lauer, 1995; Nakov, 2008; Hendrickx et al., 2013), and inventories of semantic relati"
P16-1187,J10-4006,0,0.0261791,"such as level of corpus preprocessing, context window size and number of dimensions. The results obtained have a high correlation with human judgments, being comparable to or outperforming the state of the art for some datasets (Spearman’s ρ=.82 for the Reddy dataset). 1 Introduction Distributional semantic models (DSMs) use context information to represent the meaning of lexical units as vectors. They normally focus on the accurate semantic representation of single words. It is based on single words that many optimizations for these models have been proposed (Lin, 1999; Erk and Pad´o, 2010; Baroni and Lenci, 2010). This is particularly true for word embeddings, that is, a type of DSM where distributional vectors are obtained as a by-product of training a neural network to learn a function between words and their contexts (Mikolov et al., 2013a). Simultaneously, there has been intensive research on models to compose individual word vectors in order to create representations for larger units such as phrases, sentences and even whole documents (Mitchell and Lapata, 2010; Mikolov et al., 2013a). Larger units can often be assumed to have their meanings derived from their parts according to the language’s gr"
P16-1187,P14-1023,0,0.0505953,"have been studied include the choice of association and similarity measures (Curran and Moens, 2002) and the use of subsampling and negative sampling techniques (Mikolov et al., 2013c). However, the particular effects may be heterogeneous and depend on the task and model (Lapesa and Evert, 2014). In this paper, we examine the impact of both corpus and context parameters for a variety of models, for the task of nominal compound compositionality prediction in English and French. For the choice of particular DSM, contradictory results have been published showing the superiority of neural models (Baroni et al., 2014) and of more traditional but carefully designed models (Levy et al., 2015). The former were also reported as a better fit to behavioral data on semantic prim1987 ing tasks (Mandera et al., 2016). Moreover, these evaluations are often performed on single-word similarity tasks (Freitag et al., 2005; CamachoCollados et al., 2015) and little has been said about the use of word embeddings for the compositionality prediction of multiword expressions. Two notable exceptions are the recent works of Salehi et al. (2015) and Yazdani et al. (2015). Salehi et al. (2015) show that word embeddings are more"
P16-1187,P15-2001,0,0.0650836,"Missing"
P16-1187,S13-2025,0,0.164865,"Missing"
P16-1187,P02-1030,0,0.182521,"Factors related to context representation include the context window size and the number of context dimensions adopted for a model (Lapesa and Evert, 2014); the choice of contexts to be used with targets (syntactic dependencies vs. bag-of-words) (Agirre et al., 2009); the use of morphosyntactic information (Pad´o and Lapata, 2003; Pad´o and Lapata, 2007); context filtering (Riedl and Biemann, 2012; Padr´o et al., 2014a); and dimensionality reduction methods (van de Cruys et al., 2012). Important model parameters that have been studied include the choice of association and similarity measures (Curran and Moens, 2002) and the use of subsampling and negative sampling techniques (Mikolov et al., 2013c). However, the particular effects may be heterogeneous and depend on the task and model (Lapesa and Evert, 2014). In this paper, we examine the impact of both corpus and context parameters for a variety of models, for the task of nominal compound compositionality prediction in English and French. For the choice of particular DSM, contradictory results have been published showing the superiority of neural models (Baroni et al., 2014) and of more traditional but carefully designed models (Levy et al., 2015). The"
P16-1187,P06-2064,0,0.0247328,"achine translation (to translate non-compositional compounds as a unit), word sense disambiguation (to avoid assigning a sense to parts of non-compositional compounds), and semantic parsing (to identify complex predicates and their arguments). Even when larger units are explicitly represented in DSMs (McCarthy et al., 2003; Reddy et al., 2011; Mikolov et al., 2013c; Ferret, 2014), it is not clear whether the quality of these representations is comparable to the representations of single words. In particular, when building vectors for larger units, their generally lower frequencies in corpora (Kim and Baldwin, 2006) may combine with morphosyntactic phenomena to increase sparsity even further, often requiring non-trivial preprocessing (lemmatization and word reordering) to conflate variants. This paper presents a large-scale multilingual evaluation of DSMs and their parameters for the task of compositionality prediction of nominal compounds in French and English. We examine parameters like the level of corpus preprocessing, the size of the context window and the number of dimensions for context representation. Additionally, we compare standard DSMs based on positive pointwise mutual information (PPMI) 198"
P16-1187,P13-4006,0,0.0113018,"ant contexts (highest PPMI) for each target. No further dimensionality reduction is applied. In PPMI-TopK, we use a fixed global list of 1000 contexts, built by looking at the most frequent words in the corpus: the top 50 are skipped, and the next 1000 are taken (Salehi et al., 2015). No further dimensionality reduction is applied. 5 Syntactic context definition is planned as future work. 6 PPMI vectors are built using minimantics https:// github.com/ceramisch/minimantics. In PPMI-SVD, for each target, contexts that appear less than 1000 times are discarded.7 We then use the Dissect toolkit8 (Dinu et al., 2013) in order to build a PPMI matrix and reduce its dimensionality using singular value decomposition (SVD) to factorize the matrix. w2v Uses the word2vec toolkit based on neural networks to predict target/context cooccurrence (Mikolov et al., 2013a). We build models from two variants of word2vec: CBOW (w2v-cbow) and skipgram (w2v-sg). In both cases, the configurations are the default ones, except for the following: no hierarchical softmax; negative sampling of 25; frequent-word downsampling weight of 10−6 ; runs 15 training iterations. We use the default minimum word count threshold of 5. glove W"
P16-1187,Q14-1041,0,0.0402451,"ality of translations into multiple languages (Salehi et al., 2014). Other proposals to explicitly represent the semantics of nominal compounds include the use of paraphrases (Lauer, 1995; Nakov, 2008; Hendrickx et al., 2013), and inventories of semantic relations (Girju et al., 2005). The ability of DSMs for accurately capturing semantic information may be affected by a number of factors involved in constructing the models, such as the source corpus, context representation, and parameters of the model. Relevant corpus parameters include size (Ferret, 2013; Mikolov et al., 2013c) and quality (Lapesa and Evert, 2014). Factors related to context representation include the context window size and the number of context dimensions adopted for a model (Lapesa and Evert, 2014); the choice of contexts to be used with targets (syntactic dependencies vs. bag-of-words) (Agirre et al., 2009); the use of morphosyntactic information (Pad´o and Lapata, 2003; Pad´o and Lapata, 2007); context filtering (Riedl and Biemann, 2012; Padr´o et al., 2014a); and dimensionality reduction methods (van de Cruys et al., 2012). Important model parameters that have been studied include the choice of association and similarity measures"
P16-1187,P10-2017,0,0.0168323,"Missing"
P16-1187,W15-0904,0,0.141369,"in cash cow versus tears in crocodile tears. Indeed, various annotation scales have been proposed as means to collect human judgments about compositionality. Particularly for nominal compounds, Reddy et al. (2011) used a 6-point scale to collect judgments on the literal or figurative use of nominal compounds and its components in English. Similar judgments have also been collected for 244 German compounds, for which an average of 30 judgments on a scale from 1 to 7 were gathered through crowdsourcing (Roller et al., 2013). An alternative to multi-point scales is the binary judgment adopted by Farahmand et al. (2015), for a dataset of English nominal compounds. There has been much interest in creating semantic representations of larger units, such as phrases (Mikolov et al., 2013b), sentences and 1 In French, one can also use a preposition and optional determiner, like cancer du poumon (lung cancer). 2 It refers to an initiative that provides money to many people without much effort. documents (Le and Mikolov, 2014), and in examining whether it is possible to accurately derive the semantics of a compound or multiword expression from its parts (McCarthy et al., 2003; Baldwin et al., 2003; Tratz and Hovy, 2"
P16-1187,P13-1055,0,0.0503805,"n (McCarthy et al., 2003) and looking at the literality of translations into multiple languages (Salehi et al., 2014). Other proposals to explicitly represent the semantics of nominal compounds include the use of paraphrases (Lauer, 1995; Nakov, 2008; Hendrickx et al., 2013), and inventories of semantic relations (Girju et al., 2005). The ability of DSMs for accurately capturing semantic information may be affected by a number of factors involved in constructing the models, such as the source corpus, context representation, and parameters of the model. Relevant corpus parameters include size (Ferret, 2013; Mikolov et al., 2013c) and quality (Lapesa and Evert, 2014). Factors related to context representation include the context window size and the number of context dimensions adopted for a model (Lapesa and Evert, 2014); the choice of contexts to be used with targets (syntactic dependencies vs. bag-of-words) (Agirre et al., 2009); the use of morphosyntactic information (Pad´o and Lapata, 2003; Pad´o and Lapata, 2007); context filtering (Riedl and Biemann, 2012; Padr´o et al., 2014a); and dimensionality reduction methods (van de Cruys et al., 2012). Important model parameters that have been stud"
P16-1187,W05-0604,0,0.119621,"In this paper, we examine the impact of both corpus and context parameters for a variety of models, for the task of nominal compound compositionality prediction in English and French. For the choice of particular DSM, contradictory results have been published showing the superiority of neural models (Baroni et al., 2014) and of more traditional but carefully designed models (Levy et al., 2015). The former were also reported as a better fit to behavioral data on semantic prim1987 ing tasks (Mandera et al., 2016). Moreover, these evaluations are often performed on single-word similarity tasks (Freitag et al., 2005; CamachoCollados et al., 2015) and little has been said about the use of word embeddings for the compositionality prediction of multiword expressions. Two notable exceptions are the recent works of Salehi et al. (2015) and Yazdani et al. (2015). Salehi et al. (2015) show that word embeddings are more accurate in predicting compositionality than a simplistic count-based DSM. Yazdani et al. (2015) focus on the composition function, using a lightly supervised neural network to learn the best combination strategy for individual word vectors. In order to consolidate previous punctual results, we p"
P16-1187,Q15-1016,0,0.279976,"(Curran and Moens, 2002) and the use of subsampling and negative sampling techniques (Mikolov et al., 2013c). However, the particular effects may be heterogeneous and depend on the task and model (Lapesa and Evert, 2014). In this paper, we examine the impact of both corpus and context parameters for a variety of models, for the task of nominal compound compositionality prediction in English and French. For the choice of particular DSM, contradictory results have been published showing the superiority of neural models (Baroni et al., 2014) and of more traditional but carefully designed models (Levy et al., 2015). The former were also reported as a better fit to behavioral data on semantic prim1987 ing tasks (Mandera et al., 2016). Moreover, these evaluations are often performed on single-word similarity tasks (Freitag et al., 2005; CamachoCollados et al., 2015) and little has been said about the use of word embeddings for the compositionality prediction of multiword expressions. Two notable exceptions are the recent works of Salehi et al. (2015) and Yazdani et al. (2015). Salehi et al. (2015) show that word embeddings are more accurate in predicting compositionality than a simplistic count-based DSM."
P16-1187,P99-1041,0,0.169934,"impact of different parameters, such as level of corpus preprocessing, context window size and number of dimensions. The results obtained have a high correlation with human judgments, being comparable to or outperforming the state of the art for some datasets (Spearman’s ρ=.82 for the Reddy dataset). 1 Introduction Distributional semantic models (DSMs) use context information to represent the meaning of lexical units as vectors. They normally focus on the accurate semantic representation of single words. It is based on single words that many optimizations for these models have been proposed (Lin, 1999; Erk and Pad´o, 2010; Baroni and Lenci, 2010). This is particularly true for word embeddings, that is, a type of DSM where distributional vectors are obtained as a by-product of training a neural network to learn a function between words and their contexts (Mikolov et al., 2013a). Simultaneously, there has been intensive research on models to compose individual word vectors in order to create representations for larger units such as phrases, sentences and even whole documents (Mitchell and Lapata, 2010; Mikolov et al., 2013a). Larger units can often be assumed to have their meanings derived f"
P16-1187,W03-1810,0,0.0717748,".g. silver bullet, eager beaver). Precision-oriented NLP applications need to be able to identify partly-compositional and idiomatic cases and ensure meaning preservation during processing. Compositionality identification is a first step towards complete semantic interpretation in tasks such as machine translation (to translate non-compositional compounds as a unit), word sense disambiguation (to avoid assigning a sense to parts of non-compositional compounds), and semantic parsing (to identify complex predicates and their arguments). Even when larger units are explicitly represented in DSMs (McCarthy et al., 2003; Reddy et al., 2011; Mikolov et al., 2013c; Ferret, 2014), it is not clear whether the quality of these representations is comparable to the representations of single words. In particular, when building vectors for larger units, their generally lower frequencies in corpora (Kim and Baldwin, 2006) may combine with morphosyntactic phenomena to increase sparsity even further, often requiring non-trivial preprocessing (lemmatization and word reordering) to conflate variants. This paper presents a large-scale multilingual evaluation of DSMs and their parameters for the task of compositionality pre"
P16-1187,N13-1090,0,0.623429,"an’s ρ=.82 for the Reddy dataset). 1 Introduction Distributional semantic models (DSMs) use context information to represent the meaning of lexical units as vectors. They normally focus on the accurate semantic representation of single words. It is based on single words that many optimizations for these models have been proposed (Lin, 1999; Erk and Pad´o, 2010; Baroni and Lenci, 2010). This is particularly true for word embeddings, that is, a type of DSM where distributional vectors are obtained as a by-product of training a neural network to learn a function between words and their contexts (Mikolov et al., 2013a). Simultaneously, there has been intensive research on models to compose individual word vectors in order to create representations for larger units such as phrases, sentences and even whole documents (Mitchell and Lapata, 2010; Mikolov et al., 2013a). Larger units can often be assumed to have their meanings derived from their parts according to the language’s grammar, but this is not always the case (Sag et al., 2002). Many multiword units are associated with idiomatic interpretations, unrelated to the meaning of the component words (e.g. silver bullet, eager beaver). Precision-oriented NLP"
P16-1187,P08-1028,0,0.124367,"l., 2013b), sentences and 1 In French, one can also use a preposition and optional determiner, like cancer du poumon (lung cancer). 2 It refers to an initiative that provides money to many people without much effort. documents (Le and Mikolov, 2014), and in examining whether it is possible to accurately derive the semantics of a compound or multiword expression from its parts (McCarthy et al., 2003; Baldwin et al., 2003; Tratz and Hovy, 2010; Reddy et al., 2011). For the latter, proposals include using additive and multiplicative functions to combine vector representations of component words (Mitchell and Lapata, 2008; Reddy et al., 2011), calculating the overlap between the components and the expression (McCarthy et al., 2003) and looking at the literality of translations into multiple languages (Salehi et al., 2014). Other proposals to explicitly represent the semantics of nominal compounds include the use of paraphrases (Lauer, 1995; Nakov, 2008; Hendrickx et al., 2013), and inventories of semantic relations (Girju et al., 2005). The ability of DSMs for accurately capturing semantic information may be affected by a number of factors involved in constructing the models, such as the source corpus, context"
P16-1187,P03-1017,0,0.394971,"Missing"
P16-1187,J07-2002,0,0.0940225,"Missing"
P16-1187,D14-1162,0,0.096709,"iction of nominal compounds in French and English. We examine parameters like the level of corpus preprocessing, the size of the context window and the number of dimensions for context representation. Additionally, we compare standard DSMs based on positive pointwise mutual information (PPMI) 1986 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1986–1997, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics against widely used word embedding tools such as word2vec, henceforth w2v (Mikolov et al., 2013c), and GloVe (Pennington et al., 2014). We start with a discussion of related work (§2) and the materials and methods used (§3). We report on the evaluations performed (§4) and finish with conclusions and future work (§5). 2 Related Work We define nominal compounds as conventional noun phrases composed by two or more words, such as science fiction (Nakov, 2013). In English, they are often expressed as noun compounds but their syntactic realization may vary for different languages. For instance, one of the equivalent forms in French involves a denominal adjective used as modifier (e.g. cell death and the corresponding mort cellulai"
P16-1187,P16-2026,1,0.898254,"r 90 compounds and their individual word components, in a scale of literality from 0 (idiomatic) to 5 (literal), collected with Mechanical Turk (Reddy et al., 2011). For each compound, compositionality scores are averaged over its annotators. Compounds included in the dataset were selected to balance frequency range and degree of compositionality (low, middle and high). We use only the global compositionality score, ignoring individual word judgments. With a few exceptions (e.g. sacred cow), most compounds are formed exclusively by nouns. Reddy++ is a new resource created for this evaluation (Ramisch et al., 2016). It extends the Reddy set with an additional 90 English nominal compounds, in a total of 180 entries. Scores also range from 0 to 5 and were collected through Mechanical Turk and averaged over the annotators. The extra 90 entries include some adjective-noun compounds and are balanced with respect to frequency and compositionality. We focus our evaluation on this combined dataset, since it includes Reddy. However, to allow comparison with state of the art, we also report results individually for Reddy. Farahmand contains 1042 English compounds extracted from Wikipedia with binary noncompositio"
P16-1187,I11-1024,0,0.213183,"r beaver). Precision-oriented NLP applications need to be able to identify partly-compositional and idiomatic cases and ensure meaning preservation during processing. Compositionality identification is a first step towards complete semantic interpretation in tasks such as machine translation (to translate non-compositional compounds as a unit), word sense disambiguation (to avoid assigning a sense to parts of non-compositional compounds), and semantic parsing (to identify complex predicates and their arguments). Even when larger units are explicitly represented in DSMs (McCarthy et al., 2003; Reddy et al., 2011; Mikolov et al., 2013c; Ferret, 2014), it is not clear whether the quality of these representations is comparable to the representations of single words. In particular, when building vectors for larger units, their generally lower frequencies in corpora (Kim and Baldwin, 2006) may combine with morphosyntactic phenomena to increase sparsity even further, often requiring non-trivial preprocessing (lemmatization and word reordering) to conflate variants. This paper presents a large-scale multilingual evaluation of DSMs and their parameters for the task of compositionality prediction of nominal c"
P16-1187,W12-3307,0,0.00862446,"n constructing the models, such as the source corpus, context representation, and parameters of the model. Relevant corpus parameters include size (Ferret, 2013; Mikolov et al., 2013c) and quality (Lapesa and Evert, 2014). Factors related to context representation include the context window size and the number of context dimensions adopted for a model (Lapesa and Evert, 2014); the choice of contexts to be used with targets (syntactic dependencies vs. bag-of-words) (Agirre et al., 2009); the use of morphosyntactic information (Pad´o and Lapata, 2003; Pad´o and Lapata, 2007); context filtering (Riedl and Biemann, 2012; Padr´o et al., 2014a); and dimensionality reduction methods (van de Cruys et al., 2012). Important model parameters that have been studied include the choice of association and similarity measures (Curran and Moens, 2002) and the use of subsampling and negative sampling techniques (Mikolov et al., 2013c). However, the particular effects may be heterogeneous and depend on the task and model (Lapesa and Evert, 2014). In this paper, we examine the impact of both corpus and context parameters for a variety of models, for the task of nominal compound compositionality prediction in English and Fre"
P16-1187,W13-1005,0,0.149033,"ay vary considerably, independently from its status as a syntactic head or modifier, as cash in cash cow versus tears in crocodile tears. Indeed, various annotation scales have been proposed as means to collect human judgments about compositionality. Particularly for nominal compounds, Reddy et al. (2011) used a 6-point scale to collect judgments on the literal or figurative use of nominal compounds and its components in English. Similar judgments have also been collected for 244 German compounds, for which an average of 30 judgments on a scale from 1 to 7 were gathered through crowdsourcing (Roller et al., 2013). An alternative to multi-point scales is the binary judgment adopted by Farahmand et al. (2015), for a dataset of English nominal compounds. There has been much interest in creating semantic representations of larger units, such as phrases (Mikolov et al., 2013b), sentences and 1 In French, one can also use a preposition and optional determiner, like cancer du poumon (lung cancer). 2 It refers to an initiative that provides money to many people without much effort. documents (Le and Mikolov, 2014), and in examining whether it is possible to accurately derive the semantics of a compound or mul"
P16-1187,E14-1050,0,0.0217874,"effort. documents (Le and Mikolov, 2014), and in examining whether it is possible to accurately derive the semantics of a compound or multiword expression from its parts (McCarthy et al., 2003; Baldwin et al., 2003; Tratz and Hovy, 2010; Reddy et al., 2011). For the latter, proposals include using additive and multiplicative functions to combine vector representations of component words (Mitchell and Lapata, 2008; Reddy et al., 2011), calculating the overlap between the components and the expression (McCarthy et al., 2003) and looking at the literality of translations into multiple languages (Salehi et al., 2014). Other proposals to explicitly represent the semantics of nominal compounds include the use of paraphrases (Lauer, 1995; Nakov, 2008; Hendrickx et al., 2013), and inventories of semantic relations (Girju et al., 2005). The ability of DSMs for accurately capturing semantic information may be affected by a number of factors involved in constructing the models, such as the source corpus, context representation, and parameters of the model. Relevant corpus parameters include size (Ferret, 2013; Mikolov et al., 2013c) and quality (Lapesa and Evert, 2014). Factors related to context representation"
P16-1187,padro-etal-2014-comparing,1,0.919044,"Missing"
P16-1187,N15-1099,0,0.172554,"Missing"
P16-1187,D14-1047,1,0.90979,"Missing"
P16-1187,S10-1049,0,0.0561105,"and et al. (2015), for a dataset of English nominal compounds. There has been much interest in creating semantic representations of larger units, such as phrases (Mikolov et al., 2013b), sentences and 1 In French, one can also use a preposition and optional determiner, like cancer du poumon (lung cancer). 2 It refers to an initiative that provides money to many people without much effort. documents (Le and Mikolov, 2014), and in examining whether it is possible to accurately derive the semantics of a compound or multiword expression from its parts (McCarthy et al., 2003; Baldwin et al., 2003; Tratz and Hovy, 2010; Reddy et al., 2011). For the latter, proposals include using additive and multiplicative functions to combine vector representations of component words (Mitchell and Lapata, 2008; Reddy et al., 2011), calculating the overlap between the components and the expression (McCarthy et al., 2003) and looking at the literality of translations into multiple languages (Salehi et al., 2014). Other proposals to explicitly represent the semantics of nominal compounds include the use of paraphrases (Lauer, 1995; Nakov, 2008; Hendrickx et al., 2013), and inventories of semantic relations (Girju et al., 200"
P16-1187,C12-1165,0,0.205814,"Missing"
P16-1187,D15-1201,0,0.655121,"e been published showing the superiority of neural models (Baroni et al., 2014) and of more traditional but carefully designed models (Levy et al., 2015). The former were also reported as a better fit to behavioral data on semantic prim1987 ing tasks (Mandera et al., 2016). Moreover, these evaluations are often performed on single-word similarity tasks (Freitag et al., 2005; CamachoCollados et al., 2015) and little has been said about the use of word embeddings for the compositionality prediction of multiword expressions. Two notable exceptions are the recent works of Salehi et al. (2015) and Yazdani et al. (2015). Salehi et al. (2015) show that word embeddings are more accurate in predicting compositionality than a simplistic count-based DSM. Yazdani et al. (2015) focus on the composition function, using a lightly supervised neural network to learn the best combination strategy for individual word vectors. In order to consolidate previous punctual results, we present a large-scale and systematic evaluation, comparing DSMs and their parameters, on several compositionality datasets. 3 Materials and Methods We examine the impact of corpus parameters related to the target language and the degree of corpus"
P16-1187,N09-1003,0,\N,Missing
P16-2026,boos-etal-2014-identification,1,0.827008,"s the following steps: (1) compound selection; (2) sentence selection; and (3) questionnaire design. Compound selection The initial set of idiomatic and partially compositional candidates was constructed by introspection, independently for each language, since these may be harder to find in corpora because of lower frequency. This list of compounds was complemented by selecting entries from lists of frequent adjective+noun and noun+noun pairs. These were automatically extracted through POS-sequence queries using the mwetoolkit (Ramisch, 2015) from ukWaC (Baroni et al., 2009), frWaC and brWaC (Boos et al., 2014). We removed all compounds in which the complement is not an adjective in Portuguese/French (e.g. PT noun-noun abelha rainha), those in which the head is not necessarily a noun (e.g. FR aller simple, as aller is also a verb) and those in which the literal sense is very common in the corpus (e.g. EN low blow). For each language, we attempted to select a balanced set of 60 idiomatic, 60 partially compositional and 60 fully compositional compounds by rough manual preannotation.1 Sentence selection For each compound, we selected 3 sentences from a WaC corpus where the compound is used with the sam"
P16-2026,W15-0904,0,0.314258,"Missing"
P16-2026,W13-1017,0,0.258951,"Missing"
P16-2026,S13-2025,0,0.110204,"• Farahmand et al. (2015): individual binary judgments for non-compositionality and conventionality for 1,042 English noun compounds, annotated by 4 experts. One possible source of divergence among annotators is that some datasets do not take polysemy into account. Authors ask annotators to think about the most common sense of an MWE without providing context. Some of these datasets address this issue by providing example sentences to attenuate this problem. We also employ this strategy in our questionnaires. The most similar datasets to ours are the ones presented by Reddy et al. (2011) and Hendrickx et al. (2013). Our dataset combines the methodology from both of these, extending it to French and Portuguese. 3 preposition and optional determiner; e.g. lung cancer (EN) → cancer du poumon (FR), câncer de pulmão (PT). 2. N2 ADJ1 , using a denominal adjective which is derived from N1 ; e.g. cell death (EN) → mort cellulaire (FR), morte celular (PT). We describe the construction of datasets for English, French and Brazilian Portuguese. Given the two syntactic forms above, we focus on N2 ADJ1 for French and Portuguese, as its simpler structure resembles more closely the English noun-noun compound structure,"
P16-2026,W03-1810,0,0.515673,"Missing"
P16-2026,I11-1024,0,0.593144,"n a given language may correspond to a single word in the other languages. Even when it does translate as a compound, its POS pattern and level of compositionality may be widely different. 157 Figure 1: Evaluating compositionality regarding a compounds’ head. Questionnaire design We collect data for each compound through a separate HIT (Human Intelligence Task). Each HIT page contains a list of instructions followed by the questionnaire associated with that compound. In the instructions, we briefly describe the task and require that the users fill in an external identification form, following Reddy et al. (2011). This form provides us with demographics about the annotators, ensuring that they are native speakers of the target language. At the end of the form, they are also given extra example questions with annotated answers for training. After filling in the identification form, users can start working on the task. This section of the HIT is structured in 5 subtasks: 1. Read the compound itself. 2. Read 3 sentences containing the compound. 3. Provide 2 to 3 synonym expressions for the target compound seen in the sentences. 4. Using a Likert scale from 0 to 5, judge how much of the meaning of the com"
P16-2026,J08-4004,0,0.0460253,"ly accessory role in the meaning of the compound in relation to the head. EN Pearson r head-compound 0.75 Pearson r mod-compound 0.74 compound σ > 1.5 22 head σ > 1.5 23 modifier σ > 1.5 35 FR 0.81 0.89 41 44 55 PT 0.80 0.84 30 33 34 Table 1: Pearson correlation r and number of cases of high standard deviation σ. Out of all human judges, 3 of them annotated a large subset of 119 compounds in PT. For this subset, we report inter-annotator agreement. Pairwise weighted κ values range from .28 to .58 depending on the question (head, mod or comp) and on the annotator pair. Multi-rater α agreement (Artstein and Poesio, 2008) values are α = .52 for head, α = .36 for mod and α = .42 for comp scores. We have also calculated the α score of an expert annotator with himself, performing the same task a few weeks later. The score ranges from 2 We include the 90 compounds from Reddy et al. (2011), which are compatible with the new dataset. 158 5 4 4 Average compositionality score Average compositionality score 5 3 2 1 0 0 Compound Head Modifier 20 40 60 80 100 Instances 120 140 160 3 2 1 0 0 180 Compound Head Modifier 20 40 60 (a) English 80 100 Instances 120 140 160 180 (b) French Average compositionality score 5 4 3 2 1"
P16-2026,W13-1005,0,0.40842,"Missing"
P16-2026,W02-2001,1,0.764102,"Missing"
P16-2026,N15-1099,0,0.111551,"Missing"
P16-2026,D15-1201,0,0.469734,"Missing"
P16-2068,J90-1003,0,0.185498,"rence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks. 1 Introduction Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013). Traditionally, word representations have been obtained using count-based methods (Baroni et al., 2014), where the co-occurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014). Techniques for generating lower-rank representations have also been employed, such as PPMISVD (Levy et al., 2015) and GloVe (Pennington et al., 2014), both achieving state-of-the-art performance on a variety of tasks. Alternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (Baroni et al., 2014), the most notable of which is Skipgram with Negative Sampling (SGNS, Mikolov 2 2.1 Related Work PPMI-SVD Given a word w and a symmetric window of win context w"
P16-2068,J15-4004,0,0.0372613,"W For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Levy et al., 2014; Pennington et al., 2014; Levy et al., 2015). We examine, in particular, if LexVec weighted PPMI∗ factorization outperforms SVD, ˆ ) and GloVe (weighted factorization of log M Skip-gram (implicit factorization of the shifted PMI matrix), and compare the stochastic and minibatch approaches. Word similarity tasks are:3 WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001), MEN (Bruni et al., 2012), MTurk (Radinsky et al., 2011), RW (Luong et al., 2013), SimLex-999 (Hill et al., 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012), calculated using cosine. Word analogy tasks are: Google semantic (GSem) and syntactic (GSyn) (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c), using 3CosAdd and 3CosM ul (Levy et al., 2014). 1 ∗ 2 ˜c > − P P M Iwc (Ww W ) (6) 2 k > 1X ∗ = Ewi ∼Pn (w) (Ww W˜wi − P P M Iww )2 i 2 i=1 (7) ec LLexV = wc ec LLexV w We minimize eqs. (6) and (7) using two alternative approaches: Mini-Batch (MB): This variant executes gradient descent in exactly the same way as SGNS. E"
P16-2068,P14-1023,0,0.0549942,"x via stochastic gradient descent, employing a weighting scheme that assigns heavier penalties for errors on frequent cooccurrences while still accounting for negative co-occurrence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks. 1 Introduction Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013). Traditionally, word representations have been obtained using count-based methods (Baroni et al., 2014), where the co-occurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014). Techniques for generating lower-rank representations have also been employed, such as PPMISVD (Levy et al., 2015) and GloVe (Pennington et al., 2014), both achieving state-of-the-art performance on a variety of tasks. Alternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (B"
P16-2068,P12-1092,0,0.0211854,"Missing"
P16-2068,P12-1015,0,0.0149793,"nd W ˜ for GloVe (following Levy et al. SVD and W + W ˜ for LexVec. (2015), and W and W + W For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Levy et al., 2014; Pennington et al., 2014; Levy et al., 2015). We examine, in particular, if LexVec weighted PPMI∗ factorization outperforms SVD, ˆ ) and GloVe (weighted factorization of log M Skip-gram (implicit factorization of the shifted PMI matrix), and compare the stochastic and minibatch approaches. Word similarity tasks are:3 WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001), MEN (Bruni et al., 2012), MTurk (Radinsky et al., 2011), RW (Luong et al., 2013), SimLex-999 (Hill et al., 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012), calculated using cosine. Word analogy tasks are: Google semantic (GSem) and syntactic (GSyn) (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c), using 3CosAdd and 3CosM ul (Levy et al., 2014). 1 ∗ 2 ˜c > − P P M Iwc (Ww W ) (6) 2 k > 1X ∗ = Ewi ∼Pn (w) (Ww W˜wi − P P M Iww )2 i 2 i=1 (7) ec LLexV = wc ec LLexV w We minimize eqs. (6) and (7) using two alternative approaches: Mi"
P16-2068,Q15-1016,0,0.211745,"ntations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013). Traditionally, word representations have been obtained using count-based methods (Baroni et al., 2014), where the co-occurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014). Techniques for generating lower-rank representations have also been employed, such as PPMISVD (Levy et al., 2015) and GloVe (Pennington et al., 2014), both achieving state-of-the-art performance on a variety of tasks. Alternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (Baroni et al., 2014), the most notable of which is Skipgram with Negative Sampling (SGNS, Mikolov 2 2.1 Related Work PPMI-SVD Given a word w and a symmetric window of win context words to the left and win to the right, the co-occurrence matrix of elements Mwc is defined as the number of times a target word w and the context word c co-occurred in the corpus within"
P16-2068,W14-1618,0,0.348087,"ches and often outperforms state-of-the-art methods on many of these tasks. 1 Introduction Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013). Traditionally, word representations have been obtained using count-based methods (Baroni et al., 2014), where the co-occurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014). Techniques for generating lower-rank representations have also been employed, such as PPMISVD (Levy et al., 2015) and GloVe (Pennington et al., 2014), both achieving state-of-the-art performance on a variety of tasks. Alternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (Baroni et al., 2014), the most notable of which is Skipgram with Negative Sampling (SGNS, Mikolov 2 2.1 Related Work PPMI-SVD Given a word w and a symmetric window of win context words to the left and win to the right, the co-occurrence matrix of elem"
P16-2068,P98-2127,0,0.0563473,"s for errors on frequent cooccurrences while still accounting for negative co-occurrence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks. 1 Introduction Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013). Traditionally, word representations have been obtained using count-based methods (Baroni et al., 2014), where the co-occurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014). Techniques for generating lower-rank representations have also been employed, such as PPMISVD (Levy et al., 2015) and GloVe (Pennington et al., 2014), both achieving state-of-the-art performance on a variety of tasks. Alternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (Baroni et al., 2014), the most notable of which is Skipgram with Negative Sampling ("
P16-2068,W13-3512,0,0.080055,"for LexVec. (2015), and W and W + W For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Levy et al., 2014; Pennington et al., 2014; Levy et al., 2015). We examine, in particular, if LexVec weighted PPMI∗ factorization outperforms SVD, ˆ ) and GloVe (weighted factorization of log M Skip-gram (implicit factorization of the shifted PMI matrix), and compare the stochastic and minibatch approaches. Word similarity tasks are:3 WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001), MEN (Bruni et al., 2012), MTurk (Radinsky et al., 2011), RW (Luong et al., 2013), SimLex-999 (Hill et al., 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012), calculated using cosine. Word analogy tasks are: Google semantic (GSem) and syntactic (GSyn) (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c), using 3CosAdd and 3CosM ul (Levy et al., 2014). 1 ∗ 2 ˜c > − P P M Iwc (Ww W ) (6) 2 k > 1X ∗ = Ewi ∼Pn (w) (Ww W˜wi − P P M Iww )2 i 2 i=1 (7) ec LLexV = wc ec LLexV w We minimize eqs. (6) and (7) using two alternative approaches: Mini-Batch (MB): This variant executes gradient descent in"
P16-2068,N13-1090,0,0.795086,"∗c 419 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 419–424, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) where ’*’ represents the summation of the corresponding index. As this matrix is unbounded in the inferior limit, in most applications it is replaced by its positive definite version, PPMI, where negative values are set to zero. The performance of the PPMI matrix on word similarity tasks can be further improved by using contextdistribution smoothing (Levy et al., 2015) and subsampling the corpus (Mikolov et al., 2013b). As word embeddings with lower dimensionality may improve efficiency and generalization (Levy et al., 2015), the improved PPMI∗ matrix can be factorized as a product of two lower rank matrices. ∗ ˜ c> P P M Iwc ' Ww W which weights all reconstruction errors equally. However, as it does not penalize reconstruction errors for pairs with zero counts in the co-occurrence matrix, no effort is made to scatter the vectors for these pairs. 2.3 SGNS (Mikolov et al., 2013b) trains a neural network to predict the probability of observing a context word c given a target word w, sliding a symmetric wind"
P16-2068,D15-1024,0,0.0221816,"Missing"
P16-2068,D14-1162,0,0.132285,"have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013). Traditionally, word representations have been obtained using count-based methods (Baroni et al., 2014), where the co-occurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014). Techniques for generating lower-rank representations have also been employed, such as PPMISVD (Levy et al., 2015) and GloVe (Pennington et al., 2014), both achieving state-of-the-art performance on a variety of tasks. Alternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (Baroni et al., 2014), the most notable of which is Skipgram with Negative Sampling (SGNS, Mikolov 2 2.1 Related Work PPMI-SVD Given a word w and a symmetric window of win context words to the left and win to the right, the co-occurrence matrix of elements Mwc is defined as the number of times a target word w and the context word c co-occurred in the corpus within the window. The PMI matrix is define"
P16-2068,P13-1045,0,0.0135406,"entations that uses low-rank, weighted factorization of the Positive Point-wise Mutual Information matrix via stochastic gradient descent, employing a weighting scheme that assigns heavier penalties for errors on frequent cooccurrences while still accounting for negative co-occurrence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks. 1 Introduction Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013). Traditionally, word representations have been obtained using count-based methods (Baroni et al., 2014), where the co-occurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014). Techniques for generating lower-rank representations have also been employed, such as PPMISVD (Levy et al., 2015) and GloVe (Pennington et al., 2014), both achieving state-of-the-art performance on a variety of tasks. Alternatively, vector-spac"
P16-2068,P10-1040,0,0.0160689,"method for generating distributed word representations that uses low-rank, weighted factorization of the Positive Point-wise Mutual Information matrix via stochastic gradient descent, employing a weighting scheme that assigns heavier penalties for errors on frequent cooccurrences while still accounting for negative co-occurrence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks. 1 Introduction Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013). Traditionally, word representations have been obtained using count-based methods (Baroni et al., 2014), where the co-occurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014). Techniques for generating lower-rank representations have also been employed, such as PPMISVD (Levy et al., 2015) and GloVe (Pennington et al., 2014), both achieving state-of-the-art performance on"
P16-2068,C98-2122,0,\N,Missing
P18-2002,D10-1115,0,0.341391,"exsalle.com avillavicencio@inf.ufrgs.br Abstract (RNTN), which maps each symbol to separate hidden layer weights (referred to as recurrence matrices from hereon). Besides increasing model capacity while keeping computation constant, this approach has another motivation: viewing the RNN’s hidden state as being transformed by each new symbol in the sequence, it is intuitive that different symbols will transform the network’s hidden state in different ways (Sutskever et al., 2011). Various studies on compositionality similarly argue that some words are better modeled by matrices than by vectors (Baroni and Zamparelli, 2010; Socher et al., 2012). Unfortunately, having separate recurrence matrices for each symbol requires memory that is linear in the symbol vocabulary size (|V |). This is not an issue for character-level models, which have small vocabularies, but is prohibitive for word-level models which can have vocabulary size in the millions if we consider surface forms. In this paper, we propose the Restricted RNTN (r-RNTN) which uses only K < |V |recurrence matrices. Given that |V |words must be assigned K matrices, we map the most frequent K − 1 words to the first K − 1 matrices, and share the K-th matrix"
P18-2002,D12-1110,0,0.756111,"f.ufrgs.br Abstract (RNTN), which maps each symbol to separate hidden layer weights (referred to as recurrence matrices from hereon). Besides increasing model capacity while keeping computation constant, this approach has another motivation: viewing the RNN’s hidden state as being transformed by each new symbol in the sequence, it is intuitive that different symbols will transform the network’s hidden state in different ways (Sutskever et al., 2011). Various studies on compositionality similarly argue that some words are better modeled by matrices than by vectors (Baroni and Zamparelli, 2010; Socher et al., 2012). Unfortunately, having separate recurrence matrices for each symbol requires memory that is linear in the symbol vocabulary size (|V |). This is not an issue for character-level models, which have small vocabularies, but is prohibitive for word-level models which can have vocabulary size in the millions if we consider surface forms. In this paper, we propose the Restricted RNTN (r-RNTN) which uses only K < |V |recurrence matrices. Given that |V |words must be assigned K matrices, we map the most frequent K − 1 words to the first K − 1 matrices, and share the K-th matrix among the remaining wo"
P18-2002,P16-1186,0,0.0727123,"Missing"
padro-etal-2014-comparing,C04-1146,0,\N,Missing
padro-etal-2014-comparing,W02-0908,0,\N,Missing
padro-etal-2014-comparing,W05-0604,0,\N,Missing
padro-etal-2014-comparing,W03-1810,0,\N,Missing
padro-etal-2014-comparing,J10-4006,0,\N,Missing
padro-etal-2014-comparing,P06-4020,0,\N,Missing
padro-etal-2014-comparing,P94-1019,0,\N,Missing
padro-etal-2014-comparing,P07-1061,0,\N,Missing
padro-etal-2014-comparing,P98-2127,0,\N,Missing
padro-etal-2014-comparing,C98-2122,0,\N,Missing
ramisch-etal-2010-mwetoolkit,pearce-2002-comparative,0,\N,Missing
ramisch-etal-2010-mwetoolkit,messiant-etal-2008-lexschem,0,\N,Missing
ramisch-etal-2010-mwetoolkit,copestake-etal-2002-multiword,1,\N,Missing
ramisch-etal-2010-mwetoolkit,J93-1007,0,\N,Missing
ramisch-etal-2010-mwetoolkit,W03-1806,0,\N,Missing
ramisch-etal-2010-mwetoolkit,D07-1110,1,\N,Missing
ramisch-etal-2010-mwetoolkit,W06-1206,1,\N,Missing
ramisch-etal-2010-mwetoolkit,calzolari-etal-2002-towards,0,\N,Missing
ramisch-etal-2010-mwetoolkit,W02-1030,0,\N,Missing
ramisch-etal-2010-mwetoolkit,W08-2107,1,\N,Missing
ramisch-etal-2010-mwetoolkit,P07-1115,0,\N,Missing
ramisch-etal-2010-mwetoolkit,C02-1013,0,\N,Missing
S16-1140,C08-1009,0,0.0312425,"llow one to discover new MWE candidate lists, filter them and project them back on text according to some parameters. Our system uses the latter as basis for MWE identification. Word sense disambiguation (WSD) methods can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Unsupervised approaches using clustering and distributional similarity (Brody and Lapata, 2008; Goyal and Hovy, 2014) can also be employed for WSD. Both supervised and unsupervised WSD techniques have also been used to distinguish literal from idiomatic uses of MWEs (Fazly et al., 2009; Diab and Bhutada, 2009). Nonetheless, systematically choosing the most frequent sense is a surprisingly good baseline, not always easy to beat (McCarthy et al., 2007; Navigli, 2009). This was also verified for MWE disambiguation (Uchiyama et al., 2005). Thus, in this work, we implemented a simple supervised predominant-sense heuristic and will investigate more sophisticated WSD techniques as future work"
S16-1140,S01-1014,0,0.0924011,"; Schneider et al., 2014). The mwetoolkit (Ramisch, 2015) provides command-line programs that allow one to discover new MWE candidate lists, filter them and project them back on text according to some parameters. Our system uses the latter as basis for MWE identification. Word sense disambiguation (WSD) methods can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Unsupervised approaches using clustering and distributional similarity (Brody and Lapata, 2008; Goyal and Hovy, 2014) can also be employed for WSD. Both supervised and unsupervised WSD techniques have also been used to distinguish literal from idiomatic uses of MWEs (Fazly et al., 2009; Diab and Bhutada, 2009). Nonetheless, systematically choosing the most frequent sense is a surprisingly good baseline, not always easy to beat (McCarthy et al., 2007; Navigli, 2009). This was also verified for MWE disambiguation (Uchiyama et al., 2005). Thus, in this work, we implemented a simple supervised p"
S16-1140,W11-0809,0,0.413578,"brary for direct lexicon projection based on preexisting MWE lists. Finite-state transducers can also be used to take into account the internal morphology of component words and perform efficient tokenization based on MWE dictionaries (Savary, 2009). The problem of MWE identification 910 Proceedings of SemEval-2016, pages 910–917, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics has also been modeled using supervised machine learning. Probabilistic MWE taggers usually encode the data using a begin-inside-outside scheme and learn CRF-like taggers on it (Constant and Sigogne, 2011; Schneider et al., 2014). The mwetoolkit (Ramisch, 2015) provides command-line programs that allow one to discover new MWE candidate lists, filter them and project them back on text according to some parameters. Our system uses the latter as basis for MWE identification. Word sense disambiguation (WSD) methods can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers ("
S16-1140,W09-2903,0,0.0316836,"s can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Unsupervised approaches using clustering and distributional similarity (Brody and Lapata, 2008; Goyal and Hovy, 2014) can also be employed for WSD. Both supervised and unsupervised WSD techniques have also been used to distinguish literal from idiomatic uses of MWEs (Fazly et al., 2009; Diab and Bhutada, 2009). Nonetheless, systematically choosing the most frequent sense is a surprisingly good baseline, not always easy to beat (McCarthy et al., 2007; Navigli, 2009). This was also verified for MWE disambiguation (Uchiyama et al., 2005). Thus, in this work, we implemented a simple supervised predominant-sense heuristic and will investigate more sophisticated WSD techniques as future work. other words, we keep MWE candidates whose proportion of annotated instances with respect to all occurrences in the training corpus is above a threshold t, discarding the rest. The thresholds were manually chosen bas"
S16-1140,J09-1005,0,0.577136,"guation (WSD) methods can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Unsupervised approaches using clustering and distributional similarity (Brody and Lapata, 2008; Goyal and Hovy, 2014) can also be employed for WSD. Both supervised and unsupervised WSD techniques have also been used to distinguish literal from idiomatic uses of MWEs (Fazly et al., 2009; Diab and Bhutada, 2009). Nonetheless, systematically choosing the most frequent sense is a surprisingly good baseline, not always easy to beat (McCarthy et al., 2007; Navigli, 2009). This was also verified for MWE disambiguation (Uchiyama et al., 2005). Thus, in this work, we implemented a simple supervised predominant-sense heuristic and will investigate more sophisticated WSD techniques as future work. other words, we keep MWE candidates whose proportion of annotated instances with respect to all occurrences in the training corpus is above a threshold t, discarding the rest. The thresholds"
S16-1140,C14-1123,0,0.0196432,"MWE candidate lists, filter them and project them back on text according to some parameters. Our system uses the latter as basis for MWE identification. Word sense disambiguation (WSD) methods can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Unsupervised approaches using clustering and distributional similarity (Brody and Lapata, 2008; Goyal and Hovy, 2014) can also be employed for WSD. Both supervised and unsupervised WSD techniques have also been used to distinguish literal from idiomatic uses of MWEs (Fazly et al., 2009; Diab and Bhutada, 2009). Nonetheless, systematically choosing the most frequent sense is a surprisingly good baseline, not always easy to beat (McCarthy et al., 2007; Navigli, 2009). This was also verified for MWE disambiguation (Uchiyama et al., 2005). Thus, in this work, we implemented a simple supervised predominant-sense heuristic and will investigate more sophisticated WSD techniques as future work. other words, we keep"
S16-1140,W11-0818,0,0.0309068,"andidates are extracted without losing track of their tokenlevel occurrences, to guarantee that all the MWE occurrences learned from the training data are projected onto the test corpus. For semantic tagging we adopted a predominant-sense heuristic. In the remainder of this paper, we present related work (§ 2), then we present and discuss the results of the MWE identification subsystem (§ 3) and of the supersense tagging subsystem (§ 4). We then conclude and share ideas for future improvements (§ 5). 2 Related Work Practical solutions for rule-based MWE identification include tools like jMWE (Kulkarni and Finlayson, 2011), a library for direct lexicon projection based on preexisting MWE lists. Finite-state transducers can also be used to take into account the internal morphology of component words and perform efficient tokenization based on MWE dictionaries (Savary, 2009). The problem of MWE identification 910 Proceedings of SemEval-2016, pages 910–917, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics has also been modeled using supervised machine learning. Probabilistic MWE taggers usually encode the data using a begin-inside-outside scheme and learn CRF-like taggers o"
S16-1140,J07-4005,0,0.0310987,"et to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Unsupervised approaches using clustering and distributional similarity (Brody and Lapata, 2008; Goyal and Hovy, 2014) can also be employed for WSD. Both supervised and unsupervised WSD techniques have also been used to distinguish literal from idiomatic uses of MWEs (Fazly et al., 2009; Diab and Bhutada, 2009). Nonetheless, systematically choosing the most frequent sense is a surprisingly good baseline, not always easy to beat (McCarthy et al., 2007; Navigli, 2009). This was also verified for MWE disambiguation (Uchiyama et al., 2005). Thus, in this work, we implemented a simple supervised predominant-sense heuristic and will investigate more sophisticated WSD techniques as future work. other words, we keep MWE candidates whose proportion of annotated instances with respect to all occurrences in the training corpus is above a threshold t, discarding the rest. The thresholds were manually chosen based on what seemed to yield better results on the development set. Finally, we project the resulting list of MWE candidates on the test data, t"
S16-1140,P15-1108,1,0.852029,"ic combinations. • Using fixedness features to identify and disambiguate very productive patterns like ADJ_N (Ramisch et al., 2008; Fazly et al., 2009). • Replacing the C ONTIG method by a sequence tagger for contiguous MWEs (e.g. using a CRF), in order to identify unknown MWEs based on generalizations made from known MWEs (Constant and Sigogne, 2011; Schneider et al., 2014). • Developing a more realistic WSD algorithm for supersense tagging, able to tag unseen words and MWEs and to take context into account. • Taking parse trees into account to distinguish MWEs from accidental cooccurrences (Nasr et al., 2015). • Using semantic-based association measures and semantic-based features based on word embeddings to target idiomatic MWEs (Salehi et al., 2015). 915 Acknowledgments This work has been funded by the French Agence Nationale pour la Recherche through projects PARSEME-FR (ANR-14-CERA-0001) and ORFEO (ANR-12-CORP-0005), and by French-Brazilian cooperation projects CAMELEON (CAPES-COFECUB #707/11) and AIM-WEST (FAPERGSINRIA 1706-2551/13-7). Part of the results presented in this paper were obtained through research on a project titled &quot;Simplificação Textual de Expressões Complexas&quot;, sponsored by Sa"
S16-1140,P07-1115,0,0.0870416,"Missing"
S16-1140,N15-1099,0,0.024269,"Missing"
S16-1140,Q14-1016,0,0.391857,"jection based on preexisting MWE lists. Finite-state transducers can also be used to take into account the internal morphology of component words and perform efficient tokenization based on MWE dictionaries (Savary, 2009). The problem of MWE identification 910 Proceedings of SemEval-2016, pages 910–917, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics has also been modeled using supervised machine learning. Probabilistic MWE taggers usually encode the data using a begin-inside-outside scheme and learn CRF-like taggers on it (Constant and Sigogne, 2011; Schneider et al., 2014). The mwetoolkit (Ramisch, 2015) provides command-line programs that allow one to discover new MWE candidate lists, filter them and project them back on text according to some parameters. Our system uses the latter as basis for MWE identification. Word sense disambiguation (WSD) methods can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Un"
S16-1140,S16-1084,0,0.0245054,"late it into an equivalent meaning in the target language. While determining the meaning of single words is a difficult task on its own, the problem is compounded by the pervasiveness of Multiword Expressions (MWEs). MWEs are semantic units that span over multiple lexemes in the text (e.g. dry run, look up, fall flat). Their meaning cannot be inferred by applying regular composition rules on the meanings of their component words. The task of semantic tagging is thus deeply intertwined with the identification of multiword expressions. This paper presents our solution to the DiMSUM shared task (Schneider et al., 2016), where the evaluated systems are expected to perform both semantic tagging and multiword identification. Our pipeline system first detects and groups MWEs and then assigns supersense tags, as two consecutive steps. For MWE identification, we use a task-specific instantiation of the mwetoolkit (Ramisch, 2015), handling both contiguous and non-contiguous MWEs with some degree of customization (Cordeiro et al., 2015). Additionally, MWE type-level candidates are extracted without losing track of their tokenlevel occurrences, to guarantee that all the MWE occurrences learned from the training data"
villavicencio-etal-2004-multilingual,copestake-etal-2002-multiword,1,\N,Missing
villavicencio-etal-2004-multilingual,W03-1803,1,\N,Missing
villavicencio-etal-2004-multilingual,copestake-etal-2004-lexicon,1,\N,Missing
villavicencio-etal-2012-large,P06-4020,0,\N,Missing
villavicencio-etal-2012-large,W07-0605,0,\N,Missing
villavicencio-etal-2012-large,korhonen-etal-2006-large,0,\N,Missing
W00-0743,J99-1002,0,0.0259249,"e category is to be combined (where VALUE can be either f o r w a r d or backw a r d ) . As an example, in English an intransitire verb (s
p) is encoded as shown in figure 1, where only the relevant attributes are shown. In this work, we employ the rules of (forward and backward) application, (forward and backward) composition and generalised weak permuration. A more detailed description of the UBGCG used can be found in (Villavicencio 2000). The UG is implemented as a UB-GCG, erabedded in a default inheritance network of lexical types (Villavicencio 1999), implemented in the YADU framework (Lascarides and Copestake 1999). The categories and rules in the grammar are defined as types in the hierarchy, represented in terms of TDFSS and the featurestructures associated with any given category or rule are defined by the inheritance chain. With different sub-networks used to encode different kinds of linguistic knowledge, linguistic regularities are encoded near the top of a network, while types further down the network are used to represent sub-regularities or exceptions. Thus, types are concisely defined, with only specific information being described, since more general information is inherited from the supertyp"
W00-0743,E99-1039,1,0.793174,"egory, and DIRECTION, encoding the direction in which the category is to be combined (where VALUE can be either f o r w a r d or backw a r d ) . As an example, in English an intransitire verb (s
p) is encoded as shown in figure 1, where only the relevant attributes are shown. In this work, we employ the rules of (forward and backward) application, (forward and backward) composition and generalised weak permuration. A more detailed description of the UBGCG used can be found in (Villavicencio 2000). The UG is implemented as a UB-GCG, erabedded in a default inheritance network of lexical types (Villavicencio 1999), implemented in the YADU framework (Lascarides and Copestake 1999). The categories and rules in the grammar are defined as types in the hierarchy, represented in terms of TDFSS and the featurestructures associated with any given category or rule are defined by the inheritance chain. With different sub-networks used to encode different kinds of linguistic knowledge, linguistic regularities are encoded near the top of a network, while types further down the network are used to represent sub-regularities or exceptions. Thus, types are concisely defined, with only specific information being descr"
W02-2001,J95-4004,0,0.103412,"ise extraposed) for the head verb of the VPC. Here and for the subsequent methods, we assume that the maximum word length for NP complements in the split configuration for transitive VPCs is 5,2 i.e. that an NP “heavier” than this would occur more naturally in the joined configuration. We thus discount all particles which are more than 5 words from their governing verb. Additionally, we extracted a set of 73 canonical particles from the LinGO-ERG, and used this to filter out extraneous particles in the POS data. In line with our assumption of raw text to extract over, we use the Brill tagger (Brill, 1995) to automatically tag the WSJ, rather than making use of the manual POS annotation provided in the Penn Treebank. We further lemmatise the data using morph (Minnen et al., 2001) and extract VPCs based on the Brill tags. This produces a total of 135 VPCs, which we evaluate according to the standard metrics of precision (Prec), recall (Rec) and F-score (Fβ=1 ). Note that here and for the remainder of this paper, precision is calculated according to the manual annotation for the combined total of 4,173 VPC candidate types extracted by the various methods described in this paper, whereas recall is"
W02-2001,calzolari-etal-2002-towards,0,0.0380294,"ut, chunker output and a chunk grammar, respectively, with the chunk grammar method optionally combining with an attachment resolution module to determine the syntactic structure of verb–preposition pairs in ambiguous constructs. We then combine the three methods together into a single classifier, and add in a number of extra lexical and frequentistic features, producing a final F-score of 0.865 over the WSJ. 1 Introduction There is growing awareness of the pervasiveness and idiosyncrasy of multiword expressions (MWEs), and the need for a robust, structured handling thereof (Sag et al., 2002; Calzolari et al., 2002; Copestake et al., 2002). Examples of MWEs are lexically fixed expressions (e.g. ad hoc), idioms (e.g. see double), light verb constructions (e.g. make a mistake) and institutionalised phrases (e.g. kindle excitement). MWEs pose a challenge to NLP due to their syntactic and semantic idiosyncrasies, which are often unpredictable from their component parts. Largescale manual annotation of MWEs is infeasible due to their sheer volume (at least equivalent to the number of simplex words (Jackendoff, 1997)), productivity and domain-specificity. Ideally, therefore, we would like to have some means o"
W02-2001,copestake-flickinger-2000-open,0,0.00846794,"city. Ideally, therefore, we would like to have some means of automatically extracting MWEs from a given domain or corpus, allowing us to pre-tune our grammar prior to deployment. It is this task of extraction that we target in this paper. This research represents a component of the LinGO multiword expression project,1 which is targeted at extracting, adequately handling and representing MWEs of all types. As a research testbed and target resource to expand/domain-tune, we use the LinGO English Resource Grammar (LinGOERG), a linguistically-precise HPSG-based grammar under development at CSLI (Copestake and Flickinger, 2000; Flickinger, 2000). The particular MWE type we target for extraction is the English verb-particle construction. Verb-particle constructions (“VPCs”) consist of a 1 http://lingo.stanford.edu/mwe head verb and one or more obligatory particles, in the form of intransitive prepositions (e.g. hand in), adjectives (e.g. cut short) or verbs (e.g. let go) (Villavicencio and Copestake, 2002a; Villavicencio and Copestake, 2002b; Huddleston and Pullum, 2002); for the purposes of this paper, we will focus exclusively on prepositional particles—by far the most common and productive of the three types— and"
W02-2001,copestake-etal-2002-multiword,1,0.771,"chunk grammar, respectively, with the chunk grammar method optionally combining with an attachment resolution module to determine the syntactic structure of verb–preposition pairs in ambiguous constructs. We then combine the three methods together into a single classifier, and add in a number of extra lexical and frequentistic features, producing a final F-score of 0.865 over the WSJ. 1 Introduction There is growing awareness of the pervasiveness and idiosyncrasy of multiword expressions (MWEs), and the need for a robust, structured handling thereof (Sag et al., 2002; Calzolari et al., 2002; Copestake et al., 2002). Examples of MWEs are lexically fixed expressions (e.g. ad hoc), idioms (e.g. see double), light verb constructions (e.g. make a mistake) and institutionalised phrases (e.g. kindle excitement). MWEs pose a challenge to NLP due to their syntactic and semantic idiosyncrasies, which are often unpredictable from their component parts. Largescale manual annotation of MWEs is infeasible due to their sheer volume (at least equivalent to the number of simplex words (Jackendoff, 1997)), productivity and domain-specificity. Ideally, therefore, we would like to have some means of automatically extractin"
W02-2001,J93-1003,0,0.0296048,"itive prepositional verb hand in or simple transitive verb with PP adjunct), and (3) hand [the paper in here] (simple transitive verb). In such cases, we can choose to either (a) avoid committing ourselves to any one analysis, and ignore all such ambiguous cases, or (b) use some means to resolve the attachment ambiguity (i.e. whether the NP is governed by the verb, resulting in a VPC, or the preposition, resulting in a prepositional verb or free verb–preposition combination). In the latter case, we use an unsupervised attachment disambiguation method, based on the log-likelihood ratio (“LLR”, Dunning (1993)). That is, we use the chunker output to enumerate all the verb–preposition, preposition– noun and verb–noun bigrams in the WSJ data, based on chunk heads rather than strict word bigrams. We then use frequency data to pre-calculate the LLR for each such type. In the case that the verb and “particle” are joined (i.e. no NP occurs between them), we simply compare the LLR of the verb–noun and particle–noun pairs, and assume a VPC analysis in the case that the former is strictly larger than the latter. In the case that the verb and “particle” are split (i.e. we have the chunk sequence VC NC1 PC NC"
W02-2001,kaalep-muischnek-2002-using,0,0.029314,"with our research impossible. The work of Blaheta and Johnson (2001) is closer in its objectives to our research, in that it takes a parsed corpus and extracts out multiword verbs (i.e. VPCs and prepositional verbs) through the use of log-linear models. Once again, direct comparison with our results is difficult, as Blaheta and Johnson output a ranked list of all verb–preposition pairs, and subjectively evaluate the quality of different sections of the list. Additionally, they make no attempt to distinguish VPCs from prepositional verbs. The method which is perhaps closest to ours is that of Kaalep and Muischnek (2002) in extracting Estonian multiword verbs (which are similar to English VPCs in that the components of the multiword verb can be separated by other words). Kaalep and Muischnek apply the “mutual expectation” test over a range of “positioned bigrams”, similar to those used by Smadja. They test their method over three different corpora, with results ranging from a precision of 0.21 and recall of 0.86 (F-score=0.34) for the smallest corpus, to a precision of 0.03 and recall of 0.85 (F-score=0.06) for the largest corpus. That is, high levels of noise are evident in the system output, and the F-score"
W02-2001,J93-2004,0,0.0296154,"n and Radev (2000)) in that they cannot be captured effectively using N-grams, due to the variability in the number and type of words potentially interceding between the verb and particle. We are aiming for an extraction technique which is applicable to any raw text corpus, allowing us to tune grammars to novel domains. Any linguistic annotation required during the extraction process, therefore, is produced through automatic means, and it is only for reasons of accessibility and comparability with other research that we choose to work over the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). That is, other than in establishing upper bounds on the performance of the different extraction methods, we use only the raw text component of the treebank. In this paper, we first outline distinguishing features of VPCs relevant to the extraction process (§ 2). We then present and evaluate a number of simple methods for extracting VPCs based on, respectively, POS tagging (§ 3), the output of a full text chunk parser (§ 4), and a chunk grammar (§ 5). Finally, we detail enhancements to the basic methods (§ 6) and give a brief description of related research (§ 7) before concluding the paper ("
W02-2001,J93-1007,0,0.847998,"rbs (e.g. let go) (Villavicencio and Copestake, 2002a; Villavicencio and Copestake, 2002b; Huddleston and Pullum, 2002); for the purposes of this paper, we will focus exclusively on prepositional particles—by far the most common and productive of the three types— and further restrict our attention to single-particle VPCs (i.e. we ignore VPCs such as get along together ). We define VPCs to optionally select for an NP complement, i.e. to occur both transitively (e.g. hand in the paper ) and intransitively (e.g. battle on). One aspect of VPCs that makes them difficult to extract (cited in, e.g., Smadja (1993)) is that the verb and particle can be non-contiguous, e.g. hand the paper in and battle right on. This sets them apart from conventional collocations and terminology (see, e.g., Manning and Sch¨ utze (1999) and McKeown and Radev (2000)) in that they cannot be captured effectively using N-grams, due to the variability in the number and type of words potentially interceding between the verb and particle. We are aiming for an extraction technique which is applicable to any raw text corpus, allowing us to tune grammars to novel domains. Any linguistic annotation required during the extraction pro"
W02-2001,W00-0726,0,0.0208529,"rill tagger in identifying particles, we next look to full chunk 2 Note, this is the same as the maximum span length of 5 used by Smadja (1993), and above the maximum attested NP length of 3 from our corpus study (see Section 2.2). Prec 0.889 WSJ Rec 0.911 Fβ=1 0.900 Prec 0.912 CoNLL Rec Fβ=1 0.925 0.919 Table 2: Chunking performance parsing. Full chunk parsing involves partitioning up a text into syntactically-cohesive, head-final segments (“chunks”), without attempting to resolve inter-chunk dependencies. In the chunk inventory devised for the CoNLL-2000 test chunking shared task (Tjong Kim Sang and Buchholz, 2000), a dedicated particle chunk type once again exists. It is therefore possible to adopt an analogous approach to that from Method-1, in identifying particle chunks then working back to locate the verb each particle chunk is associated with. 4.1 Chunk parsing method In order to chunk parse the WSJ, we first tagged the full WSJ and Brown corpora using the Brill tagger, and then converted them into chunks based on the original Penn Treebank parse trees, with the aid of the conversion script used in preparing the CoNLL-2000 shared task data.3 We next lemmatised the data using morph (Minnen et al.,"
W02-2001,W00-1308,0,0.0146913,"ools data as described above. As indicated in the first line of Table 1 (“Brill”), the simple POS-based method results in a precision of 1.000, recall of 0.177 and F-score of 0.301. In order to determine the upper bound on performance for this method, we ran the extraction method over the original tagging from the Penn Treebank. This resulted in an F-score of 0.774 (“Penn” in Table 1). The primary reason for the large disparity between the Brill tagger output and original Penn Treebank annotation is that it is notoriously difficult to differentiate between particles, prepositions and adverbs (Toutanova and Manning, 2000). Over the WSJ, the Brill tagger achieves a modest tag recall of 0.103 for particles, and tag precision of 0.838. That is, it is highly conservative in allocating particle tags, to the extent that it recognises only two particle types for the whole of the WSJ: out and down. 4 Method-2: Simple Chunk-based Extraction To overcome the shortcomings of the Brill tagger in identifying particles, we next look to full chunk 2 Note, this is the same as the maximum span length of 5 used by Smadja (1993), and above the maximum attested NP length of 3 from our corpus study (see Section 2.2). Prec 0.889 WSJ"
W02-2001,W00-1427,0,\N,Missing
W02-2001,W00-0735,0,\N,Missing
W03-1808,W03-1809,0,0.200543,"when VPCs are generated from these classes. More investigation is needed to verify whether the unattested combinations, specially in the lower ranked classes are invalid or simply did not occur in the dictionaries or in the corpus, because the problem of data sparseness is especially accute for VPCs. Moreover, it is also necessary to determine the precise semantics of these VPCs, even though we expect that the more productive classes generate VPCs compositionally, combining the semantics of the verb and particle together. Possible alternatives for dealing with this issue are discussed by both Bannard et al. (2003) and McCarthy et al. (2003). Furthermore, although there are some cases where it appears reasonable to treat VPCs as fully productive, there are also cases of semi-productivity (e.g. verbs denoting cooking processes and aspectual up: boil up and heat up, but not ?saut´e up), as discussed by Villavicencio and Copestake (2002), so it is important to determine which classes are fully productive and which are not. 6 Discussion We investigated the identification of regular patterns among verb-particle constructions using dictionaries, corpora and Levin’s classes. These results suggest that Levin’s"
W03-1808,copestake-flickinger-2000-open,0,0.0431856,"sis for this decision. 3 Dictionaries and VPCs Dictionaries are a major source of information about VPCs. In Table 1 we can see the coverage of phrasal verbs (PVs) in several dictionaries and lexicons: Collins Cobuild Dictionary of Phrasal Verbs (Collins-PV), Cambridge International Dictionary of Phrasal Verbs (CIDE-PV), the electronic versions of the Alvey Natural Language Tools (ANLT) lexicon (Carroll and Grover, 1989) (which was derived from the Longman Dictionary of Contemporary English, LDOCE), the Comlex lexicon (Macleod and Grishman, 1998), and the LinGO English Resource Grammar (ERG) (Copestake and Flickinger, 2000) version of November 2001. This table shows in the second column the number of PV entries for each of Top Particles 600 500 up out off down away VPCs 400 300 200 100 0 ANLT Comlex ERG A+C A+C+E Dictionaries Figure 1: Top Ranked Particles I these dictionaries, including not only VPCs but also other kinds of PV. The third column shows the number of VPC entries (available only for the electronic dictionaries). Table 1: Phrasal Verb Entries in Dictionaries Dictionary PVs VPCs ANLT 6,439 2,906 CIDE-PV over 4,500 Collins-PV over 3,000 Comlex 12,564 4,039 ERG 533 337 As we can see from these numbers,"
W03-1808,W03-1810,0,0.202915,"om these classes. More investigation is needed to verify whether the unattested combinations, specially in the lower ranked classes are invalid or simply did not occur in the dictionaries or in the corpus, because the problem of data sparseness is especially accute for VPCs. Moreover, it is also necessary to determine the precise semantics of these VPCs, even though we expect that the more productive classes generate VPCs compositionally, combining the semantics of the verb and particle together. Possible alternatives for dealing with this issue are discussed by both Bannard et al. (2003) and McCarthy et al. (2003). Furthermore, although there are some cases where it appears reasonable to treat VPCs as fully productive, there are also cases of semi-productivity (e.g. verbs denoting cooking processes and aspectual up: boil up and heat up, but not ?saut´e up), as discussed by Villavicencio and Copestake (2002), so it is important to determine which classes are fully productive and which are not. 6 Discussion We investigated the identification of regular patterns among verb-particle constructions using dictionaries, corpora and Levin’s classes. These results suggest that Levin’s classes provide us with pro"
W03-1808,W02-2001,1,\N,Missing
W04-0411,calzolari-etal-2002-towards,0,0.0555863,"the words in the MWE (e.g. eat up, where the particle up adds a completive sense to eat). Given the flexibility and variation in form of MWEs and the complex interrelations that may be found between their components, an encoding that treats them as invariant strings (a words with spaces approach), will not be adequate to fully describe any such expression appropriately with the exception of Computer Laboratory University of Cambridge William Gates Building, JJ Thomson Avenue Cambridge, CB3 0FD, UK aac10,bmw20,faml2 @cl.cam.ac.uk  the simplest fixed cases such as ad hoc ((Sag et al., 2002), (Calzolari et al., 2002)). Different strategies for encoding MWEs have been employed by different lexical resources with varying degrees of success, depending on the type of MWE. One case is the Alvey Tools Lexicon (Carroll and Grover, 1989), which has a good coverage of phrasal verbs, providing extensive information about their syntactic aspects (variation in word order, subcategorisation, etc), but it does not distinguish compositional from non-compositional entries neither does it specify entries that can be productively formed. WordNet, on the other hand, covers a large number of MWEs (Fellbaum, 1998), but does n"
W04-0411,copestake-flickinger-2000-open,1,0.702664,"te name like v rel. A type like transverb embodies the constraints defined for a given construction (in this case transitive verbs), in a particular grammar, and these vary from grammar to grammar. Thus, these words can be expanded into full feature structures during processing according to the constraints defined in a specific grammar. Table 1: LINGO ERG lexical database encoding identifier like tv 1 orthography like type trans-verb predicate like v rel This table shows a minimal encoding for simplex words, but it can serve as basis for a more complete one. That is the case of the LinGO ERG (Copestake and Flickinger, 2000) lexicon, which adopts for its database version, a compatible but more complex encoding which is successfully used to describe simplex words (Copestake et al., 2004). In the next sections, we investigate what would be necessary for extending this encoding for successfully capturing MWEs. 3 Idioms Idioms constitute a complex case of MWEs, allowing a great deal of variation. Some idioms are 1 The identifier and semantic relation names follow the standard adopted by the LinGO ERG (Copestake and Flickinger, 2000), while the grammatical type names are also compatible with it. very flexible and can"
W04-0411,copestake-etal-2002-multiword,1,0.863803,"variation in word order, subcategorisation, etc), but it does not distinguish compositional from non-compositional entries neither does it specify entries that can be productively formed. WordNet, on the other hand, covers a large number of MWEs (Fellbaum, 1998), but does not provide information about their variability. Neither of these resources covers idioms. The challenge in designing adequate lexical resources for MWEs, is to ensure that the variability and the extra dimensions required by the different types of MWE can be captured. Such a move is called for by Calzolari et al. (2002) and Copestake et al. (2002). Calzolari et al. (2002) discuss these problems while attempting to establish the standards for MWE description in the context of multilingual lexical resources. Their focus is on MWEs that are productive and that present regularities that can be generalised and applied to other classes of words that have similar properties. Copestake et al. (2002) present an initial schema for MWE description and we build on these ideas here, by proposing an architecture for a lexical encoding of MWEs, which allows for a unified treatment of different kinds of MWE. In what follows, we start by laying out the"
W04-0411,villavicencio-etal-2004-multilingual,1,0.819615,"MWE, in the MWE Type table. Different types of MWEs can be straightforwardly described using this encoding, as discussed in terms of idioms and VPCs. A database employing this encoding can be integrated with a particular grammar, providing the grammar system with a useful repertoire of MWEs. This is the case of the MWE grammar (Villavicencio, 2003) and of the wide-coverage LinGO ERG (Flickinger, 2004), both implemented on the framework of HPSG and successfully integrated with this database. This encoding is also used as basis of the architecture for a multilingual database of MWEs defined by Villavicencio et al. (2004), which has the added complexity of having to record the correspondences and differences in MWEs in different languages: different word orders, different lexical and syntactic constructions, etc. In terms of usage, this encoding means that the search facilities provided by the database can help the user investigate MWEs with particular properties. This in turn can be used to aid the addition of new MWEs to the database by analogy with existing MWEs with similar characteristics. 7 Acknowledgements This research was supported in part by the NTT/Stanford Research Collaboration, research project o"
W04-0411,W03-1808,1,0.922437,"seem to follow productive patterns (e.g. the resultative combinations walk/jump/run up/down/out/in/away/around/... from joining these verbs and the directional/locative particles up, down, out, in, away, around, ...). This is discussed in Fraser (1976), who notes that the semantic properties of verbs seem to affect their possibility of combination with particles. For productive VPCs, one possibility is then to use the entries of verbs already listed in a lexical resource to productively generate VPC entries by combining them with particles according to their semantic classes, as discussed by Villavicencio (2003). However, there are also cases of semi-productivity, since the possibilities of combinations are not fully predictable from a particular verb and particle (e.g. phone/ring/call/*telephone up). Thus, although some classes of VPCs can be productively generated from verb entries, to avoid overgeneration we adopt an approach where the remaining VPCs need to be explicitly licensed by the specification of the appropriate VPC entry. To sum up, for VPC entries an appropriate encoding needs to maintain the link between a VPC and the corresponding simplex form, from where the VPC inherits many of its c"
W04-0411,W98-0707,0,\N,Missing
W04-0411,copestake-etal-2004-lexicon,1,\N,Missing
W06-1206,baldwin-etal-2004-road,0,0.134886,"Missing"
W06-1206,W05-1008,0,0.0425203,"Missing"
W06-1206,P04-1057,0,0.0200567,"Missing"
W06-1206,1999.tc-1.8,0,0.0293775,"of van Noord (2004), the word sequences are used, mainly because the cost to compute and count the word sequences is minimum. The parsability of a sequence wi . . . wj is defined as: R(wi . . . wj ) = C(wi . . . wj , OK) C(wi . . . wj ) them. The output of the error mining phase proposes a set of n-grams, which also contain MWEs. Therefore, the task is to distinguish the MWEs from the other cases. To do this, first we propose to use the World Wide Web as a very large corpus from which we collect evidence that enables us to rule out noisy cases (due to spelling errors, for instance), following Grefenstette (1999), Keller et al. (2002), Kilgarriff and Grefenstette (2003) and Villavicencio (2005). The candidates that are kept can be semi-automatically included in the grammar, by employing a lexical type predictor, whose output we use in order to add lexical entries to the lexicon, with a possible manual check by a grammar writer. This procedure significantly speeds up the process of grammar development, relieving the grammar developer of some of the burden by automatically detecting parse failures and providing semi-automatic means for handling them. The paper starts with a discussion of MWEs and of som"
W06-1206,zhang-kordoni-2006-automated,1,0.83552,"rther subtypes. We will call the maximum lexical types after extension the atomic lexical types. Then the lexicon will be a multi-valued mapping from the word stems to the atomic lexical types. Needless to underline here that all we have exp( i θi fi (t, c)) P p(t|c) = P 0 t0 ∈T exp( i θi fi (t , c)) P (3) where feature fi (t, c) may encode arbitrary characteristics of the context. The parameters < θ1 , θ2 , . . . > can be evaluated by maximising the pseudo-likelihood on a training corpus (Malouf, 2002). The detailed design and feature selection for the lexical type predictor are described in Zhang and Kordoni (2006). 5 Lexical ambiguity is not considered here for the unknowns. In principle, this constraint can be relaxed by allowing the classifier to return more than one results by, setting a confidence threshold, for example. 42 In the experiment described here, we have used the latest version of the Redwoods Treebank in order to train the lexical type predictor with morphological features and context words/POS tags features 6 . We have then extracted from the BNC 6248 sentences, which contain at least one of the 311 MWE candidates verified with World Wide Web in the way described in the previous sectio"
W06-1206,W02-1030,0,0.0377286,"the word sequences are used, mainly because the cost to compute and count the word sequences is minimum. The parsability of a sequence wi . . . wj is defined as: R(wi . . . wj ) = C(wi . . . wj , OK) C(wi . . . wj ) them. The output of the error mining phase proposes a set of n-grams, which also contain MWEs. Therefore, the task is to distinguish the MWEs from the other cases. To do this, first we propose to use the World Wide Web as a very large corpus from which we collect evidence that enables us to rule out noisy cases (due to spelling errors, for instance), following Grefenstette (1999), Keller et al. (2002), Kilgarriff and Grefenstette (2003) and Villavicencio (2005). The candidates that are kept can be semi-automatically included in the grammar, by employing a lexical type predictor, whose output we use in order to add lexical entries to the lexicon, with a possible manual check by a grammar writer. This procedure significantly speeds up the process of grammar development, relieving the grammar developer of some of the burden by automatically detecting parse failures and providing semi-automatic means for handling them. The paper starts with a discussion of MWEs and of some of the characteristi"
W06-1206,J03-3001,0,0.157918,"used, mainly because the cost to compute and count the word sequences is minimum. The parsability of a sequence wi . . . wj is defined as: R(wi . . . wj ) = C(wi . . . wj , OK) C(wi . . . wj ) them. The output of the error mining phase proposes a set of n-grams, which also contain MWEs. Therefore, the task is to distinguish the MWEs from the other cases. To do this, first we propose to use the World Wide Web as a very large corpus from which we collect evidence that enables us to rule out noisy cases (due to spelling errors, for instance), following Grefenstette (1999), Keller et al. (2002), Kilgarriff and Grefenstette (2003) and Villavicencio (2005). The candidates that are kept can be semi-automatically included in the grammar, by employing a lexical type predictor, whose output we use in order to add lexical entries to the lexicon, with a possible manual check by a grammar writer. This procedure significantly speeds up the process of grammar development, relieving the grammar developer of some of the burden by automatically detecting parse failures and providing semi-automatic means for handling them. The paper starts with a discussion of MWEs and of some of the characteristics that make them so challenging for"
W06-1206,W02-2018,0,0.0498277,"hat corresponds to the occurrence of the word in the given context5 . We use a single classifier to predict the atomic lexical type. There are normally hundreds of atomic lexical types for a large grammar. So the classification model should be able to handle a large number of output classes. We choose the Maximum Entropy-based model because it can easily handle thousands of features and a large number of possible outputs. It also has the advantages of general feature representation and no independence assumption between features. With the efficient parameter estimation algorithms discussed by Malouf (2002), the training of the model is now very fast. For our prediction model, the probability of a lexical type t given an unknown word and its context c is: Atomic Lexical Types Lexicalist grammars are normally composed of a limited number of rules and a lexicon with rich linguistic features attached to each entry. Some grammar formalisms have a type inheriting system to encode various constraints, and a flat structure of the lexicon with each entry mapped onto one type in the inheritance hierarchy. The following discussion is based on Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1"
W06-1206,A00-2022,0,0.072191,"Missing"
W08-2107,W03-1810,0,0.635709,"ere no other complement is required, or it may occur as a transitive VPC which requires a further NP complement (e.g. in She gave up alcohol while she was pregnant ). Since in English particles tend to be homographs with prepositions (up, out, in), a verb followed by a preposition/particle and an NP can be ambiguous between a transitive VPC and a prepositional verb (e.g. rely on, in He relies on his wife for everything). Some criteria that characterise VPCs are discussed by Bolinger (1971):2 proposed using a variety of approaches such as statistical, substitutional, distributional, etc. (e.g. McCarthy et al. (2003), Bannard (2005) and Fazly and Stevenson (2006)). In particular, Fazly and Stevenson (2006) look at the correlation between syntactic fixedness (in terms of e.g. passivisation, choice of determiner type and pluralisation) and non-compositionality of verb-noun compounds such as shoot the breeze. In this work we investigate the automatic extraction of VPCs, looking into a variety of methods, combining linguistic with statistical information, ranging from frequencies to association measures: Mutual Information (MI), χ2 and Entropy. We also investigate the determination of compositionality of VPCs"
W08-2107,P04-1036,0,0.0291894,"t of the synonym synsets are related to secondary senses or very specific uses of a verb and are thus not correctly disambiguated. In what concerns the WNS sets, only the smallest and first synset were kept, suggesting again that it may not be a good idea to maximise the synonyms set and for future work, we intent to establish a threshold for a synset to be taken into account. In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Wordnet resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al. (2004)). On the other hand, the algorithm selected the The results confirm that the use of statistical and linguistic information to automatically identify verb-particle constructions presents a reasonable way of improving coverage of existing lexical resources in a very simple and straightforward manner. In terms of grammar engineering, the information about compositional candidates belonging to productive classes provides us with the basis for constructing a family of fine-grained redundancy rules for these classes. These rules are applied in a constrained way to verbs already in the lexicon, acco"
W08-2107,baldwin-etal-2004-road,0,0.0410734,"s (MWEs), like compound nouns (science fiction) and phrasal verbs (carry out) (e.g. Pearce (2002), Evert and Krenn (2005) and Zhang et al. (2006)). Some of them employ language and/or type dependent linguistic knowledge for the task, while others employ independent statistical methods, such as Mutual Information and Log-likelihood (e.g. Pearce (2002) and, Zhang et al. (2006)), or even a combination of them (e.g. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 See Baldwin et al. (2004) for a discussion of the effects of multiword expressions like VPCs on a parser’s performance. 49 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 49–56 Manchester, August 2008 (2002)). Idiomatic VPCs, on the other hand, cannot have their meaning determined by interpreting their components literally (e.g. get on, meaning to be on friendly terms with someone). The third class is that of aspectual VPCs, which have the particle providing the verb with an endpoint, suggesting that the action described by the verb is performed completely, thoroughly o"
W08-2107,pearce-2002-comparative,0,0.605132,"TALP Laboratory, Joseph Fourier University - Grenoble INP (France) ♠ Department of Computer Sciences, Bath University (UK) ♥ Institute of Physics, Federal University of Rio Grande do Sul (Brazil) {ceramisch,avillavicencio,lfsmoura}@inf.ufrgs.br, idiart@if.ufrgs.br Abstract Baldwin (2005) and Sharoff (2004)), as basis for helping to determine whether a given sequence of words is in fact an MWE. Although some research aims at developing methods for dealing with MWEs in general (e.g. Zhang et al. (2006), Ramisch et al. (2008)), there is also some work that deals with specific types of MWEs (e.g. Pearce (2002) on collocations and Villavicencio (2005) on verb-particle constructions (VPCs)) as each of these MWE types has distinct distributional and linguistic characteristics. VPCs are combinations of verbs and particles, such as take off in Our plane took off late, that due to their complex characteristics and flexible nature, provide a real challenge for NLP. In particular, there is a lack of adequate resources to identify and treat them, and those that are available provide only limited coverage, in face of the huge number of combinations in use. For tasks like parsing and generation, it is essenti"
W08-2107,W04-0403,0,0.0694786,"Missing"
W08-2107,copestake-flickinger-2000-open,0,0.0327811,"Missing"
W08-2107,W06-1206,1,0.818583,"eonardo Moura♣ and Marco Idiart♥ ♣ Institute of Informatics, Federal University of Rio Grande do Sul (Brazil) ♦ GETALP Laboratory, Joseph Fourier University - Grenoble INP (France) ♠ Department of Computer Sciences, Bath University (UK) ♥ Institute of Physics, Federal University of Rio Grande do Sul (Brazil) {ceramisch,avillavicencio,lfsmoura}@inf.ufrgs.br, idiart@if.ufrgs.br Abstract Baldwin (2005) and Sharoff (2004)), as basis for helping to determine whether a given sequence of words is in fact an MWE. Although some research aims at developing methods for dealing with MWEs in general (e.g. Zhang et al. (2006), Ramisch et al. (2008)), there is also some work that deals with specific types of MWEs (e.g. Pearce (2002) on collocations and Villavicencio (2005) on verb-particle constructions (VPCs)) as each of these MWE types has distinct distributional and linguistic characteristics. VPCs are combinations of verbs and particles, such as take off in Our plane took off late, that due to their complex characteristics and flexible nature, provide a real challenge for NLP. In particular, there is a lack of adequate resources to identify and treat them, and those that are available provide only limited cover"
W08-2107,E06-1043,0,0.0463223,"may occur as a transitive VPC which requires a further NP complement (e.g. in She gave up alcohol while she was pregnant ). Since in English particles tend to be homographs with prepositions (up, out, in), a verb followed by a preposition/particle and an NP can be ambiguous between a transitive VPC and a prepositional verb (e.g. rely on, in He relies on his wife for everything). Some criteria that characterise VPCs are discussed by Bolinger (1971):2 proposed using a variety of approaches such as statistical, substitutional, distributional, etc. (e.g. McCarthy et al. (2003), Bannard (2005) and Fazly and Stevenson (2006)). In particular, Fazly and Stevenson (2006) look at the correlation between syntactic fixedness (in terms of e.g. passivisation, choice of determiner type and pluralisation) and non-compositionality of verb-noun compounds such as shoot the breeze. In this work we investigate the automatic extraction of VPCs, looking into a variety of methods, combining linguistic with statistical information, ranging from frequencies to association measures: Mutual Information (MI), χ2 and Entropy. We also investigate the determination of compositionality of VPCs verifying whether the degree of semantic flexi"
W09-2901,W03-0409,0,0.0531132,"Missing"
W09-2901,baldwin-etal-2004-road,0,0.0284776,"Missing"
W09-2901,J96-2004,0,0.0134529,"Missing"
W09-2901,W08-2107,1,0.906598,"Missing"
W09-2901,W07-1102,0,0.150597,"Missing"
W09-2901,W02-2001,1,0.799703,"rom asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and Tiedemann, 2006; Caseli et al., 2009)). As basis for helping to determine whether a given sequence of words is in fact an MWE (e.g. ad hoc vs the small boy) some of these works employ linguistic knowledge for the task (Villavicencio, 2005), while others employ statistical methods (Pearce, 2002; Evert and Krenn, 2005; Zhang et al., 2006; Villavicencio et al., 2007) or combine them with some kinds of linguistic information such as syntactic and semantic properties (Baldwin and Villavicencio, 2002; Van de Cruys and na Villada Moir´on, 2007) or automatic word alignment (na Villada Moir´on and Tiedemann, 2006). Related Work The term Multiword Expression has been used to describe a large number of distinct but related phenomena, such as phrasal verbs (e.g. come along), nominal compounds (e.g. frying pan), institutionalised phrases (e.g. bread and butter), and many others (Sag et al., 2002). They are very frequent in everyday language and this is reflected in several existing grammars and lexical resources, where almost half of the entries are Multiword Expressions. However, due to their h"
W09-2901,J03-3005,0,0.0524543,". Section 5 presents the evaluation methodology and analyses the results and section 6 finishes this paper with some conclusions and proposals for future work. 2 A variety of approaches has been proposed for automatically identifying MWEs, differing basically in terms of the type of MWE and language to which they apply, and the sources of information they use. Although some work on MWEs is type independent (e.g. (Zhang et al., 2006; Villavicencio et al., 2007)), given the heterogeneity of MWEs much of the work looks instead at specific types of MWE like collocations (Pearce, 2002), compounds (Keller and Lapata, 2003) and VPCs (Baldwin, 2005; Villavicencio, 2005; Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart, 2008). Some of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and Tiedemann, 2006; Caseli et al., 2009)). As basis for helping to determine whether a given sequence of words is in fact an MWE (e.g. ad hoc vs the small boy) some"
W09-2901,W06-1206,1,0.936008,"g them. Section 3 presents the resources used while section 4 describes the methods proposed to extract MWEs as a statistically-driven byproduct of an automatic word alignment process. Section 5 presents the evaluation methodology and analyses the results and section 6 finishes this paper with some conclusions and proposals for future work. 2 A variety of approaches has been proposed for automatically identifying MWEs, differing basically in terms of the type of MWE and language to which they apply, and the sources of information they use. Although some work on MWEs is type independent (e.g. (Zhang et al., 2006; Villavicencio et al., 2007)), given the heterogeneity of MWEs much of the work looks instead at specific types of MWE like collocations (Pearce, 2002), compounds (Keller and Lapata, 2003) and VPCs (Baldwin, 2005; Villavicencio, 2005; Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart, 2008). Some of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e"
W09-2901,W97-0311,0,0.139171,"s the alignment just for ranking the MWE candidates which were extracted on the basis of association measures (log-likelihood and salience) and head dependence heuristic (in parsed data). Our approach, as described in details by Caseli et al. (2009), also follows to some extent that of Zhang et al. (2006), as missing lexical entries for MWEs and related constructions are detected via error mining methods, and this paper focuses on the extraction of generic MWEs as a byproduct of an automatic word alignment. Another related work is the automatic detection of noncompositional compounds (NCC) by Melamed (1997) in which NCCs are identified by analyzing statistical translation models trained in a huge corpus by a time-demanding process. Given this context, our approach proposes the use of alignment techniques for identifying MWEs, looking at sequences detected by the aligner as containing more than one word, which form the MWE candidates. As a result, sequences of two or more consecutive source words are treated as MWE candidates regardless of whether they are translated as one or more target words. 3 4 4.1 Statistically-Driven and Alignment-Based methods Statistically-Driven method Statistical measu"
W09-2901,W06-2405,0,0.22375,"Missing"
W09-2901,P00-1056,0,0.633914,"that a source sequence of words be aligned with a single target word. The method looks for the sequences of source words that are frequently joined together during the alignment despite the number of target words involved. These features indicate that the method priorizes precision in spite of recall. It is also important to say that although the sequences of source and target words resemble the phrases used in the phrase-based statistical machine translation (SMT), they are indeed a refinement of them. More specifically, although both approaches rely on word alignments performed by GIZA++2 (Och and Ney, 2000), in the alignment-based approach not all sequences of words are considered as phrases (and MWE candidates) but just those with an alignment n : m (n &gt;= 2) with a target sequence. To confirm this assumption a phrase-based SMT system was trained with the same corpus used in our experiments and the number of phrases extracted following both approaches were compared. While the SMT extracted 819,208 source phrases, our alignment-based approach (without applying any part-of-speech or frequency filter) extracted only 34,277. These results show that the alignmentbased approach refines in some way the"
W09-2901,pearce-2002-comparative,0,0.0642607,"tic word alignment process. Section 5 presents the evaluation methodology and analyses the results and section 6 finishes this paper with some conclusions and proposals for future work. 2 A variety of approaches has been proposed for automatically identifying MWEs, differing basically in terms of the type of MWE and language to which they apply, and the sources of information they use. Although some work on MWEs is type independent (e.g. (Zhang et al., 2006; Villavicencio et al., 2007)), given the heterogeneity of MWEs much of the work looks instead at specific types of MWE like collocations (Pearce, 2002), compounds (Keller and Lapata, 2003) and VPCs (Baldwin, 2005; Villavicencio, 2005; Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart, 2008). Some of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and Tiedemann, 2006; Caseli et al., 2009)). As basis for helping to determine whether a given sequence of words is in fact an MW"
W09-2901,W06-2403,0,0.107735,"terms of the type of MWE and language to which they apply, and the sources of information they use. Although some work on MWEs is type independent (e.g. (Zhang et al., 2006; Villavicencio et al., 2007)), given the heterogeneity of MWEs much of the work looks instead at specific types of MWE like collocations (Pearce, 2002), compounds (Keller and Lapata, 2003) and VPCs (Baldwin, 2005; Villavicencio, 2005; Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart, 2008). Some of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and Tiedemann, 2006; Caseli et al., 2009)). As basis for helping to determine whether a given sequence of words is in fact an MWE (e.g. ad hoc vs the small boy) some of these works employ linguistic knowledge for the task (Villavicencio, 2005), while others employ statistical methods (Pearce, 2002; Evert and Krenn, 2005; Zhang et al., 2006; Villavicencio et al., 2007) or combine them with some kinds of linguistic infor"
W09-2901,W07-1104,0,0.278284,"Missing"
W09-2901,D07-1110,1,\N,Missing
W10-0607,W07-0610,0,\N,Missing
W10-2303,W10-0607,1,0.355805,"Missing"
W11-0812,2009.mtsummit-btm.1,0,0.276847,"scuss related work, in §3 we present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominalizations (Teufel and Grefenstette, 1995; Dras, 1995"
W11-0812,1992.tmi-1.3,0,0.0203849,"ian Portuguese, focusing on the development of a lexical resource for NLP tasks, such as SRL. The remainder of this paper is organized as follows: in §2 we discuss related work, in §3 we present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs"
W11-0812,W10-1812,0,0.139354,"Missing"
W11-0812,W10-1810,0,0.110853,"e present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominalizations (Teufel and Grefenstette, 1995; Dras, 1995; Hwang et al., 2010)"
W11-0812,C10-3015,1,0.709696,"es from active sentences, both afﬁrmative and negative. Cases which present intervening material between the verb and the other element of the CP are not captured, but this is not a serious problem considering the size of our corpus, although it inﬂuences the frequencies used in candidate selection. In order to facilitate human analysis of candidate lists, we used the mwetoolkit4 : a tool that has been developed speciﬁcally to extract MWEs from corpora, which encompasses candidate extraction through pattern matching, candidate ﬁltering (e.g. through association measures) and evaluation tools (Ramisch et al., 2010). After generating separate lists of candidates for each pattern, we ﬁltered out all those occurring less than 10 times in the corpus. The entries resulting of automatic identiﬁcation were classiﬁed by their frequency and their annotation is discussed in the following section. 4 Discussion Each pattern of POS tags returned a large number of candidates. Our expectation was to identify CPs among the most frequent candidates. First we annotated “interesting” candidates and then, in a deep analysis, we judged their idiomaticity. In the Table 1, we show the total number of candidates extracted befo"
W11-0812,C90-3043,0,0.541419,"er is organized as follows: in §2 we discuss related work, in §3 we present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominal"
W11-0812,W04-0401,0,0.0631246,"as follows: in §2 we discuss related work, in §3 we present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominalizations (Teufel and Gre"
W11-0812,E95-1014,0,0.786068,"n et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominalizations (Teufel and Grefenstette, 1995; Dras, 1995; Hwang et al., 2010). These approaches are not adopted here because our goal is precisely to identify which are the verbs, the nouns and other lexical elements that take part in CPs. Similar motivation to study LVCs/SVCs (for SRL) is found within the scope of Framenet (Atkins et al., 2003) and Propbank (Hwang et al., 2010). These projects have taken different decisions on how to annotate such constructions. Framenet annotates the head of the construction (noun or adjective) as argument taker (or frame evoker) and the light verb separately; Propbank, on its turn, ﬁrst annotates sep"
W11-0815,W02-2001,1,0.876989,"emented in a lexicographic tool called Xtract. More recently there has been the release of the mwetoolkit (Ramisch et al., 2010) for the automatic extraction of MWEs from monolingual corpora, that both generates and validates MWE candidates. As generation is based on surface forms, for the validation, a series of criteria for removing noise are provided, including some (language independent) association measures such as mutual information, dice coefficient and maximum likelihood. Several other researchers have proposed a number of computational techniques that deal with the discovery of MWEs: Baldwin and Villavicencio (2002) for verb-particle constructions, Pearce (2002) and Evert and Krenn (2005) for collocations, Nicholson and Baldwin (2006) for compound nouns and many others. For our experiments, we used some standard statistical measures such as mutual information, pointwise mutual information, chi-square, permutation entropy (Zhang et al., 2006), dice coefficient, and t-test to extract MWEs from a collection of documents (i.e. we consider the collection of documents indexed by the IR system as our corpus). 3 Materials and Methods Based on the hypothesis that the MWEs can improve the results of IR systems, we"
W11-0815,W03-1812,0,0.28345,"ment of Multiword Expressions applied to Information Retrieval Otavio Costa Acosta, Aline Villavicencio, Viviane P. Moreira Institute of Informatics Federal University of Rio Grande do Sul (Brazil) {ocacosta,avillavicencio,viviane}@inf.ufrgs.br Abstract a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die). For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003). It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words (Jackendoff, 1997). However, these ratios are probably underestimated when considering domain-specific language, in which the specialized vocabulary and terminology are composed mostly by MWEs. The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions. A MWE typically expresses concepts and ideas that usually cannot be expressed by a sing"
W11-0815,calzolari-etal-2002-towards,0,0.779906,"as a single unit. Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit. 1 Introduction One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction. The term “multiword expression” has been used to describe a large set of distinct constructions, for instance support verbs, noun compounds, institutionalized phrases and so on. Calzolari et al. (2002) defines MWEs as a sequence of words that acts as a single unit at some level of linguistic analysis. The nature of MWEs can be quite heterogeneous and each of the different classes has specific characteristics, posing a challenge to the implementation of mechanisms that provide unified treatment for them. For instance, even if a standard system capable of identifying boundaries between words, i.e. In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system. IR systems aim to provide users with quick access to data th"
W11-0815,pearce-2002-comparative,0,0.296146,"has been the release of the mwetoolkit (Ramisch et al., 2010) for the automatic extraction of MWEs from monolingual corpora, that both generates and validates MWE candidates. As generation is based on surface forms, for the validation, a series of criteria for removing noise are provided, including some (language independent) association measures such as mutual information, dice coefficient and maximum likelihood. Several other researchers have proposed a number of computational techniques that deal with the discovery of MWEs: Baldwin and Villavicencio (2002) for verb-particle constructions, Pearce (2002) and Evert and Krenn (2005) for collocations, Nicholson and Baldwin (2006) for compound nouns and many others. For our experiments, we used some standard statistical measures such as mutual information, pointwise mutual information, chi-square, permutation entropy (Zhang et al., 2006), dice coefficient, and t-test to extract MWEs from a collection of documents (i.e. we consider the collection of documents indexed by the IR system as our corpus). 3 Materials and Methods Based on the hypothesis that the MWEs can improve the results of IR systems, we carried out an evaluation experiment. The goal"
W11-0815,C10-3015,1,0.868422,"many researchers in NLP over the past years. With the recent increase in efficiency and accuracy of techniques for preprocessing texts, such as tagging and parsing, these can become an aid in improving the performance of MWE detection techniques. In terms of practical MWE identification systems, a well known 102 approach is that of Smadja (1993), who uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora. This approach is implemented in a lexicographic tool called Xtract. More recently there has been the release of the mwetoolkit (Ramisch et al., 2010) for the automatic extraction of MWEs from monolingual corpora, that both generates and validates MWE candidates. As generation is based on surface forms, for the validation, a series of criteria for removing noise are provided, including some (language independent) association measures such as mutual information, dice coefficient and maximum likelihood. Several other researchers have proposed a number of computational techniques that deal with the discovery of MWEs: Baldwin and Villavicencio (2002) for verb-particle constructions, Pearce (2002) and Evert and Krenn (2005) for collocations, Nic"
W11-0815,J93-1007,0,0.692417,"of a source language are often not equivalent to their word-by-word translation in the target language (e.g. guarda-chuva in Portuguese as umbrella in English and not as ?store rain). The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years. With the recent increase in efficiency and accuracy of techniques for preprocessing texts, such as tagging and parsing, these can become an aid in improving the performance of MWE detection techniques. In terms of practical MWE identification systems, a well known 102 approach is that of Smadja (1993), who uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora. This approach is implemented in a lexicographic tool called Xtract. More recently there has been the release of the mwetoolkit (Ramisch et al., 2010) for the automatic extraction of MWEs from monolingual corpora, that both generates and validates MWE candidates. As generation is based on surface forms, for the validation, a series of criteria for removing noise are provided, including some (language independent) association measures such as mutual information, dice coeffi"
W11-0815,W06-1206,1,0.914919,"Missing"
W11-0815,W06-1208,0,\N,Missing
W11-0822,C10-3015,1,0.595171,"ee and Pedersen, 2003), and filtering like in UCS (Evert, 2004). The pattern matching and n-gram counting steps are the focus of the improvements described in this paper. We present an experimental environment for computer-assisted extraction of Multiword Expressions (MWEs) from corpora. Candidate extraction works in two steps: generation and filtering. We focus on recent improvements in the former, for which we increased speed and flexibility. We present examples that show the potential gains for users and applications. 2 1 Project Description The mwetoolkit was presented and demonstrated in Ramisch et al. (2010b) and in Ramisch et al. (2010a), and applied to several languages (Linardaki et al., 2010) and domains (Ramisch et al., 2010c). It is a downloadable open-source1 set of commandline tools mostly written in Python. Our target users are researchers with a background in computational linguistics. The system performs language- and type-independent candidate extraction in two steps2 : 1. Candidate generation • Pattern matching3 • n-gram counting 2. Candidate filtering • Thresholds, stopwords and patterns • Association measures, classifiers An Example Our toy corpus, consisting of the first 20K sent"
W11-0822,ramisch-etal-2010-mwetoolkit,1,0.704795,"ee and Pedersen, 2003), and filtering like in UCS (Evert, 2004). The pattern matching and n-gram counting steps are the focus of the improvements described in this paper. We present an experimental environment for computer-assisted extraction of Multiword Expressions (MWEs) from corpora. Candidate extraction works in two steps: generation and filtering. We focus on recent improvements in the former, for which we increased speed and flexibility. We present examples that show the potential gains for users and applications. 2 1 Project Description The mwetoolkit was presented and demonstrated in Ramisch et al. (2010b) and in Ramisch et al. (2010a), and applied to several languages (Linardaki et al., 2010) and domains (Ramisch et al., 2010c). It is a downloadable open-source1 set of commandline tools mostly written in Python. Our target users are researchers with a background in computational linguistics. The system performs language- and type-independent candidate extraction in two steps2 : 1. Candidate generation • Pattern matching3 • n-gram counting 2. Candidate filtering • Thresholds, stopwords and patterns • Association measures, classifiers An Example Our toy corpus, consisting of the first 20K sent"
W11-0822,C10-2120,1,0.86255,"ee and Pedersen, 2003), and filtering like in UCS (Evert, 2004). The pattern matching and n-gram counting steps are the focus of the improvements described in this paper. We present an experimental environment for computer-assisted extraction of Multiword Expressions (MWEs) from corpora. Candidate extraction works in two steps: generation and filtering. We focus on recent improvements in the former, for which we increased speed and flexibility. We present examples that show the potential gains for users and applications. 2 1 Project Description The mwetoolkit was presented and demonstrated in Ramisch et al. (2010b) and in Ramisch et al. (2010a), and applied to several languages (Linardaki et al., 2010) and domains (Ramisch et al., 2010c). It is a downloadable open-source1 set of commandline tools mostly written in Python. Our target users are researchers with a background in computational linguistics. The system performs language- and type-independent candidate extraction in two steps2 : 1. Candidate generation • Pattern matching3 • n-gram counting 2. Candidate filtering • Thresholds, stopwords and patterns • Association measures, classifiers An Example Our toy corpus, consisting of the first 20K sent"
W11-4511,J90-2002,0,0.0863271,"ressions. In this paper we propose an approach for lexical alignment combining statistical and linguistic information. We describe the development of a baseline discriminative aligner and a set of language dependent post-processing functions that allow the inclusion of shallow linguistic knowledge. The post-processing functions were designed to significantly improve word alignment mainly on verb-particle constructs both over our baseline and over Giza++. 1. Introduction Automatic lexical (word) alignment is a previous step necessary for the creation of an empirical machine translation system [Brown et al. 1990, Somers 1999]. Given a sentencealigned parallel corpus, the lexical alignment process consists of producing a set of relations (alignments) between the lexical units of both languages. The lexical units are usually simple words, but may also be multiword expressions (MWEs), which can be defined as “idiosyncratic interpretations that cross word boundaries (or spaces)” [Sag et al. 2002]. The task of lexical alignment has traditionally been done in a completely statistical, unsupervised manner, whereby a generative aligner tries to infer the parameters of a model of the statistical process by wh"
W11-4511,J93-2003,0,0.0322795,"l corpus, the lexical alignment process consists of producing a set of relations (alignments) between the lexical units of both languages. The lexical units are usually simple words, but may also be multiword expressions (MWEs), which can be defined as “idiosyncratic interpretations that cross word boundaries (or spaces)” [Sag et al. 2002]. The task of lexical alignment has traditionally been done in a completely statistical, unsupervised manner, whereby a generative aligner tries to infer the parameters of a model of the statistical process by which a source sentence generates a target one. [Brown et al. 1993] describe a set of increasingly complex generative processes (the IBM models) and algorithms to estimate its parameters. The difficulty in adding complex linguistic knowledge to generative models has given rise to several so-called discriminative lexical aligners such as in [Moore 2005] and in [Niehues and Vogel 2008]. In a discriminative aligner it is not necessary to model a complex statistical problem, instead a set of feature functions are created each one capturing a specific facet of a word alignment. Thus, the features are combined (usually linearly) and it is then necessary to search"
W11-4511,J96-2004,0,0.00677343,"s us to have a good number of VPC alignments, without having to annotate a prohibitively large portion of the corpus. VPCs in the lexical alignments were explicitly marked, both in the complete and in the partial corpus. The annotation was made using YAWAT [Germann 2008] by 2 Portuguese native speakers fluent in English, following a set of guidelines based on [Caseli et al. 2005]. Any verb in English followed by a particle or subordinating conjunction which had to be aligned as a unit was considered a VPC. In order to measure our inter-annotator agreement, we calculated the kappa (κ) measure [Carletta 1996], and obtained a value of 0.78. According to [Carletta 1996], among other authors, a value of κ between 0.67 and 0.80 indicates a good agreement between annotators. These two corpora were further divided into tuning and test sets. The tuning set of alignments was used during the development and parameter tuning of the aligner, while the held out test set was used in the evaluation. These four different annotated corpora are summarized in table 1b. For the partial corpora, the number of sentences is not equal to the number of sentences actually annotated, because only those containing VPCs wer"
W11-4511,deksne-etal-2008-dictionary,0,0.0211591,"structs (VPCs), which are combinations of verb and particle such as turn up and made up. In terms of syntax, VPCs can have complex subcategorization frames, such as transitive VPCs, which take a NP argument between the verb and the particle, e.g. He made the whole story up. The semantics of VPCs ranges from more transparent (e.g. clear up where the particle introduces a sense of completion) to more opaque cases (e.g. make out as kiss). An adequate processing of MWEs is important for precise machine translation and can benefit from being taken into account during the task of lexical alignment [Deksne et al. 2008]. Therefore, in this paper we apply machine learning techniques to train post-processing heuristics that can be used to enrich a baseline alignment. We show how this approach can be applied to identify VPCs in the task of lexical alignment and, by providing a more precise alignment of VPCs, improve the general performance over the baseline. This paper is structured as follows: in section 2 we describe the resources that were used and in section 3 the alignment methods developed. Section 4 discusses the results obtained, and finally in section 5 we present our conclusions and suggest direction"
W11-4511,P06-1097,0,0.184478,"ll possible alignments 97 Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, pages 97–106, c Cuiab´ a, MT, Brazil, October 24–26, 2011. 2011 Sociedade Brasileira de Computa¸c˜ ao for the one which has the highest score. For example, [Moore 2005] linearly combines a small number of features, while [Liu et al. 2005] propose a very similar technique, but using a log-linear combination of the individual features. With respect to parameter training, techniques range from voted perceptron [Moore 2005] to conditional random field [Niehues and Vogel 2008], while [Fraser and Marcu 2006] go even further and propose a semi-supervised parameter optimization for their discriminative aligner. The search for the best alignment is made with a modified hillclimbing method in [Fraser and Marcu 2006], while [Liu et al. 2005] use a greedy algorithm. One subject which has drawn comparatively little attention is the correct alignment of MWEs. MWEs are a significant part of the lexicon of a speaker, perhaps as numerous as the single words [Jackendoff 2002], and various techniques to identify and process them have been proposed using different kinds of information, from syntax [Baldwin 20"
W11-4511,P08-4006,0,0.0130996,"lected a subset of the Opus corpus following two types of annotation that gave rise to two different corpora: • the complete corpus contains annotations for all the correspondences (pairs of words or MWEs alignments) • the partial corpus contains annotations only for the correspondences involving VPCs This two-fold annotation approach allows us to have a good number of VPC alignments, without having to annotate a prohibitively large portion of the corpus. VPCs in the lexical alignments were explicitly marked, both in the complete and in the partial corpus. The annotation was made using YAWAT [Germann 2008] by 2 Portuguese native speakers fluent in English, following a set of guidelines based on [Caseli et al. 2005]. Any verb in English followed by a particle or subordinating conjunction which had to be aligned as a unit was considered a VPC. In order to measure our inter-annotator agreement, we calculated the kappa (κ) measure [Carletta 1996], and obtained a value of 0.78. According to [Carletta 1996], among other authors, a value of κ between 0.67 and 0.80 indicates a good agreement between annotators. These two corpora were further divided into tuning and test sets. The tuning set of alignme"
W11-4511,P05-1057,0,0.0203221,"t is not necessary to model a complex statistical problem, instead a set of feature functions are created each one capturing a specific facet of a word alignment. Thus, the features are combined (usually linearly) and it is then necessary to search the space of all possible alignments 97 Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, pages 97–106, c Cuiab´ a, MT, Brazil, October 24–26, 2011. 2011 Sociedade Brasileira de Computa¸c˜ ao for the one which has the highest score. For example, [Moore 2005] linearly combines a small number of features, while [Liu et al. 2005] propose a very similar technique, but using a log-linear combination of the individual features. With respect to parameter training, techniques range from voted perceptron [Moore 2005] to conditional random field [Niehues and Vogel 2008], while [Fraser and Marcu 2006] go even further and propose a semi-supervised parameter optimization for their discriminative aligner. The search for the best alignment is made with a modified hillclimbing method in [Fraser and Marcu 2006], while [Liu et al. 2005] use a greedy algorithm. One subject which has drawn comparatively little attention is the correc"
W11-4511,H05-1011,0,0.132385,"word boundaries (or spaces)” [Sag et al. 2002]. The task of lexical alignment has traditionally been done in a completely statistical, unsupervised manner, whereby a generative aligner tries to infer the parameters of a model of the statistical process by which a source sentence generates a target one. [Brown et al. 1993] describe a set of increasingly complex generative processes (the IBM models) and algorithms to estimate its parameters. The difficulty in adding complex linguistic knowledge to generative models has given rise to several so-called discriminative lexical aligners such as in [Moore 2005] and in [Niehues and Vogel 2008]. In a discriminative aligner it is not necessary to model a complex statistical problem, instead a set of feature functions are created each one capturing a specific facet of a word alignment. Thus, the features are combined (usually linearly) and it is then necessary to search the space of all possible alignments 97 Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, pages 97–106, c Cuiab´ a, MT, Brazil, October 24–26, 2011. 2011 Sociedade Brasileira de Computa¸c˜ ao for the one which has the highest score. For example, [M"
W11-4511,W08-0303,0,0.0199634,"spaces)” [Sag et al. 2002]. The task of lexical alignment has traditionally been done in a completely statistical, unsupervised manner, whereby a generative aligner tries to infer the parameters of a model of the statistical process by which a source sentence generates a target one. [Brown et al. 1993] describe a set of increasingly complex generative processes (the IBM models) and algorithms to estimate its parameters. The difficulty in adding complex linguistic knowledge to generative models has given rise to several so-called discriminative lexical aligners such as in [Moore 2005] and in [Niehues and Vogel 2008]. In a discriminative aligner it is not necessary to model a complex statistical problem, instead a set of feature functions are created each one capturing a specific facet of a word alignment. Thus, the features are combined (usually linearly) and it is then necessary to search the space of all possible alignments 97 Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, pages 97–106, c Cuiab´ a, MT, Brazil, October 24–26, 2011. 2011 Sociedade Brasileira de Computa¸c˜ ao for the one which has the highest score. For example, [Moore 2005] linearly combines a s"
W11-4511,J03-1002,0,0.0264481,"lel corpus used in our experiments was the Opus subtitle corpus [Tiedemann 2009]. This corpus was built using freely available movie subtitles found in the Web, which were automatically sentence aligned.1 For the experiments reported in this paper, we used the English-Portuguese portion of the Opus corpus after some preprocessing steps to remove tags and to tokenize words. Table 1a presents the total amount of sentences and tokens in each language, where en stands for English and pt for Portuguese. The parameter estimation of Giza++ was performed based on the complete unannotated Opus corpus [Och and Ney 2003], since it is an unsupervised process. A manually annotated corpus was also needed to both estimate parameters of our discriminative 1 The automatic sentence alignment was not manually corrected. 98 Lang en pt # Sentences 351,106 351,106 # Tokens 3,077,113 2,605,376 (a) The opus en-pt subtitle corpus Corpus Complete tune Partial tune Complete test Partial test # Sent 600 900 500 600 # Alignments 4019 206 4395 142 # VPC 78 103 107 71 (b) The corpora use in the exeriments Table 1. Corpora aligner and also to evaluate it. This gold standard corpus was built from a selected a subset of the Opus c"
W11-4511,W06-1204,0,0.0169088,"w that it is possible to significantly improve alignment in all the relevant metrics in the test set. We also show that machine learning techniques can be very efficient in inducing the post-processing rules considered in this work. Since dealing with MWEs is an important open problem in natural language processing, the approach proposed in this paper shows that it is possible to improve the alignment of at least some classes of MWEs using simple and effective shallow linguistic knowledge. To the best of our knowledge, existing efforts to deal with MWEs in the lexical alignment task (e.g. in [Venkatapathy and Joshi 2006]) have used only statistical information, while we combine statistical information generated by Giza++ with shallow linguistic knowledge using machine learning. For future work, we intend to investigate the application of the same technique on other types of multiword expressions, such as light verbs or compound nouns. We also intend to add some statistical association measures to the feature vectors, in order to gauge if a combination of grammatical knowledge and statistical information can improve the results. 6. Acknowledgements Acknowledgements go to CNPq, for having financed this researc"
W11-4511,zhang-etal-2004-interpreting,0,0.0324836,"Missing"
W12-0905,P06-4020,0,0.0649719,"Missing"
W12-0910,W07-0605,0,0.0494071,"Missing"
W12-0910,P06-4020,0,\N,Missing
W12-0911,calzolari-etal-2002-towards,0,0.0602377,"fficulties may arise as the interpretation of these expressions often demands more knowledge than just about (1) unitary words and (2) word-to-word relations. This introduces a distinction between what a learner is able to computationally disambiguate or figure out automatically from language and what must be explicitly stored/memorized and retrieved whole from memory at the time of There has been considerable discussion about the challenges imposed by Multiword Expressions (MWEs) which in addition to crossing word boundaries act as a single lexical unit at some levels of linguistic analysis (Calzolari et al., 2002; Sag et al., 2002; Fillmore, 2003). They include a wide range of grammatical constructions such as verb-particle constructions (VPCs), idioms, compound nouns and listable word configurations, 43 Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 43–50, c Avignon, France, April 24 2012. 2012 Association for Computational Linguistics tion 3 presents the resources and methods used in this paper. The analyses of VPCs in children and adults sentences are in section 4. We finish with conclusions and possibilities of future works. use, rather than b"
W12-0911,pearce-2002-comparative,0,0.129259,"nd an arm and a leg for a nominal egg (Fillmore, 2003). For second language (L2) learners in particular (Wray, 2002) MWEs are indeed a well-known cause of problems and less likely to be used by them than by native speakers in informal spoken contexts (Siyanova and Schmitt, 2007). Even if L2 learners may be capable of producing a large number of MWEs, their underlying intuitions and fluency do not match those of native speakers (Siyanova and Schmitt, 2008) and they may produce marked combinations that are not conventionally used together (e.g. plastic surgery/?operation, strong/?powerful tea) (Pearce, 2002; Siyanova and Schmitt, 2007). Given the potential additional sources of complexity of MWEs for learning, in this paper we investigate whether children shy away from using them when they communicate. We focus on a particular type of MWEs, VPCs, which present a wide range of syntactic and semantic idyosincrasies examining whether children produce proportionally less VPCs than adults. In addition, we analyze whether any potential added processing costs for VPCs are reflected in a reduced choice of VPCs or verbs to form these combinations in child-produced sentences compared to adult usage. Final"
W12-0911,ramisch-etal-2010-mwetoolkit,1,0.868102,"Missing"
W12-0911,villavicencio-etal-2012-large,1,0.291982,"ed to produce even more object dropping errors for VPCs than children with typ3 Materials and Methods For this work we use the English corpora from the CHILDES database (MacWhinney, 1995) containing transcriptions of child-produced and child-directed speech from interactions involving children of different age groups and in a variety of settings, from naturalistic longitudinal studies to task oriented latitudinal cases. These corpora are available in raw, part-of-speech-tagged, lemmatized and parsed formats (Sagae et al., 2010). Moreover the English CHILDES Verb Construction Database (ECVCD) (Villavicencio et al., 2012) also adds for each sentence the RASP parsing and grammatical relations (Briscoe and Carroll, 2006), verb semantic classes (Levin, 1993), age of acquisition, familiarity, frequency (Coltheart, 1981) and other psycholinguistic and distributional characteristics. These annotated sentences are divided into two groups according to the speaker annotation available in CHILDES, the Adults Set and the Children Set contain respectively all the sentences spoken by adults and by children1 , as shown in table 1 as Parsed. VPCs in these corpora are detected by looking in the RASP annotation for all occurre"
W12-0911,P06-2006,0,\N,Missing
W12-3301,N10-1029,0,0.0915846,"evaluate and analyse the lexical AMs used in MWE extraction on small samples of bigram candidates. Pearce (2002), systematically evaluates a set of techniques for MWE extraction on a small test set of English collocations. Analogously, Pecina (2005) and Ramisch et al. (2008) present extensive comparisons of individual AMs and of their combination for MWE extraction in Czech, German and English. There have also been efforts for the extrinsic evaluation of MWEs for NLP applications such as information retrieval (Xu et al., 2010), word sense disambiguation (Finlayson and Kulkarni, 2011) and MT (Carpuat and Diab, 2010). One recent initiative aiming at more comparable eval2 We consider only freely available, downloadable and openly documented tools. Therefore, outside the scope of this work are proprietary tools, terminology and lexicography tools, translation aid tools and published techniques for which no available implementation is provided. 1 Proceedings of the 2012 Student Research Workshop, pages 1–6, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics uations of MWE acquisition approaches was in the form of a shared task (Grégoire et al., 2008). However, the prese"
W12-3301,W11-0805,0,0.0793016,"(2005) and Seretan (2008) specifically evaluate and analyse the lexical AMs used in MWE extraction on small samples of bigram candidates. Pearce (2002), systematically evaluates a set of techniques for MWE extraction on a small test set of English collocations. Analogously, Pecina (2005) and Ramisch et al. (2008) present extensive comparisons of individual AMs and of their combination for MWE extraction in Czech, German and English. There have also been efforts for the extrinsic evaluation of MWEs for NLP applications such as information retrieval (Xu et al., 2010), word sense disambiguation (Finlayson and Kulkarni, 2011) and MT (Carpuat and Diab, 2010). One recent initiative aiming at more comparable eval2 We consider only freely available, downloadable and openly documented tools. Therefore, outside the scope of this work are proprietary tools, terminology and lexicography tools, translation aid tools and published techniques for which no available implementation is provided. 1 Proceedings of the 2012 Student Research Workshop, pages 1–6, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics uations of MWE acquisition approaches was in the form of a shared task (Grégoire e"
W12-3301,W11-0818,0,0.114271,"Missing"
W12-3301,pearce-2002-comparative,0,0.168553,"l., 2002). Existing approaches are either generic but present relatively low pre1 The equivalent expressions in French would be raining ropes, in German raining young dogs, in Portuguese raining Swiss knives, etc. MWE Acquisition Approaches Efforts for the evaluation of MWE acquisition approaches usually focus on a single technique or compare the quality of association measures (AMs) used to rank a fixed annotated list of MWEs. For instance, Evert and Krenn (2005) and Seretan (2008) specifically evaluate and analyse the lexical AMs used in MWE extraction on small samples of bigram candidates. Pearce (2002), systematically evaluates a set of techniques for MWE extraction on a small test set of English collocations. Analogously, Pecina (2005) and Ramisch et al. (2008) present extensive comparisons of individual AMs and of their combination for MWE extraction in Czech, German and English. There have also been efforts for the extrinsic evaluation of MWEs for NLP applications such as information retrieval (Xu et al., 2010), word sense disambiguation (Finlayson and Kulkarni, 2011) and MT (Carpuat and Diab, 2010). One recent initiative aiming at more comparable eval2 We consider only freely available,"
W12-3301,P05-2003,0,0.560253,"opes, in German raining young dogs, in Portuguese raining Swiss knives, etc. MWE Acquisition Approaches Efforts for the evaluation of MWE acquisition approaches usually focus on a single technique or compare the quality of association measures (AMs) used to rank a fixed annotated list of MWEs. For instance, Evert and Krenn (2005) and Seretan (2008) specifically evaluate and analyse the lexical AMs used in MWE extraction on small samples of bigram candidates. Pearce (2002), systematically evaluates a set of techniques for MWE extraction on a small test set of English collocations. Analogously, Pecina (2005) and Ramisch et al. (2008) present extensive comparisons of individual AMs and of their combination for MWE extraction in Czech, German and English. There have also been efforts for the extrinsic evaluation of MWEs for NLP applications such as information retrieval (Xu et al., 2010), word sense disambiguation (Finlayson and Kulkarni, 2011) and MT (Carpuat and Diab, 2010). One recent initiative aiming at more comparable eval2 We consider only freely available, downloadable and openly documented tools. Therefore, outside the scope of this work are proprietary tools, terminology and lexicography"
W12-3301,W11-0821,0,0.0674415,"ge 5,000 133,859 145,888 50,000 1,355,482 1,483,428 500,000 13,164,654 14,584,617 Table 1: Number of sentences and of words of each fragment of the Europarl corpus in fr and in en. of statistical AMs. It is an integrated framework for MWE treatment, providing from corpus preprocessing facilities to the automatic evaluation of the resulting list with respect to a reference. Its input is a corpus annotated with POS, lemmas and dependency syntax, or if these are not available, raw text. 3. Ngram Statistics Package7 (NSP) is a traditional approach for the statistical analysis of n-grams in texts (Pedersen et al., 2011). It provides tools for counting n-grams and calculating AMs, where an ngram is a sequence of n words occurring either contiguously or within a window of w words in a sentence. While most of the measures are only applicable to bigrams, some of them are also extended to trigrams and 4-grams. The set of available AMs includes robust and theoretically sound measures such as log-likelihood and Fischer’s exact test. Although there is no direct support to linguistic information such as POS, it is possible to simulate them to some extent using the same workaround as for LocMax. 4. UCS toolkit8 provid"
W12-3301,C10-3015,1,0.654588,"information in order to target a specific type of construction.4 The evaluation includes both LocalMaxs Strict which prioritizes high precision (henceforth LocMax-S) and LocalMaxs Relaxed which focuses on high recall (henceforth LocMax-R). A variation of the original algorithm, SENTA, has been proposed to deal with noncontiguous expressions (da Silva et al., 1999). However, it is computationally costly5 and there is no freely available implementation. 2. MWE toolkit6 (mwetk) is an environment for type and language-independent MWE acquisition, integrating linguistic and frequency information (Ramisch et al., 2010). It generates a targeted list of MWE candidates extracted and filtered according to user-defined criteria like POS sequences and a set 3 http://hlt.di.fct.unl.pt/luis/multiwords/ index.html 4 Although this can be simulated by concatenating words and POS tags together in order to form a token. 5 It is based on the calculation of all possible n-grams in a sentence, which explode in number when going from contiguous to noncontiguous n-grams. 6 http://mwetoolkit.sourceforge.net 2 # sentences # en words # fr words Small Medium Large 5,000 133,859 145,888 50,000 1,355,482 1,483,428 500,000 13,164,6"
W12-3301,W10-3708,0,0.0682433,"list of MWEs. For instance, Evert and Krenn (2005) and Seretan (2008) specifically evaluate and analyse the lexical AMs used in MWE extraction on small samples of bigram candidates. Pearce (2002), systematically evaluates a set of techniques for MWE extraction on a small test set of English collocations. Analogously, Pecina (2005) and Ramisch et al. (2008) present extensive comparisons of individual AMs and of their combination for MWE extraction in Czech, German and English. There have also been efforts for the extrinsic evaluation of MWEs for NLP applications such as information retrieval (Xu et al., 2010), word sense disambiguation (Finlayson and Kulkarni, 2011) and MT (Carpuat and Diab, 2010). One recent initiative aiming at more comparable eval2 We consider only freely available, downloadable and openly documented tools. Therefore, outside the scope of this work are proprietary tools, terminology and lexicography tools, translation aid tools and published techniques for which no available implementation is provided. 1 Proceedings of the 2012 Student Research Workshop, pages 1–6, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics uations of MWE acquisiti"
W12-3301,W10-3700,0,\N,Missing
W12-3904,baroni-bernardini-2004-bootcat,0,0.0329397,"set of seven ontologies that cover the different aspects of the domain of organizing scientific conferences. We have used this dataset as the basis for generating our corpora. 3 Methodology The main contribution of this paper is the proposal of the methodology to build corpora. This section describes the proposed methodology presenting our own corpus crawler, but also its application to construct three corpora, in English, Portuguese, and French. These corpora are constructed from the MultiFarm dataset. 3.1 Tools and Resources Instead of using an off-the-shelf web corpus tool such as BootCaT (Baroni and Bernardini, 2004), we implemented our own corpus crawler. This allowed us to have more control on query and corpus construction process. Even though our corpus construc4 www.cs.vu.nl/˜laurah/oaei/2009 oaei.ontologymatching.org/2008/ mldirectory 6 web.informatik.uni-mannheim.de/ multifarm 5 tion strategy is similar to the one implemented in BootCaT, there are some significant practical issues to take into account, such as: • The predominance of multiword keywords; • The use of the fixed keyword conference; • The expert tuning of the cleaning process; • The use of a long term support search AP[b]. Besides, BootC"
W12-3904,1999.tc-1.8,0,0.095455,"the resulting corpora are evaluated (§4) and discussed 25 Proceedings of the First Workshop on Multilingual Modeling, pages 25–31, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics (§5). We conclude by outlining their future applications (§ 6). 2 Related Work Web as corpus (WAC) approaches have been successfully adopted in many cases where data sparseness plays a major limiting role, either in specific linguistic constructions and words in a language (e.g. compounds and multiword expressions), or for less resourced languages in general1 . For instance, Grefenstette (1999) uses WAC for machine translation of compounds from French into English, Keller et al. (2002) for adjective-noun, noun-noun and verb-object bigram discovery, and Kim and Nakov (2011) for compound interpretation. Although a corpus derived from the web may contain noise, the sheer size of data available should compensate for that. Baroni and Ueyama (2006) discuss in details the process of corpus construction from web pages for both generic and domainspecific corpora. In particular, they focus on the cleaning process applied to filter the crawled web pages. Much of the methodology applied in our"
W12-3904,W11-1217,0,0.028453,"Computational Linguistics (§5). We conclude by outlining their future applications (§ 6). 2 Related Work Web as corpus (WAC) approaches have been successfully adopted in many cases where data sparseness plays a major limiting role, either in specific linguistic constructions and words in a language (e.g. compounds and multiword expressions), or for less resourced languages in general1 . For instance, Grefenstette (1999) uses WAC for machine translation of compounds from French into English, Keller et al. (2002) for adjective-noun, noun-noun and verb-object bigram discovery, and Kim and Nakov (2011) for compound interpretation. Although a corpus derived from the web may contain noise, the sheer size of data available should compensate for that. Baroni and Ueyama (2006) discuss in details the process of corpus construction from web pages for both generic and domainspecific corpora. In particular, they focus on the cleaning process applied to filter the crawled web pages. Much of the methodology applied in our work is similar to their proposed approach (see §3). Moreover, when access to parallel corpora is limited, comparable corpora can minimize data sparseness, as discussed by Skadina et"
W12-3904,W02-1030,0,0.0693583,"Missing"
W12-3904,D11-1060,0,0.0852301,"ssociation for Computational Linguistics (§5). We conclude by outlining their future applications (§ 6). 2 Related Work Web as corpus (WAC) approaches have been successfully adopted in many cases where data sparseness plays a major limiting role, either in specific linguistic constructions and words in a language (e.g. compounds and multiword expressions), or for less resourced languages in general1 . For instance, Grefenstette (1999) uses WAC for machine translation of compounds from French into English, Keller et al. (2002) for adjective-noun, noun-noun and verb-object bigram discovery, and Kim and Nakov (2011) for compound interpretation. Although a corpus derived from the web may contain noise, the sheer size of data available should compensate for that. Baroni and Ueyama (2006) discuss in details the process of corpus construction from web pages for both generic and domainspecific corpora. In particular, they focus on the cleaning process applied to filter the crawled web pages. Much of the methodology applied in our work is similar to their proposed approach (see §3). Moreover, when access to parallel corpora is limited, comparable corpora can minimize data sparseness, as discussed by Skadina et"
W12-3904,P03-1054,0,0.00322028,"pplied to remove very short sentences (less than 3 words), email addresses, URLs and dates, since the main purpose of the corpus is related to concept, instance and relations extraction. Finally, heuristics to filter out page menus and footnotes are included, leaving only the text of the body of the page. The raw version of the text still contains those expressions in case they are needed for other purposes. In the second step, the text undergoes linguistic annotation, where sentences are automatically lemmatized, POS tagged and parsed. Three well-known parsers were employed: Stanford parser (Klein and Manning, 2003) for texts in English, PALAVRAS (Bick, 2000) for texts in Portuguese, and Berkeley parser (Petrov et al., 2006) for texts in French. 4 Evaluation The characteristics of the resulting corpora are summarized in tables 2 and 3. Column D of table 2 shows that the number of documents retrieved is much higher in en than in pt and fr, and this is not proportional to the number of queries (Q). Indeed, if we look in table 3 at the average ratio of documents retrieved per query (D/Q), the en queries return much more documents than queries in other languages. This indicates that the search engine returns"
W12-3904,W11-1205,0,0.0292069,". Comparable corpora is a very active research subject, being in the core of several European projects (e.g. TTC2 , Accurat3 ). Nonetheless, to date most of 1 Kilgarriff (2007) warns about the dangers of statistics heavily based on a search engine. However, since we use the downloaded texts of web pages instead of search engine count estimators, this does not affect the results obtained in this work. 2 www.ttc-project.eu 3 www.accurat-project.eu 26 the research on comparable corpora seems to focus on lexicographic tasks (Forsyth and Sharoff, 2011; Sharoff, 2006), bilingual lexicon extraction (Morin and Prochasson, 2011), and more generally on machine translation and related applications (Ion et al., 2011). Likewise, there is much to be gained from the potential mutual benefits of comparable corpora and ontology-related tasks. Regarding multilingually aligned ontologies, very few data sets have been made available for use in the research community. Examples include the vlcr4 and the mldirectory5 datasets. The former contains a reduced set of alignments between the thesaurus of the Netherlands Institute for Sound and Vision and two other resources, English WordNet and DBpedia. The latter consists of a set of a"
W12-3904,P06-1055,0,0.016967,"of the corpus is related to concept, instance and relations extraction. Finally, heuristics to filter out page menus and footnotes are included, leaving only the text of the body of the page. The raw version of the text still contains those expressions in case they are needed for other purposes. In the second step, the text undergoes linguistic annotation, where sentences are automatically lemmatized, POS tagged and parsed. Three well-known parsers were employed: Stanford parser (Klein and Manning, 2003) for texts in English, PALAVRAS (Bick, 2000) for texts in Portuguese, and Berkeley parser (Petrov et al., 2006) for texts in French. 4 Evaluation The characteristics of the resulting corpora are summarized in tables 2 and 3. Column D of table 2 shows that the number of documents retrieved is much higher in en than in pt and fr, and this is not proportional to the number of queries (Q). Indeed, if we look in table 3 at the average ratio of documents retrieved per query (D/Q), the en queries return much more documents than queries in other languages. This indicates that the search engine returns more distinct results in en and more duplicate URLs in fr and in pt. The high discrepancy in 7 research.google"
W15-5617,P14-1023,0,0.095487,"e para o inglˆes, mas tamb´em para outras l´ınguas, como o francˆes. Para essas l´ınguas, o desenvolvimento de conjuntos de testes e gold standards dispon´ıveis para a comunidade, tais como o English Lexical Substitution Task8 [McCarthy and Navigli 2009], o TOEFL [Landauer and Dumais 1997] e o teste derivado do TOEFL, o WordNet-Based Synonymy Test (WBST) [Freitag et al. 2005], tem permitido a comparac¸a˜ o direta de t´ecnicas diferentes e a quantificac¸a˜ o precisa de melhorias na qualidade dos recursos gerados. Quest˜oes como a influˆencia do m´etodo usado (baseado em contagem ou preditivo) [Baroni et al. 2014, Lebret and Collobert 2015], da medida de associac¸a˜ o e medida de similaridade [Lin 1998, Padr´o et al. 2014], do tipo de contexto (bag-of-words ou dependˆencias sint´aticas) e de seu tamanho (1 x 2 x 5 x n palavras em torno de cada palavraalvo) [Freitag et al. 2005] tˆem sido cuidadosamente analisadas para determinar a melhor estrat´egia para se obter um tesauro de qualidade de acordo com l´ıngua, tamanho e tipo de corpus. Para o portuguˆes, ainda faltam estudos comparativos e conjuntos de dados e gold standards. Este trabalho tem por objetivo contribuir na criac¸a˜ o de gold standards que"
W15-5617,J10-4006,0,0.0423185,"Net [Navigli and Ponzetto 2010] e o Onto.PT [Oliveira and Gomes 2014]. Dentre esses, como veremos mais adiante, optamos por usar o BabelNet como comparac¸a˜ o devido principalmente a` sua cobertura e a` distinc¸a˜ o de polissemia. Para a construc¸a˜ o autom´atica de tesauros distribucionais a partir de corpora, tradicionalmente, utiliza-se como base a hip´otese distribucional de Harris de que se pode conhecer uma palavra pelas palavras que costumam ocorrer com ela [Lin 1998]. H´a duas principais abordagens para a construc¸a˜ o de tesauros: uma, mais tradicional, baseada em contagem [Lin 1998, Baroni and Lenci 2010] e outra, mais recente, baseada em redes neurais [Mikolov et al. 2010]. Avaliac¸o˜ es sobre a qualidade dos recursos gerados por cada abordagem existem para algumas l´ınguas e dom´ınios [Padr´o et al. 2014]; por´em, avaliac¸o˜ es comparativas das duas abordagens ainda s˜ao raras [Baroni et al. 2014, Lebret and Collobert 2015] e reportam resultados divergentes. Por exemplo, comparando modelos tradicionais e modelos preditivos em 14 tarefas diferentes, os modelos preditivos obtiveram os melhores resultados [Baroni et al. 2014], mas, em outras tarefas, ambos os modelos obtiveram resultados compa"
W15-5617,W11-2501,0,0.15233,"bucionais e´ uma tarefa complexa, pois faltam recursos que mec¸am a similaridade entre palavras. Para realizar a avaliac¸a˜ o, pode-se utilizar uma validac¸a˜ o por ju´ızes; contudo, essa forma de avaliac¸a˜ o e´ lenta e custosa. Uma alternativa e´ a utilizac¸a˜ o de ontologias lexicais, como a WordNet, comparando a ontologia e o tesauro. A avaliac¸a˜ o tamb´em pode ser indireta, atrav´es de tarefas que necessitam da quantificac¸a˜ o da similaridade, tais como: Detecc¸a˜ o de relac¸o˜ es semˆanticas: objetiva agrupar as palavras segundo uma relac¸a˜ o predeterminada. Datasets incluem o BLESS [Baroni and Lenci 2011], com 200 substantivos agrupados em 17 classes; o ESSLLI, [Baroni et al. 2008] com 44 conceitos em 6 classes; o Strudel, com 83 conceitos e 10 classes [Baroni et al. 2010] 9 ; e o SemEval 2010 Task 8 [Hendrickx et al. 2010], baseado em 9 relac¸o˜ es semˆanticas profundas. Identificac¸a˜ o da preferˆencia selecional de verbos: objetiva identificar qual a relac¸a˜ o sint´atica mais adequada entre um verbo e um substantivo. Existem conjuntos de 211 verbos [Pad´o 2007] e de 100 verbos [McRae et al. 1998]. Identificac¸a˜ o de analogia: usa exemplos da relac¸a˜ o para fazer inferˆencia de analogias"
W15-5617,C12-3044,0,0.049934,"Missing"
W15-5617,dias-da-silva-etal-2008-automatic,0,0.0280813,"m relac¸o˜ es entre palavras, pode ser medida pelo n´umero de iniciativas dedicadas a (re)produzi-los para outras l´ınguas, tais como a EuroWordNet1 [Vossen 1998] e a Global WordNet Association2 [Bond and Paik 2012]. Tais recursos tˆem sido utilizados em in´umeras aplicac¸o˜ es de tecnologia de linguagem, como sistemas de perguntas e respostas, de simplificac¸a˜ o de texto e de an´alise de sentimentos. Para o portuguˆes, est˜ao dispon´ıveis o Onto.PT3 [Gonc¸alo Oliveira and Gomes 2010], OpenWN-PT4 [de Paiva et al. 2012], MultiWordnet of Portuguese5 , o WordNet.PT6 [Marrafa 2002], WordNet.Br7 [Dias-da-Silva et al. 2008], 1 http://www.illc.uva.nl/EuroWordNet/ http://globalwordnet.org/wordnets-in-the-world/ 3 http://ontopt.dei.uc.pt 4 https://github.com/arademaker/openWordnet-PT 5 http://mwnpt.di.fc.ul.pt/ 6 http://www.clul.ul.pt/clg/wordnetpt/index.html 7 http://143.107.183.175:21380/wordnetbr 2 131 Tesauros Distribucionais para o Portuguˆes: avaliac¸a˜ o de metodologias entre outros. A construc¸a˜ o manual desse tipo de recurso requer conhecimento especializado, al´em de ser cara e demorada. Al´em disso, o recurso resultante e´ est´atico, tem cobertura limitada e se aplica a um dom´ınio geral. Por isso, com"
W15-5617,P13-4006,0,0.0276568,"e frequˆencia. 135 Tesauros Distribucionais para o Portuguˆes: avaliac¸a˜ o de metodologias ˜ Tabela 1. Tamanho em numero ´ de palavras-alvo nas diferentes relac¸oes ˆ semanticas no gold standard proposto Verbos Subst Total Sinˆonimos Hiperˆonimos 500 500 1667 1667 2167 2167 Hipˆonimos 500 1667 2167 Antˆonimos 200 200 400 Outros 1000 3334 4334 Total 2700 8535 11235 3.3. Tesauros de contagem O tesauro baseado em contagem foi constru´ıdo seguindo Padr´o et. al [Padr´o et al. 2014]: descartaram-se palavras com ocorrˆencia de bigrama menor que 5 no corpus e utilizou-se a implementac¸a˜ o DISSECT [Dinu et al. 2013]. O tipo de contexto usado s˜ao as palavras em torno de substantivo ou verbo como uma bag-of-words, isto e´ , uma janela de n palavras de contexto antes e depois da palavra-alvo. Foram gerados tesauros com dois tamanhos de janela: 5 e 10. Assim, um contexto (s, c, t) e´ a ocorrˆencia de substantivo s, contexto c e marcac¸a˜ o t, e o n´umero de ocorrˆencias de um contexto em um corpus e´ representado por ||s,c,t||. Por exemplo, a frase “O c˜ao comeu o osso” gera duas triplas (s = “c˜ao”, c = “comer”, t = “verbo”) e (s = “c˜ao”, c = “osso”, t = “substantivo”). Utilizaram-se apenas contextos com"
W15-5617,W05-0604,0,0.459079,"gerados podem complementar a informac¸a˜ o de recursos lexicais e ontol´ogicos como a WordNet. Assim, muita atenc¸a˜ o tem sido devotada para construc¸a˜ o, avaliac¸a˜ o e aprimoramento sistem´aticos de tesauros distribucionais, principalmente para o inglˆes, mas tamb´em para outras l´ınguas, como o francˆes. Para essas l´ınguas, o desenvolvimento de conjuntos de testes e gold standards dispon´ıveis para a comunidade, tais como o English Lexical Substitution Task8 [McCarthy and Navigli 2009], o TOEFL [Landauer and Dumais 1997] e o teste derivado do TOEFL, o WordNet-Based Synonymy Test (WBST) [Freitag et al. 2005], tem permitido a comparac¸a˜ o direta de t´ecnicas diferentes e a quantificac¸a˜ o precisa de melhorias na qualidade dos recursos gerados. Quest˜oes como a influˆencia do m´etodo usado (baseado em contagem ou preditivo) [Baroni et al. 2014, Lebret and Collobert 2015], da medida de associac¸a˜ o e medida de similaridade [Lin 1998, Padr´o et al. 2014], do tipo de contexto (bag-of-words ou dependˆencias sint´aticas) e de seu tamanho (1 x 2 x 5 x n palavras em torno de cada palavraalvo) [Freitag et al. 2005] tˆem sido cuidadosamente analisadas para determinar a melhor estrat´egia para se obter u"
W15-5617,W10-2302,0,0.0693733,"Missing"
W15-5617,S10-1006,0,0.0789848,"Missing"
W15-5617,P12-1092,0,0.0170658,"ac¸a˜ o para fazer inferˆencia de analogias morfol´ogicas, sint´aticas e semˆanticas [Mikolov et al. 2013b], tais como man est´a para woman assim como king est´a para queen [Mikolov et al. 2013a]. Identificac¸a˜ o de itens relacionados semanticamente: objetiva identificar palavras que s˜ao relacionadas por alguma relac¸a˜ o semˆantica (p.ex., tigre e animal, areia e praia). Datasets incluem 65 pares de substantivos [Rubenstein and Goodenough 1965], 80 substantivos (TOEFL [Landauer and Dumais 1997]), 353 pares (WordSim353 [Finkelstein et al. 2001]), 2003 pares com sentenc¸as de contexto (SCWS [Huang et al. 2012]) e 3000 pares (MEN [Bruni et al. 2014]). No TOEFL, para cada uma das 80 palavras-alvo, h´a quatro alternativas, dentre as quais se deve identificar a palavra mais pr´oxima semanticamente. J´a o WordNet-Based Synonymy Test (WBST) [Freitag et al. 2005] e´ uma extens˜ao gerada automaticamente a partir da WordNet. 3. Materiais e m´etodos Nesta sec¸a˜ o, apresentamos a metodologia de criac¸a˜ o do recurso de avaliac¸a˜ o utilizado, o Brazilian BabelNet-based Semantic Gold Standard (B 2 SG), o corpus utilizado para o treino dos modelos e, por fim, o desenvolvimento dos tesauros distribucionais. 9"
W15-5617,N13-1090,0,0.0414643,"sses; o ESSLLI, [Baroni et al. 2008] com 44 conceitos em 6 classes; o Strudel, com 83 conceitos e 10 classes [Baroni et al. 2010] 9 ; e o SemEval 2010 Task 8 [Hendrickx et al. 2010], baseado em 9 relac¸o˜ es semˆanticas profundas. Identificac¸a˜ o da preferˆencia selecional de verbos: objetiva identificar qual a relac¸a˜ o sint´atica mais adequada entre um verbo e um substantivo. Existem conjuntos de 211 verbos [Pad´o 2007] e de 100 verbos [McRae et al. 1998]. Identificac¸a˜ o de analogia: usa exemplos da relac¸a˜ o para fazer inferˆencia de analogias morfol´ogicas, sint´aticas e semˆanticas [Mikolov et al. 2013b], tais como man est´a para woman assim como king est´a para queen [Mikolov et al. 2013a]. Identificac¸a˜ o de itens relacionados semanticamente: objetiva identificar palavras que s˜ao relacionadas por alguma relac¸a˜ o semˆantica (p.ex., tigre e animal, areia e praia). Datasets incluem 65 pares de substantivos [Rubenstein and Goodenough 1965], 80 substantivos (TOEFL [Landauer and Dumais 1997]), 353 pares (WordSim353 [Finkelstein et al. 2001]), 2003 pares com sentenc¸as de contexto (SCWS [Huang et al. 2012]) e 3000 pares (MEN [Bruni et al. 2014]). No TOEFL, para cada uma das 80 palavras-alvo,"
W15-5617,P10-1023,0,0.562453,"[Freitag et al. 2005] tˆem sido cuidadosamente analisadas para determinar a melhor estrat´egia para se obter um tesauro de qualidade de acordo com l´ıngua, tamanho e tipo de corpus. Para o portuguˆes, ainda faltam estudos comparativos e conjuntos de dados e gold standards. Este trabalho tem por objetivo contribuir na criac¸a˜ o de gold standards que possam ser usados para avaliac¸o˜ es comparativas desses m´etodos, atrav´es da construc¸a˜ o de um teste similar ao TOEFL para o portuguˆes. O Brazilian BabelNetbased Semantic Gold Standard (B 2 SG) foi automaticamente gerado a partir do BabelNet [Navigli and Ponzetto 2010], contendo quest˜oes que envolvem o c´alculo de similaridade entre uma determinada palavra e candidatos a palavras semanticamente relacionadas. Este artigo tamb´em visa a responder parte das quest˜oes sobre a qualidade dos tesauros gerados com foco no portuguˆes, atrav´es de uma investigac¸a˜ o comparativa entre dois m´etodos para a construc¸a˜ o de tesauros (baseado em contagem e preditivo). Esses t´opicos s˜ao discutidos no artigo da seguinte forma: em §2, s˜ao apresentados os trabalhos relacionados sobre tesauros distribucionais e, em §3, os materiais e m´etodos empregados. A avaliac¸a˜ o"
W15-5617,padro-etal-2014-comparing,1,0.838091,"Missing"
W15-5617,D14-1047,1,0.823424,"Missing"
W15-5620,P98-1013,0,0.74135,"mais de 6 mil instˆancias e 15 mil argumentos anotados e se encontra dispon´ıvel para download nos formatos XML e SQL. Este artigo tamb´em descreve uma an´alise comparativa entre os dois corpora, mostrando que a distribuic¸a˜ o de pap´eis semˆanticos na linguagem n˜ao especializada e´ diferente da linguagem especializada. 1. Introduc¸a˜ o Muitos dos avanc¸os recentes na Linguistica Computacional (LC), Processamento de Linguagem Natural (PLN) e a´ reas afins se devem a` disponibilizac¸a˜ o de recursos lexicais e ontol´ogicos para a comunidade, tais como o WordNet [Fellbaum 1998] e a FrameNet [Baker et al. 1998]. Em particular, recursos l´exicos com informac¸o˜ es de pap´eis semˆanticos de verbos representam uma contribuic¸a˜ o interdisciplinar para essas a´ reas. Na Lingu´ıstica, esse tipo de recurso subsidia a descric¸a˜ o da l´ıngua em foco, tendo em vista que representa um cat´alogo estruturado de seus verbos com as respectivas informac¸o˜ es sint´aticas e semˆanticas. No PLN, esse tipo de recurso pode ser empregado para a an´alise semˆantica de sentenc¸as, o reconhecimento autom´atico de significado e outras tarefas associadas. Temos, por exemplo, trabalhos que usam informac¸a˜ o semˆantica par"
W15-5620,dias-da-silva-etal-2008-automatic,0,0.0522274,"Missing"
W15-5620,W11-4519,0,0.0367962,"Missing"
W15-5620,C12-1053,0,0.0265957,"a l´ıngua em foco, tendo em vista que representa um cat´alogo estruturado de seus verbos com as respectivas informac¸o˜ es sint´aticas e semˆanticas. No PLN, esse tipo de recurso pode ser empregado para a an´alise semˆantica de sentenc¸as, o reconhecimento autom´atico de significado e outras tarefas associadas. Temos, por exemplo, trabalhos que usam informac¸a˜ o semˆantica para resoluc¸a˜ o de an´aforas [Kong and Zhou 2012], sumarizac¸a˜ o autom´atica [Yoshikawa et al. 2012], 161 VerbLexPor: um recurso l´exico com anotac¸a˜ o de pap´eis semˆanticos para o portuguˆees traduc¸a˜ o autom´atica [Feng et al. 2012, Jones et al. 2012] etc. Para o portuguˆes do Brasil, h´a trˆes recursos relativamente similares que contemplam verbos e argumentos: o PropBank.Br [Duran et al. 2011, Duran and Alu´ısio 2012], a VerbNet.Br [Scarton 2013] e a FrameNet Brasil [Salom˜ao 2009]. Neste artigo, apresentamos um recurso l´exico diferenciado com informac¸o˜ es de pap´eis semˆanticos, o VerbLexPor, que foi extra´ıdo de dois corpora: um de dom´ınio espec´ıfico com linguagem especializada (artigos de Cardiologia) e outro gen´erico com linguagem n˜ao especializada (textos do jornal Di´ario Ga´ucho). O recurso foi anotado p"
W15-5620,C12-1083,0,0.0120315,"tendo em vista que representa um cat´alogo estruturado de seus verbos com as respectivas informac¸o˜ es sint´aticas e semˆanticas. No PLN, esse tipo de recurso pode ser empregado para a an´alise semˆantica de sentenc¸as, o reconhecimento autom´atico de significado e outras tarefas associadas. Temos, por exemplo, trabalhos que usam informac¸a˜ o semˆantica para resoluc¸a˜ o de an´aforas [Kong and Zhou 2012], sumarizac¸a˜ o autom´atica [Yoshikawa et al. 2012], 161 VerbLexPor: um recurso l´exico com anotac¸a˜ o de pap´eis semˆanticos para o portuguˆees traduc¸a˜ o autom´atica [Feng et al. 2012, Jones et al. 2012] etc. Para o portuguˆes do Brasil, h´a trˆes recursos relativamente similares que contemplam verbos e argumentos: o PropBank.Br [Duran et al. 2011, Duran and Alu´ısio 2012], a VerbNet.Br [Scarton 2013] e a FrameNet Brasil [Salom˜ao 2009]. Neste artigo, apresentamos um recurso l´exico diferenciado com informac¸o˜ es de pap´eis semˆanticos, o VerbLexPor, que foi extra´ıdo de dois corpora: um de dom´ınio espec´ıfico com linguagem especializada (artigos de Cardiologia) e outro gen´erico com linguagem n˜ao especializada (textos do jornal Di´ario Ga´ucho). O recurso foi anotado por um linguista com"
W15-5620,C12-1090,0,0.0176815,"exicos com informac¸o˜ es de pap´eis semˆanticos de verbos representam uma contribuic¸a˜ o interdisciplinar para essas a´ reas. Na Lingu´ıstica, esse tipo de recurso subsidia a descric¸a˜ o da l´ıngua em foco, tendo em vista que representa um cat´alogo estruturado de seus verbos com as respectivas informac¸o˜ es sint´aticas e semˆanticas. No PLN, esse tipo de recurso pode ser empregado para a an´alise semˆantica de sentenc¸as, o reconhecimento autom´atico de significado e outras tarefas associadas. Temos, por exemplo, trabalhos que usam informac¸a˜ o semˆantica para resoluc¸a˜ o de an´aforas [Kong and Zhou 2012], sumarizac¸a˜ o autom´atica [Yoshikawa et al. 2012], 161 VerbLexPor: um recurso l´exico com anotac¸a˜ o de pap´eis semˆanticos para o portuguˆees traduc¸a˜ o autom´atica [Feng et al. 2012, Jones et al. 2012] etc. Para o portuguˆes do Brasil, h´a trˆes recursos relativamente similares que contemplam verbos e argumentos: o PropBank.Br [Duran et al. 2011, Duran and Alu´ısio 2012], a VerbNet.Br [Scarton 2013] e a FrameNet Brasil [Salom˜ao 2009]. Neste artigo, apresentamos um recurso l´exico diferenciado com informac¸o˜ es de pap´eis semˆanticos, o VerbLexPor, que foi extra´ıdo de dois corpora: u"
W15-5620,P12-2068,0,0.0278589,"de verbos representam uma contribuic¸a˜ o interdisciplinar para essas a´ reas. Na Lingu´ıstica, esse tipo de recurso subsidia a descric¸a˜ o da l´ıngua em foco, tendo em vista que representa um cat´alogo estruturado de seus verbos com as respectivas informac¸o˜ es sint´aticas e semˆanticas. No PLN, esse tipo de recurso pode ser empregado para a an´alise semˆantica de sentenc¸as, o reconhecimento autom´atico de significado e outras tarefas associadas. Temos, por exemplo, trabalhos que usam informac¸a˜ o semˆantica para resoluc¸a˜ o de an´aforas [Kong and Zhou 2012], sumarizac¸a˜ o autom´atica [Yoshikawa et al. 2012], 161 VerbLexPor: um recurso l´exico com anotac¸a˜ o de pap´eis semˆanticos para o portuguˆees traduc¸a˜ o autom´atica [Feng et al. 2012, Jones et al. 2012] etc. Para o portuguˆes do Brasil, h´a trˆes recursos relativamente similares que contemplam verbos e argumentos: o PropBank.Br [Duran et al. 2011, Duran and Alu´ısio 2012], a VerbNet.Br [Scarton 2013] e a FrameNet Brasil [Salom˜ao 2009]. Neste artigo, apresentamos um recurso l´exico diferenciado com informac¸o˜ es de pap´eis semˆanticos, o VerbLexPor, que foi extra´ıdo de dois corpora: um de dom´ınio espec´ıfico com linguagem especializad"
W16-1804,W15-0904,0,0.134987,"seful, e.g. to decide how an MWE should be translated (Cap et al., 2015). Many datasets with compositionality judgments have been collected (e.g. Gurrutxaga and Alegria (2013) and McCarthy et al. (2003)). Reddy et al. (2011) asked Mechanical Turkers to annotate 90 English noun-noun compounds on a scale from 0 to 5 with respect to the literality of member words. This resource has been used to evaluate compositionality prediction systems (Salehi et al., 2015). A similar resource has been created for German by Roller et al. (2013), who propose two filtering techniques adopted in our experiments. Farahmand et al. (2015) created a dataset of 1042 compounds in English with binary annotations by 4 experts. The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction functions (Yazdani et al., 2015). In this paper we report a cross-lingual examination of quality measures and filtering strategies for compound compositionality annotations. Using the dataset by Reddy et al. (2011) and its extension to English, French and Portuguese by Ramisch et al. (2016), we examine the filters reported by Roller et al. (2013) for German and assess whether they improve overall dataset"
W16-1804,P16-2026,1,0.80584,"been created for German by Roller et al. (2013), who propose two filtering techniques adopted in our experiments. Farahmand et al. (2015) created a dataset of 1042 compounds in English with binary annotations by 4 experts. The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction functions (Yazdani et al., 2015). In this paper we report a cross-lingual examination of quality measures and filtering strategies for compound compositionality annotations. Using the dataset by Reddy et al. (2011) and its extension to English, French and Portuguese by Ramisch et al. (2016), we examine the filters reported by Roller et al. (2013) for German and assess whether they improve overall dataset quality in these three languages. This analysis aims at studying the distributions and characteristics of the human ratings, examining quality measures for the collected data, and measuring the impact of simple filtering techniques on these quality measures. In particular, we look at how the scores obtained are distributed across the compositionality scale, whether the scores of the individual components are correlated with This paper analyzes datasets with numerical scores that"
W16-1804,I11-1024,0,0.426514,"Judgments Silvio Cordeiro1,2 , Carlos Ramisch1 , Aline Villavicencio2 1 Aix Marseille Université, CNRS, LIF UMR 7279 (France) 2 Institute of Informatics, Federal University of Rio Grande do Sul (Brazil) silvioricardoc@gmail.com carlos.ramisch@lif.univ-mrs.fr avillavicencio@inf.ufrgs.br Abstract Low values imply idiomaticity, while high values imply compositionality. This information can be useful, e.g. to decide how an MWE should be translated (Cap et al., 2015). Many datasets with compositionality judgments have been collected (e.g. Gurrutxaga and Alegria (2013) and McCarthy et al. (2003)). Reddy et al. (2011) asked Mechanical Turkers to annotate 90 English noun-noun compounds on a scale from 0 to 5 with respect to the literality of member words. This resource has been used to evaluate compositionality prediction systems (Salehi et al., 2015). A similar resource has been created for German by Roller et al. (2013), who propose two filtering techniques adopted in our experiments. Farahmand et al. (2015) created a dataset of 1042 compounds in English with binary annotations by 4 experts. The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction function"
W16-1804,W13-1005,0,0.665579,"imply idiomaticity, while high values imply compositionality. This information can be useful, e.g. to decide how an MWE should be translated (Cap et al., 2015). Many datasets with compositionality judgments have been collected (e.g. Gurrutxaga and Alegria (2013) and McCarthy et al. (2003)). Reddy et al. (2011) asked Mechanical Turkers to annotate 90 English noun-noun compounds on a scale from 0 to 5 with respect to the literality of member words. This resource has been used to evaluate compositionality prediction systems (Salehi et al., 2015). A similar resource has been created for German by Roller et al. (2013), who propose two filtering techniques adopted in our experiments. Farahmand et al. (2015) created a dataset of 1042 compounds in English with binary annotations by 4 experts. The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction functions (Yazdani et al., 2015). In this paper we report a cross-lingual examination of quality measures and filtering strategies for compound compositionality annotations. Using the dataset by Reddy et al. (2011) and its extension to English, French and Portuguese by Ramisch et al. (2016), we examine the filters r"
W16-1804,W15-0909,0,0.0827082,"ramisch@lif.univ-mrs.fr avillavicencio@inf.ufrgs.br Abstract Low values imply idiomaticity, while high values imply compositionality. This information can be useful, e.g. to decide how an MWE should be translated (Cap et al., 2015). Many datasets with compositionality judgments have been collected (e.g. Gurrutxaga and Alegria (2013) and McCarthy et al. (2003)). Reddy et al. (2011) asked Mechanical Turkers to annotate 90 English noun-noun compounds on a scale from 0 to 5 with respect to the literality of member words. This resource has been used to evaluate compositionality prediction systems (Salehi et al., 2015). A similar resource has been created for German by Roller et al. (2013), who propose two filtering techniques adopted in our experiments. Farahmand et al. (2015) created a dataset of 1042 compounds in English with binary annotations by 4 experts. The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction functions (Yazdani et al., 2015). In this paper we report a cross-lingual examination of quality measures and filtering strategies for compound compositionality annotations. Using the dataset by Reddy et al. (2011) and its extension to English,"
W16-1804,D15-1201,0,0.241386,"ked Mechanical Turkers to annotate 90 English noun-noun compounds on a scale from 0 to 5 with respect to the literality of member words. This resource has been used to evaluate compositionality prediction systems (Salehi et al., 2015). A similar resource has been created for German by Roller et al. (2013), who propose two filtering techniques adopted in our experiments. Farahmand et al. (2015) created a dataset of 1042 compounds in English with binary annotations by 4 experts. The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction functions (Yazdani et al., 2015). In this paper we report a cross-lingual examination of quality measures and filtering strategies for compound compositionality annotations. Using the dataset by Reddy et al. (2011) and its extension to English, French and Portuguese by Ramisch et al. (2016), we examine the filters reported by Roller et al. (2013) for German and assess whether they improve overall dataset quality in these three languages. This analysis aims at studying the distributions and characteristics of the human ratings, examining quality measures for the collected data, and measuring the impact of simple filtering tec"
W16-1804,W03-1810,0,\N,Missing
W16-1804,W15-0903,0,\N,Missing
W16-1804,W13-1017,0,\N,Missing
W16-4119,W10-1001,0,0.161569,"eived considerable attention from the research community (DuBay, 2004). The task of attributing a readability level to a text has a wide range of applications, including support for student reading material selection (Petersen and Ostendorf, 2009) or help for clinical patients (Feng et al., 2009). It can also be used for ensuring that instructions and policies are written in an easily comprehensible way even for readers with low education (McClure, 1987). It can also contribute to the task of text simplification, evaluating the obtained version to indicate if further simplification is needed (Aluisio et al., 2010). Recently, authors such as Petersen and Ostendorf (2009), Vajjala and Meurers (2014) and Scarton et al. (2010) have started treating this task as one of text classification, using corpora manually annotated with readability classifications to train automatic learning models, based on a large set of text metrics, including deeper features, for example derived from n-gram language models and parse trees. However, an important limitation to this approach is the small availability of reliably annotated train data. Moreover, this task is known to be very subjective, and even human annotators prese"
W16-4119,P06-4020,0,0.0106533,"e worked with counts based based in syntactic analysis, including part of speeches (18 for Portuguese and 20 for English) and dependency tags (72 for Portuguese and 27 for English), besides 7 measures of verb analysis, including verb transitivity, passive voice, average number of modifiers and average sub-categorization frame length. In Portuguese, we also analysed the incidence of verbs in the imperative mood. All these counts are frequently used as indicators of syntactic complexity, according to the online tool Coh-Metrix (McNamara et al., 2002). The parsers Palavras (Bick, 2000) and Rasp (Briscoe et al., 2006) were used to obtain these features for Portuguese and English, respectively. Since we want to assess the contribution of different feature categories, a few specific groups were defined. The selected groups were: sub-categorization (transitivity, average number of modifiers, average sub-categorization frame length), classical readability formulas, descriptors (counts of sentences, words, syllables, letters and types and TTR) and corpora-based (incidence of unknown words, average frequency in a general corpus, lists of simple words). Moreover, we also divided our complete feature sets in three"
W16-4119,P11-2117,0,0.0184104,"cles for adults with two different levels of simplification (natural and strong). Brasil Escola (BrEscola), a corpus of educational materials for children and teenagers, Wikibooks, a corpus of virtual books for readers of different proficiency levels (Beginner, Intermediary, Advanced and Professional), and Britannica Biographies (BB), a corpus of biographies with versions in three different readability levels (Elementary, Medium and High), were collected especially for this study, crawling different sections of the websites of the same names2 . The Simple Wikipedia (SW) corpus was compiled by Coster and Kauchak (2011), pairing articles from the English and Simple English versions of Wikipedia3 . In the case of the corpora Wikilivros, ZH, Wikibooks and BB, which consider more than two readability levels, tests were also done with adapted binary versions, in order to verify the impact of the number of classes in the classifier performance. For that, in Wikilivros and BB, the most simple and most difficult levels were selected. In ZH, we used the original and the natural simplification class, since the strong simplification was exaggerated for our classification purposes, and, in Wikibooks, we discarded the B"
W16-4119,W11-2308,0,0.0374013,"Missing"
W16-4119,E09-1027,0,0.0342607,"already parsed and annotated with 134 different textual attributes, along with the agreement among the various classifiers. 1 Introduction Text readability assessment refers to measuring how easy it is for a reader to read and understand a given text. In this context methods for automatic readability assessment have received considerable attention from the research community (DuBay, 2004). The task of attributing a readability level to a text has a wide range of applications, including support for student reading material selection (Petersen and Ostendorf, 2009) or help for clinical patients (Feng et al., 2009). It can also be used for ensuring that instructions and policies are written in an easily comprehensible way even for readers with low education (McClure, 1987). It can also contribute to the task of text simplification, evaluating the obtained version to indicate if further simplification is needed (Aluisio et al., 2010). Recently, authors such as Petersen and Ostendorf (2009), Vajjala and Meurers (2014) and Scarton et al. (2010) have started treating this task as one of text classification, using corpora manually annotated with readability classifications to train automatic learning models,"
W16-4119,W12-2207,0,0.0546483,"Missing"
W16-4119,C12-1065,0,0.0201373,"ical and syntactic features and also with traditional formulas. Investigating the contribution of syntactic features, it was observed that they were not good enough separately, but contributed to the general performance. Complementarily, Vajjala and Meurers (2014) applied 152 lexical and syntactic attributes to classify a corpus of subtitles from different BBC channels for children and adults, also using SVMs. The most predictive attribute was shown to be the age of acquisition. Similar approaches were applied in multiple other languages, including Italian (Dell’Orletta et al., 2011), German (Hancke et al., 2012) and Basque (Gonzalez-Dios et al., 2014). In Portuguese, Scarton and Alu´ısio (2010) classified articles for children and adults from local newspapers, using a SVM trained on 48 psycholinguistic features and obtaining an F-measure of 0.944. Wagner Filho et al. (2016) proposed a framework to take advantage of the increasing availability of language content in the Web to create large repositories of text suitable for different reading levels, incorporating a readability classifier to a pipeline similar to the one used by Baroni et al. (2009) to build a series of large Web corpora, such as the uk"
W16-4119,P10-1023,0,0.0322501,"glish and Portuguese versions) (Flesch and others, 1946; Martins et al., 1996), the Coleman-Liau Index (Coleman and Liau, 1975), the Flesch Grade Level, the Automated Readability Index (Senter and Smith, 1967), Fog (Gunning, 1952), SMOG (Mc Laughlin, 1969) and, for English, the Dale-Chall Formula (Dale and Chall, 1948) as well.4 In order to account for word ambiguity, a metric based on the hypothesis that more commonly used words, and therefore easier to understand (Vajjala and Meurers, 2014), tend to present multiple meanings in a language, we used the average number of senses from BabelNet (Navigli and Ponzetto, 2010), for Portuguese, and WordNet (Miller, 1995), for English. Moreover, following Si and Callan (2001), we worked with the average frequency in a general corpus (AFGC) and standard deviation as frequency measures, based on the hypothesis that words with higher frequencies in a general corpus tend to be more knownand, therefore, included in more levels of texts, while rarer words are more inclined to be restricted to more complex levels. We also worked with a series of closed word lists to count word classes (stopwords, prepositions, articles, pronouns, personal and possessive pronouns (PP), conju"
W16-4119,W14-1203,0,0.2756,"of attributing a readability level to a text has a wide range of applications, including support for student reading material selection (Petersen and Ostendorf, 2009) or help for clinical patients (Feng et al., 2009). It can also be used for ensuring that instructions and policies are written in an easily comprehensible way even for readers with low education (McClure, 1987). It can also contribute to the task of text simplification, evaluating the obtained version to indicate if further simplification is needed (Aluisio et al., 2010). Recently, authors such as Petersen and Ostendorf (2009), Vajjala and Meurers (2014) and Scarton et al. (2010) have started treating this task as one of text classification, using corpora manually annotated with readability classifications to train automatic learning models, based on a large set of text metrics, including deeper features, for example derived from n-gram language models and parse trees. However, an important limitation to this approach is the small availability of reliably annotated train data. Moreover, this task is known to be very subjective, and even human annotators present a high disagreement rate in their evaluations (Petersen and Ostendorf, 2009). In t"
W17-6941,S10-1007,0,0.070865,"Missing"
W17-6941,cholakov-etal-2014-lexical,0,0.0430054,"Missing"
W17-6941,J90-1003,0,0.568905,"itutes, all compounds have been annotated with corpus frequency, association strength and compositionality information. Frequency is used as a predictor of human familiarity with a word, assuming that the higher the frequency the more familiar the compound. The association strength is used as an indication of the conventionality of a compound, with the assumption that the higher the strength, the more conventional the compound is. This follows the agreement between association measures and conventionality found by Farahmand et al. (2015). In this work we use pointwise mutual information (PMI, Church and Hanks (1990)), an association measure widely used for MWEs. 12Both frequency and PMI are calculated based on counts from a combined corpus of around 1.91 billion tokens. The corpus is formed by a concatenation of the brWaC (Wagner Filho et al., 2016; Boos et al., 2014), the Brazilian Corpus (Berber Sardinha et al., 2008), and the Portuguese Wikipedia. For compositionality we use the scores collected by Ramisch et al. (2016). This ensures a balance of compositionality, since the dataset was designed to contain 60 compositional (e.g., acampamento militar – lit. camp military [military camp]), 60 partly comp"
W17-6941,W14-3348,0,0.0130917,"r and type of responses collected for the construction of the dataset.1 LexSubNC is potentially useful for the evaluation and development of several NLP tasks and applications. For example, it could be used to tune the development of distributional semantic models that maximize the similarity between an MWE and its substitutes, similarly to what is currently done for single words (Hill et al., 2015; Levy et al., 2015). It could also be used for the evaluation of machine translation methods that focus on non-compositional expressions, similarly to what is currently done for instance in METEOR (Denkowski and Lavie, 2014). For automatic text simplification, paraphrases could be used to replace non-compositional expressions by more explicit paraphrases (Specia et al., 2012). This paper is structured as follows: we discuss similar resources and the techniques used to collect them (§2), and describe the protocol used for collecting human responses (§3). The responses are analyzed for possible correlations between characteristics of the compounds and the responses provided (§4). We finish with conclusions and a discussion of future work (§5). 2 Related Work A variety of protocols have been adopted for collecting s"
W17-6941,W15-0904,0,0.0619943,"at http://pageperso.lif.univ-mrs.fr/~carlos.ramisch/ ?page=downloads/compounds&lang=en. any part of speech, as long as the resulting paraphrase was a well-formed noun phrase. In all of these cases, the compounds were compositional: they could be paraphrased using combinations of their parts, and lexical substitution could be performed for each component individually. Other MWE datasets contain compositionality assessment, and, as a consequence, they also include idiomatic cases. Datasets for nominal compound compositionality are available in English (Reddy et al., 2011; Ramisch et al., 2016; Farahmand et al., 2015), German (Roller et al., 2013), Portuguese and French (Ramisch et al., 2016), and for noun-verb expressions in Basque (Gurrutxaga and Alegria, 2013). For instance, Reddy et al. (2011) collected numerical scores for 90 English nominal compounds regarding their compositionality. Data for each compound and component word was gathered through crowdsourcing using a 6-point scale ranging from totally idiomatic (0) to fully compositional (5). Ramisch et al. (2016) extended this set with additional compounds and also applied it to other languages, generating a total of 180 compounds per language for E"
W17-6941,W13-1017,0,0.0499273,"Missing"
W17-6941,S13-2025,0,0.0655306,"Missing"
W17-6941,J15-4004,0,0.239499,"n-gram frequencies, concreteness, imageability, and conventionality. Depending on the target word, more than one possible alternative substitution may fulfill the criteria and produce an acceptable result (e.g. acquire/buy/purchase a painting). Resources such as thesauri, containing semantically related words (Fellbaum, 1998; Lin, 1998), and word norms, with information about word properties (Nelson et al., 2004), may be used to inform these tasks. Various initiatives for collecting word norms resulted in datasets such as the South Florida Association Norms (Nelson et al., 2004), SimLex-999 (Hill et al., 2015), Hyperlex (Vuli´c et al., 2016) and Rare Words (Luong et al., 2013). These resources form valuable gold standards for evaluating the quality of a variety of tasks and applications, including text simplification and machine translation. However, they often concentrate on single words as targets. The collection of norms for longer units is particularly challenging due to the arbitrary interactions between their member words, particularly if they involve multiword expressions (MWEs), such as nominal compounds or verbal idioms. Available MWE datasets often target specific types of MWEs such as ve"
W17-6941,Q15-1016,0,0.0188473,"re then manually validated and classified according to the particular semantic relations involved. We examine the impact of factors like frequency, conventionality and compositionality on the number and type of responses collected for the construction of the dataset.1 LexSubNC is potentially useful for the evaluation and development of several NLP tasks and applications. For example, it could be used to tune the development of distributional semantic models that maximize the similarity between an MWE and its substitutes, similarly to what is currently done for single words (Hill et al., 2015; Levy et al., 2015). It could also be used for the evaluation of machine translation methods that focus on non-compositional expressions, similarly to what is currently done for instance in METEOR (Denkowski and Lavie, 2014). For automatic text simplification, paraphrases could be used to replace non-compositional expressions by more explicit paraphrases (Specia et al., 2012). This paper is structured as follows: we discuss similar resources and the techniques used to collect them (§2), and describe the protocol used for collecting human responses (§3). The responses are analyzed for possible correlations betwee"
W17-6941,P98-2127,0,0.0351344,"lving lexical substitution, alternatives need to be identified for a given target word (McCarthy and Navigli, 2007, 2009), usually in a particular context. Candidates can be chosen to maximize word properties that are relevant for the particular task, such as unigram and n-gram frequencies, concreteness, imageability, and conventionality. Depending on the target word, more than one possible alternative substitution may fulfill the criteria and produce an acceptable result (e.g. acquire/buy/purchase a painting). Resources such as thesauri, containing semantically related words (Fellbaum, 1998; Lin, 1998), and word norms, with information about word properties (Nelson et al., 2004), may be used to inform these tasks. Various initiatives for collecting word norms resulted in datasets such as the South Florida Association Norms (Nelson et al., 2004), SimLex-999 (Hill et al., 2015), Hyperlex (Vuli´c et al., 2016) and Rare Words (Luong et al., 2013). These resources form valuable gold standards for evaluating the quality of a variety of tasks and applications, including text simplification and machine translation. However, they often concentrate on single words as targets. The collection of norms"
W17-6941,W13-3512,0,0.0410611,"y. Depending on the target word, more than one possible alternative substitution may fulfill the criteria and produce an acceptable result (e.g. acquire/buy/purchase a painting). Resources such as thesauri, containing semantically related words (Fellbaum, 1998; Lin, 1998), and word norms, with information about word properties (Nelson et al., 2004), may be used to inform these tasks. Various initiatives for collecting word norms resulted in datasets such as the South Florida Association Norms (Nelson et al., 2004), SimLex-999 (Hill et al., 2015), Hyperlex (Vuli´c et al., 2016) and Rare Words (Luong et al., 2013). These resources form valuable gold standards for evaluating the quality of a variety of tasks and applications, including text simplification and machine translation. However, they often concentrate on single words as targets. The collection of norms for longer units is particularly challenging due to the arbitrary interactions between their member words, particularly if they involve multiword expressions (MWEs), such as nominal compounds or verbal idioms. Available MWE datasets often target specific types of MWEs such as verb-particle constructions (McCarthy et al., 2003) and noun compounds"
W17-6941,W03-1810,0,0.0628767,"2016) and Rare Words (Luong et al., 2013). These resources form valuable gold standards for evaluating the quality of a variety of tasks and applications, including text simplification and machine translation. However, they often concentrate on single words as targets. The collection of norms for longer units is particularly challenging due to the arbitrary interactions between their member words, particularly if they involve multiword expressions (MWEs), such as nominal compounds or verbal idioms. Available MWE datasets often target specific types of MWEs such as verb-particle constructions (McCarthy et al., 2003) and noun compounds (Reddy et al., 2011), and tend to focus on numerical scores that model compositionality and conventionality. Resources with lexical substitutes or paraphrases for MWEs are rare and often only include compositional expressions (Hendrickx et al., 2013). However, MWEs may also involve some degree of semantic or statistical idiosyncrasy with respect to regular combinations (Baldwin and Kim, 2010) and these may have an impact on the quality of the collected data. For instance, there may be less agreement among annotators for an idiomatic nominal compound like Black Friday as it"
W17-6941,S07-1009,0,0.0293613,"of these characteristics on the suggestions of lexical substitution made by native speakers. No strong correlations are found for these factors on the number or type of responses provided. However, a significant effect of compositionality is found in the use of one of the component words (head or modifier) as a substitute. The resulting resource, LexSubNC, contains over 1,500 manually validated substitutes for 180 compounds, further classified according to the type of response. 1 Introduction In tasks involving lexical substitution, alternatives need to be identified for a given target word (McCarthy and Navigli, 2007, 2009), usually in a particular context. Candidates can be chosen to maximize word properties that are relevant for the particular task, such as unigram and n-gram frequencies, concreteness, imageability, and conventionality. Depending on the target word, more than one possible alternative substitution may fulfill the criteria and produce an acceptable result (e.g. acquire/buy/purchase a painting). Resources such as thesauri, containing semantically related words (Fellbaum, 1998; Lin, 1998), and word norms, with information about word properties (Nelson et al., 2004), may be used to inform th"
W17-6941,S10-1002,0,0.0573036,"Missing"
W17-6941,P16-2026,1,0.850379,"is publicly available at http://pageperso.lif.univ-mrs.fr/~carlos.ramisch/ ?page=downloads/compounds&lang=en. any part of speech, as long as the resulting paraphrase was a well-formed noun phrase. In all of these cases, the compounds were compositional: they could be paraphrased using combinations of their parts, and lexical substitution could be performed for each component individually. Other MWE datasets contain compositionality assessment, and, as a consequence, they also include idiomatic cases. Datasets for nominal compound compositionality are available in English (Reddy et al., 2011; Ramisch et al., 2016; Farahmand et al., 2015), German (Roller et al., 2013), Portuguese and French (Ramisch et al., 2016), and for noun-verb expressions in Basque (Gurrutxaga and Alegria, 2013). For instance, Reddy et al. (2011) collected numerical scores for 90 English nominal compounds regarding their compositionality. Data for each compound and component word was gathered through crowdsourcing using a 6-point scale ranging from totally idiomatic (0) to fully compositional (5). Ramisch et al. (2016) extended this set with additional compounds and also applied it to other languages, generating a total of 180 com"
W17-6941,I11-1024,0,0.14838,"These resources form valuable gold standards for evaluating the quality of a variety of tasks and applications, including text simplification and machine translation. However, they often concentrate on single words as targets. The collection of norms for longer units is particularly challenging due to the arbitrary interactions between their member words, particularly if they involve multiword expressions (MWEs), such as nominal compounds or verbal idioms. Available MWE datasets often target specific types of MWEs such as verb-particle constructions (McCarthy et al., 2003) and noun compounds (Reddy et al., 2011), and tend to focus on numerical scores that model compositionality and conventionality. Resources with lexical substitutes or paraphrases for MWEs are rare and often only include compositional expressions (Hendrickx et al., 2013). However, MWEs may also involve some degree of semantic or statistical idiosyncrasy with respect to regular combinations (Baldwin and Kim, 2010) and these may have an impact on the quality of the collected data. For instance, there may be less agreement among annotators for an idiomatic nominal compound like Black Friday as it may be perceived as being related to var"
W17-6941,W13-1005,0,0.0175762,".fr/~carlos.ramisch/ ?page=downloads/compounds&lang=en. any part of speech, as long as the resulting paraphrase was a well-formed noun phrase. In all of these cases, the compounds were compositional: they could be paraphrased using combinations of their parts, and lexical substitution could be performed for each component individually. Other MWE datasets contain compositionality assessment, and, as a consequence, they also include idiomatic cases. Datasets for nominal compound compositionality are available in English (Reddy et al., 2011; Ramisch et al., 2016; Farahmand et al., 2015), German (Roller et al., 2013), Portuguese and French (Ramisch et al., 2016), and for noun-verb expressions in Basque (Gurrutxaga and Alegria, 2013). For instance, Reddy et al. (2011) collected numerical scores for 90 English nominal compounds regarding their compositionality. Data for each compound and component word was gathered through crowdsourcing using a 6-point scale ranging from totally idiomatic (0) to fully compositional (5). Ramisch et al. (2016) extended this set with additional compounds and also applied it to other languages, generating a total of 180 compounds per language for English, Portuguese, and French"
W17-6941,C98-2122,0,\N,Missing
W17-6941,J17-4004,0,\N,Missing
W18-1209,P14-1023,0,0.0661863,"ons of rare and out-ofvocabulary words. 1 Introduction Low dimensional word representations (embeddings) have become a key component in modern NLP systems for language modeling, parsing, sentiment classification, and many others. These embeddings are usually derived by employing the distributional hypothesis: that similar words appear in similar contexts (Harris, 1954). The models that perform the word embedding can be divided into two classes: predictive, which learn a target or context word distribution, and counting, which use a raw, weighted, or factored word-context co-occurrence matrix (Baroni et al., 2014). The most well-known predictive model, which has become eponymous with word embedding, is word2vec (Mikolov et al., 2013a). Popular counting models include PPMI-SVD (Levy et al., 2014), GloVe (Pennington et al., 2014), and LexVec (Salle et al., 2016b). These models all learn word-level representations, which presents two main problems: 1) Learned information is not explicitly shared among the representations as each word has an independent vector. 2) There is no clear way to represent out-of-vocabulary (OOV) words. fastText (Bojanowski et al., 2017) addresses these issues in the Skip-gram wor"
W18-1209,W13-3512,0,0.509279,"ctors obtained running an SVD of a four-gram to four-gram co-occurrence matrix. Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed. There are many models that use character-level subword information to form word representations (Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016; Wieting et al., 2016; Verwimp et al., 2017), as well as fastText (the model on which we base our work). Closely related are models that use morphological segmentation in learning word representations (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Mitchell and Steedman, 2015; Cotterell and Sch¨utze, 2015; Bhatia et al., 2016). Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, Cotterell et al. (2016) and V´ulic et al. (2017) retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to ge"
W18-1209,D16-1047,0,0.0401644,"vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed. There are many models that use character-level subword information to form word representations (Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016; Wieting et al., 2016; Verwimp et al., 2017), as well as fastText (the model on which we base our work). Closely related are models that use morphological segmentation in learning word representations (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Mitchell and Steedman, 2015; Cotterell and Sch¨utze, 2015; Bhatia et al., 2016). Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, Cotterell et al. (2016) and V´ulic et al. (2017) retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words. 3 1 Lwc = (u> vc − P P M Iwc )2 (2) 2 w k 1X 2 Lw = Eci ∼Pn (c) (u> w vci − P P M Iwci ) 2"
W18-1209,Q17-1010,0,0.310979,"or factored word-context co-occurrence matrix (Baroni et al., 2014). The most well-known predictive model, which has become eponymous with word embedding, is word2vec (Mikolov et al., 2013a). Popular counting models include PPMI-SVD (Levy et al., 2014), GloVe (Pennington et al., 2014), and LexVec (Salle et al., 2016b). These models all learn word-level representations, which presents two main problems: 1) Learned information is not explicitly shared among the representations as each word has an independent vector. 2) There is no clear way to represent out-of-vocabulary (OOV) words. fastText (Bojanowski et al., 2017) addresses these issues in the Skip-gram word2vec model by 1 Our implementation of subword LexVec is available at https://github.com/alexandres/lexvec 66 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 66–71 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics OOV words by quantitatively evaluating nearestneighbors. Results show that, like fastText, both LexVec n-gram and (to a lesser degree) unsupervised morpheme models give coherent answers. This paper discusses related word (§2), introduces the subword LexVec model (§3), describe"
W18-1209,N13-1090,0,0.754388,"y component in modern NLP systems for language modeling, parsing, sentiment classification, and many others. These embeddings are usually derived by employing the distributional hypothesis: that similar words appear in similar contexts (Harris, 1954). The models that perform the word embedding can be divided into two classes: predictive, which learn a target or context word distribution, and counting, which use a raw, weighted, or factored word-context co-occurrence matrix (Baroni et al., 2014). The most well-known predictive model, which has become eponymous with word embedding, is word2vec (Mikolov et al., 2013a). Popular counting models include PPMI-SVD (Levy et al., 2014), GloVe (Pennington et al., 2014), and LexVec (Salle et al., 2016b). These models all learn word-level representations, which presents two main problems: 1) Learned information is not explicitly shared among the representations as each word has an independent vector. 2) There is no clear way to represent out-of-vocabulary (OOV) words. fastText (Bojanowski et al., 2017) addresses these issues in the Skip-gram word2vec model by 1 Our implementation of subword LexVec is available at https://github.com/alexandres/lexvec 66 Proceedings"
W18-1209,P15-1126,0,0.0179972,"currence matrix. Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed. There are many models that use character-level subword information to form word representations (Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016; Wieting et al., 2016; Verwimp et al., 2017), as well as fastText (the model on which we base our work). Closely related are models that use morphological segmentation in learning word representations (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Mitchell and Steedman, 2015; Cotterell and Sch¨utze, 2015; Bhatia et al., 2016). Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, Cotterell et al. (2016) and V´ulic et al. (2017) retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words. 3 1 Lwc = (u> vc − P P M Iwc )2 (2) 2 w"
W18-1209,W16-1603,0,0.0322851,"loss functions are minimized for every observed (w, c) pair and target word w: Related Work Word embeddings that leverage subword information were first introduced by Sch¨utze (1993) which represented a word of as the sum of four-gram vectors obtained running an SVD of a four-gram to four-gram co-occurrence matrix. Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed. There are many models that use character-level subword information to form word representations (Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016; Wieting et al., 2016; Verwimp et al., 2017), as well as fastText (the model on which we base our work). Closely related are models that use morphological segmentation in learning word representations (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Mitchell and Steedman, 2015; Cotterell and Sch¨utze, 2015; Bhatia et al., 2016). Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, Cotterell et al"
W18-1209,S17-2001,0,0.0148051,"le.com avillavicencio@inf.ufrgs.br Abstract representing a word by the sum of a unique vector and a set of shared character n-grams (from hereon simply referred to as n-grams) vectors. This addresses both issues above as learned information is shared through the n-gram vectors and from these OOV word representations can be constructed. In this paper we propose incorporating subword information into counting models using a strategy similar to fastText. We use LexVec as the counting model as it generally outperforms PPMI-SVD and GloVe on intrinsic and extrinsic evaluations (Salle et al., 2016a; Cer et al., 2017; Wohlgenannt et al., 2017; Konkol et al., 2017), but the method proposed here should transfer to GloVe unchanged. The LexVec objective is modified 1 such that a word’s vector is the sum of all its subword vectors. We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor (Virpioja, 2013) to learn whether more linguistically motivated subwords offer any advantage over simple n-grams. To evaluate the impact subword information has on in-vocabulary (IV) word representations, we run intrinsic evaluations consisting of word similarity and wor"
W18-1209,W16-2504,0,0.0251924,"tion has on in-vocabulary (IV) word representations, we run intrinsic evaluations consisting of word similarity and word analogy tasks. The incorporation of subword information results in similar gains (and losses) to that of fastText over Skip-gram. Whereas incorporating n-gram subwords tends to capture more syntactic information, unsupervised morphemes better preserve semantics while also improving syntactic results. Given that intrinsic performance can correlate poorly with performance on downstream tasks (Tsvetkov et al., 2015), we also conduct evaluation using the VecEval suite of tasks (Nayak et al., 2016), in which all subword models, including fastText, show no significant improvement over word-level models. We verify the model’s ability to represent The positive effect of adding subword information to word embeddings has been demonstrated for predictive models. In this paper we investigate whether similar benefits can also be derived from incorporating subwords into counting models. We evaluate the impact of different types of subwords (n-grams and unsupervised morphemes), with results confirming the importance of subword information in learning representations of rare and out-ofvocabulary w"
W18-1209,N15-1140,0,0.0694461,"Missing"
W18-1209,D14-1162,0,0.106919,"nd many others. These embeddings are usually derived by employing the distributional hypothesis: that similar words appear in similar contexts (Harris, 1954). The models that perform the word embedding can be divided into two classes: predictive, which learn a target or context word distribution, and counting, which use a raw, weighted, or factored word-context co-occurrence matrix (Baroni et al., 2014). The most well-known predictive model, which has become eponymous with word embedding, is word2vec (Mikolov et al., 2013a). Popular counting models include PPMI-SVD (Levy et al., 2014), GloVe (Pennington et al., 2014), and LexVec (Salle et al., 2016b). These models all learn word-level representations, which presents two main problems: 1) Learned information is not explicitly shared among the representations as each word has an independent vector. 2) There is no clear way to represent out-of-vocabulary (OOV) words. fastText (Bojanowski et al., 2017) addresses these issues in the Skip-gram word2vec model by 1 Our implementation of subword LexVec is available at https://github.com/alexandres/lexvec 66 Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 66–71 c New Orleans, Louisiana,"
W18-1209,P16-1156,0,0.0769155,"Missing"
W18-1209,C14-1015,0,0.0306198,"to four-gram co-occurrence matrix. Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed. There are many models that use character-level subword information to form word representations (Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016; Wieting et al., 2016; Verwimp et al., 2017), as well as fastText (the model on which we base our work). Closely related are models that use morphological segmentation in learning word representations (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Mitchell and Steedman, 2015; Cotterell and Sch¨utze, 2015; Bhatia et al., 2016). Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, Cotterell et al. (2016) and V´ulic et al. (2017) retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words. 3 1 Lwc ="
W18-1209,J15-4004,0,0.0614757,"Missing"
W18-1209,P16-2068,1,0.927611,", Brazil alex@alexsalle.com avillavicencio@inf.ufrgs.br Abstract representing a word by the sum of a unique vector and a set of shared character n-grams (from hereon simply referred to as n-grams) vectors. This addresses both issues above as learned information is shared through the n-gram vectors and from these OOV word representations can be constructed. In this paper we propose incorporating subword information into counting models using a strategy similar to fastText. We use LexVec as the counting model as it generally outperforms PPMI-SVD and GloVe on intrinsic and extrinsic evaluations (Salle et al., 2016a; Cer et al., 2017; Wohlgenannt et al., 2017; Konkol et al., 2017), but the method proposed here should transfer to GloVe unchanged. The LexVec objective is modified 1 such that a word’s vector is the sum of all its subword vectors. We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor (Virpioja, 2013) to learn whether more linguistically motivated subwords offer any advantage over simple n-grams. To evaluate the impact subword information has on in-vocabulary (IV) word representations, we run intrinsic evaluations consisting of word"
W18-1209,I17-1023,0,0.0140911,"representing a word by the sum of a unique vector and a set of shared character n-grams (from hereon simply referred to as n-grams) vectors. This addresses both issues above as learned information is shared through the n-gram vectors and from these OOV word representations can be constructed. In this paper we propose incorporating subword information into counting models using a strategy similar to fastText. We use LexVec as the counting model as it generally outperforms PPMI-SVD and GloVe on intrinsic and extrinsic evaluations (Salle et al., 2016a; Cer et al., 2017; Wohlgenannt et al., 2017; Konkol et al., 2017), but the method proposed here should transfer to GloVe unchanged. The LexVec objective is modified 1 such that a word’s vector is the sum of all its subword vectors. We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor (Virpioja, 2013) to learn whether more linguistically motivated subwords offer any advantage over simple n-grams. To evaluate the impact subword information has on in-vocabulary (IV) word representations, we run intrinsic evaluations consisting of word similarity and word analogy tasks. The incorporation of subword in"
W18-1209,D15-1243,0,0.0337692,"words offer any advantage over simple n-grams. To evaluate the impact subword information has on in-vocabulary (IV) word representations, we run intrinsic evaluations consisting of word similarity and word analogy tasks. The incorporation of subword information results in similar gains (and losses) to that of fastText over Skip-gram. Whereas incorporating n-gram subwords tends to capture more syntactic information, unsupervised morphemes better preserve semantics while also improving syntactic results. Given that intrinsic performance can correlate poorly with performance on downstream tasks (Tsvetkov et al., 2015), we also conduct evaluation using the VecEval suite of tasks (Nayak et al., 2016), in which all subword models, including fastText, show no significant improvement over word-level models. We verify the model’s ability to represent The positive effect of adding subword information to word embeddings has been demonstrated for predictive models. In this paper we investigate whether similar benefits can also be derived from incorporating subwords into counting models. We evaluate the impact of different types of subwords (n-grams and unsupervised morphemes), with results confirming the importance"
W18-1209,E17-1040,0,0.0551529,"Missing"
W18-1209,P17-1006,0,0.0576131,"Missing"
W18-1209,D16-1157,0,0.0247433,"ry observed (w, c) pair and target word w: Related Work Word embeddings that leverage subword information were first introduced by Sch¨utze (1993) which represented a word of as the sum of four-gram vectors obtained running an SVD of a four-gram to four-gram co-occurrence matrix. Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed. There are many models that use character-level subword information to form word representations (Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016; Wieting et al., 2016; Verwimp et al., 2017), as well as fastText (the model on which we base our work). Closely related are models that use morphological segmentation in learning word representations (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Mitchell and Steedman, 2015; Cotterell and Sch¨utze, 2015; Bhatia et al., 2016). Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, Cotterell et al. (2016) and V´ulic et al. (2017) retrof"
W18-1209,D15-1176,0,\N,Missing
W18-1209,W14-1618,0,\N,Missing
W19-5101,J17-4005,0,0.0694648,"Missing"
W19-5101,J19-1001,1,0.887029,"Missing"
