2004.jeptalnrecital-long.13,W02-1402,0,0.0464137,"Missing"
2004.jeptalnrecital-long.13,C94-1084,1,0.703648,"Missing"
2004.jeptalnrecital-long.13,C02-1166,0,0.0777862,"Missing"
2004.jeptalnrecital-long.13,P99-1067,0,0.365691,"Missing"
2007.jeptalnrecital-poster.9,2006.jeptalnrecital-poster.16,1,0.666092,"Missing"
2009.jeptalnrecital-court.10,P07-1056,0,0.0379372,"Missing"
2009.jeptalnrecital-court.10,W02-1011,0,0.0107397,"Missing"
2011.jeptalnrecital-long.21,baccianella-etal-2010-sentiwordnet,0,0.014958,"Missing"
2011.jeptalnrecital-long.21,H05-1045,0,0.0754371,"Missing"
2011.jeptalnrecital-long.21,P10-2049,0,0.0317463,"Missing"
2011.jeptalnrecital-long.21,W06-0301,0,0.0785931,"Missing"
2011.jeptalnrecital-long.21,ruppenhofer-etal-2008-finding,0,0.0417156,"Missing"
2011.jeptalnrecital-long.21,P02-1053,0,0.00471925,"Missing"
2011.jeptalnrecital-long.21,W07-1216,0,0.0234626,"Missing"
2012.amta-papers.5,C10-1073,0,0.254525,"kly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by lowering its comparability or adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) show that the discourse categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo and Gaussier (2010) show that they improve the quality of a lexicon if they improve the comparability of the corpus by selecting a smaller - but more comparable - corpus from an initial set of documents. Consequently, one solution for increasing the number or translation pairs is to focus on identifying translation variants. This paper explores the feasibility of identifying ”fertile” translations in comparable corpora. In parallel texts processing, the notion of fertility has been defined by Brown et al. (1993). They defined the fertility of a source word e as the number of target words to which e is connected"
2012.amta-papers.5,J93-2003,0,0.0559399,"categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo and Gaussier (2010) show that they improve the quality of a lexicon if they improve the comparability of the corpus by selecting a smaller - but more comparable - corpus from an initial set of documents. Consequently, one solution for increasing the number or translation pairs is to focus on identifying translation variants. This paper explores the feasibility of identifying ”fertile” translations in comparable corpora. In parallel texts processing, the notion of fertility has been defined by Brown et al. (1993). They defined the fertility of a source word e as the number of target words to which e is connected in a randomly selected alignment. Similarly, we call a fertile translation a translation pair in which the target term has more words than the source term. We propose to identify such translations with a method mixing morphological analysis and compositional translation : (i) the source term is decomposed into morphemes: postmenopausal is split into post- + menopause1 ; (ii) the morphemes are translated as bound morphemes or fully autonomous words: post- becomes post- or apr`es and menopause b"
2012.amta-papers.5,J96-2004,0,0.108904,"Missing"
2012.amta-papers.5,E09-1016,0,0.414699,"tiword term to multi-word term alignment and uses lexical words3 as atomic components : rate of evap3 as opposed to grammatical words: preposition, determiners, etc. oration is translated into French taux d’´evaporation by translating rate as taux and evaporation as e´ vaporation using dictionary lookup. Recomposition may be done by permutating the translated components (Morin and Daille, 2010) or with translation patterns (Baldwin and Tanaka, 2004). Sublexical compositional translation deals with single-word term translation. The atomic components are subparts of the source single-word term. Cartoni (2009) translates neologisms created by prefixation with a special formalism called Bilingual Lexeme Formation Rules. Atomic components are the prefix and the lexical base: Italian neologism anticonstituzionale ’anticonstitution’ is translated into French anticonstitution by translating the prefix anti- as anti- and the lexical base constituzionale as constitution. Weller et al. (2011) translate two types of single-word term. German single-word term formed by the concatenation of two neoclassical roots are decomposed into these two roots, then the roots are translated into target language roots and"
2012.amta-papers.5,I05-1062,1,0.801395,"bilingual lexicon extraction, compositional translation (CT ) consists in decomposing the source term into atomic components (D), translating these components into the target language (T ), recomposing the translated components into target terms (R) and finally filtering the generated translations with a selection function (S): CT (“ab”) = S(R(T (D(“ab”)))) = S(R(T ({a, b}))) = S(R({T (a) × T (b)})) = S(R({A , B})) = S({A , B}, {B , A}) = “BA” Related work Most of the research work in lexicon extraction from comparable corpora concentrates on samelength term alignment. To our knowledge, only Daille and Morin (2005) and Weller et al. (2011) tried to align terms of different lengths. Daille and Morin (2005) focus on the specific case of multiword terms whose meanings are non-compositional and tried to align these multi-word terms with either single-word terms or multi-word terms using a context-based approach2 .Weller et al. (2011) concentrate on aligning German N OUN -N OUN compounds to N OUN N OUN and N OUN P REP N OUN 1 We use the following notations for morphemes: trailing hyphen for prefixes (a-), leading hyphen for suffixes (-a), both for confixes (-a-) and no hyphen for autonomous morphemes (a). Mo"
2012.amta-papers.5,W97-0119,0,0.30839,"c case of multiword terms whose meanings are non-compositional and tried to align these multi-word terms with either single-word terms or multi-word terms using a context-based approach2 .Weller et al. (2011) concentrate on aligning German N OUN -N OUN compounds to N OUN N OUN and N OUN P REP N OUN 1 We use the following notations for morphemes: trailing hyphen for prefixes (a-), leading hyphen for suffixes (-a), both for confixes (-a-) and no hyphen for autonomous morphemes (a). Morpheme boundaries are represented by a plus sign (+). 2 Context-based methods were introduced by Rapp (1995) and Fung (1997). They consist in comparing the contexts in which the source and target terms occur. Their drawback is that they need the source and target terms to be very frequent. Principle of compositional translation where “ab” is a source term composed of a and b, “BA” is a target term composed of B and A and there exists a bilingual resource linking a to A and b to B. 2.2 Implementations of compositional translation Existing implementations differ on the kind of atomic components they use for translation. Lexical compositional translation (Grefenstette, 1999; Baldwin and Tanaka, 2004; Robitaille et al."
2012.amta-papers.5,W04-3208,0,0.0854358,"anslated texts tend to bear features like explication, simplification, normalization and leveling out. For instance, an English-French comparable corpus may contain the English term post-menopausal but not its “normalized” or “canonical” translation in French (post-m´enopausique). However, there might be some morphological or paraphrastic variants in the French texts like post-m´enopause ’post-menopause’ or apr`es la m´enopause ’after the menopause’. The solution that consists in increasing the size of the corpus in order to find more translation pairs or to extract parallel segments of text (Fung and Cheung, 2004; Rauf and Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domain-specific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by lowering its comparability or adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than"
2012.amta-papers.5,1999.tc-1.8,0,0.02692,"xt-based methods were introduced by Rapp (1995) and Fung (1997). They consist in comparing the contexts in which the source and target terms occur. Their drawback is that they need the source and target terms to be very frequent. Principle of compositional translation where “ab” is a source term composed of a and b, “BA” is a target term composed of B and A and there exists a bilingual resource linking a to A and b to B. 2.2 Implementations of compositional translation Existing implementations differ on the kind of atomic components they use for translation. Lexical compositional translation (Grefenstette, 1999; Baldwin and Tanaka, 2004; Robitaille et al., 2006; Morin and Daille, 2010) deals with multiword term to multi-word term alignment and uses lexical words3 as atomic components : rate of evap3 as opposed to grammatical words: preposition, determiners, etc. oration is translated into French taux d’´evaporation by translating rate as taux and evaporation as e´ vaporation using dictionary lookup. Recomposition may be done by permutating the translated components (Morin and Daille, 2010) or with translation patterns (Baldwin and Tanaka, 2004). Sublexical compositional translation deals with single"
2012.amta-papers.5,I11-1097,0,0.0405941,"was present in the target texts. The final test set for English-to-French experiments contains 1839 morphologically constructed source terms. The test set for English-to-German contains 1824 source terms. 4.3 Resources used in the translation step T Tables 2 and 3 show the size of the resources we used for translation. General language dictionary We used the general language dictionary which is part of the linguistic analysis suite X ELDA. Domain-specific dictionary We built this resource automatically by extracting pairs of cognates from the comparable corpora. We used the same technique as (Hauer and Kondrak, 2011): a SVM classifier trained on examples taken from online dictionaries6 . Morpheme translation table To our knowledge, there exists no publicly available morphology-based bilingual dictionary. Consequently, we asked trans6 http://www.dicts.info/uddl.php lators to create an ad hoc morpheme translation table for our experiment. This morpheme translation table links the English bound morphemes contained in the source terms to their French or German equivalents. The equivalents can be bound morphemes or lexical items. In order to handle the variation phenomena described in section 2.3, we used a di"
2012.amta-papers.5,P07-1084,1,0.915502,"Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domain-specific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by lowering its comparability or adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) show that the discourse categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo and Gaussier (2010) show that they improve the quality of a lexicon if they improve the comparability of the corpus by selecting a smaller - but more comparable - corpus from an initial set of documents. Consequently, one solution for increasing the number or translation pairs is to focus on identifying translation variants. This paper explores the feasibility of identifying ”fertile” translations in comparable corpora. In parallel texts processing, the notion of fertil"
2012.amta-papers.5,C00-2163,0,0.0365669,"n function uses the entries of the bound morphemes translation table (242 entries) and a list of 85k lexical items composed of the entries of the general language dictionary and English words extracted from the Leipzig Corpus (Quasthoff et al., 2006) which is a general language corpus. 5 5.1 texts of the corpus (Weller et al., 2011; Morin and Daille, 2010) or by using a search engine (Robitaille et al., 2006). Unlike alignment evaluation in parallel texts, there is no reference alignmens to which the selected translations can be compared and we cannot use standard evaluation metrics like AER (Och and Ney, 2000). It is also difficult to find reference lexicons in specific domains since the goal of the extraction process is to create such lexicons. Furthermore, we also wish to evaluate if the algorithm can identify non-canonical translations which, by definition, can not be found in a reference lexicon. Usually, the candidate translations are annotated manually as correct or incorrect by native speakers. Baldwin and Takana (2004) use two standards for evaluation: gold-standard, silver-standard. Gold-standard is the set of candidate translations which correspond to canonical, reference translations. Si"
2012.amta-papers.5,quasthoff-etal-2006-corpus,0,0.0808403,"xes EN→FR 38k→60k 6.7k→6.7k 242→729 50→134 185→574 7→21 EN→DE 38k→70k 6.4k→6.4k 242→761 50→166 185→563 7→32 Table 2: Nb. of entries in the multilingual resources Synonyms Morphol. EN→EN 5.1k→7.6k 5.9k→15k FR→FR 2.4k→3.2k 7.1k→18k DE→DE 4.2k→4.9k 7.4k→16k Table 3: Nb. of entries in the monolingual resources 4.4 Resources used in the decomposition step (D) The decomposition function uses the entries of the bound morphemes translation table (242 entries) and a list of 85k lexical items composed of the entries of the general language dictionary and English words extracted from the Leipzig Corpus (Quasthoff et al., 2006) which is a general language corpus. 5 5.1 texts of the corpus (Weller et al., 2011; Morin and Daille, 2010) or by using a search engine (Robitaille et al., 2006). Unlike alignment evaluation in parallel texts, there is no reference alignmens to which the selected translations can be compared and we cannot use standard evaluation metrics like AER (Och and Ney, 2000). It is also difficult to find reference lexicons in specific domains since the goal of the extraction process is to create such lexicons. Furthermore, we also wish to evaluate if the algorithm can identify non-canonical translation"
2012.amta-papers.5,P95-1050,0,0.383095,"s on the specific case of multiword terms whose meanings are non-compositional and tried to align these multi-word terms with either single-word terms or multi-word terms using a context-based approach2 .Weller et al. (2011) concentrate on aligning German N OUN -N OUN compounds to N OUN N OUN and N OUN P REP N OUN 1 We use the following notations for morphemes: trailing hyphen for prefixes (a-), leading hyphen for suffixes (-a), both for confixes (-a-) and no hyphen for autonomous morphemes (a). Morpheme boundaries are represented by a plus sign (+). 2 Context-based methods were introduced by Rapp (1995) and Fung (1997). They consist in comparing the contexts in which the source and target terms occur. Their drawback is that they need the source and target terms to be very frequent. Principle of compositional translation where “ab” is a source term composed of a and b, “BA” is a target term composed of B and A and there exists a bilingual resource linking a to A and b to B. 2.2 Implementations of compositional translation Existing implementations differ on the kind of atomic components they use for translation. Lexical compositional translation (Grefenstette, 1999; Baldwin and Tanaka, 2004; R"
2012.amta-papers.5,E09-1003,0,0.0594834,"bear features like explication, simplification, normalization and leveling out. For instance, an English-French comparable corpus may contain the English term post-menopausal but not its “normalized” or “canonical” translation in French (post-m´enopausique). However, there might be some morphological or paraphrastic variants in the French texts like post-m´enopause ’post-menopause’ or apr`es la m´enopause ’after the menopause’. The solution that consists in increasing the size of the corpus in order to find more translation pairs or to extract parallel segments of text (Fung and Cheung, 2004; Rauf and Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domain-specific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by lowering its comparability or adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2"
2012.amta-papers.5,E06-1029,0,0.744304,") and Fung (1997). They consist in comparing the contexts in which the source and target terms occur. Their drawback is that they need the source and target terms to be very frequent. Principle of compositional translation where “ab” is a source term composed of a and b, “BA” is a target term composed of B and A and there exists a bilingual resource linking a to A and b to B. 2.2 Implementations of compositional translation Existing implementations differ on the kind of atomic components they use for translation. Lexical compositional translation (Grefenstette, 1999; Baldwin and Tanaka, 2004; Robitaille et al., 2006; Morin and Daille, 2010) deals with multiword term to multi-word term alignment and uses lexical words3 as atomic components : rate of evap3 as opposed to grammatical words: preposition, determiners, etc. oration is translated into French taux d’´evaporation by translating rate as taux and evaporation as e´ vaporation using dictionary lookup. Recomposition may be done by permutating the translated components (Morin and Daille, 2010) or with translation patterns (Baldwin and Tanaka, 2004). Sublexical compositional translation deals with single-word term translation. The atomic components are s"
2015.jeptalnrecital-court.17,saggion-2004-identifying,0,0.0948425,"Missing"
2015.jeptalnrecital-court.20,de-loupy-el-beze-2000-using,0,0.0933033,"Missing"
2015.jeptalnrecital-court.20,1992.tmi-1.9,0,0.215236,"Missing"
2015.jeptalnrecital-court.20,N09-2059,0,0.0655745,"Missing"
2015.jeptalnrecital-court.20,P95-1026,0,0.663919,"Missing"
2015.jeptalnrecital-court.20,C92-2070,0,\N,Missing
2016.jeptalnrecital-long.14,P93-1002,0,0.662704,"Missing"
2016.jeptalnrecital-long.14,C02-2020,0,0.175148,"Missing"
2016.jeptalnrecital-long.14,P91-1017,0,0.676568,"Missing"
2016.jeptalnrecital-long.14,C02-1166,0,0.151418,"Missing"
2016.jeptalnrecital-long.14,J93-1003,0,0.190903,"Missing"
2016.jeptalnrecital-long.14,W95-0114,0,0.464512,"Missing"
2016.jeptalnrecital-long.14,W97-0119,0,0.372832,"Missing"
2016.jeptalnrecital-long.14,I13-1196,1,0.814261,"Missing"
2016.jeptalnrecital-long.14,W05-0809,0,0.108135,"Missing"
2016.jeptalnrecital-long.14,P07-1084,1,0.812567,"Missing"
2016.jeptalnrecital-long.14,P14-1121,1,0.851589,"Missing"
2016.jeptalnrecital-long.14,2001.mtsummit-papers.46,0,0.234148,"Missing"
2016.jeptalnrecital-long.14,P95-1050,0,0.521078,"Missing"
2016.jeptalnrecital-long.14,P99-1067,0,0.420831,"Missing"
2016.jeptalnrecital-long.14,I11-2003,1,0.893912,"Missing"
2016.jeptalnrecital-long.14,W99-0602,0,0.195673,"Missing"
2016.jeptalnrecital-long.18,C10-2042,0,0.0463208,"Missing"
2016.jeptalnrecital-long.18,P14-1119,0,0.0299692,"Missing"
2016.jeptalnrecital-poster.17,S15-2065,0,0.0328462,"Missing"
2016.jeptalnrecital-poster.17,E03-1009,0,0.124599,"Missing"
2016.jeptalnrecital-poster.17,S15-2128,0,0.0498756,"Missing"
2016.jeptalnrecital-poster.17,P10-1052,0,0.0298885,"Missing"
2016.jeptalnrecital-poster.17,S15-2127,0,0.0308444,"Missing"
2016.jeptalnrecital-poster.17,S15-2083,0,0.0523059,"Missing"
2016.jeptalnrecital-poster.17,H05-1044,0,0.123924,"Missing"
2016.jeptalnrecital-poster.28,C04-1095,0,0.0634623,"Missing"
2019.jeptalnrecital-court.28,P18-2026,0,0.031103,"Missing"
2019.jeptalnrecital-court.28,J93-1003,0,0.0902183,"Missing"
2019.jeptalnrecital-court.28,D15-1181,0,0.0548646,"Missing"
2019.jeptalnrecital-tia.1,C16-2015,0,0.165963,"Missing"
2019.jeptalnrecital-tia.1,N18-2105,0,0.091569,"Missing"
2019.jeptalnrecital-tia.1,P16-4003,1,0.360451,"Missing"
2019.jeptalnrecital-tia.1,W03-1802,1,0.870979,"Missing"
2019.jeptalnrecital-tia.1,2019.jeptalnrecital-tia.1,1,0.0530913,"Missing"
2019.jeptalnrecital-tia.1,L18-1284,0,0.195291,"Missing"
2020.coling-main.549,Q19-1011,0,0.0255003,"Missing"
2020.coling-main.549,A00-2004,0,0.500584,"segments are merged if they are highly correlated. On the contrary, if their similarity is below a certain threshold, a shift is determined (Hearst, 1997; Riedl and Biemann, 2012). When sufficient topically annotated training data are available, deep neural approaches based on CNN (Wang et al., 2017) or LSTM (Koshorek et al., 2018) can be efficiently applied (Li et al., 2018; Arnold et al., 2019). Until now, text segmentation methods have exclusively addressed data sets lying within the scope of narrative and expository texts or user dialogues texts and sometimes artificially generated data (Choi, 2000; Jeong and Titov, 2010; Glavaˇs et al., 2016; Koshorek et al., 2018). In this paper, we address transcriptions of ancient devotional manuscripts (thereafter also “MS”), from the Middle Ages, known as “books of hours”. Books of hours were used by lay people as a guidance in their daily prayers. They represent an important source of information on the late Middle Ages’ religious and social practices, and provide opportunities for historical analysis in order to better understand the cultures and faiths of the European society. More than 10,000 manuscripts of ca. 300 pages in average are preserv"
2020.coling-main.549,N19-1423,0,0.00606591,"e MS Arsenal 637. The first column refers to transcriptions, the second column to the gold reference of level 1 (Gospel Lections), and the third to the predicted labels of the SVM classifier. We consider the task of segmentation as a classification problem at the line break level. Each line is represented by its corresponding section’s label. We assume that if enough lines of a given section are correctly classified, segmentation can be efficiently performed thanks to a greedy merging approach. To perform line classification, we chose to experiment with Support Vector machines (SVM) and BERT (Devlin et al., 2019)6 . For SVM, Tf-Idf features are calculated over unigrams and bigrams at the line level. We also used BERT for multi-class classification (the multi-class resides in all the section labels as depicted in Table 1). Each line is associated with its corresponding class (of level 1, level 2 or level 3). Then, BERT is trained to predict the section label of each line. We also, used BERT for sentence pair classification assuming that more information can be captured if we take advantage of the next line of books of hours to predict the current line label. We refer to this approach as BERT* by contra"
2020.coling-main.549,N09-1040,0,0.260281,"This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons. org/licenses/by/4.0/. 6240 Proceedings of the 28th International Conference on Computational Linguistics, pages 6240–6251 Barcelona, Spain (Online), December 8-13, 2020 of books of hours segmentation as a classification problem and propose a greedy two-step bottom-up approach that achieves significant results on books of hours. 2 Related Work Text segmentation is the task of splitting documents into topically coherent fragments for a better text readability and analysis (Hearst, 1994; Eisenstein, 2009; Glavaˇs et al., 2016). It is also useful in other NLP and IR (Moens and Busser, 2001) applications such as: summarization, document navigation and indexing, passage retrieval, etc. Segmentation can be content-based where each topic is characterised by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers (Fauconnier et al., 2014) whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of unsupervised appr"
2020.coling-main.549,C98-1062,0,0.449671,"rised by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers (Fauconnier et al., 2014) whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of unsupervised approaches exploit lexical cohesion to detect coherent segments (Hearst, 1997; Choi, 2000) thanks to term repetitions (Hearst, 1994), semantic relations using lexical chains (Morris and Hirst, 1991), dictionary (Kozima, 1993), collocation networks (Ferret et al., 1998), or patterns of lexical co-occurrences (Hearst, 1997) such as discourse structures (Nomoto and Nitta, 1994). Early unsupervised methods include: TextTiling, a TF Cosine based approach (Hearst, 1994), LCSeg, based on lexical chains (Galley et al., 2003), U00, a probabilistic dynamic programming approach (Utiyama and Isahara, 2001), TopicTiling, a topic modeling approach based on Latent Dirichlet Analysis (LDA) (Riedl and Biemann, 2012). Glavaˇs et al. (2016) proposed a semantic relatedness graph approach that exploits word embeddings. Alemi and Ginsparg (2015) and Naili et al. (2017) studied t"
2020.coling-main.549,P03-1071,0,0.351987,"or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of unsupervised approaches exploit lexical cohesion to detect coherent segments (Hearst, 1997; Choi, 2000) thanks to term repetitions (Hearst, 1994), semantic relations using lexical chains (Morris and Hirst, 1991), dictionary (Kozima, 1993), collocation networks (Ferret et al., 1998), or patterns of lexical co-occurrences (Hearst, 1997) such as discourse structures (Nomoto and Nitta, 1994). Early unsupervised methods include: TextTiling, a TF Cosine based approach (Hearst, 1994), LCSeg, based on lexical chains (Galley et al., 2003), U00, a probabilistic dynamic programming approach (Utiyama and Isahara, 2001), TopicTiling, a topic modeling approach based on Latent Dirichlet Analysis (LDA) (Riedl and Biemann, 2012). Glavaˇs et al. (2016) proposed a semantic relatedness graph approach that exploits word embeddings. Alemi and Ginsparg (2015) and Naili et al. (2017) studied the contribution of word embeddings on classical segmentation approaches. Text segmentation has also been addressed as a multi-document segmentation problem. Sun et al. (2007) for instance, proposed a method for shared topic detection and topic segmentat"
2020.coling-main.549,S16-2016,0,0.0512384,"Missing"
2020.coling-main.549,2020.lrec-1.97,1,0.318447,"prayers are used in several hours of the day and several times within one copy, thus generating section ambiguities. For historians, automating the generation of table of contents is key to understand this complex historical source. While the building block of the mainstream text segmentation methods is closely related to topical shifts, the liturgical aspect of books of hours exhibits shallow topical relations and a strong correlation between their sections and subsections. As consequence, the topical shift hypothesis becomes inconsistent for this type of data, as has been recently shown in (Hazem et al., 2020). We address the task This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons. org/licenses/by/4.0/. 6240 Proceedings of the 28th International Conference on Computational Linguistics, pages 6240–6251 Barcelona, Spain (Online), December 8-13, 2020 of books of hours segmentation as a classification problem and propose a greedy two-step bottom-up approach that achieves significant results on books of hours. 2 Related Work Text segmentation is the task of splitting documents into topically coherent fragments for a better text readability and an"
2020.coling-main.549,P94-1002,0,0.805753,"ress the task This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons. org/licenses/by/4.0/. 6240 Proceedings of the 28th International Conference on Computational Linguistics, pages 6240–6251 Barcelona, Spain (Online), December 8-13, 2020 of books of hours segmentation as a classification problem and propose a greedy two-step bottom-up approach that achieves significant results on books of hours. 2 Related Work Text segmentation is the task of splitting documents into topically coherent fragments for a better text readability and analysis (Hearst, 1994; Eisenstein, 2009; Glavaˇs et al., 2016). It is also useful in other NLP and IR (Moens and Busser, 2001) applications such as: summarization, document navigation and indexing, passage retrieval, etc. Segmentation can be content-based where each topic is characterised by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers (Fauconnier et al., 2014) whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of"
2020.coling-main.549,J97-1003,0,0.913737,"overarching differences underlying conception about Church. 1 Introduction Text segmentation is essential in many downstream applications including document understanding and navigation, summarization, information retrieval and discourse parsing (Purver, 2011; Riedl and Biemann, 2012; Li et al., 2018). Traditional unsupervised approaches assume a high correlation between segments and subtopics. Therefore, based on a prior text decomposition, two adjacent segments are merged if they are highly correlated. On the contrary, if their similarity is below a certain threshold, a shift is determined (Hearst, 1997; Riedl and Biemann, 2012). When sufficient topically annotated training data are available, deep neural approaches based on CNN (Wang et al., 2017) or LSTM (Koshorek et al., 2018) can be efficiently applied (Li et al., 2018; Arnold et al., 2019). Until now, text segmentation methods have exclusively addressed data sets lying within the scope of narrative and expository texts or user dialogues texts and sometimes artificially generated data (Choi, 2000; Jeong and Titov, 2010; Glavaˇs et al., 2016; Koshorek et al., 2018). In this paper, we address transcriptions of ancient devotional manuscript"
2020.coling-main.549,J15-3002,0,0.0256454,"classical segmentation approaches. Text segmentation has also been addressed as a multi-document segmentation problem. Sun et al. (2007) for instance, proposed a method for shared topic detection and topic segmentation of multiple similar documents based on weighted mutual information, while Jeong and Titov (2010) proposed an unsupervised bayesian approach that models both shared and document-specific topics. Supervised approaches have also modeled semantic cohesion. Some methods performed segmentation at the sentence level to discover Elementary Discourse Units (EDU) (Hernault et al., 2010; Joty et al., 2015) while others focused on dialogue. Neural network approaches have also been applied such as: TextTiling-like embedding approach for query-reply dialogue segmentation (Song et al., 2016), multi-party dialogue for EDU using sequential model (Shi and Huang, 2019) and reinforcement learning (Takanobu et al., 2018). Recently, Li et al. (2018) proposed SegBot, a bidirectional RNN coupled with a pointer network that addresses both topic segmentation and EDU. Also, LSTM or CNN based approaches have been proposed, for instance through bidirectional layers (Sheikh et al., 2017), sentence embedding-based"
2020.coling-main.549,C14-1005,0,0.0173235,"(Morris, 1988) as well as synthetic data sets (Choi, 2000; Galley et al., 2003) were often used. Later on, data sets with hierarchical structure were addressed, which required a more fine-grained subtopic structure analysis (Yaari, 1997; Eisenstein, 2009). Yaari (1997) proposed one of the first approaches for hierarchical text segmentation: a supervised agglomerative bottom-up clustering method exploiting paragraph hierarchy. A pioneer unsupervised approach for hierarchical text segmentation was introduced by Eisenstein (2009) using a bayesian generative model with dynamic programming. Also, Kazantseva and Szpakowicz (2014) proposed a clustering algorithm based on topical trees to perform hierarchical segmentation. Recently, several data sets have been published showing various types of structures: artificial added to automatic speech recognition transcripts of news videos (Sheikh et al., 2017), encyclopedic reflecting Wikipedia article structure (Koshorek et al., 2018; Arnold et al., 2019) or topical in goal-oriented dialogues (Takanobu et al., 2018). Our data set encompasses books of hours, each of them with a complex original structure described in the following section. 6241 3 Books of Hours Books of hours c"
2020.coling-main.549,N18-2075,0,0.14391,"and navigation, summarization, information retrieval and discourse parsing (Purver, 2011; Riedl and Biemann, 2012; Li et al., 2018). Traditional unsupervised approaches assume a high correlation between segments and subtopics. Therefore, based on a prior text decomposition, two adjacent segments are merged if they are highly correlated. On the contrary, if their similarity is below a certain threshold, a shift is determined (Hearst, 1997; Riedl and Biemann, 2012). When sufficient topically annotated training data are available, deep neural approaches based on CNN (Wang et al., 2017) or LSTM (Koshorek et al., 2018) can be efficiently applied (Li et al., 2018; Arnold et al., 2019). Until now, text segmentation methods have exclusively addressed data sets lying within the scope of narrative and expository texts or user dialogues texts and sometimes artificially generated data (Choi, 2000; Jeong and Titov, 2010; Glavaˇs et al., 2016; Koshorek et al., 2018). In this paper, we address transcriptions of ancient devotional manuscripts (thereafter also “MS”), from the Middle Ages, known as “books of hours”. Books of hours were used by lay people as a guidance in their daily prayers. They represent an important"
2020.coling-main.549,P93-1041,0,0.750297,"nt-based where each topic is characterised by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers (Fauconnier et al., 2014) whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of unsupervised approaches exploit lexical cohesion to detect coherent segments (Hearst, 1997; Choi, 2000) thanks to term repetitions (Hearst, 1994), semantic relations using lexical chains (Morris and Hirst, 1991), dictionary (Kozima, 1993), collocation networks (Ferret et al., 1998), or patterns of lexical co-occurrences (Hearst, 1997) such as discourse structures (Nomoto and Nitta, 1994). Early unsupervised methods include: TextTiling, a TF Cosine based approach (Hearst, 1994), LCSeg, based on lexical chains (Galley et al., 2003), U00, a probabilistic dynamic programming approach (Utiyama and Isahara, 2001), TopicTiling, a topic modeling approach based on Latent Dirichlet Analysis (LDA) (Riedl and Biemann, 2012). Glavaˇs et al. (2016) proposed a semantic relatedness graph approach that exploits word embeddings. Alemi and Ginsp"
2020.coling-main.549,P06-1004,0,0.0820343,"6.1 Experimental Setup Data To test SVM and BERT classifiers as well as books of hours segmentation, we used four books of hours (Arsenal 637, Beaune 55, Caen FMM.273 and Zurich Rh.169). The 6 remaining books were used for training. As books of hours are mostly written in Latin, we used the bert-base-multilingualcased model. For the fine-tuning phase of BERT, we used the simpletransformers7 library and its default parameters setting with 50 epochs8 . Baselines We evaluated: (i) five unsupervised approaches: TextTiling (Hearst, 1994), C99 (Choi, 2000), U00 (Utiyama and Isahara, 2001), MinCut (Malioutov and Barzilay, 2006), HierBays (Eisenstein, 2009). Due to the lack of large annotated training data, we did not evaluate other classifiers-based and deep learning-based approaches on the book of hours corpus. Evaluation Metrics The approaches are evaluated in terms of Pk (Beeferman et al., 1999) and Windowdiff (W D) (Pevzner and Hearst, 2002) metrics. Pk is an error metric which combines precision and recall to estimate the relative contributions of the different feature types. Nonetheless, it exhibits several drawbacks. Pk is affected by segment size variation. It also penalizes more heavily false negatives than"
2020.coling-main.549,J91-1002,0,0.482448,"ieval, etc. Segmentation can be content-based where each topic is characterised by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers (Fauconnier et al., 2014) whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of unsupervised approaches exploit lexical cohesion to detect coherent segments (Hearst, 1997; Choi, 2000) thanks to term repetitions (Hearst, 1994), semantic relations using lexical chains (Morris and Hirst, 1991), dictionary (Kozima, 1993), collocation networks (Ferret et al., 1998), or patterns of lexical co-occurrences (Hearst, 1997) such as discourse structures (Nomoto and Nitta, 1994). Early unsupervised methods include: TextTiling, a TF Cosine based approach (Hearst, 1994), LCSeg, based on lexical chains (Galley et al., 2003), U00, a probabilistic dynamic programming approach (Utiyama and Isahara, 2001), TopicTiling, a topic modeling approach based on Latent Dirichlet Analysis (LDA) (Riedl and Biemann, 2012). Glavaˇs et al. (2016) proposed a semantic relatedness graph approach that exploits word"
2020.coling-main.549,C94-2187,0,0.650551,"lso use topic markers (Fauconnier et al., 2014) whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of unsupervised approaches exploit lexical cohesion to detect coherent segments (Hearst, 1997; Choi, 2000) thanks to term repetitions (Hearst, 1994), semantic relations using lexical chains (Morris and Hirst, 1991), dictionary (Kozima, 1993), collocation networks (Ferret et al., 1998), or patterns of lexical co-occurrences (Hearst, 1997) such as discourse structures (Nomoto and Nitta, 1994). Early unsupervised methods include: TextTiling, a TF Cosine based approach (Hearst, 1994), LCSeg, based on lexical chains (Galley et al., 2003), U00, a probabilistic dynamic programming approach (Utiyama and Isahara, 2001), TopicTiling, a topic modeling approach based on Latent Dirichlet Analysis (LDA) (Riedl and Biemann, 2012). Glavaˇs et al. (2016) proposed a semantic relatedness graph approach that exploits word embeddings. Alemi and Ginsparg (2015) and Naili et al. (2017) studied the contribution of word embeddings on classical segmentation approaches. Text segmentation has also been add"
2020.coling-main.549,J02-1002,0,0.175458,"For the fine-tuning phase of BERT, we used the simpletransformers7 library and its default parameters setting with 50 epochs8 . Baselines We evaluated: (i) five unsupervised approaches: TextTiling (Hearst, 1994), C99 (Choi, 2000), U00 (Utiyama and Isahara, 2001), MinCut (Malioutov and Barzilay, 2006), HierBays (Eisenstein, 2009). Due to the lack of large annotated training data, we did not evaluate other classifiers-based and deep learning-based approaches on the book of hours corpus. Evaluation Metrics The approaches are evaluated in terms of Pk (Beeferman et al., 1999) and Windowdiff (W D) (Pevzner and Hearst, 2002) metrics. Pk is an error metric which combines precision and recall to estimate the relative contributions of the different feature types. Nonetheless, it exhibits several drawbacks. Pk is affected by segment size variation. It also penalizes more heavily false negatives than false positives and overpenalizes near misses. Hence, a second measure, W D, a variant of Pk , is also used as it equally penalizes false positives and near misses. 6.2 Results Arsenal637 Beaune55 Caen FMM.273 Zurich Rh.169 Level 1 73.89 68.39 67.78 66.63 SVM Level 2 60.02 54.74 57.30 46.42 Level 3 48.79 47.85 53.14 43.63"
2020.coling-main.549,W12-3307,0,0.113902,"tation boundaries. We show that the main state-of-the-art segmentation methods are either inefficient or inapplicable for books of hours and propose a bottom-up greedy approach that considerably enhances the segmentation results. We stress the importance of such hierarchical segmentation of books of hours for historians to explore their overarching differences underlying conception about Church. 1 Introduction Text segmentation is essential in many downstream applications including document understanding and navigation, summarization, information retrieval and discourse parsing (Purver, 2011; Riedl and Biemann, 2012; Li et al., 2018). Traditional unsupervised approaches assume a high correlation between segments and subtopics. Therefore, based on a prior text decomposition, two adjacent segments are merged if they are highly correlated. On the contrary, if their similarity is below a certain threshold, a shift is determined (Hearst, 1997; Riedl and Biemann, 2012). When sufficient topically annotated training data are available, deep neural approaches based on CNN (Wang et al., 2017) or LSTM (Koshorek et al., 2018) can be efficiently applied (Li et al., 2018; Arnold et al., 2019). Until now, text segmenta"
2020.coling-main.549,P01-1064,0,0.615875,"ad range of unsupervised approaches exploit lexical cohesion to detect coherent segments (Hearst, 1997; Choi, 2000) thanks to term repetitions (Hearst, 1994), semantic relations using lexical chains (Morris and Hirst, 1991), dictionary (Kozima, 1993), collocation networks (Ferret et al., 1998), or patterns of lexical co-occurrences (Hearst, 1997) such as discourse structures (Nomoto and Nitta, 1994). Early unsupervised methods include: TextTiling, a TF Cosine based approach (Hearst, 1994), LCSeg, based on lexical chains (Galley et al., 2003), U00, a probabilistic dynamic programming approach (Utiyama and Isahara, 2001), TopicTiling, a topic modeling approach based on Latent Dirichlet Analysis (LDA) (Riedl and Biemann, 2012). Glavaˇs et al. (2016) proposed a semantic relatedness graph approach that exploits word embeddings. Alemi and Ginsparg (2015) and Naili et al. (2017) studied the contribution of word embeddings on classical segmentation approaches. Text segmentation has also been addressed as a multi-document segmentation problem. Sun et al. (2007) for instance, proposed a method for shared topic detection and topic segmentation of multiple similar documents based on weighted mutual information, while J"
2020.coling-main.549,D17-1139,0,0.0718028,"uding document understanding and navigation, summarization, information retrieval and discourse parsing (Purver, 2011; Riedl and Biemann, 2012; Li et al., 2018). Traditional unsupervised approaches assume a high correlation between segments and subtopics. Therefore, based on a prior text decomposition, two adjacent segments are merged if they are highly correlated. On the contrary, if their similarity is below a certain threshold, a shift is determined (Hearst, 1997; Riedl and Biemann, 2012). When sufficient topically annotated training data are available, deep neural approaches based on CNN (Wang et al., 2017) or LSTM (Koshorek et al., 2018) can be efficiently applied (Li et al., 2018; Arnold et al., 2019). Until now, text segmentation methods have exclusively addressed data sets lying within the scope of narrative and expository texts or user dialogues texts and sometimes artificially generated data (Choi, 2000; Jeong and Titov, 2010; Glavaˇs et al., 2016; Koshorek et al., 2018). In this paper, we address transcriptions of ancient devotional manuscripts (thereafter also “MS”), from the Middle Ages, known as “books of hours”. Books of hours were used by lay people as a guidance in their daily praye"
2020.computerm-1.13,P08-3001,0,0.0241656,"Five teams have participated in the TermEval shared task. All teams submitted results for English, three submitted for French and two for Dutch. We submitted results for the French and English data sets. The Precision, recall, and F1-score were calculated twice: once including and once excluding Named Entities. Automatic terminology extraction (ATE) is a very challenging task beneficial to a broad range of natural language processing applications, including machine translation, bilingual lexicon induction, thesauri construction (Lin, 1998; Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Hagiwara, 2008; Andrade et al., 2013; Rigouts Terryn et al., 2019), to cite a few. Traditionally, this task is conducted by a terminologist, but hand-operated exploration, indexation, and maintenance of domain-specific corpora and terminologies is a costly enterprise. The automatization aims to ease effort demanded to manually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. Despite being a well-established research domain for decades, NLP methods still fail to meet human standards, and ATE is still considered an unsolved problem with considerable room f"
2020.computerm-1.13,P14-1119,0,0.0952674,"Missing"
2020.computerm-1.13,P98-2127,0,0.195329,"a sets are described in detail in (Rigouts Terryn et al., 2019). Five teams have participated in the TermEval shared task. All teams submitted results for English, three submitted for French and two for Dutch. We submitted results for the French and English data sets. The Precision, recall, and F1-score were calculated twice: once including and once excluding Named Entities. Automatic terminology extraction (ATE) is a very challenging task beneficial to a broad range of natural language processing applications, including machine translation, bilingual lexicon induction, thesauri construction (Lin, 1998; Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Hagiwara, 2008; Andrade et al., 2013; Rigouts Terryn et al., 2019), to cite a few. Traditionally, this task is conducted by a terminologist, but hand-operated exploration, indexation, and maintenance of domain-specific corpora and terminologies is a costly enterprise. The automatization aims to ease effort demanded to manually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. Despite being a well-established research domain for decades, NLP methods still fail to meet human standards, and"
2020.computerm-1.13,L18-1284,0,0.024631,"nually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. Despite being a well-established research domain for decades, NLP methods still fail to meet human standards, and ATE is still considered an unsolved problem with considerable room for improvement. If it is generally admitted that terms are single words or multiword expressions representing domain-specific concepts and that terminologies are the body of terms used with a particular domain, the lack of annotated data and agreement between researchers make ATE evaluation very difficult (Terryn et al., 2018). In order to gather researchers around a common evaluation scheme, TermEval shared task (Rigouts Terryn et al., 2019) offers a unified framework aiming a better ATE’s comprehension and analysis 1 . The shared task provides four data sets: Corruption, dressage, wind energy and heart failure; in three languages: English, French and Dutch. With the advance of neural network language models and following the current trend and excellent results obtained by transformer architecture on other NLP tasks, we have decided to experiment and compare two classification methods, one feature-based and the BE"
2020.computerm-1.13,W16-4702,0,0.287767,"we have empirically determined that only the elements that correlate at more than a certain threshold (mean correlation) with our target class are retained for classification (bolded in 1). 3.2. BERT BERT has proven to be efficient in many downstream NLP tasks (Devlin et al., 2018) including next sentence prediction, question answering and named entity recognition (NER). It can also be used for feature extraction or classification. Prior to the emergence of transformer-based architectures like BERT, several deep learning architectures for terminology extraction have been proposed. Wang et al. (2016) introduce a weakly-supervised classification-based approach. Amjadian et al. (2016) leverage local and global embeddings to encapsulate the meaning and behavior of the term for the classification step, although they only work with unigram terms. We must note that exploring these architectures is not the focus of this work; we mainly want to observe how BERTbased models can be used for ATE and how they perform in comparison to more traditional feature-based methods. In order to do that, we use different versions of BERT as a binary classifier for term prediction. For English, we use RoBERTa (L"
2020.computerm-1.13,I13-1150,0,0.0188269,"participated in the TermEval shared task. All teams submitted results for English, three submitted for French and two for Dutch. We submitted results for the French and English data sets. The Precision, recall, and F1-score were calculated twice: once including and once excluding Named Entities. Automatic terminology extraction (ATE) is a very challenging task beneficial to a broad range of natural language processing applications, including machine translation, bilingual lexicon induction, thesauri construction (Lin, 1998; Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Hagiwara, 2008; Andrade et al., 2013; Rigouts Terryn et al., 2019), to cite a few. Traditionally, this task is conducted by a terminologist, but hand-operated exploration, indexation, and maintenance of domain-specific corpora and terminologies is a costly enterprise. The automatization aims to ease effort demanded to manually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. Despite being a well-established research domain for decades, NLP methods still fail to meet human standards, and ATE is still considered an unsolved problem with considerable room for improvement. If it"
2020.computerm-1.13,P06-2111,0,0.102401,"Missing"
2020.computerm-1.13,I08-2084,0,0.0686425,"Missing"
2020.computerm-1.13,U16-1011,0,0.248174,"eral tests, we have empirically determined that only the elements that correlate at more than a certain threshold (mean correlation) with our target class are retained for classification (bolded in 1). 3.2. BERT BERT has proven to be efficient in many downstream NLP tasks (Devlin et al., 2018) including next sentence prediction, question answering and named entity recognition (NER). It can also be used for feature extraction or classification. Prior to the emergence of transformer-based architectures like BERT, several deep learning architectures for terminology extraction have been proposed. Wang et al. (2016) introduce a weakly-supervised classification-based approach. Amjadian et al. (2016) leverage local and global embeddings to encapsulate the meaning and behavior of the term for the classification step, although they only work with unigram terms. We must note that exploring these architectures is not the focus of this work; we mainly want to observe how BERTbased models can be used for ATE and how they perform in comparison to more traditional feature-based methods. In order to do that, we use different versions of BERT as a binary classifier for term prediction. For English, we use RoBERTa (L"
2020.computerm-1.13,W03-1610,0,0.176588,"described in detail in (Rigouts Terryn et al., 2019). Five teams have participated in the TermEval shared task. All teams submitted results for English, three submitted for French and two for Dutch. We submitted results for the French and English data sets. The Precision, recall, and F1-score were calculated twice: once including and once excluding Named Entities. Automatic terminology extraction (ATE) is a very challenging task beneficial to a broad range of natural language processing applications, including machine translation, bilingual lexicon induction, thesauri construction (Lin, 1998; Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Hagiwara, 2008; Andrade et al., 2013; Rigouts Terryn et al., 2019), to cite a few. Traditionally, this task is conducted by a terminologist, but hand-operated exploration, indexation, and maintenance of domain-specific corpora and terminologies is a costly enterprise. The automatization aims to ease effort demanded to manually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. Despite being a well-established research domain for decades, NLP methods still fail to meet human standards, and ATE is still consi"
2020.computerm-1.7,P99-1050,0,0.320176,"Missing"
2020.computerm-1.7,S17-1002,0,0.0638067,"Missing"
2020.computerm-1.7,P02-1006,0,0.030563,"Missing"
2020.computerm-1.7,Q19-1027,0,0.0279449,"Missing"
2020.computerm-1.7,P16-4003,1,0.813413,"ojection of lexicalsemantic relations, we did not use the 259 DRV pairs and excluded them from RefCD. We also excluded the 225 pairs of verbs because TermSuite only extracts noun phrases. Since RefCD does not contain information between simple terms describing other relations, like co-hyponyms, our study on semantic relations between MWTs concentrates on QSYN, HYP, and ANTI. The distribution of the three relation categories is imbalanced, as shown in table 2. TermSuite The MWT candidates were extracted from the PANACEA corpus through TermSuit, a terminology extraction tool developed at LS2N2 (Cram and Daille, 2016). TermSuit only extracts noun phrases; the candidates are provided with their part of speech, specificity, and frequency. Table 1 illustrates the extracted candidates. For this study, we only consider the candidates composed of two lexical words (e.g. milieu naturel ‘natural environment’). Pairs Terms ANTI HYP QSYN 116 107 191 122 523 415 total 830 429 Table 2: Number of terms and semantic relations in RefCD 5. 5.1. 1 http://www.panacea-lr.eu/en/info-for-researchers/datasets/monolingual-corpora 2 https://www.ls2n.fr Generation of semantically-linked MWTs Raw projection We extracted all the MWT"
2020.computerm-1.7,daille-hazem-2014-semi,1,0.790075,"Missing"
2020.computerm-1.7,N19-1423,0,0.0112122,"Missing"
2020.computerm-1.7,P98-1082,0,0.515765,"Missing"
2020.computerm-1.7,L18-1045,1,0.866609,"Missing"
2020.computerm-1.9,P16-4003,1,0.830232,"Nist, the terms alphabetically ordered with their definitions; 6.1. Term Extraction Approaches Term Extraction Tools In this section we provide a description of the chosen tools to execute the terminology extraction. Cyber The Cybersecurity term list contains candidate terms taken from the post-processed texts connected together through the main semantic relationships proper to thesauri (Broughton, 2008), i.e., hierarchical, synonymy, association. These relations are respectively formalized by standard tags (ISO/TC 46/SC 9 2011 and 2013): 6.1.1. TermSuite - Variants Detection Tool TermSuite (Cram and Daille, 2016) is a toolkit for terminology extraction and multilingual term alignment. Its performance is quite immediate when it runs over big data sets. The term extraction provided by TermSuite is a list of representative terms that are presented together with different properties, e.g., their frequency, accuracy, specificity. Terms are therefore ordered according to their unithood and application to the domain. One of the main feature that broader term broader term (BT) that stands for hyperonyms; 65 shapes the quality of this software is its syntactic and morphological variants detection among terms,"
2020.computerm-1.9,daille-hazem-2014-semi,1,0.903205,"that can provide semantic knowledge density and granularity about the lexicon that is meant to be represented (Barri`ere, 2006). These structures are in literature known as TKBs (Terminological Knowledge Bases) (Condamines, 2018), and, indeed, they support the modalities of systematizing the specialized knowledge by merging the skills proper to linguistics and knowledge engineering. The ways in which the candidate terms are extracted from a specific domainoriented corpus (Loginova Clouet et al., 2012) usually follow text pre-processing procedures and extraction of single and multi-word units (Daille and Hazem, 2014) from texts filtered out by frequency measures, then they can undergo a phase of variation recognition (Weller et al., 2011) and other statistical calculations to determine the specificity, accuracy, similarity in the texts from which they come from (Cabr´e et al., 2001). The reason why the domain-oriented terms are called ‘candidates’ (Condamines, 2018) is linked to the fact that in the terminologists’ activity the need of experts’ validation is frequently required, this because just the subjective selection by terminologists might not be exhaustive and fully consistent with the domain expert"
2020.computerm-1.9,dellorletta-etal-2014-t2k,0,0.0273813,"Missing"
2020.computerm-1.9,N19-1423,0,0.0126985,"Missing"
2020.computerm-1.9,J06-1005,0,0.0593317,"nguistic structures that are very frequent within a corpus of documents (Lefeuvre, 2017), patterns allow to discover among terms which are the conceptual relations (Bernier-Colborne and Barri`ere, 2018). The study of patterns dates way back, at the end of 90’ the works of Hearst (1992) were, for instance, firstly focused on the configuration of Noun Phrases followed by other morpho-syntactic structures to be found 63 in texts. Many authors in the literature studied the ways nominal and verbal phrases allow to identify semantic relations between terms through syntagmatic or phrasal structures (Girju et al., 2006). The typologies of lexico-syntactic markers help in retrieving the desired semantic information about the terminology proper to a specialized domain (Nguyen et al., 2017), that’s the case of the casual relationships between terms. This particular kind of connection is notably described in the works of Barri`ere (2002) in which the author gives a wide-ranging perspective for investigating the causal relationships in informative texts. As the author underlines, it is not an easy task to group the causative verbs that should isolate the representative terms of a domain to be linked through a cau"
2020.computerm-1.9,P08-3001,0,0.0494634,"is a context-dependent procedure: in considering the source area of study and having some technical knowledge about it, terminologists can much easily analyse in an autonomous and accurate way a combination of semantic relationships (Condamines, 2008). For what concerns semantic similarity methods in the literature, they have firstly been applied to single word terms (SWTs) using a variety of approaches such as: lexiconbased approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Hagiwara, 2008; Hazem and Daille, 2014) and distributed approaches such in (Mikolov et al., 2013; Bojanowski et al., 2016). This procedure helps in configuring the associations between terms with respect to synonyms connections retrieved from corpora. On this point, it is important to highlight the relevance of extracting reliable lists of candidate terms that could represent the starting point from which to set up a conceptual modeling of a thesaurus as well as a basis to analyse and define the internal domainspecific synonyms and hyperonyms (Meyer and Mackintosh, 1996). 4. The structure phase of the thesa"
2020.computerm-1.9,L18-1045,1,0.827773,"xtracted terms. This work represents the first attempt to use BERT model for terminology extraction. Overall, BERT obtained the best results with minimum Word Embedding-based Word embedding models have been showing to be very effective in word representation. They have been applied in several NLP tasks including word disambiguation, semantic similarity, bilingual lexicon induction (Mikolov et al., 2013; Arora et al., 2017; Bojanowski et al., 2016), etc. For semantic similarity, and more precisely synonym extraction of multi-word terms, two compositionality-based techniques have been proposed (Hazem and Daille, 2018). The first technique called Semi-compositional word embeddings is based on distributional analysis (Hazem and Daille, 2014) and assumes that the head or a tail is shared by two semantically related terms. The second technique called Full-compositional word embeddings is inspired by the idea that phrases can be represented by an element-wise sum of the word embeddings of semantically related words of its parts (Arora et al., 2017). In our experiments we follow the principle of the second technique and apply it to the automatic extraction of hyperonyms, synonyms, related and causative terms. Th"
2020.computerm-1.9,C92-2082,0,0.578753,"rsued, starting from lexico-syntactic patterns conformation (Condamines, 2007), and experimenting other solutions such as the ones proposed by (Grefenstette, 1994) with “Sextant”, or (Kageura et al., 2000) with their methodology in considering the common entries in two different thesauri and constructing pairs of codes. As linguistic structures that are very frequent within a corpus of documents (Lefeuvre, 2017), patterns allow to discover among terms which are the conceptual relations (Bernier-Colborne and Barri`ere, 2018). The study of patterns dates way back, at the end of 90’ the works of Hearst (1992) were, for instance, firstly focused on the configuration of Noun Phrases followed by other morpho-syntactic structures to be found 63 in texts. Many authors in the literature studied the ways nominal and verbal phrases allow to identify semantic relations between terms through syntagmatic or phrasal structures (Girju et al., 2006). The typologies of lexico-syntactic markers help in retrieving the desired semantic information about the terminology proper to a specialized domain (Nguyen et al., 2017), that’s the case of the casual relationships between terms. This particular kind of connection"
2020.computerm-1.9,I13-1150,0,0.0586893,"Missing"
2020.computerm-1.9,S18-1116,0,0.0525625,"Missing"
2020.computerm-1.9,C16-2015,0,0.02004,"through the denominative, conceptual and linguistic variants included in the terminological output it is possible to detect in which ways terms are expanded by other semantic elements, reduced, related to an opposite one, or appearing in several linguistic conformations, e.g., cyber security or cybersecurity. Below a list of few examples to show the variations given by the outputs in TermSuite terminological extraction for Cybersecurity domain in Italian language that can help in detecting semantic associations to be included in the thesaurus: 6.1.3. Pke - Keyphrases Identification Tool PKE (Boudin, 2016) is an open-source python keyphrase extraction toolkit that implements several keyphrase extraction approaches. From a linguistic point of view, PKE resulted to be very efficient in terms of providing a semiautomatic structuring of information since many candidate terms, which have been selected as being part of the Cybersecurity thesaurus, are grouped alongside with other ones that, in turn, could represent their associative semantic chains. For this section we provide as well related examples for the terms outputs precision: • denominative variants: NPN: hacker (21 matches) del telefono (mob"
2020.computerm-1.9,C00-1058,0,0.223007,"mantic similarity procedures and patterns configuration related to the causative connections. The automatized methodologies used for the configuration of thesauri’s structure (Yang and Powers, 2008b; Morin and Jacquemin, 1999), can quicken the process related to the arrangement of textual relations network to shape the informative tissue of a domain. To achieve this framework system different approaches can be pursued, starting from lexico-syntactic patterns conformation (Condamines, 2007), and experimenting other solutions such as the ones proposed by (Grefenstette, 1994) with “Sextant”, or (Kageura et al., 2000) with their methodology in considering the common entries in two different thesauri and constructing pairs of codes. As linguistic structures that are very frequent within a corpus of documents (Lefeuvre, 2017), patterns allow to discover among terms which are the conceptual relations (Bernier-Colborne and Barri`ere, 2018). The study of patterns dates way back, at the end of 90’ the works of Hearst (1992) were, for instance, firstly focused on the configuration of Noun Phrases followed by other morpho-syntactic structures to be found 63 in texts. Many authors in the literature studied the ways"
2020.computerm-1.9,2019.jeptalnrecital-tia.1,1,0.792748,"alignment. Its performance is quite immediate when it runs over big data sets. The term extraction provided by TermSuite is a list of representative terms that are presented together with different properties, e.g., their frequency, accuracy, specificity. Terms are therefore ordered according to their unithood and application to the domain. One of the main feature that broader term broader term (BT) that stands for hyperonyms; 65 shapes the quality of this software is its syntactic and morphological variants detection among terms, e.g, lexical reduction, composition, coordination, derivation (Lanza and Daille, 2019). Variants identification given by the output list in TermSuite represents one of the methods selected to retrieve hyperonyms as well as synonyms in the source corpus. In fact, through the denominative, conceptual and linguistic variants included in the terminological output it is possible to detect in which ways terms are expanded by other semantic elements, reduced, related to an opposite one, or appearing in several linguistic conformations, e.g., cyber security or cybersecurity. Below a list of few examples to show the variations given by the outputs in TermSuite terminological extraction"
2020.computerm-1.9,N13-1090,0,0.625383,"nd having some technical knowledge about it, terminologists can much easily analyse in an autonomous and accurate way a combination of semantic relationships (Condamines, 2008). For what concerns semantic similarity methods in the literature, they have firstly been applied to single word terms (SWTs) using a variety of approaches such as: lexiconbased approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Hagiwara, 2008; Hazem and Daille, 2014) and distributed approaches such in (Mikolov et al., 2013; Bojanowski et al., 2016). This procedure helps in configuring the associations between terms with respect to synonyms connections retrieved from corpora. On this point, it is important to highlight the relevance of extracting reliable lists of candidate terms that could represent the starting point from which to set up a conceptual modeling of a thesaurus as well as a basis to analyse and define the internal domainspecific synonyms and hyperonyms (Meyer and Mackintosh, 1996). 4. The structure phase of the thesaurus for Cybersecurity has started by evaluating the list of terms extracted by us"
2020.computerm-1.9,P99-1050,0,0.553306,"ge of a specific domain of study as a controlled vocabulary. This paper aims at presenting an analysis of the best performing NLP approaches, i.e., patterns configuration, semantic similarity, morphosyntactic variation given by term extractors, in enhancing a semantic structure of an existing Italian thesaurus about the technical domain of Cybersecurity. Constructing thesauri by carrying out minimum handcrafted activities is currently highly demanded (Azevedo et al., 2015). Hence, several methods to automatically build and maintain a thesaurus have been proposed so far (G¨untzer et al., 1989; Morin and Jacquemin, 1999; Yang and Powers, 2008a; Schandl and Blumauer, 2010). However, the quality of automatically generated thesauri tends to be rather weaker in their content and structure with respect to the conventional handcrafted ones (Ryan, 2014). To guarantee the currency of a thesaurus (Batini et al., 2009) it is crucial to whether improve existing methods or to develop new efficient techniques for discovering terms and their relations. On the perspective of using existing NLP tools for constructing a thesaurus, choosing the most appropriate ones is not an easy task since the performance varies depending o"
2020.computerm-1.9,D17-1022,0,0.0385766,"Missing"
2020.computerm-1.9,W16-4706,0,0.0899767,"Missing"
2020.computerm-1.9,L18-1284,0,0.368874,"demonstrate that the TKBs comply with the specialized corpus knowledge flow. Hence, together with certain groups of experts’ supervision, other tools support the accuracy validation, i.e., the gold standards (Barri`ere, 2006). This task is meant to give results on the way terms that have been selected to be part of a semantic resource – designed to represent a specialized language – can be aligned with others included in reference texts. These target texts can be in the same language as the one of the source corpus, and could present less difficulties in the matching system, or multilingual (Terryn et al., 2018), in these cases using translations from existing semantic resources could represent a solution. In this paper, the gold standards taken into account are in Italian language or have been translated in Italian – Nist and Iso – this reflects the native purpose of the project that was intended to provide a guidance for the understanding of the Cybersecurity domain in Italian language. 1. Pattern based system: the causative patterns aim at enhancing the associative relationship proper to thesauri configuration; 2. Variants recognition: semantic variation is useful to detect hierarchical and associ"
2020.computerm-1.9,P06-2111,0,0.163905,"Missing"
2020.computerm-1.9,W03-1610,0,0.163541,"ry given the domain-oriented nature of the casual connections. Indeed, retrieving this type of patterns is a context-dependent procedure: in considering the source area of study and having some technical knowledge about it, terminologists can much easily analyse in an autonomous and accurate way a combination of semantic relationships (Condamines, 2008). For what concerns semantic similarity methods in the literature, they have firstly been applied to single word terms (SWTs) using a variety of approaches such as: lexiconbased approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Hagiwara, 2008; Hazem and Daille, 2014) and distributed approaches such in (Mikolov et al., 2013; Bojanowski et al., 2016). This procedure helps in configuring the associations between terms with respect to synonyms connections retrieved from corpora. On this point, it is important to highlight the relevance of extracting reliable lists of candidate terms that could represent the starting point from which to set up a conceptual modeling of a thesaurus as well as a basis to analyse and define the internal doma"
2020.lrec-1.97,A00-2004,0,0.735819,"Level1 Virgin Virgin Virgin Virgin Virgin Virgin Level2 Level3 Matins Nocturn Matins Nocturn Matins Nocturn Matins Nocturn Matins Nocturn Matins Nocturn Figure 5: CSV representation of annotations of a book of hours (Extracts from the Arsenal 1194). Figure 5 illustrates an example of the provided annotations extracted from Arsenal 1194. The ”Virgin” label refers to the Hours of the Virgin (level 1), the ”Matins” label refers to the Matins of level 2 and ”Nocturn” label, refers to Nocturn of level 3. For text segmentation, we provide the gold standard files with section delimitation following Choi (2000) format. Hence, each of the annotated books has one Choi format per level. This allows to evaluate the segmentation of each level separately or all together. 5. Text Segmentation Text segmentation is closely related to topic analysis and can be addressed following three axis: (i) syntagmatic axis where a text is delimited into homogeneous topics (ex: audio transcriptions); (ii) paradigmatic axis in which topics are identified and (iii) functional axis in which segment topics relations are linked for text structuring (ex: summarization) (Ferret et al., 1998). Segmentation can be content-based w"
2020.lrec-1.97,N09-1040,0,0.929946,"is characterized by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. The addressed texts were mainly linear, in which case a sequential analysis of topical changes was applied (Hearst, 1994; Choi, 2000) (expository texts were usually used). Later on, hierarchical texts were addressed which required a more fine-grained subtopic structure analysis (Yaari, 1997; Eisenstein, 2009). Most of the approaches dedicated to text segmentation perform a lexical level analysis to detect segments coherence, and use (i) patterns of lexical co-occurrence (Hearst, 1997) such as discourse structure (Nomoto and Nitta, 1994) or (ii) lexical cohesion (Morris and Hirst, 1991) based on term repetition (Hearst, 1994) and semantic relations extracted via a thesaurus (Morris and Hirst, 1991), a dictionary (Kozima, 1993) or automatically using a collocation network (Ferret et al., 1998). Lexical cohesion has inspired a broad range of unsupervised approaches including TextTiling (Hearst, 1994)"
2020.lrec-1.97,C98-1062,0,0.817574,"ard files with section delimitation following Choi (2000) format. Hence, each of the annotated books has one Choi format per level. This allows to evaluate the segmentation of each level separately or all together. 5. Text Segmentation Text segmentation is closely related to topic analysis and can be addressed following three axis: (i) syntagmatic axis where a text is delimited into homogeneous topics (ex: audio transcriptions); (ii) paradigmatic axis in which topics are identified and (iii) functional axis in which segment topics relations are linked for text structuring (ex: summarization) (Ferret et al., 1998). Segmentation can be content-based where each topic is characterized by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. The addressed texts were mainly linear, in which case a sequential analysis of topical changes was applied (Hearst, 1994; Choi, 2000) (expository texts were usually used). Later on, hierarchical texts were addressed which required a more fin"
2020.lrec-1.97,P03-1071,0,0.534219,"lexical level analysis to detect segments coherence, and use (i) patterns of lexical co-occurrence (Hearst, 1997) such as discourse structure (Nomoto and Nitta, 1994) or (ii) lexical cohesion (Morris and Hirst, 1991) based on term repetition (Hearst, 1994) and semantic relations extracted via a thesaurus (Morris and Hirst, 1991), a dictionary (Kozima, 1993) or automatically using a collocation network (Ferret et al., 1998). Lexical cohesion has inspired a broad range of unsupervised approaches including TextTiling (Hearst, 1994) (a tfxIdf Cosine-based approach), LSeg based on lexical chains (Galley et al., 2003), U00 (Utiyama and Isahara, 2001) a probabilistic dynamic programming approach, TopicTiling (Riedl and Biemann, 2012) a topic modeling approach based on Latent Dirichlet Analysis (LDA), etc. Two main types of texts were addressed by lexical cohesion based approaches, that is: technical and scientific documents (Hearst, 1997) in which term repetition is a strong indicator when a specific vocabulary is used; and narrative texts (Morris and Hirst, 1991; Kozima, 1993) where term repetition is not sufficient as concepts may be expressed in different ways and so, thesaurus and dictionaries may be re"
2020.lrec-1.97,P94-1002,0,0.926069,"content of books of hours. This is a great challenge in digital humanities that brings together researchers from the fields of document recognition, NLP and history. The study of the content of books of hours aims at providing opportunities for historical analysis to better understand the cultures and faiths from the 13th c. to the 16th c. In addition, its complex logical entangled structure offers a new type of resource for text segmentation. Since traditional segmentation data sets lie within the scope of expository texts, narrative or issued from spoken an written dialogues (Kozima, 1993; Hearst, 1994; Nomoto and Nitta, 1994; Utiyama and Isahara, 2001) or more recently from Wikipedia (Arnold et 1 https://library.harvard.edu/collections/ picturingprayer/exhibition.html 776 Figure 1: Examples of text recognition results (hyp) and their manual annotations (ref) on three books of hours manuscripts: Harvard Typ 32, Metz 1581 and Harvard Rich 9. The writings are of type ”Textualis” on the left and center sides, and of type ”Cursiva” on the right side. al., 2019), books of hours arguably constitute a new genre of segmentation texts that exhibits difficulties with regard to length, structure and m"
2020.lrec-1.97,J97-1003,0,0.924055,"(ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. The addressed texts were mainly linear, in which case a sequential analysis of topical changes was applied (Hearst, 1994; Choi, 2000) (expository texts were usually used). Later on, hierarchical texts were addressed which required a more fine-grained subtopic structure analysis (Yaari, 1997; Eisenstein, 2009). Most of the approaches dedicated to text segmentation perform a lexical level analysis to detect segments coherence, and use (i) patterns of lexical co-occurrence (Hearst, 1997) such as discourse structure (Nomoto and Nitta, 1994) or (ii) lexical cohesion (Morris and Hirst, 1991) based on term repetition (Hearst, 1994) and semantic relations extracted via a thesaurus (Morris and Hirst, 1991), a dictionary (Kozima, 1993) or automatically using a collocation network (Ferret et al., 1998). Lexical cohesion has inspired a broad range of unsupervised approaches including TextTiling (Hearst, 1994) (a tfxIdf Cosine-based approach), LSeg based on lexical chains (Galley et al., 2003), U00 (Utiyama and Isahara, 2001) a probabilistic dynamic programming approach, TopicTiling (R"
2020.lrec-1.97,E06-1035,0,0.0881898,"scientific documents (Hearst, 1997) in which term repetition is a strong indicator when a specific vocabulary is used; and narrative texts (Morris and Hirst, 1991; Kozima, 1993) where term repetition is not sufficient as concepts may be expressed in different ways and so, thesaurus and dictionaries may be required to extract semantic relations between terms. Ferret et al. (1998), introduced a mixed approach to deal with both types of texts. Also, a bunch of supervised approaches was introduced mainly to deal with discourse (Joty et al., 2015), multiparty dialogue and chat forums segmentation (Hsueh et al., 2006; Hernault et al., 2010) and to perform segmentation at the sentence level to discover Elementary Discourse Units 781 (EDU) (Hernault et al., 2010; Joty et al., 2015). These approaches often combine lexical coherence information with dialogue features using for instance a decision tree classifier (Hsueh et al., 2006), Conditional Random Fields (CRF) (Hernault et al., 2010; Joty et al., 2015) or Neural network approaches such as TextTiling-like embedding approach for query-reply dialog segmentation (Song et al., 2016), multi-party dialog for EDU using sequential model (Shi and Huang, 2019), rei"
2020.lrec-1.97,J15-3002,0,0.355241,"ddressed by lexical cohesion based approaches, that is: technical and scientific documents (Hearst, 1997) in which term repetition is a strong indicator when a specific vocabulary is used; and narrative texts (Morris and Hirst, 1991; Kozima, 1993) where term repetition is not sufficient as concepts may be expressed in different ways and so, thesaurus and dictionaries may be required to extract semantic relations between terms. Ferret et al. (1998), introduced a mixed approach to deal with both types of texts. Also, a bunch of supervised approaches was introduced mainly to deal with discourse (Joty et al., 2015), multiparty dialogue and chat forums segmentation (Hsueh et al., 2006; Hernault et al., 2010) and to perform segmentation at the sentence level to discover Elementary Discourse Units 781 (EDU) (Hernault et al., 2010; Joty et al., 2015). These approaches often combine lexical coherence information with dialogue features using for instance a decision tree classifier (Hsueh et al., 2006), Conditional Random Fields (CRF) (Hernault et al., 2010; Joty et al., 2015) or Neural network approaches such as TextTiling-like embedding approach for query-reply dialog segmentation (Song et al., 2016), multi-"
2020.lrec-1.97,N18-2075,0,0.216686,"veral state of the art approaches: (i) five unsupervised approaches: TextTiling (Hearst, 1994), clustering model (C99) (Choi, 2000), probabilistic dynamic programming model (U00) (Utiyama and Isahara, 2001), minimum cut model (MinCut) (Malioutov and Barzilay, 2006), the hierarchical bayesian model (HierBays) (Eisenstein, 2009); and (ii) one supervised approach: TopicTiling (Riedl and Biemann, 2012), a topic modeling-based approach. Due to the lack of large annotated training data, we do not evaluate other supervised (Hsueh et al., 2006; Joty et al., 2015) and deep learning (Song et al., 2016; Koshorek et al., 2018; Shi and Huang, 2019) approaches. The experiments were conducted on the manual transcribed book Medievalist and on the automatically transcribed book Harvard 253. The Arsenal 1194 book of hours was used as training data for the TopicTiling approach. 6.1. If a change in shifts corresponds to a topical change in traditional segmentation data sets, this is not the case for books of hours where some segments can be highly correlated. A shift corresponds to a frontier between two segments or categories Medievalist Level1 Level2 Harvard 253 Level1 Level2 Pk WD Pk WD Pk WD Pk WD TextTiling 64.4 94.2"
2020.lrec-1.97,P93-1041,0,0.876088,"cess the whole content of books of hours. This is a great challenge in digital humanities that brings together researchers from the fields of document recognition, NLP and history. The study of the content of books of hours aims at providing opportunities for historical analysis to better understand the cultures and faiths from the 13th c. to the 16th c. In addition, its complex logical entangled structure offers a new type of resource for text segmentation. Since traditional segmentation data sets lie within the scope of expository texts, narrative or issued from spoken an written dialogues (Kozima, 1993; Hearst, 1994; Nomoto and Nitta, 1994; Utiyama and Isahara, 2001) or more recently from Wikipedia (Arnold et 1 https://library.harvard.edu/collections/ picturingprayer/exhibition.html 776 Figure 1: Examples of text recognition results (hyp) and their manual annotations (ref) on three books of hours manuscripts: Harvard Typ 32, Metz 1581 and Harvard Rich 9. The writings are of type ”Textualis” on the left and center sides, and of type ”Cursiva” on the right side. al., 2019), books of hours arguably constitute a new genre of segmentation texts that exhibits difficulties with regard to length, s"
2020.lrec-1.97,P06-1004,0,0.541533,"than false positives and overpenalizes near misses. Hence, a second measure, WindowDiff, a variant of Pk , has been also used as it equally penalizes false positives and near misses. 6.3. Results Table 4 reports the segmentation results of the Medievalist and the Harvard 253 books of hours on the first and second levels24 . Experiments and Results We evaluate several state of the art approaches: (i) five unsupervised approaches: TextTiling (Hearst, 1994), clustering model (C99) (Choi, 2000), probabilistic dynamic programming model (U00) (Utiyama and Isahara, 2001), minimum cut model (MinCut) (Malioutov and Barzilay, 2006), the hierarchical bayesian model (HierBays) (Eisenstein, 2009); and (ii) one supervised approach: TopicTiling (Riedl and Biemann, 2012), a topic modeling-based approach. Due to the lack of large annotated training data, we do not evaluate other supervised (Hsueh et al., 2006; Joty et al., 2015) and deep learning (Song et al., 2016; Koshorek et al., 2018; Shi and Huang, 2019) approaches. The experiments were conducted on the manual transcribed book Medievalist and on the automatically transcribed book Harvard 253. The Arsenal 1194 book of hours was used as training data for the TopicTiling app"
2020.lrec-1.97,J91-1002,0,0.81773,"bullets, numbering, bold, etc. The addressed texts were mainly linear, in which case a sequential analysis of topical changes was applied (Hearst, 1994; Choi, 2000) (expository texts were usually used). Later on, hierarchical texts were addressed which required a more fine-grained subtopic structure analysis (Yaari, 1997; Eisenstein, 2009). Most of the approaches dedicated to text segmentation perform a lexical level analysis to detect segments coherence, and use (i) patterns of lexical co-occurrence (Hearst, 1997) such as discourse structure (Nomoto and Nitta, 1994) or (ii) lexical cohesion (Morris and Hirst, 1991) based on term repetition (Hearst, 1994) and semantic relations extracted via a thesaurus (Morris and Hirst, 1991), a dictionary (Kozima, 1993) or automatically using a collocation network (Ferret et al., 1998). Lexical cohesion has inspired a broad range of unsupervised approaches including TextTiling (Hearst, 1994) (a tfxIdf Cosine-based approach), LSeg based on lexical chains (Galley et al., 2003), U00 (Utiyama and Isahara, 2001) a probabilistic dynamic programming approach, TopicTiling (Riedl and Biemann, 2012) a topic modeling approach based on Latent Dirichlet Analysis (LDA), etc. Two ma"
2020.lrec-1.97,C94-2187,0,0.516968,"oks of hours. This is a great challenge in digital humanities that brings together researchers from the fields of document recognition, NLP and history. The study of the content of books of hours aims at providing opportunities for historical analysis to better understand the cultures and faiths from the 13th c. to the 16th c. In addition, its complex logical entangled structure offers a new type of resource for text segmentation. Since traditional segmentation data sets lie within the scope of expository texts, narrative or issued from spoken an written dialogues (Kozima, 1993; Hearst, 1994; Nomoto and Nitta, 1994; Utiyama and Isahara, 2001) or more recently from Wikipedia (Arnold et 1 https://library.harvard.edu/collections/ picturingprayer/exhibition.html 776 Figure 1: Examples of text recognition results (hyp) and their manual annotations (ref) on three books of hours manuscripts: Harvard Typ 32, Metz 1581 and Harvard Rich 9. The writings are of type ”Textualis” on the left and center sides, and of type ”Cursiva” on the right side. al., 2019), books of hours arguably constitute a new genre of segmentation texts that exhibits difficulties with regard to length, structure and many ambiguities. Also, t"
2020.lrec-1.97,J02-1002,0,0.499244,"Table 3 resumes the number of segments according to the first and second levels of segmentation. At the first level, we note that the number of shifts23 is equal except (Hardvard 253 that does not contain the Suffrages section). However, for the second level, we see different number of shifts. 23 Number of categories BoH #Level1 #Level2 Arsenal 1194 8 34 Medievalist 8 55 Harvard 253 7 38 Table 3: Illustration of the number of boundaries or shifts per level for each book of hours. 6.2. Evaluation Metrics The approaches are evaluated in terms of Pk (Beeferman et al., 1999) and Windowdiff (W D) (Pevzner and Hearst, 2002) metrics. Pk is an error metric which combines precision and recall to estimates the relative contributions of the different feature types. Nonetheless, it exhibits several drawbacks as mentioned in (Pevzner and Hearst, 2002). Pk is affected by segment size variation. It also penalizes more heavily false negatives than false positives and overpenalizes near misses. Hence, a second measure, WindowDiff, a variant of Pk , has been also used as it equally penalizes false positives and near misses. 6.3. Results Table 4 reports the segmentation results of the Medievalist and the Harvard 253 books of"
2020.lrec-1.97,W12-3307,0,0.902157,") such as discourse structure (Nomoto and Nitta, 1994) or (ii) lexical cohesion (Morris and Hirst, 1991) based on term repetition (Hearst, 1994) and semantic relations extracted via a thesaurus (Morris and Hirst, 1991), a dictionary (Kozima, 1993) or automatically using a collocation network (Ferret et al., 1998). Lexical cohesion has inspired a broad range of unsupervised approaches including TextTiling (Hearst, 1994) (a tfxIdf Cosine-based approach), LSeg based on lexical chains (Galley et al., 2003), U00 (Utiyama and Isahara, 2001) a probabilistic dynamic programming approach, TopicTiling (Riedl and Biemann, 2012) a topic modeling approach based on Latent Dirichlet Analysis (LDA), etc. Two main types of texts were addressed by lexical cohesion based approaches, that is: technical and scientific documents (Hearst, 1997) in which term repetition is a strong indicator when a specific vocabulary is used; and narrative texts (Morris and Hirst, 1991; Kozima, 1993) where term repetition is not sufficient as concepts may be expressed in different ways and so, thesaurus and dictionaries may be required to extract semantic relations between terms. Ferret et al. (1998), introduced a mixed approach to deal with bo"
2020.lrec-1.97,P01-1064,0,0.901129,"great challenge in digital humanities that brings together researchers from the fields of document recognition, NLP and history. The study of the content of books of hours aims at providing opportunities for historical analysis to better understand the cultures and faiths from the 13th c. to the 16th c. In addition, its complex logical entangled structure offers a new type of resource for text segmentation. Since traditional segmentation data sets lie within the scope of expository texts, narrative or issued from spoken an written dialogues (Kozima, 1993; Hearst, 1994; Nomoto and Nitta, 1994; Utiyama and Isahara, 2001) or more recently from Wikipedia (Arnold et 1 https://library.harvard.edu/collections/ picturingprayer/exhibition.html 776 Figure 1: Examples of text recognition results (hyp) and their manual annotations (ref) on three books of hours manuscripts: Harvard Typ 32, Metz 1581 and Harvard Rich 9. The writings are of type ”Textualis” on the left and center sides, and of type ”Cursiva” on the right side. al., 2019), books of hours arguably constitute a new genre of segmentation texts that exhibits difficulties with regard to length, structure and many ambiguities. Also, the vast majority of state of"
2021.jeptalnrecital-taln.10,P98-1120,0,0.63144,"Missing"
2021.jeptalnrecital-taln.10,C96-2121,0,0.247437,"Missing"
2021.jeptalnrecital-taln.10,W14-1618,0,0.064754,"Missing"
2021.jeptalnrecital-taln.10,Q15-1016,0,0.0726251,"Missing"
2021.jeptalnrecital-taln.10,N13-1090,0,0.115129,"Missing"
2021.jeptalnrecital-taln.10,L18-1228,0,0.0333152,"Missing"
2021.jeptalnrecital-taln.10,radev-etal-2002-evaluating,0,0.0153727,"Missing"
2021.jeptalnrecital-taln.10,2007.jeptalnrecital-long.37,0,0.157858,"Missing"
2021.jeptalnrecital-taln.10,C08-1114,0,0.156047,"Missing"
2021.jeptalnrecital-taln.10,R19-1147,0,0.0206954,"Missing"
2021.jeptalnrecital-taln.10,P16-1158,0,0.022487,"Missing"
boulaknadel-etal-2008-multi,J90-1003,0,\N,Missing
boulaknadel-etal-2008-multi,C00-1077,0,\N,Missing
boulaknadel-etal-2008-multi,N04-4038,0,\N,Missing
boulaknadel-etal-2008-multi,nenadic-etal-2002-automatic,0,\N,Missing
C00-1032,C92-3150,0,0.0416288,"ative status is evaluated thanks to a thesaurus of the domain. 1 Introduction Identifying relational adjectives (RAdj) such as malarial, and noun phrases in which they appear such as malarial mosquitoes, could be interesting in several elds of NLP, such as terminology acquisition, topic detection, updating of thesauri, because they hold a naming function acknowledged by linguists: (Levi, 1978), (MelisPuchulu, 1991), etc. The use of RAdj is particularly frequent in scienti c elds (Monceaux, 1993). Paradoxically, terminology acquisition systems such as TERMINO (David and Plante, 1990), LEXTER (Bourigault, 1992), TERMS (Justeson and Katz, 1995), have not been concerned with RAdj. Even (Ibekwe-Sanjua, 1998) in her study of term variations for identifying research topics from texts does not take into account derivational variants. Our concern is: 1. To identify noun phrases in which relational adjectives appear, as well as the prepositional phrases by which they could be paraphrased. We will see through another source presented in section 2 that this property of paraphrase can be used to identify these adjectives. 2. To check the naming character of these adjectives and to evaluate the naming character"
C00-1032,H92-1022,0,0.0164511,"Missing"
C00-1032,P98-1092,0,0.0242997,"ational adjectives (RAdj) such as malarial, and noun phrases in which they appear such as malarial mosquitoes, could be interesting in several elds of NLP, such as terminology acquisition, topic detection, updating of thesauri, because they hold a naming function acknowledged by linguists: (Levi, 1978), (MelisPuchulu, 1991), etc. The use of RAdj is particularly frequent in scienti c elds (Monceaux, 1993). Paradoxically, terminology acquisition systems such as TERMINO (David and Plante, 1990), LEXTER (Bourigault, 1992), TERMS (Justeson and Katz, 1995), have not been concerned with RAdj. Even (Ibekwe-Sanjua, 1998) in her study of term variations for identifying research topics from texts does not take into account derivational variants. Our concern is: 1. To identify noun phrases in which relational adjectives appear, as well as the prepositional phrases by which they could be paraphrased. We will see through another source presented in section 2 that this property of paraphrase can be used to identify these adjectives. 2. To check the naming character of these adjectives and to evaluate the naming character of the noun phrases in which they appear. Moreover, identifying both the adjective and the prep"
C00-1032,J97-3003,0,0.0167413,"nsg where: S is the relational suÆx to be deleted from the end of an adjective. The result of this deletion is the stem R; M is the mutative segment to be concatenated to R in order to form a noun; exceptions list the adjectives that should not be submitted to this rule. For example, the rule [ -e +e ]fageg says that if there is an adjective which ends with e, we should strip this ending from it and append the string e to the stem except if this adjective belongs to the list of exceptions, namely age. We extract these morphological rules from the corpora following the method presented in (Mikheev, 1997) with the di erence that we don&apos;t limit the length of the mutative segment. The relational suÆxes are known, only the mutative segments have to be guessed. For the lemma of an adjective ending with a relational suÆx in the corpus Adj , we strip this suÆx of Adj and store the resulting stem in R. Then, we try to segment this stem R to each noun Noun appearing in the corpus. If the subtraction result in an non-empty string, the system creates a morphological rule where the mutative segment is the result of the subtraction of R to Noun . We thus obtained couples (Adj , Noun ) associated to a morp"
C00-1032,W93-0231,0,0.032888,"nd compound nouns which include a RAdj are then quanti ed, their linguistic precision is measured and their informative status is evaluated thanks to a thesaurus of the domain. 1 Introduction Identifying relational adjectives (RAdj) such as malarial, and noun phrases in which they appear such as malarial mosquitoes, could be interesting in several elds of NLP, such as terminology acquisition, topic detection, updating of thesauri, because they hold a naming function acknowledged by linguists: (Levi, 1978), (MelisPuchulu, 1991), etc. The use of RAdj is particularly frequent in scienti c elds (Monceaux, 1993). Paradoxically, terminology acquisition systems such as TERMINO (David and Plante, 1990), LEXTER (Bourigault, 1992), TERMS (Justeson and Katz, 1995), have not been concerned with RAdj. Even (Ibekwe-Sanjua, 1998) in her study of term variations for identifying research topics from texts does not take into account derivational variants. Our concern is: 1. To identify noun phrases in which relational adjectives appear, as well as the prepositional phrases by which they could be paraphrased. We will see through another source presented in section 2 that this property of paraphrase can be used to"
C00-1032,P99-1044,0,\N,Missing
C00-1032,C98-1089,0,\N,Missing
C12-1046,C10-1073,0,0.354473,"mainspecific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) show that the discourse categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo & Gaussier (2010) show that they improve the quality of the extracted lexicon if they improve the comparability of the corpus by selecting a smaller – but more comparable – corpus from an initial set of documents. This paper proposes methods for ranking and extracting canonical translations as well as translation variants, with a special focus on the extraction of fertile translations. In parallel texts processing, the notion of fertility has been defined by Brown et al. (1993). They defined the fertility of a source word e as the number of target words to which e is connected in a randomly selected alignment."
C12-1046,J93-2003,0,0.0340751,"al. (2007) show that the discourse categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo & Gaussier (2010) show that they improve the quality of the extracted lexicon if they improve the comparability of the corpus by selecting a smaller – but more comparable – corpus from an initial set of documents. This paper proposes methods for ranking and extracting canonical translations as well as translation variants, with a special focus on the extraction of fertile translations. In parallel texts processing, the notion of fertility has been defined by Brown et al. (1993). They defined the fertility of a source word e as the number of target words to which e is connected in a randomly selected alignment. Similarly, we call a fertile translation a translation pair in which the target term has more words than the source term. The identification of fertile translations is useful because (i) they frequentlty correspond to non-canonical translations, e.g. paraphrastic variants and (ii) they tend to correspond to vulgarized forms of technical terms (e.g. « cytotoxic » vs. « toxic to the cells ») which are useful when the translator translates lay science texts. Up t"
C12-1046,J96-2004,0,0.0158976,"t translation is a canonical translation like cytoprotection → Zellschutz (DE), protection des cellules 'protection of the cells' (FR). An acceptable translation is a variant of the canonical translation: cytoprotection → protéger les cellules 'protect the cells', cytoprotecteur 'cytoprotective'. A related translation is a translation which is only semantically related to the source term: insecure → ohne Sicherheit 'without safety'. All other translations are wrong translations. We computed inter-annotator agreement on a set of 100 randomly selected translations. We used the Kappa statistics (Carletta, 1996) and obtained a high agreement (0.77 for English to German translations and 0.71 for English to French). 4 4.1 Results Related work Generally, systems are compared using the TopN precision: the percentage of source terms with at least one exact translation among the TopN candidate translations. Compositional-translation methods tend to give better results when they are applied to general language texts rather than domain-specific texts. Indeed, it is easier to find translations of the components since they belong to the general language and large corpora are also easier to collect. Working wit"
C12-1046,E09-1016,0,0.233667,", no hyphen for autonomous morphemes (a) and a plus sign (+) for intra-word morpheme boundaries. The term confix is borrowed from Martinet (1979) and refers to neoclassical (Latin or Ancient Greek) roots. 747 taux d'évaporation by translating rate to taux and evaporation to évaporation using dictionary lookup. Recomposition may be done by permuting the translated components (Morin & Daille, 2010) or with translation patterns (Baldwin & Tanaka, 2004). Sublexical compositional translation deals with single-word term translation. The atomic components are subparts of the source single-word term. Cartoni (2009) translates neologisms created by prefixation with a formalism called Bilingual Lexeme Formation Rules. Atomic components are the prefix and the lexical base: Italian neologism ricostruire 'rebuild' is translated into French reconstruire by translating the prefix ri- to re- and the lexical base costruire as construire. Weller et al. (2011) translate two types of single-word term. German single-word terms formed by the concatenation of two neoclassical roots are decomposed into these two roots, then the roots are translated into target language roots and recomposed into an English or French sin"
C12-1046,R11-1048,0,0.0237131,"p to now, fertility has received little attention in the field of comparable corpora processing. To our knowledge, only Daille & Morin (2005) and Weller et al. (2011) tried to extract translation pairs of different lengths from comparable corpora. Daille & Morin (2005) focus on the specific case of multi-word terms whose meaning is not compositional and tried to align these multi-word terms with either single-word terms or multi-word terms using a context-based approach. Weller 746 et al. (2011) concentrate on translating noun compounds as noun phrases. Similar to the approach presented here, Claveau & Kijak (2011) use translation equivalences between morphemes to generate translations and can handle fertility. However it is not suited for comparable corpora since it requires domain-specific parallel data (in their case, a multilingual terminology) to learn alignment probabilities. Our method is based on compositional translation. We chose this approach because: (i) according to Namer & Baud (2007), compositional terms form a major part of the new terms found in technical and scientific domains, this is not restricted to the field of biomedicine as it is generally believed ; (ii) compositionality-based"
C12-1046,I05-1062,1,0.647465,"elected alignment. Similarly, we call a fertile translation a translation pair in which the target term has more words than the source term. The identification of fertile translations is useful because (i) they frequentlty correspond to non-canonical translations, e.g. paraphrastic variants and (ii) they tend to correspond to vulgarized forms of technical terms (e.g. « cytotoxic » vs. « toxic to the cells ») which are useful when the translator translates lay science texts. Up to now, fertility has received little attention in the field of comparable corpora processing. To our knowledge, only Daille & Morin (2005) and Weller et al. (2011) tried to extract translation pairs of different lengths from comparable corpora. Daille & Morin (2005) focus on the specific case of multi-word terms whose meaning is not compositional and tried to align these multi-word terms with either single-word terms or multi-word terms using a context-based approach. Weller 746 et al. (2011) concentrate on translating noun compounds as noun phrases. Similar to the approach presented here, Claveau & Kijak (2011) use translation equivalences between morphemes to generate translations and can handle fertility. However it is not su"
C12-1046,W11-4610,1,0.826635,"quence, one of the difficulties with comparable corpora is that the translation of a source term may not be present in its “normalized” or “canonical” form but rather in the form of a morphological or paraphrastic variant (e.g. postmenopausal translates to après la ménopause 'after the menopause' instead of postménopausique). Another limitation is that algorithms output, for each source term, a set of candidate translations instead of just one target term. This state of affairs makes it very challenging for translators to use lexicons extracted from comparable corpora in real-life situations (Delpech, 2011). The solution that consists in increasing the size of the corpus in order to find more translation pairs or to extract parallel segments of text (Fung & Cheung, 2004; Rauf & Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domainspecific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of d"
C12-1046,2012.amta-papers.5,1,0.309745,"ns of the four. Its output is a set of single or multi-word candidate translations. For instance, postoophorectomy may be translated to postovariectomie 'postoophorectomy' or après l'ovariectomie 'after the oophorectomy' or après l'ablation des ovaires 'after the removal of the ovaries'. Section 3.2 explains the algorithm for generating candidate translations. Section 3.3 describes different methods for ranking the candidate translations. 2.2 Generation algorithm The generation method is described in the algorithm 1. A detailed version of the algorithm can be found in the feasibility study of Delpech et al. (2012). Algorithm 1 Generate translations Require: source_term, target_corpus translations ← Ø for all {c1, … ci} in DECOMPOSE (source_term) do for all {e1, … ej} in CONCATENATE ({c1, … ci}) do for all {t1, … tk} in {TRANSLATE (e1) × … TRANSLATE (ej)} do if k ≠ j then continue for all{t1, … tk} in PERMUTATE ({t1, … tk}) do for all {w1, … wl} in CONCATENATE ({t1, … tk}) do for all match in MATCH ({w1, … wl}, target_corpus) do add match to translations return translations The DECOMPOSE function splits the source term into minimal components {c1, … ci} by matching substrings of the term with lists of p"
C12-1046,J93-1003,0,0.0205869,"rce phrase to the target phrase. From these alignments, we obtain P(y|x) with the following formula: ∑ P ( y∣x)= p (t∣ s) {a∈ A∣ poss =x , post =y } ∑ p (t∣s) {a∈ A∣ poss=x } The context similarity (CONT) corresponds to the method used for ranking translations in context-based approaches. For each source term and target term we build a context vector. This vector indicates the number of times the term co-occurs with each word of the corpus within a 2 http://www.temis.com 751 contextual window of 5 words around the term. The number of co-occurrences is normalized with the log-likelihood ratio (Dunning, 1993). Then, the vector of the source term is translated into the target language. Finally, the source vector and the target vector are compared: the most similar the vectors, the most likely the target and source terms are translations of each other. The similarity between source vector s and target vector t is computed with the weighted jaccard: ∑ WeightedJaccard ( s , t)= min( c( s , w ) , c(t , w )) w∈ s∩t ∑ max (c( s , w) , c(t , w ))+ w∈s ∩t ∑ w ∈ s ∖t c( s , w)+ ∑ c (t , w) w∈t∖ s where c(s, w), respectively c(t, w), is the normalized number of co-occurrences between the source, respectively"
C12-1046,W97-0119,0,0.310218,"n for hekurudhë). 1.2 Ranking and selection methods Generally, compositional translation generates several possible translations for one source term. One has to find a way to rank the translations from the most to the least reliable. Garera & Yarowsky (2008) tried two ranking methods: (i) a probability score P based on the number of different languages exhibiting the association between the literal gloss and the fluent translation ; (ii) the probability score P combined with the similarity of the source and target words' contexts using context-based methods like in the work of Rapp (1995) and Fung (1997). Robitaille et al. (2006) extract translation pairs from a corpus built by querying a search engine with a set of seed translation pairs. They select the candidate translations which are semantically related to the target seed terms. The semantic similarity measure is based on the number of hits containing the seed term and/or the candidate translation (Jaccard coefficient). Other works simply select the candidate translations which occur in the target corpus (Weller et al., 2001 ; Morin and Daille, 2010) or which are significantly attested on the Web (Cartoni, 2009). Only Baldwin and Takana"
C12-1046,W04-3208,0,0.0176812,"ather in the form of a morphological or paraphrastic variant (e.g. postmenopausal translates to après la ménopause 'after the menopause' instead of postménopausique). Another limitation is that algorithms output, for each source term, a set of candidate translations instead of just one target term. This state of affairs makes it very challenging for translators to use lexicons extracted from comparable corpora in real-life situations (Delpech, 2011). The solution that consists in increasing the size of the corpus in order to find more translation pairs or to extract parallel segments of text (Fung & Cheung, 2004; Rauf & Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domainspecific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) sho"
C12-1046,I08-1053,0,0.108481,"refix ri- to re- and the lexical base costruire as construire. Weller et al. (2011) translate two types of single-word term. German single-word terms formed by the concatenation of two neoclassical roots are decomposed into these two roots, then the roots are translated into target language roots and recomposed into an English or French single-word term, e.g. Kalori1metrie2 is translated as calori1metry2. German NOUN1+NOUN2 compounds are translated into French and English NOUN1 NOUN2 or NOUN1 PREP NOUN2 multi-word terms, e.g. Elektronen N1-mikroskopN2 is translated to electronN1 microscopeN2. Garera & Yarowsky (2008) translate various compound sequences (NOUN1+NOUN2, ADJ1+NOUN2 …). They generate an English literal gloss of the compounds with the compositional method (for instance, the English gloss for the Albanian word hekurudhë 'railway' is iron path). Then, they search for entries in Lx-to-English dictionaries where the entry in language Lx is a word-to-word translation of the English gloss (e.g. iron path matches the German entry Eisenbahn and the Italian entry ferrovia). The final candidate translations are the fluent English translations proposed by the bilingual dictionaries (e.g. Eisenbahn and fer"
C12-1046,1999.tc-1.8,0,0.148055,"5). Once the candidate translations have been generated, one generally ranks them and selects the TopN candidate translations. Generation methods are described in section 1.1. Ranking methods are described in section 2.3. 1.1 Generation methods Compositional translation consists in decomposing the source term into atomic components, translating these components into the target language and recomposing the translated components into target terms. Existing implementations differ on the kind of atomic components they use for translation. Lexical compositional translation (Baldwin & Tanaka, 2004; Grefenstette, 1999; Morin & Daille, 2009; Robitaille et al., 2006) deals with multi-word term to multi-word term alignment and uses lexical words as atomic components: rate of evaporation is translated into French as 1 We use the following notations: trailing hyphen for prefixes (a-), leading hyphen for suffixes (-a), both for confixes (a-), no hyphen for autonomous morphemes (a) and a plus sign (+) for intra-word morpheme boundaries. The term confix is borrowed from Martinet (1979) and refers to neoclassical (Latin or Ancient Greek) roots. 747 taux d'évaporation by translating rate to taux and evaporation to é"
C12-1046,I11-1097,0,0.0204218,"5k 451.75k 398.9k TABLE 1: Composition and size of corpora (nb. of words) 3.2 Resources for generation Tables 2 and 3 show the size of the resources we used for generation. General language dictionary: We used the dictionary which is part of the XELDA software. This dictionary was used for generating translations but also for computing the corpus comparability and for translating the context vectors for the context similarity measure (CONT score). Cognate dictionary: We built this resource automatically by extracting pairs of cognates from the comparable corpora. We used the same technique as Hauer & Kondrak (2011): a SVM classifier trained on examples taken from online dictionaries 6. Morpheme translation table: this resource was created manually by translators since there exists no publicly available morphology-based bilingual dictionary. This translation table links the English bound morphemes contained in the source terms to their French or German equivalents (which can be bound morphemes or lexical items). In order to handle the variation phenomena described in section 1.3, we used a dictionary of synonyms and lists of morphologically related words. The dictionary of synonyms is part of the XELDA s"
C12-1046,2008.amta-papers.11,0,0.0295586,"rately or in a combined manner. The frequency (FREQ) corresponds to the number of occurrences of the translation in the target corpus divided by the total number of words in the target corpus. The part-of-speech translation probability (POS) corresponds to P(y|x), the probability that a source term with part-of-speech x will be translated to a target term with part(s)-of-speech y, e.g. it is more probable that a NOUN is translated by another NOUN or by a NOUN PREP NOUN sequence rather than an ADVERB. The part-of-speech translation probabilities were acquired by running the software ANYMALIGN (Lardilleux, 2008) on the EMEA corpus (Tiedemann, 2009) which had been previously pos-tagged with the linguistic analyzer XELDA2. ANYMALIGN outputs a phrase translation table. Each line of the translation table corresponds to an alignment a = {lems, poss, lemt, post, p(s|t), p(t|s)} where poss is the parts-of-speech of the source phrase, post is the parts-ofspeech of the target phrase and pos(t|s) is the probability of translating the source phrase to the target phrase. From these alignments, we obtain P(y|x) with the following formula: ∑ P ( y∣x)= p (t∣ s) {a∈ A∣ poss =x , post =y } ∑ p (t∣s) {a∈ A∣ poss=x } T"
C12-1046,P07-1084,1,0.905648,"xt (Fung & Cheung, 2004; Rauf & Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domainspecific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) show that the discourse categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo & Gaussier (2010) show that they improve the quality of the extracted lexicon if they improve the comparability of the corpus by selecting a smaller – but more comparable – corpus from an initial set of documents. This paper proposes methods for ranking and extracting canonical translations as well as translation variants, with a special focus on the extraction of fertile translations. In parallel texts processing, the notion of fertility has been defined by Brown et al"
C12-1046,quasthoff-etal-2006-corpus,0,0.0198487,"o handle the variation phenomena described in section 1.3, we used a dictionary of synonyms and lists of morphologically related words. The dictionary of synonyms is part of the XELDA software. Morphologically related words were collected by stemming the words of the comparable corpora and the entries of the bilingual dictionary with the algorithm of Porter (1980). The DECOMPOSE function uses the entries of the morpheme translation table (242 entries) and a list of 85k lexical items composed of the entries of the general language dictionary and English words extracted from the Leipzig Corpus (Quasthoff et al., 2006). 5 We used the measure defined by Bo & Gaussier (2010) which indicates, given a bilingual dictionary, the expectation of finding, for each word of the source corpus, its translation in the target corpus and vice-versa. 6 http://www.dicts.info/uddl.php 753 EN → FR EN → DE General language 38k → 60k 38k → 70k Domain specific 6.7k → 6.7k 6.4k → 6.4k Morphemes (TOTAL) 242 → 729 242 → 761 Prefixes 50 → 134 50 → 166 Confixes 185 → 574 185 → 563 Suffixes 7 → 21 7 → 32 TABLE 2: Nb. of entries in the multilingual resources EN → EN FR → FR DE → DE Synonyms 5.1k → 7.6k 2.4k → 3.2k 4.2k → 4.9k Morphologi"
C12-1046,P95-1050,0,0.400212,"ntial translation for hekurudhë). 1.2 Ranking and selection methods Generally, compositional translation generates several possible translations for one source term. One has to find a way to rank the translations from the most to the least reliable. Garera & Yarowsky (2008) tried two ranking methods: (i) a probability score P based on the number of different languages exhibiting the association between the literal gloss and the fluent translation ; (ii) the probability score P combined with the similarity of the source and target words' contexts using context-based methods like in the work of Rapp (1995) and Fung (1997). Robitaille et al. (2006) extract translation pairs from a corpus built by querying a search engine with a set of seed translation pairs. They select the candidate translations which are semantically related to the target seed terms. The semantic similarity measure is based on the number of hits containing the seed term and/or the candidate translation (Jaccard coefficient). Other works simply select the candidate translations which occur in the target corpus (Weller et al., 2001 ; Morin and Daille, 2010) or which are significantly attested on the Web (Cartoni, 2009). Only Bal"
C12-1046,E09-1003,0,0.0222349,"a morphological or paraphrastic variant (e.g. postmenopausal translates to après la ménopause 'after the menopause' instead of postménopausique). Another limitation is that algorithms output, for each source term, a set of candidate translations instead of just one target term. This state of affairs makes it very challenging for translators to use lexicons extracted from comparable corpora in real-life situations (Delpech, 2011). The solution that consists in increasing the size of the corpus in order to find more translation pairs or to extract parallel segments of text (Fung & Cheung, 2004; Rauf & Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domainspecific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) show that the discourse ca"
C12-1046,E06-1029,0,0.642367,"een generated, one generally ranks them and selects the TopN candidate translations. Generation methods are described in section 1.1. Ranking methods are described in section 2.3. 1.1 Generation methods Compositional translation consists in decomposing the source term into atomic components, translating these components into the target language and recomposing the translated components into target terms. Existing implementations differ on the kind of atomic components they use for translation. Lexical compositional translation (Baldwin & Tanaka, 2004; Grefenstette, 1999; Morin & Daille, 2009; Robitaille et al., 2006) deals with multi-word term to multi-word term alignment and uses lexical words as atomic components: rate of evaporation is translated into French as 1 We use the following notations: trailing hyphen for prefixes (a-), leading hyphen for suffixes (-a), both for confixes (a-), no hyphen for autonomous morphemes (a) and a plus sign (+) for intra-word morpheme boundaries. The term confix is borrowed from Martinet (1979) and refers to neoclassical (Latin or Ancient Greek) roots. 747 taux d'évaporation by translating rate to taux and evaporation to évaporation using dictionary lookup. Recompositio"
C12-1110,1999.tc-1.8,0,\N,Missing
C12-1110,J93-2003,0,\N,Missing
C12-1110,C02-2020,0,\N,Missing
C12-1110,C02-1065,0,\N,Missing
C12-1110,E06-1029,0,\N,Missing
C12-1110,W03-1803,0,\N,Missing
C12-1110,W04-0404,0,\N,Missing
C12-1110,P99-1067,0,\N,Missing
C12-1110,W02-0902,0,\N,Missing
C12-1110,P04-1067,0,\N,Missing
C12-1110,P06-1011,0,\N,Missing
C12-1110,C10-1070,0,\N,Missing
C12-1110,P07-1084,1,\N,Missing
C12-1110,I11-2003,1,\N,Missing
C16-1277,I13-1062,1,0.943583,"tribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 2945 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2945–2955, Osaka, Japan, December 11-17 2016. experiments on bibliographic records in three domains belonging to humanities and social sciences: linguistics, information science and archaeology. Along with this approach come two contributions. First, we present a simple yet efficient assignment extension of a state-of-the-art graph-based keyphrase extraction method, TopicRank (Bougouin et al., 2013). Second, we circumvent the need for a controlled vocabulary by leveraging reference keyphrases from training data and further take advantage of their relationship within the training data. 2 2.1 Related Work Keyphrase extraction Keyphrase extraction is the most common approach to tackle the automatic keyphrase annotation task. Previous work includes many approaches (Hasan and Ng, 2014), from statistical ranking (Salton et al., 1975) to binary classification (Witten et al., 1999), through graph-based ranking (Mihalcea and Tarau, 2004) of keyphrase candidates. As our approach uses graph-based r"
C16-1277,Y09-1013,0,0.023846,"omains. More than half of the keyphrases of linguistics and information science domains can only be assigned, which confirms that these two datasets are difficult to process with keyword extraction approaches alone. 4.2 Document preprocessing We apply the following preprocessing steps to each document: sentence segmentation, word tokenization and Part-of-Speech (POS) tagging. Sentence segmentation is performed with the PunktSentenceTokenizer provided by the Python Natural Language ToolKit (NLTK) (Bird et al., 2009), word tokenization using the Bonsai word tokenizer5 and POS tagging with MElt (Denis and Sagot, 2009). 4.3 Baselines To show the effectiveness of our approach, we compare TopicCoRank and its variants (TopicCoRankextr and TopicCoRankassign ) with TopicRank and KEA++. For KEA++, we use the thesauri maintained by Inist6 to index the bibliographic records of Linguistics, Information Science and Archaeology. 4.4 TopicCoRank setting The λt and λk parameters of TopicCoRank were tuned on the development sets, and set to 0.1 and 0.5 respectively. This empirical setup means that the importance of topics is much more influenced by controlled keyphrases than other topics, and that the importance of contr"
C16-1277,C10-2042,0,0.51203,"m training data and further take advantage of their relationship within the training data. 2 2.1 Related Work Keyphrase extraction Keyphrase extraction is the most common approach to tackle the automatic keyphrase annotation task. Previous work includes many approaches (Hasan and Ng, 2014), from statistical ranking (Salton et al., 1975) to binary classification (Witten et al., 1999), through graph-based ranking (Mihalcea and Tarau, 2004) of keyphrase candidates. As our approach uses graph-based ranking, we focus on the latter. For a detailed overview of keyphrase extraction methods, refer to (Hasan and Ng, 2010; Hasan and Ng, 2014). Since the seminal work of Mihalcea and Tarau (2004), graph-based ranking approaches to keyphrase extraction are becoming increasingly popular. The original idea behind these approaches is to build a graph from the document and rank its nodes according to their importance using centrality measures. In TextRank (Mihalcea and Tarau, 2004), the input document is represented as a co-occurrence graph in which nodes are words. Two words are connected by an edge if they co-occur in a fixed-sized window of words. A random walk algorithm is used to iteratively rank the words, then"
C16-1277,P14-1119,0,0.45299,"se assignment state-of-the art methods. 1 Introduction Keyphrases are words and phrases that give a synoptic picture of what is important within a document. They are useful in many tasks such as document indexing (Gutwin et al., 1999), text categorization (Hulth and Megyesi, 2006) or summarization (Litvak and Last, 2008). However, most documents do not provide keyphrases, and the daily flow of new documents makes the manual keyphrase annotation impractical. As a consequence, automatic keyphrase annotation has received special attention in the NLP community and many methods have been proposed (Hasan and Ng, 2014). The task of automatic keyphrase annotation consists in identifying the main concepts, or topics, addressed in a document. Such task is crucial to access relevant scientific documents that could be useful for researchers. Keyphrase annotation methods fall into two broad categories: keyphrase extraction and keyphrase assignment methods. Keyphrase extraction methods extract the most important words or phrases occurring in a document, while assignment methods provide controlled keyphrases from a domain-specific terminology (controlled vocabulary). The automatic keyphrase annotation task is often"
C16-1277,P06-1068,0,0.0350572,"aper proposes a new method to perform both keyphrase extraction and keyphrase assignment in an integrated and mutual reinforcing manner. Experiments have been carried out on datasets covering different domains of humanities and social sciences. They show statistically significant improvements compared to both keyphrase extraction and keyphrase assignment state-of-the art methods. 1 Introduction Keyphrases are words and phrases that give a synoptic picture of what is important within a document. They are useful in many tasks such as document indexing (Gutwin et al., 1999), text categorization (Hulth and Megyesi, 2006) or summarization (Litvak and Last, 2008). However, most documents do not provide keyphrases, and the daily flow of new documents makes the manual keyphrase annotation impractical. As a consequence, automatic keyphrase annotation has received special attention in the NLP community and many methods have been proposed (Hasan and Ng, 2014). The task of automatic keyphrase annotation consists in identifying the main concepts, or topics, addressed in a document. Such task is crucial to access relevant scientific documents that could be useful for researchers. Keyphrase annotation methods fall into"
C16-1277,W08-1404,0,0.117383,"keyphrase extraction and keyphrase assignment in an integrated and mutual reinforcing manner. Experiments have been carried out on datasets covering different domains of humanities and social sciences. They show statistically significant improvements compared to both keyphrase extraction and keyphrase assignment state-of-the art methods. 1 Introduction Keyphrases are words and phrases that give a synoptic picture of what is important within a document. They are useful in many tasks such as document indexing (Gutwin et al., 1999), text categorization (Hulth and Megyesi, 2006) or summarization (Litvak and Last, 2008). However, most documents do not provide keyphrases, and the daily flow of new documents makes the manual keyphrase annotation impractical. As a consequence, automatic keyphrase annotation has received special attention in the NLP community and many methods have been proposed (Hasan and Ng, 2014). The task of automatic keyphrase annotation consists in identifying the main concepts, or topics, addressed in a document. Such task is crucial to access relevant scientific documents that could be useful for researchers. Keyphrase annotation methods fall into two broad categories: keyphrase extractio"
C16-1277,D10-1036,0,0.35076,"´ λq ` λ ÿ vj PEpvi q Spvj q |Epvj q| (1) where λ is a damping factor that has been set to 0.85 by Brin and Page (1998) for a trade-off between ranking accuracy and fast convergence. Following up the work of Mihalcea and Tarau (2004), Wan and Xiao (2008) added edge weights (cooccurrence numbers) to the random walk and further improved the graph with co-occurrence information borrowed from similar documents. To extract keyphrases from a document, they first look for five similar documents, then use them to add new edges between words within the graph and reinforce the weight of existing edges. Liu et al. (2010) biased multiple graphs with topic probabilities drawn from LDA (Latent Dirichlet Allocation) (Blei et al., 2003), to rank the words regarding each graph and to merge the rankings together. This method performs as many rankings as the number of topics and gives higher importance scores to high-ranking words for as many topics as possible. By doing so, Liu et al. (2010) increase the topic coverage provided by the extracted keyphrases. Most recently, Zhang et al. (2013) and Bougouin et al. (2013) explored further the value of topics for keyphrase extraction. Zhang et al. (2013) used graph co-ran"
C16-1277,W11-0316,0,0.141108,"bularies that are costly to create and to maintain. Furthermore, they are able to identify new concepts that have not been yet recorded in the thesaurus or ontologies. However, extraction methods often output ill-formed or inappropriate keyphrases (Medelyan and Witten, 2008), and they produce only keyphrases that actually occur in the document. Observations made on manually assigned keyphrases from scientific papers of specialized domains show that professional human indexers both extract keyphrases from the content of the document and assign keyphrases based on their knowledge of the domain (Liu et al., 2011). Here, we propose an approach that mimics this behaviour and jointly extracts and assigns keyphrases. We use two graph representations, one for the document and one for the specialized domain. Then, we apply a co-ranking algorithm to perform both keyphrase extraction and assignment in a mutually reinforcing manner. We perform This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 2945 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 29"
C16-1277,I13-1002,0,0.613551,"five similar documents, then use them to add new edges between words within the graph and reinforce the weight of existing edges. Liu et al. (2010) biased multiple graphs with topic probabilities drawn from LDA (Latent Dirichlet Allocation) (Blei et al., 2003), to rank the words regarding each graph and to merge the rankings together. This method performs as many rankings as the number of topics and gives higher importance scores to high-ranking words for as many topics as possible. By doing so, Liu et al. (2010) increase the topic coverage provided by the extracted keyphrases. Most recently, Zhang et al. (2013) and Bougouin et al. (2013) explored further the value of topics for keyphrase extraction. Zhang et al. (2013) used graph co-ranking to improve the method of Liu et al. (2010) by introducing LDA topics right inside the graph. Bougouin et al. (2013) proposed to represent topics as clusters of similar keyphrase candidates within the document (i.e. words and phrases from the document), to rank these topics instead of the words and to extract the most representative candidate as keyphrase for each important topic. As our work extends that of Bougouin et al. (2013), we present a detailed descriptio"
C16-1277,W04-3252,0,\N,Missing
C94-1084,C92-3150,0,0.182728,"Missing"
C94-1084,C88-1016,0,0.0119322,"Missing"
C94-1084,W94-0104,1,0.707102,"Missing"
C94-1084,J93-1003,0,0.0174044,"Missing"
C94-1084,P90-1032,0,0.0154514,"Missing"
C94-1084,E93-1015,0,0.194105,"Missing"
C94-1084,J90-1003,0,\N,Missing
daille-etal-2004-french,C02-1166,0,\N,Missing
daille-etal-2004-french,C02-1011,0,\N,Missing
daille-etal-2004-french,W02-1402,0,\N,Missing
daille-etal-2004-french,P99-1067,0,\N,Missing
daille-etal-2004-french,C94-1084,1,\N,Missing
daille-hazem-2014-semi,ferret-2010-testing,0,\N,Missing
daille-hazem-2014-semi,W04-2607,0,\N,Missing
daille-hazem-2014-semi,P08-3001,0,\N,Missing
daille-hazem-2014-semi,P90-1034,0,\N,Missing
daille-hazem-2014-semi,W03-1610,0,\N,Missing
daille-hazem-2014-semi,C10-1070,0,\N,Missing
daille-hazem-2014-semi,P06-2111,0,\N,Missing
daille-hazem-2014-semi,P07-1084,1,\N,Missing
daille-hazem-2014-semi,P98-2127,0,\N,Missing
daille-hazem-2014-semi,C98-2122,0,\N,Missing
daille-hazem-2014-semi,I13-1150,0,\N,Missing
daille-hazem-2014-semi,C12-1110,1,\N,Missing
F12-2011,W04-0404,0,0.0541818,"Missing"
F12-2011,C02-2020,0,0.049523,"Missing"
F12-2011,P04-1067,0,0.08128,"Missing"
F12-2011,1999.tc-1.8,0,0.0327037,"Missing"
F12-2011,P08-1088,0,0.0753205,"Missing"
F12-2011,C10-2055,0,0.025603,"Missing"
F12-2011,W02-0902,0,0.106214,"Missing"
F12-2011,C10-1070,0,0.0376199,"Missing"
F12-2011,C10-1073,0,0.0274471,"Missing"
F12-2011,P06-1011,0,0.0587709,"Missing"
F12-2011,P99-1067,0,0.0513396,"Missing"
F12-2011,E06-1029,0,0.0339711,"Missing"
F12-2011,P10-1011,0,0.0382427,"Missing"
F12-2011,C02-1065,0,0.0648446,"Missing"
F12-2011,W03-1803,0,0.0668664,"Missing"
F13-1023,W04-0404,0,0.087688,"Missing"
F13-1023,E09-1016,0,0.0392405,"Missing"
F13-1023,C00-1032,1,0.71285,"Missing"
F13-1023,I11-1097,0,0.0312636,"Missing"
F13-1023,P95-1050,0,0.315621,"Missing"
F13-1023,I11-2003,1,0.900113,"Missing"
F13-2008,P11-2095,0,0.0421305,"Missing"
F13-2008,E03-1076,0,0.126354,"Missing"
F13-2008,P11-1140,0,0.0945922,"La composition est un mécanisme de formation des mots qui consiste à combiner deux (ou plusieurs) éléments lexicaux autonomes pour former une unité de sens. Ce phénomène est notamment présent dans les langues allemande, néerlandaise, grecque, suédoise, danoise, finlandaise et russe. Le traitement des mots composés est une diﬃculté pour les systèmes de traitement automatique des langues parce que la plupart des composés ne sont pas recensés dans les ressources lexicales. Ainsi leur reconnaissance et leur segmentation seraient bénéfiques pour des tâches variées du TAL : traduction automatique (Macherey et al. (2011), Weller et Heid (2012)), recherche d’information (Braschler et Ripplinger, 2004), recherche d’information multilingue (Chen et Gey, 2001), etc. Les mécanismes de composition sont plus ou moins complexes en fonction des langues. Dans les langues très analytiques comme les langues française et anglaise les composants sont simplement concaténés : FR kilowatt-heure, EN parrotfish 1 , « poisson perroquet ». Dans les langues ayant une morphologie riche, des transformations sont possibles aux frontières des parties composantes. La terminaison du mot peut être omise, et/ou des morphèmes « frontières"
F13-2008,weller-heid-2012-analyzing,0,0.0227535,"Missing"
fourour-etal-2002-incremental,C96-1071,0,\N,Missing
fourour-etal-2002-incremental,E95-1004,0,\N,Missing
fourour-etal-2002-incremental,A97-1030,0,\N,Missing
I05-1062,C02-1011,0,0.0455052,"Missing"
I05-1062,W02-1402,0,0.0369952,"n single words and MWTs by exploiting the term contexts. After explaining the diﬃculties involved in aligning MWTs and specifying our approach, we show the adopted process for bilingual terminology extraction and the resources used in our experiments. Finally, we evaluate our approach and demonstrate its signiﬁcance, particularly in relation to non-compositional MWT alignment. 1 Introduction Traditional research into the automatic compilation of bilingual dictionaries from corpora exploits parallel texts, i.e. a text and its translation [17]. From sentenceto-sentence aligned corpora, symbolic [2], statistical [11], or combined [7] techniques are used for word and expression alignments. The use of parallel corpora raises two problems: – as a parallel corpus is a pair of translated texts, the vocabulary appearing in the translated text is highly inﬂuenced by the source text, especially for technical domains; – such corpora are diﬃcult to obtain for paired languages not involving English. New methods try to exploit comparable corpora: texts that are of the same text type and on the same subject without a source text-target text relationship. The main studies concentrate on ﬁnding in such"
I05-1062,C02-2020,0,0.632581,"Missing"
I05-1062,W03-1802,1,0.572123,"extraction, a word-to-word assumption being generally adopted. – When a MWT is translated into a MWT of the same length, the target sequence is not typically composed of the translation of its parts [13]. For example, the French term plantation ´energ´etique is translated into English as fuel plantation where fuel is not the translation of ´energ´etique. This property is referred to as “non-compositionality”. French-English Terminology Extraction from Comparable Corpora 709 – A MWT could appear in texts under diﬀerent forms reﬂecting either syntactic, morphological or semantic variations [12],[5]. Term variations should be taken into account in the translation process. For example, the French sequences am´enagement de la forˆet and am´enagement forestier refer to the same MWT and are both translated into the same English term: forest management. We propose tackling these three problems, fertility, non-compositionality, and variations, by using both linguistic and statistical methods. First, MWTs are identiﬁed in both the source and target language using a monolingual term extraction program. Second, a statistical alignment algorithm is used to link MWTs in the source language to singl"
I05-1062,C94-1084,1,0.554837,"ng the term contexts. After explaining the diﬃculties involved in aligning MWTs and specifying our approach, we show the adopted process for bilingual terminology extraction and the resources used in our experiments. Finally, we evaluate our approach and demonstrate its signiﬁcance, particularly in relation to non-compositional MWT alignment. 1 Introduction Traditional research into the automatic compilation of bilingual dictionaries from corpora exploits parallel texts, i.e. a text and its translation [17]. From sentenceto-sentence aligned corpora, symbolic [2], statistical [11], or combined [7] techniques are used for word and expression alignments. The use of parallel corpora raises two problems: – as a parallel corpus is a pair of translated texts, the vocabulary appearing in the translated text is highly inﬂuenced by the source text, especially for technical domains; – such corpora are diﬃcult to obtain for paired languages not involving English. New methods try to exploit comparable corpora: texts that are of the same text type and on the same subject without a source text-target text relationship. The main studies concentrate on ﬁnding in such corpora translation candidates for"
I05-1062,C02-1166,0,0.686416,"Missing"
I05-1062,P99-1067,0,0.816182,"Missing"
I08-1013,J97-3003,0,\N,Missing
I08-1013,W97-0119,0,\N,Missing
I08-1013,1999.tc-1.8,0,\N,Missing
I08-1013,C02-1166,0,\N,Missing
I08-1013,J93-2003,0,\N,Missing
I08-1013,C02-2020,0,\N,Missing
I08-1013,E06-1029,0,\N,Missing
I08-1013,W04-0404,0,\N,Missing
I08-1013,P99-1067,0,\N,Missing
I08-1013,P97-1063,0,\N,Missing
I08-1013,P07-1084,1,\N,Missing
I11-1173,J08-4004,0,0.12866,"Missing"
I11-1173,barron-cedeno-etal-2010-corpus,0,0.064459,"Missing"
I11-1173,W03-1004,0,0.0201731,"irs from a total of 28,441 paragraph pairs which took about 71 hours of work. The second phase was done manually by two annotators who independently selected similar pairs from the candidate pairs. The similarity definition given to the annotators is an intuitive definition which states that two paragraphs are similar if one of the main information that the paragraph conveys is common. This definition is slightly different from the definition given by Shrestha (2011a) based on sub-topics. There exist few definitions on text similarity but they are all specific to the size of the text segment (Barzilay, 2003) or entities within the sentences (Hatzivassiloglou and Klavans, 2001) which make them unsuitable for a general text similarity definition. In Table 1, we present a positive and a negative example to further explain the definition. The first block presents a positive example whose main information in common is that the royal family will remain at Balmoral Castle. The paragraph pair in the second block is not similar even though the information about Dodi’s father is a businessman is common because the main information conveyed by the paragraphs is different. We used kappa statistics (Carletta,"
I11-1173,J96-2004,0,0.254296,"Missing"
I11-1173,W99-0625,0,0.0127804,"h promising results. 1 Introduction In the field of Natural Language Processing (NLP), annotated monolingual corpora are used to build and test a wide range of applications such as information retrieval, summarization, plagiarism detection, dictionary building and so on. With a number of applications to be built, the field of NLP requires a wide range of monolingual corpus with different annotations on different level of text segments. Our focus is on the fast and easy way of aligning short text segments based on similarity. These annotations are usually done manually by annotators as done by Hatzivassiloglou et al. (1999) where a corpus with alignments between similar short texts are created by two or more annotators who look at each possible short text pair independently and analyse them to make a decision on whether each pair should be aligned as similar or not. Finally, the annotators discuss the disagreements between their annotations and come to an agreement with reasoning. A corpus containing n number of short texts will generate n(n−1) number of short text pairs for compar2 ing similarities which becomes a tedious and time consuming task even if a corpus contains a few hundred of short texts. For exampl"
I11-1173,2011.jeptalnrecital-recital.5,1,0.676452,"ate similar pairs which is a subset of the total short text pairs within which all the actual similar pairs are present. The number of candidate similar pairs will be less than the total number of short text pairs which allows many annotators in the second phase to efficiently annotate the small set of text pairs manually in less human hours. The manual reduction of search space is done by going through all the possible short text pairs and selecting the candidate similar pairs using a criteria which states that: each short text in a candidate similar pair consists at least one common entity (Shrestha, 2011a). This criteria for selection theoretically guarantees that all the actual similar pairs will be present in the candidate similar pairs because for two short texts to be similar they must have at least one entity in common. The entities that we use are noun, noun phrase, and transitive verb (Loberger and Shoup, 2009). Two entities are said to be common when they both have the same meaning or in other words share the same concept for example, the entities ‘crashed’, ‘rammed into a wall’, ‘fatal impact’ can all be mapped to the concept ‘crashed’ and the entities ‘Prince Charles’, ‘heir to the"
I11-1173,2011.jeptalnrecital-recitalcourt.1,1,0.820964,"ate similar pairs which is a subset of the total short text pairs within which all the actual similar pairs are present. The number of candidate similar pairs will be less than the total number of short text pairs which allows many annotators in the second phase to efficiently annotate the small set of text pairs manually in less human hours. The manual reduction of search space is done by going through all the possible short text pairs and selecting the candidate similar pairs using a criteria which states that: each short text in a candidate similar pair consists at least one common entity (Shrestha, 2011a). This criteria for selection theoretically guarantees that all the actual similar pairs will be present in the candidate similar pairs because for two short texts to be similar they must have at least one entity in common. The entities that we use are noun, noun phrase, and transitive verb (Loberger and Shoup, 2009). Two entities are said to be common when they both have the same meaning or in other words share the same concept for example, the entities ‘crashed’, ‘rammed into a wall’, ‘fatal impact’ can all be mapped to the concept ‘crashed’ and the entities ‘Prince Charles’, ‘heir to the"
I11-1173,I05-5002,0,0.0407026,"ompar2 ing similarities which becomes a tedious and time consuming task even if a corpus contains a few hundred of short texts. For example, the corpus we use consists of 239 paragraphs, explained in section 3.1, generating a total of 28,441 text pairs to compare. There are few publicly available annotated corpus, some are manually annotated like the TDT corpus1 for topic detection and tracking and the METER corpus (Gaizauskas et al., 2001) for detection of text reuse and some are automatically annotated like the PAN-PC-10 (Barr´on-Cede˜no et al., 2010) for plagiarism detection and the MSRPC (Dolan and Brockett, 2005) for paraphrase detection. Annotating a corpus automatically is easier and faster than manual annotation but they have a major limitation which allows the corpus to include only a subset of the problem which prevents the corpus to represent many of the naturally occurring instances. This limitation in turn might cause some incompleteness issues on the applications built on it as mentioned by Barr´on-Cede˜no et al. (2010) and Dolan and Brockett (2005). To reduce this effect of coverage in a corpus, annotations on corpus are done manually. We propose manual annotation to be done in two phases. A"
I11-1173,L10-1000,0,\N,Missing
I11-2003,2005.mtsummit-papers.11,0,0.0387463,"Missing"
I11-2003,W07-2456,0,0.0313367,"Missing"
I11-2003,J03-1002,0,0.00539921,"Missing"
I11-2003,P95-1050,0,0.652703,"Missing"
I13-1046,P13-2133,0,0.0122308,"and align the best source and target sentences that contain the term and its translation candidates. We report results with two language pairs (French-English and FrenchGerman) using domain-specific comparable corpora. Our method significantly improves the top 1, top 5 and top 10 precisions of a domain-specific bilingual lexicon, and thus, provides a better useroriented results. 1 Introduction Comparable corpora have been the subject of interest for extracting bilingual lexicons by several researchers (Rapp, 1995; Fung and Mckeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Morin et al., 2008; Bouamor et al., 2013, among others). Rapp (1995) was the first to suggest that if a word A co-occurs frequently with another word B in one language, then the translation of A and the translation of B should co-occur frequently in another language. Approaches emerging from (Rapp, 1995) make different assumptions to extract bilingual lexicon from comparable corpora. However, they are all based on the assumption that a translation pair shares some similar context in comparable corpora. We refer to such approaches that depend on co-occurrences of 401 International Joint Conference on Natural Language Processing, page"
I13-1046,P11-1140,0,0.0260098,"s technical. The results are also significantly better with the French-English language pair than with the French-German language pair. In fact, domainspecific corpora contain many terms that are compound nouns. In the German language, many compound nouns may be written as single units (e.g. German term “Produktionsstandort” is translated into French by “site de production”). Therefore, the distributional approach may consider such German terms as one word when computing co-occurrences. One way to overcome this problem would be to perform splitting before applying the distributional approach (Macherey et al., 2011). To analyze the results obtained by the distributional method in more depth, we measured the comparability of Wind Energy corpora for the different language pairs, using the comparability measure presented by Li et al. (2011). For the French-English corpora, we obtained a comparability value of 0.81. As for the French-German corpora, we obtained a comparability value of 0.70. This implies that our French-German corpora are less comparable than the French-English corpora, and partly justifies the reason behind obtaining worse results with the French-German pair using the distributional method."
I13-1046,C02-2020,0,0.0610457,"Missing"
I13-1046,W04-3208,0,0.174003,"ntains terms that are highly related to the “Breast Cancer” subject (e.g. chemotherapy, histological). Our assumption is that the best context (represented by sentences) can be extracted for a term as well as for its translation candidates, and that these extracted sentences can be aligned in order to re-rank the translation candidates of the term. After obtaining some candidate translations for Figure 1: Method to score a translation pair (source term and target term) 402 Parallel sentence (or fragment) extraction from comparable corpora has received the attention of a number of researchers (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Munteanu and Marcu, 2006; Smith et al., 2010; Hunsicker et al., 2012, among others), to enrich parallel text used by statistical machine translation (SMT) systems. They conducted experiments with large corpora (mainly news stories) which were noisy parallel, comparable (contain topic alignments or articles published in similar circumstances), or very non-parallel (Fung and Cheung, 2004). Usually, these approaches perform document-level alignments before extracting parallel sentences. The domain-specific corpora we use contain few documents (ranging from 38 to 262 do"
I13-1046,J05-4003,0,0.590585,"ighly related to the “Breast Cancer” subject (e.g. chemotherapy, histological). Our assumption is that the best context (represented by sentences) can be extracted for a term as well as for its translation candidates, and that these extracted sentences can be aligned in order to re-rank the translation candidates of the term. After obtaining some candidate translations for Figure 1: Method to score a translation pair (source term and target term) 402 Parallel sentence (or fragment) extraction from comparable corpora has received the attention of a number of researchers (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Munteanu and Marcu, 2006; Smith et al., 2010; Hunsicker et al., 2012, among others), to enrich parallel text used by statistical machine translation (SMT) systems. They conducted experiments with large corpora (mainly news stories) which were noisy parallel, comparable (contain topic alignments or articles published in similar circumstances), or very non-parallel (Fung and Cheung, 2004). Usually, these approaches perform document-level alignments before extracting parallel sentences. The domain-specific corpora we use contain few documents (ranging from 38 to 262 documents for each corpus) a"
I13-1046,P06-1011,0,0.0285608,"st Cancer” subject (e.g. chemotherapy, histological). Our assumption is that the best context (represented by sentences) can be extracted for a term as well as for its translation candidates, and that these extracted sentences can be aligned in order to re-rank the translation candidates of the term. After obtaining some candidate translations for Figure 1: Method to score a translation pair (source term and target term) 402 Parallel sentence (or fragment) extraction from comparable corpora has received the attention of a number of researchers (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Munteanu and Marcu, 2006; Smith et al., 2010; Hunsicker et al., 2012, among others), to enrich parallel text used by statistical machine translation (SMT) systems. They conducted experiments with large corpora (mainly news stories) which were noisy parallel, comparable (contain topic alignments or articles published in similar circumstances), or very non-parallel (Fung and Cheung, 2004). Usually, these approaches perform document-level alignments before extracting parallel sentences. The domain-specific corpora we use contain few documents (ranging from 38 to 262 documents for each corpus) and no parallel sentences."
I13-1046,W97-0119,0,0.339849,"stani and B´eatrice Daille and Emmanuel Morin LINA UMR CNRS 6241 - University of Nantes 2 rue de la Houssini`ere, BP 92208 44322 Nantes, France {rima.harastani,beatrice.daille,emmanuel.morin}@univ-nantes.fr Abstract words to extract a bilingual lexicon by distributional approaches. Results obtained from distributional approaches vary according to many parameters. For example, one of the parameters that impacts the performance of distributional approaches is the way the context of a word is defined. Various approaches defined contexts differently: windows (Rapp, 1999), sentences or paragraphs (Fung and Mckeown, 1997), or by taking into consideration syntax dependencies based on POS tags (Gamallo, 2007). However, the most common way the context of a word is defined is by choosing words within windows centered around the word (Laroche and Langlais, 2010), usually of small sizes (e.g. a window of size 3 is used by Rapp (1999)). Domain-specific comparable corpora have been used for bilingual terminology extraction. These corpora are of modest sizes since large domainspecific corpora are not available for many domains (Morin et al., 2008). As a matter of fact, distributional approaches perform best with large"
I13-1046,2007.mtsummit-papers.26,0,0.79304,"Missing"
I13-1046,P95-1050,0,0.465207,"ra. For a source term and a list of translation candidates, we propose a method to identify and align the best source and target sentences that contain the term and its translation candidates. We report results with two language pairs (French-English and FrenchGerman) using domain-specific comparable corpora. Our method significantly improves the top 1, top 5 and top 10 precisions of a domain-specific bilingual lexicon, and thus, provides a better useroriented results. 1 Introduction Comparable corpora have been the subject of interest for extracting bilingual lexicons by several researchers (Rapp, 1995; Fung and Mckeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Morin et al., 2008; Bouamor et al., 2013, among others). Rapp (1995) was the first to suggest that if a word A co-occurs frequently with another word B in one language, then the translation of A and the translation of B should co-occur frequently in another language. Approaches emerging from (Rapp, 1995) make different assumptions to extract bilingual lexicon from comparable corpora. However, they are all based on the assumption that a translation pair shares some similar context in comparable corpora. We refer to such approaches th"
I13-1046,P99-1067,0,0.632628,"ired from Comparable Corpora Rima Harastani and B´eatrice Daille and Emmanuel Morin LINA UMR CNRS 6241 - University of Nantes 2 rue de la Houssini`ere, BP 92208 44322 Nantes, France {rima.harastani,beatrice.daille,emmanuel.morin}@univ-nantes.fr Abstract words to extract a bilingual lexicon by distributional approaches. Results obtained from distributional approaches vary according to many parameters. For example, one of the parameters that impacts the performance of distributional approaches is the way the context of a word is defined. Various approaches defined contexts differently: windows (Rapp, 1999), sentences or paragraphs (Fung and Mckeown, 1997), or by taking into consideration syntax dependencies based on POS tags (Gamallo, 2007). However, the most common way the context of a word is defined is by choosing words within windows centered around the word (Laroche and Langlais, 2010), usually of small sizes (e.g. a window of size 3 is used by Rapp (1999)). Domain-specific comparable corpora have been used for bilingual terminology extraction. These corpora are of modest sizes since large domainspecific corpora are not available for many domains (Morin et al., 2008). As a matter of fact,"
I13-1046,2012.eamt-1.37,0,0.0675383,"gical). Our assumption is that the best context (represented by sentences) can be extracted for a term as well as for its translation candidates, and that these extracted sentences can be aligned in order to re-rank the translation candidates of the term. After obtaining some candidate translations for Figure 1: Method to score a translation pair (source term and target term) 402 Parallel sentence (or fragment) extraction from comparable corpora has received the attention of a number of researchers (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Munteanu and Marcu, 2006; Smith et al., 2010; Hunsicker et al., 2012, among others), to enrich parallel text used by statistical machine translation (SMT) systems. They conducted experiments with large corpora (mainly news stories) which were noisy parallel, comparable (contain topic alignments or articles published in similar circumstances), or very non-parallel (Fung and Cheung, 2004). Usually, these approaches perform document-level alignments before extracting parallel sentences. The domain-specific corpora we use contain few documents (ranging from 38 to 262 documents for each corpus) and no parallel sentences. Furthermore, they are of modest size (about"
I13-1046,I11-2003,1,0.848816,"Stt ) = 1 − (6) |A| 3. Longest contiguous span: it is defined by (Munteanu and Marcu, 2005) as being the longest “pair of substrings in which the words in one substring are connected only to words in the other substring”. We assume that the length of a span must be greater than 2. The longest span is divided by the length of the smaller sentence, then: f3 (Sts , Stt ) = span(Sts , Stt ) min(|Sts |, |Stt |) 5 We first need to extract translations for a list of domain-specific terms in comparable corpora. In order to do this, we pre-process corpora and align terms with the free tool TermSuite5 (Rocheteau and Daille, 2011). The distributional method that is implemented in TermSuite is the one described in (Rapp, 1999). TermSuite provides a chosen number of translations for a term. Translations are ranked according to the scores provided by the distributional method. We try to enhance the top candidate translations of each reference source term by applying our re-ranking method. (7) 4. Number of connected bi-grams: this feature function is defined as the number of found connected bi-grams divided by the number of connected words in A, then: f4 (Sts , Stt ) = bi-grams(Sts , Stt ) |A| Evaluation (8) 5.1 The optima"
I13-1046,W02-0902,0,0.0327377,"candidates, we propose a method to identify and align the best source and target sentences that contain the term and its translation candidates. We report results with two language pairs (French-English and FrenchGerman) using domain-specific comparable corpora. Our method significantly improves the top 1, top 5 and top 10 precisions of a domain-specific bilingual lexicon, and thus, provides a better useroriented results. 1 Introduction Comparable corpora have been the subject of interest for extracting bilingual lexicons by several researchers (Rapp, 1995; Fung and Mckeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Morin et al., 2008; Bouamor et al., 2013, among others). Rapp (1995) was the first to suggest that if a word A co-occurs frequently with another word B in one language, then the translation of A and the translation of B should co-occur frequently in another language. Approaches emerging from (Rapp, 1995) make different assumptions to extract bilingual lexicon from comparable corpora. However, they are all based on the assumption that a translation pair shares some similar context in comparable corpora. We refer to such approaches that depend on co-occurrences of 401 International Joint Confe"
I13-1046,N10-1063,0,0.0132274,"hemotherapy, histological). Our assumption is that the best context (represented by sentences) can be extracted for a term as well as for its translation candidates, and that these extracted sentences can be aligned in order to re-rank the translation candidates of the term. After obtaining some candidate translations for Figure 1: Method to score a translation pair (source term and target term) 402 Parallel sentence (or fragment) extraction from comparable corpora has received the attention of a number of researchers (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Munteanu and Marcu, 2006; Smith et al., 2010; Hunsicker et al., 2012, among others), to enrich parallel text used by statistical machine translation (SMT) systems. They conducted experiments with large corpora (mainly news stories) which were noisy parallel, comparable (contain topic alignments or articles published in similar circumstances), or very non-parallel (Fung and Cheung, 2004). Usually, these approaches perform document-level alignments before extracting parallel sentences. The domain-specific corpora we use contain few documents (ranging from 38 to 262 documents for each corpus) and no parallel sentences. Furthermore, they ar"
I13-1046,C10-1070,0,0.0434659,"Missing"
I13-1046,P11-2083,0,\N,Missing
I13-1062,Y09-1013,0,0.0508812,"ces. In our experiments, missing keyphrases have not been removed. However, we evaluate with stemmed forms of candidates and reference keyphrases to reduce mismatches. 4.2 Preprocessing For each dataset, we apply the following preprocessing steps: sentence segmentation, word tokenization and Part-of-Speech tagging. For word tokenization, we use the TreebankWordTokenizer provided by the python Natural Language ToolKit (Bird et al., 2009) for English and the Bonsai word tokenizer6 for French. For Partof-Speech tagging, we use the Stanford POStagger (Toutanova et al., 2003) for English and MElt (Denis and Sagot, 2009) for French. 4.3 Baselines For comparison purpose, we use three baselines. The first baseline is TF-IDF (Sp¨arck Jones, 1972), commonly used because of the difficulty to achieve competitive results against it (Hasan and Ng, 2010). This method relies on a collection of documents and assumes that the k keyphrase candidates containing words with the highest TF-IDF weights are the keyphrases of the document. As TopicRank aims to be an improvement of the state-of-the-art graph-based methods for keyphrase extraction, the last two baselines are TextRank (Mihalcea and Tarau, 2004) and SingleRank (Wan"
I13-1062,C10-2042,0,0.224042,"e set of keyphrases that covers the main topics of a document. To do so, we simply select a keyphrase candidate from each of the top-ranked clusters. Clustering keyphrase candidates into topics also eliminates redundancy while reinforcing edges. This is very important because the ranking performance strongly depends on the conciseness of the graph, as well as its ability to precisely represent semantic relations within a document. Hence, another advantage of our approach is the use of a complete graph that better captures the semantic relations between topics. To evaluate TopicRank, we follow Hasan and Ng (2010) who stated that multiple datasets must be used to evaluate and fully understand the strengths and weaknesses of a method. We use four evaluation datasets of different languages, document sizes and domains, and compare the keyphrases extracted by TopicRank against three baselines (TF-IDF and two graph-based methods). TopicRank outperforms the baselines on three of the datasets. As for the fourth one, an additional experiment shows that an improvement could be achieved with a more effective selection strategy. Keyphrase extraction is the task of identifying single or multi-word expressions that"
I13-1062,W03-1028,0,0.901587,"ent Candidate Extraction Candidate Clustering Graph-Based Ranking Keyphrase Selection Keyphrases 3.2 Graph-Based Ranking Figure 1: Processing steps of TopicRank. 3.1 TopicRank represents a document by a complete graph in which topics are vertices and edges are weighted according to the strength of the semantic relations between vertices. Then, TextRank’s graph-based ranking model is used to assign a significance score to each topic. Topic Identification Keyphrases describe the most important topics of a document, thus the first step is to identify the keyphrase candidates that represent them. Hulth (2003) stated that most keyphrases assigned by human readers are noun phrases. Hence, the most important topics of a document can be found by extracting their most significant noun phrases. We follow Wan and Xiao (2008) and extract the longest sequences of nouns and adjectives from the document as keyphrase candidates. Other methods use syntactically filtered n-grams that are most likely to contain a larger number of candidates matching with reference keyphrases, but the n-gram restricted length is a problem. Indeed, ngrams do not always capture as much information as the longest noun phrases. Also,"
I13-1062,S10-1004,0,0.154747,", document sizes and domains. The first dataset, formerly used by Hulth (2003), contains 2000 English abstracts of journal papers from the Inspec database. The 2000 abstracts are divided into three sets: a training set, which contains 1000 abstracts, a validation set containing 500 abstracts and a test set containing the 500 remaining abstracts. In our experiments we use the 500 abstracts from the test set. Several reference keyphrase sets are available with this dataset. Just as Hulth (2003), we use the uncontrolled reference, created by professional indexers. The second dataset was built by Kim et al. (2010) for the keyphrase extraction task of the SemEval 2010 evaluation campaign. This dataset is (3) tk ∈Vj where Vi are the topics voting for ti and λ is a damping factor generally defined to 0.85 (Brin and Page, 1998). 3.3 Keyphrase Selection Keyphrase selection is the last step of TopicRank. For each topic, only the most representative keyphrase candidate is selected. This selection avoids redundancy and leads to a good cover4 The similarity between two candidates is computed with the stem overlap measure used by the clustering algorithm. 546 Documents Corpus Inspec SemEval WikiNews DEFT Keyphra"
I13-1062,W12-1101,0,0.0209973,"Missing"
I13-1062,W03-1805,0,0.0281762,"ument summarization (Litvak and Last, 2008) or document clustering (Han et al., 2007). Although scientific articles usually provide them, most of the documents have no associated keyphrases. Therefore, the problem of automatically assigning keyphrases to documents is an active field of research. Automatic keyphrase extraction methods are divided into two categories: supervised and unsupervised methods. Supervised methods recast keyphrase extraction as a binary classification task (Witten et al., 1999), whereas unsupervised methods apply different kinds of techniques such as language modeling (Tomokiyo and Hurst, 2003), clustering (Liu et al., 2009) or graph-based ranking (Mihalcea and Tarau, 2004). 543 International Joint Conference on Natural Language Processing, pages 543–551, Nagoya, Japan, 14-18 October 2013. candidates such as “unique nash equilibrium” or “exact nash equilibrium” which are longer, then have a better score, are extracted too. With TopicRank, we aim to circumvent this by ranking clusters of single and multi-word expressions instead of words. Wan and Xiao (2008) use a small number of nearest neighbor documents to compute more accurate word co-occurrences and reinforce edge weights in the"
I13-1062,Y09-2035,0,0.0151643,"ages 543–551, Nagoya, Japan, 14-18 October 2013. candidates such as “unique nash equilibrium” or “exact nash equilibrium” which are longer, then have a better score, are extracted too. With TopicRank, we aim to circumvent this by ranking clusters of single and multi-word expressions instead of words. Wan and Xiao (2008) use a small number of nearest neighbor documents to compute more accurate word co-occurrences and reinforce edge weights in the word graph. Borrowing cooccurrence information from multiple documents, their approach improves the word ranking performance. Instead of using words, Liang et al. (2009) use keyphrase candidates as vertices. Applied to Chinese, their method uses query log knowledge to determine phrase boundaries. Tsatsaronis et al. (2010) propose to connect vertices employing semantic relations computed using WordNet (Miller, 1995) or Wikipedia. They also experiment with different random walk algorithms, such as HITS (Kleinberg, 1999) or modified PageRank. Liu et al. (2010) consider the topics of words using a Latent Dirichlet Allocation model (Blei et al., 2003, LDA). As done by Haveliwala (2003) for Information Retrieval, they propose to decompose PageRank into multiple Pag"
I13-1062,W08-1404,0,0.0243544,"te graph. A graph-based ranking model is applied to assign a significance score to each topic. Keyphrases are then generated by selecting a candidate from each of the topranked topics. We conducted experiments on four evaluation datasets of different languages and domains. Results show that TopicRank significantly outperforms state-of-the-art methods on three datasets. 1 Introduction Keyphrases are single or multi-word expressions that represent the main topics of a document. Keyphrases are useful in many tasks such as information retrieval (Medelyan and Witten, 2008), document summarization (Litvak and Last, 2008) or document clustering (Han et al., 2007). Although scientific articles usually provide them, most of the documents have no associated keyphrases. Therefore, the problem of automatically assigning keyphrases to documents is an active field of research. Automatic keyphrase extraction methods are divided into two categories: supervised and unsupervised methods. Supervised methods recast keyphrase extraction as a binary classification task (Witten et al., 1999), whereas unsupervised methods apply different kinds of techniques such as language modeling (Tomokiyo and Hurst, 2003), clustering (Liu"
I13-1062,D09-1027,0,0.73248,"008) or document clustering (Han et al., 2007). Although scientific articles usually provide them, most of the documents have no associated keyphrases. Therefore, the problem of automatically assigning keyphrases to documents is an active field of research. Automatic keyphrase extraction methods are divided into two categories: supervised and unsupervised methods. Supervised methods recast keyphrase extraction as a binary classification task (Witten et al., 1999), whereas unsupervised methods apply different kinds of techniques such as language modeling (Tomokiyo and Hurst, 2003), clustering (Liu et al., 2009) or graph-based ranking (Mihalcea and Tarau, 2004). 543 International Joint Conference on Natural Language Processing, pages 543–551, Nagoya, Japan, 14-18 October 2013. candidates such as “unique nash equilibrium” or “exact nash equilibrium” which are longer, then have a better score, are extracted too. With TopicRank, we aim to circumvent this by ranking clusters of single and multi-word expressions instead of words. Wan and Xiao (2008) use a small number of nearest neighbor documents to compute more accurate word co-occurrences and reinforce edge weights in the word graph. Borrowing cooccurr"
I13-1062,D10-1036,0,0.136069,"word co-occurrences and reinforce edge weights in the word graph. Borrowing cooccurrence information from multiple documents, their approach improves the word ranking performance. Instead of using words, Liang et al. (2009) use keyphrase candidates as vertices. Applied to Chinese, their method uses query log knowledge to determine phrase boundaries. Tsatsaronis et al. (2010) propose to connect vertices employing semantic relations computed using WordNet (Miller, 1995) or Wikipedia. They also experiment with different random walk algorithms, such as HITS (Kleinberg, 1999) or modified PageRank. Liu et al. (2010) consider the topics of words using a Latent Dirichlet Allocation model (Blei et al., 2003, LDA). As done by Haveliwala (2003) for Information Retrieval, they propose to decompose PageRank into multiple PageRanks specific to various topics. A topic-biased PageRank is computed for each topic and corresponding word scores are combined. As this method uses a LDA model, it requires training data. With TopicRank, we also consider topics, but our aim is to use a single document, the document to be analyzed. The rest of this paper is organized as follows. Section 2 presents the existing methods for t"
I13-1062,C10-1121,0,0.113374,"Missing"
I13-1062,W04-3252,0,\N,Missing
I13-1062,N03-1033,0,\N,Missing
L16-1190,W11-0705,0,0.0525275,"(sets of synonyms) which are linked by different semantic relations such as hypernymy, or meronymy. SentiWordNet is an extension of WordNet that assigns to each synsets three sentiment scores : positivity, negativity and objectivity. Those resources have been used extensively to build opinion analysis system. They can be used as it, or to extend or build sentiment lexicons. For example, (Toh and Wang, 2014) uses the syntactic categories from WordNet as features for a CRF to extract aspects and WordNet relations such as antonymy and synonymy to extend an existing opinion lexicons. Similarly, (Agarwal et al., 2011) look for synonyms in WordNet to find the polarity of words absent from an opinion lexicon. Going further, (Badaro et al., 2014) build an Arabic version of SentiWordNet (an extension of WordNet that assigns to each synsets three sentiment scores : positivity, negativity and objectivity) by combining an Arabic version of WordNet and the English version of WordNet and SentiWordNet. While the previous studies build on a manually constructed lexicon, many systems have been proposed to build resources automatically based on lexical similarity for opinion mining. For that purpose, rich word represen"
L16-1190,W14-3623,0,0.0313172,"of WordNet that assigns to each synsets three sentiment scores : positivity, negativity and objectivity. Those resources have been used extensively to build opinion analysis system. They can be used as it, or to extend or build sentiment lexicons. For example, (Toh and Wang, 2014) uses the syntactic categories from WordNet as features for a CRF to extract aspects and WordNet relations such as antonymy and synonymy to extend an existing opinion lexicons. Similarly, (Agarwal et al., 2011) look for synonyms in WordNet to find the polarity of words absent from an opinion lexicon. Going further, (Badaro et al., 2014) build an Arabic version of SentiWordNet (an extension of WordNet that assigns to each synsets three sentiment scores : positivity, negativity and objectivity) by combining an Arabic version of WordNet and the English version of WordNet and SentiWordNet. While the previous studies build on a manually constructed lexicon, many systems have been proposed to build resources automatically based on lexical similarity for opinion mining. For that purpose, rich word representations have been proposed to compute the lexical similarity. In (Castellucci et al., 2013), the authors combine three kernels i"
L16-1190,S13-2060,0,0.0438869,"Missing"
L16-1190,C14-1067,1,0.930625,"perplane for which the polarity of words in the vector space depends on its position relative to the hyperplane. They evaluate their model by predicting the polarity of movie reviews. The most common way to evaluate sentiment analysis systems, is by comparing the prediction of the systems against a gold corpus. For example, SEMEVAL (Nakov et al., 2013; Rosenthal et al., 2014) proposes a task in which participants are asked to predict the polarity (positive, negative, neutral) of tweets and SMS. Another task focuses 1196 Model Ferret 2013 base Ferret 2013 best rerank Ferret 2014 synt Spectral (Claveau et al., 2014) W2V dim=50 w=5 W2V dim=100 w=5 W2V dim=200 w=5 W2V dim=300 w=5 W2V dim=400 w=5 W2V dim=50 w=9 W2V dim=100 w=9 W2V dim=200 w=9 W2V dim=300 w=9 W2V dim=400 w=9 W2V Google news MAP 5,6 6,1 7,9 8,97 2,89 3,65 3,92 5,25 5,06 3,12 4,14 4,42 4,07 4,39 5,82 R-Prec 7,7 8,4 10,7 10,94 3,89 4,84 5,44 6,25 6,43 4,11 5,55 5,60 5,53 5,51 7,51 P@1 22,5 24,8 29,4 31,05 13,48 18,49 22,18 18,67 20,37 13,11 17,18 17,69 20,50 17,81 13,28 P@5 14,1 15,4 18,9 18,44 7,36 9,62 11,39 10,72 11,44 7,80 9,25 10,71 11,13 9,95 11,60 P@10 10,8 11,7 14,6 13,76 5,44 7,04 8,32 7,73 8,29 5,68 6,79 7,47 8,02 7,43 8,94 P@50 5,3 5"
L16-1190,D09-1061,0,0.0815288,"Missing"
L16-1190,N15-1184,0,0.0380321,", many systems have been proposed to build resources automatically based on lexical similarity for opinion mining. For that purpose, rich word representations have been proposed to compute the lexical similarity. In (Castellucci et al., 2013), the authors combine three kernels into a SVM model where each kernel tries to capture an aspect of the opinion. A Bag of Word Kernel is used to compute the similarity of ngrams between tweets; A lexical semantic kernel build a Word Space from a matrix of co-occurrence context scores; And a smoothed partial tree kernel handles the syntactic information. (Faruqui et al., 2015) use an ontology to improve the effectiveness of word vectors and evaluate their work on word similarity and sentiment analysis tasks. They refine word vectors by minimizing the distance of words within the vector space and between vectors projected in the ontology. In (Maas et al., 2011), the authors build a vector space using a continuous mixture distribution over words in an unsupervised fashion. Then, they use annotated data to find an hyperplane for which the polarity of words in the vector space depends on its position relative to the hyperplane. They evaluate their model by predicting t"
L16-1190,P98-2127,0,0.211303,"participants are asked to identify and summarize the opinions expressed towards all aspects of an entity. An aspect is a constituent of an entity targeted by an opinion, for example, an aspect of the entity laptop is its battery. Yet, these task-based evaluations do not allow for a direct evaluation of the lexical similarity to represent the sentiment properties of the words, as provided by word embeddings or spectral representations. 3. 3.1. Lexical and distributional semantic similarities From distributional semantics to word embeddings Since the pioneering work of (Grefenstette, 1994) and (Lin, 1998), building distributional thesauri has been widely studied. They all rely on the hypothesis that each word is semantically characterized by all the contexts in which it appears. Many techniques, implementing this hypothesis has been proposed, and recently, (Claveau et al., 2014) proposed to use Information Retrieval metrics to build distributional thesauri represented as a (weighted) graphs of neighbors. A word is thus represented by its links with other words. This distributional method, which gives state-ofthe-art results on lexical similarity tasks, is called Spectral representation hereaft"
L16-1190,P11-1015,0,0.0604378,"el where each kernel tries to capture an aspect of the opinion. A Bag of Word Kernel is used to compute the similarity of ngrams between tweets; A lexical semantic kernel build a Word Space from a matrix of co-occurrence context scores; And a smoothed partial tree kernel handles the syntactic information. (Faruqui et al., 2015) use an ontology to improve the effectiveness of word vectors and evaluate their work on word similarity and sentiment analysis tasks. They refine word vectors by minimizing the distance of words within the vector space and between vectors projected in the ontology. In (Maas et al., 2011), the authors build a vector space using a continuous mixture distribution over words in an unsupervised fashion. Then, they use annotated data to find an hyperplane for which the polarity of words in the vector space depends on its position relative to the hyperplane. They evaluate their model by predicting the polarity of movie reviews. The most common way to evaluate sentiment analysis systems, is by comparing the prediction of the systems against a gold corpus. For example, SEMEVAL (Nakov et al., 2013; Rosenthal et al., 2014) proposes a task in which participants are asked to predict the p"
L16-1190,S13-2053,0,0.0262794,"scores are independent: one synset may have a non-zero score for both positive and negative values. SentiWordNet also defines an ’objective’ score for each synset as: 1 − (positive_value + negative_value). Table 6 reports the results with the same experiment settings as for the ANEW lexicon. Here again, the very low coefficients tend to show that there are almost no correlation between the sentiment-based proximity and the semantic one, either computed (Spectral and Word2Vec) or manually assessed (SimLex999). 4.3. Building classes of similar words: NRC emotion lexicon The NRC emotion lexicon (Mohammad et al., 2013) is a large list of words associated with eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). These emotions and sentiments are encoded as binary properties (the word has or not the emotional property), that were manually obtained through Amazon’s Mechanical Turk. Given a word with its emotion and valence properties, we want to know if the lexical similarity helps finding words sharing the exact same emotion and valence properties. In order to evaluate that, we set up the following experiment: given a query word (ex"
L16-1190,S13-2052,0,0.0414896,"tance of words within the vector space and between vectors projected in the ontology. In (Maas et al., 2011), the authors build a vector space using a continuous mixture distribution over words in an unsupervised fashion. Then, they use annotated data to find an hyperplane for which the polarity of words in the vector space depends on its position relative to the hyperplane. They evaluate their model by predicting the polarity of movie reviews. The most common way to evaluate sentiment analysis systems, is by comparing the prediction of the systems against a gold corpus. For example, SEMEVAL (Nakov et al., 2013; Rosenthal et al., 2014) proposes a task in which participants are asked to predict the polarity (positive, negative, neutral) of tweets and SMS. Another task focuses 1196 Model Ferret 2013 base Ferret 2013 best rerank Ferret 2014 synt Spectral (Claveau et al., 2014) W2V dim=50 w=5 W2V dim=100 w=5 W2V dim=200 w=5 W2V dim=300 w=5 W2V dim=400 w=5 W2V dim=50 w=9 W2V dim=100 w=9 W2V dim=200 w=9 W2V dim=300 w=9 W2V dim=400 w=9 W2V Google news MAP 5,6 6,1 7,9 8,97 2,89 3,65 3,92 5,25 5,06 3,12 4,14 4,42 4,07 4,39 5,82 R-Prec 7,7 8,4 10,7 10,94 3,89 4,84 5,44 6,25 6,43 4,11 5,55 5,60 5,53 5,51 7,51"
L16-1190,S14-2004,0,0.0729124,"Missing"
L16-1190,S14-2009,0,0.0144449,"n the vector space and between vectors projected in the ontology. In (Maas et al., 2011), the authors build a vector space using a continuous mixture distribution over words in an unsupervised fashion. Then, they use annotated data to find an hyperplane for which the polarity of words in the vector space depends on its position relative to the hyperplane. They evaluate their model by predicting the polarity of movie reviews. The most common way to evaluate sentiment analysis systems, is by comparing the prediction of the systems against a gold corpus. For example, SEMEVAL (Nakov et al., 2013; Rosenthal et al., 2014) proposes a task in which participants are asked to predict the polarity (positive, negative, neutral) of tweets and SMS. Another task focuses 1196 Model Ferret 2013 base Ferret 2013 best rerank Ferret 2014 synt Spectral (Claveau et al., 2014) W2V dim=50 w=5 W2V dim=100 w=5 W2V dim=200 w=5 W2V dim=300 w=5 W2V dim=400 w=5 W2V dim=50 w=9 W2V dim=100 w=9 W2V dim=200 w=9 W2V dim=300 w=9 W2V dim=400 w=9 W2V Google news MAP 5,6 6,1 7,9 8,97 2,89 3,65 3,92 5,25 5,06 3,12 4,14 4,42 4,07 4,39 5,82 R-Prec 7,7 8,4 10,7 10,94 3,89 4,84 5,44 6,25 6,43 4,11 5,55 5,60 5,53 5,51 7,51 P@1 22,5 24,8 29,4 31,05"
L16-1190,S14-2038,0,0.0204386,"Some studies have proposed to use existing resources to build or extend opinion lexicons. Among them, WordNet is certainly one of the most known semantic lexicon for English. Words are regrouped into synsets (sets of synonyms) which are linked by different semantic relations such as hypernymy, or meronymy. SentiWordNet is an extension of WordNet that assigns to each synsets three sentiment scores : positivity, negativity and objectivity. Those resources have been used extensively to build opinion analysis system. They can be used as it, or to extend or build sentiment lexicons. For example, (Toh and Wang, 2014) uses the syntactic categories from WordNet as features for a CRF to extract aspects and WordNet relations such as antonymy and synonymy to extend an existing opinion lexicons. Similarly, (Agarwal et al., 2011) look for synonyms in WordNet to find the polarity of words absent from an opinion lexicon. Going further, (Badaro et al., 2014) build an Arabic version of SentiWordNet (an extension of WordNet that assigns to each synsets three sentiment scores : positivity, negativity and objectivity) by combining an Arabic version of WordNet and the English version of WordNet and SentiWordNet. While t"
L16-1304,P14-1119,0,0.0204875,"is appropriate for the evaluation of automatic keyphrase extraction methods. Keywords: TermITH-Eval, structured resource, automatic evaluation, keyphrase extraction. 1. Introduction and Motivation Keyphrases are textual units (words and phrases) that represent the most important topics of a document. Keyphrase extraction is the task of automatically detecting those topics in the content of a document. The common practice to evaluate the performance of keyphrase extraction systems is to compute the number of exact matches between extracted keyphrases and (human assigned) reference keyphrases (Hasan and Ng, 2014). However, this leads to overly pessimistic scores since variations in the extracted keyphrases that might be judged as correct cannot be taken into account (Zesch and Gurevych, 2009). Producing a more reliable estimate of system performance is not an easy task as assessing whether a textual unit is a keyphrase is highly subjective (Kim et al., 2010). Yet, a handful of attempts have been made in this direction (Zesch and Gurevych, 2009; Kim et al., 2010) but with limited success. The initiating work of Zesch and Gurevych (2009) stated the need for partial matching instead of exact matching but"
L16-1304,C10-1065,0,0.123835,"etecting those topics in the content of a document. The common practice to evaluate the performance of keyphrase extraction systems is to compute the number of exact matches between extracted keyphrases and (human assigned) reference keyphrases (Hasan and Ng, 2014). However, this leads to overly pessimistic scores since variations in the extracted keyphrases that might be judged as correct cannot be taken into account (Zesch and Gurevych, 2009). Producing a more reliable estimate of system performance is not an easy task as assessing whether a textual unit is a keyphrase is highly subjective (Kim et al., 2010). Yet, a handful of attempts have been made in this direction (Zesch and Gurevych, 2009; Kim et al., 2010) but with limited success. The initiating work of Zesch and Gurevych (2009) stated the need for partial matching instead of exact matching but did not show the effectiveness of their measure compared with a human evaluation. Kim et al. (2010) improved the measure of Zesch and Gurevych (2009) and evaluated the correlation of both the original and improved measures with human evaluations. Computing the correlation between an automatic evaluation measure and human evaluators is an effective w"
L16-1304,R09-1086,0,0.171152,"ction and Motivation Keyphrases are textual units (words and phrases) that represent the most important topics of a document. Keyphrase extraction is the task of automatically detecting those topics in the content of a document. The common practice to evaluate the performance of keyphrase extraction systems is to compute the number of exact matches between extracted keyphrases and (human assigned) reference keyphrases (Hasan and Ng, 2014). However, this leads to overly pessimistic scores since variations in the extracted keyphrases that might be judged as correct cannot be taken into account (Zesch and Gurevych, 2009). Producing a more reliable estimate of system performance is not an easy task as assessing whether a textual unit is a keyphrase is highly subjective (Kim et al., 2010). Yet, a handful of attempts have been made in this direction (Zesch and Gurevych, 2009; Kim et al., 2010) but with limited success. The initiating work of Zesch and Gurevych (2009) stated the need for partial matching instead of exact matching but did not show the effectiveness of their measure compared with a human evaluation. Kim et al. (2010) improved the measure of Zesch and Gurevych (2009) and evaluated the correlation of"
L16-1304,I13-1062,1,0.873881,"enriched. The pre-indexing system relies on pattern matching between text and predefined expressions related to potential keyphrases. The predefined expressions requires constant updating in order to generate appropriate keyphrases. 2.2. Automatic Keyphrase Extraction We selected three keyphrase extraction methods to extract 30 keyphrases (10 each) per bibliographic record. The methods cover the main techniques used for automatic keyphrase extraction: the statistical method TF-IDF (Salton et al., 1975), the classification method KEA (Witten et al., 1999) and the graph-based method TopicRank (Bougouin et al., 2013). TF-IDF is a simple and common keyphrase extraction method that ranks the textual units of a document according to their TF-IDF score, frequently used in Information Retrieval. The idea is to give a high importance score to textual units which are both frequent in the document and specific to it. The specificity of a textual unit regarding a document is obtained using a collection of documents. The lower the number of documents in which a textual unit occurs, the more specific this textual unit is. KEA also relies on simple statistics. According to KEA, a keyphrase can be recognized by its im"
L16-1304,Y09-1013,0,0.0155089,"h topic is its textual unit that appears first within the document. For comparison purposes, we implemented each method and integrated them on top of the same preprocessing tools. Every document is first segmented into sentences, sentences are tokenized into words and words are labelled according their morphological class (Part-of-Speech tagging — POS tagging). We performed sentence segmentation with the PunktSentenceTokenizer provided by the Python Natural Language ToolKit (NLTK)(Bird et al., 2009), word tokenization using the French tokenizer Bonsai included with the French POS tagger MElt (Denis and Sagot, 2009), which we use for POS tagging. 2.3. Manual Evaluation Guidelines Four evaluators took part in the manual evaluation. Being chosen for their indexing experience and their expertise in the selected scientific disciplines, evaluators have been asked to follow the guidelines described below. After reading the title and the abstract of a bibliographic record, evaluators needed to assess if the automatically extracted keyphrases were relevant to the bibliographic record. This assessment is made regarding two aspects: appropriateness and silence. 2.3.1. Appropriateness Appropriateness is a property"
L16-1496,D10-1115,0,0.0411602,"e the composed vector c is a weighted sum of the two input vectors: c = αu + βv (α and β being two scalars); the full additive model (fulladd) where the two vectors u and v are first multiplied by weight matrices and then added as follows: c = Au + Bv; the dilation model where one of the input vectors (u or v) is first decomposed into a vector parallel to the other and an orthogonal vector. Before recombining, the parallel vector is dilated by a factor λ giving the following result: c = (λ − 1)hu, vi + hu, viv. In addition, Lazaridou et al. (2013) applyed the lexical function model (lexfunc) (Baroni and Zamparelli, 2010) where the distributional representation of one element in a composition is not a vector but a function. They also used the DSM at the stem level as a baseline. Our approach is inspired by the work of Lazaridou et al. (2013) and uses the additive model (wadd) to combine several distributional modellings at the morpheme level. 3. origin, such as hydro + logy = hydrology. Neoclassical elements are not considered as lexical units because they never independently occur in the texts, that is they are always seen in the combined form with other elements (e.g., biology) (Amiot and Dal, 2008; Namer, 2"
L16-1496,C02-2020,0,0.351904,"sh compounds. We show promising results using distributional analysis at the root and affix levels. We also show that the adapted approach significantly improve bilingual lexicon extraction from comparable corpora compared to the approach at the word level. Keywords: Bilingual lexicon extraction, comparable corpora, morphemes, compound term, distributional analysis 1. Introduction Nowadays comparable corpora are widely used in many applications of natural language processing, particularly in bilingual terminology extraction where parallel corpora are a scarce resource (Rapp, 1995; Fung, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). In the task of bilingual lexicon extraction from comparable corpora, the acquisition of translational pairs is mainly based on the Harris’ distributional hypothesis (Harris, 1954) which states that words with similar meaning tend to occur in similar contexts (hypothesis that has been extended to the bilingual scenario). It is well-known that the efficiency of distributional methods heavily depends on the quality and the size of comparable corpora (Morin et al., 2007). If the quality of bilingual lexicons can always be improved by using more data, this is true onl"
L16-1496,I08-1013,1,0.803067,"ays be improved by using more data, this is true only if the training data is reasonably well-matched to the desired output (Morin and Hazem, 2014). In the case of specialised comparable corpora, the amount of data is limited and often small (Rapp, 1995; Morin et al., 2007). This presents a problem for distributional methods that fail to extract infrequent single-word terms (SWTs) as well as multiword terms (MWTs). In the case of MWTs, it has become a standard practice to apply the principle of compositionality on its parts to extract their corresponding translations (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). While in the case of SWTs, distributional methods often treat them as single tokens without considering their compositional property (Rapp, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). One can note that a reasonable amount of SWTs are compound terms consisting of a combination of two (or more) lexical elements to form a unit of meaning (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). To handle derivational morphology, Guevara (2010) and Lazaridou et al. (2013) identify the stem of rare derived words and use its derivational"
L16-1496,C12-1046,1,0.906043,"more data, this is true only if the training data is reasonably well-matched to the desired output (Morin and Hazem, 2014). In the case of specialised comparable corpora, the amount of data is limited and often small (Rapp, 1995; Morin et al., 2007). This presents a problem for distributional methods that fail to extract infrequent single-word terms (SWTs) as well as multiword terms (MWTs). In the case of MWTs, it has become a standard practice to apply the principle of compositionality on its parts to extract their corresponding translations (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). While in the case of SWTs, distributional methods often treat them as single tokens without considering their compositional property (Rapp, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). One can note that a reasonable amount of SWTs are compound terms consisting of a combination of two (or more) lexical elements to form a unit of meaning (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). To handle derivational morphology, Guevara (2010) and Lazaridou et al. (2013) identify the stem of rare derived words and use its derivational vector to derive the d"
L16-1496,2012.amta-papers.5,1,0.927083,"more data, this is true only if the training data is reasonably well-matched to the desired output (Morin and Hazem, 2014). In the case of specialised comparable corpora, the amount of data is limited and often small (Rapp, 1995; Morin et al., 2007). This presents a problem for distributional methods that fail to extract infrequent single-word terms (SWTs) as well as multiword terms (MWTs). In the case of MWTs, it has become a standard practice to apply the principle of compositionality on its parts to extract their corresponding translations (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). While in the case of SWTs, distributional methods often treat them as single tokens without considering their compositional property (Rapp, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). One can note that a reasonable amount of SWTs are compound terms consisting of a combination of two (or more) lexical elements to form a unit of meaning (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). To handle derivational morphology, Guevara (2010) and Lazaridou et al. (2013) identify the stem of rare derived words and use its derivational vector to derive the d"
L16-1496,J93-1003,0,0.208853,"addition to the lexeme and to the single term, we can add the context words of the single term that have been already observed in the corpus. Hence, all the words that appear in the context of abnormal will be added to the context vector of the prefix ab-. This approach is noted V ect(lem) for distributional approach using lexemes and lemmas and the context words of the lemmas. 3. Each lemma or lexeme of the context vector is weighted according to a given association measure such as the point-wise mutual information (Fano, 1961), the discounted odds ratio (Evert, 2005) or the log-likelihood (Dunning, 1993); 4. Each source context vector is translated into the target language using a bilingual dictionary; 5. A similarity measure such as the Cosine (Salton and Lesk, 1968) or the weighted Jaccard (Grefenstette, 1994) is applied between each translated source context vector and all the target vectors; 6. The translation candidates are ranked according to their similarity scores. The correct translation of the English prefix ab- is the French prefix a-. Knowing that (automatically thanks to our approach), we can derive from the single word abnormal that its French translation is anormal. This can be"
L16-1496,P13-1055,0,0.0581787,"Missing"
L16-1496,W95-0114,0,0.0706073,"ch and English compounds. We show promising results using distributional analysis at the root and affix levels. We also show that the adapted approach significantly improve bilingual lexicon extraction from comparable corpora compared to the approach at the word level. Keywords: Bilingual lexicon extraction, comparable corpora, morphemes, compound term, distributional analysis 1. Introduction Nowadays comparable corpora are widely used in many applications of natural language processing, particularly in bilingual terminology extraction where parallel corpora are a scarce resource (Rapp, 1995; Fung, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). In the task of bilingual lexicon extraction from comparable corpora, the acquisition of translational pairs is mainly based on the Harris’ distributional hypothesis (Harris, 1954) which states that words with similar meaning tend to occur in similar contexts (hypothesis that has been extended to the bilingual scenario). It is well-known that the efficiency of distributional methods heavily depends on the quality and the size of comparable corpora (Morin et al., 2007). If the quality of bilingual lexicons can always be improved by usin"
L16-1496,2007.mtsummit-papers.26,0,0.0784106,"Missing"
L16-1496,W10-2805,0,0.10104,"ty on its parts to extract their corresponding translations (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). While in the case of SWTs, distributional methods often treat them as single tokens without considering their compositional property (Rapp, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). One can note that a reasonable amount of SWTs are compound terms consisting of a combination of two (or more) lexical elements to form a unit of meaning (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). To handle derivational morphology, Guevara (2010) and Lazaridou et al. (2013) identify the stem of rare derived words and use its derivational vector to derive the distributional meaning of morphologically complex words from their parts. Delpech et al. (2012a) extract translations of morphologically constructed terms by exploiting a manually constructed translation list of equivalence at the morpheme-level. In this paper, we apply the compositional property at the morpheme level to automatically build a bilingual list of morphemes (roots and affixes), resource that is not always available and difficult to construct manually. We evaluate our"
L16-1496,E03-1076,0,0.0852286,"pounds (including hyphen-separated). The major kinds of compounds are native and neoclassical compounds. The first kind includes only native elements, which means not borrowed from another language, suh as parrot + fish = parrotfish. The second kind, neoclassical compounding, combines some elements of Greek or Latin etymological 3111 1. Each single term of the source and the target language is split into roots or affixes and lexemes. However, many splitting tools are available, either designed for one language such as DeriF (Namer 2003) for the French language, or language independent such as Koehn and Knight (2003) algorithm or COMPOST (Loginova Clouet and Daille, 2014). The single term abnormal for instance is split into the prefix ab- and the lexeme normal; 2. Each lexeme is added to the context vector of its corresponding affix or root according to the co-occurrence of the lexeme with the affix or the root. The lexeme normal for instance will be added to the context vector of the prefix ab- with the co-occurrence value of abwith normal. This corresponds to the occurrence of abnormal in the source corpus. This approach is noted lexem for distributional approach using lexemes. At this step four variant"
L16-1496,C10-1070,0,0.274466,"ng results using distributional analysis at the root and affix levels. We also show that the adapted approach significantly improve bilingual lexicon extraction from comparable corpora compared to the approach at the word level. Keywords: Bilingual lexicon extraction, comparable corpora, morphemes, compound term, distributional analysis 1. Introduction Nowadays comparable corpora are widely used in many applications of natural language processing, particularly in bilingual terminology extraction where parallel corpora are a scarce resource (Rapp, 1995; Fung, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). In the task of bilingual lexicon extraction from comparable corpora, the acquisition of translational pairs is mainly based on the Harris’ distributional hypothesis (Harris, 1954) which states that words with similar meaning tend to occur in similar contexts (hypothesis that has been extended to the bilingual scenario). It is well-known that the efficiency of distributional methods heavily depends on the quality and the size of comparable corpora (Morin et al., 2007). If the quality of bilingual lexicons can always be improved by using more data, this is true only if the training data is rea"
L16-1496,P13-1149,0,0.337426,"extract their corresponding translations (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). While in the case of SWTs, distributional methods often treat them as single tokens without considering their compositional property (Rapp, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). One can note that a reasonable amount of SWTs are compound terms consisting of a combination of two (or more) lexical elements to form a unit of meaning (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). To handle derivational morphology, Guevara (2010) and Lazaridou et al. (2013) identify the stem of rare derived words and use its derivational vector to derive the distributional meaning of morphologically complex words from their parts. Delpech et al. (2012a) extract translations of morphologically constructed terms by exploiting a manually constructed translation list of equivalence at the morpheme-level. In this paper, we apply the compositional property at the morpheme level to automatically build a bilingual list of morphemes (roots and affixes), resource that is not always available and difficult to construct manually. We evaluate our automatic bilingual morpheme"
L16-1496,W14-5702,1,0.908419,"Missing"
L16-1496,P11-1140,0,0.0241671,"cal compounds. Thus, the morphemes under consideration for the distributional analysis are neoclassical elements and prefixes, including elements that are not purely neoclassical elements but look like them, such as the element radio in radiology. 4. Bilingual morpheme extraction Our aim is to extract for each source morpheme (root or affix) its corresponding translation in a target language. To do so, we adapted the well-known distributional method to the morpheme level as follows: Various forms of compounds Compounding has different forms. First of all, we can talk about “closed compounds” (Macherey et al., 2011) written as single words (e.g, toolbar) in contrast to “open compounds”, which are space-separated but form a unit of meaning (e.g., operating system). We only deal with closed compounds (including hyphen-separated). The major kinds of compounds are native and neoclassical compounds. The first kind includes only native elements, which means not borrowed from another language, suh as parrot + fish = parrotfish. The second kind, neoclassical compounding, combines some elements of Greek or Latin etymological 3111 1. Each single term of the source and the target language is split into roots or aff"
L16-1496,P14-1121,1,0.897411,"n from comparable corpora, the acquisition of translational pairs is mainly based on the Harris’ distributional hypothesis (Harris, 1954) which states that words with similar meaning tend to occur in similar contexts (hypothesis that has been extended to the bilingual scenario). It is well-known that the efficiency of distributional methods heavily depends on the quality and the size of comparable corpora (Morin et al., 2007). If the quality of bilingual lexicons can always be improved by using more data, this is true only if the training data is reasonably well-matched to the desired output (Morin and Hazem, 2014). In the case of specialised comparable corpora, the amount of data is limited and often small (Rapp, 1995; Morin et al., 2007). This presents a problem for distributional methods that fail to extract infrequent single-word terms (SWTs) as well as multiword terms (MWTs). In the case of MWTs, it has become a standard practice to apply the principle of compositionality on its parts to extract their corresponding translations (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). While in the case of SWTs, distributional methods often treat them as single tokens without conside"
L16-1496,P07-1084,1,0.784604,"minology extraction where parallel corpora are a scarce resource (Rapp, 1995; Fung, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). In the task of bilingual lexicon extraction from comparable corpora, the acquisition of translational pairs is mainly based on the Harris’ distributional hypothesis (Harris, 1954) which states that words with similar meaning tend to occur in similar contexts (hypothesis that has been extended to the bilingual scenario). It is well-known that the efficiency of distributional methods heavily depends on the quality and the size of comparable corpora (Morin et al., 2007). If the quality of bilingual lexicons can always be improved by using more data, this is true only if the training data is reasonably well-matched to the desired output (Morin and Hazem, 2014). In the case of specialised comparable corpora, the amount of data is limited and often small (Rapp, 1995; Morin et al., 2007). This presents a problem for distributional methods that fail to extract infrequent single-word terms (SWTs) as well as multiword terms (MWTs). In the case of MWTs, it has become a standard practice to apply the principle of compositionality on its parts to extract their corresp"
L16-1496,P95-1050,0,0.445959,"bset of French and English compounds. We show promising results using distributional analysis at the root and affix levels. We also show that the adapted approach significantly improve bilingual lexicon extraction from comparable corpora compared to the approach at the word level. Keywords: Bilingual lexicon extraction, comparable corpora, morphemes, compound term, distributional analysis 1. Introduction Nowadays comparable corpora are widely used in many applications of natural language processing, particularly in bilingual terminology extraction where parallel corpora are a scarce resource (Rapp, 1995; Fung, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). In the task of bilingual lexicon extraction from comparable corpora, the acquisition of translational pairs is mainly based on the Harris’ distributional hypothesis (Harris, 1954) which states that words with similar meaning tend to occur in similar contexts (hypothesis that has been extended to the bilingual scenario). It is well-known that the efficiency of distributional methods heavily depends on the quality and the size of comparable corpora (Morin et al., 2007). If the quality of bilingual lexicons can always be impr"
L16-1496,P99-1067,0,0.166048,"ix levels and hope that this work can serve as a cornerstone for future work on this task involving more languages. We also confirm that the adapted approach significantly improves bilingual terminology extraction from comparable corpora compared to the baseline system. 2. Related work Distributional semantic models (DSM) have been successfully used in many natural language processing tasks (Guevara, 2010). Bilingual terminology extraction from comparable corpora for instance, is usually based on the bilingual distributional semantic models (BDSMs) when dealing with single word terms (SWT’s) (Rapp, 1999; Gamallo, 3110 2007; Laroche and Langlais, 2010; Morin and Hazem, 2014). To extract a SWT’s translation, a similarity measure is applied between the translated context vector of the source SWT and the context vectors of all the target SWTs. The candidates are ranked according to their similarity scores. One of the main problems that encounter distributional methods such as BDSMs is data sparseness. Taking into account the derivational morphology property of a SWT should resolve the latter problem. Compositional methods which have originally been developed for phrases have been successfully ap"
L16-1496,E06-1029,0,\N,Missing
L16-1690,C14-1029,0,0.0373625,"Missing"
L16-1690,2015.jeptalnrecital-court.20,1,0.876464,"Missing"
L18-1045,I13-1150,0,0.253919,"MWTs that manage length variability. We evaluate our approach on two specialized domain corpora, a French/English corpus of the wind energy domain and a French/English corpus of the breast cancer domain and show superior results compared to baseline approaches. Keywords: Synonym extraction, Multi-word terms, Compositionality, Word embeddings 1. Introduction Synonyms acquisition has mainly concerned single word terms (SWTs) using a variety of approaches such as: lexicon-based approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Lin, 1998; Hagiwara, 2008), etc. However, exploring multi-word terms (MWTs) and their synonyms or semantically related terms can be useful in many applications such as: word sense disambiguation, machine translation, information retrieval, text simplification, etc. MWTs are motivated combinations that clearly convey the concept they designate. The requirement of term transparency argues in favor of compositional semantics for complex terms. Compositionality means that the whole meaning can be deduced from the meaning of its components and the syntactic rule by whic"
L18-1045,J93-1003,0,0.258676,"reafter the main steps of the distributional approach: • The context vector vwis of a given source word wis is first built. The vector vwis contains all the words that co-occur with wis within a window of n words that surround wis . Let us denote by occ(wis , wjs ) the co-occurrence count of wis and a given word of its context wjs . • The process of building context vectors is repeated for all words of the specialized corpus. • Words of the context vectors are weighted using association measures such as the point-wise mutual information (noted MI) (Fano, 1961), the log-likelihood (noted LLR) (Dunning, 1993) or the discounted odds-ratio (noted LO) (Laroche and Langlais, 2010). These measures aim at strengthening the correlation between a word and all the words of its context vector. Semi-Compositional Approach Like the compositional approach, the semi-compositional variant is based on the principle of compositionality of MWTs. The main difference lies on the nature of the substituted elements of the MWT. It is no longer constrained by the sole relation of synonymy like in Hamon and Nazarenko (2001). Hazem and Daille (2014) generalized the substitution on MWT elements to semantically related terms"
L18-1045,P08-3001,0,0.433597,"on two specialized domain corpora, a French/English corpus of the wind energy domain and a French/English corpus of the breast cancer domain and show superior results compared to baseline approaches. Keywords: Synonym extraction, Multi-word terms, Compositionality, Word embeddings 1. Introduction Synonyms acquisition has mainly concerned single word terms (SWTs) using a variety of approaches such as: lexicon-based approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Lin, 1998; Hagiwara, 2008), etc. However, exploring multi-word terms (MWTs) and their synonyms or semantically related terms can be useful in many applications such as: word sense disambiguation, machine translation, information retrieval, text simplification, etc. MWTs are motivated combinations that clearly convey the concept they designate. The requirement of term transparency argues in favor of compositional semantics for complex terms. Compositionality means that the whole meaning can be deduced from the meaning of its components and the syntactic rule by which they are combined (Partee et al., 1990). Pirrelli et"
L18-1045,daille-hazem-2014-semi,1,0.832183,"h such as: wind turbine/wind machine1 ; MWT synonyms of variable length such as: wind farm/wind power plant; to non compositional MWT synonyms such as: pole tower/mast. Few works addressed the acquisition of MWT synonyms. The main approaches that have been proposed in the experimental literature deal with the acquisition of synonyms of MWTs that are compositional and often of the same length. Synonym extraction approaches implement the principle of compositionality by substituting parts of the MWT by synonyms provided by a dictionary (Hamon and Nazarenko, 2001), or by distributional analysis (Hazem and Daille, 2014). It has been recently shown that words, phrases, sentences, paragraphs and more generally, pieces of texts of any length can be efficiently represented by word embeddings using operations on vectors and matrices like addition or multipli1 In the renewable energy domain. cation (Mitchell and Lapata, 2010; Mikolov et al., 2013b; Socher et al., 2011; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Arora et al., 2017; Hazem et al., 2017). For phrase representation, Mikolov et al. (2013b) have shown for instance that the embedding v"
L18-1045,hazem-etal-2017-mappsent,1,0.924887,"nonyms provided by a dictionary (Hamon and Nazarenko, 2001), or by distributional analysis (Hazem and Daille, 2014). It has been recently shown that words, phrases, sentences, paragraphs and more generally, pieces of texts of any length can be efficiently represented by word embeddings using operations on vectors and matrices like addition or multipli1 In the renewable energy domain. cation (Mitchell and Lapata, 2010; Mikolov et al., 2013b; Socher et al., 2011; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Arora et al., 2017; Hazem et al., 2017). For phrase representation, Mikolov et al. (2013b) have shown for instance that the embedding vector of the phrase Volga river is similar to the addition of the embedding vector of Volga and the embedding vector of river. The addition property that word embbeding models exhibit offers key information for representing phrases and by extension MWTs and there synonyms or quasi-synonyms. Drawing inspiration from these findings and based on the principle of compositionality and distributed approaches, we propose several techniques based on word embedding models to deal with synonyms acquisition of"
L18-1045,P14-1062,0,0.00832632,"oaches implement the principle of compositionality by substituting parts of the MWT by synonyms provided by a dictionary (Hamon and Nazarenko, 2001), or by distributional analysis (Hazem and Daille, 2014). It has been recently shown that words, phrases, sentences, paragraphs and more generally, pieces of texts of any length can be efficiently represented by word embeddings using operations on vectors and matrices like addition or multipli1 In the renewable energy domain. cation (Mitchell and Lapata, 2010; Mikolov et al., 2013b; Socher et al., 2011; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Arora et al., 2017; Hazem et al., 2017). For phrase representation, Mikolov et al. (2013b) have shown for instance that the embedding vector of the phrase Volga river is similar to the addition of the embedding vector of Volga and the embedding vector of river. The addition property that word embbeding models exhibit offers key information for representing phrases and by extension MWTs and there synonyms or quasi-synonyms. Drawing inspiration from these findings and based on the principle of compositionality and distributed approaches, we propose sev"
L18-1045,E14-1057,0,0.0603223,"Missing"
L18-1045,C10-1070,0,0.162668,"The context vector vwis of a given source word wis is first built. The vector vwis contains all the words that co-occur with wis within a window of n words that surround wis . Let us denote by occ(wis , wjs ) the co-occurrence count of wis and a given word of its context wjs . • The process of building context vectors is repeated for all words of the specialized corpus. • Words of the context vectors are weighted using association measures such as the point-wise mutual information (noted MI) (Fano, 1961), the log-likelihood (noted LLR) (Dunning, 1993) or the discounted odds-ratio (noted LO) (Laroche and Langlais, 2010). These measures aim at strengthening the correlation between a word and all the words of its context vector. Semi-Compositional Approach Like the compositional approach, the semi-compositional variant is based on the principle of compositionality of MWTs. The main difference lies on the nature of the substituted elements of the MWT. It is no longer constrained by the sole relation of synonymy like in Hamon and Nazarenko (2001). Hazem and Daille (2014) generalized the substitution on MWT elements to semantically related terms of any type. They extended the compositional rules R1 and R2 by repl"
L18-1045,P98-2127,0,0.248239,"ur approach on two specialized domain corpora, a French/English corpus of the wind energy domain and a French/English corpus of the breast cancer domain and show superior results compared to baseline approaches. Keywords: Synonym extraction, Multi-word terms, Compositionality, Word embeddings 1. Introduction Synonyms acquisition has mainly concerned single word terms (SWTs) using a variety of approaches such as: lexicon-based approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Lin, 1998; Hagiwara, 2008), etc. However, exploring multi-word terms (MWTs) and their synonyms or semantically related terms can be useful in many applications such as: word sense disambiguation, machine translation, information retrieval, text simplification, etc. MWTs are motivated combinations that clearly convey the concept they designate. The requirement of term transparency argues in favor of compositional semantics for complex terms. Compositionality means that the whole meaning can be deduced from the meaning of its components and the syntactic rule by which they are combined (Partee et al., 19"
L18-1045,P07-1084,1,0.692211,"amon and Nazarenko (2001). To extract French synonyms of single-word terms we used the on-line dictionary DES 7 . DES contains 49,168 entries and 201,511 synonym relations. The initial database has been constructed from seven dictionaries. The extraction of English synonyms has been conducted using the lexical database WordNet 8 . WordNet contains approximately 117,000 synsets. The main relation among words in WordNet is synonymy. 4.2. Distributional Method Settings Using the distributional method, three main parameters need to be set: the size of the window used to build the context vectors (Morin et al., 2007; Gamallo, 2008), the association measure (the log-likelihood (Dunning, 1993), the point-wise mutual information (Fano, 1961), the discounted odds-ratio (Laroche and Langlais, 2010),...) and the similarity measure (the weighted Jaccard index 7 http://www.crisco.unicaen.fr/des/ synonyms 8 http://wordnetweb.princeton.edu/perl/ webwn/ 300 Method Hamon&Nazarenko Mikolov Semi-Comp (MI-COS) Semi-Comp (LO-COS) Semi-Comp (LLR-JAC) Semi-Comp (SG50) Semi-Comp (SG100) Semi-Comp (SG300) Semi-Comp (CBOW50) Semi-Comp (CBOW100) Semi-Comp (CBOW300) Full-Comp (SG100) Full-Comp (SG200) Full-Comp (SG300) Full-Co"
L18-1045,P06-2111,0,0.295935,"Missing"
L18-1045,W03-1610,0,0.48785,"pproach for the automatic acquisition of synonyms of MWTs that manage length variability. We evaluate our approach on two specialized domain corpora, a French/English corpus of the wind energy domain and a French/English corpus of the breast cancer domain and show superior results compared to baseline approaches. Keywords: Synonym extraction, Multi-word terms, Compositionality, Word embeddings 1. Introduction Synonyms acquisition has mainly concerned single word terms (SWTs) using a variety of approaches such as: lexicon-based approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Lin, 1998; Hagiwara, 2008), etc. However, exploring multi-word terms (MWTs) and their synonyms or semantically related terms can be useful in many applications such as: word sense disambiguation, machine translation, information retrieval, text simplification, etc. MWTs are motivated combinations that clearly convey the concept they designate. The requirement of term transparency argues in favor of compositional semantics for complex terms. Compositionality means that the whole meaning can be deduced from the"
L18-1056,2005.jeptalnrecital-long.13,0,0.123254,"Missing"
L18-1056,E14-2005,0,0.0129989,"minal anaphora resolution, we apply the knowledge-poor approach of Mitkov (2002): it only requires part-of speech tagging and chunk identification as the linguistic preprocessing. The algorithm identifies the nominal chunks that precede an anaphorical pronoun, within a distance of two sentences, then checks the inflectional agreement with the anaphora and finally applies indicators to rank the nominal chunks. Each indicator gives either a positive or a negative score. The nominal chunk with the highest combined score is chosen as the antecedent of the anaphorical pronoun. We use RDRPOSTagger (Nguyen et al., 2014) to extract the nominal chunks. This tagger is designed for French and gives the gender and the number of a word: these informations are used for the inflectional agreement part of the anaphora resolution. Mitkov (2002) listed 10 indicators. We kept 6 indicators as such: definiteness, givenness, lexical reiteration, non prepositional noun phrases, collocation pattern preference, and referential distance. We adapted two other indicators, i.e. section heading preference 3 373 https://github.com/nicolashernandez/PyRATA 5.1. and domain terminology preference, to the type of our texts. To represent"
L18-1056,L16-1013,0,0.0246917,"l pleut des cordes” that translates into “it’s raining cats and dogs” (whereas “cordes” usually translates into “ropes”); at the pragmatical level as in “il lui parle” (“he talks to him/her”) where “il” is an anaphorical pronoun which referent has to be found between the preceding masculine nominal groups and “lui” is also an anaphorical pronoun which referent can be either a masculine, or a feminine nominal group. To our knowledge, in the field of natural language processing, the difficulties coming from dyslexia have been studied in the writing of children with dyslexia (Rello et al., 2016; Rauschenberger et al., 2016) but not in their reading. Evaluating the difficulty of a text is a field of natural language processing that has been recently studied (Fran¸cois and Watrin, 2011; Gala et al., 2014; Ho Dac et al., 2016; Tanguy et al., 2016; M¨ uller et al., 2016). Nonetheless, these works rather focused on predicting the lexical complexity and the graduation of words in a text whereas we are more interested in establishing a diagnosis of the difficulties appearing in a text but also to propose an explanation for the identified difficulties. Indeed, our goal is to allow the visualization of the difficulties o"
P07-1084,C02-1011,0,0.0343127,"Missing"
P07-1084,C02-2020,0,0.786859,"Missing"
P07-1084,C02-1166,0,0.638566,"Missing"
P07-1084,1999.tc-1.8,0,0.0479716,"exical units of the context vectors, which depends on the coverage of the bilingual dictionary vis-à-vis the corpus, is an important step of the direct approach: more elements of the context vector are translated more the context vector will be discrimating for selecting translations in the target language. If the bilingual dictionary provides several translations for a lexical unit, we consider all of them but weight the different translations by their frequency in the target language. If an MWT cannot be directly translated, we generate possible translations by using a compositional method (Grefenstette, 1999). For each element of the MWT found in the bilingual dictionary, we generate all the translated combinations identified by the term extraction program. For example, in the case of the MWT fatigue chronique (chronic fatigue), we have the fol, , lowing four translations for fatigue: , and the following two translations for chronique: , . Next, we generate all combinations of translated elements (See Table 1 7 ) and select those which refer to an existing MWT in the target language. Here, only one term has been identified by the Japanese terminology extraction program: . . In this approach, when"
P07-1084,P99-1067,0,0.865212,"ed and classified into two discourse categories: one contains only scientific documents and the other contains both scientific and popular science documents. We used a state-of-the-art multilingual terminology mining chain composed of two term extraction programs, one in each language, and an alignment program. The term extraction programs are publicly available and both extract multi-word terms that are more precise and specific to a particular scientific domain than single word terms. The alignment program makes use of the direct context-vector approach (Fung, 1998; Peters and Picchi, 1998; Rapp, 1999) slightly modified to handle both singleand multi-word terms. We evaluated the candidate translations of multi-word terms using a reference list compiled from publicly available resources. We found that taking discourse type into account resulted in candidate translations of a better quality even when the corpus size is reduced by half. Thus, even using a state-of-the-art alignment method wellknown as data greedy, we reached the conclusion that the quantity of data is not sufficient to obtain a terminological list of high quality and that a real comparability of corpora is required. 2 Multilin"
P07-1084,E06-1029,0,0.519904,"lowing four translations for fatigue: , and the following two translations for chronique: , . Next, we generate all combinations of translated elements (See Table 1 7 ) and select those which refer to an existing MWT in the target language. Here, only one term has been identified by the Japanese terminology extraction program: . . In this approach, when it is not possible to translate all parts of an MWT, or when the translated combinations are not identified by the term extraction program, the MWT is not taken into account in the translation process. This approach differs from that used by (Robitaille et al., 2006) for French/Japanese translation. They first decompose the French MWT into combinations of shorter multi-word units (MWU) elements. This approach makes the direct translation of a subpart of the MWT possible if it is present in the         7 the French word order is inverted to take into account the different constraints between French and Japanese. 667 fatigue          Table 1: Illustration of the compositional method. The underlined Japanese MWT actually exists.  bilingual dictionary. For an MWT of length , (Robitaille et al., 2006) produce all the combin"
P16-4003,W14-5702,1,0.88696,"Missing"
P16-4003,I11-2003,1,0.837389,"x terms in languages such as German and Russian are mostly compounds, while in Roman languages they are MWT. TermSuite extracts single terms and any kind of complex terms. For some generic domains and some applications, large amounts of data have to be processed. TermSuite is scalable and has been applied to corpora of 1.1 gigabytes using a personal computer configuration. Finally, TermSuite identifies a broad range of term variants, from spelling to syntactic variants that may be used to structure the extracted terminology with various conceptual relations. Since the first TermSuite release (Rocheteau and Daille, 2011), several enhancements about TET have been made. We developed UIMA Tokens Regex, a tool to define term and variant patterns using word annotations within the UIMA framework (Ferrucci and Lally, 2004) and a grouping tool to cluster terms and variants. Both tools are designed to treat in an uniform way all linguistic kinds of complex terms. After a brief reminder of TermSuite general architecture, we present its term spotting tool UIMA Tokens Regex, its variant grouping tool, and the variant specifications we design for English, French, Spanish, German, and Russian. FiWe introduce, TermSuite, a"
S10-1038,W09-2902,0,0.0547127,"Missing"
S10-1038,C08-1122,0,0.0562796,"Missing"
tsuji-etal-2002-extracting,P97-1017,0,\N,Missing
vernier-etal-2010-learning,banea-etal-2008-bootstrapping,0,\N,Missing
vernier-etal-2010-learning,ruppenhofer-etal-2008-finding,0,\N,Missing
vernier-etal-2010-learning,P02-1053,0,\N,Missing
vernier-etal-2010-learning,strapparava-valitutti-2004-wordnet,0,\N,Missing
vernier-etal-2010-learning,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
vernier-etal-2010-learning,stoyanov-cardie-2008-annotating,0,\N,Missing
W03-1802,J93-1003,0,0.0168936,"nerate one or several nouns for a given adjective. We generate a noun for each relational suffix class. A class of suffixes includes the allomorphic variants. This overgeneration method used in information retrieval by (Jacquemin, 2001) gives low noise because the base noun must not only be an attested form in the corpus, but must also appear as an extension of a head noun. At the end of the linguistic processing, the term extractor proposes as output: 1. a list of pilot terms ranked from the most representative of the corpus to the least thanks to the Loglikelihood coefficient introduced by (Dunning, 1993). 2. for each pilot term, a XML structure is provided which gathers all the base structures and the variations encountered.      where: S is the relational suffix to be deleted from the end of an adjective. The result of this deletion is the stem R; M is the mutative segment to be concatenated to R in order to form a noun. For example, the rule [ -´e +e ] says that if there is an adjective which ends with e´ , we should strip this ending from it and append the string e to the stem. The algorithm below resumes the successive steps for identifying relational adjectives: 1. Examine each"
W03-1802,P99-1050,0,0.0451482,"Missing"
W09-3110,E06-2001,0,0.0327464,"rpora compilation, because translated resources are rare and there is a lack of resources when the languages involved do not include English. Furthermore, the amount of multilingual documents available on the Web ensures the possibility of automatically compiling them. Nevertheless, this task can not be summarized to a simple collection of documents sharing vocabulary. It is necessary to respect the common characteristics of texts in corpora, established before the compilation, according to the corpus finality (McEnery and Xiao, 2007). Many works are about compilation of corpora from the Web (Baroni and Kilgarriff, 2006) but none, in our knowledge, focuses on compilation of comparable corpora, which has to satisfy many constraints. We fix three comparability levels: domain, topic and type of discourse. Our goal is to automate recognition of these comparability levels in documents, in order to include them into a corpus. We work on Web documents on specialized scientific domains in French and Japanese languages. As document topics can be filtered with keywords in the Web search (Chakrabarti et al., 1999), we focus in this paper on automatic recognition of types of discourse that can be found in scientific docu"
W09-3110,C02-2020,0,0.0552411,"Missing"
W09-3110,C00-1032,1,0.710143,"(Quinlan, 1993), since both of them seem to be the most appropriate to our data (small corpora, binary classification, less than 100 features). Lexical Features Some of our lexical criteria are specific to the scientific documents, like bibliographies and bibliographic quotations, specialized vocabulary or the measurement units. To measure the terminological density (proportion of specialized vocabulary in the text) in French, we evaluate terms with stems of Greek-Latin (Namer and Baud, 2007) and suffix characters of relational adjectives that are particularly frequent in scientific domains (Daille, 2000). We listed about 50 stems such as inter-, auto- or nano-, and the 10 relational suffixes such such as -ique or -al. For Japanese, we listed prefix characteristics of names of disease or symptoms (先天性 (congenital), 遺伝性(hereditary), etc.). These stems can be found in both type of discourse, but not in the same proportions. Specialized terms are used in both type of discourse in different ways. For example, the term “ovarectomie” (ovarectomy) can be frequent in a scientific document and used once in a popular science documents to explain it and then replaced by “ablation des ovaires” (ovary abla"
W09-3110,C02-1166,0,0.0672515,"s of works, which induces different choices: • General language works, where texts of corpora usually share a domain and a period. Fung and Yee (1998) used a corpus composed of newspaper in English and Chinese on a specific period to extract words translations, using IR and NLP methods. Rapp (1999) used a English / German corpus, composed of documents coming from newspapers as well as scientific papers to study alignment methods and bilingual lexicon extraction from non-parallel corpora (which can be considered as comparable); • Specialized language works, where choice of criteria is various. Déjean et al. (2002) used a corpus composed of scientific abstracts from 56 Feature URL pattern Document’s format Meta tags Title tag Pages layout Pages background Images Links Paragraphs Item lists Number of sentences Typography Document’s length sions (structural, modal and lexical), whose combination characterizes scientific or popular science discourse. A specialized comparable corpus can be compiled from a single type of discourse document collection through several steps. Last part of this paper focuses on the automation of these steps using the IBM Unstructured Information Management Architecture (UIMA). 3"
W09-3110,P99-1067,0,0.0335771,"). The choice of the common characteristics, which define the content of corpora, affects the degree of comparability, notion used to quantify how two corpora can be comparable. The choice of these characteristics depends on the finality of the corpus. Among papers on comparable corpora, we distinguish two types of works, which induces different choices: • General language works, where texts of corpora usually share a domain and a period. Fung and Yee (1998) used a corpus composed of newspaper in English and Chinese on a specific period to extract words translations, using IR and NLP methods. Rapp (1999) used a English / German corpus, composed of documents coming from newspapers as well as scientific papers to study alignment methods and bilingual lexicon extraction from non-parallel corpora (which can be considered as comparable); • Specialized language works, where choice of criteria is various. Déjean et al. (2002) used a corpus composed of scientific abstracts from 56 Feature URL pattern Document’s format Meta tags Title tag Pages layout Pages background Images Links Paragraphs Item lists Number of sentences Typography Document’s length sions (structural, modal and lexical), whose combin"
W09-3110,P98-1069,0,0.0233603,"20). Comparability is ensured using characteristics which can refer to the text creation context (period, author...), or to the text itself (topic, genre...). The choice of the common characteristics, which define the content of corpora, affects the degree of comparability, notion used to quantify how two corpora can be comparable. The choice of these characteristics depends on the finality of the corpus. Among papers on comparable corpora, we distinguish two types of works, which induces different choices: • General language works, where texts of corpora usually share a domain and a period. Fung and Yee (1998) used a corpus composed of newspaper in English and Chinese on a specific period to extract words translations, using IR and NLP methods. Rapp (1999) used a English / German corpus, composed of documents coming from newspapers as well as scientific papers to study alignment methods and bilingual lexicon extraction from non-parallel corpora (which can be considered as comparable); • Specialized language works, where choice of criteria is various. Déjean et al. (2002) used a corpus composed of scientific abstracts from 56 Feature URL pattern Document’s format Meta tags Title tag Pages layout Pag"
W09-3110,nazar-etal-2008-suite,0,0.0301015,"used in a durable way must include this step. Documentation of the corpus includes information about the compilation (creator, date, method, resources, etc.) and information about the corpus documents. Text Encoding Initiative (TEI) standard has been created in order to conserve in an uniformed way this kind of information in a corpus 6 . A corpus quality highly depends on the first two steps. Moreover, these steps are directly linked to the creator use of the corpus. The first step must be realized by the user to create an relevant corpus. Although second step can be computerizable (Rogelio Nazar and Cabré, 2008), we choose to keep it manual in order to guarantee corpus quality. We decided to work on a system which realizes the last steps, i.e. normalization, annotation and documentation, starting from a collection of documents selected by a user. Our tool has been developed on Unstructured Information Management Architecture (UIMA) that has been created by IBM Research Division (Ferrucci and Lally, 2004). Unstructured data (texts, images, etc.) collections can be easily treated on this platform and many libraries are available. Our tool starts with a web documents or texts collection and is composed"
W09-3110,C98-1066,0,\N,Missing
W14-5702,N09-1046,0,0.137801,"s with the help of a monolingual dictionary. Other methods resort to the corpora to validate the analyses. A pioneering work using corpus statistics for compound splitting was done by Koehn and Knight (2003). The algorithm generates all possible segmentations for a given word (taking into account some linking morphemes), and gives a probability for each segmentation, estimated from the geometric mean of the component frequencies in the corpus. The segmentation with the highest score is classed as the best. Probabilistic splitting methods using machine learning technologies have been proposed (Dyer, 2009; Hewlett and Cohen, 2011; Macherey et al., 2011). Actually they are less precise than the languagespecific methods, but their advantage is the usability for any language. Statistical methods also tend to integrate some linguistic knowledge (e.g. list of linking morphemes). 3 Compound Splitting Method Our concern was to design a corpus-driven system that could be applied to different languages, but also able to integrate linguistic knowledge. For a given word, the system makes the decision as to whether it is a compound, and for compounds it gives one or several candidate analyses ranked by th"
W14-5702,P11-2095,0,0.0308519,"elp of a monolingual dictionary. Other methods resort to the corpora to validate the analyses. A pioneering work using corpus statistics for compound splitting was done by Koehn and Knight (2003). The algorithm generates all possible segmentations for a given word (taking into account some linking morphemes), and gives a probability for each segmentation, estimated from the geometric mean of the component frequencies in the corpus. The segmentation with the highest score is classed as the best. Probabilistic splitting methods using machine learning technologies have been proposed (Dyer, 2009; Hewlett and Cohen, 2011; Macherey et al., 2011). Actually they are less precise than the languagespecific methods, but their advantage is the usability for any language. Statistical methods also tend to integrate some linguistic knowledge (e.g. list of linking morphemes). 3 Compound Splitting Method Our concern was to design a corpus-driven system that could be applied to different languages, but also able to integrate linguistic knowledge. For a given word, the system makes the decision as to whether it is a compound, and for compounds it gives one or several candidate analyses ranked by their scores (in this work"
W14-5702,E03-1076,0,0.116083,"gated in linguistic studies. But this language is rarely subject to experiments in automatic compound splitting. The first reason is that most of English compounds are formed by simple concatenation (airfoil = air + foil, streamtube = stream + tube), so their splitting is supposed to be straightforward. The second reason is that many compounds in highly compounding languages should be translated into English as multi-word expressions. That is why the works addressing automatic compound splitting in the context of machine translation often admit that English contains only few closed compounds (Koehn and Knight, 2003; Macherey et al., 2011). In these works, the use of English parallel texts helps to extract the multi-word equivalents of compounds from the texts in highly compounding languages. We assume that English compound terms are still worth splitting and analyzing. The first reason is that the assumption of independent occurring of English compound elements fails when we consider neoclassical compounds. The second ground is that distinguishing between compounds and non-compound out-of-dictionary words (named entities, derivative forms, etc.) can be problematic. This work is licenced under a Creative"
W14-5702,P11-1140,0,0.658047,"ting, which uses corpus frequencies, lexical data and optionally linguistic rules. This is a supervised method which requires a small amount of segmented compounds as input. We evaluate the method on two languages that rarely serve as a material for automatic splitting systems: English and Russian. The results obtained are competitive with those of a state-of-the-art corpus-driven approach. 1 Introduction Compounding is a method of word formation consisting of a combination of two (or more) lexical elements that form a unit of meaning. In this work we only handle so called ”closed compounds” (Macherey et al., 2011), i.e. those forming also a graphical unit. A great number of languages resort to this word formation. In some of them such as German, Dutch (Germanic family), Estonian or Finnish (Uralic family) compounding is very regular and well described. In other languages it is less productive (e.g. Slavic family), or even marginal (most of Romance languages). This phenomenon is particularly productive in specialized domains because of the necessity to denote the domain concepts in a very concise and precise way. In addition, specialized texts contain many neoclassical compounds (Namer, 2009), i.e. comp"
W14-5702,schmid-etal-2004-smor,0,0.0338196,"line of another application: machine translation (Koehn and Knight, 2003; Macherey et al., 2011; Stymne et al., 2013), information retrieval (Braschler and Ripplinger, 2004; Chen and Gey, 2001), etc. To deal with the non-independent compound elements in morphologically rich languages, different solutions have been proposed. It is possible to store separately the compound stems and the linking morphemes (the morphemes that are inserted at the component boundaries to form a compound), and to have a grammar to combine them. This solution is realized in the morphological analyser for German SMOR (Schmid et al., 2004). The construction of such a finite-state morphology for a new language is a costly task in terms of time and efforts. Another approach is to formalize the component modifications as a set of rules describing addition, but also deletion and substitution of some character sequences on the component boundaries within a compound. Thus, the compound splitter BananaSplit (Ott, 2005) uses a set of linguistic rules for German to restore independent forms from compound forms, and validates the restored forms with the help of a monolingual dictionary. Other methods resort to the corpora to validate the"
W14-5702,J13-4009,0,0.0646582,". The article has the following structure. Section 2 gives a review of some compound splitting methods proposed in the literature. Section 3 presents our splitting method. In Section 4 the experiments and data are described. We discuss the results and analyse the errors. We also compare our system to the state-of-the-art corpus-based method of Koehn and Knight (2003). We conclude with Section 5. 2 Related Works Compound splitting was addressed in many NLP works, as a standalone task or in the pipeline of another application: machine translation (Koehn and Knight, 2003; Macherey et al., 2011; Stymne et al., 2013), information retrieval (Braschler and Ripplinger, 2004; Chen and Gey, 2001), etc. To deal with the non-independent compound elements in morphologically rich languages, different solutions have been proposed. It is possible to store separately the compound stems and the linking morphemes (the morphemes that are inserted at the component boundaries to form a compound), and to have a grammar to combine them. This solution is realized in the morphological analyser for German SMOR (Schmid et al., 2004). The construction of such a finite-state morphology for a new language is a costly task in terms"
W15-3405,C02-2020,0,0.0609228,"t languages in particular. The experiments have shown that in some cases the quality of the extracted lexicon has been enhanced. 1 2 Bilingual Lexicon Extraction Initially designed for parallel corpora (Chen, 1993), and due to the scarcity of this kind of resources (Martin et al., 2005), bilingual lexicon extraction then tried to deal with comparable corpora instead (Fung, 1995; Rapp, 1995). An algorithm using comparable corpora is the standard method (Fung and McKeown, 1997) closely based on the notion of context vectors. Many implementations have been designed in order to do so (Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2010). A context vector w is, for a given word w, the representation of its contexts ct1 . . . cti and the number of occurrences found in the window of a corpus. In this approach, context vectors are calculated both in source and target languages corpora. They are also normalized according to association scores. Then, thanks to a seed dictionary, source context vectors are transferred into target language. The similarity between the translated context vector w for a given source word w to translate and all target context vectors t lead to the creation of a list of ranked candid"
W15-3405,P91-1017,0,0.544106,"in, 2013) or the use of unbalanced corpora (Morin and Hazem, 2014). Among them, and in the case of comparable corpora, we can denote that none looked into pivot-language approaches. Nevertheless, the idea of involving a pivot language for translation tasks is not recent. Bilingual lexicon extraction from parallel corpora has already been improved via the use of an intermediary language (Kwon et al., 2013; Seo et al., 2014; Kim et al., 2015), so does statistical translation (Simard, 1999; Och and Ney, 2001). Those works lay on the assumption that another language brings additional information (Dagan and Itai, 1991). 3 3.2 The second method based on pivot dictionaries consists in translating both source and target context vectors into pivot language. Thus, the operation of computing similarity occurs in the vectorial space of the pivot language. In order to do so, the context vector of a word in source language to translate is computed as it is usually done in the standard method. The second step is to transfer the source and target context vectors into the pivot language using source/pivot and target/pivot dictionaries. At this stage, we gather in the pivot language the translated source and all target"
W15-3405,W95-0114,0,0.257603,"nvestigated to which extent a third language could be interesting to bypass the original alignment. We have defined two original alignment approaches involving pivot languages and we have evaluated over four languages and two pivot languages in particular. The experiments have shown that in some cases the quality of the extracted lexicon has been enhanced. 1 2 Bilingual Lexicon Extraction Initially designed for parallel corpora (Chen, 1993), and due to the scarcity of this kind of resources (Martin et al., 2005), bilingual lexicon extraction then tried to deal with comparable corpora instead (Fung, 1995; Rapp, 1995). An algorithm using comparable corpora is the standard method (Fung and McKeown, 1997) closely based on the notion of context vectors. Many implementations have been designed in order to do so (Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2010). A context vector w is, for a given word w, the representation of its contexts ct1 . . . cti and the number of occurrences found in the window of a corpus. In this approach, context vectors are calculated both in source and target languages corpora. They are also normalized according to association scores. Then, thanks to a seed"
W15-3405,2001.mtsummit-papers.46,0,0.149772,"urus (D´ejean et al., 2002), implication of predictive methods for word co-occurrence counts (Hazem and Morin, 2013) or the use of unbalanced corpora (Morin and Hazem, 2014). Among them, and in the case of comparable corpora, we can denote that none looked into pivot-language approaches. Nevertheless, the idea of involving a pivot language for translation tasks is not recent. Bilingual lexicon extraction from parallel corpora has already been improved via the use of an intermediary language (Kwon et al., 2013; Seo et al., 2014; Kim et al., 2015), so does statistical translation (Simard, 1999; Och and Ney, 2001). Those works lay on the assumption that another language brings additional information (Dagan and Itai, 1991). 3 3.2 The second method based on pivot dictionaries consists in translating both source and target context vectors into pivot language. Thus, the operation of computing similarity occurs in the vectorial space of the pivot language. In order to do so, the context vector of a word in source language to translate is computed as it is usually done in the standard method. The second step is to transfer the source and target context vectors into the pivot language using source/pivot and t"
W15-3405,hazem-morin-2012-adaptive,1,0.851129,"in this field aims at improving the Introduction The main goal of this work is to investigate to which extent bilingual lexicon extraction using comparable corpora can be improved using a third language when dealing with poor resource language pairs. Indeed, the quality of the result of the extracted bilingual lexicon strongly depends on the quality of the resources, that is to say the corpora and a general language bilingual dictionary. In this study, we stress the key role of the potential high quality resources of the pivot language (Chiao and Zweigenbaum, 2004; Morin and Prochasson, 2011; Hazem and Morin, 2012). The idea of involving a third language is to benefit from the lexical information conveyed by the additional language. We also assume that in the case of not so usual language pairs the two comparable corpora are of medium quality, and the bilingual dictionary seems weak, due to the nonexistence of such a dictionary. We expect as a consequence a bad quality of the extracted lexicon. Nevertheless, we are highly confident that a language for which 32 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 32–37, c Beijing, China, July 30, 2015. 2015 Association for C"
W15-3405,P95-1050,0,0.220061,"to which extent a third language could be interesting to bypass the original alignment. We have defined two original alignment approaches involving pivot languages and we have evaluated over four languages and two pivot languages in particular. The experiments have shown that in some cases the quality of the extracted lexicon has been enhanced. 1 2 Bilingual Lexicon Extraction Initially designed for parallel corpora (Chen, 1993), and due to the scarcity of this kind of resources (Martin et al., 2005), bilingual lexicon extraction then tried to deal with comparable corpora instead (Fung, 1995; Rapp, 1995). An algorithm using comparable corpora is the standard method (Fung and McKeown, 1997) closely based on the notion of context vectors. Many implementations have been designed in order to do so (Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2010). A context vector w is, for a given word w, the representation of its contexts ct1 . . . cti and the number of occurrences found in the window of a corpus. In this approach, context vectors are calculated both in source and target languages corpora. They are also normalized according to association scores. Then, thanks to a seed dictionary, s"
W15-3405,I13-1196,1,0.828741,"t vectors translated into target language. We can say that we transferred the context vectors via a pivot language. Finally, the last step of similarity computation stays unchanged: for one source word w for which we want to find the translation in target language, we compute the similarity between its context vector transferred successively w and all target context vectors t. This method is presented in Figure 1. quality of the extracted lexicon. For instance, we can cite the use of a bilingual thesaurus (D´ejean et al., 2002), implication of predictive methods for word co-occurrence counts (Hazem and Morin, 2013) or the use of unbalanced corpora (Morin and Hazem, 2014). Among them, and in the case of comparable corpora, we can denote that none looked into pivot-language approaches. Nevertheless, the idea of involving a pivot language for translation tasks is not recent. Bilingual lexicon extraction from parallel corpora has already been improved via the use of an intermediary language (Kwon et al., 2013; Seo et al., 2014; Kim et al., 2015), so does statistical translation (Simard, 1999; Och and Ney, 2001). Those works lay on the assumption that another language brings additional information (Dagan and"
W15-3405,P99-1067,0,0.236893,"and two pivot languages in particular. The experiments have shown that in some cases the quality of the extracted lexicon has been enhanced. 1 2 Bilingual Lexicon Extraction Initially designed for parallel corpora (Chen, 1993), and due to the scarcity of this kind of resources (Martin et al., 2005), bilingual lexicon extraction then tried to deal with comparable corpora instead (Fung, 1995; Rapp, 1995). An algorithm using comparable corpora is the standard method (Fung and McKeown, 1997) closely based on the notion of context vectors. Many implementations have been designed in order to do so (Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2010). A context vector w is, for a given word w, the representation of its contexts ct1 . . . cti and the number of occurrences found in the window of a corpus. In this approach, context vectors are calculated both in source and target languages corpora. They are also normalized according to association scores. Then, thanks to a seed dictionary, source context vectors are transferred into target language. The similarity between the translated context vector w for a given source word w to translate and all target context vectors t lead to the creati"
W15-3405,I11-2003,1,0.903509,"Missing"
W15-3405,W13-2502,0,0.0196461,"d in Figure 1. quality of the extracted lexicon. For instance, we can cite the use of a bilingual thesaurus (D´ejean et al., 2002), implication of predictive methods for word co-occurrence counts (Hazem and Morin, 2013) or the use of unbalanced corpora (Morin and Hazem, 2014). Among them, and in the case of comparable corpora, we can denote that none looked into pivot-language approaches. Nevertheless, the idea of involving a pivot language for translation tasks is not recent. Bilingual lexicon extraction from parallel corpora has already been improved via the use of an intermediary language (Kwon et al., 2013; Seo et al., 2014; Kim et al., 2015), so does statistical translation (Simard, 1999; Och and Ney, 2001). Those works lay on the assumption that another language brings additional information (Dagan and Itai, 1991). 3 3.2 The second method based on pivot dictionaries consists in translating both source and target context vectors into pivot language. Thus, the operation of computing similarity occurs in the vectorial space of the pivot language. In order to do so, the context vector of a word in source language to translate is computed as it is usually done in the standard method. The second st"
W15-3405,C10-1073,0,0.291438,"Missing"
W15-3405,W99-0602,0,0.118765,"ilingual thesaurus (D´ejean et al., 2002), implication of predictive methods for word co-occurrence counts (Hazem and Morin, 2013) or the use of unbalanced corpora (Morin and Hazem, 2014). Among them, and in the case of comparable corpora, we can denote that none looked into pivot-language approaches. Nevertheless, the idea of involving a pivot language for translation tasks is not recent. Bilingual lexicon extraction from parallel corpora has already been improved via the use of an intermediary language (Kwon et al., 2013; Seo et al., 2014; Kim et al., 2015), so does statistical translation (Simard, 1999; Och and Ney, 2001). Those works lay on the assumption that another language brings additional information (Dagan and Itai, 1991). 3 3.2 The second method based on pivot dictionaries consists in translating both source and target context vectors into pivot language. Thus, the operation of computing similarity occurs in the vectorial space of the pivot language. In order to do so, the context vector of a word in source language to translate is computed as it is usually done in the standard method. The second step is to transfer the source and target context vectors into the pivot language usin"
W15-3405,W05-0809,0,0.0257099,"ora demonstrated that more than two languages can be useful to improve the alignments. Our works have investigated to which extent a third language could be interesting to bypass the original alignment. We have defined two original alignment approaches involving pivot languages and we have evaluated over four languages and two pivot languages in particular. The experiments have shown that in some cases the quality of the extracted lexicon has been enhanced. 1 2 Bilingual Lexicon Extraction Initially designed for parallel corpora (Chen, 1993), and due to the scarcity of this kind of resources (Martin et al., 2005), bilingual lexicon extraction then tried to deal with comparable corpora instead (Fung, 1995; Rapp, 1995). An algorithm using comparable corpora is the standard method (Fung and McKeown, 1997) closely based on the notion of context vectors. Many implementations have been designed in order to do so (Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2010). A context vector w is, for a given word w, the representation of its contexts ct1 . . . cti and the number of occurrences found in the window of a corpus. In this approach, context vectors are calculated both in source and target languag"
W15-3405,P14-1121,1,0.76887,"t we transferred the context vectors via a pivot language. Finally, the last step of similarity computation stays unchanged: for one source word w for which we want to find the translation in target language, we compute the similarity between its context vector transferred successively w and all target context vectors t. This method is presented in Figure 1. quality of the extracted lexicon. For instance, we can cite the use of a bilingual thesaurus (D´ejean et al., 2002), implication of predictive methods for word co-occurrence counts (Hazem and Morin, 2013) or the use of unbalanced corpora (Morin and Hazem, 2014). Among them, and in the case of comparable corpora, we can denote that none looked into pivot-language approaches. Nevertheless, the idea of involving a pivot language for translation tasks is not recent. Bilingual lexicon extraction from parallel corpora has already been improved via the use of an intermediary language (Kwon et al., 2013; Seo et al., 2014; Kim et al., 2015), so does statistical translation (Simard, 1999; Och and Ney, 2001). Those works lay on the assumption that another language brings additional information (Dagan and Itai, 1991). 3 3.2 The second method based on pivot dict"
W15-3405,W11-1205,1,0.859115,"ed translation is. Research in this field aims at improving the Introduction The main goal of this work is to investigate to which extent bilingual lexicon extraction using comparable corpora can be improved using a third language when dealing with poor resource language pairs. Indeed, the quality of the result of the extracted bilingual lexicon strongly depends on the quality of the resources, that is to say the corpora and a general language bilingual dictionary. In this study, we stress the key role of the potential high quality resources of the pivot language (Chiao and Zweigenbaum, 2004; Morin and Prochasson, 2011; Hazem and Morin, 2012). The idea of involving a third language is to benefit from the lexical information conveyed by the additional language. We also assume that in the case of not so usual language pairs the two comparable corpora are of medium quality, and the bilingual dictionary seems weak, due to the nonexistence of such a dictionary. We expect as a consequence a bad quality of the extracted lexicon. Nevertheless, we are highly confident that a language for which 32 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 32–37, c Beijing, China, July 30, 2015"
W15-3405,W97-0119,0,\N,Missing
W15-3405,C02-1166,0,\N,Missing
W15-3405,P93-1002,0,\N,Missing
W19-4730,J93-1003,0,0.400846,"t, sunt numerati, numerati cum, cum eis; 3grams: Levitae autem in, autem in tribu, non sunt numerati, sunt numerati cum, numerati cum eis; 4grams: Levitae autem in tribu, non sunt numerati cum, sunt numerati cum eis; and 5grams: non sunt numerati cum eis. Once the context vectors have been computed, an association measure is used as a way to better characterize the contextual relation between the head of the vector (familiarum suarum) and its constituents. We consider three different association measures: mutual information (Fano, 1961), discounted odds ratio (Evert, 2005) and log-likelihood (Dunning, 1993). Finally, to extract the candidates, we compute cosine similarity (Salton and Lesk, 1968) between all ngrams of the corpus. Our adaptation takes into account broken ngrams. Hence, in addition to the above cited ngrams, based on non sunt numerati cum eis, we add the following bigrams: non numerati, non cum, non eis, sunt cum, sunt eis, numerati eis. Therefore, we assume that the unigrams sunt, numerati, and cum may not appear or were omitted. (1) where D(i,j) represents the distance between two ngrams i et j and Suppcost(i), InsCost(i) represent respectively the deletion, insertion costs of i."
W19-8617,C16-2015,1,0.848181,"during model evaluation for reducing the number of mismatches associated with commonly used lexical overlap metrics. 5 Baseline, unsupervised: MultipartiteRank The second baseline we consider, MultipartiteRank (Boudin, 2018), represents the state-of-theart in unsupervised graph-based keyphrase extraction. It relies on a multipartite graph representation to enforce topical diversity while ranking keyphrase candidates. Just as FirstPhrases, this model is bound to the content of the document and cannot generate missing keyphrases. We use the implementation of MultipartiteRank available in pke9 (Boudin, 2016). State-of-the-art, supervised: CopyRNN The generative neural model we include in this study is CopyRNN (Meng et al., 2017), an encoder-decoder model that incorporates a copying mechanism (Gu et al., 2016) in order to be able to generate phrases that rarely occur. When properly trained, this model was shown to be very effective in extracting keyphrases from scientific abstracts. CopyRNN has been further extended by (Chen et al., 2018) to include correlation constraints among keyphrases which we do not include here as it yields comparable results. Two models were trained to bring evidence on th"
W19-8617,N18-2105,1,0.899875,"Missing"
W19-8617,D18-1439,0,0.405978,"ion systems. This task falls under the task of automatic keyphrase extraction which can also be the subtask of finding keyphrases that only appear in the input document. Generating keyphrases can be seen as a particular instantiation of text summarization, where the goal is not to produce a well-formed piece of text, but a coherent set of phrases that convey the most salient information. Those phrases may or may not appear in the document, the latter requiring some form of abstraction to be generated. State-of-the-art systems for this task rely on recurrent neural networks (Meng et al., 2017; Chen et al., 2018, 2019), and hence require large amounts of annotated training data to achieve good performance. As gold annoOnline news are particularly relevant to keyphrase generation since they are a natural fit for faceted navigation (Tunkelang, 2009) or topic detection and tracking (Allan, 2012). Also, and not less importantly, they are available in large quantities and are sometimes accompanied by metadata containing human-assigned keyphrases initially intended for search engines. Here, we divert these annotations from their primary purpose, and use them as gold-standard labels to automatically build o"
W19-8617,N19-1292,0,0.206661,"Missing"
W19-8617,P96-1003,0,0.0791971,"domain and include nonexpert annotations. In this paper we present KPTimes, a large-scale dataset of news texts paired with editor-curated keyphrases. Exploring the dataset, we show how editors tag documents, and how their annotations differ from those found in existing datasets. We also train and evaluate state-of-the-art neural keyphrase generation models on KPTimes to gain insights on how well they perform on the news domain. The dataset is available online at https:// github.com/ygorg/KPTimes. 1 Introduction Keyphrases are single or multi-word lexical units that best summarise a document (Evans and Zhai, 1996). As such, they are of great importance for indexing, categorising and browsing digital libraries (Witten et al., 2009). Yet, very few documents have keyphrases assigned, thus raising the need for automatic keyphrase generation systems. This task falls under the task of automatic keyphrase extraction which can also be the subtask of finding keyphrases that only appear in the input document. Generating keyphrases can be seen as a particular instantiation of text summarization, where the goal is not to produce a well-formed piece of text, but a coherent set of phrases that convey the most salien"
W19-8617,P16-1154,0,0.0386521,"titeRank (Boudin, 2018), represents the state-of-theart in unsupervised graph-based keyphrase extraction. It relies on a multipartite graph representation to enforce topical diversity while ranking keyphrase candidates. Just as FirstPhrases, this model is bound to the content of the document and cannot generate missing keyphrases. We use the implementation of MultipartiteRank available in pke9 (Boudin, 2016). State-of-the-art, supervised: CopyRNN The generative neural model we include in this study is CopyRNN (Meng et al., 2017), an encoder-decoder model that incorporates a copying mechanism (Gu et al., 2016) in order to be able to generate phrases that rarely occur. When properly trained, this model was shown to be very effective in extracting keyphrases from scientific abstracts. CopyRNN has been further extended by (Chen et al., 2018) to include correlation constraints among keyphrases which we do not include here as it yields comparable results. Two models were trained to bring evidence on the necessity to have datasets from multiple domains. CopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes), the two models use the same architecture. Perform"
W19-8617,D13-1073,0,0.0607463,"Missing"
W19-8617,D18-1208,0,0.0290571,"KPTimes are arguably more uniform and consistent, through the use of tag suggestions, which, as we will soon discuss in §5.3, makes it easier for supervised approaches to learn a good model. 5.2 Baseline: FirstPhrase 100 80 KPCrowd (readers) KP20k (authors) KPTimes (editors) -22.4 60 Position is a strong feature for keyphrase extraction, simply because texts are usually written so that the most important ideas go first (Marcu, 1997). In news summarization for example, the lead baseline –that is, the first sentences from the document–, while incredibly simple, is still a competitive baseline (Kedzie et al., 2018). Similar to the lead baseline, we compute the FirstPhrases baseline that extracts the first N keyphrase candidates8 from a document. 40 +3.4 20 0 1 +3.2 +2.0 2 3 4 Number of assignments +1.9 5 Figure 2: Distributions of gold keyphrase assignments. Next, we further looked at the characteristics of the gold keyphrases in KPTimes. Table 1 shows that the number of gold keyphrases per document is similar to the one observed for KP20k while the number of missing keyphrases is higher. This indicates that editors are more likely to generalize and assign keyphrases that do not occur in the document (≈"
W19-8617,S10-1004,0,0.10269,"Missing"
W19-8617,P97-1013,0,0.11624,"Missing"
W19-8617,marujo-etal-2012-supervised,0,0.0802035,"Missing"
W19-8617,P17-1054,0,0.325183,"c keyphrase generation systems. This task falls under the task of automatic keyphrase extraction which can also be the subtask of finding keyphrases that only appear in the input document. Generating keyphrases can be seen as a particular instantiation of text summarization, where the goal is not to produce a well-formed piece of text, but a coherent set of phrases that convey the most salient information. Those phrases may or may not appear in the document, the latter requiring some form of abstraction to be generated. State-of-the-art systems for this task rely on recurrent neural networks (Meng et al., 2017; Chen et al., 2018, 2019), and hence require large amounts of annotated training data to achieve good performance. As gold annoOnline news are particularly relevant to keyphrase generation since they are a natural fit for faceted navigation (Tunkelang, 2009) or topic detection and tracking (Allan, 2012). Also, and not less importantly, they are available in large quantities and are sometimes accompanied by metadata containing human-assigned keyphrases initially intended for search engines. Here, we divert these annotations from their primary purpose, and use them as gold-standard labels to au"
W94-0104,C92-3150,0,0.128878,"Missing"
W94-0104,C90-3010,0,0.0356811,"Missing"
W94-0104,J90-1003,0,0.015881,"Missing"
W94-0104,C94-1084,1,0.733232,"Missing"
W94-0104,J93-1003,0,0.0675582,"Missing"
W94-0104,P90-1032,0,0.0516719,"Missing"
