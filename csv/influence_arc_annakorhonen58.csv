2020.acl-main.257,Q17-1002,0,0.0183207,"t activate selectively or more strongly for a particular function such as modalityspecific or category-specific semantics (such as objects/actions, abstract/concrete, animate/inanimate, animals, fruits/vegetables, colours, body parts, countries, flowers, etc.) (Warrington, 1975; Warrington and McCarthy, 1987; McCarthy and Warrington, 1988). This indicates a function-specific 2873 division of lower-level semantic processing. Singlespace distributional word models have been found to partially correlate to these distributed brain activity patterns (Mitchell et al., 2008; Huth et al., 2012, 2016; Anderson et al., 2017), but fail to explain the full spectrum of fine-grained word associations humans are able to make. Our work has been partly inspired by this literature. Compositional Distributional Semantics. Partially motivated by similar observations, prior work frequently employs tensor-based methods for composing separate tensor spaces (Coecke et al., 2010): there, syntactic categories are often represented by tensors of different orders based on assumptions on their relations. One fundamental difference is made between atomic types (e.g., nouns) versus compositional types (e.g., verbs). Atomic types are"
2020.acl-main.257,J10-4006,0,0.0840012,"th neural training, leading to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed da"
2020.acl-main.257,D08-1007,0,0.0581951,"Missing"
2020.acl-main.257,P09-1068,0,0.0571221,"additionally evaluate our models on a number of other established datasets (Sayeed et al., 2016). Event Similarity (3 Variables: SVO). A standard task to measure the plausibility of SVO structures (i.e., events) is event similarity (Grefenstette and Sadrzadeh, 2011a; Weber et al., 2018): the goal is to score similarity between SVO triplet pairs and correlate the similarity scores to humanelicited similarity judgements. Robust and flexible event representations are important to many core areas in language understanding such as script learning, narrative generation, and discourse understanding (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Modi, 2016; Weber et al., 2018). We evaluate event similarity on two benchmarking data sets: GS199 (Grefenstette and Sadrzadeh, 2011a) and KS108 (Kartsaklis and Sadrzadeh, 2014). GS199 contains 199 pairs of SV O triplets/events. In the GS199 data set only the V is varied, while S and O are fixed in the pair: this evaluation prevents the model from relying only on simple lexical overlap for similarity computation.2 KS108 contains 108 event pairs for the same task, but is specifically constructed without any lexical overlap between the events in each pair. For this t"
2020.acl-main.257,P10-1046,0,0.060836,"usibility of the SVO combinations by scoring them against human judgments. We report consistent gains over established word representation methods, as well as over two recent tensor-based architectures (Tilk et al., 2016; Weber et al., 2018) which are designed specifically for solving the event similarity task. Furthermore, we investigate the generality of our approach by also applying it to other types of structures. We conduct additional experiments in a 4-role setting, where indirect objects are also modeled, along with a selectional preference evaluation of 2-role SV and VO relationships (Chambers and Jurafsky, 2010; Van de Cruys, 2014), yielding the highest scores on several established benchmarks. 2 Background and Motivation Representation Learning. Standard word representation models such as skip-gram negative sampling (SGNS) (Mikolov et al., 2013b,a), Glove (Pennington et al., 2014), or FastText (Bojanowski et al., 2017) induce a single word embedding space capturing broad semantic relatedness (Hill et al., 2015). For instance, SGNS makes use of two vector spaces for this purpose, which are referred to as Aw and Ac . SGNS has been shown to approximately correspond to factorising a matrix M = Aw ATc ,"
2020.acl-main.257,D14-1082,0,0.00762763,"igh similarity score of 6.53, whereas ’river meet sea’ and ’river satisfy sea’ have been given a low score of 1.84. Accuracy Using an example from Sayeed et al. (2016), the human participants were asked “how common is it for a {snake, monster, baby, cat} to frighten someone/something” (agent role) as opposed to “how common is it for a {snake, monster, baby, cat} to be frightened by someone/something” (patient role). 2877 Training Data. We parse the ukWaC corpus (Baroni et al., 2009) and the British National Corpus (BNC) (Leech, 1992) using the Stanford Parser with Universal Dependencies v1.4 (Chen and Manning, 2014; Nivre et al., 2016) and extract cooccurring subjects, verbs and objects. All words are lowercased and lemmatised, and tuples containing non-alphanumeric characters are excluded. We also remove tuples with (highly frequent) pronouns as subjects, and filter out training examples containing words with frequency lower than 50. After preprocessing, the final training corpus comprises 22M SVO triplets in total. Table 2 additionally shows training data statistics when training in the 2-group setup (SV and VO) and in the 4-group setup (when adding indirect objects: SVO+iO). We report the number of e"
2020.acl-main.257,W09-0211,0,0.0610713,"Missing"
2020.acl-main.257,D14-1004,0,0.0502357,"Missing"
2020.acl-main.257,W16-1605,0,0.0209594,"ile effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propose to induce functionspecific vector spaces which enab"
2020.acl-main.257,Q17-1010,0,0.285232,"V), I(O)). The space is optimised such that vectors for plausible SVO compositions will be close. Note that one word can have several vectors, for example chicken can occur both as S and O. Introduction Word representations are in ubiquitous usage across all areas of natural language processing (NLP) (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Standard approaches rely on the distributional hypothesis (Harris, 1954; Sch¨utze, 1993) and learn a single word vector space based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). This purely context-based training produces general word representations that capture the broad notion of semantic relatedness and conflate a variety of possible semantic relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrkˇsi´c et al., 2017) as it fails to distinguish between fine-grained word associations. In this work we propose to learn a joint functionspecific word vector space that accounts for the different roles and functions a word can take in text. The"
2020.acl-main.257,J10-4007,0,0.031176,"Missing"
2020.acl-main.257,P19-1318,0,0.0205895,"actorising a matrix M = Aw ATc , where elements in M represent the co-occurrence strengths between words and their context words (Levy and Goldberg, 2014b). Both matrices represent the same vocabulary: therefore, only one of them is needed in practice to represent each word. Typically only Aw is used while Ac is discarded, or the two vector spaces are averaged to produce the final space. Levy and Goldberg (2014a) used dependencybased contexts, resulting in two separate vector spaces; however, the relation types were embedded into the vocabulary and the model was trained only in one direction. Camacho-Collados et al. (2019) proposed to learn separate sets of relation vectors in addition to standard word vectors and showed that such relation vectors encode knowledge that is often complementary to what is coded in word vectors. Rei et al. (2018) and Vuli´c and Mrkˇsi´c (2018) described related task-dependent neural nets for mapping word embeddings into relation-specific spaces for scoring lexical entailment. In this work, we propose a task-independent approach and extend it to work with a variable number of relations. Neuroscience. Theories from cognitive linguistics and neuroscience reveal that single-space repre"
2020.acl-main.257,W15-1106,0,0.122987,"ing to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propo"
2020.acl-main.257,N15-1003,0,0.119856,"ing to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propo"
2020.acl-main.257,D11-1129,0,0.0855066,"Missing"
2020.acl-main.257,W11-2507,0,0.311832,"SVO study (V) researcher (S), scientist (S), subject (O), art (O) eat (V) food (O), cat (S), dog (S) need (V) help (O), implementation (S), support (O) Table 1: Nearest neighbours in a function-specific space trained for the SVO structure. In the Joint SVO space (bottom) we show nearest neighbors for verbs (V) from the two other subspaces (O and S). Mann and Ruhlen, 2011). In language, this event understanding information is typically captured by the SVO structures and, according to the cognitive science literature, is well aligned with how humans process sentences (McRae et al., 1997, 1998; Grefenstette and Sadrzadeh, 2011a; Kartsaklis and Sadrzadeh, 2014); it reflects the likely distinct storage and processing of objects (typically nouns) and actions (typically verbs) in the brain (Caramazza and Hillis, 1991; Damasio and Tranel, 1993). The quantitative results are reported on two established test sets for compositional event similarity (Grefenstette and Sadrzadeh, 2011a; Kartsaklis and Sadrzadeh, 2014). This task requires reasoning over SVO structures and quantifies the plausibility of the SVO combinations by scoring them against human judgments. We report consistent gains over established word representation"
2020.acl-main.257,P14-2050,0,0.194288,"and Motivation Representation Learning. Standard word representation models such as skip-gram negative sampling (SGNS) (Mikolov et al., 2013b,a), Glove (Pennington et al., 2014), or FastText (Bojanowski et al., 2017) induce a single word embedding space capturing broad semantic relatedness (Hill et al., 2015). For instance, SGNS makes use of two vector spaces for this purpose, which are referred to as Aw and Ac . SGNS has been shown to approximately correspond to factorising a matrix M = Aw ATc , where elements in M represent the co-occurrence strengths between words and their context words (Levy and Goldberg, 2014b). Both matrices represent the same vocabulary: therefore, only one of them is needed in practice to represent each word. Typically only Aw is used while Ac is discarded, or the two vector spaces are averaged to produce the final space. Levy and Goldberg (2014a) used dependencybased contexts, resulting in two separate vector spaces; however, the relation types were embedded into the vocabulary and the model was trained only in one direction. Camacho-Collados et al. (2019) proposed to learn separate sets of relation vectors in addition to standard word vectors and showed that such relation vec"
2020.acl-main.257,P16-1020,0,0.018132,"nal solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propose to induce functionspecific vector spaces which enable a better model of associations between concepts and consequently improved event representations by encoding the relevant information directly into the parameters for each word during training. Word vectors offer several advantages over tensors: a large reduction in parameters and fixed dimensionality across concepts. This facilitates their reuse and transfer across different tasks. For this reason, we find our multidirection"
2020.acl-main.257,N16-1118,0,0.0466753,"Missing"
2020.acl-main.257,J15-4004,1,0.911699,"in ubiquitous usage across all areas of natural language processing (NLP) (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Standard approaches rely on the distributional hypothesis (Harris, 1954; Sch¨utze, 1993) and learn a single word vector space based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). This purely context-based training produces general word representations that capture the broad notion of semantic relatedness and conflate a variety of possible semantic relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrkˇsi´c et al., 2017) as it fails to distinguish between fine-grained word associations. In this work we propose to learn a joint functionspecific word vector space that accounts for the different roles and functions a word can take in text. The space can be trained for a specific structure, such as SVO, and each word in a particular role will have a separate representation. Vectors for plausible SVO compositions will then be optimized to lie close together, as i"
2020.acl-main.257,C12-2054,0,0.0226717,"tandalone: their meaning is independent from other types. On the other hand, verbs are compositional as they rely on their subjects and objects for their exact meaning. Due to this added complexity, the compositional types are often represented with more parameters than the atomic types, e.g., with a matrix instead of a vector. The goal is then to compose constituents into a semantic representation which is independent of the underlying grammatical structure. Therefore, a large body of prior work is concerned with finding appropriate composition functions (Grefenstette and Sadrzadeh, 2011a,b; Kartsaklis et al., 2012; Milajevs et al., 2014) to be applied on top of word representations. Since this approach represents different syntactic structures with tensors of varying dimensions, comparing syntactic constructs is not straightforward. This compositional approach thus struggles with transferring the learned knowledge to downstream tasks. State-of-the-art compositional models (Tilk et al., 2016; Weber et al., 2018) combine similar tensor-based approaches with neural training, leading to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of"
2020.acl-main.257,P08-1028,0,0.227251,"Missing"
2020.acl-main.257,K16-1008,0,0.0310574,"tablished datasets (Sayeed et al., 2016). Event Similarity (3 Variables: SVO). A standard task to measure the plausibility of SVO structures (i.e., events) is event similarity (Grefenstette and Sadrzadeh, 2011a; Weber et al., 2018): the goal is to score similarity between SVO triplet pairs and correlate the similarity scores to humanelicited similarity judgements. Robust and flexible event representations are important to many core areas in language understanding such as script learning, narrative generation, and discourse understanding (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Modi, 2016; Weber et al., 2018). We evaluate event similarity on two benchmarking data sets: GS199 (Grefenstette and Sadrzadeh, 2011a) and KS108 (Kartsaklis and Sadrzadeh, 2014). GS199 contains 199 pairs of SV O triplets/events. In the GS199 data set only the V is varied, while S and O are fixed in the pair: this evaluation prevents the model from relying only on simple lexical overlap for similarity computation.2 KS108 contains 108 event pairs for the same task, but is specifically constructed without any lexical overlap between the events in each pair. For this task function-specific representations a"
2020.acl-main.257,Q17-1022,1,0.909677,"Missing"
2020.acl-main.257,L16-1262,0,0.0649244,"Missing"
2020.acl-main.257,D14-1162,0,0.108435,"arity score of 6.53, whereas ’river meet sea’ and ’river satisfy sea’ have been given a low score of 1.84. Accuracy Using an example from Sayeed et al. (2016), the human participants were asked “how common is it for a {snake, monster, baby, cat} to frighten someone/something” (agent role) as opposed to “how common is it for a {snake, monster, baby, cat} to be frightened by someone/something” (patient role). 2877 Training Data. We parse the ukWaC corpus (Baroni et al., 2009) and the British National Corpus (BNC) (Leech, 1992) using the Stanford Parser with Universal Dependencies v1.4 (Chen and Manning, 2014; Nivre et al., 2016) and extract cooccurring subjects, verbs and objects. All words are lowercased and lemmatised, and tuples containing non-alphanumeric characters are excluded. We also remove tuples with (highly frequent) pronouns as subjects, and filter out training examples containing words with frequency lower than 50. After preprocessing, the final training corpus comprises 22M SVO triplets in total. Table 2 additionally shows training data statistics when training in the 2-group setup (SV and VO) and in the 4-group setup (when adding indirect objects: SVO+iO). We report the number of e"
2020.acl-main.257,K15-1026,1,0.838922,"across all areas of natural language processing (NLP) (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Standard approaches rely on the distributional hypothesis (Harris, 1954; Sch¨utze, 1993) and learn a single word vector space based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). This purely context-based training produces general word representations that capture the broad notion of semantic relatedness and conflate a variety of possible semantic relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrkˇsi´c et al., 2017) as it fails to distinguish between fine-grained word associations. In this work we propose to learn a joint functionspecific word vector space that accounts for the different roles and functions a word can take in text. The space can be trained for a specific structure, such as SVO, and each word in a particular role will have a separate representation. Vectors for plausible SVO compositions will then be optimized to lie close together, as illustrated by Figure 1."
2020.acl-main.257,D16-1017,0,0.246401,"nct storage and processing of objects (typically nouns) and actions (typically verbs) in the brain (Caramazza and Hillis, 1991; Damasio and Tranel, 1993). The quantitative results are reported on two established test sets for compositional event similarity (Grefenstette and Sadrzadeh, 2011a; Kartsaklis and Sadrzadeh, 2014). This task requires reasoning over SVO structures and quantifies the plausibility of the SVO combinations by scoring them against human judgments. We report consistent gains over established word representation methods, as well as over two recent tensor-based architectures (Tilk et al., 2016; Weber et al., 2018) which are designed specifically for solving the event similarity task. Furthermore, we investigate the generality of our approach by also applying it to other types of structures. We conduct additional experiments in a 4-role setting, where indirect objects are also modeled, along with a selectional preference evaluation of 2-role SV and VO relationships (Chambers and Jurafsky, 2010; Van de Cruys, 2014), yielding the highest scores on several established benchmarks. 2 Background and Motivation Representation Learning. Standard word representation models such as skip-gram"
2020.acl-main.257,N18-1103,1,0.908435,"Missing"
2020.acl-main.257,P18-2101,1,0.90757,"Missing"
2020.acl-main.257,P99-1014,0,0.453354,"Missing"
2020.acl-main.257,W16-2518,0,0.142282,"itional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propose to induce functionspe"
2020.acl-main.337,P17-1151,0,0.0118419,"nal information from semantic lexicons by encouraging linked words to have similar vector representations. Mrkˇsi´c et al. (2017) proposed an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. Glavaˇs and Vuli´c (2018) use the linguistic constraints as training examples to learn an explicit specialization function with deep neural network architecture. There are also several studies that expand the method for acquiring a word vector to consider the uncertainty of a word meaning via Gaussian models (Vilnis and McCallum, 2015; Athiwaratkun and Wilson, 2017) and word polysemy by introducing several vectors for each word (Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Athiwaratkun et al., 2018). In this study, we only considered a vector for representing each word, but inspired by these studies, we explored models that can consider the geometry of the distribution and the existence of subgroups. The problem we tackled is similar to a selectional preference acquisition task. There have been a number of studies on selectional preference acquisition. Resnik (1996) presented an informationWe started with a centroid-based model, which"
2020.acl-main.337,P18-1001,0,0.0468905,"roving the semantic quality of word vectors by injecting constraints extracted from lexical resources. Glavaˇs and Vuli´c (2018) use the linguistic constraints as training examples to learn an explicit specialization function with deep neural network architecture. There are also several studies that expand the method for acquiring a word vector to consider the uncertainty of a word meaning via Gaussian models (Vilnis and McCallum, 2015; Athiwaratkun and Wilson, 2017) and word polysemy by introducing several vectors for each word (Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Athiwaratkun et al., 2018). In this study, we only considered a vector for representing each word, but inspired by these studies, we explored models that can consider the geometry of the distribution and the existence of subgroups. The problem we tackled is similar to a selectional preference acquisition task. There have been a number of studies on selectional preference acquisition. Resnik (1996) presented an informationWe started with a centroid-based model, which is a simple but widely used way of representing a set of word vectors (e.g., Baroni et al. 2014; Woodsend and Lapata 2015) and assumes that how likely a wo"
2020.acl-main.337,P14-1023,0,0.0550035,"2014; Neelakantan et al., 2014; Tian et al., 2014; Athiwaratkun et al., 2018). In this study, we only considered a vector for representing each word, but inspired by these studies, we explored models that can consider the geometry of the distribution and the existence of subgroups. The problem we tackled is similar to a selectional preference acquisition task. There have been a number of studies on selectional preference acquisition. Resnik (1996) presented an informationWe started with a centroid-based model, which is a simple but widely used way of representing a set of word vectors (e.g., Baroni et al. 2014; Woodsend and Lapata 2015) and assumes that how likely a word in the vector space is a member of a word class is proportional to the proximity to the centroid vectors of the class members. We then explored models that take the geometry of the distribution and the existence of subgroups into account. Here, we made two assumptions: vectors of words belonging to a certain word class are distributed with different variances depending on the direction, and most word sets will consist of several subgroups. We then explored the models that also consider negative instances. We assumed that the vector"
2020.acl-main.337,P18-1004,0,0.0289074,"Missing"
2020.acl-main.337,S13-1035,0,0.0372203,"Missing"
2020.acl-main.337,isahara-etal-2008-development,0,0.0163271,"nding order of the number of the different accusative arguments and chose the top 1,000 of them. Both datasets consisted of 1,000 verbs with at least 250 unique direct objects. We selected 200 direct objects as Wc from the most frequent 250 direct objects and the other 50 direct objects as wt for each verb. Thus, the number of tasks N was 50,000, i.e., 50 tasks for each of the 1,000 verbs. We used 2,000 negative instances against 200 positive instances to build models with negative instances. 5.2.2 WordNet datasets We used word sets extracted from English and Japanese WordNet (Fellbaum, 1998; Isahara et al., 2008) as the second type. For example, a word set consists of {dog, llama, hedgehog, wolf, etc.}, which are all hyponyms of the same synonym set (synset n01886756, placental ). We extracted the pair of a synset ID and a set of words in the synset and its hyponyms in a distance of at most five from the target synset in the WordNet hyponym tree, as shown in Figure 3. We did not use multiword expressions or words whose word vectors are not included in any of the three pre-trained word embeddings. We extracted synsets that have at least 250 words. There are 109 word sets for English datasets and 3661 1"
2020.acl-main.337,D13-1169,0,0.0674774,"Missing"
2020.acl-main.337,D14-1110,0,0.0606867,"Missing"
2020.acl-main.337,N13-1090,0,0.289623,"sed models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership. 1 Centroid of positive instances Figure 1: 2D t-SNE projection of GloVe vectors. The 200 plus symbols (+) represent the word vectors that can be a direct object of the verb play (positive instances) and the 1000 squares (  ) represent other word vectors (negative instances). Introduction Several studies have been successful in representing the meaning of a word with a vector in a continuous vector space (e.g., Mikolov et al. 2013a; Pennington et al. 2014). These representations are useful for a range of natural language processing (NLP) tasks. The interpretation and geometry of the word embeddings have also attracted attention (e.g., Kim and de Marneffe 2013; Mimno and Thompson 2017). However, little attention has been paid to the distribution of words belonging to a certain word class in a word vector space, though empirical analysis of such a distribution provides a better understanding of word vector spaces and insight into algorithmic choices for several NLP tasks, including selectional preference acquisition and"
2020.acl-main.337,M92-1003,0,0.654303,"words for scoring was 1,000, including the target word wt . For OffSet, SVML , and SVMR , we make Wn by extracting words from the other word sets subject to the constraint Wo ∩ Wn = {}. We regarded the problem as a ranking task and adopted the mean reciprocal rank (MRR) as the metric for evaluation. The MRR is calculated by the following equation: MRR = N 1 X 1 , N rank(wti ) (7) i=1 where rank(wti ) is the rank of the target word wti for each task. We tune the parameters to maximize the MRR in parameter tuning. We measured the statistical significance with an approximate randomization test (Chinchor, 1992) with 99,999 iterations and significance level α = 0.05 after Bonferroni correction. To satisfy the independence assumption, we treated each verb (for the SP datasets) or synset (for the WordNet datasets) as the unit of a randomization test. 5.4 Experimental results 5.4.1 Results on the SP datasets Tables 1 and 2 show the experimental results on the SP dataset for English and Japanese, respectively. In these tables, the best scores for each word embedding model and the scores with no significant difference from the best score are indicated in bold. In addition, the CENT score and the scores wi"
2020.acl-main.337,D17-1308,0,0.11236,"Ve vectors. The 200 plus symbols (+) represent the word vectors that can be a direct object of the verb play (positive instances) and the 1000 squares (  ) represent other word vectors (negative instances). Introduction Several studies have been successful in representing the meaning of a word with a vector in a continuous vector space (e.g., Mikolov et al. 2013a; Pennington et al. 2014). These representations are useful for a range of natural language processing (NLP) tasks. The interpretation and geometry of the word embeddings have also attracted attention (e.g., Kim and de Marneffe 2013; Mimno and Thompson 2017). However, little attention has been paid to the distribution of words belonging to a certain word class in a word vector space, though empirical analysis of such a distribution provides a better understanding of word vector spaces and insight into algorithmic choices for several NLP tasks, including selectional preference acquisition and entity set expansion. Figure 1 shows a 2D projection of word embeddings. We extracted 200 words that can be a direct object of the verb play (positive instances) and 1000 other words (negative instances) and projected their GloVe vectors (Pennington et al., 2"
2020.acl-main.337,D14-1004,0,0.0515996,"Missing"
2020.acl-main.337,N19-1423,0,0.0180099,"Missing"
2020.acl-main.337,J10-4007,0,0.0751001,"Missing"
2020.acl-main.337,N15-1184,0,0.0443188,"Missing"
2020.acl-main.337,Q17-1022,1,0.889024,"Missing"
2020.acl-main.337,D14-1113,0,0.0373902,"c et al. (2017) proposed an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. Glavaˇs and Vuli´c (2018) use the linguistic constraints as training examples to learn an explicit specialization function with deep neural network architecture. There are also several studies that expand the method for acquiring a word vector to consider the uncertainty of a word meaning via Gaussian models (Vilnis and McCallum, 2015; Athiwaratkun and Wilson, 2017) and word polysemy by introducing several vectors for each word (Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Athiwaratkun et al., 2018). In this study, we only considered a vector for representing each word, but inspired by these studies, we explored models that can consider the geometry of the distribution and the existence of subgroups. The problem we tackled is similar to a selectional preference acquisition task. There have been a number of studies on selectional preference acquisition. Resnik (1996) presented an informationWe started with a centroid-based model, which is a simple but widely used way of representing a set of word vectors (e.g., Baroni et al. 2014; Woodsend an"
2020.acl-main.337,D14-1162,0,0.0928728,"finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership. 1 Centroid of positive instances Figure 1: 2D t-SNE projection of GloVe vectors. The 200 plus symbols (+) represent the word vectors that can be a direct object of the verb play (positive instances) and the 1000 squares (  ) represent other word vectors (negative instances). Introduction Several studies have been successful in representing the meaning of a word with a vector in a continuous vector space (e.g., Mikolov et al. 2013a; Pennington et al. 2014). These representations are useful for a range of natural language processing (NLP) tasks. The interpretation and geometry of the word embeddings have also attracted attention (e.g., Kim and de Marneffe 2013; Mimno and Thompson 2017). However, little attention has been paid to the distribution of words belonging to a certain word class in a word vector space, though empirical analysis of such a distribution provides a better understanding of word vector spaces and insight into algorithmic choices for several NLP tasks, including selectional preference acquisition and entity set expansion. Figu"
2020.acl-main.337,N18-1202,0,0.089843,"Missing"
2020.acl-main.337,P15-1173,0,0.0361802,"Missing"
2020.acl-main.337,P11-2128,0,0.0168263,"LQVWDQFHV FDUGV JROI UROHV SDUW WHQQLV JDPH FKHVV FDUGV Figure 2: Examples of distributions modeled by (a) CENT, (b) GM, (c) GMM, and (d) OffSet. theoretic approach that inferred selectional preferences based on the WordNet hypernym hierarchy. Erk et al. (2010) described a method that uses corpus-driven distributional similarity metrics for selectional preference induction. Van de Cruys (2014) investigated the use of neural networks for selectional preference acquisition. An entity set expansion task (Pantel et al., 2009) is also similar to our problem and has been well studied. For example, Sadamitsu et al. (2011) disambiguated entity word senses and alleviated semantic drift by extracting topic information from LDA for entity set expansion. Zhang et al. (2016) proposed a joint model for entity set expansion and attribute extraction. In this study, we seek to understand how these vectors are distributed in the pre-trained word vector space without using contextual or lexical information. A comparison with the state-of-the-art models for selectional preference induction and entity set expansion is beyond the scope of this work. 3 Our objective is to distinguish the word wt from the words in Wo , given W"
2020.acl-main.337,P16-1211,1,0.890724,"Missing"
2020.acl-main.337,C14-1016,0,0.0207434,"n algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. Glavaˇs and Vuli´c (2018) use the linguistic constraints as training examples to learn an explicit specialization function with deep neural network architecture. There are also several studies that expand the method for acquiring a word vector to consider the uncertainty of a word meaning via Gaussian models (Vilnis and McCallum, 2015; Athiwaratkun and Wilson, 2017) and word polysemy by introducing several vectors for each word (Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Athiwaratkun et al., 2018). In this study, we only considered a vector for representing each word, but inspired by these studies, we explored models that can consider the geometry of the distribution and the existence of subgroups. The problem we tackled is similar to a selectional preference acquisition task. There have been a number of studies on selectional preference acquisition. Resnik (1996) presented an informationWe started with a centroid-based model, which is a simple but widely used way of representing a set of word vectors (e.g., Baroni et al. 2014; Woodsend and Lapata 2015) and"
2020.acl-main.337,D15-1295,0,0.0222964,"t al., 2014; Tian et al., 2014; Athiwaratkun et al., 2018). In this study, we only considered a vector for representing each word, but inspired by these studies, we explored models that can consider the geometry of the distribution and the existence of subgroups. The problem we tackled is similar to a selectional preference acquisition task. There have been a number of studies on selectional preference acquisition. Resnik (1996) presented an informationWe started with a centroid-based model, which is a simple but widely used way of representing a set of word vectors (e.g., Baroni et al. 2014; Woodsend and Lapata 2015) and assumes that how likely a word in the vector space is a member of a word class is proportional to the proximity to the centroid vectors of the class members. We then explored models that take the geometry of the distribution and the existence of subgroups into account. Here, we made two assumptions: vectors of words belonging to a certain word class are distributed with different variances depending on the direction, and most word sets will consist of several subgroups. We then explored the models that also consider negative instances. We assumed that the vectors of the words that do not"
2020.acl-main.337,P16-1023,0,0.0465921,"Missing"
2020.acl-main.618,P15-1165,0,0.0732183,"Missing"
2020.acl-main.618,P18-1072,1,0.867314,"Missing"
2020.acl-main.618,D19-1449,1,0.869192,"Missing"
2020.cl-4.5,E17-1088,0,0.0206243,"-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning—available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages. 848 Vuli´c et al. Multi-SimLex 1. Introduction The lack of annotated training and evaluation data for many tasks and domains hinders the development of computational models for the majority of the world’s languages (Snyder and Barzilay 2010; Adams et al. 2017; Ponti et al. 2019a; Joshi et al. 2020). The necessity to guide and advance multilingual and crosslingual NLP through annotation efforts that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steerin"
2020.cl-4.5,D18-1214,0,0.0617967,"Missing"
2020.cl-4.5,P18-1073,0,0.159528,"to principal component analysis) from the input distributional word vectors, since they do not contribute toward distinguishing the actual semantic meaning of different words. The method contains a single (tunable) hyperparameter ddA , which denotes the number of the dominating directions to remove from the initial representations. Previous work has verified the usefulness of ABTT in several English lexical semantic tasks such as semantic similarity, word analogies, and concept categorization, as well as in sentence-level text classification tasks (Mu, Bhat, and Viswanath 2018). (3) UNCOVEC (Artetxe et al. 2018) adjusts the similarity order of an arbitrary input word embedding space, and can emphasize either syntactic or semantic information in the transformed vectors. In short, it transforms the input space X into an adjusted space XWα through a linear map Wα controlled by a single hyperparameter α. The nth -order similarity transformation of the input word vector space X (for which n = 1) can be obtained as M n (X) = M 1 (XW (n − 1)/2 ), with Wα = QΓ α , where Q and Γ are the matrices obtained via eigendecomposition of X T X = QΓQT . Γ is a diagonal matrix containing eigenvalues of X T X; Q is an o"
2020.cl-4.5,P98-1013,0,0.103027,"Missing"
2020.cl-4.5,J82-2005,0,0.629873,"Missing"
2020.cl-4.5,L18-1618,0,0.021365,"999. On the other hand, Camacho-Collados et al. (2017) sampled a new set of 500 English concept pairs to ensure wider topical coverage and balance across similarity spectra, and then translated those pairs to German, Italian, Spanish, and Farsi (SEMEVAL-500). A similar approach was followed by Ercan and Yıldız (2018) for Turkish, by Huang et al. (2019) for Mandarin Chinese, and by Sakaizawa and Komachi (2018) for Japanese. Netisopakul, Wohlgenannt, and Pulich (2019) translated the concatenation of SimLex-999, WordSim-353, and the English SEMEVAL-500 into Thai and then reannotated it. Finally, Barzegar et al. (2018) translated English SimLex-999 and WordSim-353 to 11 resource-rich target languages (German, French, Russian, Italian, Dutch, Chinese, Portuguese, Swedish, Spanish, Arabic, Farsi), but they did not provide details concerning the translation process and the 3 More formally, colexification is a phenomenon when different meanings can be expressed by the same word in a language (Franc¸ois 2008). For instance, the two senses that are distinguished in English as time and weather are co-lexified in Croatian: the word vrijeme is used in both cases. 854 Vuli´c et al. Multi-SimLex resolution of translat"
2020.cl-4.5,N18-1083,0,0.0292232,"ially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation of semantic similarity on a continuous scale (i.e., gradience/strength of semantic similarity) (Budanitsky and Hirst 2006; Hill, Reichart, and Korhonen 2015). For any pair of words, this relation measures whether (and to what extent) their referents"
2020.cl-4.5,J19-2006,0,0.0292702,"es to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation of semantic similarity on a continuous scale (i.e., gradience/strength of semantic similarity) (Budanitsky and Hirst 2006; Hill, Reichart, and Korhonen 2015). For any pair of words, this relation measures whether (and to what extent) their referents share the same (func"
2020.cl-4.5,E17-2036,0,0.0144067,"2) Source: SemEval-17: Task 2 (henceforth SEMEVAL-500; Camacho-Collados et al. 2017). We start from the full data set of 500 concept pairs to extract a total of 334 concept pairs for English Multi-SimLex a) which contain only single-word concepts, b) which are not named entities, c) where POS tags of the two concepts are the same, d) where both concepts occur in the top 250K most frequent word types in the English Wikipedia, and e) which do not already occur in SimLex-999. The original concepts were sampled as to span all the 34 domains available as part of BabelDomains (Camacho-Collados and Navigli 2017), which roughly correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample. 3) Source: CARD-660 (Pilehvar et al. 2018). Sixty-seven word pairs are taken from this data set focused on rare word similarity, applying the same selection criteria a to e utilized for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus (Baroni et al. 2009). CARD-660 contains some words that are very rare (logboat), domain-specific (erythroleukemia), and slang (2mrw), which might be difficult to tr"
2020.cl-4.5,S17-2002,0,0.0611659,"uli´c 2018; Ponti et al. 2018b; Lauscher et al. 2019), and dictionary and thesaurus construction (Cimiano, Hotho, and Staab 2005; Hill et al. 2016). Despite the proven usefulness of semantic similarity data sets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian (Leviant and Reichart 2015), whereas some language types and low-resource languages typically lack similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish [Ercan and Yıldız 2018], 500 in Farsi [Camacho-Collados et al. 2017], or 300 in Finnish [Venekoski and Vankka 2017]) and coverage (e.g., all data sets that originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity data set spanning 1,888 concept pairs (see §4). 1 This lexical relation is, somewhat imprecisely, also termed true or pure semantic similarity (Hill, Reichart, and Korhonen 2015; Kiela, Hill, and Clark 2015); see the ensuing discussion in §2.1. 849 Computational Linguistics Volume 46, Numbe"
2020.cl-4.5,D14-1082,0,0.0211604,"Missing"
2020.cl-4.5,2020.acl-main.747,0,0.168947,"Missing"
2020.cl-4.5,D18-1269,0,0.383476,"the coverage also to languages that are resourcelean and/or typologically diverse (e.g., Welsh, Kiswahili, as in this work). Multilingual Data Sets for Natural Language Understanding. The Multi-SimLex initiative and corresponding data sets are also aligned with the recent efforts on procuring multilingual benchmarks that can help advance computational modeling of natural language understanding across different languages. For instance, pretrained multilingual language models such as multilingual BERT (Devlin et al. 2019) or XLM (Conneau and Lample 2019) are typically probed on XNLI test data (Conneau et al. 2018b) for crosslingual natural language inference. XNLI was created by translating examples from the English MultiNLI data set, and projecting its sentence labels (Williams, Nangia, and Bowman 2018). Other recent multilingual data sets target the task of question answering based on reading comprehension: i) MLQA (Lewis et al. 2019) includes 7 languages; ii) XQuAD (Artetxe, Ruder, and Yogatama 2019) 10 languages; and iii) TyDiQA (Clark et al. 2020) 9 widely spoken typologically diverse languages. While MLQA and XQuAD result from the translation from an English data set, TyDiQA was built independen"
2020.cl-4.5,Q19-1041,1,0.810306,"ingual and crosslingual NLP through annotation efforts that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 diffe"
2020.cl-4.5,C18-1323,0,0.354998,"6), text simplification (Glavaˇs and Vuli´c 2018; Ponti et al. 2018b; Lauscher et al. 2019), and dictionary and thesaurus construction (Cimiano, Hotho, and Staab 2005; Hill et al. 2016). Despite the proven usefulness of semantic similarity data sets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian (Leviant and Reichart 2015), whereas some language types and low-resource languages typically lack similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish [Ercan and Yıldız 2018], 500 in Farsi [Camacho-Collados et al. 2017], or 300 in Finnish [Venekoski and Vankka 2017]) and coverage (e.g., all data sets that originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity data set spanning 1,888 concept pairs (see §4). 1 This lexical relation is, somewhat imprecisely, also termed true or pure semantic similarity (Hill, Reichart, and Korhonen 2015; Kiela, Hill, and Clark 2015); see the ensuing discussion in §2.1. 8"
2020.cl-4.5,D19-1006,0,0.0554256,"n 2019). 875 Computational Linguistics Volume 46, Number 4 Impact of Unsupervised Post-Processing. First, the results in Table 12 suggest that applying dimension-wise mean centering to the initial vector spaces has positive impact on word similarity scores in all test languages and for all models, both static and contextualized (see the + MC rows in Table 12). Mimno and Thompson (2017) show that distributional word vectors have a tendency toward narrow clusters in the vector space (i.e., they occupy a narrow cone in the vector space and are therefore anisotropic [Mu, Bhat, and Viswanath 2018; Ethayarajh 2019]), and are prone to the undesired effect of hubness (Radovanovi´c, Nanopoulos, and Ivanovi´c 2010; Lazaridou, Dinu, and Baroni 2015).18 Applying dimension-wise mean centering has the effect of spreading the vectors across the hyperplane and mitigating the hubness issue, which consequently improves wordlevel similarity, as it emerges from the reported results. Previous work has already validated the importance of mean centering for clustering-based tasks (Suzuki et al. 2013), bilingual lexicon induction with crosslingual word embeddings (Artetxe, Labaka, and Agirre 2018a; Zhang et al. 2019; Vu"
2020.cl-4.5,N15-1184,0,0.266878,"trinsic evaluations of specific WE models as a proxy for their reliability for downstream applications (Collobert and Weston 2008; Baroni and Lenci 2010; Hill, Reichart, and Korhonen 2015); intuitively, the more WEs are misaligned with human judgments of similarity, the more their performance on actual tasks is expected to be degraded. Moreover, word representations can be specialized (a.k.a. retrofitted) by disentangling word relations of similarity and association. In particular, linguistic constraints sourced from external databases (such as synonyms from WordNet) can be injected into WEs (Faruqui et al. 2015; Wieting et al. 2015; Mrkˇsi´c et al. 2017; Lauscher et al. 2019; Kamath et al. 2019, inter alia) in order to enforce a particular relation in a distributional semantic space while preserving the original adjacency properties. 2.3 Similarity and Language Variation: Semantic Typology In this work, we tackle the concept of (true and gradient) semantic similarity from a multilingual perspective. Although the same meaning representations may be shared by all human speakers at a deep cognitive level, there is no one-to-one mapping between the words in the lexicons of different languages. This make"
2020.cl-4.5,N18-2029,1,0.762404,"Missing"
2020.cl-4.5,P19-1070,1,0.912198,"Missing"
2020.cl-4.5,Q16-1002,1,0.880245,"Missing"
2020.cl-4.5,J15-4004,1,0.93809,"Missing"
2020.cl-4.5,D18-1043,0,0.0596882,"Missing"
2020.cl-4.5,2020.acl-main.560,0,0.0211952,"guiding future developments in multilingual lexical semantics and representation learning—available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages. 848 Vuli´c et al. Multi-SimLex 1. Introduction The lack of annotated training and evaluation data for many tasks and domains hinders the development of computational models for the majority of the world’s languages (Snyder and Barzilay 2010; Adams et al. 2017; Ponti et al. 2019a; Joshi et al. 2020). The necessity to guide and advance multilingual and crosslingual NLP through annotation efforts that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zema"
2020.cl-4.5,D18-1330,0,0.0361587,"Missing"
2020.cl-4.5,W14-1503,0,0.0323243,"wski et al. 2017) and contextualized WEs learned from modeling word sequences (Peters et al. 2018; Devlin et al. 2019, inter alia). As a result, in the induced representations, geometrical closeness (measured, e.g., through cosine distance) conflates genuine similarity with broad relatedness. For 852 Vuli´c et al. Multi-SimLex instance, the vectors for antonyms such as sober and drunk, by definition dissimilar, might be neighbors in the semantic space under the distributional hypothesis. Similar to work on distributional representations that predated the WE era (Sahlgren 2006), Turney (2012), Kiela and Clark (2014), and Melamud et al. (2016) demonstrated that different choices of hyperparameters in WE algorithms (such as context window) emphasize different relations in the resulting representations. Likewise, Agirre et al. (2009) and Levy and Goldberg (2014) discovered that WEs learned from texts annotated with syntactic information mirror similarity better than simple local bag-of-words neighborhoods. The failure of WEs to capture semantic similarity, in turn, affects model performance in several NLP applications where such knowledge is crucial. In particular, Natural Language Understanding tasks such"
2020.cl-4.5,W16-1607,0,0.0601778,"Missing"
2020.cl-4.5,kipper-etal-2004-extending,0,0.0569229,"Missing"
2020.cl-4.5,D19-1279,0,0.0717479,"Missing"
2020.cl-4.5,2020.emnlp-main.363,1,0.891144,"Missing"
2020.cl-4.5,P15-1027,0,0.0827227,"Missing"
2020.cl-4.5,P14-2050,0,0.0586512,"their associated meaning confounds the two distinct relations (Hill, Reichart, and Korhonen 2015; Schwartz, Reichart, and Rappoport 2015; Vuli´c et al. 2017b). As a result, distributional methods obscure a crucial facet of lexical meaning. This limitation also reflects onto word embeddings (WEs), representations of words as low-dimensional vectors that have become indispensable for a wide range of NLP applications (Collobert et al. 2011; Chen and Manning 2014; Melamud et al. 2016, inter alia). In particular, it involves both static WEs learned from co-occurrence patterns (Mikolov et al. 2013; Levy and Goldberg 2014; Bojanowski et al. 2017) and contextualized WEs learned from modeling word sequences (Peters et al. 2018; Devlin et al. 2019, inter alia). As a result, in the induced representations, geometrical closeness (measured, e.g., through cosine distance) conflates genuine similarity with broad relatedness. For 852 Vuli´c et al. Multi-SimLex instance, the vectors for antonyms such as sober and drunk, by definition dissimilar, might be neighbors in the semantic space under the distributional hypothesis. Similar to work on distributional representations that predated the WE era (Sahlgren 2006), Turney"
2020.cl-4.5,2020.emnlp-main.484,0,0.0477066,"Missing"
2020.cl-4.5,D18-1521,0,0.0264898,"Missing"
2020.cl-4.5,D17-1308,0,0.0696529,"Missing"
2020.cl-4.5,N19-1386,0,0.0334837,"Missing"
2020.cl-4.5,Q17-1022,1,0.934218,"Missing"
2020.cl-4.5,L18-1381,0,0.0518909,"Missing"
2020.cl-4.5,D18-1169,0,0.15982,"me prominent English word pair data sets such as WordSim-353 (Finkelstein et al. 2002), MEN (Bruni, Tran, and Baroni 2014), or Stanford Rare Words (Luong, Socher, and Manning 2013) did not discriminate between similarity and relatedness, the importance of this distinction was established by Hill, Reichart, and Korhonen (2015) (see again the discussion in §2.1) through the creation of SimLex-999. This inspired other similar data sets that focused on different lexical properties. For instance, SimVerb-3500 (Gerz et al. 2016) provided similarity ratings for 3,500 English verbs, whereas CARD-660 (Pilehvar et al. 2018) aimed at measuring the semantic similarity of infrequent concepts. Semantic Similarity Data Sets in Other Languages. Motivated by the impact of data sets such as SimLex-999 and SimVerb-3500 on representation learning in English, a line of related work put focus on creating similar resources in other languages. The dominant approach is translating and reannotating the entire original English SimLex-999 data set, as done previously for German, Italian, and Russian (Leviant and Reichart 2015), Hebrew and Croatian (Mrkˇsi´c et al. 2017), and Polish (Mykowiecka, Marciniak, and Rychlik 2018). Venek"
2020.cl-4.5,P19-1493,0,0.0740204,"Missing"
2020.cl-4.5,J19-3005,1,0.889644,"Missing"
2020.cl-4.5,D18-1026,1,0.925849,"Missing"
2020.cl-4.5,Q17-1020,0,0.0167721,"that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation"
2020.cl-4.5,D18-1299,0,0.0140965,"i.e., the distributional information).1 Data sets that quantify the strength of semantic similarity between concept pairs such as SimLex-999 (Hill, Reichart, and Korhonen 2015) or SimVerb-3500 (Gerz et al. 2016) have been instrumental in improving models for distributional semantics and representation learning. Discerning between semantic similarity and relatedness/association is not only crucial for theoretical studies on lexical semantics (see §2), but has also been shown to benefit a range of language understanding tasks in NLP. Examples include dialog state tracking (Mrkˇsi´c et al. 2017; Ren et al. 2018), spoken language understanding (Kim et al. 2016; Kim, de Marneffe, and Fosler-Lussier 2016), text simplification (Glavaˇs and Vuli´c 2018; Ponti et al. 2018b; Lauscher et al. 2019), and dictionary and thesaurus construction (Cimiano, Hotho, and Staab 2005; Hill et al. 2016). Despite the proven usefulness of semantic similarity data sets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian (Leviant and Reichart 2015), whereas some language types and low-resource languages typically lack similar evaluation data. Eve"
2020.cl-4.5,Q19-1044,1,0.813505,"elines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation of semantic similarity on a continuous scal"
2020.cl-4.5,L18-1152,0,0.223909,"2015), Hebrew and Croatian (Mrkˇsi´c et al. 2017), and Polish (Mykowiecka, Marciniak, and Rychlik 2018). Venekoski and Vankka (2017) applied this process only to a subset of 300 concept pairs from the English SimLex-999. On the other hand, Camacho-Collados et al. (2017) sampled a new set of 500 English concept pairs to ensure wider topical coverage and balance across similarity spectra, and then translated those pairs to German, Italian, Spanish, and Farsi (SEMEVAL-500). A similar approach was followed by Ercan and Yıldız (2018) for Turkish, by Huang et al. (2019) for Mandarin Chinese, and by Sakaizawa and Komachi (2018) for Japanese. Netisopakul, Wohlgenannt, and Pulich (2019) translated the concatenation of SimLex-999, WordSim-353, and the English SEMEVAL-500 into Thai and then reannotated it. Finally, Barzegar et al. (2018) translated English SimLex-999 and WordSim-353 to 11 resource-rich target languages (German, French, Russian, Italian, Dutch, Chinese, Portuguese, Swedish, Spanish, Arabic, Farsi), but they did not provide details concerning the translation process and the 3 More formally, colexification is a phenomenon when different meanings can be expressed by the same word in a language (Franc¸ois 20"
2020.cl-4.5,P19-1072,0,0.0344182,"Missing"
2020.cl-4.5,K15-1026,1,0.904015,"Missing"
2020.cl-4.5,P18-1072,1,0.919137,"Missing"
2020.cl-4.5,N16-1161,0,0.0133804,"For instance, we have highlighted how sharing the same encoder parameters across multiple languages may harm performance. However, it remains unclear if, and to what extent, the input language embeddings present in XLM -100 but absent in 886 Vuli´c et al. Multi-SimLex M - BERT help mitigate this issue. In addition, pretrained language embeddings can be obtained both from typological databases (Littell et al. 2017) and from neural architectures (Malaviya, Neubig, and Littell 2017). Plugging these embeddings into the encoders in lieu of embeddings trained end-to-end as suggested by prior work (Tsvetkov et al. 2016; Ammar et al. 2016; Ponti et al. 2019b) might extend the coverage to more resourcelean languages. Another important follow-up analysis might involve the comparison of the performance of representation learning models on multilingual data sets for both word-level semantic similarity and sentence-level natural language understanding. In particular, Multi-SimLex fills a gap in available resources for multilingual NLP and might help understand how lexical and compositional semantics interact if put alongside existing resources such as XNLI (Conneau et al. 2018b) for natural language inference or"
2020.cl-4.5,P19-1490,1,0.894602,"Missing"
2020.cl-4.5,Q15-1025,0,0.0155434,"f specific WE models as a proxy for their reliability for downstream applications (Collobert and Weston 2008; Baroni and Lenci 2010; Hill, Reichart, and Korhonen 2015); intuitively, the more WEs are misaligned with human judgments of similarity, the more their performance on actual tasks is expected to be degraded. Moreover, word representations can be specialized (a.k.a. retrofitted) by disentangling word relations of similarity and association. In particular, linguistic constraints sourced from external databases (such as synonyms from WordNet) can be injected into WEs (Faruqui et al. 2015; Wieting et al. 2015; Mrkˇsi´c et al. 2017; Lauscher et al. 2019; Kamath et al. 2019, inter alia) in order to enforce a particular relation in a distributional semantic space while preserving the original adjacency properties. 2.3 Similarity and Language Variation: Semantic Typology In this work, we tackle the concept of (true and gradient) semantic similarity from a multilingual perspective. Although the same meaning representations may be shared by all human speakers at a deep cognitive level, there is no one-to-one mapping between the words in the lexicons of different languages. This makes the comparison of s"
2020.cl-4.5,2020.acl-main.536,0,0.0322002,"Missing"
2020.cl-4.5,D19-1077,0,0.0931228,". Because the concept pairs in Multi-SimLex are lowercased, 12 We also tested another encoding method where we fed pairs instead of single words/concepts into the pretrained encoder. The rationale is that the other concept in the pair can be used as a disambiguation signal. However, this method consistently led to sub-par performance across all experimental runs. 873 Computational Linguistics Volume 46, Number 4 we use the uncased version of M - BERT.13 M - BERT comprises all Multi-SimLex languages, and its evident ability to perform crosslingual transfer (Pires, Schlinger, and Garrette 2019; Wu and Dredze 2019; Wang et al. 2020) also makes it a convenient baseline model for crosslingual experiments later in §8. The second multilingual model we consider, XLM -100,14 is pretrained on Wikipedia dumps of 100 languages, and encodes each concept into a 1,280-dimensional representation. In contrast to M - BERT, XLM -100 drops the next-sentence prediction objective and adds a crosslingual masked language modeling objective. For both encoders, the representations of each concept are computed as averages over the first H = 4 hidden layers in all experiments.15 Besides M - BERT and XLM, covering multiple lang"
2020.cl-4.5,K18-2001,0,0.0637127,"Missing"
2020.cl-4.5,P19-1307,0,0.0177035,"2018; Ethayarajh 2019]), and are prone to the undesired effect of hubness (Radovanovi´c, Nanopoulos, and Ivanovi´c 2010; Lazaridou, Dinu, and Baroni 2015).18 Applying dimension-wise mean centering has the effect of spreading the vectors across the hyperplane and mitigating the hubness issue, which consequently improves wordlevel similarity, as it emerges from the reported results. Previous work has already validated the importance of mean centering for clustering-based tasks (Suzuki et al. 2013), bilingual lexicon induction with crosslingual word embeddings (Artetxe, Labaka, and Agirre 2018a; Zhang et al. 2019; Vuli´c et al. 2019), and for modeling lexical semantic change (Schlechtweg et al. 2019). However, to the best of our knowledge, the results summarized in Table 12 are the first evidence that also confirms its importance for semantic similarity in a wide array of languages. In sum, as a general rule of thumb, we suggest always mean-centering representations for semantic tasks. The results further indicate that additional post-processing methods such as ABTT and UNCOVEC on top of mean-centered vector spaces can lead to further gains in most languages. The gains are even visible for languages t"
2020.cl-4.5,K19-1021,1,0.900219,"Missing"
2020.cl-4.5,C98-1013,0,\N,Missing
2020.cl-4.5,J10-4006,0,\N,Missing
2020.cl-4.5,P94-1019,0,\N,Missing
2020.cl-4.5,J06-1003,0,\N,Missing
2020.cl-4.5,N09-1003,0,\N,Missing
2020.cl-4.5,D14-1034,1,\N,Missing
2020.cl-4.5,W13-3512,0,\N,Missing
2020.cl-4.5,D15-1242,0,\N,Missing
2020.cl-4.5,P15-2001,0,\N,Missing
2020.cl-4.5,kamholz-etal-2014-panlex,0,\N,Missing
2020.cl-4.5,N15-1104,0,\N,Missing
2020.cl-4.5,N16-1060,1,\N,Missing
2020.cl-4.5,Q17-1010,0,\N,Missing
2020.cl-4.5,P16-1024,1,\N,Missing
2020.cl-4.5,J17-4004,1,\N,Missing
2020.cl-4.5,E17-1016,1,\N,Missing
2020.cl-4.5,E17-2002,0,\N,Missing
2020.cl-4.5,P17-1042,0,\N,Missing
2020.cl-4.5,P18-1004,1,\N,Missing
2020.cl-4.5,P18-1142,1,\N,Missing
2020.cl-4.5,D18-1027,0,\N,Missing
2020.cl-4.5,D18-1024,0,\N,Missing
2020.cl-4.5,K18-1028,0,\N,Missing
2020.cl-4.5,N19-1391,0,\N,Missing
2020.cl-4.5,N19-1131,0,\N,Missing
2020.cl-4.5,K17-1013,1,\N,Missing
2020.cl-4.5,N19-1423,0,\N,Missing
2020.cl-4.5,N18-1101,0,\N,Missing
2020.cl-4.5,P19-4007,1,\N,Missing
2020.cl-4.5,W19-4310,1,\N,Missing
2020.cl-4.5,D19-1449,1,\N,Missing
2020.cl-4.5,D19-1288,1,\N,Missing
2020.cl-4.5,D19-1226,1,\N,Missing
2020.cl-4.5,D19-1165,0,\N,Missing
2020.cl-4.5,K19-1004,1,\N,Missing
2020.cl-4.5,D19-2007,1,\N,Missing
2020.cl-4.5,W17-0228,0,\N,Missing
2020.coling-main.118,2020.acl-srw.36,0,0.0330834,"Missing"
2020.coling-main.118,Q17-1010,0,0.34049,"r).1 We transform the constraints from C into a BERT-compatible input format and feed them as additional training examples for the model. The encoding of a constraint is then forwarded to the relation classifier, which predicts whether the input word pair represents a valid lexical relation. From Linguistic Constraints to Training Instances. We start from a set of linguistic constraints C = {(w1 , w2 )i }Ni=1 and an auxiliary static word embedding space Xaux ∈ Rd . The space Xaux can be obtained via any standard static word embedding model such as Skip-Gram (Mikolov et al., 2013) or fastText (Bojanowski et al., 2017) (used in this work). Each constraint c = (w1 , w2 ) corresponds to a true/positive relation of semantic similarity, and thus represents a positive training example for the model (label 1). For each positive example c, we create corresponding negative examples following prior work on specialization of static embeddings (Wieting et al., 2015; Glavaˇs and Vuli´c, 2018; Ponti et al., 2019). We first group positive constraints from C into mini-batches B p of size k. For each positive example c = (w1 , w2 ), we create two negatives cˆ1 = (wˆ 1 , w2 ) and cˆ2 = (w1 , wˆ 2 ) such that wˆ 1 is the wor"
2020.coling-main.118,S17-2001,0,0.0785084,"Missing"
2020.coling-main.118,2020.acl-main.747,0,0.0992258,"Missing"
2020.coling-main.118,N19-1423,0,0.606051,"arity, yields better performance than the lexically blind “vanilla” BERT on several language understanding tasks. Concretely, LIBERT outperforms BERT in 9 out of 10 tasks of the GLUE benchmark and is on a par with BERT in the remaining one. Moreover, we show consistent gains on 3 benchmarks for lexical simplification, a task where knowledge about word-level semantic similarity is paramount, as well as large gains on lexical reasoning probes. 1 Introduction Unsupervised pretraining models, such as GPT and GPT-2 (Radford et al., 2018; Radford et al., 2019), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019) yield state-of-the-art performance on a wide range of natural language processing tasks. All these models rely on language modeling (LM) objectives that exploit the knowledge encoded in large text corpora. BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP). Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences. While several concurrent research threads are focused on makin"
2020.coling-main.118,I05-5002,0,0.0570753,"Missing"
2020.coling-main.118,N15-1184,0,0.133257,"Missing"
2020.coling-main.118,P15-2011,1,0.904732,"Missing"
2020.coling-main.118,P18-1004,1,0.919583,"Missing"
2020.coling-main.118,P19-1476,1,0.878044,"Missing"
2020.coling-main.118,J15-4004,1,0.777108,"tances from a batch B p of k positive training instances. Next, we transform each instance (i.e., a pair of words) into a “BERT-compatible” format, i.e., into a sequence of WordPiece (Wu et al., 2016) tokens.2 We split both w1 and w2 into WordPiece tokens, insert the special separator token (with a randomly initialized embedding) before and after the tokens of w2 and prepend the whole sequence with BERT’s sequence start token, as shown in this example for the constraint (mended, regenerated):3 1 As the goal is to inform the BERT model on the relation of true semantic similarity between words (Hill et al., 2015), according to prior work on static word embeddings, the sets of both synonym pairs and direct hyponym-hypernym pairs are useful to boost the model’s ability to capture true semantic similarity, which in turn has a positive effect on downstream language understanding applications. See the work of Hill et al. (2015) and Vuli´c (2018) for further details regarding the relationship between direct hyponym-hypernym pairs and true semantic similarity. 2 We use the same 30K WordPiece vocabulary as Devlin et al. (2019). Sharing WordPieces helps our word-level task as lexico-semantic relationships are"
2020.coling-main.118,P14-2075,0,0.0658816,"emantic similarity between the fastText vectors (Bojanowski et al., 2017) of the original word w and the candidate ci , and (4) word frequency of ci in the top 12 million texts of Wikipedia and in the Children’s Book Test corpus.8 Based on the individual features, we next rank the candidates in C and consequently, obtain a set of ranks for each ci . The best candidate is chosen according to its average rank across all features. In our experiments, we fix the number of candidates k to 6. Evaluation Data. We run the evaluation on three standard datasets for lexical simplification: (1) LexMTurk (Horn et al., 2014). The dataset consists of 500 English instances, which are collected from Wikipedia. The complex word and the simpler substitutions were annotated by 50 crowd workers on Amazon Mechanical Turk. (2) BenchLS (Paetzold and Specia, 2016) is a merge of LexMTurk and LSeval (De Belder and Moens, 2010) containing 929 sentences. The latter dataset focuses on text simplification for children. The authors of BenchLS applied additional corrections over the instances of the two datasets. (3) NNSeval (Paetzold and Specia, 2017) is an English dataset focused on text simplification for non-native speakers and"
2020.coling-main.118,W18-3003,0,0.0138404,"with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external constraints, but rather the entire embedding space. In explicit retrofitting models (Glavaˇs and Vuli´c, 2018; Glavaˇs and Vuli´c, 2019), a (deep, non-linear) specialization function is directly learned from external constraints. Post-specialization models (Vuli´c et al., 2018; Ponti et al., 2018; Kamath et al., 2019; Biesialska et al., 2020), instead, propagate lexico-semantic information to unseen words by imitating the transformation undergone by seen words dur"
2020.coling-main.118,W19-4310,1,0.883945,"Missing"
2020.coling-main.118,D15-1242,0,0.0229554,"ely researched problem. For instance, clearly discerning between true/pure semantic similarity and broader conceptual relatedness in static embeddings benefits a range of natural language understanding tasks such as dialog state tracking (Mrkˇsi´c et al., 2017), text simplification (Glavaˇs and Vuli´c, 2018), and spoken language understanding (Kim et al., 2016). The most widespread solution relies on the use of specialization algorithms to enrich word embeddings with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external co"
2020.coling-main.118,2020.deelio-1.5,1,0.878646,"Missing"
2020.coling-main.118,C18-1205,0,0.0281817,"al knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external constraints, but rather the entire embedding space. In explicit retrofitting models (Glavaˇs and Vuli´c, 2018; Glavaˇs and Vuli´c, 2019), a (deep, non-linear) specialization function is directly learned from external constraints. Post-specialization models (Vuli´c et al., 2018; Ponti et al., 2018; Kamath et al., 2019; Biesialska et al., 2020), instead, propagate lexico-semantic information to unseen words by imitating the transformation undergone by seen words during the initial specializ"
2020.coling-main.118,P15-1145,0,0.0248839,"em. For instance, clearly discerning between true/pure semantic similarity and broader conceptual relatedness in static embeddings benefits a range of natural language understanding tasks such as dialog state tracking (Mrkˇsi´c et al., 2017), text simplification (Glavaˇs and Vuli´c, 2018), and spoken language understanding (Kim et al., 2016). The most widespread solution relies on the use of specialization algorithms to enrich word embeddings with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external constraints, but rat"
2020.coling-main.118,N16-1018,0,0.0427133,"Missing"
2020.coling-main.118,Q17-1022,1,0.931397,"Missing"
2020.coling-main.118,Q16-1030,0,0.0174702,"clearly discerning between true/pure semantic similarity and broader conceptual relatedness in static embeddings benefits a range of natural language understanding tasks such as dialog state tracking (Mrkˇsi´c et al., 2017), text simplification (Glavaˇs and Vuli´c, 2018), and spoken language understanding (Kim et al., 2016). The most widespread solution relies on the use of specialization algorithms to enrich word embeddings with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external constraints, but rather the entire embeddi"
2020.coling-main.118,L16-1491,0,0.0237812,"pus.8 Based on the individual features, we next rank the candidates in C and consequently, obtain a set of ranks for each ci . The best candidate is chosen according to its average rank across all features. In our experiments, we fix the number of candidates k to 6. Evaluation Data. We run the evaluation on three standard datasets for lexical simplification: (1) LexMTurk (Horn et al., 2014). The dataset consists of 500 English instances, which are collected from Wikipedia. The complex word and the simpler substitutions were annotated by 50 crowd workers on Amazon Mechanical Turk. (2) BenchLS (Paetzold and Specia, 2016) is a merge of LexMTurk and LSeval (De Belder and Moens, 2010) containing 929 sentences. The latter dataset focuses on text simplification for children. The authors of BenchLS applied additional corrections over the instances of the two datasets. (3) NNSeval (Paetzold and Specia, 2017) is an English dataset focused on text simplification for non-native speakers and consists in total of 239 instances. Similar to BenchLS, the dataset is based on LexMTurk, but filtered for a) instances that contain a complex target word for non-native speakers, and b) simplification candidates that were found to"
2020.coling-main.118,N18-1202,0,0.0455618,"or the word-level semantic similarity, yields better performance than the lexically blind “vanilla” BERT on several language understanding tasks. Concretely, LIBERT outperforms BERT in 9 out of 10 tasks of the GLUE benchmark and is on a par with BERT in the remaining one. Moreover, we show consistent gains on 3 benchmarks for lexical simplification, a task where knowledge about word-level semantic similarity is paramount, as well as large gains on lexical reasoning probes. 1 Introduction Unsupervised pretraining models, such as GPT and GPT-2 (Radford et al., 2018; Radford et al., 2019), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019) yield state-of-the-art performance on a wide range of natural language processing tasks. All these models rely on language modeling (LM) objectives that exploit the knowledge encoded in large text corpora. BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP). Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences. While several concurrent resea"
2020.coling-main.118,D19-1005,0,0.0962985,"e text corpora. BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP). Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences. While several concurrent research threads are focused on making BERT optimization more robust (Liu et al., 2019) or on imprinting external world knowledge on its representations (Sun et al., 2019; Zhang et al., 2019; Sun et al., 2020; Liu et al., 2020; Peters et al., 2019; Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broader topical relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015; Wietin"
2020.coling-main.118,D19-1250,0,0.0534456,"Missing"
2020.coling-main.118,2020.emnlp-main.617,1,0.883586,"Missing"
2020.coling-main.118,D18-1026,1,0.91771,"Missing"
2020.coling-main.118,D19-1226,1,0.894989,"Missing"
2020.coling-main.118,D16-1264,0,0.10254,"Missing"
2020.coling-main.118,D18-1299,0,0.0170595,"n the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018, inter alia) in order to emphasize a particular lexical relation in a specialized embedding space. For instance, lexically informed word vectors specialized for pure semantic similarity result in substantial gains in a number of downstream tasks where such similarity plays an important role, e.g., in dialog state tracking (Mrkˇsi´c et al., 2017; Ren et al., 2018) or for lexical This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1371 Proceedings of the 28th International Conference on Computational Linguistics, pages 1371–1383 Barcelona, Spain (Online), December 8-13, 2020 simplification (Glavaˇs and Vuli´c, 2018; Ponti et al., 2019). Existing specialization methods are, however, not directly applicable to unsupervised pretraining models because they are either (1) tied to a particular training objective of a static word embedding model, or (2) predicated"
2020.coling-main.118,K15-1026,0,0.0583297,"rch threads are focused on making BERT optimization more robust (Liu et al., 2019) or on imprinting external world knowledge on its representations (Sun et al., 2019; Zhang et al., 2019; Sun et al., 2020; Liu et al., 2020; Peters et al., 2019; Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broader topical relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018, inter alia) in order to emphasize a particular lexical relation in a specialized embedding space. For instance, lexically informed word vectors specialized for pure semantic similarity result in substantial gains in a number of downstream tasks where such similarity plays an important role, e.g., in dialog state tracki"
2020.coling-main.118,D13-1170,0,0.0139938,"Missing"
2020.coling-main.118,N18-1103,1,0.886385,"Missing"
2020.coling-main.118,N18-1048,1,0.919448,"Missing"
2020.coling-main.118,W18-3018,1,0.88785,"Missing"
2020.coling-main.118,W18-5446,0,0.34382,"ances in the training batch: LLRC = − ∑ ln yˆ k · yk . (2) k where y ∈ {[0, 1], [1, 0]} is the true relation label for a word-pair training instance. 4 Language Understanding Evaluation To isolate the effects of injecting linguistic knowledge into BERT, we train base BERT and LIBERT in the same setting: the only difference is that we additionally update the parameters of LIBERT’s Transformer encoder based on the gradients of the LRC loss LLRC from Eq. (2). In the first set of experiments, we probe the usefulness of injecting semantic similarity knowledge on the well-known suite of GLUE tasks (Wang et al., 2018), while we also present the results on lexical simplification, another task that has been shown to benefit from lexico-semantic similarity specialization (Glavaˇs and Vuli´c, 2018), later in §5. 4.1 Experimental Setup Pretraining Data. We minimize BERT’s original objective LMLM + LNSP on training examples coming from English Wikipedia.4 We obtain the set of constraints C for the LLRC term from the body of previous work on semantic specialization of static word embeddings (Zhang et al., 2014; Vuli´c et al., 2018; Ponti et al., 2018). In particular, we collect 1,023,082 synonymy pairs from WordN"
2020.coling-main.118,Q19-1040,0,0.0209636,"lization of static word embeddings (Zhang et al., 2014; Vuli´c et al., 2018; Ponti et al., 2018). In particular, we collect 1,023,082 synonymy pairs from WordNet (Miller, 1995) and Roget’s Thesaurus (Kipfer, 2009) and 326,187 direct hyponym-hypernym pairs (Vuli´c and Mrkˇsi´c, 2018) from WordNet, and use them as positive instances for the binary classifier (LRC).5 Fine-Tuning (Downstream) Tasks. We evaluate BERT and LIBERT on the the following tasks from the GLUE benchmark (Wang et al., 2018), where sizes of training, development, and test datasets for each task are provided in Table 1: CoLA (Warstadt et al., 2019): Binary sentence classification, predicting if sentences from linguistic publications are grammatically acceptable; 4 We acknowledge that training the models on larger corpora would likely lead to better absolute downstream scores; however, the main goal of this work is not to achieve state-of-the-art downstream performance, but to compare the base model against its lexically informed counterpart. 5 Note again that similar to work of Vuli´c (2018), both WordNet synonyms and direct hyponym-hypernym pairs are treated exactly the same: as positive examples for the relation of true semantic simil"
2020.coling-main.118,Q15-1025,0,0.524219,", 2019; Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broader topical relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018, inter alia) in order to emphasize a particular lexical relation in a specialized embedding space. For instance, lexically informed word vectors specialized for pure semantic similarity result in substantial gains in a number of downstream tasks where such similarity plays an important role, e.g., in dialog state tracking (Mrkˇsi´c et al., 2017; Ren et al., 2018) or for lexical This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1371 Proceedings of the 28"
2020.coling-main.118,N18-1101,0,0.139902,"Missing"
2020.coling-main.118,P14-2089,0,0.0618712,"ations is an extensively researched problem. For instance, clearly discerning between true/pure semantic similarity and broader conceptual relatedness in static embeddings benefits a range of natural language understanding tasks such as dialog state tracking (Mrkˇsi´c et al., 2017), text simplification (Glavaˇs and Vuli´c, 2018), and spoken language understanding (Kim et al., 2016). The most widespread solution relies on the use of specialization algorithms to enrich word embeddings with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words foun"
2020.coling-main.118,D14-1161,0,0.0234709,"e probe the usefulness of injecting semantic similarity knowledge on the well-known suite of GLUE tasks (Wang et al., 2018), while we also present the results on lexical simplification, another task that has been shown to benefit from lexico-semantic similarity specialization (Glavaˇs and Vuli´c, 2018), later in §5. 4.1 Experimental Setup Pretraining Data. We minimize BERT’s original objective LMLM + LNSP on training examples coming from English Wikipedia.4 We obtain the set of constraints C for the LLRC term from the body of previous work on semantic specialization of static word embeddings (Zhang et al., 2014; Vuli´c et al., 2018; Ponti et al., 2018). In particular, we collect 1,023,082 synonymy pairs from WordNet (Miller, 1995) and Roget’s Thesaurus (Kipfer, 2009) and 326,187 direct hyponym-hypernym pairs (Vuli´c and Mrkˇsi´c, 2018) from WordNet, and use them as positive instances for the binary classifier (LRC).5 Fine-Tuning (Downstream) Tasks. We evaluate BERT and LIBERT on the the following tasks from the GLUE benchmark (Wang et al., 2018), where sizes of training, development, and test datasets for each task are provided in Table 1: CoLA (Warstadt et al., 2019): Binary sentence classification"
2020.coling-main.118,P19-1139,0,0.144719,"M) objectives that exploit the knowledge encoded in large text corpora. BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP). Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences. While several concurrent research threads are focused on making BERT optimization more robust (Liu et al., 2019) or on imprinting external world knowledge on its representations (Sun et al., 2019; Zhang et al., 2019; Sun et al., 2020; Liu et al., 2020; Peters et al., 2019; Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broader topical relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resource"
2020.coling-main.118,2020.acl-main.201,0,0.015309,"ther the entire embedding space. In explicit retrofitting models (Glavaˇs and Vuli´c, 2018; Glavaˇs and Vuli´c, 2019), a (deep, non-linear) specialization function is directly learned from external constraints. Post-specialization models (Vuli´c et al., 2018; Ponti et al., 2018; Kamath et al., 2019; Biesialska et al., 2020), instead, propagate lexico-semantic information to unseen words by imitating the transformation undergone by seen words during the initial specialization. This family of models can also transfer specialization across languages (Glavaˇs and Vuli´c, 2018; Ponti et al., 2019; Zhang et al., 2020). The goal of this work is to move beyond similarity-based specialization of static word embeddings only. We present a novel methodology for enriching unsupervised pretraining models such as BERT (Devlin et al., 2019) with readily available discrete lexico-semantic knowledge, and measure the benefits of such semantic specialization on similarity-oriented downstream applications. 1372 2.2 Injecting Knowledge into Unsupervised Pretraining Models Unsupervised pretraining models do retain some of the limitations of static word embeddings. First, they still conflate separate lexico-semantic relatio"
2020.coling-main.416,P19-1509,0,0.0234685,"al aspect of language (Clark, 1996) can be captured by artificial multi-agent games (Kirby, 2002; Mordatch and Abbeel, 2018), in which agents have to communicate about some shared input space (e.g., images). A common emergent communication protocol has been adopted in a large body of recent research: a speaker encodes a piece of information into a sequence of discrete symbols (emergent language) and a listener then aims to decipher the sequence and recover the original piece of information (Lazaridou et al., 2017; Havrylov and Titov, 2017; Lazaridou et al., 2018; Bouchacourt and Baroni, 2018; Chaabouni et al., 2019; Li and Bowling, 2019; Chaabouni et al., 2020; Luna et al., 2020; Kharitonov and Baroni, 2020, inter alia). The present work is partly inspired by the work of Lee et al. (2018), who train agents to communicate about images with their natural language captions and use their parameters as encoder-decoders for machine translation. However, this framework relies on the availability of natural language captions (whereas we use only artificial languages emerging from raw images). Moreover, it does not cast EC as pretraining followed by NMT few-shot fine-tuning; rather, it learns a model in a single"
2020.coling-main.416,2020.acl-main.407,0,0.0838667,"tured by artificial multi-agent games (Kirby, 2002; Mordatch and Abbeel, 2018), in which agents have to communicate about some shared input space (e.g., images). A common emergent communication protocol has been adopted in a large body of recent research: a speaker encodes a piece of information into a sequence of discrete symbols (emergent language) and a listener then aims to decipher the sequence and recover the original piece of information (Lazaridou et al., 2017; Havrylov and Titov, 2017; Lazaridou et al., 2018; Bouchacourt and Baroni, 2018; Chaabouni et al., 2019; Li and Bowling, 2019; Chaabouni et al., 2020; Luna et al., 2020; Kharitonov and Baroni, 2020, inter alia). The present work is partly inspired by the work of Lee et al. (2018), who train agents to communicate about images with their natural language captions and use their parameters as encoder-decoders for machine translation. However, this framework relies on the availability of natural language captions (whereas we use only artificial languages emerging from raw images). Moreover, it does not cast EC as pretraining followed by NMT few-shot fine-tuning; rather, it learns a model in a single stage. These differences make our approach no"
2020.coling-main.416,2020.acl-main.747,0,0.0845909,"Missing"
2020.coling-main.416,N19-1423,0,0.0219833,"ew-shot machine translation, and inductive biases for language. To all of these we cannot do full justice given space constraints. Pretraining for Transfer Learning. Unsupervised pretraining on large collections of unlabelled text yields general-purpose contextualized word representations (Peters et al., 2018; Howard and Ruder, 2018) 4717 that are beneficial across a range of downstream NLP tasks. The current dominant paradigm is training a Transformer-based deep model (Vaswani et al., 2017) relying on masked language modeling or a similar objective, as proposed in the omnipresent BERT model (Devlin et al., 2019) and its extensions (Liu et al., 2019; Conneau and Lample, 2019; Song et al., 2019; Joshi et al., 2020), and then fine-tuning the model further on a downstream task (Wang et al., 2019). Often this approach exploits large textual data and deep models spanning even billions of parameters (Conneau et al., 2020; Raffel et al., 2019; Brown et al., 2020). In this work, we refrain from chasing task leaderboards (Linzen, 2020) and posit a fundamental question about language learning instead. Emergent Communication. The functional aspect of language (Clark, 1996) can be captured by artificial multi-age"
2020.coling-main.416,2020.acl-main.143,0,0.106931,"an inform models of language. Inductive Biases for Language. Finally, a series of recent works has investigated how to construct neural models that are inductively biased towards learning new natural languages. This endeavour is motivated both by the need of sample efficiency and concerns of cognitive realism, as children can acquire language from limited stimuli (Chomsky, 1978). In particular, neural weights reflecting linguistic universals in phonotactics can be learned via approximate Bayesian inference (Ponti et al., 2019b) or meta-learning (McCoy et al., 2020). Papadimitriou and Jurafsky (2020) found that recurrent models pretrained on non-linguistic data with latent structure (such as music or code) facilitate natural language tasks. To our knowledge, we are the first to propose grounded communication as a non-linguistic source for 4718 pretraining, based on the hypothesis that modal and functional knowledge is a crucial inductive bias for fast and effective language acquisition. 3 Model Architecture The proposed method comprises the standard two stages of transfer learning. First, as detailed in § 3.1, we pretrain two speaker-listener agents via emergent communication on image ref"
2020.coling-main.416,P15-2139,0,0.0284187,"follow a simple architecture from prior work (Houlsby et al., 2019), and comprise linear layers with residual connections and dropout, as illustrated in Figure 1. 3.3 Regularisation with Annealing During fine-tuning, we also add to the objective an annealed regulariser for the encoder-decoder parameters (which, on the other hand, does not apply to the adapter module). These parameters are initialised using the parameters w? transferred from the EC agents. We can then define a regularisation term that prevents the parameters w from drifting away from their initialisation w? during fine-tuning (Duong et al., 2015): R = α kw − w? k2 (7) where α is a positive real-valued tunable hyper-parameter denoting the strength of the regularisation penalty. Note that this amounts to placing a prior N (w? , Iα−1 ) on the encoder-decoder parameters. However, the contribution of the log-prior in Eq. (7) to the posterior probability of the parameters should stay fixed, whereas the contribution of the negative log-likelihood in Eq. (6) should grow linearly with the number of examples. In other words, the likelihood should be able to overwhelm the prior in the limit of infinite data. For this reason, the importance of th"
2020.coling-main.416,I17-1014,0,0.0609495,"Missing"
2020.coling-main.416,W16-3210,0,0.0602061,"Missing"
2020.coling-main.416,D19-1384,0,0.104738,"raw images offers a favourable inductive bias for natural language tasks. In particular, we experiment with initialising an encoder-decoder model for few-shot neural machine translation with parameters pretrained on emergent communication. In the past, emergent communication has mostly attracted theoretical interest as a tool to shed light on cooperative behaviours, the compositional properties of emergent communication protocols (Lazaridou et al., 2017; Havrylov and Titov, 2017; Cao et al., 2018; Li and Bowling, 2019; Kaji´c et al., 2020), and natural language evolution (Kottur et al., 2017; Graesser et al., 2019). To our knowledge, this is the first preliminary study on deploying artificial languages from emergent communication in natural language applications. Conversely, our method also constitutes an extrinsic evaluation protocol to probe the properties of different emergent languages. The underlying assumption is that they should facilitate downstream tasks only to the extent that they share common characteristics with natural languages. In particular, we run in-depth analyses on the impact that the rate of communication success and maximum sequence length have on NMT performance. For the sake of"
2020.coling-main.416,N18-1032,0,0.020273,"idou et al., 2020) aims at enhancing emergent communication success by encouraging agents to imitate natural language data supplied at the beginning of training. Our work goes the opposite direction and investigates whether an emergent communication protocol pretrained without any human language data can benefit downstream NLP applications such as machine translation. Few-shot Neural Machine Translation. Our work addresses the problem of few-shot machine translation with limited parallel data. Differently from previous methods (Lample et al., 2018b; Lample et al., 2018a; Lample et al., 2018c; Gu et al., 2018a; Artetxe et al., 2018), our approach does not draw upon auxiliary language data for pretraining, which usually consists of machine translation tasks on other languages (Gu et al., 2018b) or domains (Sharaf et al., 2020), multilingual training (Aharoni et al., 2019; Liu et al., 2020), language model pretraining on monolingual data (Conneau and Lample, 2019; Siddhant et al., 2020), back-translation techniques on monolingual data (Platanios et al., 2018; Edunov et al., 2018), leveraging bilingual dictionaries (Duan et al., 2020), treebanks (Ponti et al., 2018), or image captions (Nakayama and N"
2020.coling-main.416,D18-1398,0,0.0253552,"idou et al., 2020) aims at enhancing emergent communication success by encouraging agents to imitate natural language data supplied at the beginning of training. Our work goes the opposite direction and investigates whether an emergent communication protocol pretrained without any human language data can benefit downstream NLP applications such as machine translation. Few-shot Neural Machine Translation. Our work addresses the problem of few-shot machine translation with limited parallel data. Differently from previous methods (Lample et al., 2018b; Lample et al., 2018a; Lample et al., 2018c; Gu et al., 2018a; Artetxe et al., 2018), our approach does not draw upon auxiliary language data for pretraining, which usually consists of machine translation tasks on other languages (Gu et al., 2018b) or domains (Sharaf et al., 2020), multilingual training (Aharoni et al., 2019; Liu et al., 2020), language model pretraining on monolingual data (Conneau and Lample, 2019; Siddhant et al., 2020), back-translation techniques on monolingual data (Platanios et al., 2018; Edunov et al., 2018), leveraging bilingual dictionaries (Duan et al., 2020), treebanks (Ponti et al., 2018), or image captions (Nakayama and N"
2020.coling-main.416,P18-1031,0,0.0573029,"Missing"
2020.coling-main.416,2020.tacl-1.5,0,0.0124059,"e given space constraints. Pretraining for Transfer Learning. Unsupervised pretraining on large collections of unlabelled text yields general-purpose contextualized word representations (Peters et al., 2018; Howard and Ruder, 2018) 4717 that are beneficial across a range of downstream NLP tasks. The current dominant paradigm is training a Transformer-based deep model (Vaswani et al., 2017) relying on masked language modeling or a similar objective, as proposed in the omnipresent BERT model (Devlin et al., 2019) and its extensions (Liu et al., 2019; Conneau and Lample, 2019; Song et al., 2019; Joshi et al., 2020), and then fine-tuning the model further on a downstream task (Wang et al., 2019). Often this approach exploits large textual data and deep models spanning even billions of parameters (Conneau et al., 2020; Raffel et al., 2019; Brown et al., 2020). In this work, we refrain from chasing task leaderboards (Linzen, 2020) and posit a fundamental question about language learning instead. Emergent Communication. The functional aspect of language (Clark, 1996) can be captured by artificial multi-agent games (Kirby, 2002; Mordatch and Abbeel, 2018), in which agents have to communicate about some share"
2020.coling-main.416,D14-1086,0,0.0310622,"neau et al., 2020) or few examples in a target resourcepoor language (Lauscher et al., 2020). However, even raw texts required for pretraining are scant (Kornai, 2013): for instance, Wikipedia dumps cover 278 languages out of the 7,097 spoken world-wide (Eberhard et al., 2020). For this reason, we push the idea of cross-lingual knowledge transfer even further, exploring and profiling a setting where not even raw natural language data for a target language are available for unsupervised pretraining. In their stead, we exploit artificial languages emerging from a referential game on raw images (Kazemzadeh et al., 2014; Lazaridou et al., 2017). In particular, we encourage agents to cooperate in identifying images among distractors by communicating over vocabularies whose meanings are unknown. The key intuition is that, whereas lexicalisation is mostly arbitrary (Saussure, 1916), communication grounded in a real-world environment does constrain what languages are likely or This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 4716 Proceedings of the 28th International Conference on Computational Linguistics, pages"
2020.coling-main.416,2020.blackboxnlp-1.2,0,0.0979359,"y, 2002; Mordatch and Abbeel, 2018), in which agents have to communicate about some shared input space (e.g., images). A common emergent communication protocol has been adopted in a large body of recent research: a speaker encodes a piece of information into a sequence of discrete symbols (emergent language) and a listener then aims to decipher the sequence and recover the original piece of information (Lazaridou et al., 2017; Havrylov and Titov, 2017; Lazaridou et al., 2018; Bouchacourt and Baroni, 2018; Chaabouni et al., 2019; Li and Bowling, 2019; Chaabouni et al., 2020; Luna et al., 2020; Kharitonov and Baroni, 2020, inter alia). The present work is partly inspired by the work of Lee et al. (2018), who train agents to communicate about images with their natural language captions and use their parameters as encoder-decoders for machine translation. However, this framework relies on the availability of natural language captions (whereas we use only artificial languages emerging from raw images). Moreover, it does not cast EC as pretraining followed by NMT few-shot fine-tuning; rather, it learns a model in a single stage. These differences make our approach not only applicable to truly resource-lean languag"
2020.coling-main.416,2005.mtsummit-papers.11,0,0.0920641,"dal MT, contains multilingual captions for ≈ 30k images. We discard images and run text-only fine-tuning and evaluation on English-German (EN - DE) and English-Czech (EN - CS) in both directions. We rely on the default training set of 29,000 pairs of parallel sentences, which we also subsample to simulate true few-shot scenarios: we randomly select 500, 1,000, and 10,000 sentence pairs for the lower-resource setups. In all experimental runs, we use the original validation set spanning 1,014 sentence pairs and the default test set spanning 1,000 pairs. We also run experiments on Europarl data (Koehn, 2005) from OPUS (Tiedemann, 2009) for two language pairs: English-Romanian (EN - RO) and English-French (EN - FR), again in both directions. We retain only sentences with a length between 5 and 15 words to construct data sets whose average sentence length is similar to that of Multi30k. We then randomly sample 10,000 parallel sentences as our (largest) training set, while two other disjoint random samples of 1,500 sentence pairs are used for validation and test, respectively. As with Multi30k, we again sample 500 and 1,000 training instances from the full set of 10k examples to simulate few-shot se"
2020.coling-main.416,D17-1321,0,0.0615366,"Missing"
2020.coling-main.416,D18-1549,0,0.0534502,"Missing"
2020.coling-main.416,2020.emnlp-main.363,1,0.828492,"Missing"
2020.coling-main.416,2020.acl-main.685,0,0.0680796,"ameters as encoder-decoders for machine translation. However, this framework relies on the availability of natural language captions (whereas we use only artificial languages emerging from raw images). Moreover, it does not cast EC as pretraining followed by NMT few-shot fine-tuning; rather, it learns a model in a single stage. These differences make our approach not only applicable to truly resource-lean languages but also substantially superior in performance on a same dataset such as English-German Multi30k (see § 5). Another strand of recent research (Lowe et al., 2019; Lowe et al., 2020; Lazaridou et al., 2020) aims at enhancing emergent communication success by encouraging agents to imitate natural language data supplied at the beginning of training. Our work goes the opposite direction and investigates whether an emergent communication protocol pretrained without any human language data can benefit downstream NLP applications such as machine translation. Few-shot Neural Machine Translation. Our work addresses the problem of few-shot machine translation with limited parallel data. Differently from previous methods (Lample et al., 2018b; Lample et al., 2018a; Lample et al., 2018c; Gu et al., 2018a;"
2020.coling-main.416,2020.acl-main.465,0,0.0186262,"m is training a Transformer-based deep model (Vaswani et al., 2017) relying on masked language modeling or a similar objective, as proposed in the omnipresent BERT model (Devlin et al., 2019) and its extensions (Liu et al., 2019; Conneau and Lample, 2019; Song et al., 2019; Joshi et al., 2020), and then fine-tuning the model further on a downstream task (Wang et al., 2019). Often this approach exploits large textual data and deep models spanning even billions of parameters (Conneau et al., 2020; Raffel et al., 2019; Brown et al., 2020). In this work, we refrain from chasing task leaderboards (Linzen, 2020) and posit a fundamental question about language learning instead. Emergent Communication. The functional aspect of language (Clark, 1996) can be captured by artificial multi-agent games (Kirby, 2002; Mordatch and Abbeel, 2018), in which agents have to communicate about some shared input space (e.g., images). A common emergent communication protocol has been adopted in a large body of recent research: a speaker encodes a piece of information into a sequence of discrete symbols (emergent language) and a listener then aims to decipher the sequence and recover the original piece of information (L"
2020.coling-main.416,2020.tacl-1.47,0,0.0224364,"uman language data can benefit downstream NLP applications such as machine translation. Few-shot Neural Machine Translation. Our work addresses the problem of few-shot machine translation with limited parallel data. Differently from previous methods (Lample et al., 2018b; Lample et al., 2018a; Lample et al., 2018c; Gu et al., 2018a; Artetxe et al., 2018), our approach does not draw upon auxiliary language data for pretraining, which usually consists of machine translation tasks on other languages (Gu et al., 2018b) or domains (Sharaf et al., 2020), multilingual training (Aharoni et al., 2019; Liu et al., 2020), language model pretraining on monolingual data (Conneau and Lample, 2019; Siddhant et al., 2020), back-translation techniques on monolingual data (Platanios et al., 2018; Edunov et al., 2018), leveraging bilingual dictionaries (Duan et al., 2020), treebanks (Ponti et al., 2018), or image captions (Nakayama and Nishida, 2017; Elliott and K´ad´ar, 2017; Lee et al., 2018). On the contrary, we ground our neural model on visual knowledge acquired from agent interactions without any observation of human language, and then fine-tune our model on translation tasks even with as few as 500 to 1, 000 t"
2020.coling-main.416,2020.findings-emnlp.397,1,0.769368,"i-agent games (Kirby, 2002; Mordatch and Abbeel, 2018), in which agents have to communicate about some shared input space (e.g., images). A common emergent communication protocol has been adopted in a large body of recent research: a speaker encodes a piece of information into a sequence of discrete symbols (emergent language) and a listener then aims to decipher the sequence and recover the original piece of information (Lazaridou et al., 2017; Havrylov and Titov, 2017; Lazaridou et al., 2018; Bouchacourt and Baroni, 2018; Chaabouni et al., 2019; Li and Bowling, 2019; Chaabouni et al., 2020; Luna et al., 2020; Kharitonov and Baroni, 2020, inter alia). The present work is partly inspired by the work of Lee et al. (2018), who train agents to communicate about images with their natural language captions and use their parameters as encoder-decoders for machine translation. However, this framework relies on the availability of natural language captions (whereas we use only artificial languages emerging from raw images). Moreover, it does not cast EC as pretraining followed by NMT few-shot fine-tuning; rather, it learns a model in a single stage. These differences make our approach not only applicable t"
2020.coling-main.416,N18-1202,0,0.0826799,"Missing"
2020.coling-main.416,2020.emnlp-demos.7,1,0.810541,"Missing"
2020.coling-main.416,2020.emnlp-main.617,1,0.869992,"Missing"
2020.coling-main.416,D18-1039,0,0.0218539,"t machine translation with limited parallel data. Differently from previous methods (Lample et al., 2018b; Lample et al., 2018a; Lample et al., 2018c; Gu et al., 2018a; Artetxe et al., 2018), our approach does not draw upon auxiliary language data for pretraining, which usually consists of machine translation tasks on other languages (Gu et al., 2018b) or domains (Sharaf et al., 2020), multilingual training (Aharoni et al., 2019; Liu et al., 2020), language model pretraining on monolingual data (Conneau and Lample, 2019; Siddhant et al., 2020), back-translation techniques on monolingual data (Platanios et al., 2018; Edunov et al., 2018), leveraging bilingual dictionaries (Duan et al., 2020), treebanks (Ponti et al., 2018), or image captions (Nakayama and Nishida, 2017; Elliott and K´ad´ar, 2017; Lee et al., 2018). On the contrary, we ground our neural model on visual knowledge acquired from agent interactions without any observation of human language, and then fine-tune our model on translation tasks even with as few as 500 to 1, 000 training instances. We rely on few-shot MT as a standard, well-known, and sound testbed to empirically validate the crucial question of this work, that is, whether emergent"
2020.coling-main.416,P18-1142,1,0.88487,"Missing"
2020.coling-main.416,J19-3005,1,0.856711,"Missing"
2020.coling-main.416,D19-1288,1,0.864033,"Missing"
2020.coling-main.416,W18-6319,0,0.0148031,"MT model is the standard seq2seq model whose architecture is exactly the same as our proposed model, but now with randomly initialised parameters (rather than transferred from EC). We extensively search the hyper-parameter space of the baseline model (Sennrich and Zhang, 2019) and adopt Adam optimiser with learning rate of 0.001, β1 = 0.9, β2 = 0.999,  = 1e-08, a dropout rate of 0.2, a batch size of 128, a hidden-state size of 512, an embedding size of 256, and a max sequence length of 80. For all models, we rely on beam search with beam size 12 for decoding. The evaluation metric is BLEU-4 (Post, 2018). 5 Results and Analysis In what follows, we report the NMT results of our proposed model on all language pairs. We then perform an ablation study highlighting the individual contributions—of the customised adapter layer, the strategies for annealing the regulariser, and emergent communication pretraining—to the final results. Finally, we assess the impact of the rate of communication success and maximum sequence length on downstream NMT performances. Main Results. The BLEU scores of the model leveraging both EC pretraining and adapters are shown in Table 1 for the Multi30k dataset, and in Tab"
2020.coling-main.416,P19-1021,0,0.0201136,"on the scores on the EN - DE validation set (in the 1k training setup) and fixed to those values in all other experiments and for all other language pairs. For a fair comparison, the other hyper-parameters for fine-tuning are set identically to the NMT baseline introduced in the next paragraph. NMT Baseline and Evaluation Details. The baseline NMT model is the standard seq2seq model whose architecture is exactly the same as our proposed model, but now with randomly initialised parameters (rather than transferred from EC). We extensively search the hyper-parameter space of the baseline model (Sennrich and Zhang, 2019) and adopt Adam optimiser with learning rate of 0.001, β1 = 0.9, β2 = 0.999,  = 1e-08, a dropout rate of 0.2, a batch size of 128, a hidden-state size of 512, an embedding size of 256, and a max sequence length of 80. For all models, we rely on beam search with beam size 12 for decoding. The evaluation metric is BLEU-4 (Post, 2018). 5 Results and Analysis In what follows, we report the NMT results of our proposed model on all language pairs. We then perform an ablation study highlighting the individual contributions—of the customised adapter layer, the strategies for annealing the regulariser"
2020.coling-main.416,P16-1162,0,0.028224,"(EN - FR), again in both directions. We retain only sentences with a length between 5 and 15 words to construct data sets whose average sentence length is similar to that of Multi30k. We then randomly sample 10,000 parallel sentences as our (largest) training set, while two other disjoint random samples of 1,500 sentence pairs are used for validation and test, respectively. As with Multi30k, we again sample 500 and 1,000 training instances from the full set of 10k examples to simulate few-shot settings. For each language pair, we lowercase and tokenise the data using byte-pair encoding (BPE) (Sennrich et al., 2016). Our BPE vocabularies are derived from all 29,000 training pairs (for the Multi30k language pairs) and 10,000 training pairs (for the Europarl language pairs). We again use Adam in the same configuration as EC pretraining, except for setting the dropout rate to 0.2. The hyper-parameters of the annealed regulariser are set to α = 5 and λ = 0.998 based on the scores on the EN - DE validation set (in the 1k training setup) and fixed to those values in all other experiments and for all other language pairs. For a fair comparison, the other hyper-parameters for fine-tuning are set identically to t"
2020.coling-main.416,2020.ngt-1.5,0,0.0482746,"Missing"
2020.coling-main.416,D19-1077,0,0.0203283,"s and extrinsic evaluation of artificial languages. 1 Introduction Zero-shot and few-shot learning are notoriously challenging for neural networks (Bottou and Bousquet, 2008; Vinyals et al., 2016; Ravi and Larochelle, 2017). However, they are a prerequisite for natural language processing in most languages, which suffer from the paucity of annotated data (Ponti et al., 2019a). State-of-the-art models rely on knowledge transfer, whereby an encoder is pretrained via language modeling on texts from multiple languages, and subsequently ‘fine-tuned’ on labelled examples of resource-rich languages (Wu and Dredze, 2019; Conneau et al., 2020) or few examples in a target resourcepoor language (Lauscher et al., 2020). However, even raw texts required for pretraining are scant (Kornai, 2013): for instance, Wikipedia dumps cover 278 languages out of the 7,097 spoken world-wide (Eberhard et al., 2020). For this reason, we push the idea of cross-lingual knowledge transfer even further, exploring and profiling a setting where not even raw natural language data for a target language are available for unsupervised pretraining. In their stead, we exploit artificial languages emerging from a referential game on raw ima"
2020.coling-main.416,D19-1143,0,0.0264866,"r. A second MLP2 is used by 1 Note that we use each listener module as an MT encoder and each speaker module as a decoder. In addition, we train two separate agents because vocabulary sizes of SRC and TRG languages are different and we adopt disjoint input embeddings. 2 We will experiment with Transformer-based architectures (Vaswani et al., 2017) in future work. Our choice of GRU is also partially motivated by recent results in few-shot MT showing on-par or even slightly stronger performance of recurrent networks over Transformers when only a small number of parallel sentences are available (Zhou et al., 2019). 3 Another common approach is based on reinforcement learning, but recent work suggests that it is less effective and converges more slowly than Gumbel-Softmax for EC tasks (Havrylov and Titov, 2017; Lee et al., 2018). 4719 Speaker to project each GRU hidden state—one for each time step—into vectors with dimensionality equal to the predefined vocabulary size of the emergent language. Image Inference. Given the input image, the generated message describing the image, and K confounding images, Listeners must now guess the correct input image among the distractors. To do so, a second GRU layer d"
2020.coling-main.423,N09-1003,0,0.0866242,"ings of similarity scores computed between word embeddings produced by representation models are compared against ranked human similarity judgments. The dataset design involving sets of word pairs and their associated rating on a discrete scale has been particularly common, due to its reliance on non-expert native speaker judgments, quicker and cheaper to obtain than the large expert-curated lexical-semantic or semanticsyntactic resources such as WordNet (Fellbaum, 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datas"
2020.coling-main.423,2020.emnlp-main.618,0,0.013873,"ining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of this work is to address this deficit by releasing a multiling"
2020.coling-main.423,D14-1034,1,0.80299,"er languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datasets are harder to come by. In English, these include datasets of Yang and Powers (2006) (130 verb pairs), Baker et al. (2014) (143 verb pairs), Gerz et al. (2016) (3,500 verb pairs). A recent multilingual word similarity dataset, Multi-SimLex (Vuli´c et al., 2020), extends coverage of verb semantic similarity to 469 verb pairs in 12 languages, including Mandarin Chinese, Finnish, and Polish. Another recently introduced large-scale English verb resource of Majewska et al. (2020) (hereafter SpA-Verb) comprises verb classes and unmatched coverage of nearly 30k verb similarity scores. In this work, we demonstrate that their large-scale dataset creation methodology based on spatial arrangement (SpAM) can be extended to o"
2020.coling-main.423,Q17-1010,0,0.0123746,"nge of direction). Whereas in Italian and English, verbs describing motion towards the speaker/listener form a distinct cluster. These preliminary analyses suggest that the collected semantic multi-arrangement data may support many other, fine-grained and in-depth lexical-typological analyses in future work, e.g., focusing on cross-lingual comparisons of the organisation of different semantic fields and examination of the most salient meaning dimensions underlying a given conceptual space. 4 Evaluation Evaluation is focused on two types of representation architectures: static word embeddings (Bojanowski et al., 2017) and more recently proposed large pretrained encoders (Devlin et al., 2019). We compare their ability to capture word-level semantics across languages and domains of verb meaning. We also contrast the performance of language-specific BERT models with their massively multilingual counterpart (Devlin et al., 2019), and examine the impact of computing word-level representations in context, rather than by feeding items to a pretrained model in isolation. Representation Models. We evaluate FAST T EXT (FT) as a representative non-contextualised word embedding model with proven representation capabil"
2020.coling-main.423,N19-1423,0,0.404351,"he performance of large language-specific pretraining models with their multilingual equivalent on semantic clustering and lexical similarity, across different domains of verb meaning. We release the data from both phases as a large-scale multilingual resource, comprising 85 verb classes and nearly 130k pairwise similarity scores, offering a wealth of possibilities for further evaluation and research on multilingual verb semantics. 1 Introduction Many recent efforts in semantic modeling have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of lan"
2020.coling-main.423,C18-1323,0,0.017561,"or semanticsyntactic resources such as WordNet (Fellbaum, 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datasets are harder to come by. In English, these include datasets of Yang and Powers (2006) (130 verb pairs), Baker et al. (2014) (143 verb pairs), Gerz et al. (2016) (3,500 verb pairs). A recent multilingual word similarity dataset, Multi-SimLex (Vuli´c et al., 2020), extends coverage of verb semantic similarity to 469 verb pairs in 12 languages, including Mandarin Chinese, Finnish, and Polish. Another recently int"
2020.coling-main.423,D16-1235,1,0.79979,"Missing"
2020.coling-main.423,J15-4004,1,0.908022,"uced by representation models are compared against ranked human similarity judgments. The dataset design involving sets of word pairs and their associated rating on a discrete scale has been particularly common, due to its reliance on non-expert native speaker judgments, quicker and cheaper to obtain than the large expert-curated lexical-semantic or semanticsyntactic resources such as WordNet (Fellbaum, 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datasets are harder to come by. In English, these include datasets"
2020.coling-main.423,S13-2049,0,0.0308016,"Missing"
2020.coling-main.423,kipper-etal-2006-extending,1,0.609614,"n Word similarity has been widely used as a go-to intrinsic evaluation task, in which rankings of similarity scores computed between word embeddings produced by representation models are compared against ranked human similarity judgments. The dataset design involving sets of word pairs and their associated rating on a discrete scale has been particularly common, due to its reliance on non-expert native speaker judgments, quicker and cheaper to obtain than the large expert-curated lexical-semantic or semanticsyntactic resources such as WordNet (Fellbaum, 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these d"
2020.coling-main.423,D19-1279,0,0.0158964,"erb semantics. 1 Introduction Many recent efforts in semantic modeling have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned represen"
2020.coling-main.423,J99-4009,0,0.0289894,"which optimises the evidence collected for the dissimilarity estimates (see Figure 1). The final representational dissimilarity matrix (RDM) estimate is produced by statistically combining the evidence from multiple subsequent 2D arrangements and contains a dissimilarity estimate for each pairing of words in the set (see Kriegeskorte and Mur (2012) for the details). The dissimilarities collected for each Phase 1 class are then normalised to ensure inter-class consistency in the final dataset. The main advantages of the spatial arrangement method lie in its intuitiveness, rooted in psychology (Lakoff and Johnson, 1999; Gärdenfors, 2004; Casasanto, 2008), and flexibility, due to the reliance on fluid item placements simultaneously expressing multi-way similarity judgments, rather than discrete numerical scores. By repeatedly considering subsets of items, the users reflect on relative differences in meaning between different configurations of words, which decreases bias from placement error, order of presentation and judgment context. The two-phase design offers a practical advantage for porting the method to other languages. The approach starts from a verb sample, rather than a set of word pairs, which allo"
2020.coling-main.423,K19-1004,1,0.892956,"Missing"
2020.coling-main.423,2020.lrec-1.705,1,0.859868,"Missing"
2020.coling-main.423,L18-1008,0,0.0142817,"large pretrained encoders (Devlin et al., 2019). We compare their ability to capture word-level semantics across languages and domains of verb meaning. We also contrast the performance of language-specific BERT models with their massively multilingual counterpart (Devlin et al., 2019), and examine the impact of computing word-level representations in context, rather than by feeding items to a pretrained model in isolation. Representation Models. We evaluate FAST T EXT (FT) as a representative non-contextualised word embedding model with proven representation capabilities on diverse NLP tasks (Mikolov et al., 2018) and coverage of 157 languages. For multi-word expressions, we compute their representations by averaging the vectors of their constituent words. We contrast the performance of FT vectors with the omnipresent state-of-the-art BERT model (Devlin et al., 2019). We derive word-level BERT representations of words and multi-word expressions in two different ways: (a) in isolation and (b) in context. In method (a), we follow the steps of Liu et al. (2019) by (1) feeding each item to the pretrained model in isolation, (2) averaging 3 The easier, higher-IAA classes tend to include verbs whose meanings"
2020.coling-main.423,Q17-1022,1,0.893661,"Missing"
2020.coling-main.423,2020.acl-main.720,0,0.011818,"recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of this work is to address this deficit by releasing a multilingual resource targeting verb semantics in"
2020.coling-main.423,P19-1493,0,0.0239074,"s in semantic modeling have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of t"
2020.coling-main.423,2020.tacl-1.54,0,0.012083,"ular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of this work is to address this deficit by releasing a multilingual resource targeting verb semantics in a typologically diverse selection of languages where no such datasets have hitherto been available. The motivation behind the specific focus on verbs is twofold: (i) the importance of accurate and nuanced representation of verb meaning in light of their pivotal role in sentence structure and the still subpar verbal reasoning ability of SOTA models (Rogers et al., 2020), and (ii) the scarcity of verb data in evaluation datasets currently available. To this end, we employ a recently proposed two-phase data collection method (Majewska et al., 2020) combining semantic clustering (Phase 1) and finer-grained spatial arrangements of words based on their similarity (Phase 2), and evaluate its cross-lingual applicability. Using cross-lingual mappings, This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 4810 Proceedings of the 28th International Conference on Computationa"
2020.coling-main.423,L18-1152,0,0.0326317,"(Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datasets are harder to come by. In English, these include datasets of Yang and Powers (2006) (130 verb pairs), Baker et al. (2014) (143 verb pairs), Gerz et al. (2016) (3,500 verb pairs). A recent multilingual word similarity dataset, Multi-SimLex (Vuli´c et al., 2020), extends coverage of verb semantic similarity to 469 verb pairs in 12 languages, including Mandarin Chinese, Finnish, and Polish. Another recently introduced large-scale English verb resource of Majewska et al. (2020) (hereafter"
2020.coling-main.423,C10-1119,1,0.808447,"ased for ZH, JA (BERT- BASE with and without whole word masking (+WWM)), PL, FI, and IT (BERT- BASE and BERT- BASE - XXL trained on a larger Italian corpus), available in the Transformers repository (Wolf et al., 2019).4 4.1 Semantic Verb Clustering First, we evaluate the models on semantic clustering, where the task is to group the starting verb sample (Table 1, N verbs) into clusters based on semantic similarity. For each vector collection, we apply the spectral clustering algorithm (Meila and Shi, 2001; Yu and Shi, 2003), shown to produce strong results in previous work on verb clustering (Sun et al., 2010; Scarton et al., 2014; Vuli´c et al., 2017), and evaluate the produced groupings against the Phase 1 classes in each language using standard clustering evaluation metrics, modified purity (M P UR) (i.e., mean precision of induced verb clusters) and weighted class accuracy (WACC), calculated as follows: P M P UR = C∈Clust,nprev(C) >1 ntest_verbs nprev(C) P (1) WACC = ndom(C) ntest_verbs C∈Gold (2) where (1) each cluster C from the set of all KClust automatically induced clusters Clust is associated with its prevalent Phase 1 class, and nprev(C) is the number of verbs in an induced cluster C ap"
2020.coling-main.423,D17-1270,1,0.90543,"Missing"
2020.coling-main.423,2020.cl-4.5,1,0.881421,"Missing"
2020.coling-main.423,D19-1575,0,0.0122398,"Many recent efforts in semantic modeling have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most lan"
2020.coling-main.423,D19-1077,0,0.0176768,"ng have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of this work is to addres"
2020.emnlp-main.185,2020.acl-main.747,0,0.168246,"Missing"
2020.emnlp-main.185,D18-1269,0,0.379567,"hkin et al., 2018; Sap et al., 2019, inter alia). Unfortunately, the extensive efforts related to this thread of research have so far been limited only to the English language.2 Such a narrow scope not only curbs the development of natural language understanding tools in other languages (Bender, 2011; Ponti et al., 2019a), but also exacerbates the Anglocentric bias in modeling commonsense reasoning. In fact, the expectations about typical situations do vary across cultures (Thomas, 1983). Datasets that cover multiple languages for other natural understanding tasks, such as language inference (Conneau et al., 2018), question answering (Lewis et al., 2020; Artetxe et al., 2020a; Clark et al., 2020), and paraphrase identification (Yang et al., 2019b) have received increasing attention. In fact, the requirement to generalise to new languages encourages the development of more versatile language understanding models, which can be ported across different grammars and lexica. These efforts have recently culminated in the integration of several multilingual tasks into the XTREME evaluation suite (Hu et al., 2020). However, a compre1 Moreover, there are often multiple legitimate chains of sentences that can be"
2020.emnlp-main.185,W17-1504,0,0.0993792,"rted across different grammars and lexica. These efforts have recently culminated in the integration of several multilingual tasks into the XTREME evaluation suite (Hu et al., 2020). However, a compre1 Moreover, there are often multiple legitimate chains of sentences that can be invoked in between premises and hypotheses. In short, commonsense reasoning does not just involve understanding what is possible, but also ranking what is most plausible. 2 The only exception is direct translation of the 272 paired English Winograd Schema Challenge instances to Japanese (Shibata et al., 2015), French (Amsili and Seminck, 2017), and Portuguese (Melo et al., 2020). 2362 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2362–2376, c November 16–20, 2020. 2020 Association for Computational Linguistics PREMISE qu en th en Sipasqa cereal mikhunanpi kuruta tarirqan. The girl found a bug in her cereal. ตาของฉันแดงและบวม My eyes became red and puffy. R C CHOICE 1 Payqa pukunman n˜ uq˜nuta churakurqan. She poured milk in the bowl. ฉันรองไห I was sobbing. CHOICE 2 Payqa manam mikhuyta munarqanchu. She lost her appetite. ฉันหัวเราะ I was laughing. Table 1: Examples of forward (Resu"
2020.emnlp-main.185,2020.emnlp-main.618,0,0.023636,"Missing"
2020.emnlp-main.185,N19-1423,0,0.197445,"ce in digital texts. Since resource-rich languages tend to belong to a few families and areas, samples inspired by this criterion are highly biased and not indicative of true models’ performance (Gerz et al., 2018; Ponti et al., 2019a; Joshi et al., 2020; Lauscher et al., 2020). Following this guiding principle, we select 11 languages from 11 distinct families, and 5 geographical macro-areas (Africa, Eurasia, Papunesia, North America, and South America). We leverage XCOPA to benchmark a series of state-of-the-art pretrained multilingual models, including XLM - R (Conneau et al., 2020), MBERT (Devlin et al., 2019), and multilingual USE (Yang et al., 2019a). Two XCOPA languages (i.e., Southern Quechua and Haitian Creole) are out-of-sample for the pretrained models: this naturally raises the question of how to adapt the pretrained models to such unseen languages. In particular, we investigate the resource-lean scenarios where either some monolingual data or a bilingual dictionary with English (or both) are available for the target language. In summary, we offer the following contributions. 1) We create the first large-scale multilingual evaluation set for commonsense reasoning, spanning 11 languages, and"
2020.emnlp-main.185,2020.acl-main.421,0,0.58447,", the extensive efforts related to this thread of research have so far been limited only to the English language.2 Such a narrow scope not only curbs the development of natural language understanding tools in other languages (Bender, 2011; Ponti et al., 2019a), but also exacerbates the Anglocentric bias in modeling commonsense reasoning. In fact, the expectations about typical situations do vary across cultures (Thomas, 1983). Datasets that cover multiple languages for other natural understanding tasks, such as language inference (Conneau et al., 2018), question answering (Lewis et al., 2020; Artetxe et al., 2020a; Clark et al., 2020), and paraphrase identification (Yang et al., 2019b) have received increasing attention. In fact, the requirement to generalise to new languages encourages the development of more versatile language understanding models, which can be ported across different grammars and lexica. These efforts have recently culminated in the integration of several multilingual tasks into the XTREME evaluation suite (Hu et al., 2020). However, a compre1 Moreover, there are often multiple legitimate chains of sentences that can be invoked in between premises and hypotheses. In short, commonse"
2020.emnlp-main.185,P18-1231,0,0.0284989,"ltiple-choice problems where the most sensible option is chosen. Another line of evaluation involves commonsense-enabled reading comprehension and question answering (Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). Multilingual Evaluation of Natural Language Understanding. While the above commonsense reasoning datasets are limited to English, several multilingual datasets for other natural language understanding tasks are available, e.g., lexical semantic similarity (Multi-SimLex; Vuli´c et al., 2020), document classification (MLDoc; Schwenk and Li, 2018), sentiment analysis (Barnes et al., 2018), and natural language inference (XNLI; Conneau et al., 2018). Other recent multilingual sets target the QA task based on reading comprehenseen scripts (e.g., both HT and QU are written in Latin script). 13 Abductive reasoning is inference to the most plausible explanation of incomplete observations (Peirce, 1960). sion: MLQA (Lewis et al., 2020) in 7 languages, XQuAD (Artetxe et al., 2020b) in 10; and TyDiQA (Clark et al., 2020) in 11 typologically diverse languages. Further, PAWS-X (Yang et al., 2019b) evaluates paraphrase identification in 6 languages. A standard and pragmatic approach to m"
2020.emnlp-main.185,2020.acl-main.653,0,0.124033,"Missing"
2020.emnlp-main.185,D19-1243,0,0.0381807,"Missing"
2020.emnlp-main.185,E17-2002,0,0.162717,"Missing"
2020.emnlp-main.185,2020.acl-main.560,0,0.0659782,"lar is still missing. In order to address this gap, we develop a novel dataset, XCOPA (see examples in Table 1), by carefully translating and re-annotating the validation and test sets of English COPA into 11 target languages. A key design choice is the selection of a typologically diverse sample of languages. In particular, we privilege variety over the abundance in digital texts. Since resource-rich languages tend to belong to a few families and areas, samples inspired by this criterion are highly biased and not indicative of true models’ performance (Gerz et al., 2018; Ponti et al., 2019a; Joshi et al., 2020; Lauscher et al., 2020). Following this guiding principle, we select 11 languages from 11 distinct families, and 5 geographical macro-areas (Africa, Eurasia, Papunesia, North America, and South America). We leverage XCOPA to benchmark a series of state-of-the-art pretrained multilingual models, including XLM - R (Conneau et al., 2020), MBERT (Devlin et al., 2019), and multilingual USE (Yang et al., 2019a). Two XCOPA languages (i.e., Southern Quechua and Haitian Creole) are out-of-sample for the pretrained models: this naturally raises the question of how to adapt the pretrained models to such"
2020.emnlp-main.185,kamholz-etal-2014-panlex,0,0.029351,"HT and QU by concatenating their respective Wikipedia dumps with their respective text from the JW300 corpus (Agi´c and Vuli´c, 2019). In total, the training size is 5,710,426 tokens for HT, and 2,263,134 tokens for QU. 2) S. Sentences in English (EN). This could prevent (catastrophic) forgetting of the source language while fine-tuning, which presumably may occur with T only. We create the English corpus of comparable size to HT and QU corpora by randomly sampling 200K sentences from EN Wikipedia. 3) D. A bilingual EN – HT and EN – QU dictionary. The dictionaries were extracted from PanLex (Kamholz et al., 2014): we retain the 5k most reliable word translation pairs according to the available PanLex confidence scores. We create a synthetic corpus from the dictionary (termed D-corpus henceforth) by concatenating each translation pair from the dictionary into a quasi-sentence. 4) T-REP. T data with all occurrences of target language terms from the 5K dictionary replaced with their English translations. HT QU CO-ZS Even massively multilingual encoders like MBERT and XLM-R, pretrained on corpora of over 100 languages, cover only a fraction of the world’s 7,000+ languages. In fact, the majority of the wor"
2020.emnlp-main.185,W15-2137,0,0.0276317,"are hidden (Niven and Kao, 2019). Finally, in §4.4 we explore several strategies to adapt massively multilingual models to new languages not observed during pretraining, such as Quechua and Haitian Creole. 4.1 Baselines We evaluate baselines in several combinations of experimental setups based on: 1) different methods for cross-lingual transfer, either based on model transfer or machine translation; 2) different multilingual pretrained encoders; 3) different sources of training and validation data. Cross-lingual Transfer Methods. We consider two high-level methods for cross-lingual transfer (Tiedemann, 2015; Ponti et al., 2019a): 1) multilingual model transfer (MuMoTr), whereby a Transformer-based encoder is pretrained on multiple languages in an unsupervised fashion, and subsequently trained on English annotated data for multiple-choice classification, therefore enabling zero-shot generalisation to the other languages. 2) translate test (TrTe), whereby target test data7 are translated into English via Google Translate. This includes all languages except for QU, for which the service is not available. Multilingual Encoders. For model transfer, we evaluate the following state-of-the-art pretraine"
2020.emnlp-main.185,2020.cl-4.5,1,0.86898,"Missing"
2020.emnlp-main.185,N18-1101,0,0.0568428,"written in Latin script). 13 Abductive reasoning is inference to the most plausible explanation of incomplete observations (Peirce, 1960). sion: MLQA (Lewis et al., 2020) in 7 languages, XQuAD (Artetxe et al., 2020b) in 10; and TyDiQA (Clark et al., 2020) in 11 typologically diverse languages. Further, PAWS-X (Yang et al., 2019b) evaluates paraphrase identification in 6 languages. A standard and pragmatic approach to multilingual dataset creation is translation from an existing (English) dataset, e.g., Multi-SimLex from the extended English SimLex-999 (Hill et al., 2015), XNLI from MultiNLI (Williams et al., 2018), XQuAD from SQuAD (Rajpurkar et al., 2016), and PAWS-X from PAWS (Zhang et al., 2019). TyDiQA, however, was built independently in each language. Finally, a large number of tasks has been recently integrated into unified multilingual evaluation suites, XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020). 6 Conclusion and Future Work We presented the Cross-lingual Choice of Plausible Alternatives (XCOPA), a multilingual evaluation benchmark for causal commonsense reasoning. All XCOPA instances are aligned across 11 languages, which enables cross-lingual comparisons. The language selection"
2020.emnlp-main.185,2020.acl-demos.12,0,0.139316,"Missing"
2020.emnlp-main.185,D19-1382,0,0.392467,"limited only to the English language.2 Such a narrow scope not only curbs the development of natural language understanding tools in other languages (Bender, 2011; Ponti et al., 2019a), but also exacerbates the Anglocentric bias in modeling commonsense reasoning. In fact, the expectations about typical situations do vary across cultures (Thomas, 1983). Datasets that cover multiple languages for other natural understanding tasks, such as language inference (Conneau et al., 2018), question answering (Lewis et al., 2020; Artetxe et al., 2020a; Clark et al., 2020), and paraphrase identification (Yang et al., 2019b) have received increasing attention. In fact, the requirement to generalise to new languages encourages the development of more versatile language understanding models, which can be ported across different grammars and lexica. These efforts have recently culminated in the integration of several multilingual tasks into the XTREME evaluation suite (Hu et al., 2020). However, a compre1 Moreover, there are often multiple legitimate chains of sentences that can be invoked in between premises and hypotheses. In short, commonsense reasoning does not just involve understanding what is possible, but"
2020.emnlp-main.185,D19-1454,0,0.0645931,"Missing"
2020.emnlp-main.185,L18-1560,0,0.0311419,"ers et al., 2019b) are cast as openended multiple-choice problems where the most sensible option is chosen. Another line of evaluation involves commonsense-enabled reading comprehension and question answering (Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). Multilingual Evaluation of Natural Language Understanding. While the above commonsense reasoning datasets are limited to English, several multilingual datasets for other natural language understanding tasks are available, e.g., lexical semantic similarity (Multi-SimLex; Vuli´c et al., 2020), document classification (MLDoc; Schwenk and Li, 2018), sentiment analysis (Barnes et al., 2018), and natural language inference (XNLI; Conneau et al., 2018). Other recent multilingual sets target the QA task based on reading comprehenseen scripts (e.g., both HT and QU are written in Latin script). 13 Abductive reasoning is inference to the most plausible explanation of incomplete observations (Peirce, 1960). sion: MLQA (Lewis et al., 2020) in 7 languages, XQuAD (Artetxe et al., 2020b) in 10; and TyDiQA (Clark et al., 2020) in 11 typologically diverse languages. Further, PAWS-X (Yang et al., 2019b) evaluates paraphrase identification in 6 languag"
2020.emnlp-main.185,D18-1009,0,0.027514,"Levesque et al., 2012; Morgenstern and Ortiz, 2015). WSC consists in a pronoun coreference resolution task with paired instances, and has been recently expanded into the WinoGrande dataset (Sakaguchi et al., 2020) through crowd-sourcing. Several recent evaluation sets target particular aspects of commonsense, e.g., abductive reasoning (Bhagavatula et al., 2020),13 intents and reactions to events (Rashkin et al., 2018), social (Sap et al., 2019) and physical (Bisk et al., 2020) interactions, or visual commonsense (Zellers et al., 2019a). Others, e.g., CommonsenseQA (Talmor et al., 2019), SWAG (Zellers et al., 2018), and HellaSWAG (Zellers et al., 2019b) are cast as openended multiple-choice problems where the most sensible option is chosen. Another line of evaluation involves commonsense-enabled reading comprehension and question answering (Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). Multilingual Evaluation of Natural Language Understanding. While the above commonsense reasoning datasets are limited to English, several multilingual datasets for other natural language understanding tasks are available, e.g., lexical semantic similarity (Multi-SimLex; Vuli´c et al., 2020), document cl"
2020.emnlp-main.185,P19-1472,0,0.0271269,"ational modeling of commonsense reasoning is the Winograd Schema Challenge (WSC; Levesque et al., 2012; Morgenstern and Ortiz, 2015). WSC consists in a pronoun coreference resolution task with paired instances, and has been recently expanded into the WinoGrande dataset (Sakaguchi et al., 2020) through crowd-sourcing. Several recent evaluation sets target particular aspects of commonsense, e.g., abductive reasoning (Bhagavatula et al., 2020),13 intents and reactions to events (Rashkin et al., 2018), social (Sap et al., 2019) and physical (Bisk et al., 2020) interactions, or visual commonsense (Zellers et al., 2019a). Others, e.g., CommonsenseQA (Talmor et al., 2019), SWAG (Zellers et al., 2018), and HellaSWAG (Zellers et al., 2019b) are cast as openended multiple-choice problems where the most sensible option is chosen. Another line of evaluation involves commonsense-enabled reading comprehension and question answering (Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). Multilingual Evaluation of Natural Language Understanding. While the above commonsense reasoning datasets are limited to English, several multilingual datasets for other natural language understanding tasks are available,"
2020.emnlp-main.185,J15-4004,1,\N,Missing
2020.emnlp-main.185,P11-1132,0,\N,Missing
2020.emnlp-main.185,J19-3005,1,\N,Missing
2020.emnlp-main.185,D18-1029,1,\N,Missing
2020.emnlp-main.185,N19-1131,0,\N,Missing
2020.emnlp-main.185,P19-1310,1,\N,Missing
2020.emnlp-main.185,N19-5004,0,\N,Missing
2020.emnlp-main.185,N19-1421,0,\N,Missing
2020.emnlp-main.185,D19-1288,1,\N,Missing
2020.emnlp-main.186,D19-1572,0,0.0168032,"fectively combine these two types of language distance measures, call for further research that will advance our understanding of: 1) what knowledge is captured in monolingual and cross-lingual embedding spaces (Gerz et al., 2018; Pires et al., 2019; Artetxe et al., 2020); 2) how that knowledge complements or overlaps with linguistic knowledge compiled into lexical-semantic and typological databases (Dryer and Haspelmath, 2013; Wichmann et al., 2018; Ponti et al., 2019); and 3) how to use the combined knowledge for more effective transfer in cross-lingual NLP applications (Ponti et al., 2018; Eisenschlos et al., 2019). The differences in embedding spaces of different languages do not only depend on linguistic properties of the languages in consideration, but also on other factors such as the chosen training algorithm, underlying training domain, or training data size and quality (Søgaard et al., 2018; Arora et al., 2019; Vuli´c et al., 2020). In future research we also plan an in-depth study of these factors and their relation to our spectral analysis. We believe that the main insights from this study will inform and guide different cross-lingual transfer learning methods and scenarios in future work. Thes"
2020.emnlp-main.186,P19-1070,1,0.899741,"Missing"
2020.emnlp-main.186,D18-1330,0,0.0777051,"Missing"
2020.emnlp-main.186,kamholz-etal-2014-panlex,0,0.0256814,"Missing"
2020.emnlp-main.186,D19-1167,0,0.0284714,"the first step towards the development of more robust multilingually applicable NLP technology (O’Horan et al., 2016; Bjerva et al., 2019; Ponti et al., 2019). For instance, selecting suitable source languages is a prerequisite for successful cross-lingual transfer of dependency parsers or POS taggers (Naseem et al., 2012; Ponti et al., 2018; de Lhoneux et al., 2018). In another example, with all other factors kept similar (e.g., training data size, domain similarity), the quality of machine translation also depends heavily on the properties and language proximity of the actual language pair (Kudugunta et al., 2019). In this work, we contribute to this research endeavor by proposing a suite of spectral-based measures that capture the degree of isomorphism (Søgaard et al., 2018) between the monolingual embedding spaces of two languages. Our main hypothesis is that the potential to align two embedding spaces and learn transfer functions can be estimated through the differences between the monolingual embeddings’ spectra. We therefore discuss representative statistics of the spectrum of an embedding space (i.e., the set of the singular values of the embedding matrix), such as its condition number or its sor"
2020.emnlp-main.186,D18-1543,0,0.0348147,"Missing"
2020.emnlp-main.186,E17-2002,0,0.396657,"the embedding matrix), such as its condition number or its sorted list of singular values. We then derive measures for the isomorphism between two embedding spaces based on these statistics. To validate our hypothesis, we perform an extensive empirical evaluation with a range of crosslingual NLP tasks. This analysis reveals that our proposed spectrum-based isomorphism measures better correlate and explain greater variance than previous isomorphism measures (Søgaard et al., 2018; Patra et al., 2019). In addition, our measures also outperform standard approaches based on linguistic information (Littell et al., 2017), The first part of our empirical analysis targets bilingual lexicon induction (BLI), a cross-lingual task that received plenty of attention, in particular as a case study to investigate the impact of crosslanguage variation on task performance (Søgaard et al., 2018; Artetxe et al., 2018). Its popularity stems from its simple task formulation and reduced 2377 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2377–2390, c November 16–20, 2020. 2020 Association for Computational Linguistics resource requirements, which makes it widely applicable across"
2020.emnlp-main.186,P12-1066,0,0.0250075,"1 Introduction The effectiveness of joint multilingual modeling and cross-lingual transfer in cross-lingual NLP is critically impacted by the actual languages in consideration (Bender, 2011; Ponti et al., 2019). Characterizing, measuring, and understanding this cross-language variation is often the first step towards the development of more robust multilingually applicable NLP technology (O’Horan et al., 2016; Bjerva et al., 2019; Ponti et al., 2019). For instance, selecting suitable source languages is a prerequisite for successful cross-lingual transfer of dependency parsers or POS taggers (Naseem et al., 2012; Ponti et al., 2018; de Lhoneux et al., 2018). In another example, with all other factors kept similar (e.g., training data size, domain similarity), the quality of machine translation also depends heavily on the properties and language proximity of the actual language pair (Kudugunta et al., 2019). In this work, we contribute to this research endeavor by proposing a suite of spectral-based measures that capture the degree of isomorphism (Søgaard et al., 2018) between the monolingual embedding spaces of two languages. Our main hypothesis is that the potential to align two embedding spaces and"
2020.emnlp-main.186,P19-1018,0,0.418289,"re discuss representative statistics of the spectrum of an embedding space (i.e., the set of the singular values of the embedding matrix), such as its condition number or its sorted list of singular values. We then derive measures for the isomorphism between two embedding spaces based on these statistics. To validate our hypothesis, we perform an extensive empirical evaluation with a range of crosslingual NLP tasks. This analysis reveals that our proposed spectrum-based isomorphism measures better correlate and explain greater variance than previous isomorphism measures (Søgaard et al., 2018; Patra et al., 2019). In addition, our measures also outperform standard approaches based on linguistic information (Littell et al., 2017), The first part of our empirical analysis targets bilingual lexicon induction (BLI), a cross-lingual task that received plenty of attention, in particular as a case study to investigate the impact of crosslanguage variation on task performance (Søgaard et al., 2018; Artetxe et al., 2018). Its popularity stems from its simple task formulation and reduced 2377 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2377–2390, c November 16–2"
2020.emnlp-main.186,P19-1493,0,0.0231134,"tistic of effective condition number in §2.1. Our study is also the first to compare language distance measures that are based on discrete linguistic information (Littell et al., 2017) with measures of isomorphism (i.e., our spectral-based measures, IS, GH), which can also be used as proxy language distance measures. Our findings, suggesting that it is possible to effectively combine these two types of language distance measures, call for further research that will advance our understanding of: 1) what knowledge is captured in monolingual and cross-lingual embedding spaces (Gerz et al., 2018; Pires et al., 2019; Artetxe et al., 2020); 2) how that knowledge complements or overlaps with linguistic knowledge compiled into lexical-semantic and typological databases (Dryer and Haspelmath, 2013; Wichmann et al., 2018; Ponti et al., 2019); and 3) how to use the combined knowledge for more effective transfer in cross-lingual NLP applications (Ponti et al., 2018; Eisenschlos et al., 2019). The differences in embedding spaces of different languages do not only depend on linguistic properties of the languages in consideration, but also on other factors such as the chosen training algorithm, underlying training"
2020.emnlp-main.186,J19-3005,1,0.889996,"Missing"
2020.emnlp-main.186,P18-1142,1,0.89642,"Missing"
2020.emnlp-main.186,W19-4328,0,0.0612639,"Missing"
2020.emnlp-main.186,P18-1072,1,0.760711,"Missing"
2020.emnlp-main.186,D19-1449,1,0.902371,"Missing"
2020.emnlp-main.186,2020.emnlp-main.257,1,0.862162,"Missing"
2020.emnlp-main.333,D16-1250,0,0.0296999,"analysis (Sch¨onemann, 1966). W = arg min W |V | X kWti − si k2 s.t. WT W = I. (1) i=1 As described in Eq. 2, we then learn a linear mapping M to transform the source space towards the average of source and the rotated target space, by minimizing the squared Euclidean distance between each transformed source vector Msi and the mean vector µi (µi = (si + Wti )/2). M is the mapping we will use to transform the original contextualized space. Following Doval et al. (2018), M is found via a closed-form solution. M = arg min M |V | X kMsi − µi k2 (2) i For improved alignment quality, as advised by Artetxe et al. (2016), we normalize and meancenter4 the embeddings in S and T a priori. 4 Experiments Task Descriptions5 We evaluate on three Within Word tasks. Usage Similarity (Usim) (Erk et al., 2013) dataset measures graded similarity of the same word in pairs of different contexts on the scale from 1 to 5. Word in Context (WiC) (Pilehvar and Camacho-Collados, 2019) dataset challenges a system to predict a binary choice of whether a pair of contexts for the same word belongs to the same 4 We pre-process representations with the same centering and normalization in all tasks. Our reported results are similar or"
2020.emnlp-main.333,Q17-1010,0,0.0718176,"ngs, and offer insights on the reasons behind the improvement. Our method also has minimum computational complexity and requires no labeled data. 2 Background This section briefly introduces the contextualized/static models that we experimented in this study. For static models, we select three representative methods. SGNS (Mikolov et al., 2013), as the most successful variant of word2vec, trains a log linear model to predict context words given a target word with negative sampling in the objective. FastText improves over SGNS by training at the n-gram level and can generalize to unseen words (Bojanowski et al., 2017). In addition to these two prediction-based models, we also include 4066 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4066–4075, c November 16–20, 2020. 2020 Association for Computational Linguistics one count-based model, GloVe (Pennington et al., 2014). GloVe is trained to encode semantic relations exhibited in the ratios of word-word occurrence probabilities into word vectors. As opposed to static embeddings, contextualized models provide dynamic lexical representations as hidden layers in deep neural networks typically pre-trained with langu"
2020.emnlp-main.333,N18-2031,0,0.0158169,"t changes (Shi et al., 2019). To summarize the analysis, our controlled experiments confirm our two hypotheses that the transformation brings two independent effects: improved overall inter-word semantic space and improved within-word contextualization. Our qualitative analysis shows that the improved within-word contextualization is likely to be the result of context variance reduction. 5 Related Work It has been shown that combining different static word representations (for example through averaging or concatenation) into a meta embedding can usually lead to better lexical representations (Coates and Bollegala, 2018; Yin and Sch¨utze, 2016). While these task-independent meta embedding techniques are mainly applied on static embeddings, research has started to explore leveraging ensemble contextualized models when performing fine-tuning on a specific task (Devlin et al., 2019; Xu et al., 2020). Our method, as a post-processing transformation over task-independent contextual representations, is inherently different from these meta embedding and ensemble approaches. Computationally, our method does not require maintaining multiple models at test time, and is therefore more efficient. Our method is also by f"
2020.emnlp-main.333,P19-1285,0,0.0406146,"Missing"
2020.emnlp-main.333,N19-1423,0,0.57062,"meaning both across different usages of a word and across different words as they are used in context. We demonstrate that while the original contextual representations can be improved by another embedding space from either contextualized or static models, the static embeddings, which have lower computational requirements, provide the most gains. 1 Introduction Word representations are fundamental in Natural Language Processing (NLP) (Bengio et al., 2003). Recently, there has been a surge of contextualized models that achieve state-of-the-art in many NLP benchmark tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019). Even better performance has been reported from finetuning or training multiple contextualized models for a specific task such as question answering (Devlin et al., 2019; Xu et al., 2020). However, little has been explored on directly leveraging the many off-the-shelf pre-trained models to improve taskindependent representations for lexical semantics. Furthermore, classic static embeddings are often overlooked in this trend towards contextualized models. As opposed to contextualized embeddings that generate dynamic representations for words in context, s"
2020.emnlp-main.333,D18-1027,0,0.419572,"or the best performance, we use the Large Cased1 variant for each contextualized model. Since our study focuses on generic lexical representations and many of the lexical semantic tasks do not provide training data, we extract features2 from these contextualized models without fine-tuning the weights for a specific task. This feature-based approach is also more efficient compared with finetuning the increasingly larger models which can have hundreds of millions of parameters. 3 Method Our method3 is built from a recently proposed cross-lingual alignment technique called meeting in the middle (Doval et al., 2018). Their method relies on manual translations to learn a transformation over an orthogonal alignment for better cross-lingual static embeddings. We show that by a similar alignment + transformation technique, we can improve monolingual contextualized embeddings without resorting to any labeled data. The direct correspondence among contextualized and static embeddings for alignment is not straightforward, as contextualized models can compute infinite representations for infinite contexts. Inspired by previous study (Schuster et al., 2019) that found contextualized embeddings roughly form word cl"
2020.emnlp-main.333,J13-3003,1,0.857793,"average of source and the rotated target space, by minimizing the squared Euclidean distance between each transformed source vector Msi and the mean vector µi (µi = (si + Wti )/2). M is the mapping we will use to transform the original contextualized space. Following Doval et al. (2018), M is found via a closed-form solution. M = arg min M |V | X kMsi − µi k2 (2) i For improved alignment quality, as advised by Artetxe et al. (2016), we normalize and meancenter4 the embeddings in S and T a priori. 4 Experiments Task Descriptions5 We evaluate on three Within Word tasks. Usage Similarity (Usim) (Erk et al., 2013) dataset measures graded similarity of the same word in pairs of different contexts on the scale from 1 to 5. Word in Context (WiC) (Pilehvar and Camacho-Collados, 2019) dataset challenges a system to predict a binary choice of whether a pair of contexts for the same word belongs to the same 4 We pre-process representations with the same centering and normalization in all tasks. Our reported results are similar or better than the results from un-preprocessed representations. 5 Appendix C reports details for each task and experiment. 4067 meaning or not. We follow the advised training scheme in"
2020.emnlp-main.333,S19-1002,0,0.034744,"Missing"
2020.emnlp-main.333,J15-4004,1,0.786565,"r a pair of contexts for the same word belongs to the same 4 We pre-process representations with the same centering and normalization in all tasks. Our reported results are similar or better than the results from un-preprocessed representations. 5 Appendix C reports details for each task and experiment. 4067 meaning or not. We follow the advised training scheme in the original paper to learn a cosine similarity threshold on the representations. The recently proposed CoSimlex (Armendariz et al., 2019) task provides contexts for selected word pairs from the word similarity benchmark SimLex-999 (Hill et al., 2015) and measures the graded contextual effect. We use the English dataset from this task. Its first subtask, CoSimlex-I, evaluates the change in similarity between the same word pair under different contexts. As it requires a system to capture different contextual representations of the same word in order to correctly predict the change of similarity to the other word in the pair, CoSimlex-I indirectly measures within-word contextual effect and therefore provides our third Within Word task. The second CoSimLex subtask, CoSimlex-II, is an Inter Word task as it requires a system to predict the abso"
2020.emnlp-main.333,K19-1004,1,0.882179,"Missing"
2020.emnlp-main.333,2021.ccl-1.108,0,0.136444,"Missing"
2020.emnlp-main.333,S13-2035,0,0.0156121,"ic anchors with guidance from other contextualized/static embeddings. We show leveraging static embeddings, with no labeled data, consistently improves (across almost all configurations) on both Inter Word and surprisingly Within Word context-aware lexical semantic tasks. We also perform controlled analysis to highlight, in isolation, the improvement from the transformation on both contextualization and on an overall interword semantic space. In the future, we plan to apply the transformed representations on more lexical semantics tasks such as word sense disambiguation within an application (Navigli and Vannella, 2013). 6 A simple meta embedding baseline that concatenates contextualized and static representations generally impairs the performance. (Appendix F) 4070 Acknowledgments We thank the anonymous reviewers for their helpful feedback on this work. We acknowledge Peterhouse College at University of Cambridge for funding Qianchu Liu’s PhD research. The work was also supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909) awarded to Anna Korhonen. We also appreciate many helpful discussions and feedback from our colleagues in the Language Technology Lab. Referenc"
2020.emnlp-main.333,D14-1113,0,0.0996041,"Missing"
2020.emnlp-main.333,D14-1162,0,0.0927143,"tative methods. SGNS (Mikolov et al., 2013), as the most successful variant of word2vec, trains a log linear model to predict context words given a target word with negative sampling in the objective. FastText improves over SGNS by training at the n-gram level and can generalize to unseen words (Bojanowski et al., 2017). In addition to these two prediction-based models, we also include 4066 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4066–4075, c November 16–20, 2020. 2020 Association for Computational Linguistics one count-based model, GloVe (Pennington et al., 2014). GloVe is trained to encode semantic relations exhibited in the ratios of word-word occurrence probabilities into word vectors. As opposed to static embeddings, contextualized models provide dynamic lexical representations as hidden layers in deep neural networks typically pre-trained with language modeling objectives. In our study, we choose three state-of-the-art contextualized models. BERT (Devlin et al., 2019) trains bidirectional transformers (Vaswani et al., 2017) with masked language modeling and next sentence prediction objectives. Liu et al. (2019b)’s RoBERTa further improves upon BE"
2020.emnlp-main.333,N18-1202,0,0.0555882,"textual variations of meaning both across different usages of a word and across different words as they are used in context. We demonstrate that while the original contextual representations can be improved by another embedding space from either contextualized or static models, the static embeddings, which have lower computational requirements, provide the most gains. 1 Introduction Word representations are fundamental in Natural Language Processing (NLP) (Bengio et al., 2003). Recently, there has been a surge of contextualized models that achieve state-of-the-art in many NLP benchmark tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019). Even better performance has been reported from finetuning or training multiple contextualized models for a specific task such as question answering (Devlin et al., 2019; Xu et al., 2020). However, little has been explored on directly leveraging the many off-the-shelf pre-trained models to improve taskindependent representations for lexical semantics. Furthermore, classic static embeddings are often overlooked in this trend towards contextualized models. As opposed to contextualized embeddings that generate dynamic representations fo"
2020.emnlp-main.586,2020.acl-main.421,0,0.0383607,", are reported for EN and DE. 4 Results and Discussion A summary of the results is shown in Figure 2 for LSIM, in Figure 3a for BLI, in Figure 3b for CLIR, in Figure 4a and Figure 4b for RELP, and in Figure 4c for WA. These results offer multiple axes of comparison, and the ensuing discussion focuses on the central questions Q1-Q3 posed in §1.6 Monolingual versus Multilingual LMs. Results across all tasks validate the intuition that languagespecific monolingual LMs contain much more lexical information for a particular target language than massively multilingual models such as mBERT or XLM-R (Artetxe et al., 2020). We see large drops between MONO.* and MULTI.* configurations even for very high-resource languages (EN and DE), and they are even more prominent for FI and TR. Encompassing 100+ training languages with limited model capacity, multilingual models suffer from the “curse of multilinguality” (Conneau et al., 2020): they must trade off monolingual lexical information coverage (and consequently monolingual performance) for a wider language coverage.7 How Important is Context? Another observation that holds across all configurations concerns the usefulness of providing contexts drawn from external"
2020.emnlp-main.586,Q19-1004,0,0.020417,"et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across dif"
2020.emnlp-main.586,Q17-1010,0,0.807302,"r layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers. 1 Introduction and Motivation Language models (LMs) based on deep Transformer networks (Vaswani et al., 2017), pretrained on unprecedentedly large amounts of text, offer unmatched performance in virtually every NLP task (Qiu et al., 2020). Models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edm"
2020.emnlp-main.586,C18-1140,0,0.0236558,"fastText (Bojanowski et al., 2017). This study has only scratched the surface of this research avenue. In future work, we plan to investigate how domains of external corpora affect AOC configurations, and how to sample representative contexts from the corpora. We will also extend the study to more languages, more lexical semantic probes, and other larger underlying LMs. The difference in performance across layers also calls for more sophisticated lexical representation extraction methods (e.g., through layer weighting or attention) similar to meta-embedding approaches (Yin and Sch¨utze, 2016; Bollegala and Bao, 2018; Kiela et al., 2018). Given the current large gaps between monolingual and multilingual LMs, we will also focus on lightweight methods to enrich lexical content in multilingual LMs (Wang et al., 2020; Pfeiffer et al., 2020). Acknowledgments This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909) awarded to Anna Korhonen. The work of Goran Glavaˇs and Robert Litschko is supported by the Baden-W¨urttemberg Stiftung (AGREE grant of the Eliteprogramm). References Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. A robust self-learning meth"
2020.emnlp-main.586,2020.nlp4convai-1.5,1,0.835928,"Missing"
2020.emnlp-main.586,D19-1627,0,0.0202607,"19; Mickus et al., 2020). As a consequence, we hypothesise that abstract, type-level information could be codified in lower layers instead. However, given the absence of a direct equivalent to a static word type embedding, we still need to establish how to extract such type-level information. In prior work, contextualised representations (and attention weights) have been interpreted in the light of linguistic knowledge mostly through probes. These consist in learned classifier predicting annotations like POS tags (Pimentel et al., 2020) and word senses (Peters et al., 2018; Reif et al., 2019; Chang and Chen, 2019), or linear transformations to a space where distances mirror dependency tree structures (Hewitt and Manning, 2019).1 In this work, we explore several unsupervised word-level representation extraction strategies and configurations for lexico-semantic tasks (i.e., probes), stemming from different combinations of the components detailed in Table 1 and illustrated in Figure 1. In particular, we assess the impact of: 1) encoding tokens with monolingual LM-pretrained Transformers vs. with their mas1 The interplay between the complexity of a probe and its accuracy, as well as its effect on the overa"
2020.emnlp-main.586,2020.acl-main.493,0,0.0265766,"Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across different languages is currently lacking. We present such a study, spanning six typologically diverse languages for which comparable pretrained BERT models and evaluation data are"
2020.emnlp-main.586,2020.acl-main.747,0,0.0810209,"Missing"
2020.emnlp-main.586,D19-1572,0,0.019846,"match configurations that average subword embeddings from multiple contexts (AOC-10 and AOC100). However, it is worth noting that 1) perfor5 Note that RELP is structurally different from the other four tasks: instead of direct computations with word embeddings, called metric learning or similarity-based evaluation (Ruder et al., 2019), it uses them as features in a neural architecture. 6 Full results are available in the appendix. 7 For a particular target language, monolingual performance can be partially recovered by additional in-language monolingual training via masked language modeling (Eisenschlos et al., 2019; Pfeiffer et al., 2020). In a side experiment, we have also verified that the same holds for lexical information coverage. 7225 Spearman ρ correlation Spearman ρ correlation 0.55 0.45 0.35 0.25 0.15 L≤2 L≤4 L≤6 L ≤ 8 L ≤ 10 Average over layers 0.55 0.45 0.35 0.25 0.15 L ≤ 12 L≤2 (a) English L≤6 L ≤ 8 L ≤ 10 Average over layers L ≤ 12 (b) Finnish 0.4 Spearman ρ correlation 0.65 Spearman ρ correlation L≤4 0.55 0.45 0.35 L≤2 L≤4 L≤6 L ≤ 8 L ≤ 10 Average over layers 0.3 0.2 0.1 0.0 L≤2 L ≤ 12 (c) Mandarin Chinese L≤4 L≤6 L ≤ 8 L ≤ 10 Average over layers L ≤ 12 (d) Russian Figure 2: Spearman’s ρ c"
2020.emnlp-main.586,D19-1006,0,0.406652,"gers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across different languages is currently lacking. We present such a study, spanning six typologically diverse languages for which comparable pretrained BERT models and evaluation data are readily available. We dissect the pipeline for extracting lexical representations, and divide it into crucial components, including: the underlying source LM, the selection of subword tokens, external corpora, and which Transformer layers to average o"
2020.emnlp-main.586,D18-1029,1,0.828082,"Missing"
2020.emnlp-main.586,N18-2029,1,0.893221,"Missing"
2020.emnlp-main.586,P19-1070,1,0.894028,"Missing"
2020.emnlp-main.586,P19-1356,0,0.0244048,"word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across different languages is currently lacking. We present such a study, spanning six typologically diverse languages for which comparable pretr"
2020.emnlp-main.586,D18-1176,0,0.0193142,"l., 2017). This study has only scratched the surface of this research avenue. In future work, we plan to investigate how domains of external corpora affect AOC configurations, and how to sample representative contexts from the corpora. We will also extend the study to more languages, more lexical semantic probes, and other larger underlying LMs. The difference in performance across layers also calls for more sophisticated lexical representation extraction methods (e.g., through layer weighting or attention) similar to meta-embedding approaches (Yin and Sch¨utze, 2016; Bollegala and Bao, 2018; Kiela et al., 2018). Given the current large gaps between monolingual and multilingual LMs, we will also focus on lightweight methods to enrich lexical content in multilingual LMs (Wang et al., 2020; Pfeiffer et al., 2020). Acknowledgments This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909) awarded to Anna Korhonen. The work of Goran Glavaˇs and Robert Litschko is supported by the Baden-W¨urttemberg Stiftung (AGREE grant of the Eliteprogramm). References Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. A robust self-learning method for fully unsuperv"
2020.emnlp-main.586,2005.mtsummit-papers.11,0,0.115125,"esented in the respective fastText (FT) vectors, which were trained on lowercased monolingual Wikipedias by Bojanowski et al. (2017). The equivalent vocabulary coverage allows a direct comparison to fastText vectors, which we use as a baseline static WE method in all evaluation tasks. To retain the same vocabulary across all configurations, in AOC variants we back off to the related ISO variant for words that have zero occurrences in external corpora. For all AOC vector variants, we leverage 1M sentences of maximum sequence length 512, which we randomly sample from external corpora: Europarl (Koehn, 2005) for EN, DE, FI, available via OPUS (Tiedemann, 2009); the United Nations Parallel Corpus for RU and ZH (Ziemski et al., 2016), and monolingual TR WMT17 data (Bojar et al., 2017). Evaluation Tasks. We carry out the evaluation on five standard and diverse lexical semantic tasks: Task 1: Lexical semantic similarity (LSIM) is the most widespread intrinsic task for evaluation of traditional word embeddings (Hill et al., 2015). The evaluation metric is the Spearman’s rank correlation between the average of human-elicited semantic similarity scores for word pairs and the cosine similarity between th"
2020.emnlp-main.586,2020.acl-main.375,0,0.0252215,"Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across different languages is currently lacking. We present such a study, spanning six typologically diverse languages for which comparable pretrained BERT models and e"
2020.emnlp-main.586,D14-1162,0,0.0919991,"aim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers. 1 Introduction and Motivation Language models (LMs) based on deep Transformer networks (Vaswani et al., 2017), pretrained on unprecedentedly large amounts of text, offer unmatched performance in virtually every NLP task (Qiu et al., 2020). Models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter"
2020.emnlp-main.586,N18-1202,0,0.25049,"e that translation pairs indeed obtain similar representations (Q4), but the similarity depends on the extraction configuration, as well as on the typological distance between the two languages. 2 Lexical Representations from Pretrained Language Models Classical static word embeddings (Bengio et al., 2003; Mikolov et al., 2013b; Pennington et al., 2014) are grounded in distributional semantics, as they infer the meaning of each word type from its co-occurrence patterns. However, LM-pretrained Transformer encoders have introduced at least two levels of misalignment with the classical approach (Peters et al., 2018; Devlin et al., 2019). First, representations are assigned to word tokens and are affected by the current context and position within a sentence (Mickus et al., 2020). Second, tokens may correspond to subword strings rather than complete word forms. This begs the question: do pretrained encoders still retain a notion of lexical concepts, abstracted from their instances in texts? Analyses of lexical semantic information in large pretrained LMs have been limited so far, focusing only on the English language and on the task of word sense disambiguation. Reif et al. (2019) showed that senses are"
2020.emnlp-main.586,2020.emnlp-main.617,1,0.88287,"Missing"
2020.emnlp-main.586,2020.acl-main.420,0,0.0210954,"ame token tends not to be self-similar across different contexts (Ethayarajh, 2019; Mickus et al., 2020). As a consequence, we hypothesise that abstract, type-level information could be codified in lower layers instead. However, given the absence of a direct equivalent to a static word type embedding, we still need to establish how to extract such type-level information. In prior work, contextualised representations (and attention weights) have been interpreted in the light of linguistic knowledge mostly through probes. These consist in learned classifier predicting annotations like POS tags (Pimentel et al., 2020) and word senses (Peters et al., 2018; Reif et al., 2019; Chang and Chen, 2019), or linear transformations to a space where distances mirror dependency tree structures (Hewitt and Manning, 2019).1 In this work, we explore several unsupervised word-level representation extraction strategies and configurations for lexico-semantic tasks (i.e., probes), stemming from different combinations of the components detailed in Table 1 and illustrated in Figure 1. In particular, we assess the impact of: 1) encoding tokens with monolingual LM-pretrained Transformers vs. with their mas1 The interplay between"
2020.emnlp-main.586,2020.emnlp-main.185,1,0.865778,"Missing"
2020.emnlp-main.586,N19-1112,0,0.283984,"ults indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers. 1 Introduction and Motivation Language models (LMs) based on deep Transformer networks (Vaswani et al., 2017), pretrained on unprecedentedly large amounts of text, offer unmatched performance in virtually every NLP task (Qiu et al., 2020). Models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al.,"
2020.emnlp-main.586,K19-1004,1,0.869925,"Missing"
2020.emnlp-main.586,2021.ccl-1.108,0,0.114032,"Missing"
2020.emnlp-main.586,2020.tacl-1.54,0,0.0262276,"., 2017), pretrained on unprecedentedly large amounts of text, offer unmatched performance in virtually every NLP task (Qiu et al., 2020). Models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019)"
2020.emnlp-main.586,D19-1374,0,0.0238678,"BLI, the respective scores are 0.486 and 0.315 with ISO, and 0.503 and 0.334 with AOC-10. Similar observations hold for FI and ZH LSIM, and also in the RELP task. In RELP, it is notable that ‘BERT-based’ embeddings can recover more lexical relation knowledge than standard FT vectors. These findings reveal that pretrained LMs indeed implicitly capture plenty of lexical type-level knowledge (which needs to be ‘recovered’ from the models); this also suggests why pretrained LMs have been successful in tasks where this knowledge is directly useful, such as NER and POS tagging (Tenney et al., 2019; Tsai et al., 2019). Finally, we also note that gains with AOC over ISO are much more pronounced for the under-performing MULTI.* configurations: this indicates that MONO models store more lexical information even in absence of context. How Important are Special Tokens? The results reveal that the inclusion of special tokens [CLS] and [SEP] into type-level embedding extraction deteriorates the final lexical information contained in the embeddings. This finding holds for different languages, underlying LMs, and averaging across various layers. The NOSPEC configurations consistently outperform their ALL and WITHCL"
2020.emnlp-main.586,2020.emnlp-main.14,0,0.0225443,"or dependency tree structures (Hewitt and Manning, 2019).1 In this work, we explore several unsupervised word-level representation extraction strategies and configurations for lexico-semantic tasks (i.e., probes), stemming from different combinations of the components detailed in Table 1 and illustrated in Figure 1. In particular, we assess the impact of: 1) encoding tokens with monolingual LM-pretrained Transformers vs. with their mas1 The interplay between the complexity of a probe and its accuracy, as well as its effect on the overall procedure, remain controversial (Pimentel et al., 2020; Voita and Titov, 2020). 7223 Component Source LM Label Short Description MONO Language-specific (i.e., monolingually pretrained) BERT Multilingual BERT, pretrained on 104 languages (with shared subword vocabulary) MULTI ISO Context AOC -M NOSPEC Subword Tokens ALL WITHCLS Layerwise Avg Each vocabulary word w is encoded in isolation, without any external context Average-over-context: average over word’s encodings from M different contexts/sentences Special tokens [CLS] and [SEP] are excluded from subword embedding averaging Both special tokens [CLS] and [SEP] are included into subword embedding averaging [CLS] is in"
2020.emnlp-main.586,2020.cl-4.5,1,0.881404,"Missing"
2020.emnlp-main.586,D19-1575,0,0.0186957,"Averaging across layers bottom-to-top (i.e., from L0 to L12 ) is beneficial across the board, but we notice that scores typically saturate or even decrease in some tasks and languages when we include 8 For this reason, we report the results of tions only in the NOSPEC setting. AOC configurahigher layers into averaging: see the scores with *.AVG(L≤10) and *.AVG(L≤12) configurations, e.g., for FI LSIM; EN/DE RELP, and summary BLI and CLIR scores. This hints to the fact that two strategies typically used in prior work, either to take the vectors only from the embedding layer L0 (Wu et al., 2020; Wang et al., 2019) or to average across all layers (Liu et al., 2019b), extract suboptimal word representations for a wide range of setups and languages. The sweet spot for n in *.AVG(L≤n) configurations seems largely task- and language-dependent, as peak scores are obtained with different n-s. Whereas averaging across all layers generally hurts performance, the results strongly suggest that averaging across layer subsets (rather than selecting a single layer) is widely useful, especially across bottom-most layers: e.g., L ≤ 6 with MONO.ISO.NOSPEC yields an average score of 0.561 in LSIM, 0.076 in CLIR, and 0.4"
2020.lrec-1.705,N09-1003,0,0.89788,"sed in recent years, as reflected in the publication of a large verb similarity dataset for English, SimVerb3500 (hereafter SimVerb) (Gerz et al., 2016). However, the need for high-quality, wide-coverage lexical resources targeting verb semantics has by no means been satisfied. Rich lexical resources encoding information about verbs’ semantic properties such as FrameNet (Baker et al., 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006) are still unavailable for most languages, and evaluation datasets dedicated to or dominated by nouns are by far predominant (Finkelstein et al., 2002; Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Therefore, we propose methodology aimed at alleviating the evaluation data scarcity problem and overcoming the bottleneck of manual gold standard creation. We present a novel approach to obtaining semantic similarity data by means of a two-phase design consisting in (1) bottom-up semantic clustering of verbs into relatednessbased classes and (2) spatial similarity judgments obtained via a multi-arrangement method so far employed only in psychology and cognitive neuroscience research and with visual stimuli (Kriegeskorte and Mur, 2012; Mur et al., 2013;"
2020.lrec-1.705,N19-1050,0,0.0169763,"he dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synonyms, only one correct), and on analogy (Mikolov et al.,"
2020.lrec-1.705,W16-2519,0,0.0178536,"verb pairs. Verb-only datasets include YP-130 (Yang and Powers, 2006) (130 verb pairs) and the dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one ta"
2020.lrec-1.705,P98-1013,0,0.705002,"ding (Jackendoff, 1972; Levin, 1993; McRae et al., 1997; Altmann and Kamide, 1999; Resnik and Diab, 2000; Ferretti et al., 2001; Sauppe, 2016, inter alia). The demand for verb-specific resources to support NLP has been recognised in recent years, as reflected in the publication of a large verb similarity dataset for English, SimVerb3500 (hereafter SimVerb) (Gerz et al., 2016). However, the need for high-quality, wide-coverage lexical resources targeting verb semantics has by no means been satisfied. Rich lexical resources encoding information about verbs’ semantic properties such as FrameNet (Baker et al., 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006) are still unavailable for most languages, and evaluation datasets dedicated to or dominated by nouns are by far predominant (Finkelstein et al., 2002; Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Therefore, we propose methodology aimed at alleviating the evaluation data scarcity problem and overcoming the bottleneck of manual gold standard creation. We present a novel approach to obtaining semantic similarity data by means of a two-phase design consisting in (1) bottom-up semantic clustering of verbs into relatednessbased"
2020.lrec-1.705,D14-1034,1,0.83064,"g and spatial arrangements of words. Despite their wide usefulness, most available datasets used for intrinsic evaluation in distributional semantics are restricted in size and coverage, many do not distinguish similarity and relatedness, and only a few target verbs in particular. The prominent word pair datasets include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), comprising 353 noun pairs, and SimLex-999 (Hill et al., 2015), comprising 999 word pairs out of which 222 are verb pairs. Verb-only datasets include YP-130 (Yang and Powers, 2006) (130 verb pairs) and the dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relie"
2020.lrec-1.705,W11-2501,0,0.0337348,"e which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synonyms, only one correct), and on analogy (Mikolov et al., 2013; Gladkova et al., 2016) and semantic relation datasets (Baroni and Lenci, 2011). The largest verb-focused dataset currently available, SimVerb, is a result of a crowd-sourcing effort involving over 800 raters, each completing the pairwise similarity rating task for 79 verb pairs. In this paper, we present an alternative novel approach which allows an annotator to implicitly express multiple pairwise similarity judgments by a single mouse drag, instead of having to consider each word pair independently. This lets us scale up the data collection and, starting from the same set of verbs as those used in SimVerb, generate similarity ratings for over 8 times as many verb pair"
2020.lrec-1.705,P14-1023,0,0.0147114,"e still limited, and few and far between. Rich expert-created resources such as WordNet (Miller, 1995; Fellbaum, 1998), VerbNet (Kipper Schuler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-h"
2020.lrec-1.705,W16-2502,0,0.0255245,"i are considered in the context of the entire sample. Each placement simultaneously communicates similarity relationship of the item to all other items in the set. SpAM taps into the spatial nature of humans’ mental representation of concept similarity (Lakoff and Johnson, 1999; 5750 G¨ardenfors, 2004; Casasanto, 2008). It allows for a freer, intuitive expression of similarity judgments as continuous distances, rather than requiring assignment of discrete numerical ratings. The latter, although ubiquitous in intrinsic evaluation of distributional semantic models, have a number of limitations (Batchkarov et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016; Kiritchenko and Mohammad, 2017). Human ratings of isolated pairs of words are likely to be biased by word frequency, prototypicality, order of presentation and speed of association, rather than reflecting semantic factors. At the same time, degrees of similarity and subtle meaning distinctions between words are very difficult to quantify and translate onto a discrete scale without context or points of reference. This leads to inconsistencies in annotations by the same rater or across raters. By allowing repeated multi-wise, relative continuous"
2020.lrec-1.705,Q17-1010,0,0.0301069,"huler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhard). Similarity defined as association, i.e., the mental activation of a term when another is presented (Chiarello et"
2020.lrec-1.705,P12-1015,0,0.111911,"as reflected in the publication of a large verb similarity dataset for English, SimVerb3500 (hereafter SimVerb) (Gerz et al., 2016). However, the need for high-quality, wide-coverage lexical resources targeting verb semantics has by no means been satisfied. Rich lexical resources encoding information about verbs’ semantic properties such as FrameNet (Baker et al., 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006) are still unavailable for most languages, and evaluation datasets dedicated to or dominated by nouns are by far predominant (Finkelstein et al., 2002; Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Therefore, we propose methodology aimed at alleviating the evaluation data scarcity problem and overcoming the bottleneck of manual gold standard creation. We present a novel approach to obtaining semantic similarity data by means of a two-phase design consisting in (1) bottom-up semantic clustering of verbs into relatednessbased classes and (2) spatial similarity judgments obtained via a multi-arrangement method so far employed only in psychology and cognitive neuroscience research and with visual stimuli (Kriegeskorte and Mur, 2012; Mur et al., 2013; Charest et al., 201"
2020.lrec-1.705,J06-1003,0,0.0402948,"inantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhard). Similarity defined as association, i.e., the mental activation of a term when another is presented (Chiarello et al., 1990; Lemaire and Denhiere, 2006), e.g., knife-murder, has been estimated in terms of frequency of co-occurrence of words in language (and the physical world) (Turney, 2001; Turney and Pantel, 2010; McRae et al., 2012; Bruni et al., 2012). In contrast to associative relatedness, a concept of semantic similarity defined in terms of shared superordinate cat"
2020.lrec-1.705,W16-2506,0,0.0148593,"ontext of the entire sample. Each placement simultaneously communicates similarity relationship of the item to all other items in the set. SpAM taps into the spatial nature of humans’ mental representation of concept similarity (Lakoff and Johnson, 1999; 5750 G¨ardenfors, 2004; Casasanto, 2008). It allows for a freer, intuitive expression of similarity judgments as continuous distances, rather than requiring assignment of discrete numerical ratings. The latter, although ubiquitous in intrinsic evaluation of distributional semantic models, have a number of limitations (Batchkarov et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016; Kiritchenko and Mohammad, 2017). Human ratings of isolated pairs of words are likely to be biased by word frequency, prototypicality, order of presentation and speed of association, rather than reflecting semantic factors. At the same time, degrees of similarity and subtle meaning distinctions between words are very difficult to quantify and translate onto a discrete scale without context or points of reference. This leads to inconsistencies in annotations by the same rater or across raters. By allowing repeated multi-wise, relative continuous similarity judgments,"
2020.lrec-1.705,D16-1235,1,0.902677,"Missing"
2020.lrec-1.705,W16-2507,0,0.0140033,"ample. Each placement simultaneously communicates similarity relationship of the item to all other items in the set. SpAM taps into the spatial nature of humans’ mental representation of concept similarity (Lakoff and Johnson, 1999; 5750 G¨ardenfors, 2004; Casasanto, 2008). It allows for a freer, intuitive expression of similarity judgments as continuous distances, rather than requiring assignment of discrete numerical ratings. The latter, although ubiquitous in intrinsic evaluation of distributional semantic models, have a number of limitations (Batchkarov et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016; Kiritchenko and Mohammad, 2017). Human ratings of isolated pairs of words are likely to be biased by word frequency, prototypicality, order of presentation and speed of association, rather than reflecting semantic factors. At the same time, degrees of similarity and subtle meaning distinctions between words are very difficult to quantify and translate onto a discrete scale without context or points of reference. This leads to inconsistencies in annotations by the same rater or across raters. By allowing repeated multi-wise, relative continuous similarity judgments, SpAM addresses shortcoming"
2020.lrec-1.705,N16-2002,0,0.0123547,"relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synonyms, only one correct), and on analogy (Mikolov et al., 2013; Gladkova et al., 2016) and semantic relation datasets (Baroni and Lenci, 2011). The largest verb-focused dataset currently available, SimVerb, is a result of a crowd-sourcing effort involving over 800 raters, each completing the pairwise similarity rating task for 79 verb pairs. In this paper, we present an alternative novel approach which allows an annotator to implicitly express multiple pairwise similarity judgments by a single mouse drag, instead of having to consider each word pair independently. This lets us scale up the data collection and, starting from the same set of verbs as those used in SimVerb, genera"
2020.lrec-1.705,J15-4004,1,0.936771,"publication of a large verb similarity dataset for English, SimVerb3500 (hereafter SimVerb) (Gerz et al., 2016). However, the need for high-quality, wide-coverage lexical resources targeting verb semantics has by no means been satisfied. Rich lexical resources encoding information about verbs’ semantic properties such as FrameNet (Baker et al., 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006) are still unavailable for most languages, and evaluation datasets dedicated to or dominated by nouns are by far predominant (Finkelstein et al., 2002; Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Therefore, we propose methodology aimed at alleviating the evaluation data scarcity problem and overcoming the bottleneck of manual gold standard creation. We present a novel approach to obtaining semantic similarity data by means of a two-phase design consisting in (1) bottom-up semantic clustering of verbs into relatednessbased classes and (2) spatial similarity judgments obtained via a multi-arrangement method so far employed only in psychology and cognitive neuroscience research and with visual stimuli (Kriegeskorte and Mur, 2012; Mur et al., 2013; Charest et al., 2014). We show how it c"
2020.lrec-1.705,S13-2049,0,0.448125,"om the final sample due to their very low frequency, resulting in a 825-verb sample. 5751 4.1. Figure 2: The rough clustering task layout (zoomed in). Verbs can be dragged onto the ‘new category’ circle to create a new grouping, onto ‘copy’ to create a duplicate label, or ‘Trash’ to dispose of the unwanted duplicate. Participants The rough clustering task was first tested by two native English speakers. They produced clusters with an encouraging degree of overlap. It was computed using the B-Cubed metric (Bagga and Baldwin, 1998) extended by Amig´o et al. (2009) to overlapping clusters and by Jurgens and Klapaftis (2013) to fuzzy clusters, as used in related work (Jurgens and Klapaftis, 2013; Majewska et al., 2018). B-Cubed, based on precision and recall, estimates the overlap between two clusterings X and Y at the item level. Let U represent the collection of items, Xi the set of clusters containing item i in clustering X, Yi the set of clusters containing i in clustering Y ; let Ji be the set of items in Xi but excluding i and Ki be the set of items in Yi but excluding i. B-Cubed precision P and recall R are defined as: P = 1 X 1 X min(|Xi ∩ Xj |, |Yi ∩ Yj |) |U |i∈U |Ji |j∈J |Xi ∩ Xj | i numerous as the bi"
2020.lrec-1.705,kipper-etal-2006-extending,1,0.837416,"Missing"
2020.lrec-1.705,N16-1095,0,0.0182253,"ts include YP-130 (Yang and Powers, 2006) (130 verb pairs) and the dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synony"
2020.lrec-1.705,P17-2074,0,0.121665,"rs, 2006) (130 verb pairs) and the dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synonyms, only one correct), and on an"
2020.lrec-1.705,J99-4009,0,0.887765,"se combinations possible. However, in SpAM a subject arranges multiple stimuli simultaneously in a two-dimensional space (e.g. on a computer screen), expressing (dis)similarity through the relative positions of items within that space. The inter-stimulus Euclidean distances represent pairwise dissimilarities. This set-up ensures that all stimuli are considered in the context of the entire sample. Each placement simultaneously communicates similarity relationship of the item to all other items in the set. SpAM taps into the spatial nature of humans’ mental representation of concept similarity (Lakoff and Johnson, 1999; 5750 G¨ardenfors, 2004; Casasanto, 2008). It allows for a freer, intuitive expression of similarity judgments as continuous distances, rather than requiring assignment of discrete numerical ratings. The latter, although ubiquitous in intrinsic evaluation of distributional semantic models, have a number of limitations (Batchkarov et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016; Kiritchenko and Mohammad, 2017). Human ratings of isolated pairs of words are likely to be biased by word frequency, prototypicality, order of presentation and speed of association, rather than reflecting"
2020.lrec-1.705,P14-2050,0,0.0227907,"few and far between. Rich expert-created resources such as WordNet (Miller, 1995; Fellbaum, 1998), VerbNet (Kipper Schuler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhar"
2020.lrec-1.705,L18-1153,1,0.89982,"Missing"
2020.lrec-1.705,W16-2523,0,0.024829,"e in legibility. Furthermore, the two-phase set-up handles ambiguity by permitting copying verb labels to capture different senses in Phase 1. The rough clustering phase guarantees that each verb label is presented in the context of related verbs in the arena in Phase 2, a necessary prerequisite for meaningful similarity judgments in psychology (Turner et al., 1987).2 The actual sense is implied by the surrounding words: this helps avoid mismatches in similarity judgments between participants for ambiguous verbs. What is more, this avoids the common problem of ambiguous low similarity scores (Milajevs and Griffiths, 2016) that conflate similarity judgments on antonyms (vanish - appear) and completely unrelated notions (fry - appear), and focuses on judgments between comparable concepts. 3.3. Data To test the scaling-up potential of our approach and to enable direct comparisons with the standard pairwise similarity rating methods, we select the 827 verbs from SimVerb (Gerz et al., 2016) as our item sample.3 The sample presents a challenge due to its size (i.e., it is almost nine times as 2 Following Turner et al. (1987), ‘stimuli can only be compared in so far as they have already been categorised as identical,"
2020.lrec-1.705,Q17-1022,1,0.925243,"Missing"
2020.lrec-1.705,D14-1162,0,0.0815648,"expert-created resources such as WordNet (Miller, 1995; Fellbaum, 1998), VerbNet (Kipper Schuler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhard). Similarity defined as"
2020.lrec-1.705,D18-1169,0,0.013689,"arena (inhale - exhale and sink - swim). This tendency is also illustrated by the RDM in Figure 3: separate clusters are formed by verbs such as raise, rise, grow and diminish, decline, lower, and finish is kept separate from begin and start. Crucially, our spatial approach records simultaneous judgments on multiple related words, which helps improve judgment consistency (e.g., word pairs holding analogous relations have similar scores) and allows making subtle distinctions based on varying degrees of similarity by means of 9 The ρSV scores are promising compared to the ρ = 0.612 SimVerb IAA (Pilehvar et al., 2018), despite the fact that the easy cases of verb pairs involving very disparate verbs (in different classes) are not included in our results. 10 There are exceptions: positive (e.g. love) and negative (e.g. hate) emotion verbs form two different classes; there are also separate groupings with ‘construction’ and ‘destruction’ verbs. See Table 1. 5754 600 5000 500 4000 400 Frequency Frequency 6000 3000 300 2000 200 1000 100 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 SpA-Verb dissimilarity score 1 2 3 4 5 6 7 SimVerb similarity score 8 9 10 Figure 4: Score distributio"
2020.lrec-1.705,K15-1026,0,0.0190291,"er, 1995; Fellbaum, 1998), VerbNet (Kipper Schuler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhard). Similarity defined as association, i.e., the mental activation of"
2020.lrec-1.705,J06-3003,0,0.103961,".e., the mental activation of a term when another is presented (Chiarello et al., 1990; Lemaire and Denhiere, 2006), e.g., knife-murder, has been estimated in terms of frequency of co-occurrence of words in language (and the physical world) (Turney, 2001; Turney and Pantel, 2010; McRae et al., 2012; Bruni et al., 2012). In contrast to associative relatedness, a concept of semantic similarity defined in terms of shared superordinate category (Lupker, 1984; Resnik, 1995) (taxonomical similarity (Turney and Pantel, 2010)) or shared semantic features (Tversky, 1977; Frenck-Mestre and Bueno, 1999; Turney, 2006) has been proposed. Here, similarity is quantified in terms of degree of overlap in semantic properties, e.g., shared function or physical features, with synonyms occupying the top region of the similarity scale (e.g. fiddleviolin (Cruse, 1986)). In this work, we reserve the term (semantic) similarity for this latter definition of closeness of meaning, and distinguish it from the more general relatedness, which also includes association, as in previous work (Resnik, 1995; Resnik and Diab, 2000; Agirre et al., 2009; Hill et al., 2015; Gerz et al., 2016). We explore how this distinction is captu"
2020.lrec-1.705,D16-1157,0,0.0274248,"Missing"
2020.repl4nlp-1.7,P19-1070,1,0.874475,"Missing"
2020.repl4nlp-1.7,P19-4007,1,0.895056,"Missing"
2020.repl4nlp-1.7,D18-1330,0,0.113978,"uli´c♦ Anna Korhonen♦ Goran Glavaš♣ ♦ Language Technology Lab, TAL, University of Cambridge ♣ Data and Web Science Group, University of Mannheim {iv250,alk23}@cam.ac.uk goran@informatik.uni-mannheim.de Abstract sources is the main reason for popularity of the socalled projection-based CLWE methods (Mikolov et al., 2013a; Artetxe et al., 2016, 2018a). These models align two independently trained monolingual word vector spaces post-hoc, using limited bilingual supervision in the form of several hundred to several thousand word translation pairs (Mikolov et al., 2013a; Vuli´c and Korhonen, 2016; Joulin et al., 2018; Ruder et al., 2018). Some models even align the monolingual spaces using only identical strings (Smith et al., 2017; Søgaard et al., 2018) or numerals (Artetxe et al., 2017). The most recent work focused on fully unsupervised CLWE induction: they extract seed translation lexicons relying on topological similarities between monolingual spaces (Conneau et al., 2018; Artetxe et al., 2018a; Hoshen and Wolf, 2018; Alaux et al., 2019). In this work, we do not focus on projection itself: rather, we investigate a transformation of input monolingual word vector spaces that facilitates the projection"
2020.repl4nlp-1.7,kamholz-etal-2014-panlex,0,0.0195343,"Missing"
2020.repl4nlp-1.7,N19-1162,0,0.0281812,"rs as well as in different BLI setups and with different CLWE methods. In future work, we will test other unsupervised post-processors, and also probe similar methods that inject external lexical knowledge into monolingual word vectors towards improved BLI. We also plan to probe if similar gains still hold with recently proposed more sophisticated self-learning methods (Karan et al., 2020), non-linear mappingbased CLWE methods (Glavaš and Vuli´c, 2020; Mohiuddin and Joty, 2020). Another idea is to also apply a similar principle to contextualised word representations in cross-lingual settings (Schuster et al., 2019; Liu et al., 2019). Mikel Artetxe, Gorka Labaka, Iñigo Lopez-Gazpio, and Eneko Agirre. 2018c. Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation. In Proceedings of CoNLL, pages 282–291. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the ACL, 5:135–146. Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018. Word translation without parallel data. In Proceedings of ICLR. Yerai Doval, Jose Camacho-Colla"
2020.repl4nlp-1.7,2020.acl-main.618,1,0.838234,"Missing"
2020.repl4nlp-1.7,C12-1089,0,0.0444421,"I setups), and in combination with two different projection methods. 1 Introduction Cross-lingual word embeddings (CLWEs) are a mainstay of modern cross-lingual NLP (Ruder et al., 2019b). CLWE models induce a shared cross-lingual vector space in which words with similar meanings obtain similar vectors regardless of their language. Their usefulness has been attested in tasks such as bilingual lexicon induction (BLI) (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Litschko et al., 2018), machine translation (Artetxe et al., 2018b; Lample et al., 2018), document classification (Klementiev et al., 2012), and many others (Ruder et al., 2019b). Importantly, CLWEs are one of the central mechanisms for facilitating transfer of language technologies for low-resource languages, which often lack sufficient bilingual signal for obvious transfer via machine translation. Lack of language re45 Proceedings of the 5th Workshop on Representation Learning for NLP (RepL4NLP-2020), pages 45–54 c July 9, 2020. 2020 Association for Computational Linguistics formation on both monolingual spaces before any standard projection-based CLWE framework yields consistent BLI gains for a wide array of languages. We run"
2020.repl4nlp-1.7,J98-1004,0,0.762655,"each self-learning iteration k, a dictionary D(k) is first used to learn the joint space (k) (k) Y (k) = XW x ∪ ZW z . The mutual crosslingual nearest neighbours in Y (k) are then used to extract the new dictionary D(k+1) . Relying on mutual nearest neighbours partially removes the noise, leading to better performance. For more technical Unsupervised Monolingual Post-processing. We now outline the simple post-processing method of Artetxe et al. (2018c) used in this work, and then extend it to the bilingual setup. The core idea is to generalise the notion of first-and second-order similarity (Schütze, 1998)2 to nth-order similarity. Let us define the (standard, first-order) similarity matrix of the source language space X as M1 (X) = XX T (similar for Z). The second-order similarity can then be defined as M2 (X) = XX T XX T , where it holds M2 (X) = M1 (M1 (X)); the nth-order similarity is then Mn (X) = (XX T )n . The embeddings of words wi and wj are given by the rows i and j of each Mn matrix. We are then looking for a general linear transformation that adjusts the similarity order of input 1 Recent empirical studies (Glavaš et al., 2019; Vuli´c et al., 2019) show that, under fair evaluation,"
2020.repl4nlp-1.7,K19-1004,1,0.879568,"Missing"
2020.repl4nlp-1.7,D19-1449,1,0.839786,"Missing"
2020.repl4nlp-1.7,P16-1024,1,0.882173,"Missing"
2020.semeval-1.2,P18-1073,0,0.0246037,"traints in target languages by translating EN constraints to target languages via Google Translate. A similar approach of automatic constraint translation has already been proven very effective in the context of symmetric similarity-based specialization of embedding spaces for low-resource languages (Ponti et al., 2019). This way, Wang et al. (2020) obtain an LE-specialized embedding space for each language. Following that, in the second step, they learn a linear projection mapping between the LE-specialized monolingual spaces with the VecMap tool for inducing bilingual word embedding spaces (Artetxe et al., 2018). Recent comparative evaluations (Glavaˇs et al., 2019; Vuli´c et al., 2019a) rendered VecMap as one of the most robust algorithms for inducing cross-lingual embedding spaces. The word translations obtained with Google Translate when translating EN constraints are also forwarded to VecMap as supervision for inducing bilingual embedding spaces. 5 Official Evaluation We now report the official results of our evaluation. We first describe the baselines (Section 5.1) and then show the performances for all submitted runs (Section 5.2).5 5.1 Baselines For the Dist track we use simple cosine similari"
2020.semeval-1.2,I13-1095,0,0.0123381,"-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relation holds between concepts (“To which degree is"
2020.semeval-1.2,Q17-1010,0,0.00755369,"bust algorithms for inducing cross-lingual embedding spaces. The word translations obtained with Google Translate when translating EN constraints are also forwarded to VecMap as supervision for inducing bilingual embedding spaces. 5 Official Evaluation We now report the official results of our evaluation. We first describe the baselines (Section 5.1) and then show the performances for all submitted runs (Section 5.2).5 5.1 Baselines For the Dist track we use simple cosine similarity between distributional word vectors as a baseline. To this end, we use the 300-dimensional FastText embeddings (Bojanowski et al., 2017) trained on Wikipedias of respective languages.6 For the cross-lingual (sub)tasks we induce the bilingual embedding spaces via the simple Procrustes alignment (Smith et al., 2017), using 5K word translation dictionaries, as described in Glavaˇs et al. (2019). Since LE is an asymmetric relation and cosine similarity is a symmetric measure, we did not expect this baseline to be particularly competitive and expected most participants to outperform it. For the Any track, we used GLEN, our recent neural explicit specialization model for LE (Glavaˇs and Vuli´c, 2019) as a competitive baseline. GLEN"
2020.semeval-1.2,S16-1168,0,0.059972,"lly validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014"
2020.semeval-1.2,D15-1075,0,0.0445674,"ded LE prediction. 1 Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degr"
2020.semeval-1.2,P15-2001,0,0.0651471,"Missing"
2020.semeval-1.2,S17-2002,0,0.336887,"we previously created and published (Vuli´c et al., 2017; Vuli´c et al., 2019b), covering four languages (EN, DE, IT, HR) and extended those datasets to two new languages (TR, SQ). For completeness, we describe the details of the annotation process and the creation of final multilingual and cross-lingual datasets for the shared task. Starting Point: Graded LE in English. HyperLex (Vuli´c et al., 2017) comprises 2,616 English (EN) word pairs (2,163 noun pairs and 453 verb pairs) annotated for the graded LE relation. Unlike in symmetric similarity datasets (Hill et al., 2015; Gerz et al., 2016; Camacho-Collados et al., 2017), word order in each pair (X, Y ) is important: this means that pairs (X, Y ) and (Y, X) can obtain drastically different graded LE ratings. The word pairs were first sampled from WordNet to represent a spectrum of different word relations (e.g., hyponymy-hypernymy, meronymy, co-hyponymy, synonymy, antonymy, no relation). The ratings in the [0, 6] interval were then collected through crowdsourcing by posing the graded LE “To what degree is X a type of Y?” question to human subjects, with each pair rated by at least 10 raters: the score of 6 indicates a perfect LE relation between the concepts"
2020.semeval-1.2,D18-1269,0,0.0318256,"n annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014;"
2020.semeval-1.2,N13-1073,0,0.0750154,"resume something similar to be the case with LE and the proposed 4lang graphs – it is inherently difficult to create a reliable LE score based on paths and distances in a symbolic representation that is a (directed) graph. Team UAlberta (Hauer et al., 2020). The approach of UAlberta for cross-lingual binary LE detection combines sentence-level translations (i.e., parallel corpora), distributional word vectors (i.e., word embeddings) and multilingual lexical resources. Their base method, dubbed BITEXT, mines candidates for cross-lingual LE from parallel corpora – they simply run the FastAlign (Dyer et al., 2013) word alignment algorithm and assume that the LE relation holds between all aligned pairs of words. As clarified by the authors, this will, in most cases, extract cross-lingual synonyms, which, strictly speaking, do satisfy the LE relation; also, in some cases, the alignments will be established between close (e.g., first order) hyponymy-hypernymy pairs – in this case, however, the bitext alignment of words alone does not suggest the direction of the LE relation. The authors simply declare any pair of words from our cross-lingual datasets to stand in the LE relation if they find this pair in t"
2020.semeval-1.2,ehrmann-etal-2014-representing,0,0.0286213,"re of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online),"
2020.semeval-1.2,E17-1056,1,0.777636,"all languages and language pairs, for both binary LE detection and graded LE prediction. 1 Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) sugg"
2020.semeval-1.2,N15-1184,0,0.0325169,"Missing"
2020.semeval-1.2,P14-1113,0,0.0244977,"has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 202"
2020.semeval-1.2,P05-1014,0,0.0147749,"Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models an"
2020.semeval-1.2,D16-1235,1,0.900545,"Missing"
2020.semeval-1.2,P18-1004,1,0.899108,"Missing"
2020.semeval-1.2,P19-1476,1,0.837489,"Missing"
2020.semeval-1.2,P19-1070,1,0.894937,"Missing"
2020.semeval-1.2,D17-1185,1,0.900423,"Missing"
2020.semeval-1.2,P16-1193,0,0.0238705,"SQ). We offered two different evaluation tracks. In the distributional (Dist) track we allowed only for fully distributional systems, capturing LE only on the basis of unannotated corpora. In contrast, the Any track invited systems that exploit any kind of additional external resources, including lexico-semantic networks. Overall, we did not observe any empirically confirmed strong systems in the Dist track, further corroborating the findings from prior work that building LE-oriented vectors distributionally is more difficult than for some other relations such as broader semantic relatedness (Henderson and Popa, 2016). However, several runs submitted to the Any track pushed the state of the art both in binary LE detection and graded LE prediction, for most of the languages and language pairs in our evaluation. 2 Data We started from the LE datasets we previously created and published (Vuli´c et al., 2017; Vuli´c et al., 2019b), covering four languages (EN, DE, IT, HR) and extended those datasets to two new languages (TR, SQ). For completeness, we describe the details of the annotation process and the creation of final multilingual and cross-lingual datasets for the shared task. Starting Point: Graded LE in"
2020.semeval-1.2,J15-4004,1,0.896991,"Data We started from the LE datasets we previously created and published (Vuli´c et al., 2017; Vuli´c et al., 2019b), covering four languages (EN, DE, IT, HR) and extended those datasets to two new languages (TR, SQ). For completeness, we describe the details of the annotation process and the creation of final multilingual and cross-lingual datasets for the shared task. Starting Point: Graded LE in English. HyperLex (Vuli´c et al., 2017) comprises 2,616 English (EN) word pairs (2,163 noun pairs and 453 verb pairs) annotated for the graded LE relation. Unlike in symmetric similarity datasets (Hill et al., 2015; Gerz et al., 2016; Camacho-Collados et al., 2017), word order in each pair (X, Y ) is important: this means that pairs (X, Y ) and (Y, X) can obtain drastically different graded LE ratings. The word pairs were first sampled from WordNet to represent a spectrum of different word relations (e.g., hyponymy-hypernymy, meronymy, co-hyponymy, synonymy, antonymy, no relation). The ratings in the [0, 6] interval were then collected through crowdsourcing by posing the graded LE “To what degree is X a type of Y?” question to human subjects, with each pair rated by at least 10 raters: the score of 6 in"
2020.semeval-1.2,W19-4310,1,0.900241,"Missing"
2020.semeval-1.2,P15-2020,1,0.902688,"Missing"
2020.semeval-1.2,S15-1019,0,0.138539,"in more detail the approaches adopted by the three teams who submitted their system description papers.3 Team BMEAUT (Kov´acs et al., 2020). The BMEAUT method for LE detection and prediction is a rule-based approach that exploits Wiktionary definitions (Meyer and Gurevych, 2012) and relies on dependency parsing and semantic graphs. In the first step, the authors apply the dict to 4lang tool (Recski et al., 2016) on Wiktionary definitions of concepts (which can be both unigrams and multi-word expressions, i.e., phrases) in order to induce the directed graphs conforming to the 4lang formalism (Kornai et al., 2015). 4lang graphs are directed graphs with concepts as nodes and three types of edges: 0 0 edges of type 0 denote attribution (cat → − four-legged), lexical entailment (cat → − mammal), or 0 unary predication (cat → − meow); edges of type 1 and 2 denote relations between the predicate and its 1 2 subject and object, respectively (e.g., cat ← − catch → − mouse).4 Kov´acs et al. (2020) first extract definitions from Wiktionary using language-specific templates. Each definition is then transformed into a 4lang graph with the help of a language-specific Universal Dependencies (Nivre et al., 2016) par"
2020.semeval-1.2,P19-1313,0,0.0113187,"a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relation holds between concepts (“To which degree is X a type of Y?”).1 The graded nature of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License."
2020.semeval-1.2,Q15-1016,0,0.0406784,"rediction for SQ) and each language-pair in (3) and (4) (e.g., binary LE detection for HR-TR) instantiates one concrete subtask. We allowed participants to submit their predictions for an arbitrary set of subtasks. Moreover, the participants were allowed to tackle only graded LE prediction or only binary LE detection. Evaluation Metrics. For each graded LE prediction subtask, we measured the alignment of predictions and gold LE scores using the Spearman’s Rank Correlation Coefficient (Spearman ρ), which is in line with previous work on similar concept pair scoring datasets (Hill et al., 2015; Levy et al., 2015; Vuli´c et 27 al., 2017, inter alia). For the binary LE detection subtasks we resorted to the standard F1 measure. 4 Participating Systems We now describe in more detail the approaches adopted by the three teams who submitted their system description papers.3 Team BMEAUT (Kov´acs et al., 2020). The BMEAUT method for LE detection and prediction is a rule-based approach that exploits Wiktionary definitions (Meyer and Gurevych, 2012) and relies on dependency parsing and semantic graphs. In the first step, the authors apply the dict to 4lang tool (Recski et al., 2016) on Wiktionary definitions of"
2020.semeval-1.2,S10-1002,0,0.040472,"egree is X a type of Y?”).1 The graded nature of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluati"
2020.semeval-1.2,W13-0904,0,0.0173071,"c lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relation holds between concepts (“To which degree is X a type of Y?”).1 The graded nature of the"
2020.semeval-1.2,Q17-1022,1,0.903619,"Missing"
2020.semeval-1.2,S13-2005,0,0.0319147,"”).1 The graded nature of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barc"
2020.semeval-1.2,D18-1026,1,0.886103,"Missing"
2020.semeval-1.2,D19-1226,1,0.869316,"Missing"
2020.semeval-1.2,W16-1622,0,0.0342201,"Missing"
2020.semeval-1.2,P18-2057,0,0.0129808,"died in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical entailment from multiple perspectives. Namely, the su"
2020.semeval-1.2,E14-4008,0,0.0287191,"Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared ta"
2020.semeval-1.2,P16-1226,0,0.0129442,"ection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reaso"
2020.semeval-1.2,E17-1007,0,0.0156047,"graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical enta"
2020.semeval-1.2,P06-1101,0,0.114198,"em runs that push state-of-the-art across all languages and language pairs, for both binary LE detection and graded LE prediction. 1 Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vaguen"
2020.semeval-1.2,N18-1056,0,0.334657,".org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical entailment from multiple perspectives. Namely, the subtasks covered both monolingual and cross-lingual setups as well as both binary LE detection and graded LE prediction (i.e., prediction of a degree to which LE holds between concepts"
2020.semeval-1.2,N18-1103,1,0.907335,"Missing"
2020.semeval-1.2,J17-4004,1,0.90587,"Missing"
2020.semeval-1.2,N18-1048,1,0.882981,"Missing"
2020.semeval-1.2,D19-1449,1,0.894355,"Missing"
2020.semeval-1.2,P19-1490,1,0.629745,"Missing"
2020.semeval-1.2,N16-1142,0,0.0991043,"http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical entailment from multiple perspectives. Namely, the subtasks covered both monolingual and cross-lingual setups as well as both binary LE detection and graded LE prediction (i.e., prediction of a degree to which LE"
2020.semeval-1.2,2020.semeval-1.31,0,0.482762,"quire a bilingual word embedding space, merely two monolingual word embedding spaces: sets X and Y are obtained by thresholding monolingual word embedding similarities (the threshold value is tuned on the development portions of our LE sets). Finally, all possible pairs (xi , yj ) ∈ X × Y are considered to stand in the LE relation. Finally, in the third run, the authors couple the bitext-based FastText aligner with the BABEL A LIGN algorithm, which aligns concepts across languages based on BabelNet (Navigli and Ponzetto, 2012), a massively multilingual lexico-semantic network. Team SHIKEBLCU (Wang et al., 2020). The approach of Wang et al. (2020) extends the wellestablished line of work based on specializing (i.e., fine-tuning) distributional word vectors for lexical relations, be it symmetric semantic similarity (Faruqui et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c et al., 2018; Glavaˇs and Vuli´c, 2018; Ponti et al., 2018) or the asymmetric LE relation (Vuli´c and Mrkˇsi´c, 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019; Vuli´c et al., 2019b), using constraints from external lexico-semantic resources like WordNet for supervision. At the core of the approach is the Lexical-Entailment Attract Re"
2020.semeval-1.2,C14-1212,0,0.0213805,"ordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this"
2020.semeval-1.2,N18-1101,0,0.0246649,"Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relat"
2021.acl-long.410,Q17-1010,0,0.0713222,"nt mapping-based approach (Mikolov et al., 2013a; Smith et al., 2017) with the V EC M AP framework (Artetxe et al., 2018). We run main BLI evaluations for 10 language pairs spanning EN, DE, RU, FI, TR.7 Word Vocabularies and Baselines. We extract decontextualized type-level WEs in each language both from the original BERTs (termed BERT- REG)4 and the L EX F IT-ed BERT models for exactly the same vocabulary. Following Vuli´c et al. (2020), the vocabularies cover the top 100K most frequent words represented in the respective fastText (FT) vectors, trained on lowercased monolingual Wikipedias by Bojanowski et al. (2017).5 The equivalent vocabulary coverage allows for a direct comparison of all WEs regardless of the induction/extraction method; this also includes the FT Task 3: Lexical Relation Prediction (RELP). We assess the usefulness of lexical knowledge in WEs to learn relation classifiers for standard lexical relations (i.e., synonymy, antonymy, hypernymy, meronymy, plus no relation) via a state-ofthe-art neural model for RELP which learns solely based on input type-level WEs (Glavaš and Vuli´c, 2018). We use the WordNet-based evaluation data of Glavaš and Vuli´c (2018) for EN, DE, ES; they contain 10K"
2021.acl-long.410,2020.acl-main.431,0,0.547689,"ll pipeline for obtaining decontextualized word representations, based on lexically fine-tuning pretrained LMs via dual-encoder networks (Step 1, §2.1), and then extracting the representations from their (fine-tuned) layers (Step 2, §2.2). Introduction Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017). This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkši´c et al., 2017; Lauscher et al., 2020). Our work takes inspiration from the methods to correc"
2021.acl-long.410,P13-1133,0,0.0173955,"LI fall under similarity-based evaluation tasks (Ruder et al., 2019). sults demonstrate that the inexpensive lexical finetuning procedure can indeed turn large pretrained LMs into effective decontextualized word encoders, and this can be achieved for a reasonably wide spectrum of languages for which such pretrained LMs exist. What is more, L EX F IT for all nonEN languages has been run with noisy automatically translated lexical constraints, which holds promise to support even stronger static L EX F ITbased WEs with human-curated data in the future, e.g., extracted from multilingual WordNets (Bond and Foster, 2013), PanLex (Kamholz et al., 2014), or BabelNet (Ehrmann et al., 2014). The results give rise to additional general implications. First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vuli´c et al., 2020); the role of L EX F IT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision. To further investigate the ‘rewiring’ hypothesis, in §4.1, we also run L EX F IT with a drastically reduced amount of external knowledge. BERT- REG vectors display larg"
2021.acl-long.410,D18-2029,0,0.136669,"Missing"
2021.acl-long.410,2020.conll-1.17,0,0.026315,"g decontextualized word representations, based on lexically fine-tuning pretrained LMs via dual-encoder networks (Step 1, §2.1), and then extracting the representations from their (fine-tuned) layers (Step 2, §2.2). Introduction Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017). This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkši´c et al., 2017; Lauscher et al., 2020). Our work takes inspiration from the methods to correct these distortions and"
2021.acl-long.410,2020.emnlp-main.367,0,0.0318772,"0.24 to 0.60 for FI ; BLI scores for EN - FI rise from 0.21 to 0.37), it cannot match the absolute performance peaks of L EX F IT-ed monolingual BERTs. Storing the knowledge of 100+ languages in its limited parameter budget, mBERT still cannot capture monolingual knowledge as accurately as language-specific BERTs (Conneau et al., 2020). However, we believe that its performance with L EX F IT may be further improved by leveraging recently proposed multilingual LM adaptation strategies that mitigate a mismatch between shared multilingual and language-specific vocabularies (Artetxe et al., 2020; Chung et al., 2020; Pfeiffer et al., 2020); we leave this for future work. Layerwise Averaging. A consensus in prior work (Tenney et al., 2019; Ethayarajh, 2019; Vuli´c et al., 2020) points that out-of-context lexical knowledge in pretrained LMs is typically stored in bottom Transformer layers (see Table 5). However, Table 5 also reveals that this does not hold after L EX F ITing: the tuned model requires knowledge from all layers to extract effective decontextualized WEs and reach peak task scores. Effectively, this means 11 When sampling all reduced sets, we again deliberately excluded all words occurring in"
2021.acl-long.410,2020.acl-main.747,0,0.116681,"Missing"
2021.acl-long.410,N19-1423,0,0.259071,"cal tasks, also directly questioning the usefulness of traditional WE models in the era of large neural models. 1 Word embedding extraction BERT LexFit loss u 1. SOFTMAX 2. MNEG 3. MSIM w v Pooling Pooling BERT BERT (w, v) = (dormant, asleep) Step 1: Lexical ﬁne-tuning Figure 1: Illustration of the full pipeline for obtaining decontextualized word representations, based on lexically fine-tuning pretrained LMs via dual-encoder networks (Step 1, §2.1), and then extracting the representations from their (fine-tuned) layers (Step 2, §2.2). Introduction Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than aut"
2021.acl-long.410,ehrmann-etal-2014-representing,0,0.112194,". sults demonstrate that the inexpensive lexical finetuning procedure can indeed turn large pretrained LMs into effective decontextualized word encoders, and this can be achieved for a reasonably wide spectrum of languages for which such pretrained LMs exist. What is more, L EX F IT for all nonEN languages has been run with noisy automatically translated lexical constraints, which holds promise to support even stronger static L EX F ITbased WEs with human-curated data in the future, e.g., extracted from multilingual WordNets (Bond and Foster, 2013), PanLex (Kamholz et al., 2014), or BabelNet (Ehrmann et al., 2014). The results give rise to additional general implications. First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vuli´c et al., 2020); the role of L EX F IT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision. To further investigate the ‘rewiring’ hypothesis, in §4.1, we also run L EX F IT with a drastically reduced amount of external knowledge. BERT- REG vectors display large gains over FT vectors in tasks such as RELP and LexSIMP, again hi"
2021.acl-long.410,D19-1006,0,0.449664,"the era of large neural models. 1 Word embedding extraction BERT LexFit loss u 1. SOFTMAX 2. MNEG 3. MSIM w v Pooling Pooling BERT BERT (w, v) = (dormant, asleep) Step 1: Lexical ﬁne-tuning Figure 1: Illustration of the full pipeline for obtaining decontextualized word representations, based on lexically fine-tuning pretrained LMs via dual-encoder networks (Step 1, §2.1), and then extracting the representations from their (fine-tuned) layers (Step 2, §2.2). Introduction Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vuli´"
2021.acl-long.410,N15-1184,0,0.0368031,"or static WEs. In particular, the process known as semantic specialization (or retrofitting) injects information about lexical relations from databases like WordNet (Beckwith et al., 1991) or the Paraphrase Database (Ganitkevitch et al., 2013) into WEs. Thus, it accentuates relationships of pure semantic similarity in the re5269 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5269–5283 August 1–6, 2021. ©2021 Association for Computational Linguistics fined representations (Faruqui et al., 2015; Mrkši´c et al., 2017; Ponti et al., 2019, inter alia). Our goal is to create representations that take advantage of both 1) the expressivity and lexical knowledge already stored in pretrained language models (LMs) and 2) the precision of lexical finetuning. To this effect, we develop L EX F IT, a versatile lexical fine-tuning framework, illustrated in Figure 1, drawing a parallel with universal sentence encoders like SentenceBERT (Reimers and Gurevych, 2019).1 Our working hypothesis, extensively evaluated in this paper, is as follows: pretrained encoders store a wealth of lexical knowledge,"
2021.acl-long.410,N13-1092,0,0.115153,"Missing"
2021.acl-long.410,D16-1235,1,0.882037,"Missing"
2021.acl-long.410,2020.acl-main.247,0,0.0241888,"FTMAX SOFTMAX Table 4: LexSIMP results (Accuracy ×100). BERTs. However, there are differences across their task performance: the ranking-based MNEG and MSIM variants display stronger performance on similarity-based ranking lexical tasks such as LSIM and BLI. The classification-based SOFTMAX objective is, as expected, better aligned with the RELP task, and we note slight gains with its ternary variant which leverages extra antonymy knowledge. This finding is well aligned with the recent findings demonstrating that task-specific pretraining results in stronger (sentence-level) task performance (Glass et al., 2020; Henderson et al., 2020; Lewis et al., 2020). In our case, we show that task-specific lexical fine-tuning can reshape the underlying LM’s parameters to not only act as a universal word encoder, but also towards a particular lexical task. The per-epoch time measurements from Table 1 validate the efficiency of L EX F IT as a post-training fine-tuning procedure. Previous approaches that attempted to inject lexical information (i.e., word senses and relations) into large LMs (Lauscher et al., 2020; Levine et al., 2020) relied on joint LM (re)training from scratch: it is effectively costlier than"
2021.acl-long.410,P15-2011,1,0.825395,"age.8 Task 4: Lexical Simplification (LexSIMP) aims to automatically replace complex words (i.e., specialized terms, less-frequent words) with their simpler in-context synonyms, while retaining grammaticality and conveying the same meaning as the more complex input text (Paetzold and Specia, 2017). Therefore, discerning between semantic similarity (e.g., synonymy injected via L EX F IT) and broader relatedness is critical for LexSIMP (Glavaš and Vuli´c, 2018). We adopt the standard LexSIMP evaluation protocol used in prior research on static WEs (Ponti et al., 2018, 2019). 1) We use Light-LS (Glavaš and Štajner, 2015), a languageagnostic LexSIMP tool that makes simplifications in an unsupervised way based solely on word similarity in an input (static) WE space; 2) we rely on standard LexSIMP benchmarks, available for EN (Horn et al., 2014), IT (Tonelli et al., 2016), and ES (Saggion, 2017); and 3) we report the standard Accuracy scores (Horn et al., 2014).9 Important Disclaimer. We note that the main purpose of the chosen evaluation tasks and experimental protocols is not necessarily achieving state-ofthe-art performance, but rather probing the vectors in different lexical tasks requiring different types o"
2021.acl-long.410,N18-2029,1,0.905965,"Missing"
2021.acl-long.410,P18-1004,1,0.888961,"Missing"
2021.acl-long.410,P19-1070,1,0.911229,"Missing"
2021.acl-long.410,2020.findings-emnlp.196,1,0.890486,"Missing"
2021.acl-long.410,J15-4004,1,0.91951,"a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017). This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkši´c et al., 2017; Lauscher et al., 2020). Our work takes inspiration from the methods to correct these distortions and complement the distributional signal with structured information, which were originally devised for static WEs. In particular, the process known as semantic specialization (or retrofitting) injects information about lexical relations from databases like WordNet (Beckwith et al., 1991) or the Paraphrase Database (Gan"
2021.acl-long.410,P14-2075,0,0.0258555,"eaning as the more complex input text (Paetzold and Specia, 2017). Therefore, discerning between semantic similarity (e.g., synonymy injected via L EX F IT) and broader relatedness is critical for LexSIMP (Glavaš and Vuli´c, 2018). We adopt the standard LexSIMP evaluation protocol used in prior research on static WEs (Ponti et al., 2018, 2019). 1) We use Light-LS (Glavaš and Štajner, 2015), a languageagnostic LexSIMP tool that makes simplifications in an unsupervised way based solely on word similarity in an input (static) WE space; 2) we rely on standard LexSIMP benchmarks, available for EN (Horn et al., 2014), IT (Tonelli et al., 2016), and ES (Saggion, 2017); and 3) we report the standard Accuracy scores (Horn et al., 2014).9 Important Disclaimer. We note that the main purpose of the chosen evaluation tasks and experimental protocols is not necessarily achieving state-ofthe-art performance, but rather probing the vectors in different lexical tasks requiring different types of lexical knowledge,10 and offering fair and insightful comparisons between different L EX F IT variants, as well as against standard static WEs (fastText) and non-tuned BERT-based static WEs. 4 Results and Discussion The main"
2021.acl-long.410,2021.eacl-main.10,0,0.0341408,"ss diverse languages in controlled evaluations, thus directly questioning the practical usefulness of the traditional WE models in modern NLP. Besides inducing better static WEs for lexical tasks, following the line of lexical probing work (Ethayarajh, 2019; Vuli´c et al., 2020), our goal in this work was to understand how (and how much) lexical semantic knowledge is coded in pretrained LMs, and how to ‘unlock’ the knowledge from the LMs. We hope that our work will be beneficial for all lexical tasks where static WEs from traditional WE models are still largely used (Schlechtweg et al., 2020; Kaiser et al., 2021). Despite the extensive experiments, we only scratched the surface, and can indicate a spectrum of future enhancements to the proof-of-concept L EX F IT framework beyond the scope of this work. We will test other dual-encoder loss functions, including finer-grained relation classification tasks (e.g., in the SOFTMAX variant), and hard (instead of random) negative examples (Wieting et al., 2015; Mrkši´c et al., 2017; Lauscher et al., 2020; Kalantidis et al., 2020). While in this work, for simplicity and efficiency, we focused on fully decontextualized ISO setup (see §2.2), we will also probe al"
2021.acl-long.410,kamholz-etal-2014-panlex,0,0.011838,"valuation tasks (Ruder et al., 2019). sults demonstrate that the inexpensive lexical finetuning procedure can indeed turn large pretrained LMs into effective decontextualized word encoders, and this can be achieved for a reasonably wide spectrum of languages for which such pretrained LMs exist. What is more, L EX F IT for all nonEN languages has been run with noisy automatically translated lexical constraints, which holds promise to support even stronger static L EX F ITbased WEs with human-curated data in the future, e.g., extracted from multilingual WordNets (Bond and Foster, 2013), PanLex (Kamholz et al., 2014), or BabelNet (Ehrmann et al., 2014). The results give rise to additional general implications. First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vuli´c et al., 2020); the role of L EX F IT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision. To further investigate the ‘rewiring’ hypothesis, in §4.1, we also run L EX F IT with a drastically reduced amount of external knowledge. BERT- REG vectors display large gains over FT vectors in task"
2021.acl-long.410,2020.coling-main.118,1,0.90184,"Missing"
2021.acl-long.410,2021.emnlp-main.109,1,0.821601,"Missing"
2021.acl-long.410,K19-1004,1,0.90017,"Missing"
2021.acl-long.410,P17-1163,0,0.0702013,"Missing"
2021.acl-long.410,Q17-1022,1,0.929258,"Missing"
2021.acl-long.410,N15-1100,0,0.0289612,"diversity of the selection. The final test languages are English (EN), German (DE), Spanish (ES), Finnish (FI), Italian (IT), Polish (PL), Russian (RU), and Turkish (TR). For comparability across languages, we use monolingual uncased BERT Base models for all languages (N = 12 Transformer layers, 12 attention heads, hidden layer dimensionality is 768), available (see the appendix) via the HuggingFace repository (Wolf et al., 2020). External Lexical Knowledge. We use the standard collection of EN lexical constraints from previous work on (static) word vector specialization (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018; Ponti et al., 2018, 2019). It covers the lexical relations from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009); it comprises 1,023,082 synonymy (Psyn ) word pairs and 380,873 antonymy pairs (Pant ). For all other languages, we rely on non-curated noisy lexical constraints, obtained via an automatic word translation method by Ponti et al. (2019); see the original work for the details of the translation procedure. L EX F IT: Technical Details. The implementation is based on the SBERT framework (Reimers and Gurevych, 2019), using the suggested settings: AdamW"
2021.acl-long.410,D18-1026,1,0.887916,"Missing"
2021.acl-long.410,D19-1226,1,0.877527,"Missing"
2021.acl-long.410,D19-1410,0,0.221705,"rence on Natural Language Processing, pages 5269–5283 August 1–6, 2021. ©2021 Association for Computational Linguistics fined representations (Faruqui et al., 2015; Mrkši´c et al., 2017; Ponti et al., 2019, inter alia). Our goal is to create representations that take advantage of both 1) the expressivity and lexical knowledge already stored in pretrained language models (LMs) and 2) the precision of lexical finetuning. To this effect, we develop L EX F IT, a versatile lexical fine-tuning framework, illustrated in Figure 1, drawing a parallel with universal sentence encoders like SentenceBERT (Reimers and Gurevych, 2019).1 Our working hypothesis, extensively evaluated in this paper, is as follows: pretrained encoders store a wealth of lexical knowledge, but it is not straightforward to extract that knowledge. We can expose this knowledge by rewiring their parameters through lexical fine-tuning, and turn the LMs into universal (decontextualized) word encoders. Compared to prior attempts at injecting lexical knowledge into large LMs (Lauscher et al., 2020), our L EX F IT method is innovative as it is deployed post-hoc on top of already pretrained LMs, rather than requiring joint multi-task training. Moreover, L"
2021.acl-long.410,2020.tacl-1.54,0,0.031043,"demonstrate the usefulness of L EX F IT: we report large gains over WEs extracted from vanilla LMs and over traditional WE models across 8 languages and 4 lexical tasks, even with very limited and noisy external lexical knowledge, validating the rewiring hypothesis. The code is available at: https://github.com/cambridgeltl/lexfit. 2 From Language Models to (Decontextualized) Word Encoders The motivation for this work largely stems from the recent work on probing and analyzing pretrained language models for various types of knowledge they might implicitly store (e.g., syntax, world knowledge) (Rogers et al., 2020). Here, we focus on their lexical semantic knowledge (Vuli´c et al., 2020; Liu et al., 2021), with an aim of extracting high-quality static word embeddings from the parameters of the input LMs. In what follows, we describe lexical fine-tuning via dual-encoder networks (§2.1), followed by the WE extraction pro1 These approaches are connected as they are both trained via contrastive learning on dual-encoder architectures, but they provide representations for a different granularity of meaning. cess from the fine-tuned layers of pretrained LMs (§2.2), see Figure 1. 2.1 L EX F IT: Methodology Our"
2021.acl-long.410,2021.acl-long.243,1,0.826374,"Missing"
2021.acl-long.410,2020.semeval-1.1,0,0.0402676,"trum of lexical tasks across diverse languages in controlled evaluations, thus directly questioning the practical usefulness of the traditional WE models in modern NLP. Besides inducing better static WEs for lexical tasks, following the line of lexical probing work (Ethayarajh, 2019; Vuli´c et al., 2020), our goal in this work was to understand how (and how much) lexical semantic knowledge is coded in pretrained LMs, and how to ‘unlock’ the knowledge from the LMs. We hope that our work will be beneficial for all lexical tasks where static WEs from traditional WE models are still largely used (Schlechtweg et al., 2020; Kaiser et al., 2021). Despite the extensive experiments, we only scratched the surface, and can indicate a spectrum of future enhancements to the proof-of-concept L EX F IT framework beyond the scope of this work. We will test other dual-encoder loss functions, including finer-grained relation classification tasks (e.g., in the SOFTMAX variant), and hard (instead of random) negative examples (Wieting et al., 2015; Mrkši´c et al., 2017; Lauscher et al., 2020; Kalantidis et al., 2020). While in this work, for simplicity and efficiency, we focused on fully decontextualized ISO setup (see §2.2),"
2021.acl-long.410,K15-1026,0,0.0675206,"Missing"
2021.acl-long.410,P18-1072,1,0.903042,"Missing"
2021.acl-long.410,2020.cl-4.5,1,0.903622,"Missing"
2021.acl-long.410,N18-1048,1,0.933808,"Missing"
2021.acl-long.447,D19-1607,0,0.0452479,"Missing"
2021.acl-long.447,2020.emnlp-main.40,0,0.467333,"widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially improve model performance of the target language with negligible annotation costs (Garrette and Baldridge, 2013; Hedderich et al., 2020). In this work, however, we demonst"
2021.acl-long.447,2020.emnlp-main.369,0,0.514216,"widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially improve model performance of the target language with negligible annotation costs (Garrette and Baldridge, 2013; Hedderich et al., 2020). In this work, however, we demonst"
2021.acl-long.447,P19-1493,0,0.364814,"s, we make our sampled few shots publicly available.1 1 Introduction Multilingual pretrained encoders like multilingual BERT (mBERT; Devlin et al. (2019)) and XLMR (Conneau et al., 2020) are the top performers in crosslingual tasks such as natural language inference (Conneau et al., 2018), document classification (Schwenk and Li, 2018; Artetxe and Schwenk, 2019), and argument mining (ToledoRonen et al., 2020). They enable transfer learning through language-agnostic representations in crosslingual setups (Hu et al., 2020). A widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English f"
2021.acl-long.447,2020.acl-main.467,0,0.054412,"Missing"
2021.acl-long.447,P19-1459,0,0.0154384,"he clock didn’t even work one minute ... Visually, however, very nice.”) Pretrained multilingual encoders are shown to learn and store “language-agnostic” features (Pires et al., 2019; Zhao et al., 2020); §5.3 shows that source-training mBERT on EN substantially benefits other languages, even for difficult semantic tasks like PAWSX. Conditioning on such languageagnostic features, we expect that the buckets should lead to good understanding and reasoning capabilities for a target language. However, plain few-shot finetuning still relies heavily on unintended shallow lexical cues and shortcuts (Niven and Kao, 2019; Geirhos et al., 2020) that generalize poorly. Other open research questions for future work arise: How do we overcome this excessive reliance on lexical features? How can we leverage language-agnostic features with few shots? Our standardized buckets, baseline results, and analyses are the initial step towards researching and answering these questions. 5.5 Target-Adapting Methods SotA few-shot learning methods (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020; Dhillon et al., 2020) from computer vision consist of two stages: 1) training on base-class images, and 2) few-shot finetuning"
2021.acl-long.447,P19-1015,0,0.0280507,"in up to 40 typologically diverse languages (cf., Appendix §B). 4.1 Datasets and Selection of Few Shots For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets. We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to as a bucket. We set N equal to the number of labels |T |. Following Wang et al. (2020), we sample 40 buckets for each target (i.e., non-English) language of a task to get a reliable estimation of model performance. CLS Tasks. For MLDoc and MARC, e"
2021.acl-long.447,N18-1101,0,0.0283863,"l selection in this stage. 4 Experimental Setup We consider three types of tasks requiring varying degrees of semantic and syntactic knowledge transfer: Sequence classification (CLS), namedentity recognition (NER), and part-of-speech tagging (POS) in up to 40 typologically diverse languages (cf., Appendix §B). 4.1 Datasets and Selection of Few Shots For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets. We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to a"
2021.acl-long.447,D19-1077,0,0.12812,"019), and argument mining (ToledoRonen et al., 2020). They enable transfer learning through language-agnostic representations in crosslingual setups (Hu et al., 2020). A widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially im"
2021.acl-long.447,2020.repl4nlp-1.16,0,0.269393,"dapting 40 times using different 1-shot buckets in German (DE) and Spanish (ES). Second, for a fixed 1-shot bucket, we repeat the same experiment 40 times using random seeds in {0 . . . 39}. Figure 1 presents the dev set performance distribution of the 40 runs with 40 random seeds (top) and 40 1-shot buckets (bottom). With exactly the same training data, using different random seeds yields a 1–2 accuracy difference of FS-XLT (Figure 1 top). A similar phenomenon has been observed in finetuning monolingual encoders (Dodge et al., 2020) and multilingual encoders with ZS-XLT (Keung et al., 2020a; Wu and Dredze, 2020b; Xia et al., 2020); we show this observation also holds for FS-XLT. The key takeaway is that varying the buckets is a more severe problem. It causes much larger variance (Figure 1 bottom): The maximum accuracy difference is ≈6 for DE MARC and ≈10 for ES MLDoc. This can be due to the fact that difficulty of individual examples varies in a dataset (Swayamdipta et al., 2020), resulting in different amounts of information encoded in buckets. This large variance could be an issue when comparing different few-shot learning algorithms. The bucket choice is a strong confounding factor that may obscu"
2021.acl-long.447,2020.emnlp-main.362,0,0.385788,"dapting 40 times using different 1-shot buckets in German (DE) and Spanish (ES). Second, for a fixed 1-shot bucket, we repeat the same experiment 40 times using random seeds in {0 . . . 39}. Figure 1 presents the dev set performance distribution of the 40 runs with 40 random seeds (top) and 40 1-shot buckets (bottom). With exactly the same training data, using different random seeds yields a 1–2 accuracy difference of FS-XLT (Figure 1 top). A similar phenomenon has been observed in finetuning monolingual encoders (Dodge et al., 2020) and multilingual encoders with ZS-XLT (Keung et al., 2020a; Wu and Dredze, 2020b; Xia et al., 2020); we show this observation also holds for FS-XLT. The key takeaway is that varying the buckets is a more severe problem. It causes much larger variance (Figure 1 bottom): The maximum accuracy difference is ≈6 for DE MARC and ≈10 for ES MLDoc. This can be due to the fact that difficulty of individual examples varies in a dataset (Swayamdipta et al., 2020), resulting in different amounts of information encoded in buckets. This large variance could be an issue when comparing different few-shot learning algorithms. The bucket choice is a strong confounding factor that may obscu"
2021.acl-long.447,2020.findings-emnlp.29,0,0.094877,"Missing"
2021.acl-long.447,2020.emnlp-main.608,0,0.0774621,"Missing"
2021.acl-long.447,D19-1382,0,0.0227255,"tic and syntactic knowledge transfer: Sequence classification (CLS), namedentity recognition (NER), and part-of-speech tagging (POS) in up to 40 typologically diverse languages (cf., Appendix §B). 4.1 Datasets and Selection of Few Shots For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets. We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to as a bucket. We set N equal to the number of labels |T |. Following Wang et al. (2020), we sample 40 buckets for"
2021.acl-long.447,2020.emnlp-main.660,0,0.0765103,"Missing"
2021.acl-long.447,N18-1109,0,0.0145892,"e task instead of between different tasks. Few-shot learning was first explored in computer vision (Miller et al., 2000; Fei-Fei et al., 2006; Koch et al., 2015); the aim there is to learn new concepts with only few images. Methods like prototypical networks (Snell et al., 2017) and modelagnostic meta-learning (MAML; Finn et al. (2017)) have also been applied to many monolingual (typically English) NLP tasks such as relation classification (Han et al., 2018; Gao et al., 2019), namedentity recognition (Hou et al., 2020a), word sense disambiguation (Holla et al., 2020), and text classification (Yu et al., 2018; Yin, 2020; Yin et al., 2020; Bansal et al., 2020; Gupta et al., 2020). However, recent few-shot learning methods in computer vision consisting of two simple finetuning stages, first on base-class images and then on new-class few shots, have been shown to outperform MAML and achieve SotA scores (Wang et al., 2020; Chen et al., 2020; Tian et al., 2020; Dhillon et al., 2020). Inspired by this work, we compare various fewshot finetuning methods from computer vision in the context of FS-XLT. Task Performance Variance. Deep neural networks’ performance on NLP tasks is bound to exhibit large varian"
2021.acl-long.447,N19-1131,0,0.034028,"Missing"
2021.acl-long.541,P98-1013,0,0.268885,"semanticsyntactic properties, provide a mapping between the verbs’ senses and the morpho-syntactic realisation of their arguments (Jackendoff, 1992; Levin, 1993). The potential of verb classifications lies in their predictive power: for any given verb, a set of rich semantic-syntactic properties can be inferred based on its class membership. In this work, we explicitly harness this rich linguistic knowledge to aid pretrained LMs in capturing regularities in the properties of verbs and their arguments. We select two major English lexical databases – VerbNet (Kipper Schuler, 2005) and FrameNet (Baker et al., 1998) – as sources of verb knowledge at the semantic-syntactic interface, each representing a different lexical framework. 6953 VerbNet (VN) (Kipper Schuler, 2005; Kipper et al., 2006), the largest available verb-focused lexicon, organises verbs into classes based on the overlap in their semantic properties and syntactic behaviour; it builds on the premise that a verb’s predicateargument structure informs its meaning (Levin, 1993). Each entry provides a set of thematic roles and selectional preferences for the verbs’ arguments; it also lists the syntactic contexts characteristic for the class membe"
2021.acl-long.541,2020.acl-main.463,0,0.0134269,"ased encoders, pretrained with self-supervised language modeling (LM) objectives, form the backbone of state-of-the-art models for most NLP tasks (Devlin et al., 2019; Yang et al., 2019b; Liu et al., 2019). Recent probes showed that they implicitly extract a non-negligible amount of linguistic knowledge from text corpora in an unsupervised fashion (Hewitt and Manning, 2019; Vuli´c et al., 2020; Rogers et al., 2020, inter alia). In downstream tasks, however, they often rely on spurious correlations and superficial cues (Niven and Kao, 2019) rather than a deep understanding of language meaning (Bender and Koller, 2020), which is detrimental to both generalisation and interpretability (McCoy et al., 2019). In this work, we focus on a specific facet of linguistic knowledge: reasoning about events.1 Identifying tokens in the text that mention events and classifying the temporal and causal relations among them is crucial to understand the structure of a story or dialogue (Carlson et al., 2002; Miltsakaki et al., 2004) and to ground a text in real-world facts. Verbs (with their arguments) are prominently used for expressing events (with their participants). Thus, fine-grained knowledge about verbs, e.g., the syn"
2021.acl-long.541,W11-4606,0,0.0169673,", It freed him of guilt), there exists a subset of verbs participating in a syntactic frame NP V NP S_ING (‘free-80-1’), within which there exists an even more constrained subset of verbs appearing with prepositional phrases headed specifically by the preposition from (e.g., The scientist purified the water from bacteria). 3 For instance, descriptions of transactions will include the same frame elements Buyer, Seller, Goods, Money in most languages. Indeed, English FN has inspired similar projects in other languages: e.g., Spanish (Subirats and Sato, 2004), Japanese (Ohara, 2012), and Danish (Bick, 2011). Adapter Architecture. Instead of directly finetuning all parameters of the pretrained Transformer, we opt for storing verb knowledge in a separate set of adapter parameters, keeping the verb knowledge 4 We also experimented with sentence-level tasks: we fed (a) pairs of sentence examples from VN/FN in a binary classification setup (e.g., Jackie leads Rose to the store. – Jackie escorts Rose.); and (b) individual sentences in a multi-class classification setup (predicting the correct VN class/FN frame). These variants, however, led to weaker performance. 6954 separate from the general languag"
2021.acl-long.541,Q17-1010,0,0.0124165,"data scarcity. MAVEN also demonstrates that even the state-of-the-art Transformer models fail to yield satisfying event detection performance in the general domain. The fact that it is unlikely to expect datasets of similar size for other event extraction tasks and especially for other languages only emphasises the need for external event-related knowledge and transfer learning approaches, such as the ones introduced in this work. Semantic Specialisation. Representation spaces induced through self-supervised objectives from large corpora, be it the word embedding spaces (Mikolov et al., 2013; Bojanowski et al., 2017) or those spanned by LM-pretrained Transformers (Devlin et al., 2019; Liu et al., 2019), encode only distributional knowledge. A large body of work focused on semantic specialisation of such distributional spaces by injecting lexico-semantic knowledge from external resources (e.g., WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010) or ConceptNet (Liu and Singh, 2004)) in the form of lexical constraints (Faruqui et al., 2015; Mrkši´c et al., 2017; Glavaš and Vuli´c, 2018b; Kamath et al., 2019; Vuli´c et al., 2021). Joint specialisation models (Yu and Dredze, 2014; Lauscher et al.,"
2021.acl-long.541,P17-1038,0,0.0238949,"raph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and bootstrapping for training data generation (Ferguson et al., 2018; Wang et al., 2019). The recent release of the large English event detection dataset MAVEN (Wang et al., 2020c), with annotations of event triggers only, partially remedies for English data scarcity. MAVEN also demonstrates that even the state-of-the-art Transformer models fail to yield satisfying event detection performance in the general domain. The fact that it is unlikely to expect datasets of similar size for other event extraction tasks and especially for other languages only em"
2021.acl-long.541,P15-1017,0,0.0313724,"ns capable of making fine-grained predictions in the face of data scarcity. Traditional event extraction methods relied on hand-crafted, language-specific features (Ahn, 2006; Gupta and Ji, 2009; Llorens et al., 2010; Hong et al., 2011; Li et al., 2013; Glavaš and Šnajder, 2015) (e.g., POS tags, entity knowledge), which limited their generalisation ability and effectively prevented language transfer. More recent approaches commonly resorted to word embedding input and neural text encoders such as recurrent nets (Nguyen et al., 2016; Duan et al., 2017; Sha et al., 2018) and convolutional nets (Chen et al., 2015; Nguyen and Grishman, 2015), 6959 as well as graph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through"
2021.acl-long.541,2020.acl-main.747,0,0.041835,"do not exist for a vast majority of languages. Given the inherent cross-lingual nature of verb classes and semantic frames (see English (EN) Spanish (ES) Chinese (ZH) Arabic (AR) VerbNet FrameNet 181,882 96,300 60,365 70,278 57,335 36,623 21,815 24,551 Table 1: Number of positive verb pairs in English, and in each target language obtained via VTRANS (§2.4). §2.1), we investigate the potential for verb knowledge transfer from English to target languages, without any manual target-language adjustments. Massively multilingual LMs, such as multilingual BERT (mBERT) (Devlin et al., 2019) or XLMR (Conneau et al., 2020) have become the de facto standard mechanisms for zero-shot (ZS) crosslingual transfer. In our first transfer approach: we fine-tune mBERT first on the English verb knowledge, then on English task data, and then simply make task predictions for the target language input. The second approach, dubbed VTRANS, is inspired by the work on cross-lingual transfer of semantic specialisation for static word embeddings (Glava´s et al., 2019; Ponti et al., 2019; Wang et al., 2020b). In brief (with full details in Appendix C), starting from a set of positive pairs from English VN/FN, VTRANS involves three"
2021.acl-long.541,D19-2007,1,0.874739,"Missing"
2021.acl-long.541,P18-1048,0,0.0213021,"atures (Ahn, 2006; Gupta and Ji, 2009; Llorens et al., 2010; Hong et al., 2011; Li et al., 2013; Glavaš and Šnajder, 2015) (e.g., POS tags, entity knowledge), which limited their generalisation ability and effectively prevented language transfer. More recent approaches commonly resorted to word embedding input and neural text encoders such as recurrent nets (Nguyen et al., 2016; Duan et al., 2017; Sha et al., 2018) and convolutional nets (Chen et al., 2015; Nguyen and Grishman, 2015), 6959 as well as graph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and bootstrapping for training data generation (Ferg"
2021.acl-long.541,C16-1114,0,0.0203051,"verbs) may trigger the same type of event, and conversely, the same word (verb) can evoke differ5 We provide more details about the frameworks and their corresponding annotation schemes in Appendix A. 6 E.g., in the sentence: “The rules can also affect small businesses, which sometimes pay premiums tied to employees’ health status and claims history.”, affect and pay are event triggers of type STATE and OCCURRENCE, respectively. 7 The ACE annotations distinguish 34 trigger types (e.g., Business:Merge-Org, Justice:Trial-Hearing, Conflict:Attack) and 35 argument roles. Following previous work (Hsi et al., 2016), we conflate eight time-related argument roles - e.g., ‘Time-At-End’, ‘Time-Before’, ‘Time-At-Beginning’ - into a single ‘Time’ role in order to alleviate training data sparsity. ent types of event schemata depending on the context. Adopting these tasks for evaluation thus tests whether leveraging fine-grained curated knowledge of verbs’ semantic-syntactic behaviour can improve pretrained LMs’ reasoning about event-triggering predicates and their arguments. Model Configurations. For each task, we compare the performance of the underlying “vanilla” BERT-based model (see §2.3) against its varia"
2021.acl-long.541,D18-1330,0,0.0375193,"Missing"
2021.acl-long.541,D17-1206,0,0.0158938,"edge 4 We also experimented with sentence-level tasks: we fed (a) pairs of sentence examples from VN/FN in a binary classification setup (e.g., Jackie leads Rose to the store. – Jackie escorts Rose.); and (b) individual sentences in a multi-class classification setup (predicting the correct VN class/FN frame). These variants, however, led to weaker performance. 6954 separate from the general language knowledge acquired in pretraining. This (1) allows downstream training to flexibly combine the two sources of knowledge, and (2) bypasses the issues with catastrophic forgetting and interference (Hashimoto et al., 2017; de Masson d&apos;Autume et al., 2019). We adopt the standard efficient adapter architecture of Pfeiffer et al. (2020a,c). In each Transformer layer l, we insert a single adapter (Adapterl ) after the feed-forward sub-layer. The adapter itself is a two-layer feed-forward neural network with a residual connection, consisting of a down-projection D ∈ Rh×m , a GeLU activation (Hendrycks and Gimpel, 2016), and an upprojection U ∈ Rm×h , where h is the hidden size of the Transformer model and m is the dimensionality of the adapter: Adapterl (hl , rl ) = Ul (GeLU(Dl (hl ))) + rl ; where rl is the residu"
2021.acl-long.541,W19-4310,1,0.899186,"Missing"
2021.acl-long.541,2020.deelio-1.5,1,0.869674,"Missing"
2021.acl-long.541,K19-1061,0,0.0389188,"Missing"
2021.acl-long.541,miltsakaki-etal-2004-penn,0,0.027807,"s et al., 2020, inter alia). In downstream tasks, however, they often rely on spurious correlations and superficial cues (Niven and Kao, 2019) rather than a deep understanding of language meaning (Bender and Koller, 2020), which is detrimental to both generalisation and interpretability (McCoy et al., 2019). In this work, we focus on a specific facet of linguistic knowledge: reasoning about events.1 Identifying tokens in the text that mention events and classifying the temporal and causal relations among them is crucial to understand the structure of a story or dialogue (Carlson et al., 2002; Miltsakaki et al., 2004) and to ground a text in real-world facts. Verbs (with their arguments) are prominently used for expressing events (with their participants). Thus, fine-grained knowledge about verbs, e.g., the syntactic patterns in which they partake and the semantic frames, may help pretrained encoders to achieve a deeper understanding of text and improve their performance in event-oriented downstream tasks. There already exist some expert-curated computational resources that organise verbs into classes based on their syntactic-semantic properties (Jackendoff, 1992; Levin, 1993). In particular, here we consi"
2021.acl-long.541,2020.tacl-1.54,0,0.0262688,"knowledge. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when relying on noisily translated lexical knowledge. 1 Introduction Large Transformer-based encoders, pretrained with self-supervised language modeling (LM) objectives, form the backbone of state-of-the-art models for most NLP tasks (Devlin et al., 2019; Yang et al., 2019b; Liu et al., 2019). Recent probes showed that they implicitly extract a non-negligible amount of linguistic knowledge from text corpora in an unsupervised fashion (Hewitt and Manning, 2019; Vuli´c et al., 2020; Rogers et al., 2020, inter alia). In downstream tasks, however, they often rely on spurious correlations and superficial cues (Niven and Kao, 2019) rather than a deep understanding of language meaning (Bender and Koller, 2020), which is detrimental to both generalisation and interpretability (McCoy et al., 2019). In this work, we focus on a specific facet of linguistic knowledge: reasoning about events.1 Identifying tokens in the text that mention events and classifying the temporal and causal relations among them is crucial to understand the structure of a story or dialogue (Carlson et al., 2002; Miltsakaki et"
2021.acl-long.541,S13-2001,0,0.0809782,"of BEARING with an additional participant, a BOWL. 6952 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6952–6969 August 1–6, 2021. ©2021 Association for Computational Linguistics We hypothesise that complementing pretrained LMs with verb knowledge should benefit model performance in downstream tasks that involve event extraction and processing. We first put this hypothesis to the test in English monolingual event identification and classification tasks from the TempEval (UzZaman et al., 2013) and ACE (Doddington et al., 2004) datasets. We report modest but consistent improvements in the former, and significant performance boosts in the latter, thus verifying that verb knowledge is indeed paramount for a deeper understanding of events and their structure. Moreover, expert-curated resources are not available for most of the languages spoken worldwide. Therefore, we also investigate the effectiveness of transferring verb knowledge across languages; in particular, from English to Spanish, Arabic and Chinese. The results demonstrate the success of the transfer techniques, and also shed"
2021.acl-long.541,C08-3012,0,0.14324,"Missing"
2021.acl-long.541,N18-1048,1,0.897103,"Missing"
2021.acl-long.541,D17-1270,1,0.878618,"Missing"
2021.acl-long.541,2021.acl-long.410,1,0.836059,"Missing"
2021.acl-long.541,2020.emnlp-main.586,1,0.842521,"Missing"
2021.acl-long.541,D19-1585,0,0.0136948,"r. More recent approaches commonly resorted to word embedding input and neural text encoders such as recurrent nets (Nguyen et al., 2016; Duan et al., 2017; Sha et al., 2018) and convolutional nets (Chen et al., 2015; Nguyen and Grishman, 2015), 6959 as well as graph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and bootstrapping for training data generation (Ferguson et al., 2018; Wang et al., 2019). The recent release of the large English event detection dataset MAVEN (Wang et al., 2020c), with annotations of event triggers only, partially remedies for English data scarcity. MAVEN also demonstrates that"
2021.acl-long.541,2021.findings-acl.121,0,0.0656224,"Missing"
2021.acl-long.541,2020.semeval-1.31,0,0.455707,"l target-language adjustments. Massively multilingual LMs, such as multilingual BERT (mBERT) (Devlin et al., 2019) or XLMR (Conneau et al., 2020) have become the de facto standard mechanisms for zero-shot (ZS) crosslingual transfer. In our first transfer approach: we fine-tune mBERT first on the English verb knowledge, then on English task data, and then simply make task predictions for the target language input. The second approach, dubbed VTRANS, is inspired by the work on cross-lingual transfer of semantic specialisation for static word embeddings (Glava´s et al., 2019; Ponti et al., 2019; Wang et al., 2020b). In brief (with full details in Appendix C), starting from a set of positive pairs from English VN/FN, VTRANS involves three steps: (1) automatic translation of verbs in each pair into the target language, (2) filtering of the noisy target language pairs by means of a transferred relation prediction model trained on the English examples, and (3) training the verb adapters injected into the pretrained model, now with the translated and filtered target-language verb pairs. For the monolingual target-language FN-/VN-Adapter training, we follow the protocol used for English, see §2.2. 3 Experim"
2021.acl-long.541,N19-1105,0,0.0867627,"dden state, output of the subsequent layer normalisation. 2.3 Downstream Fine-Tuning for Event Tasks The next step is downstream fine-tuning for event processing tasks. We experiment with (1) tokenlevel event trigger identification and classification and (2) span extraction for event triggers and arguments (a sequence labeling task); see §3. For the former, we mount a classification head – a simple single-layer feed-forward softmax regression classifier – on top of the Transformer augmented with VN-/FN-Adapters. For the latter, we follow the architecture from prior work (M’hamdi et al., 2019; Wang et al., 2019) and add a CRF layer (Lafferty et al., 2001) on top of the sequence of Transformer’s outputs (for subword tokens). For all tasks, we propose and evaluate two different fine-tuning regimes: (1) full fine-tuning, where we update both the original Transformer’s parameters and VN-/FN-Adapters (see 2a in Figure 1); and (2) task-adapter (TA) fine-tuning, where we keep both Transformer’s original parameters and VN/FN-Adapters frozen, while stacking a new trainable task adapter on top of the VN-/FN-Adapter in each Transformer layer (see 2b in Figure 1). 2.4 Cross-Lingual Transfer Creation of curated r"
2021.acl-long.541,2020.emnlp-main.129,0,0.280266,"l target-language adjustments. Massively multilingual LMs, such as multilingual BERT (mBERT) (Devlin et al., 2019) or XLMR (Conneau et al., 2020) have become the de facto standard mechanisms for zero-shot (ZS) crosslingual transfer. In our first transfer approach: we fine-tune mBERT first on the English verb knowledge, then on English task data, and then simply make task predictions for the target language input. The second approach, dubbed VTRANS, is inspired by the work on cross-lingual transfer of semantic specialisation for static word embeddings (Glava´s et al., 2019; Ponti et al., 2019; Wang et al., 2020b). In brief (with full details in Appendix C), starting from a set of positive pairs from English VN/FN, VTRANS involves three steps: (1) automatic translation of verbs in each pair into the target language, (2) filtering of the noisy target language pairs by means of a transferred relation prediction model trained on the English examples, and (3) training the verb adapters injected into the pretrained model, now with the translated and filtered target-language verb pairs. For the monolingual target-language FN-/VN-Adapter training, we follow the protocol used for English, see §2.2. 3 Experim"
2021.acl-long.541,Q15-1025,0,0.0310524,"two verbs belong to the same VN class or FN frame. We extract training instances from FN and VN independently. This allows for a separate analysis of the impact of verb knowledge from each resource. We generate positive training instances by extracting all unique verb pairings from the set of members of each main VN class/FN frame (e.g., walk–march), resulting in 181,882 instances created from VN and 57,335 from FN. We then generate k = 3 negative examples per positive example by combining controlled and random sampling. In controlled sampling, we follow prior work on semantic specialisation (Wieting et al., 2015; Glavaš and Vuli´c, 2018b; Lauscher et al., 2020b). For each positive example p = (w1 , w2 ) in the training batch B, we create two negatives pˆ1 = (w ˆ1 , w2 ) and pˆ2 = (w1 , w ˆ2 ); w ˆ1 is the verb from batch B other than w1 that is closest to w2 in terms of their cosine similarity in an auxiliary static word embedding space Xaux ∈ Rd ; conversely, w ˆ2 is the verb from B other than w2 closest to w1 . We additionally create one negative instance pˆ3 = (w ˆ1 ,w ˆ2 ) by randomly sampling w ˆ1 and w ˆ2 from batch B, not considering w1 and w2 . We ensure that the negatives are not present in"
2021.acl-long.541,D19-1582,0,0.0124374,"relied on hand-crafted, language-specific features (Ahn, 2006; Gupta and Ji, 2009; Llorens et al., 2010; Hong et al., 2011; Li et al., 2013; Glavaš and Šnajder, 2015) (e.g., POS tags, entity knowledge), which limited their generalisation ability and effectively prevented language transfer. More recent approaches commonly resorted to word embedding input and neural text encoders such as recurrent nets (Nguyen et al., 2016; Duan et al., 2017; Sha et al., 2018) and convolutional nets (Chen et al., 2015; Nguyen and Grishman, 2015), 6959 as well as graph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and boot"
2021.acl-long.541,P19-1522,0,0.103672,"ore the utility of verb adapters for event extraction in other languages: we investigate 1) zero-shot language transfer with multilingual Transformers and 2) transfer via (noisy automatic) translation of English verb-based lexical knowledge. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when relying on noisily translated lexical knowledge. 1 Introduction Large Transformer-based encoders, pretrained with self-supervised language modeling (LM) objectives, form the backbone of state-of-the-art models for most NLP tasks (Devlin et al., 2019; Yang et al., 2019b; Liu et al., 2019). Recent probes showed that they implicitly extract a non-negligible amount of linguistic knowledge from text corpora in an unsupervised fashion (Hewitt and Manning, 2019; Vuli´c et al., 2020; Rogers et al., 2020, inter alia). In downstream tasks, however, they often rely on spurious correlations and superficial cues (Niven and Kao, 2019) rather than a deep understanding of language meaning (Bender and Koller, 2020), which is detrimental to both generalisation and interpretability (McCoy et al., 2019). In this work, we focus on a specific facet of linguistic knowledge: reas"
2021.acl-long.541,P14-2089,0,0.036386,"et al., 2013; Bojanowski et al., 2017) or those spanned by LM-pretrained Transformers (Devlin et al., 2019; Liu et al., 2019), encode only distributional knowledge. A large body of work focused on semantic specialisation of such distributional spaces by injecting lexico-semantic knowledge from external resources (e.g., WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010) or ConceptNet (Liu and Singh, 2004)) in the form of lexical constraints (Faruqui et al., 2015; Mrkši´c et al., 2017; Glavaš and Vuli´c, 2018b; Kamath et al., 2019; Vuli´c et al., 2021). Joint specialisation models (Yu and Dredze, 2014; Lauscher et al., 2020b; Levine et al., 2020, inter alia) train the representation space from scratch on the large corpus, but augment the selfsupervised training objective with an additional objective based on external lexical constraints. Lauscher et al. (2020b) add to the Masked LM (MLM) and next sentence prediction (NSP) pretraining objectives of BERT (Devlin et al., 2019) an objective that predicts pairs of (near-)synonyms, aiming to improve word-level semantic similarity in BERT’s representation space. In a similar vein, Levine et al. (2020) add the objective that predicts WordNet super"
2021.acl-long.541,Y18-1097,0,0.00983903,"s (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and bootstrapping for training data generation (Ferguson et al., 2018; Wang et al., 2019). The recent release of the large English event detection dataset MAVEN (Wang et al., 2020c), with annotations of event triggers only, partially remedies for English data scarcity. MAVEN also demonstrates that even the state-of-the-art Transformer models fail to yield satisfying event detection performance in the general domain. The fact that it is unlikely to expect datasets of similar size for other event extraction tasks and especially for other languages only emphasises the"
2021.acl-short.72,W19-1909,0,0.0585707,"Missing"
2021.acl-short.72,Q19-1038,0,0.0287965,"AP), a technique to fine-tune B ERT on phraselevel synonyms extracted from the Unified Medical Language System (UMLS; Bodenreider 2004).1 Their S AP B ERT model currently holds state-of-theart (SotA) across all major English biomedical entity linking (BEL) datasets. However, this approach is not widely applicable to other languages: abundant external resources are available only for a few languages, hindering the development of domainspecific NLP models in all other languages. Simultaneously, exciting breakthroughs in crosslingual transfer for language understanding tasks have been achieved (Artetxe and Schwenk, 2019; Hu et al., 2020). However, it remains unclear whether such transfer techniques can be used to improve domain-specific NLP applications and mitigate the gap between knowledge-enhanced models in resource-rich versus resource-poor languages. In this paper, we thus investigate the current performance gaps in the BEL task beyond English, and propose several cross-lingual transfer techniques to improve domain-specialised representations and BEL in resource-lean languages. In particular, we first present a novel crosslingual BEL (XL - BEL) task and its corresponding evaluation benchmark in 10 typol"
2021.acl-short.72,2020.emnlp-main.253,1,0.890817,"rt. 2 Methodology Learning Background and Related Work. biomedical entity representations is at the core of BioNLP, benefiting, e.g., relational knowledge discovery (Wang et al., 2018) and literature search (Lee et al., 2016). In the current era of contextualised representations based on Transformer architectures (Vaswani et al., 2017), biomedical text encoders are pretrained via Masked Language Modelling (MLM) on diverse biomedical texts such as PubMed articles (Lee et al., 2020; Gu et al., 2020), clinical notes (Peng et al., 2019; Alsentzer et al., 2019), and even online health forum posts (Basaldella et al., 2020). However, it has been empirically verified that naively applying MLMpretrained models as entity encoders does not perform well in tasks such as biomedical entity linking (Basaldella et al., 2020; Sung et al., 2020). Recently, Liu et al. (2021) proposed S AP (SelfAlignment Pretraning), a fine-tuning method that leverages synonymy sets extracted from UMLS to improve B ERT’s ability to act as a biomedical entity encoder. Their S AP B ERT model currently achieves SotA scores on all major English BEL benchmarks. In what follows, we first outline the S AP procedure, and then discuss the extension o"
2021.acl-short.72,W19-5403,0,0.0578786,"Missing"
2021.acl-short.72,2020.coling-main.118,1,0.788893,"Missing"
2021.acl-short.72,2020.acl-main.423,0,0.0348157,"nowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data. 1 Introduction Recent work has demonstrated that it is possible to combine the strength of 1) Transformer-based encoders such as B ERT (Devlin et al., 2019; Liu et al., 2019), pretrained on large general-domain data with 2) external linguistic and world knowledge (Zhang et al., 2019; Levine et al., 2020; Lauscher et al., 2020). Such expert human-curated knowledge is crucial for NLP applications in specialised domains such as biomedicine. There, Liu et al. (2021) recently proposed self-alignment pretraining (S AP), a technique to fine-tune B ERT on phraselevel synonyms extracted from the Unified Medical Language System (UMLS; Bodenreider 2004).1 Their S AP B ERT model currently holds state-of-theart (SotA) across all major English biomedical entity linking (BEL) datasets. However, this approach is not widely applicable to other languages: abundant external resources are available only for a f"
2021.acl-short.72,2021.naacl-main.334,1,0.851504,"get languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data. 1 Introduction Recent work has demonstrated that it is possible to combine the strength of 1) Transformer-based encoders such as B ERT (Devlin et al., 2019; Liu et al., 2019), pretrained on large general-domain data with 2) external linguistic and world knowledge (Zhang et al., 2019; Levine et al., 2020; Lauscher et al., 2020). Such expert human-curated knowledge is crucial for NLP applications in specialised domains such as biomedicine. There, Liu et al. (2021) recently proposed self-alignment pretraining (S AP), a technique to fine-tune B ERT on phraselevel synonyms extracted from the Unified Medical Language System (UMLS; Bodenreider 2004).1 Their S AP B ERT model currently holds state-of-theart (SotA) across all major English biomedical entity linking (BEL) datasets. However, this approach is not widely applicable to other languages: abundant external resources are available only for a few languages, hindering the development of domainspecific NLP models in all other languages. Simultaneously, exciting breakthroughs in crosslingual transfer for l"
2021.acl-short.72,2021.ccl-1.108,0,0.0946509,"Missing"
2021.acl-short.72,I11-1029,0,0.0423863,"our XL - BEL benchmark. The statistics of the benchmark are available in Table 1. We also convert word and phrase translations into the same format (§2.1), where each ‘class’ now contains only two examples. For a translation pair (xp , xq ), we create a unique pseudo-label yxp ,xq and produce two new name-label instances (xp , yxp ,xq ) and (xq , yxp ,xq ),4 and proceed as in §2.1. This allows us to easily combine domainspecific knowledge with general translation knowledge within the same S AP framework. 3 The XL - BEL Task and Evaluation Data A general cross-lingual entity linking (EL) task (McNamee et al., 2011; Tsai and Roth, 2016) aims to map a mention of an entity in free text of any language to a controlled English vocabulary, typically obtained from a knowledge graph (KG). In this work, we propose XL - BEL, a cross-lingual biomedical EL task. Instead of grounding entity mentions to English-specific ontologies, we use UMLS as a language-agnostic KG: the XL - BEL task requires a model to associate a mention in any language to a (language-agnostic) CUI in UMLS. XL - BEL thus serves as an ideal evaluation benchmark for biomedical entity representations: it challenges the capability of both 1) repre"
2021.acl-short.72,W19-5006,0,0.018804,"ta, and pretrained models are available online at: github.com/cambridgeltl/sapbert. 2 Methodology Learning Background and Related Work. biomedical entity representations is at the core of BioNLP, benefiting, e.g., relational knowledge discovery (Wang et al., 2018) and literature search (Lee et al., 2016). In the current era of contextualised representations based on Transformer architectures (Vaswani et al., 2017), biomedical text encoders are pretrained via Masked Language Modelling (MLM) on diverse biomedical texts such as PubMed articles (Lee et al., 2020; Gu et al., 2020), clinical notes (Peng et al., 2019; Alsentzer et al., 2019), and even online health forum posts (Basaldella et al., 2020). However, it has been empirically verified that naively applying MLMpretrained models as entity encoders does not perform well in tasks such as biomedical entity linking (Basaldella et al., 2020; Sung et al., 2020). Recently, Liu et al. (2021) proposed S AP (SelfAlignment Pretraning), a fine-tuning method that leverages synonymy sets extracted from UMLS to improve B ERT’s ability to act as a biomedical entity encoder. Their S AP B ERT model currently achieves SotA scores on all major English BEL benchmarks."
2021.acl-short.72,2020.emnlp-main.365,0,0.0598287,"Missing"
2021.acl-short.72,2020.acl-main.335,0,0.204153,"016). In the current era of contextualised representations based on Transformer architectures (Vaswani et al., 2017), biomedical text encoders are pretrained via Masked Language Modelling (MLM) on diverse biomedical texts such as PubMed articles (Lee et al., 2020; Gu et al., 2020), clinical notes (Peng et al., 2019; Alsentzer et al., 2019), and even online health forum posts (Basaldella et al., 2020). However, it has been empirically verified that naively applying MLMpretrained models as entity encoders does not perform well in tasks such as biomedical entity linking (Basaldella et al., 2020; Sung et al., 2020). Recently, Liu et al. (2021) proposed S AP (SelfAlignment Pretraning), a fine-tuning method that leverages synonymy sets extracted from UMLS to improve B ERT’s ability to act as a biomedical entity encoder. Their S AP B ERT model currently achieves SotA scores on all major English BEL benchmarks. In what follows, we first outline the S AP procedure, and then discuss the extension of the method to include multilingual UMLS synonyms (§2.1), and then introduce another S AP extension which combines domain-specific synonyms with generaldomain translation data (§2.2). Training Examples. Given a min"
2021.acl-short.72,P19-1139,0,0.0265228,"available English knowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data. 1 Introduction Recent work has demonstrated that it is possible to combine the strength of 1) Transformer-based encoders such as B ERT (Devlin et al., 2019; Liu et al., 2019), pretrained on large general-domain data with 2) external linguistic and world knowledge (Zhang et al., 2019; Levine et al., 2020; Lauscher et al., 2020). Such expert human-curated knowledge is crucial for NLP applications in specialised domains such as biomedicine. There, Liu et al. (2021) recently proposed self-alignment pretraining (S AP), a technique to fine-tune B ERT on phraselevel synonyms extracted from the Unified Medical Language System (UMLS; Bodenreider 2004).1 Their S AP B ERT model currently holds state-of-theart (SotA) across all major English biomedical entity linking (BEL) datasets. However, this approach is not widely applicable to other languages: abundant external resources are a"
2021.conll-1.44,2021.eacl-main.140,0,0.027205,"different sentential contexts. CoSimLex (Armendariz et al., 2020) measures the change in similarity between two different words appearing in two different contexts: paragraphs. We follow the standard evaluation protocol, computing the cosine similarity of the contextual word representations and comparing them against human-elicited scores via Spearman’s rank correlation (ρ). The WiC classification task (Pilehvar and Camacho-Collados, 2019) challenges a model to make a binary decision on whether or not the same target word has the same meaning in two different contexts. The WiC-TSV (TSV) task (Breit et al., 2021) extends the original WiC to multiple domains with three different subtasks. In TSV-1, the task is to decide if the intended sense of the target word in the context matches the target sense described by the definition. In TSV-2, the model must identify if the intended sense (in the context) is the hyponym of the provided hypernyms. TSV-3 combines the previous two subtasks (see Breit et al. (2021) for further details). The WSD task (Navigli, 2009; Raganato et al., 2017) requires a system to select the correct label for a given target word in context from a candidate set of all possible meanings"
2021.conll-1.44,D14-1110,0,0.0244753,"C’s effects on embedding properties such as isotropy. ings of M IRRORW I C, and its impact on the contextual representation space. We release our code at github.com/cambridgeltl/MirrorWiC. 2 Related Work and Background Word-in-Context Representations. Modelling context influence on lexical meaning and creating context-aware word representations is a longstanding research goal in lexical semantics. One direction is to create discrete sense embeddings according to a fixed sense inventory such as WordNet. These embeddings can be created from the attributes in the sense inventory such as glosses (Chen et al., 2014) or from the knowledge structure (Camacho-Collados et al., 2016). We point to Camacho-Collados and Pilehvar (2018) for a thorough survey on sense embeddings. Such sense representations require a fixed and discrete sense inventory and might not be sensitive enough to the the dynamic and fluid nature of contextual changes. More recently, PLMs provide dynamic and continuous contextual representations, not tied to predefined sense inventories, computed as a function of both the target word and its context. The use of PLMs has resulted in further progress on a range of context-aware evaluation benc"
2021.conll-1.44,N19-1423,0,0.195405,"e two representations form a positive pair for contrastive fine-tuning. During fine-tuning, we pull the representations of each positive pair closer together, while at the same time pushing away representations of other WiC instances, serving as negative examples. 2020; Pedinotti and Lenci, 2020). As a consequence, they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/feature space (Garí Soler et al., 2019; Wiedemann et al., 2019; Reif et al., 2019; Garí Soler and Apidianaki, 2021). While off-the-shelf PLMs already offer a useful contextualised lexical semantic space, their contextualised representation spaces suffer from instability and anisotropy (Mickus et al., ∗ Due to the fat-tail"
2021.conll-1.44,J13-3003,0,0.0344753,"ever, even if the items in the pair happen to have similar meanings, our learning objective still instructs the model to push them away from each other. Our rationale and decision here are based on the following: (1) Such false negative pairs can act as a regularisation; and (2) in essence, one could argue that all distinct word-in-context instances have slightly different meanings since sense is a continuous function of word and context. 4 Experimental Setup tasks: Usim and CoSimLex; two word-in-context classification tasks: WiC and WiC-TSV; and oneshot Word Sense Disambiguation (WSD). Usim (Erk et al., 2013) measures the similarity between two instances of the same word occurring in two different sentential contexts. CoSimLex (Armendariz et al., 2020) measures the change in similarity between two different words appearing in two different contexts: paragraphs. We follow the standard evaluation protocol, computing the cosine similarity of the contextual word representations and comparing them against human-elicited scores via Spearman’s rank correlation (ρ). The WiC classification task (Pilehvar and Camacho-Collados, 2019) challenges a model to make a binary decision on whether or not the same tar"
2021.conll-1.44,D19-1006,0,0.0403436,"Missing"
2021.conll-1.44,2021.emnlp-main.552,0,0.0564678,"Missing"
2021.conll-1.44,W19-0423,0,0.0266549,"they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/feature space (Garí Soler et al., 2019; Wiedemann et al., 2019; Reif et al., 2019; Garí Soler and Apidianaki, 2021). While off-the-shelf PLMs already offer a useful contextualised lexical semantic space, their contextualised representation spaces suffer from instability and anisotropy (Mickus et al., ∗ Due to the fat-tailed nature of pandemic risk… However, PLMs have been shown to actually store more lexical and sentence-level information than what can be directly extracted from their offthe-shelf variants. In simple words, this knowledge must be ‘unlocked’ or exposed via additional adaptive fine-tuning (Ruder, 2021). For instance"
2021.conll-1.44,D19-1533,0,0.0166081,"We augment a randomly selected WiC instance with random span masking and apply dropout to the hidden states to create two slightly different representations of the base instance. These two representations form a positive pair for contrastive fine-tuning. During fine-tuning, we pull the representations of each positive pair closer together, while at the same time pushing away representations of other WiC instances, serving as negative examples. 2020; Pedinotti and Lenci, 2020). As a consequence, they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/feature space (Garí Soler et al., 2019; Wiedemann et al., 2019; Reif et al., 2019; Garí Soler and Apidianaki, 2021). While off-the-shelf PLMs a"
2021.conll-1.44,2021.acl-long.197,0,0.062921,"Missing"
2021.conll-1.44,2020.acl-main.423,0,0.187269,"states to create two slightly different representations of the base instance. These two representations form a positive pair for contrastive fine-tuning. During fine-tuning, we pull the representations of each positive pair closer together, while at the same time pushing away representations of other WiC instances, serving as negative examples. 2020; Pedinotti and Lenci, 2020). As a consequence, they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/feature space (Garí Soler et al., 2019; Wiedemann et al., 2019; Reif et al., 2019; Garí Soler and Apidianaki, 2021). While off-the-shelf PLMs already offer a useful contextualised lexical semantic space, their contextualised representation"
2021.conll-1.44,2021.naacl-main.334,1,0.803834,"Missing"
2021.conll-1.44,2021.emnlp-main.109,1,0.847856,"Missing"
2021.conll-1.44,2020.emnlp-main.333,1,0.928285,"ao et al., 2021) based on the contrastive learning paradigm. The fundamental limitation of extracting conEqual contribution. 562 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 562–574 November 10–11, 2021. ©2021 Association for Computational Linguistics j textual features/representations directly from the layers of the off-the-shelf PLMs is the mismatch between their (pre)training objectives and the feature extraction method. In other words, the contextual representations, typically extracted as the averages over the top four layers of a base PLM (Liu et al., 2020; Garí Soler and Apidianaki, 2021), can be seen as a by-product of training a language model, and are not directly optimised for contextual sensitivity. Inspired by the previous work on adaptive fine-tuning for word and sentence representations (Liu et al., 2021b), we propose a simple self-supervised technique termed M IRRORW I C: it rewires input PLMs to provide improved word-incontext (WiC) representations. Unlike prior work on fine-tuning towards improving WiC representations, our M IRRORW I C procedure disposes of any sense labels, annotated task data, and any external knowledge, and elici"
2021.conll-1.44,2021.emnlp-main.571,1,0.82908,"Missing"
2021.conll-1.44,2021.ccl-1.108,0,0.0776108,"Missing"
2021.conll-1.44,P19-1569,0,0.0183662,"More recently, PLMs provide dynamic and continuous contextual representations, not tied to predefined sense inventories, computed as a function of both the target word and its context. The use of PLMs has resulted in further progress on a range of context-aware evaluation benchmarks (Pilehvar and Camacho-Collados, 2019; Wang et al., 2019; Raganato et al., 2020). A body of work has aimed to enrich context-aware and sense information in the PLMs by injecting such knowledge (e.g., sense annotations from predefined sense inventories) at pretraining stage (Levine et al., 2020) or during inference (Loureiro and Jorge, 2019). Other work has attempted at combining/ensembling multiple contextualised and static type-level embeddings to refine the contextualised representation space (Liu et al., 2020; Xu et al., 2020). Inducing Text Representations from PLMs via Self-Supervision. Recently, there has been growing interest in learning completely unsupervised sentence representations from PLMs using conContributions. 1) We present a simple yet ex- trastive learning techniques (Carlsson et al., 2021; tremely effective unsupervised M IRRORW I C tech- Liu et al., 2021b; Gao et al., 2021; Yan et al., 2021; nique for eliciti"
2021.conll-1.44,2020.scil-1.35,0,0.0874921,"Missing"
2021.conll-1.44,2020.coling-main.602,0,0.0154305,"M IRROR W I C method, based on contrastive learning, for eliciting better word-in-context (WiC) representations from pretrained language models. We augment a randomly selected WiC instance with random span masking and apply dropout to the hidden states to create two slightly different representations of the base instance. These two representations form a positive pair for contrastive fine-tuning. During fine-tuning, we pull the representations of each positive pair closer together, while at the same time pushing away representations of other WiC instances, serving as negative examples. 2020; Pedinotti and Lenci, 2020). As a consequence, they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/"
2021.conll-1.44,N19-1128,0,0.0323547,"Missing"
2021.conll-1.44,E17-1010,0,0.0271206,"a binary decision on whether or not the same target word has the same meaning in two different contexts. The WiC-TSV (TSV) task (Breit et al., 2021) extends the original WiC to multiple domains with three different subtasks. In TSV-1, the task is to decide if the intended sense of the target word in the context matches the target sense described by the definition. In TSV-2, the model must identify if the intended sense (in the context) is the hyponym of the provided hypernyms. TSV-3 combines the previous two subtasks (see Breit et al. (2021) for further details). The WSD task (Navigli, 2009; Raganato et al., 2017) requires a system to select the correct label for a given target word in context from a candidate set of all possible meanings for this target word. To evaluate the feature space of the models in WSD, we create a one-shot setting where we provide one context example3 per label and perform nearest neighbour search over contextual word representations from the candidate labels. We directly test the models on the concatenated ALL test set from Raganato et al. (2017) without access to training and development data. We also perform multilingual and cross-lingual evaluation on XL-WiC (Raganato et a"
2021.conll-1.44,2020.emnlp-main.584,0,0.0500662,"Missing"
2021.conll-1.44,2021.acl-short.73,0,0.0937812,"Missing"
2021.conll-1.44,D19-1410,0,0.288673,"ic space, their contextualised representation spaces suffer from instability and anisotropy (Mickus et al., ∗ Due to the fat-tailed nature of pandemic risk… However, PLMs have been shown to actually store more lexical and sentence-level information than what can be directly extracted from their offthe-shelf variants. In simple words, this knowledge must be ‘unlocked’ or exposed via additional adaptive fine-tuning (Ruder, 2021). For instance, while off-the-shelf PLMs are not directly effective as universal sentence encoders, it is possible to convert them into such encoders through supervised (Reimers and Gurevych, 2019a; Feng et al., 2020; Liu et al., 2021a) or self-supervised fine-tuning (Carlsson et al., 2021; Liu et al., 2021b; Gao et al., 2021) based on the contrastive learning paradigm. The fundamental limitation of extracting conEqual contribution. 562 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 562–574 November 10–11, 2021. ©2021 Association for Computational Linguistics j textual features/representations directly from the layers of the off-the-shelf PLMs is the mismatch between their (pre)training objectives and the feature extraction method. In other"
2021.conll-1.44,2020.emnlp-main.586,1,0.844463,"Missing"
2021.conll-1.44,2021.acl-long.393,0,0.0325412,"nference (Loureiro and Jorge, 2019). Other work has attempted at combining/ensembling multiple contextualised and static type-level embeddings to refine the contextualised representation space (Liu et al., 2020; Xu et al., 2020). Inducing Text Representations from PLMs via Self-Supervision. Recently, there has been growing interest in learning completely unsupervised sentence representations from PLMs using conContributions. 1) We present a simple yet ex- trastive learning techniques (Carlsson et al., 2021; tremely effective unsupervised M IRRORW I C tech- Liu et al., 2021b; Gao et al., 2021; Yan et al., 2021; nique for eliciting contextual lexical knowledge. Kim et al., 2021; Zhang et al., 2021). Similar to 2) Our experiments on a range of English, mul- the supervised approaches such as Sentence-BERT tilingual, and cross-lingual context-sensitive lex- (Reimers and Gurevych, 2019b) or SapBERT (Liu ical benchmarks demonstrate that M IRRORW I C et al., 2021a), the idea is to transform an input PLM achieves consistent and substantial improvements into an effective sentence encoder via additional over different baseline PLMs, indicating its robust- fine-tuning. During self-supervised contrastive finen"
2021.conll-1.44,2021.acl-long.402,0,0.0262613,"multiple contextualised and static type-level embeddings to refine the contextualised representation space (Liu et al., 2020; Xu et al., 2020). Inducing Text Representations from PLMs via Self-Supervision. Recently, there has been growing interest in learning completely unsupervised sentence representations from PLMs using conContributions. 1) We present a simple yet ex- trastive learning techniques (Carlsson et al., 2021; tremely effective unsupervised M IRRORW I C tech- Liu et al., 2021b; Gao et al., 2021; Yan et al., 2021; nique for eliciting contextual lexical knowledge. Kim et al., 2021; Zhang et al., 2021). Similar to 2) Our experiments on a range of English, mul- the supervised approaches such as Sentence-BERT tilingual, and cross-lingual context-sensitive lex- (Reimers and Gurevych, 2019b) or SapBERT (Liu ical benchmarks demonstrate that M IRRORW I C et al., 2021a), the idea is to transform an input PLM achieves consistent and substantial improvements into an effective sentence encoder via additional over different baseline PLMs, indicating its robust- fine-tuning. During self-supervised contrastive fineness and wide applicability. 3) We offer extensive tuning, the model learns from identical"
2021.eacl-main.76,Q19-1038,0,0.0283117,"k-specific labelled data shortage. To bridge this gap, we combine semi-supervised deep generative models and multi-lingual pretraining to form a pipeline for document classification task. Compared to strong supervised learning baselines, our semi-supervised classification framework is highly competitive and outperforms the state-of-the-art counterparts in lowresource settings across several languages. 1 1 Introduction Multi-lingual pretraining has been shown to effectively use unlabelled data through learning shared representations across languages that can be transferred to downstream tasks (Artetxe and Schwenk, 2019; Devlin et al., 2019; Wu and Dredze, 2019; Conneau and Lample, 2019). Nonetheless, the lack of labelled data still leads to inferior performance of the same model compared to those trained in languages with more labelled data such as English (Zeman et al., 2018; Zhu et al., 2019). Semi-supervised learning is another appealing paradigm that supplements the labelled data with unlabelled data which is easy to acquire (Blum and Mitchell, 1998; Zhou and Li, 2005; McClosky et al., 2006, inter alia). In particular, deep generative models (DGMs) such as variational autoencoder (VAE; Kingma and Wellin"
2021.eacl-main.76,Q17-1010,0,0.0290135,"al., 2017; Alemi et al., 2018). We only run one trial with fixed random seed for both pretraining and document classification. Training details can be found in the Appendix. As our supervised baselines we compare with the following two groups: (I) NXVAE-based supervised models which are pretrained NXVAE encoder with a multi-layer perceptron classifier on top (denoted by NXVAE-z1 (qφ (y|z1 )) or NXVAEh (qφ (y|h)) depending on the representation fed into the classifier; or NXVAE-z1 models initialised with different pretrained embeddings: random initialisation (RAND), mono-lingual fastText (FT; Bojanowski et al. (2017)), unsupervised cross-lingual MUSE (Lample et al., 2018b), pretrained embeddings from Wei and Deng (2017) (PEMB), and our resulting embeddings from pretrained NXVAE (NXEMB).5 (II) We also pretrain a word-based BERT (BERTW) with parameter size akin to NXVAE on the same data, and fine-tune it directly.6 For our semi-supervised experiments, we test https://github.com/google-research/ bert/blob/master/multilingual.md. 3 https://pytorch.org/. 4 https://www.statmt.org/europarl/. 5 All embeddings are pretrained on the same Europarl data. We also trained subword-based models for BERT and NXVAE, and ob"
2021.eacl-main.76,D14-1179,0,0.0501179,"Missing"
2021.eacl-main.76,N06-1020,0,0.156333,"led data through learning shared representations across languages that can be transferred to downstream tasks (Artetxe and Schwenk, 2019; Devlin et al., 2019; Wu and Dredze, 2019; Conneau and Lample, 2019). Nonetheless, the lack of labelled data still leads to inferior performance of the same model compared to those trained in languages with more labelled data such as English (Zeman et al., 2018; Zhu et al., 2019). Semi-supervised learning is another appealing paradigm that supplements the labelled data with unlabelled data which is easy to acquire (Blum and Mitchell, 1998; Zhou and Li, 2005; McClosky et al., 2006, inter alia). In particular, deep generative models (DGMs) such as variational autoencoder (VAE; Kingma and Welling (2014)) are capable of capturing complex data distributions at scale with rich latent representations, and they have been used ∗ Work done while at Microsoft Research Cambridge. Code is available at https://github.com/ cambridgeltl/mling_sdgms. 1 for semi-supervised learning in various tasks in NLP (Xu et al., 2017; Yin et al., 2018; Choi et al., 2019; Xie and Ma, 2019), as well as inducing crosslingual word embeddings (Wei and Deng, 2017), and representation learning in combina"
2021.eacl-main.76,2005.mtsummit-papers.11,0,0.107451,"der (§4.2). All documents are lowercased. We report accuracy for evaluation following Schwenk and Li (2018). For all experiments, We use Adam (Kingma and Ba, 2015) as optimiser, but with different learning rates for both settings and pretraining. We implemented the model with Pytorch3 1.10 (Paszke et al., 2019), and use GeForce GTX 1080Ti GPUs. See the Appendix for details about model configurations and training. 4.1 LSTM Encoder with VAE Pretraining Experimental Setup. For pretraining NXVAE, we use three language pairs: EN - DE, EN - FR and DE FR constructed from Europarl v7 parallel corpus (Koehn, 2005),4 where only two language pairs are available: EN-DE and EN-FR, which consist of four datasets in total: (EN, DE)EN-DE , and (EN, FR)EN-FR . For DE-FR, we pair DE EN-DE and FR EN-FR directly as pseudo parallel data. We trim all datasets into exactly the same sentence size, and preprocess them 2 E 234 238 1000 G 252 266 1030 M 244 268 979 Total 1000 1000 4000 DE 270 229 984 240 268 1026 245 266 1022 245 237 968 1000 1000 4000 FR 227 257 999 262 237 973 258 237 998 253 269 1030 1000 1000 4000 RU 261 265 1073 288 272 1121 184 204 706 267 259 1100 1000 100 4000 ZH 294 324 1169 286 300 1215 109 93"
2021.eacl-main.76,P18-1070,0,0.0200299,"ppealing paradigm that supplements the labelled data with unlabelled data which is easy to acquire (Blum and Mitchell, 1998; Zhou and Li, 2005; McClosky et al., 2006, inter alia). In particular, deep generative models (DGMs) such as variational autoencoder (VAE; Kingma and Welling (2014)) are capable of capturing complex data distributions at scale with rich latent representations, and they have been used ∗ Work done while at Microsoft Research Cambridge. Code is available at https://github.com/ cambridgeltl/mling_sdgms. 1 for semi-supervised learning in various tasks in NLP (Xu et al., 2017; Yin et al., 2018; Choi et al., 2019; Xie and Ma, 2019), as well as inducing crosslingual word embeddings (Wei and Deng, 2017), and representation learning in combination with Transformers via pretraining (Li et al., 2020). To leverage the benefits of both worlds, we propose a pipeline method by combining semisupervised DGMs (SDGMs) based on M1+M2 model (Kingma et al., 2014) with multi-lingual pretraining. The pretrained model serves as multilingual encoder, and SDGMs can operate on top of it independently of encoding architecture. To highlight such independence, we experiment with two pretraining settings: (1"
2021.emnlp-main.109,2020.emnlp-main.253,1,0.647974,"similarities between xi and all other strings besides xi (the negatives).2 3 Experimental Setup Evaluation Tasks: Lexical. We evaluate on domain-general and domain-specific tasks: word similarity and biomedical entity linking (BEL). For the former, we rely on the Multi-SimLex evaluation set (Vuli´c et al., 2020a): it contains human-elicited word similarity scores for multiple languages. For the latter, we use NCBI-disease (NCBI, Do˘gan et al. 2014), BC5CDR-disease, BC5CDR-chemical (BC5-d, BC5-c, Li et al. 2016), AskAPatient (Limsopatham and Collier, 2016) and COMETA (stratified-general split, Basaldella et al. 2020) as our evaluation datasets. The first three datasets are in the scientific domain (i.e., the data have been extracted from scientific papers), while the latter two 2 We also experimented with another state-of-the-art contrastive learning scheme proposed by Liu et al. (2021). There, hard triplet mining combined with multi-similarity loss (MS loss) is used as the learning objective. InfoNCE and triplet mining + MS loss work mostly on par, with slight gains of one variant in some tasks, and vice versa. For simplicity and brevity, we report the results only with InfoNCE. 1444 are in the social me"
2021.emnlp-main.109,S14-2010,0,0.0705321,"Missing"
2021.emnlp-main.109,S16-1081,0,0.0605945,"Missing"
2021.emnlp-main.109,D15-1075,0,0.308897,"2021). 1 Introduction In order to address this gap, recent work has Transfer learning with pretrained Masked Lan- trained dual-encoder networks on labelled exterguage Models (MLMs) such as BERT (Devlin et al., nal resources to convert MLMs into universal lan2019) and RoBERTa (Liu et al., 2019) has been guage encoders. Most notably, Sentence-BERT widely successful in NLP, offering unmatched per- (SBERT, Reimers and Gurevych 2019) further formance in a large number of tasks (Wang et al., trains BERT and RoBERTa on Natural Language 2019a). Despite the wealth of semantic knowledge Inference (NLI, Bowman et al. 2015; Williams et al. stored in the MLMs (Rogers et al., 2020), they do 2018) and sentence similarity data (Cer et al., 2017) not produce high-quality lexical and sentence em- to obtain high-quality universal sentence embedbeddings when used off-the-shelf, without further dings. Recently, SapBERT (Liu et al., 2021) self1442 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1442–1459 c November 7–11, 2021. 2021 Association for Computational Linguistics aligns phrasal representations of the same meaning using synonyms extracted from the UMLS (Bodenreider,"
2021.emnlp-main.109,S12-1051,0,0.0536384,"e. InfoNCE and triplet mining + MS loss work mostly on par, with slight gains of one variant in some tasks, and vice versa. For simplicity and brevity, we report the results only with InfoNCE. 1444 are in the social media domain (i.e., extracted from online forums discussing health-related topics). We report Spearman’s rank correlation coefficients (ρ) for word similarity; accuracy @1/@5 is the standard evaluation measure in the BEL task. Evaluation Tasks: Sentence-Level. Evaluation on the intrinsic sentence textual similarity (STS) task is conducted on the standard SemEval 20122016 datasets (Agirre et al., 2012, 2013, 2014, 2015, 2016), STS Benchmark (STS-b, Cer et al. 2017), SICK-Relatedness (SICK-R, Marelli et al. 2014) for English; STS SemEval-17 data is used for Spanish and Arabic (Cer et al., 2017), and we also evaluate on Russian STS.3 We report Spearman’s ρ rank correlation. Evaluation in the question-answer entailment task is conducted on QNLI (Rajpurkar et al., 2016; Wang et al., 2019b). It contains 110k English QA pairs with binary entailment labels.4 Evaluation Tasks: Cross-Lingual. We also assess the benefits of Mirror-BERT on cross-lingual representation learning, evaluating on cross-li"
2021.emnlp-main.109,S13-1004,0,0.0904508,"Missing"
2021.emnlp-main.109,Q16-1028,0,0.0521718,"orms standard dropout and is even worse than not using dropout at all. As the only difference between controlled and standard dropout is the augmented features for positive pairs in the latter case, this suggests that the gain from +Mirror indeed stems from the data augmentation effect rather than from regularisation. Mirror-BERT Improves Isotropy? (Fig. 7). We argue that the gains with Mirror-BERT largely stem from its reshaping of the embedding space geometry. Isotropy (i.e., uniformity in all orientations) of the embedding space has been a favourable property for semantic similarity tasks (Arora et al., 2016; Mu and Viswanath, 2018). However, Ethayarajh (2019) shows that (off-the-shelf) MLMs’ representations are anisotropic: they reside in a narrow cone in the vector space and the average cosine similarity of (random) data points is extremely high. Sentence embeddings induced from MLMs without fine-tuning thus suffer from spatial anistropy (Li et al., 2020; Su et al., 2021). Is Mirror-BERT then improving isotropy of the embedding space?14 To investigate this claim, we inspect (1) the distributions of cosine similarities and (2) an isotropy score, as defined by Mu and Viswanath (2018). First, we r"
2021.emnlp-main.109,P18-1073,0,0.0275264,"17 data is used for Spanish and Arabic (Cer et al., 2017), and we also evaluate on Russian STS.3 We report Spearman’s ρ rank correlation. Evaluation in the question-answer entailment task is conducted on QNLI (Rajpurkar et al., 2016; Wang et al., 2019b). It contains 110k English QA pairs with binary entailment labels.4 Evaluation Tasks: Cross-Lingual. We also assess the benefits of Mirror-BERT on cross-lingual representation learning, evaluating on cross-lingual word similarity (CLWS, Multi-SimLex is used) and bilingual lexicon induction (BLI). We rely on the standard mapping-based BLI setup (Artetxe et al., 2018), and training and test sets from Glavaš et al. (2019), reporting accuracy @1 scores (with CSLS as the word retrieval method, Lample et al. 2018). Mirror-BERT: Training Resources. For finetuning (general-domain) lexical representations, we use the top 10k most frequent words in each language. For biomedical name representations, we randomly sample 10k names from the UMLS. In sentence-level tasks, for STS, we sample 10k sentences (without labels) from the training set of the STS Benchmark; for Spanish, Arabic and Russian, we sample 10k sentences from the WikiMatrix dataset (Schwenk et al., 2021"
2021.emnlp-main.109,D18-2029,0,0.115897,"Missing"
2021.emnlp-main.109,N19-1423,0,0.116457,"Missing"
2021.emnlp-main.109,ehrmann-etal-2014-representing,0,0.0845129,"shing away the negatives (§2.3). 2.1 Training Data through Self-Duplication The key to success of dual-network representation learning (Henderson et al., 2019; Reimers and Gurevych, 2019; Humeau et al., 2020; Liu et al., 2021, inter alia) is the construction of positive and negative pairs. While negative pairs can be easily obtained from randomly sampled texts, positive pairs usually need to be manually annotated. In practice, they are extracted from labelled task data (e.g., NLI) or knowledge bases that store relations such as synonymy or hypernymy (e.g., PPDB, Pavlick et al. 2015; BabelNet, Ehrmann et al. 2014; WordNet, Fellbaum 1998; UMLS). Mirror-BERT, however, does not rely on any external data to construct the positive examples. In a nutshell, given a set of non-duplicated strings X , we assign individual labels (yi ) to each string and build a dataset D = {(xi , yi )|xi ∈ X , yi ∈ {1, . . . , |X |}}. We then create self-duplicated training data D0 simply by repeating every element in D. In other words, let X = {x1 , x2 , . . .}. We then have D = {(x1 , y1 ), (x2 , y2 ), . . .} and D0 = {(x1 , y1 ), (x1 , y 1 ), (x2 , y2 ), (x2 , y 2 ), . . .} where x1 = x1 , y1 = y 1 , x2 = x2 , y2 = y 2 , . ."
2021.emnlp-main.109,D19-1006,0,0.084111,"dropout at all. As the only difference between controlled and standard dropout is the augmented features for positive pairs in the latter case, this suggests that the gain from +Mirror indeed stems from the data augmentation effect rather than from regularisation. Mirror-BERT Improves Isotropy? (Fig. 7). We argue that the gains with Mirror-BERT largely stem from its reshaping of the embedding space geometry. Isotropy (i.e., uniformity in all orientations) of the embedding space has been a favourable property for semantic similarity tasks (Arora et al., 2016; Mu and Viswanath, 2018). However, Ethayarajh (2019) shows that (off-the-shelf) MLMs’ representations are anisotropic: they reside in a narrow cone in the vector space and the average cosine similarity of (random) data points is extremely high. Sentence embeddings induced from MLMs without fine-tuning thus suffer from spatial anistropy (Li et al., 2020; Su et al., 2021). Is Mirror-BERT then improving isotropy of the embedding space?14 To investigate this claim, we inspect (1) the distributions of cosine similarities and (2) an isotropy score, as defined by Mu and Viswanath (2018). First, we randomly sample 1,000 sentence pairs from the Quora Qu"
2021.emnlp-main.109,2021.emnlp-main.552,0,0.385338,"Missing"
2021.emnlp-main.109,2021.acl-long.72,0,0.0257597,"ry of the most related work. Even prior to the emergence of large pretrained LMs (PLMs), most representation models followed the distributional hypothe- 6 Conclusion sis (Harris, 1954) and exploited the co-occurrence statistics of words/phrases/sentences in large cor- We proposed Mirror-BERT, a simple, fast, selfpora (Mikolov et al., 2013a,b; Pennington et al., supervised, and highly effective approach that trans2014; Kiros et al., 2015; Hill et al., 2016; Lo- forms large pretrained masked language models (MLMs) into universal lexical and sentence engeswaran and Lee, 2018). Recently, DeCLUTR (Giorgi et al., 2021) follows the distributional hy- coders within a minute, and without any external pothesis and formulates sentence embedding train- supervision. Mirror-BERT, based on simple unsupervised data augmentation techniques, demoning as a contrastive learning task where span pairs sampled from the same document are treated as pos- strates surprisingly strong performance in (wordlevel and sentence-level) semantic similarity tasks, itive pairs. Very recently, there has been a growing interest in using individual raw sentences for self- as well as on biomedical entity linking. The large gains over base ML"
2021.emnlp-main.109,N16-1162,1,0.835972,"cross-lingual tasks. Self-supervised text representations have a large body of literature. Here, due to space constraints, we provide a highly condensed summary of the most related work. Even prior to the emergence of large pretrained LMs (PLMs), most representation models followed the distributional hypothe- 6 Conclusion sis (Harris, 1954) and exploited the co-occurrence statistics of words/phrases/sentences in large cor- We proposed Mirror-BERT, a simple, fast, selfpora (Mikolov et al., 2013a,b; Pennington et al., supervised, and highly effective approach that trans2014; Kiros et al., 2015; Hill et al., 2016; Lo- forms large pretrained masked language models (MLMs) into universal lexical and sentence engeswaran and Lee, 2018). Recently, DeCLUTR (Giorgi et al., 2021) follows the distributional hy- coders within a minute, and without any external pothesis and formulates sentence embedding train- supervision. Mirror-BERT, based on simple unsupervised data augmentation techniques, demoning as a contrastive learning task where span pairs sampled from the same document are treated as pos- strates surprisingly strong performance in (wordlevel and sentence-level) semantic similarity tasks, itive pairs. V"
2021.emnlp-main.109,J15-4004,1,0.738393,"BERT: Training Resources. For finetuning (general-domain) lexical representations, we use the top 10k most frequent words in each language. For biomedical name representations, we randomly sample 10k names from the UMLS. In sentence-level tasks, for STS, we sample 10k sentences (without labels) from the training set of the STS Benchmark; for Spanish, Arabic and Russian, we sample 10k sentences from the WikiMatrix dataset (Schwenk et al., 2021). For QNLI, we sample 10k sentences from its training set. Training Setup and Details. The hyperparameters of word-level models are tuned on SimLex-999 (Hill et al., 2015); biomedical models are tuned on COMETA (zero-shot-general split). Sentencelevel models are tuned on the dev set of STS-b. τ in Eq. (1) is 0.04 (biomedical and sentence-level models); 0.2 (word-level). Dropout rate p is 0.1. Sentence-level models use a random span masking 3 github.com/deepmipt/deepPavlovEval We follow the setup of Li et al. (2020) and adapt QNLI to an unsupervised task by computing the AUC scores (on the development set, ≈5.4k pairs) using 0/1 labels and cosine similarity scores of QA embeddings. 4 lang.→ EN FR ET AR ZH RU ES PL avg. fastText .528 .560 .447 .409 .428 .435 .488"
2021.emnlp-main.109,P19-1070,1,0.91152,"Missing"
2021.emnlp-main.109,2021.acl-long.197,0,0.292243,"Missing"
2021.emnlp-main.109,2021.eacl-main.270,1,0.773582,"Missing"
2021.emnlp-main.109,P19-1580,0,0.029665,"Probing the impact of dropout. Table 7: Ablation study: (i) replacing dropout with drophead; (ii) the synergistic effect of dropout and random span masking in the English STS tasks. v1 == v¯ 1 controlled dropout controlled dropout dropout( v1 ) == dropout( v¯ 1 ) Figure 6: Under controlled dropout, if two strings are identical, they will have an identical set of dropout masks throughout the encoding process. other augmentation types work as well? Recent work points out that pretrained MLM are heavily overparameterised and most Transformer heads can be pruned without hurting task performance (Voita et al., 2019; Kovaleva et al., 2019; Michel et al., 2019). Zhou et al. (2020) propose a drophead method: it randomly prunes attention heads at MLM training as a regularisation step. We thus evaluate a variant of Mirror-BERT where the dropout layers are replaced with such dropheads:12 this results in even stronger STS performance, cf. Tab. 7. In short, this hints that the Mirror-BERT framework might benefit from other data and feature augmentation techniques in future work.13 Regularisation or Augmentation? (Tab. 8). When using dropout, is it possible that we are simply observing the effect of adding/remov"
2021.emnlp-main.109,2020.cl-4.5,1,0.887829,"Missing"
2021.emnlp-main.109,2021.acl-long.410,1,0.821601,"Missing"
2021.emnlp-main.109,2020.emnlp-main.586,1,0.883623,"Missing"
2021.emnlp-main.571,D18-1025,0,0.0242468,"system performance. The SemEval7158 2021 shared task MCL-WiC does focus on crosslingual WiC, but covers only five high-resource languages from three language families (English, French, Chinese, Arabic, Russian). Both XL-WiC and MCL-WiC mainly focus on common words and do not include less frequent concepts (e.g., named entities). Further, their language coverage and data availability are heavily skewed towards Indo-European languages. There are several other ‘non-WiC’ datasets designed to evaluate cross-lingual context-aware lexical representations. Bilingual Contextual Word Similarity (BCWS) (Chi and Chen, 2018) challenges a model to predict graded similarity of crosslingual word pairs given sentential context, one in each language. In the Bilingual Token-level Sense Retrieval (BTSR) task (Liu et al., 2019), given a query word in a source language context, a system must retrieve a meaning-equivalent target language word within a target language context.10 However, both BCWS and BTSR are again very restricted in terms of language coverage: BCWS covers only one language pair (EN-ZH), while BTSR contains two pairs (EN-ZH/ES). Further, they provide only test data: as such, they can merely be used as gene"
2021.emnlp-main.571,W16-2501,1,0.796206,"d within a target language context.10 However, both BCWS and BTSR are again very restricted in terms of language coverage: BCWS covers only one language pair (EN-ZH), while BTSR contains two pairs (EN-ZH/ES). Further, they provide only test data: as such, they can merely be used as general intrinsic probes for pretrained models, but cannot support fine-tuning experiments and cannot fully expose the relevance of information available in pretrained models for downstream applications. This is problematic as intrinsic tasks in general do not necessarily correlate well with downstream performance (Chiu et al., 2016; Glavaš et al., 2019). AM2 I C O vs. Entity Linking. Our work is related to the entity linking (EL) task (Rao et al., 2013; Cornolti et al., 2013; Shen et al., 2014) similarly to how the original WiC (based on WordNet knowledge) is related to WSD. EL systems must map entities in context to a predefined knowledge base (KB). While WSD relies on the WordNet sense inventory, the EL task focuses on KBs such as Wikipedia and DBPedia. When each entity mention is mapped to a unique Wiki page, this procedure is termed wikification (Mihalcea and Csomai, 2007). The cross-lingual wikification task (Ji et"
2021.emnlp-main.571,2020.acl-main.747,0,0.527258,"e., it does not support cross-lingual assessments. Further, 4) the current WiC datasets offer low human upper bounds and inflated (even superhuman) performance for some languages.1 This is due to superficial cues where 5) many examples in the current WiC datasets can be resolved relying either on the target word alone without any context or on the context alone, which eludes evaluation honing in on the interplay between target words and their corresponding contexts. In order to address these limitations and provide Pretrained language models (LMs) such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) offer a natural way to distinguish different word meanings in context without performing explicit sense disambiguation. This property of “meaning contextualization” is typically evaluated either via standard entity linking (Rao et al., 2013; Shen et al., 2014) and Word Sense Disambiguation (WSD) tasks (Navigli, 2009; Moro et al., 2014; 1 Raganato et al., 2017) or, recently, via the Word-inIn turn, this might give a false impression that some lanContext (WiC) evaluation paradigm (Pilehvar and guages in the task are ’solved’ by the current pretrained LMs. 7151 Proceedings of the 2021 Conference"
2021.emnlp-main.571,N19-1423,0,0.168088,"lable in different languages, i.e., it does not support cross-lingual assessments. Further, 4) the current WiC datasets offer low human upper bounds and inflated (even superhuman) performance for some languages.1 This is due to superficial cues where 5) many examples in the current WiC datasets can be resolved relying either on the target word alone without any context or on the context alone, which eludes evaluation honing in on the interplay between target words and their corresponding contexts. In order to address these limitations and provide Pretrained language models (LMs) such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) offer a natural way to distinguish different word meanings in context without performing explicit sense disambiguation. This property of “meaning contextualization” is typically evaluated either via standard entity linking (Rao et al., 2013; Shen et al., 2014) and Word Sense Disambiguation (WSD) tasks (Navigli, 2009; Moro et al., 2014; 1 Raganato et al., 2017) or, recently, via the Word-inIn turn, this might give a false impression that some lanContext (WiC) evaluation paradigm (Pilehvar and guages in the task are ’solved’ by the current pretrained LMs. 7151 P"
2021.emnlp-main.571,P19-1070,1,0.872566,"Missing"
2021.emnlp-main.571,N18-2017,0,0.037706,"Missing"
2021.emnlp-main.571,N18-1170,0,0.0535351,"Missing"
2021.emnlp-main.571,D17-1215,0,0.0513424,"Missing"
2021.emnlp-main.571,K19-1004,1,0.870059,"Missing"
2021.emnlp-main.571,Q14-1019,0,0.0357826,"r on the context alone, which eludes evaluation honing in on the interplay between target words and their corresponding contexts. In order to address these limitations and provide Pretrained language models (LMs) such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) offer a natural way to distinguish different word meanings in context without performing explicit sense disambiguation. This property of “meaning contextualization” is typically evaluated either via standard entity linking (Rao et al., 2013; Shen et al., 2014) and Word Sense Disambiguation (WSD) tasks (Navigli, 2009; Moro et al., 2014; 1 Raganato et al., 2017) or, recently, via the Word-inIn turn, this might give a false impression that some lanContext (WiC) evaluation paradigm (Pilehvar and guages in the task are ’solved’ by the current pretrained LMs. 7151 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7151–7162 c November 7–11, 2021. 2021 Association for Computational Linguistics a more comprehensive evaluation framework, we present AM2 I C O (Adversarial and Multilingual Meaning in Context), a novel multilingual and cross-lingual WiC task and resource. It covers a typologi"
2021.emnlp-main.571,S13-2040,0,0.0330702,"emonstrate the challenging nature of AM2 I C O. The results reveal that current SotA pretrained encoders substantially lag behind human performance, and the largest gaps are observed for low-resource languages and languages dissimilar to English. 1 Introduction Camacho-Collados, 2019; Raganato et al., 2020). Although monolingual evaluation in English is still predominant, a need has been recognized to construct similar resources for other languages to support cross-lingual evaluation and model diagnostics. This includes multilingual and crosslingual WSD benchmarks (Navigli and Ponzetto, 2012; Navigli et al., 2013; Scarlini et al., 2020; Barba et al., 2020, inter alia), cross-lingual entity linking (Tsai and Roth, 2016; Raiman and Raiman, 2018; Upadhyay et al., 2018) and, most recently, multilingual WiC (termed XL-WiC) spanning 12 languages (Raganato et al., 2020). This most recent WiC evaluation approach is particularly attractive as 1) it bypasses the dependence on modeling predefined ontologies (entity linking) and explicit sense inventories (WSD), and 2) it is framed as a simple binary classification task: for a target word w appearing in two different contexts c1 and c2 , the system must decide wh"
2021.emnlp-main.571,D12-1128,0,0.0920929,"Missing"
2021.emnlp-main.571,P19-1459,0,0.0209984,"ty of AM2 I C O. For each dataset, 7153 we recruit two annotators who each validate a random sample of 100 examples, where 50 examples are shared between the two samples and are used to compute inter-rater agreement.3 2.2 2.3 Adversarial Examples Another requirement is assessing to which extent models can grasp the meaning of a target word based on the (complex) interaction with its context. However, recently it was shown that SotA pretrained LMs exploit superficial cues while solving language understanding tasks due to spurious correlations seeping into the datasets (Gururangan et al., 2018; Niven and Kao, 2019). This hinders generalizations beyond the particular datasets and makes the models brittle to minor changes in the 3 The annotators were recruited via two crowdsourcing platforms, Prolific and Proz, depending on target language coverage. The annotators were native speakers of the target language, fluent in English, and with an undergraduate degree. 4 E.g., ‘China’ can be linked to the page ‘Republic of China (1912–1949)’ and to the page ‘Empire of China (1915–1916)’. XL-WiC MCL-WiC AM2 I C O 7,466 7,466 4,130 4,130 17 14,510 1,676 7,255 1,201 22.7 3600 2000 2766 2072 26.13 13,074 8,570 9,868 8"
2021.emnlp-main.571,N19-1128,0,0.042815,"ent in other datasets, namely the Georgian alphabet and the Bengali script (a Northern Indian abugida), for a total of 8 distinct scripts. 3 Experimental Setup We now establish a series of baselines on AM2 I C O to measure the gap between current SotA models and human performance. XLM-R6 (Conneau et al., 2020), available in the HuggingFace repository (Wolf et al., 2020). Classification. Given two contextualized representations ei,src and ei,trg for a pair of target words, two setups to make prediction are considered: the first, metric-based, is a non-parametric setup. In particular, we follow Pilehvar and CamachoCollados (2019) and score the distance δ between the representations via cosine similarity. A threshold t from the development set is set via grid search across 0.02 intervals in the [0, 1] interval. Therefore, if δ(ei,src , ei,trg ) ≥ t the pair is classified as negative, and positive otherwise. On the other hand, the fine-tuning setup is parametric: following Raganato et al. (2020), we train a logistic regression classifier that takes the concatenation of the contextualized representations [ei,src ⊕ ei,trg ] as input.7 The entire model (both the encoder and the classifier) is then fine-tuned to minimize th"
2021.emnlp-main.571,2020.emnlp-main.185,1,0.926148,"contextualized representations ei,src and ei,trg for a pair of target words, two setups to make prediction are considered: the first, metric-based, is a non-parametric setup. In particular, we follow Pilehvar and CamachoCollados (2019) and score the distance δ between the representations via cosine similarity. A threshold t from the development set is set via grid search across 0.02 intervals in the [0, 1] interval. Therefore, if δ(ei,src , ei,trg ) ≥ t the pair is classified as negative, and positive otherwise. On the other hand, the fine-tuning setup is parametric: following Raganato et al. (2020), we train a logistic regression classifier that takes the concatenation of the contextualized representations [ei,src ⊕ ei,trg ] as input.7 The entire model (both the encoder and the classifier) is then fine-tuned to minimize the cross-entropy loss of the training set examples with Adam (Kingma and Ba, 2015). We perform grid search for the learning rate in [5e−6, 1e−5, 3e−5], and train for 20 epochs selecting the checkpoint with the best performance on the dev set. Cross-lingual Transfer. In addition to supervised learning, we also carry out cross-lingual transfer experiments where data split"
2021.emnlp-main.571,J19-3005,1,0.863194,"Missing"
2021.emnlp-main.571,E17-1010,0,0.0897936,"e, which eludes evaluation honing in on the interplay between target words and their corresponding contexts. In order to address these limitations and provide Pretrained language models (LMs) such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) offer a natural way to distinguish different word meanings in context without performing explicit sense disambiguation. This property of “meaning contextualization” is typically evaluated either via standard entity linking (Rao et al., 2013; Shen et al., 2014) and Word Sense Disambiguation (WSD) tasks (Navigli, 2009; Moro et al., 2014; 1 Raganato et al., 2017) or, recently, via the Word-inIn turn, this might give a false impression that some lanContext (WiC) evaluation paradigm (Pilehvar and guages in the task are ’solved’ by the current pretrained LMs. 7151 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7151–7162 c November 7–11, 2021. 2021 Association for Computational Linguistics a more comprehensive evaluation framework, we present AM2 I C O (Adversarial and Multilingual Meaning in Context), a novel multilingual and cross-lingual WiC task and resource. It covers a typologically diverse set of 15 la"
2021.emnlp-main.571,2020.emnlp-main.584,0,0.272024,"contextualized representations ei,src and ei,trg for a pair of target words, two setups to make prediction are considered: the first, metric-based, is a non-parametric setup. In particular, we follow Pilehvar and CamachoCollados (2019) and score the distance δ between the representations via cosine similarity. A threshold t from the development set is set via grid search across 0.02 intervals in the [0, 1] interval. Therefore, if δ(ei,src , ei,trg ) ≥ t the pair is classified as negative, and positive otherwise. On the other hand, the fine-tuning setup is parametric: following Raganato et al. (2020), we train a logistic regression classifier that takes the concatenation of the contextualized representations [ei,src ⊕ ei,trg ] as input.7 The entire model (both the encoder and the classifier) is then fine-tuned to minimize the cross-entropy loss of the training set examples with Adam (Kingma and Ba, 2015). We perform grid search for the learning rate in [5e−6, 1e−5, 3e−5], and train for 20 epochs selecting the checkpoint with the best performance on the dev set. Cross-lingual Transfer. In addition to supervised learning, we also carry out cross-lingual transfer experiments where data split"
2021.emnlp-main.571,D19-1077,0,0.0167166,"6.3. However, this is still insufficient to equalize performances across the board, as the latter group of languages continues to lag behind: between DE and UR remains a gap of 7.2 points. We speculate that the reason behind this asymmetry is the fact that in addition to being resource-poor, UR, KK, BN, and KA are also typologically distant from languages where most of the examples are concentrated. Overall, these findings suggest that leveraging multiple sources is better than a single one by virtue of the transfer capabilities of massively multilingual encoders, as previously demonstrated (Wu and Dredze, 2019; Ponti et al., 2021). Few-shot Transfer. To study the differences between training on `s and `t with controlled train data size, we plot the model performance on two target languages (RU and JA) as a function of the amount of available examples across different transfer conditions in Figure 2. Comparing supervised learning (based on target language data) with zeroshot learning (based on DE data), it emerges how the former is always superior if the number of examples is the same. However, zero-shot learning may eventually surpass the peak performance of supervised learning by taking advantage"
2021.emnlp-main.571,2020.lrec-1.723,0,0.0355104,"ging nature of AM2 I C O. The results reveal that current SotA pretrained encoders substantially lag behind human performance, and the largest gaps are observed for low-resource languages and languages dissimilar to English. 1 Introduction Camacho-Collados, 2019; Raganato et al., 2020). Although monolingual evaluation in English is still predominant, a need has been recognized to construct similar resources for other languages to support cross-lingual evaluation and model diagnostics. This includes multilingual and crosslingual WSD benchmarks (Navigli and Ponzetto, 2012; Navigli et al., 2013; Scarlini et al., 2020; Barba et al., 2020, inter alia), cross-lingual entity linking (Tsai and Roth, 2016; Raiman and Raiman, 2018; Upadhyay et al., 2018) and, most recently, multilingual WiC (termed XL-WiC) spanning 12 languages (Raganato et al., 2020). This most recent WiC evaluation approach is particularly attractive as 1) it bypasses the dependence on modeling predefined ontologies (entity linking) and explicit sense inventories (WSD), and 2) it is framed as a simple binary classification task: for a target word w appearing in two different contexts c1 and c2 , the system must decide whether w conveys the sam"
2021.emnlp-main.571,P18-1072,1,0.894256,"Missing"
2021.emnlp-main.571,N16-1072,0,0.164299,"stantially lag behind human performance, and the largest gaps are observed for low-resource languages and languages dissimilar to English. 1 Introduction Camacho-Collados, 2019; Raganato et al., 2020). Although monolingual evaluation in English is still predominant, a need has been recognized to construct similar resources for other languages to support cross-lingual evaluation and model diagnostics. This includes multilingual and crosslingual WSD benchmarks (Navigli and Ponzetto, 2012; Navigli et al., 2013; Scarlini et al., 2020; Barba et al., 2020, inter alia), cross-lingual entity linking (Tsai and Roth, 2016; Raiman and Raiman, 2018; Upadhyay et al., 2018) and, most recently, multilingual WiC (termed XL-WiC) spanning 12 languages (Raganato et al., 2020). This most recent WiC evaluation approach is particularly attractive as 1) it bypasses the dependence on modeling predefined ontologies (entity linking) and explicit sense inventories (WSD), and 2) it is framed as a simple binary classification task: for a target word w appearing in two different contexts c1 and c2 , the system must decide whether w conveys the same meaning in both contexts, or not. However, the current WiC evaluation still allows"
2021.emnlp-main.571,D18-1270,0,0.0460866,"Missing"
2021.emnlp-main.571,2020.cl-4.5,1,0.894831,"Missing"
2021.findings-emnlp.410,P18-1073,0,0.0529912,"Missing"
2021.findings-emnlp.410,2020.acl-main.421,1,0.808072,"ponent inserted into a MMT such as mBERT the parameters of the adapter generator as well as (Devlin et al., 2019) or XLM-R (Conneau et al., (ii) at the typological level by conditioning on fea- 2020) with the purpose of specializing the MMT tures from the URIEL database (Littell et al., 2017). for a particular language, in order to either (a) supThe latter additionally enables zero-shot transfer port a new language not covered by the MMT’s to unseen languages. Further, we propose a vari- original multilingual pretraining (Pfeiffer et al., ant of MAD-G in which we generate adapters also 2020b; Artetxe et al., 2020) or (b) recover/improve conditioned on their Transformer layer position the performance for a particular (resource-rich) lan(see Section 3.2), allowing MAD-G to be much guage (Bapna and Firat, 2019; Rust et al., 2021). In more parameter-efficient than adapter-based trans- this work, we adopt the competitive and lightweight fer methods of prior work. (so-called bottleneck) adapter variant of Pfeiffer In experiments on zero-shot cross-lingual trans- et al. (2021a). There, only one adapter module, 4763 consisting of a successive down-projection and up-projection, is injected per Transformer layer"
2021.findings-emnlp.410,N19-1191,0,0.133549,"ross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed as a parameter-efficient means to extend multilingual models to underrepresented languages (Bapna and Firat, 2019; Üstün et al., 2020). The general practice is to train a language adapter on the unlabeled data for each language (Pfeiffer et al., 2020b) via masked language modeling (MLM). However, this generally requires substantial amounts of monolingual data, which prevents adapters from serving under-resourced languages where such additional language-specific capacity would be most useful. To address this deficiency, we propose multilingual adapter generation (MAD-G), a novel paradigm that enables the generation of adapters for low-resource languages by sharing information across languages. Instead of"
2021.findings-emnlp.410,N16-1101,0,0.0319479,"he nl languages of interest. 2 According to Pfeiffer et al. (2020a, 2021a) and Rücklé et al. (2021), such an architecture with a single adapter per Transformer layer is more parameter-efficient while performing on par with the architecture of Houlsby et al. (2019) with two adapters per Transformer layer (one after the multi-head attention sublayer and one after the feed-forward sublayer). 3 Other examples include the training of language-specific pretrained language models (Rust et al., 2021) as well as language pair-specific encoder–decoder models for machine translation (Luong et al., 2016; Firat et al., 2016). In CPG, the only language-specific parameters that we learn are the low-dimensional language embeddings λ(l) ∈ Rdl . These are used by the generator g, a hyper-network (Ha et al., 2017) component4 with its own parameterization φ, to produce the language-specific parameterization of the main model: θ (l) = gφ (λ(l) ). While g can in principle be any differentiable function (i.e., arbitrarily deep neural model), in practice it is typically set to a simple linear projection (i.e., φ = W ): gW (λ(l) ) , W λ(l) , (2) where W ∈ Rnp ×dl is a learnable weight matrix, np being the number of parameter"
2021.findings-emnlp.410,P19-1070,1,0.898419,"Missing"
2021.findings-emnlp.410,2021.eacl-main.270,1,0.690338,"Missing"
2021.findings-emnlp.410,2020.acl-main.560,0,0.0116496,"e generation of task-agnostic LAs that can support downstream cross-lingual transfer for arbitrary NLP tasks. 4 A hyper-network is a neural model that generates the parameters of another (main) neural model. 5 Training MAD-G on 95 languages with dl = 32 (this work) achieves roughly a threefold saving in parameter size. 4764 3 MAD-G: Methodology MAD-G aims to enable resource-efficient adaptation of MMTs to a wide range of previously unseen, radically resource-poor languages,6 and contribute in this manner to more sustainable (Strubell et al., 2019; Moosavi et al., 2020) and more inclusive NLP (Joshi et al., 2020). We couple (i) the computational efficiency of the light-weight adapters (cf. Section 2.1) and (ii) knowledge sharing and zero-shot language transfer capabilities of CPG (cf. Section 2.2), with (iii) external linguistic (i.e., typological) knowledge (Ponti et al., 2019a) towards supporting arbitrary NLP tasks for (even radically) resource-poor languages. MAD-G mitigates important limitations of prior work. Unlike Üstün et al. (2020), we generate taskagnostic LAs, (re)usable across NLP tasks. Unlike the MAD-X framework (Pfeiffer et al., 2020b), which trains LAs independently for each language"
2021.findings-emnlp.410,2020.emnlp-main.363,1,0.824906,"Missing"
2021.findings-emnlp.410,E17-2002,0,0.408844,"rs can be leveraged in typically achieved through the use of adapter layers arbitrary downstream tasks (Pfeiffer et al., 2020b). (Houlsby et al., 2019; Pfeiffer et al., 2020b). MAD-G shares information across languages (i) In particular, a language adapter is a light-weight at the level of hidden representations by sharing component inserted into a MMT such as mBERT the parameters of the adapter generator as well as (Devlin et al., 2019) or XLM-R (Conneau et al., (ii) at the typological level by conditioning on fea- 2020) with the purpose of specializing the MMT tures from the URIEL database (Littell et al., 2017). for a particular language, in order to either (a) supThe latter additionally enables zero-shot transfer port a new language not covered by the MMT’s to unseen languages. Further, we propose a vari- original multilingual pretraining (Pfeiffer et al., ant of MAD-G in which we generate adapters also 2020b; Artetxe et al., 2020) or (b) recover/improve conditioned on their Transformer layer position the performance for a particular (resource-rich) lan(see Section 3.2), allowing MAD-G to be much guage (Bapna and Firat, 2019; Rust et al., 2021). In more parameter-efficient than adapter-based trans-"
2021.findings-emnlp.410,2021.eacl-main.39,1,0.794583,"amed entity recognition (NER) on the MasakhaNER dataset for African languages (Adelani et al., 2021). For POS and DP, we evaluate on a substantial subset of all UD languages with available treebanks.8 We discern between three language groups in evaluation, with some examples in Table 1: (i) mBERT-seen languages are those included in mBERT’s pretraining; (ii) MAD-G-seen languages were not part of mBERT’s pretraining but are included in MAD8 For POS and DP, we omit only (i) languages with scripts unseen in mBERT’s pretraining, where mBERT’s tokenizer predominantly produces unknown (UNK) tokens (Pfeiffer et al., 2021b), (ii) languages lacking any information in URIEL, and (iii) languages whose treebanks have missing fields. For MasakhaNER, we evaluate on all dataset languages except Amharic, as Amharic also uses a script unseen by mBERT. G training; and (iii) unseen languages are those not included in mBERT pretraining nor in MAD-G training. 4.1 Baselines and MAD-G Variants mBERT is an MMT pretrained on the Wikipedias of 104 languages. We use mBERT as the base MMT for MAD-G. XLM-R is a state-of-the-art MMT pretrained on the CommonCrawl data of 100 languages (Conneau et al., 2020).9 We evaluate them in the"
2021.findings-emnlp.410,2020.emnlp-demos.7,1,0.70459,"Missing"
2021.findings-emnlp.410,2020.emnlp-main.617,1,0.622952,"Missing"
2021.findings-emnlp.410,2021.emnlp-main.800,1,0.87244,"Missing"
2021.findings-emnlp.410,P19-1493,0,0.0388325,"te that MAD-G’s transfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available taskspecific training data; and (ii) by further finetuning generated MAD-G adapters for languages with monolingual data. 1 Introduction et al., 2019) and, more recently, massively multilingual Transformers (MMTs) like mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and mT5 (Xue et al., 2021) as main vehicles of cross-lingual transfer. Although MMTs display impressive (zero-shot) cross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed as a parameter-efficient means to extend multilingual models to underrepresented languages (Bapna and Firat, 2019; Üstün et al., 2020). The gen"
2021.findings-emnlp.410,D18-1039,0,0.123067,"each language (Pfeiffer et al., 2020b) via masked language modeling (MLM). However, this generally requires substantial amounts of monolingual data, which prevents adapters from serving under-resourced languages where such additional language-specific capacity would be most useful. To address this deficiency, we propose multilingual adapter generation (MAD-G), a novel paradigm that enables the generation of adapters for low-resource languages by sharing information across languages. Instead of learning separate adapters for each language, MAD-G leverages contextual parameter generation (CPG; Platanios et al., 2018a; Ponti et al., 2019b), that is, it learns a single model that can generate a language adapter for an arbitrary target language. At the core of MAD-G is a contextual parameter generator which 1 mBERT and XLM-R have been trained on corpora from Multilingual NLP has witnessed large ad104 and 100 languages, respectively. According to Glottolog vances, with cross-lingual word embedding spaces (Hammarström et al., 2017), however, there are over 7,000 (Mikolov et al., 2013; Artetxe et al., 2018; Glavaš languages spoken around the world. 4762 Findings of the Association for Computational Linguistics"
2021.findings-emnlp.410,2020.emnlp-main.185,1,0.891273,"Missing"
2021.findings-emnlp.410,J19-3005,1,0.858703,"Missing"
2021.findings-emnlp.410,D19-1288,1,0.883413,"Missing"
2021.findings-emnlp.410,2021.emnlp-main.626,1,0.768969,"genealogical ties to high(er)-resource languages. CPG is a technique introduced by Platanios et al. (2018a) to address these drawbacks. While originally conceived for neural machine translation (NMT), CPG can be applied to any neural model f parameterized by θ, for which we aim to learn parameterizations for a number of different contexts; in multilingual NLP, these “contexts” are languages. In the instance-per-language approach, an independent parameterization θ (l) , l ∈ {1, . . . , nl }, is learned for each of the nl languages of interest. 2 According to Pfeiffer et al. (2020a, 2021a) and Rücklé et al. (2021), such an architecture with a single adapter per Transformer layer is more parameter-efficient while performing on par with the architecture of Houlsby et al. (2019) with two adapters per Transformer layer (one after the multi-head attention sublayer and one after the feed-forward sublayer). 3 Other examples include the training of language-specific pretrained language models (Rust et al., 2021) as well as language pair-specific encoder–decoder models for machine translation (Luong et al., 2016; Firat et al., 2016). In CPG, the only language-specific parameters that we learn are the low-dimens"
2021.findings-emnlp.410,2021.acl-long.243,1,0.823183,"Missing"
2021.findings-emnlp.410,2021.findings-acl.106,1,0.880987,"2) where W ∈ Rnp ×dl is a learnable weight matrix, np being the number of parameters of f . The total number of parameters learned when training nl independent models is nl np , whereas the number of parameters in the W matrix is dl np . Therefore, neglecting the small number of parameters dedicated to language embeddings, the CPG approach uses fewer parameters when dl < nl .5 More importantly, in multilingual training the generator matrix W is shared across all languages, which enables knowledge sharing across languages and leads to improved transfer performance. Platanios et al. (2018b) and Ponti et al. (2021a) opt for randomly initializing language embeddings λ(l) and learning them end-to-end. Specified like this, however, CPG cannot generalize to languages unseen in training, as it would lack embeddings for those languages at inference. To support generalization to arbitrary new languages, one must ground language embeddings in some external language representation, available for many languages. To this end, Ponti et al. (2019b) exploit typological language vectors from the URIEL database (Littell et al., 2017) directly as language embeddings to generate a full set of model parameters. In a simi"
2021.findings-emnlp.410,2020.emnlp-main.180,0,0.119503,"Missing"
2021.findings-emnlp.410,D19-1077,0,0.021338,"sfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available taskspecific training data; and (ii) by further finetuning generated MAD-G adapters for languages with monolingual data. 1 Introduction et al., 2019) and, more recently, massively multilingual Transformers (MMTs) like mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and mT5 (Xue et al., 2021) as main vehicles of cross-lingual transfer. Although MMTs display impressive (zero-shot) cross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed as a parameter-efficient means to extend multilingual models to underrepresented languages (Bapna and Firat, 2019; Üstün et al., 2020). The general practice is to tr"
2021.findings-emnlp.410,2021.naacl-main.41,0,0.0352696,"offers substantial benefits for low-resource languages, particularly on the NER task in low-resource African languages. Finally, we demonstrate that MAD-G’s transfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available taskspecific training data; and (ii) by further finetuning generated MAD-G adapters for languages with monolingual data. 1 Introduction et al., 2019) and, more recently, massively multilingual Transformers (MMTs) like mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and mT5 (Xue et al., 2021) as main vehicles of cross-lingual transfer. Although MMTs display impressive (zero-shot) cross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed a"
C08-1057,A97-1052,0,0.0209042,"h in section 2 and data in section 3. Experimental evaluation is reported in section 4. Section 5 provides discussion and section 6 concludes. 2 tactic frames (Levin, 1993). Most verb classification approaches have therefore employed shallow syntactic slots or SCFs as basic features. Some have supplemented them with further information about verb tense, voice, and/or semantic selectional preferences on argument heads.2 The preliminary experiment on biomedical verb classification (Korhonen et al., 2006) employed basic syntactic features only: SCFs extracted from corpus data using the system of Briscoe and Carroll (1997) which operates on the output of a domain-independent robust statistical parser (RASP) (Briscoe and Carroll, 2002). Because such deep syntactic features seem ideally suited for challenging biomedical data, we adopted the same basic approach, but we designed and extracted a range of novel feature sets which include additional syntactic and semantic information. The SCF extraction system assigns each occurrence of a verb in the parsed data as a member of one of the 163 verbal SCFs, builds a lexical entry for each verb (type) and SCF combination, and filters noisy entries out of the lexicon. We d"
C08-1057,briscoe-carroll-2002-robust,0,0.0117721,"ion and section 6 concludes. 2 tactic frames (Levin, 1993). Most verb classification approaches have therefore employed shallow syntactic slots or SCFs as basic features. Some have supplemented them with further information about verb tense, voice, and/or semantic selectional preferences on argument heads.2 The preliminary experiment on biomedical verb classification (Korhonen et al., 2006) employed basic syntactic features only: SCFs extracted from corpus data using the system of Briscoe and Carroll (1997) which operates on the output of a domain-independent robust statistical parser (RASP) (Briscoe and Carroll, 2002). Because such deep syntactic features seem ideally suited for challenging biomedical data, we adopted the same basic approach, but we designed and extracted a range of novel feature sets which include additional syntactic and semantic information. The SCF extraction system assigns each occurrence of a verb in the parsed data as a member of one of the 163 verbal SCFs, builds a lexical entry for each verb (type) and SCF combination, and filters noisy entries out of the lexicon. We do not employ the filter in our work because its primary aim is to filter out SCFs containing adjuncts (as opposed"
C08-1057,J06-2001,0,0.275819,"Missing"
C08-1057,P06-1044,1,0.927677,"on, predicateargument identification, event extraction and the identification of biomedical (e.g. interaction) relations. However, no such classification is available. Recent research shows that it is possible to automatically induce lexical classes from corpora with promising accuracy (Schulte im Walde, 2006; Joanis et al., 2007; Sun et al., 2008). A number of machine learning (ML) methods have been applied to classify mainly syntactic features (e.g. subcategorization frames (SCFs)) extracted from crossdomain corpora using e.g. part-of-speech tagging or robust statistical parsing techniques. Korhonen et al. (2006) have recently applied such an approach to biomedical texts. Their preliminary experiment shows encouraging results but further work is required before such an approach can be used to benefit practical BIO-NLP. We conduct a large-scale investigation to find optimal features for biomedical verb classification. We introduce a range of theoretically-motivated feature sets and evaluate them thoroughly using a robust method new to the task: a cost-based framework for pairwise clustering. Our best results compare favourably with earlier ones. Interestingly, they are obtained using feature sets which"
C08-1057,kunze-2000-extension,0,0.0703435,"Missing"
C08-1057,C00-2094,0,0.019116,"ACTIVATE GENES : WAF 1). Lexical classes can be used to abstract away from individual words, or to build a lexical organization which predicts much of the behaviour of a new word by associating it with an appropriate class. They have proved useful for various NLP application tasks, e.g. parsing, word sense dis449 1 http://www.nlm.nih.gov/research/umls Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 449–456 Manchester, August 2008 ambiguation, semantic role labeling, information extraction, question-answering, machine translation (Dorr, 1997; Prescher et al., 2000; Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005). A large-scale classification specific to the biomedical data could support key BIONLP tasks such as anaphora resolution, predicateargument identification, event extraction and the identification of biomedical (e.g. interaction) relations. However, no such classification is available. Recent research shows that it is possible to automatically induce lexical classes from corpora with promising accuracy (Schulte im Walde, 2006; Joanis et al., 2007; Sun et al., 2008). A number of machine learning (ML) methods have been applied to cl"
C08-1057,W04-3213,0,\N,Missing
C10-1078,P96-1025,0,0.0070942,"c demonstrates how verbs differ between subdomains with respect to SCFs. For example, while the Pediatrics subdomain uses the verb “govern” in a single SCF among its 12 possibilities, the Genetics subdomain distributes its usage over 7 of them. Two subdomains may both use “restrict” with high frequency (e.g. Molecular Biology and Ethics), but with different frequency distributions over SCFs. 5.3 Syntax It is difficult to measure syntactic complexity accurately without access to a hand-annotated treebank, but it is well-known that sentence length correlates strongly with processing difficulty (Collins, 1996). The first column of Table 2 gives average sentence lengths (excluding punctuation and “sentences” of fewer than three words) for selected domains. All standard errors are < 0.1. It is clear that all biomedical subdomains typically use longer sentences than newswire, though there is also variation within biomedicine, from an average length of 27 words in Molecular Biology to 24.5 words in Pediatrics. “Packaging” information in complex pre- and/or post-modified noun phrases is a characteristic feature of academic writing (Biber and Gray, 2010). This increases the information density of a sente"
C10-1078,J03-4003,0,0.0205514,"verbs and nouns to adverbs. This behavior seems non-continuous, in that subdomains either make heavy, or almost no, use of it: for example, Pediatrics has no subdomain-specific items among the its ten top adverbs by log-likelihood, while Neoplasms has “histologically”, “immunohistochemically” and “subcutaneously”. These informationdense terms could prove useful for tasks like automatic curation of subdomain vocabularies, where they imply relationships between their components, the items they modify, etc. 5.2 Verb distributional behavior Modelling verb behavior is important for both syntactic (Collins, 2003) and semantic (Korhonen et al., 2008) processing, and subdomains are known to conscript verbs into specific roles that change the distributions of their syntactic properties (Roland and Jurafsky, 1998). The four properties we considered verbs’ distributions over (SCF, POS, GR and voice) produced similar inter-subdomain JSD values. Figure 2c demonstrates how verbs differ between subdomains with respect to SCFs. For example, while the Pediatrics subdomain uses the verb “govern” in a single SCF among its 12 possibilities, the Genetics subdomain distributes its usage over 7 of them. Two subdomains"
C10-1078,P07-2009,0,0.0167067,"the added complexity of interactions in data from overlapping subdomains. To ensure sufficient data for comparing a variety of linguistic features, we discard the subdomains with less than one million words meeting the single-subdomain criterion. After review, we also drop the “Biology” subdomain, which appears to function as a catch-all for many loosely related areas. Figure 1 shows the Feature extraction We investigate subdomain variation in our corpus across a range of lexical, syntactic, sentential and discourse features. The corpus is lemmatised, tagged and parsed using the C&C pipeline (Curran et al., 2007) with the adapted part-of-speech and lexical category tagging models produced by Rimell and Clark (2009) for biomedical parsing. From this output we count occurrences of noun, verb, adjective and adverb lemmas, part-of-speech (POS) tags, grammatical relations (GRs), chunks, and lexical categories. The lemma features are Zipfian-distributed items from an open class, so we have experimented with filtering low-frequency items at various thresholds to reduce noise and improve processing speed. The other feature sets can be viewed as closed classes, where filtering is unnecessary. Since verbs are c"
C10-1078,I05-1018,0,0.233811,"nguistic notions of register and genre (Biber, 1988). In addition to the differences in vocabulary that one would expect to observe, domains can vary in many linguistic variables that affect NLP systems. The scientific domain which has received the most attention (and is the focus of this paper) is the biomedical domain. Notable examples of corpus construction projects for the biomedical domain are PennBioIE (Kulick et al., 2004) and GENIA (Kim et al., 2003). These corpora have been used to develop systems for a range of processing tasks, from entity recognition (Jin et al., 2006) to parsing (Hara et al., 2005) to coreference resolution (Nguyen and Kim, 2008). An implicit assumption in much previous work on biomedical NLP has been that particular subdomains of biomedical literature – typically molecular biology – can be used as a model of biomedical language in general. For example, GENIA consists of abstracts dealing with a specific set of subjects in molecular biology, while PennBioIE covers abstracts in two specialised domains, cancer genomics and the behaviour of a particular class of enzymes. This assumption of representativeness is understandable because linguistic annotation is labour-intensi"
C10-1078,P98-2184,0,0.0256373,"the its ten top adverbs by log-likelihood, while Neoplasms has “histologically”, “immunohistochemically” and “subcutaneously”. These informationdense terms could prove useful for tasks like automatic curation of subdomain vocabularies, where they imply relationships between their components, the items they modify, etc. 5.2 Verb distributional behavior Modelling verb behavior is important for both syntactic (Collins, 2003) and semantic (Korhonen et al., 2008) processing, and subdomains are known to conscript verbs into specific roles that change the distributions of their syntactic properties (Roland and Jurafsky, 1998). The four properties we considered verbs’ distributions over (SCF, POS, GR and voice) produced similar inter-subdomain JSD values. Figure 2c demonstrates how verbs differ between subdomains with respect to SCFs. For example, while the Pediatrics subdomain uses the verb “govern” in a single SCF among its 12 possibilities, the Genetics subdomain distributes its usage over 7 of them. Two subdomains may both use “restrict” with high frequency (e.g. Molecular Biology and Ethics), but with different frequency distributions over SCFs. 5.3 Syntax It is difficult to measure syntactic complexity accura"
C10-1078,W01-0511,0,0.0268656,"ge length of 27 words in Molecular Biology to 24.5 words in Pediatrics. “Packaging” information in complex pre- and/or post-modified noun phrases is a characteristic feature of academic writing (Biber and Gray, 2010). This increases the information density of a sentence but brings with it syntactic and semantic ambiguities. For example, the difficulty of resolving the internal structure of noun-noun compounds and strings of prepositional phrases has been the focus of ongoing research in NLP; these phenomena have also been identified as significant challenges in biomedical language processing (Rosario and Hearst, 2001; Schuman and Bergler, 2006). The second and third columns of Table 2 present average lengths for full noun phrases, defined as every word dominated by a head noun in the grammatical relation graph for a sentence, and for base nominals, defined as nouns plus premodifying adjectives and nouns only. All standard errors are ≤ 0.01. Newswire text uses the simplest noun 693 (a) LDA-induced distribution over topics (b) Adverb lemma frequencies (c) Verb distributions over subcategorization frames Figure 2: Subdomain variation plotted as heat maps and dendrograms 694 Sentence length Mol. Biology 27.0"
C10-1078,C08-1057,1,0.831438,"his behavior seems non-continuous, in that subdomains either make heavy, or almost no, use of it: for example, Pediatrics has no subdomain-specific items among the its ten top adverbs by log-likelihood, while Neoplasms has “histologically”, “immunohistochemically” and “subcutaneously”. These informationdense terms could prove useful for tasks like automatic curation of subdomain vocabularies, where they imply relationships between their components, the items they modify, etc. 5.2 Verb distributional behavior Modelling verb behavior is important for both syntactic (Collins, 2003) and semantic (Korhonen et al., 2008) processing, and subdomains are known to conscript verbs into specific roles that change the distributions of their syntactic properties (Roland and Jurafsky, 1998). The four properties we considered verbs’ distributions over (SCF, POS, GR and voice) produced similar inter-subdomain JSD values. Figure 2c demonstrates how verbs differ between subdomains with respect to SCFs. For example, while the Pediatrics subdomain uses the verb “govern” in a single SCF among its 12 possibilities, the Genetics subdomain distributes its usage over 7 of them. Two subdomains may both use “restrict” with high fr"
C10-1078,W06-3312,0,0.0303267,"olecular Biology to 24.5 words in Pediatrics. “Packaging” information in complex pre- and/or post-modified noun phrases is a characteristic feature of academic writing (Biber and Gray, 2010). This increases the information density of a sentence but brings with it syntactic and semantic ambiguities. For example, the difficulty of resolving the internal structure of noun-noun compounds and strings of prepositional phrases has been the focus of ongoing research in NLP; these phenomena have also been identified as significant challenges in biomedical language processing (Rosario and Hearst, 2001; Schuman and Bergler, 2006). The second and third columns of Table 2 present average lengths for full noun phrases, defined as every word dominated by a head noun in the grammatical relation graph for a sentence, and for base nominals, defined as nouns plus premodifying adjectives and nouns only. All standard errors are ≤ 0.01. Newswire text uses the simplest noun 693 (a) LDA-induced distribution over topics (b) Adverb lemma frequencies (c) Verb distributions over subcategorization frames Figure 2: Subdomain variation plotted as heat maps and dendrograms 694 Sentence length Mol. Biology 27.0 Genetics 26.6 Cell Biology 2"
C10-1078,W04-3111,0,0.0370193,"ability distributions from which different sets of data are drawn (Daum´e III and Marcu, 2006). The concept also mirrors the notion of variation across thematic subjects and the corpus-linguistic notions of register and genre (Biber, 1988). In addition to the differences in vocabulary that one would expect to observe, domains can vary in many linguistic variables that affect NLP systems. The scientific domain which has received the most attention (and is the focus of this paper) is the biomedical domain. Notable examples of corpus construction projects for the biomedical domain are PennBioIE (Kulick et al., 2004) and GENIA (Kim et al., 2003). These corpora have been used to develop systems for a range of processing tasks, from entity recognition (Jin et al., 2006) to parsing (Hara et al., 2005) to coreference resolution (Nguyen and Kim, 2008). An implicit assumption in much previous work on biomedical NLP has been that particular subdomains of biomedical literature – typically molecular biology – can be used as a model of biomedical language in general. For example, GENIA consists of abstracts dealing with a specific set of subjects in molecular biology, while PennBioIE covers abstracts in two special"
C10-1078,J94-4002,0,0.0154037,"ating the importance of domain modelling, Nguyen and Kim demonstrate that tailoring a pronoun resolution system to specific properties of the biomedical domain improves performance. As our corpus is not annotated for coreference we restrict our attention to types that are reliably coreferential: masculine/feminine personal pronouns (he, she and case variations), neuter personal pronouns (they, it and variations) and definite NPs with demonstrative determiners such as this and that. To filter out pleonastic pronouns we used a combination of the C+C parser’s pleonasm tag and heuristics based on Lappin and Leass (1994). To filter out the most common class of non-anaphoric demonstrative NPs we simply discarded any matching the pattern this. . . paper|study|article. Table 3 presents statistics for selected types of coreferential noun phrases in a number of domains. The results generally agree with the findings of Nguyen and Kim (2008): biomedical text is on average 200 times less likely than news text to use gendered pronouns and twice as likely to use anaphoric definite noun phrases. At the domain level, however, there is clear variation within the biomedical corpus. In contrast to Nguyen and Kim’s observati"
C10-1078,C08-1079,0,0.304314,"1988). In addition to the differences in vocabulary that one would expect to observe, domains can vary in many linguistic variables that affect NLP systems. The scientific domain which has received the most attention (and is the focus of this paper) is the biomedical domain. Notable examples of corpus construction projects for the biomedical domain are PennBioIE (Kulick et al., 2004) and GENIA (Kim et al., 2003). These corpora have been used to develop systems for a range of processing tasks, from entity recognition (Jin et al., 2006) to parsing (Hara et al., 2005) to coreference resolution (Nguyen and Kim, 2008). An implicit assumption in much previous work on biomedical NLP has been that particular subdomains of biomedical literature – typically molecular biology – can be used as a model of biomedical language in general. For example, GENIA consists of abstracts dealing with a specific set of subjects in molecular biology, while PennBioIE covers abstracts in two specialised domains, cancer genomics and the behaviour of a particular class of enzymes. This assumption of representativeness is understandable because linguistic annotation is labour-intensive and it may not be worthwhile to produce annota"
C10-1078,P07-1115,1,0.732241,"its POS (indicating e.g. tense, such as present, past, and present participle). Voice is determined from additional annotation output by the C&C parser. Table 1 shows the POS-distribution for the verb “restrict”, in two subdomains from the corpus. Finally, we record distributions over verb subcategorization frames (SCFs) taken by each verb, and over the GRs it participates in. 691 Subdomain Medical Informatics Cell Biology VB .35 .14 VBG .29 .43 VBN .06 .05 VBP .09 .10 VBZ .21 .29 Table 1: Distribution over POS tags for verb “restrict”, in two subdomains SCFs were extracted using a system of Preiss et al. (2007). To facilitate a more robust and interpretable analysis of vocabulary differences, we estimate a “topic model” of the corpus with Latent Dirichlet Analysis (Blei et al., 2003) using the MALLET toolkit.3 As preprocessing we divide the corpus into articles, removing stopwords and words shorter than 3 characters. The Gibbs sampling procedure is parameterised to induce 100 topics, each giving a coherent cluster of related words learned from the data, and to run for 1000 iterations. We collate the predicted distribution over topics for each article in a subdomain, weighted by article wordcount, to"
C10-1078,W00-0901,0,0.0510859,"nce or absence of differences between the feature sets, and to do so we calculated the Jensen-Shannon divergence and the Pearson correlation. Jensen-Shannon divergence is a finite symmetric measurement of the divergence between probability distributions, while Pearson correlation quantifies the linear relationship between two real-valued samples. The count-features are weighted, for a given subdomain, by the feature’s log-likelihood between the subdomain’s data and the rest of the corpus. Log-likelihood has been shown to perform well when comparing counts of potentially lowfrequency features (Rayson and Garside, 2000) such as found in Zipfian-distributed data. This serves to place more weight in the comparison on items that are distinctive of the subdomain with respect to the entire corpus. While the count-features are treated as a single distribution for the purposes of JSD, the verbwisefeatures are composed of many distributions, one for each verb lemma. Our approach is to combine the JSD of the verbs, weighted by the log3 http://mallet.cs.umass.edu likelihood of the verb lemma between the two subdomains in question, and normalize the distances to the interval [0, 1]. Using the lemma’s loglikelihood assu"
C10-1078,C98-2179,0,\N,Missing
C10-1113,andersen-etal-2008-bnc,0,0.0285456,"re a verb is used metaphorically. The seed phrases include e.g. stir excitement, reﬂect enthusiasm, accelerate change, grasp theory, cast doubt, suppress memory, throw remark (verb - direct object constructions) and campaign surged, factor shaped [..], tension mounted, ideology embraces, changes operated, approach focuses, example illustrates (subject verb constructions). 2.2 Corpus The search space for metaphor identiﬁcation was the British National Corpus (BNC) that was parsed using the RASP parser of Briscoe et al. (2006). We used the grammatical relations output of RASP for BNC created by Andersen et al. (2008). The system searched the corpus for the source and target domain vocabulary within a particular grammatical relation (verb-object or verbsubject). 3 Method Starting from a small seed set of metaphorical expressions, the system implicitly captures the associations that underly their production and comprehension. It generalizes over these associations by means of unsupervised verb and noun clustering. The obtained clusters then represent potential source and target concepts between which metaphorical associations hold. The knowledge of such associations is then used to annotate metaphoricity in"
C10-1113,D08-1007,0,0.110236,"ing to collect source domain vocabulary, which in turn allows us to harvest a large number of new metaphorical expressions. 3.2 Verb and Noun Clustering Since Levin (1993) published her classiﬁcation, there have been a number of attempts to automatically classify verbs into semantic classes using supervised and unsupervised approaches (Lin, 1998; Brew and Schulte im Walde, 2002; Korhonen et al., 2003; Schulte im Walde, 2006; Joanis et al., 2008; Sun and Korhonen, 2009). Similar methods were also applied to acquisition of noun classes from corpus data (Rooth et al., 1999; Pantel and Lin, 2002; Bergsma et al., 2008). We adopt a recent verb clustering approach of Sun and Korhonen (2009), who used rich syntactic and semantic features extracted using a shallow parser and a clustering method suitable for the resulting high dimensional feature space. When Sun and Korhonen evaluated their approach on 204 verbs from 17 Levin classes, they obtained 80.4 F-measure (which is high in particular for an unsupervised approach). We apply this approach to a much larger set of 1610 verbs: all the verb forms appearing in VerbNet (Kipper et al., 2006) with the exception of highly infrequent ones. In addition, we adapt the"
C10-1113,E06-1042,0,0.674756,"Missing"
C10-1113,W02-1016,0,0.271372,"Missing"
C10-1113,P06-4020,0,0.0557129,"that are single-word 1003 metaphors representing verb-subject and verbobject relations, where a verb is used metaphorically. The seed phrases include e.g. stir excitement, reﬂect enthusiasm, accelerate change, grasp theory, cast doubt, suppress memory, throw remark (verb - direct object constructions) and campaign surged, factor shaped [..], tension mounted, ideology embraces, changes operated, approach focuses, example illustrates (subject verb constructions). 2.2 Corpus The search space for metaphor identiﬁcation was the British National Corpus (BNC) that was parsed using the RASP parser of Briscoe et al. (2006). We used the grammatical relations output of RASP for BNC created by Andersen et al. (2008). The system searched the corpus for the source and target domain vocabulary within a particular grammatical relation (verb-object or verbsubject). 3 Method Starting from a small seed set of metaphorical expressions, the system implicitly captures the associations that underly their production and comprehension. It generalizes over these associations by means of unsupervised verb and noun clustering. The obtained clusters then represent potential source and target concepts between which metaphorical ass"
C10-1113,P06-2012,0,0.0560219,"for noun clustering. We employed all the argument heads and verb lemmas appearing in the subject, direct object and indirect object relations in the RASP-parsed BNC. The feature vectors were ﬁrst constructed from the corpus counts, and subsequently normalized by the sum of the feature values before applying clustering. 3.2.2 Clustering Algorithm We use spectral clustering (SPEC) for both verbs and nouns. This technique has proved to be effective in previous verb clustering works (Brew and Schulte im Walde, 2002; Sun and Korhonen, 2009) and in related NLP tasks involving high dimensional data (Chen et al., 2006). We use the MNCut algorithm for SPEC which has a wide applicability and a clear probabilistic interpretation (Meila and Shi, 2001). The task is to group a given set of words W = {wn }N n=1 into a disjoint partition of K classes. SPEC takes a similarity matrix as input. We construct it using the Jensen-Shannon divergence (JSD) as a measure. The JSD between two feature vectors w and w is djsd (w, w ) = 12 D(w||m) + 1  2 D(w ||m) where D is the Kullback-Leibler divergence, and m is the average of the w and w . The similarity matrix S is constructed where Sij = exp(−djsd (w, w )). In SPEC, t"
C10-1113,J91-1003,0,0.92557,"mputational Linguistics (Coling 2010), pages 1002–1010, Beijing, August 2010 According to Wilks, metaphors represent a violation of selectional restrictions in a given context. Consider the following example. (7) Diana and Charles did not succeed in mending their marriage. (8) The wheels of Stalin’s regime were well oiled and already turning. (5) My car drinks gasoline. (Wilks, 1978) The verb drink normally takes an animate subject and a liquid object. Therefore, drink taking a car as a subject is an anomaly, which may as well indicate metaphorical use of drink. This approach was automated by Fass (1991) in his met* system. However, Fass himself indicated a problem with the method: it detects any kind of non-literalness or anomaly in language (metaphors, metonymies and others), i.e., it overgenerates with respect to metaphor. The techniques met* uses to differentiate between those are mainly based on hand-coded knowledge, which implies a number of limitations. In a similar manner manually created knowledge in the form of WordNet (Fellbaum, 1998) is employed by the system of Krishnakumaran and Zhu (2007), which essentially differentiates between highly lexicalized metaphors included in WordNet"
C10-1113,P98-2127,0,0.277006,"pus. 3.1 Clustering Motivation Abstract concepts that are associated with the same source domain are often related to each other on an intuitive and rather structural level, but their meanings, however, are not necessarily synonymous or even semantically close. The results of previous research on corpus-based lexical semantics suggest that the linguistic environment in which a lexical item occurs can shed light on its meaning. A number of works have shown that it is possible to automatically induce semantic word classes from corpus data via clustering of contextual cues (Pereira et al., 1993; Lin, 1998; Schulte im Walde, 2006). The consensus is that the lexical items exposing similar behavior in a large body of text most likely have the same meaning. However, the concepts of marriage and political regime, that are also observed in similar lexico-syntactic environments, albeit having quite distinct meanings are likewise assigned by such methods to the same cluster. In contrast to concrete concepts, such as tea, water, coffee, beer, drink, liquid, that are clustered together due to meaning similarity, abstract concepts tend to be clustered together by association with the same source domain."
C10-1113,C88-1081,0,0.776717,"oes not employ any hand-crafted knowledge, other than the initial seed set, but, in contrast, captures metaphoricity by means of verb and noun clustering. Being the ﬁrst to employ unsupervised methods for metaphor identiﬁcation, our system operates with the precision of 0.79. 1 Introduction Besides enriching our thought and communication with novel imagery, the phenomenon of metaphor also plays a crucial structural role in our use of language. Metaphors arise when one concept is viewed in terms of the properties of the other. Below are some examples of metaphor. (1) How can I kill a process? (Martin, 1988) (2) Inﬂation has eaten up all my savings. (Lakoff and Johnson, 1980) (3) He shot down all of my arguments. (Lakoff and Johnson, 1980) (4) And then my heart with pleasure ﬁlls, And dances with the daffodils.1 In metaphorical expressions seemingly unrelated features of one concept are associated with another concept. In the computer science metaphor 1 “I wandered lonely as a cloud”, William Wordsworth, 1804. in (1) the computational process is viewed as something alive and, therefore, its forced termination is associated with the act of killing. Lakoff and Johnson (1980) explain metaphor as a s"
C10-1113,W06-3506,0,0.684132,"oblem with the method: it detects any kind of non-literalness or anomaly in language (metaphors, metonymies and others), i.e., it overgenerates with respect to metaphor. The techniques met* uses to differentiate between those are mainly based on hand-coded knowledge, which implies a number of limitations. In a similar manner manually created knowledge in the form of WordNet (Fellbaum, 1998) is employed by the system of Krishnakumaran and Zhu (2007), which essentially differentiates between highly lexicalized metaphors included in WordNet, and novel metaphorical senses. Alternative approaches (Gedigan et al., 2006) search for metaphors of a speciﬁc domain deﬁned a priori (e.g. MOTION metaphors) in a speciﬁc type of discourse (e.g. Wall Street Journal). In contrast, the scope of our experiments is the whole of the British National Corpus (BNC) (Burnard, 2007) and the domain of the expressions we identify is unrestricted. However, our technique is also distinguished from the systems of Fass (1991) and Krishnakumaran and Zhu (2007) in that it does not rely on any hand-crafted knowledge, but rather captures metaphoricity in an unsupervised way by means of verb and noun clustering. The motivation behind the"
C10-1113,P93-1024,0,0.220261,"oricity in a large corpus. 3.1 Clustering Motivation Abstract concepts that are associated with the same source domain are often related to each other on an intuitive and rather structural level, but their meanings, however, are not necessarily synonymous or even semantically close. The results of previous research on corpus-based lexical semantics suggest that the linguistic environment in which a lexical item occurs can shed light on its meaning. A number of works have shown that it is possible to automatically induce semantic word classes from corpus data via clustering of contextual cues (Pereira et al., 1993; Lin, 1998; Schulte im Walde, 2006). The consensus is that the lexical items exposing similar behavior in a large body of text most likely have the same meaning. However, the concepts of marriage and political regime, that are also observed in similar lexico-syntactic environments, albeit having quite distinct meanings are likewise assigned by such methods to the same cluster. In contrast to concrete concepts, such as tea, water, coffee, beer, drink, liquid, that are clustered together due to meaning similarity, abstract concepts tend to be clustered together by association with the same sour"
C10-1113,J98-1002,0,0.0506764,"Missing"
C10-1113,kingsbury-palmer-2002-treebank,0,0.181232,"Missing"
C10-1113,P07-1115,1,0.543632,"ur verb dataset is a subset of VerbNet compiled as follows. For all the verbs in VerbNet we 1004 extracted their occurrences (up to 10,000) from the raw corpus data collected originally by Korhonen et al. (2006) for construction of VALEX lexicon. Only the verbs found in this data more than 150 times were included in the experiment. For verb clustering, we adopted the best performing features of Sun and Korhonen (2009): automatically acquired verb subcategorization frames (SCFs) parameterized by their selectional preferences (SPs). We obtained these features using the SCF acquisition system of Preiss et al. (2007). The system tags and parses corpus data using the RASP parser and extracts SCFs from the resulting GRs using a rule-based classiﬁer which identiﬁes 168 SCF types for English verbs. It produces a lexical entry for each verb and SCF combination occurring in corpus data. We obtained SP s by clustering argument heads appearing in the subject and object slots of verbs in the resulting lexicon. Our noun dataset consists of 2000 most frequent nouns in the BNC. Following previous works on semantic noun classiﬁcation (Pantel and Lin, 2002; Bergsma et al., 2008), we used GRs as features for noun cluste"
C10-1113,P03-1009,1,0.889759,"Missing"
C10-1113,W97-0209,0,0.336393,"eported. Some of the clusters obtained as a result of applying the algorithm to our noun and verb datasets are demonstrated in Figures 1 and 2 respectively. The noun clusters represent target concepts that we expect to be associated with the same source concept (some suggested source concepts are given in Figure 1, although the system only captures those implicitly). 2 An eigenvector v is piecewise constant with respect to I if v(i) = v(j)∀i, j ∈ Ik and k ∈ 1, 2...K 1005 measure proposed by Resnik (1993) and successfully applied to a number of tasks in NLP including word sense disambiguation (Resnik, 1997). Resnik models selectional preference of a verb in probabilistic terms as the difference between the posterior distribution of noun classes in a particular relation with the verb and their prior distribution in that syntactic position regardless of the identity of the predicate. He quantiﬁes this difference using the relative entropy (or KullbackLeibler distance), deﬁning the selectional preference strength (SPS) as follows. Source: MECHANISM Target Cluster: consensus relation tradition partnership resistance foundation alliance friendship contact reserve unity link peace bond myth identity h"
C10-1113,korhonen-etal-2006-large,1,0.909373,"-word 1003 metaphors representing verb-subject and verbobject relations, where a verb is used metaphorically. The seed phrases include e.g. stir excitement, reﬂect enthusiasm, accelerate change, grasp theory, cast doubt, suppress memory, throw remark (verb - direct object constructions) and campaign surged, factor shaped [..], tension mounted, ideology embraces, changes operated, approach focuses, example illustrates (subject verb constructions). 2.2 Corpus The search space for metaphor identiﬁcation was the British National Corpus (BNC) that was parsed using the RASP parser of Briscoe et al. (2006). We used the grammatical relations output of RASP for BNC created by Andersen et al. (2008). The system searched the corpus for the source and target domain vocabulary within a particular grammatical relation (verb-object or verbsubject). 3 Method Starting from a small seed set of metaphorical expressions, the system implicitly captures the associations that underly their production and comprehension. It generalizes over these associations by means of unsupervised verb and noun clustering. The obtained clusters then represent potential source and target concepts between which metaphorical ass"
C10-1113,P99-1014,0,0.0176068,"ain. We then use unsupervised verb clustering to collect source domain vocabulary, which in turn allows us to harvest a large number of new metaphorical expressions. 3.2 Verb and Noun Clustering Since Levin (1993) published her classiﬁcation, there have been a number of attempts to automatically classify verbs into semantic classes using supervised and unsupervised approaches (Lin, 1998; Brew and Schulte im Walde, 2002; Korhonen et al., 2003; Schulte im Walde, 2006; Joanis et al., 2008; Sun and Korhonen, 2009). Similar methods were also applied to acquisition of noun classes from corpus data (Rooth et al., 1999; Pantel and Lin, 2002; Bergsma et al., 2008). We adopt a recent verb clustering approach of Sun and Korhonen (2009), who used rich syntactic and semantic features extracted using a shallow parser and a clustering method suitable for the resulting high dimensional feature space. When Sun and Korhonen evaluated their approach on 204 verbs from 17 Levin classes, they obtained 80.4 F-measure (which is high in particular for an unsupervised approach). We apply this approach to a much larger set of 1610 verbs: all the verb forms appearing in VerbNet (Kipper et al., 2006) with the exception of highl"
C10-1113,W07-0103,0,0.502837,"subject is an anomaly, which may as well indicate metaphorical use of drink. This approach was automated by Fass (1991) in his met* system. However, Fass himself indicated a problem with the method: it detects any kind of non-literalness or anomaly in language (metaphors, metonymies and others), i.e., it overgenerates with respect to metaphor. The techniques met* uses to differentiate between those are mainly based on hand-coded knowledge, which implies a number of limitations. In a similar manner manually created knowledge in the form of WordNet (Fellbaum, 1998) is employed by the system of Krishnakumaran and Zhu (2007), which essentially differentiates between highly lexicalized metaphors included in WordNet, and novel metaphorical senses. Alternative approaches (Gedigan et al., 2006) search for metaphors of a speciﬁc domain deﬁned a priori (e.g. MOTION metaphors) in a speciﬁc type of discourse (e.g. Wall Street Journal). In contrast, the scope of our experiments is the whole of the British National Corpus (BNC) (Burnard, 2007) and the domain of the expressions we identify is unrestricted. However, our technique is also distinguished from the systems of Fass (1991) and Krishnakumaran and Zhu (2007) in that"
C10-1113,J06-2001,0,0.0832386,"Missing"
C10-1113,shutova-teufel-2010-metaphor,1,0.359272,"e source and the target. The metaphor in (3) exempliﬁes a mapping of a concept of argument to that of war. The argument, which is the target concept, is viewed in terms of a battle (or a war), the source concept. The existence of such a link allows us to talk about arguments using the war terminology, thus giving rise to a number of metaphors. Characteristic to all areas of human activity (from poetic to ordinary to scientiﬁc) and, thus, to all types of discourse, metaphor becomes an important problem for natural language processing (NLP). In order to estimate the frequency of the phenomenon, Shutova and Teufel (2010) conducted a corpus study on a subset of the British National Corpus (BNC) (Burnard, 2007) representing various genres. They manually annotated metaphorical expressions in this data and found that 241 out of 761 sentences contained a metaphor, whereby in 164 phrases metaphoricity was introduced by a verb. Due to such a high frequency of their use, a system capable of recognizing and interpreting metaphorical expressions in unrestricted text would become an invaluable component of any semantics-oriented NLP application. Automatic processing of metaphor can be clearly divided into two subtasks:"
C10-1113,N10-1147,1,0.429452,"ain concepts using the verbs from the source domain lexicon. We tested our system starting with a collection of metaphorical expressions representing verbsubject and verb-object constructions, where the verb is used metaphorically. We evaluated the precision of metaphor identiﬁcation with the help of human judges. In addition to this we compared our system to a baseline built upon WordNet, whereby we demonstrated that our method goes far beyond synonymy and captures metaphors not directly related to any of those seen in the seed set. 2 2.1 Experimental Data Seed Phrases We used the dataset of Shutova (2010) as a seed set. Shutova (2010) annotated metaphorical expressions in a subset of the BNC sampling various genres: literature, newspaper/journal articles, essays on politics, international relations and history, radio broadcast (transcribed speech). The dataset consists of 62 phrases that are single-word 1003 metaphors representing verb-subject and verbobject relations, where a verb is used metaphorically. The seed phrases include e.g. stir excitement, reﬂect enthusiasm, accelerate change, grasp theory, cast doubt, suppress memory, throw remark (verb - direct object constructions) and campaign"
C10-1113,D09-1067,1,\N,Missing
C10-1113,C98-2122,0,\N,Missing
C10-1119,C08-1002,0,0.0282097,"Missing"
C10-1119,P98-1013,0,0.0945152,"Missing"
C10-1119,W02-1016,0,0.456187,"Missing"
C10-1119,P06-2012,0,0.0166757,"rona (2004). The method is introduced in the following section. The approach involves (i) taking the GRs (SUBJ , OBJ , IOBJ ) associated with verbs, (ii) extracting all the argument heads in these GRs, and (iii) clustering the resulting N most frequent argument heads into M classes. The empirically determined N 200 was used. The method produced 40 SP clusters. 5 Clustering Methods Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew and Schulte im Walde, 2002; Sun and Korhonen, 2009) and other similar NLP tasks involving high dimensional feature space (Chen et al., 2006). Following Sun and Korhonen (2009) we used the MNCut spectral clustering (Meila and Shi, 2001) which has a wide applicability and a clear probabilistic interpretation (von Luxburg, 2007; Verma and Meila, 2005). However, we extended the method to determine the optimal number of clusters automatically using the technique proposed by (Zelnik-Manor and Perona, 2004). Clustering groups a given set of verbs V = {vn }N n=1 into a disjoint partition of K classes. SPEC takes a similarity matrix as input. All our features can be viewed as probabilistic distributions because the combination of different"
C10-1119,P04-2007,0,0.0752821,"a big role in overall accuracy, and should therefore be investigated further (Sun and Korhonen, 2009). The relatively low performance of basic LP features in French suggests that at least some of the current errors are due to parsing. Future research should investigate the source of error at different stages of processing. In addition, it would be interesting to investigate whether language-specific tuning (e.g. using language specific features such as auxiliary classes) can further improve performance on French. Earlier works most closely related to ours are those of Merlo et al. (2002) and Ferrer (2004). Our results contrast with those of Ferrer who showed that a clustering approach does not transfer well from English to Spanish. However, she used basic SCF and named entity features only, and a clustering algorithm less suitable for high dimensional data. Like us, Merlo et al. (2002) created a gold standard by translating Levin classes to another language (Italian). They also applied a method developed for English to Italian, and reported good overall performance using features developed for English. Although the experiment was small (focussing on three classes and a few features only) and i"
C10-1119,C94-1042,0,0.0680517,"Missing"
C10-1119,N06-2015,0,0.0638806,"Missing"
C10-1119,P08-1050,0,0.288681,"ncluding e.g. computational lexicography, parsing, word sense disambiguation, semantic role labeling, information extraction, questionanswering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Abend et al., 2008). However, to date their exploitation has been limited because for most languages, no Levin style classification is available. Since manual classification is costly (Kipper et al., 2008) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (Joanis ´ S´eaghdha et al., 2008; Li and Brew, 2008; O and Copestake, 2008; Vlachos et al., 2009; Sun and Korhonen, 2009). However, most work on Levin type classification has focussed on English. Large-scale research on other languages such as German (Schulte im Walde, 2006) and Japanese (Suzuki and Fukumoto, 2009) has focussed on semantic classification. Although the two classification systems have shared properties, studies comparing the overlap between VerbNet and WordNet (Miller, 1995) have reported that the mapping is only partial and many to many due to fine-grained nature of classes based on synonymy (Shi and Mihalcea, 2005; Abend et al"
C10-1119,P02-1027,0,0.67511,"language more feasible. We take a recent verb clustering approach developed for English (Sun and Korhonen, 2009) and apply it to French – a major language for which no such experiment has been conducted yet. Basic NLP resources (corpora, taggers, parsers and subcategorization acquisition systems) are now sufficiently developed for this language for the application of a state-ofthe-art verb clustering approach to be realistic. Our investigation reveals similarities between the English and French classifications, supporting the linguistic hypothesis (Jackendoff, 1990) and the earlier result of Merlo et al. (2002) that Levin classes have a strong cross-linguistic basis. Not only the general methodology but also best performing features are transferable between the languages, making it possible to learn useful classes for French automatically without language-specific tuning. 2 French Gold Standard The development of an automatic verb classification approach requires at least an initial gold standard. Some syntactic (Gross, 1975) and semantic (Vossen, 1998) verb classifications exist for French, along with ones which integrate aspects of both (Saint-Dizier, 1998). Since none of these resources offer cla"
C10-1119,messiant-etal-2008-lexschem,1,0.904217,"passer Table 1: A Levin style gold standard for French • clustered the features using a method which has proved promising in both English and German experiments: spectral clustering, • evaluated the clusters both quantitatively (using the gold standard) and qualitatively, • and compared the performance to that recently obtained for English in order to gain a better understanding of the cross-linguistic and language-specific properties of verb classification This work is described in the subsequent sections. 3.1 Data: the LexSchem Lexicon We extracted the features for clustering from LexSchem (Messiant et al., 2008). This large subcategorization lexicon provides SCF frequency information for 3,297 French verbs. It was acquired fully automatically from Le Monde newspaper corpus (200M words from years 1991-2000) using ASSCI – a recent subcategorization acquisition system for French (Messiant, 2008). Systems similar to ASSCI have been used in recent verb classification works e.g. (Schulte im Walde, 2006; Li and Brew, 2008; Sun and Korhonen, 2009). Like these other systems, ASSCI takes raw corpus data as input. The data is first tagged and lemmatized using the Tree-Tagger and then parsed using Syntex (Bourig"
C10-1119,C08-1082,0,0.0846082,"Missing"
C10-1119,J05-1004,0,0.135398,"Missing"
C10-1119,D09-1067,1,0.119925,"sambiguation, semantic role labeling, information extraction, questionanswering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Abend et al., 2008). However, to date their exploitation has been limited because for most languages, no Levin style classification is available. Since manual classification is costly (Kipper et al., 2008) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (Joanis ´ S´eaghdha et al., 2008; Li and Brew, 2008; O and Copestake, 2008; Vlachos et al., 2009; Sun and Korhonen, 2009). However, most work on Levin type classification has focussed on English. Large-scale research on other languages such as German (Schulte im Walde, 2006) and Japanese (Suzuki and Fukumoto, 2009) has focussed on semantic classification. Although the two classification systems have shared properties, studies comparing the overlap between VerbNet and WordNet (Miller, 1995) have reported that the mapping is only partial and many to many due to fine-grained nature of classes based on synonymy (Shi and Mihalcea, 2005; Abend et al., 2008). Only few studies have been conducted on Levin style classifi"
C10-1119,W09-3205,0,0.218445,"Missing"
C10-1119,W09-0210,1,0.901693,"parsing, word sense disambiguation, semantic role labeling, information extraction, questionanswering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Abend et al., 2008). However, to date their exploitation has been limited because for most languages, no Levin style classification is available. Since manual classification is costly (Kipper et al., 2008) automatic approaches have been proposed recently which could be used to learn novel classifications in a cost-effective manner (Joanis ´ S´eaghdha et al., 2008; Li and Brew, 2008; O and Copestake, 2008; Vlachos et al., 2009; Sun and Korhonen, 2009). However, most work on Levin type classification has focussed on English. Large-scale research on other languages such as German (Schulte im Walde, 2006) and Japanese (Suzuki and Fukumoto, 2009) has focussed on semantic classification. Although the two classification systems have shared properties, studies comparing the overlap between VerbNet and WordNet (Miller, 1995) have reported that the mapping is only partial and many to many due to fine-grained nature of classes based on synonymy (Shi and Mihalcea, 2005; Abend et al., 2008). Only few studies have been conducte"
C10-1119,W04-3213,0,\N,Missing
C10-1119,J06-2001,0,\N,Missing
C10-1119,P08-3010,1,\N,Missing
C10-1119,C98-1013,0,\N,Missing
C12-1041,P11-1051,0,0.023453,"Missing"
C12-1041,W10-1913,1,0.918046,"Missing"
C12-1041,D11-1025,1,0.85233,"Missing"
C12-1041,liakata-etal-2010-corpora,0,0.179839,"Missing"
C12-1041,P08-1093,0,0.0490471,"Missing"
C12-1041,C10-1098,0,0.0532332,"Missing"
C12-1041,P10-1057,0,0.0767163,"Missing"
C12-1041,C10-1101,0,0.0980948,"Missing"
C12-1041,H05-1001,0,0.0804155,"Missing"
C12-1041,J02-4002,0,0.651433,"Missing"
C12-1041,D09-1155,0,0.326214,"Missing"
C12-1041,N03-1033,0,0.0324925,"Missing"
C12-1165,P09-1004,0,0.0254928,"roaches solely make use of distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007). All approaches model two-way verbargument co-occurrences, with the exception of Van de Cruys (2009) which models three-way verb-subject-object co-occurrences. To our knowledge, no previous method has learned SCFs and SPs jointly. Scheible (2010) used SCF s as features in a Predicate-Argument Clustering (Schulte im Walde et al., 2008) approach to SP acquisition, but did not evaluate the resulting clusters for SCF s and found that the SP method did not outperform previous methods. Abend et al. (2009) used co-occurrence measures to perform unsupervised argument-adjunct discrimination for PPs, but not full SCFs. Our method makes use of non-negative tensor factorization (NTF) (Shashua and Hazan, 2005). Tensor factorization is the multilinear generalization of matrix factorization. It has been extensively studied in the field of statistics (Kolda and Bader, 2009), and has yielded promising results on SP acquisition (Van de Cruys, 2009). We introduce a novel way of considering SCFs with an arbitrary number of arguments, and SPs as multi-way co-occurrences in the context of these larger SCFs. T"
C12-1165,W10-1612,0,0.307879,"Missing"
C12-1165,P05-1038,0,0.0793575,"COLING 2012: Technical Papers, pages 2703–2720, COLING 2012, Mumbai, December 2012. 2703 1 Introduction Verb subcategorization lexicons and selectional preference models capture two related aspects of verbal predicate-argument structure, with subcategorization describing the syntactic arguments taken by a verb, and selectional preferences describing the semantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and gen"
C12-1165,D07-1017,0,0.0330541,"uments into account. Lippincott et al. (2012) developed a graphical model for inducing verb frames in corpus data. The model identifies argument types of verbs but not sets of SCFs taken by a verb, as full scale SCF systems do. Recent SP acquisition approaches use latent semantic information to model SPs, making use of probabilistic models, such as latent Dirichlet allocation (LDA) (Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), or non-negative tensor factorization (NTF) 2705 (Van de Cruys, 2009). Other approaches solely make use of distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007). All approaches model two-way verbargument co-occurrences, with the exception of Van de Cruys (2009) which models three-way verb-subject-object co-occurrences. To our knowledge, no previous method has learned SCFs and SPs jointly. Scheible (2010) used SCF s as features in a Predicate-Argument Clustering (Schulte im Walde et al., 2008) approach to SP acquisition, but did not evaluate the resulting clusters for SCF s and found that the SP method did not outperform previous methods. Abend et al. (2009) used co-occurrence measures to perform unsupervised argument-"
C12-1165,W05-0621,0,0.458257,"2012. 2703 1 Introduction Verb subcategorization lexicons and selectional preference models capture two related aspects of verbal predicate-argument structure, with subcategorization describing the syntactic arguments taken by a verb, and selectional preferences describing the semantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a num"
C12-1165,A97-1052,0,0.365805,"which we investigate the model’s ability to induce preferences for the co-occurrence of a particular verb lemma and all of its arguments at the same time. The model achieves a high accuracy of 77.8 on this new evaluation. We also perform a qualitative evaluation which shows that the joint model is capable of learning rich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tu"
C12-1165,briscoe-carroll-2002-robust,0,0.0250342,"ext of these larger SCFs. The resulting model provides an ideal framework for joint acquisition of SCF and SP information. The only form of supervision in the model is parameter estimation and choice of the best feature set via cross-validation. 3 Subcategorization Frame Inventory To facilitate thorough qualitative evaluation (Section 5.6), we defined our SCFs in terms of syntactic slots, and in the form of common GRs. Finer-grained inventories including lexicalized elements and semantic interpretation were left for future work (see Section 7). We use the GR types produced by the RASP parser (Briscoe and Carroll, 2002). Altogether we experimented with combinations of nine GR types out of the 131 which can be headed by verbs, selected on the basis of their frequency in the parsed BNC corpus and relevance for subcategorization. For this initial experiment, we focused on higher-frequency arguments since they will have the greatest impact on downstream applications. Our first eight basic GR types are as follows. In subject position we included non-clausal subjects (SUBJ)2 , ignoring sentences with clausal subjects, which are much less frequent. Since objects are key arguments for subcategorization, we included"
C12-1165,chesley-salmon-alt-2006-automatic,0,0.291885,"evaluation which shows that the joint model is capable of learning rich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds are used to select frames for the final lexicon. These ‘inductive’ approaches have achieved respectable accuracy (60-70 F-measure against a dictionary) and are more portable than earlier methods. However, their ability to improve in accura"
C12-1165,D10-1088,0,0.190764,"Missing"
C12-1165,D10-1113,0,0.0175113,"have achieved respectable accuracy (60-70 F-measure against a dictionary) and are more portable than earlier methods. However, their ability to improve in accuracy is limited by their inability to incorporate information beyond the GR co-occurrences and heuristics that identify candidate SCFs on a per-sentence basis. Such cues provide no capacity for learning further from the data, e.g. from the lexical content of verbal arguments or from other GRs which are not part of the SCF. Unsupervised machine learning has been applied to tasks where portability is equally important (Blei et al., 2003; Dinu and Lapata, 2010) but its application to SCF acquisition remains limited. Carroll and Rooth (1996) combined a head-lexicalized context-free grammar with an expectation-maximization (EM) algorithm to acquire an SCF lexicon. D˛ ebowski (2009) used a filtering method based on the point-wise co-occurrence of arguments in parsed data to acquire a Polish SCF lexicon, but this method does not take the semantics of the verb’s arguments into account. Lippincott et al. (2012) developed a graphical model for inducing verb frames in corpus data. The model identifies argument types of verbs but not sets of SCFs taken by a"
C12-1165,P07-1028,0,0.0469768,"12) developed a graphical model for inducing verb frames in corpus data. The model identifies argument types of verbs but not sets of SCFs taken by a verb, as full scale SCF systems do. Recent SP acquisition approaches use latent semantic information to model SPs, making use of probabilistic models, such as latent Dirichlet allocation (LDA) (Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), or non-negative tensor factorization (NTF) 2705 (Van de Cruys, 2009). Other approaches solely make use of distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007). All approaches model two-way verbargument co-occurrences, with the exception of Van de Cruys (2009) which models three-way verb-subject-object co-occurrences. To our knowledge, no previous method has learned SCFs and SPs jointly. Scheible (2010) used SCF s as features in a Predicate-Argument Clustering (Schulte im Walde et al., 2008) approach to SP acquisition, but did not evaluate the resulting clusters for SCF s and found that the SP method did not outperform previous methods. Abend et al. (2009) used co-occurrence measures to perform unsupervised argument-adjunct discrimination for PPs, b"
C12-1165,J02-3001,0,0.0962391,"mantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausal complement). (1) [Our October review]SUBJ comprehensively [shows]VERB [you]DOBJ [what’s in store in next month’s magazine]CC"
C12-1165,han-etal-2000-handling,0,0.0723478,"gument structure, with subcategorization describing the syntactic arguments taken by a verb, and selectional preferences describing the semantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, dire"
C12-1165,ienco-etal-2008-automatic,0,0.284255,"e joint model is capable of learning rich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds are used to select frames for the final lexicon. These ‘inductive’ approaches have achieved respectable accuracy (60-70 F-measure against a dictionary) and are more portable than earlier methods. However, their ability to improve in accuracy is limited by thei"
C12-1165,kawahara-kurohashi-2010-acquiring,0,0.239393,"ects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds are used to select frames for the final lexicon. These ‘inductive’ approaches have achieved respectable accuracy (60-70 F-measure against a dictionary) and are more portable than earlier methods. However, their ability to improve in accuracy is limited by their inability to incorporate information beyond the GR co-occurrences and heuristics that identify candidate SC"
C12-1165,W02-0907,1,0.698114,"el’s ability to induce preferences for the co-occurrence of a particular verb lemma and all of its arguments at the same time. The model achieves a high accuracy of 77.8 on this new evaluation. We also perform a qualitative evaluation which shows that the joint model is capable of learning rich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds ar"
C12-1165,korhonen-etal-2006-large,1,0.926297,"the outer products of N (in this case three) vectors. Figure 2: Graphical representation of the NTF as the sum of outer products. Computationally, the NTF model is fitted by applying an alternating least-squares algorithm. In each iteration, two of the modes are fixed and the third one is fitted in a least squares sense. This process is repeated until convergence.3 4.2 Construction of verb-argument tensors In order to discover SCFs and SPs, we construct a tensor that contains the multi-way cooccurrences of a verb and its different arguments. 4.2.1 Corpus data We used a subset of the corpus of Korhonen et al. (2006), which consists of up to 10,000 sentences for each of approximately 6400 verbs, with data taken from five large British and American cross-domain corpora. To ensure sufficient data for each verb, we included verbs with at least 500 occurrences, yielding a total of 1993 verbs. The corpus data was tokenized, POS -tagged, lemmatized, and parsed with the RASP system (Briscoe and Carroll, 2002). RASP uses a tag-sequence grammar, and is unlexicalized, so that the parser’s lexicon does not interfere with SCF acquisition. RASP produces output in the form of GRs. Passive sentences and those with claus"
C12-1165,lenci-etal-2008-unsupervised,0,0.308432,"ich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds are used to select frames for the final lexicon. These ‘inductive’ approaches have achieved respectable accuracy (60-70 F-measure against a dictionary) and are more portable than earlier methods. However, their ability to improve in accuracy is limited by their inability to incorporate information"
C12-1165,P12-1044,1,0.661234,"Rs which are not part of the SCF. Unsupervised machine learning has been applied to tasks where portability is equally important (Blei et al., 2003; Dinu and Lapata, 2010) but its application to SCF acquisition remains limited. Carroll and Rooth (1996) combined a head-lexicalized context-free grammar with an expectation-maximization (EM) algorithm to acquire an SCF lexicon. D˛ ebowski (2009) used a filtering method based on the point-wise co-occurrence of arguments in parsed data to acquire a Polish SCF lexicon, but this method does not take the semantics of the verb’s arguments into account. Lippincott et al. (2012) developed a graphical model for inducing verb frames in corpus data. The model identifies argument types of verbs but not sets of SCFs taken by a verb, as full scale SCF systems do. Recent SP acquisition approaches use latent semantic information to model SPs, making use of probabilistic models, such as latent Dirichlet allocation (LDA) (Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), or non-negative tensor factorization (NTF) 2705 (Van de Cruys, 2009). Other approaches solely make use of distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk"
C12-1165,P08-3010,0,0.0181622,"ble of learning rich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds are used to select frames for the final lexicon. These ‘inductive’ approaches have achieved respectable accuracy (60-70 F-measure against a dictionary) and are more portable than earlier methods. However, their ability to improve in accuracy is limited by their inability to in"
C12-1165,W05-1002,0,0.44833,"ion Verb subcategorization lexicons and selectional preference models capture two related aspects of verbal predicate-argument structure, with subcategorization describing the syntactic arguments taken by a verb, and selectional preferences describing the semantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their sy"
C12-1165,J05-3003,0,0.569078,"Missing"
C12-1165,P10-1045,0,0.135579,"Missing"
C12-1165,P07-1115,1,0.94609,"nduce preferences for the co-occurrence of a particular verb lemma and all of its arguments at the same time. The model achieves a high accuracy of 77.8 on this new evaluation. We also perform a qualitative evaluation which shows that the joint model is capable of learning rich lexical information about both syntactic and semantic aspects of verb behaviour in data. 2 Related Work Recent SCF acquisition approaches use the output of an unlexicalized parser to generate SCF hypotheses, followed by statistical filtering and/or smoothing to remove noise. Briscoe and Carroll (1997); Korhonen (2002); Preiss et al. (2007) use handcrafted rules to match parser output to a pre-defined set of SCFs, achieving an F-measure of about 70 against a manually annotated gold standard, while O’Donovan et al. (2005); Chesley and Salmon-Alt (2006); Ienco et al. (2008); Messiant (2008); Lenci et al. (2008); Altamirano and Alonso i Alemany (2010); Kawahara and Kurohashi (2010) induce the inventory of SCFs from parsed corpus data. Candidate frames are identified by grammatical relation (GR) co-occurrences, often aided by language-specific heuristics. Statistical filtering or empirically-tuned thresholds are used to select frame"
C12-1165,D11-1130,0,0.285999,"tested. As the two types of lexical information – SCFs and SPs – are closely interlinked and can complement each other, it would make sense to acquire them jointly. However, to the best of our knowledge, no previous work has developed a model for their joint acquisition. Unsupervised machine learning is attractive for lexical acquisition because it works where little labeled data is available, and ports easily between tasks and languages. Increasingly sophisticated techniques have been applied to SP induction (Rooth et al., 1999; Van de Cruys, 2009; Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011) while work 2704 on unsupervised SCF acquisition has been limited (Carroll and Rooth, 1996). In this paper we present a largely unsupervised method for the joint acquisition of SCFs and SPs, adapting a method that has been successfully used for SP induction (Van de Cruys, 2009) so that it learns whether a verb subcategorizes for a particular argument slot together with which lexical items occur in the slot. Our method uses a co-occurrence model augmented with a factorization algorithm to cluster verbs from a large corpus. Specifically, we use non-negative tensor factorization (NTF) (Shashua an"
C12-1165,W97-0209,0,0.142124,"pport NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausal complement). (1) [Our October review]SUBJ comprehensively [shows]VERB [you]DOBJ [what’s in store in next month’s magazine]CCOMP . Predicting the set of SCFs for a verb can be viewed as a multi-"
C12-1165,P99-1014,0,0.495283,"he full range of verbal arguments, including e.g. clausal complements, has not been tested. As the two types of lexical information – SCFs and SPs – are closely interlinked and can complement each other, it would make sense to acquire them jointly. However, to the best of our knowledge, no previous work has developed a model for their joint acquisition. Unsupervised machine learning is attractive for lexical acquisition because it works where little labeled data is available, and ports easily between tasks and languages. Increasingly sophisticated techniques have been applied to SP induction (Rooth et al., 1999; Van de Cruys, 2009; Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011) while work 2704 on unsupervised SCF acquisition has been limited (Carroll and Rooth, 1996). In this paper we present a largely unsupervised method for the joint acquisition of SCFs and SPs, adapting a method that has been successfully used for SP induction (Van de Cruys, 2009) so that it learns whether a verb subcategorizes for a particular argument slot together with which lexical items occur in the slot. Our method uses a co-occurrence model augmented with a factorization algorithm to cluster verbs"
C12-1165,scheible-2010-evaluation,0,0.017901,"ormation to model SPs, making use of probabilistic models, such as latent Dirichlet allocation (LDA) (Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), or non-negative tensor factorization (NTF) 2705 (Van de Cruys, 2009). Other approaches solely make use of distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007). All approaches model two-way verbargument co-occurrences, with the exception of Van de Cruys (2009) which models three-way verb-subject-object co-occurrences. To our knowledge, no previous method has learned SCFs and SPs jointly. Scheible (2010) used SCF s as features in a Predicate-Argument Clustering (Schulte im Walde et al., 2008) approach to SP acquisition, but did not evaluate the resulting clusters for SCF s and found that the SP method did not outperform previous methods. Abend et al. (2009) used co-occurrence measures to perform unsupervised argument-adjunct discrimination for PPs, but not full SCFs. Our method makes use of non-negative tensor factorization (NTF) (Shashua and Hazan, 2005). Tensor factorization is the multilinear generalization of matrix factorization. It has been extensively studied in the field of statistics"
C12-1165,J06-2001,0,0.468846,"Missing"
C12-1165,P08-1057,0,0.0542037,"location (LDA) (Ó Séaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), or non-negative tensor factorization (NTF) 2705 (Van de Cruys, 2009). Other approaches solely make use of distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007). All approaches model two-way verbargument co-occurrences, with the exception of Van de Cruys (2009) which models three-way verb-subject-object co-occurrences. To our knowledge, no previous method has learned SCFs and SPs jointly. Scheible (2010) used SCF s as features in a Predicate-Argument Clustering (Schulte im Walde et al., 2008) approach to SP acquisition, but did not evaluate the resulting clusters for SCF s and found that the SP method did not outperform previous methods. Abend et al. (2009) used co-occurrence measures to perform unsupervised argument-adjunct discrimination for PPs, but not full SCFs. Our method makes use of non-negative tensor factorization (NTF) (Shashua and Hazan, 2005). Tensor factorization is the multilinear generalization of matrix factorization. It has been extensively studied in the field of statistics (Kolda and Bader, 2009), and has yielded promising results on SP acquisition (Van de Cruy"
C12-1165,D11-1097,1,0.853085,"on about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausal complement). (1) [Our October review]SUBJ comprehensively [shows]VERB [you]DOBJ [what’s in store in next month’s magazine]CCOMP . Predicting the set of SCFs for a verb can be viewed as a multi-way co-occurrence problem of a verb and its differe"
C12-1165,D11-1095,1,0.711543,"apture two related aspects of verbal predicate-argument structure, with subcategorization describing the syntactic arguments taken by a verb, and selectional preferences describing the semantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show take"
C12-1165,P10-1097,0,0.0405782,"s requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausal complement). (1) [Our October review]SUBJ comprehensively [shows]VERB [you]DOBJ [what’s in store in next month’s magazine]CCOMP . Predicting the set of SCFs for a verb can be viewed as a multi-way co-occurrence pro"
C12-1165,W09-0211,1,0.880443,"Missing"
C12-1165,C00-2137,0,0.0131193,"or head features, extended PPs, and split clausal types (row 9), without losing out on F-score. This suggests that lexical-semantic features are valuable for SCF acquisition. Another trend is towards more accurate models with fewer additional features; individual features and pairs of features seem to provide the most improvement (rows 1-7) over the base model (row 14), but the model with all additional features (row 16) has markedly worse performance, which may indicate a data sparsity problem. We carried out significance tests for the mentioned model differences using stratified shuffling (Yeh, 2000). These tests indicate that most of the models (rows 1-11) have significantly higher F-score than the baseline, and most show significant pairwise differences in precision and recall. Parameter tuning with cross-validation resulted in a θvoid of 0.4 (though exploration of the models in Table 4 showed that some models performed better with even lower values). This means that the model only needs to assign a relatively low confidence score to the void feature to infer that a slot is not part of an SCF. This is probably because adjuncts and other noise in the data means that these slots are fille"
C12-1165,P09-2019,0,0.06059,"ve for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausal complement). (1) [Our October review]SUBJ comprehensively [shows]VERB [you]DOBJ [what’s in store in next month’s magazine]CCOMP . Predicting the set"
C12-1165,P11-1156,0,0.0464203,"nd selectional preferences describing the semantic preferences verbs have for their arguments. Each type of information can support NLP tasks requiring information about predicate-argument structure. For example, subcategorization has proved useful for parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (Han et al., 2000; Hajiˇc et al., 2002), while selectional preferences have benefited parsing (Zhou et al., 2011), semantic role labeling (Gildea and Jurafsky, 2002; Zapirain et al., 2009), and word sense disambiguation (Resnik, 1997; Thater et al., 2010; Seaghdha and Korhonen, 2011). Verb subcategorization frame (SCF) induction involves identifying the arguments of a verb lemma in a corpus, and generalizing about the frames taken by the verb, where each frame includes a number of arguments and their syntactic types. Consider e.g. sentence (1), where the verb show takes the frame SUBJ-DOBJ-CCOMP (subject, direct object, and clausal complement). (1) [Our October review]SUBJ comprehensively [shows]VERB [yo"
C12-1165,W98-1505,0,\N,Missing
C12-1165,P10-1044,0,\N,Missing
C12-2097,P04-1056,0,0.0167077,"ross-document (e.g. sentence similarity) levels, and can be flexibly applied to both fully unsupervised and transductive learning scenarios, depending on how much prior knowledge about the task is actually available. Although the transductive learning scenario can realistically occur when developing new corpora or applications, it has not been addressed in previous work on our task. Corpus level Inference A number of recent models have obtained improved performance by sharing information between sentences and documents in large text collections (Sutton and McCallum, 2004; Taskar et al., 2002; Bunescu and Mooney, 2004; Finkel et al., 2005; Gupta et al., 2010; Rush et al., 2012; Reichart and Barzilay, 2012; Ganchev et al., 2010; Gillenwater et al., 2010; Mann and McCallum, 2010; Liang et al., 2009; Roth and Yih, 2005). We follow these works and model inter-sentence similarity across multiple scientific documents. To the best of our knowledge, this is the first model for the AZ classification task that explicitly shares information among sentences in different documents. 3 Model Given a set of scientific documents our goal is to assign each sentence in these documents into a category that represents its role"
C12-2097,P05-1045,0,0.0244601,"ce similarity) levels, and can be flexibly applied to both fully unsupervised and transductive learning scenarios, depending on how much prior knowledge about the task is actually available. Although the transductive learning scenario can realistically occur when developing new corpora or applications, it has not been addressed in previous work on our task. Corpus level Inference A number of recent models have obtained improved performance by sharing information between sentences and documents in large text collections (Sutton and McCallum, 2004; Taskar et al., 2002; Bunescu and Mooney, 2004; Finkel et al., 2005; Gupta et al., 2010; Rush et al., 2012; Reichart and Barzilay, 2012; Ganchev et al., 2010; Gillenwater et al., 2010; Mann and McCallum, 2010; Liang et al., 2009; Roth and Yih, 2005). We follow these works and model inter-sentence similarity across multiple scientific documents. To the best of our knowledge, this is the first model for the AZ classification task that explicitly shares information among sentences in different documents. 3 Model Given a set of scientific documents our goal is to assign each sentence in these documents into a category that represents its role in the information s"
C12-2097,P10-2036,0,0.0196981,"os, depending on how much prior knowledge about the task is actually available. Although the transductive learning scenario can realistically occur when developing new corpora or applications, it has not been addressed in previous work on our task. Corpus level Inference A number of recent models have obtained improved performance by sharing information between sentences and documents in large text collections (Sutton and McCallum, 2004; Taskar et al., 2002; Bunescu and Mooney, 2004; Finkel et al., 2005; Gupta et al., 2010; Rush et al., 2012; Reichart and Barzilay, 2012; Ganchev et al., 2010; Gillenwater et al., 2010; Mann and McCallum, 2010; Liang et al., 2009; Roth and Yih, 2005). We follow these works and model inter-sentence similarity across multiple scientific documents. To the best of our knowledge, this is the first model for the AZ classification task that explicitly shares information among sentences in different documents. 3 Model Given a set of scientific documents our goal is to assign each sentence in these documents into a category that represents its role in the information structure of the document. As our data is biomedical, we use the version of the AZ scheme adapted for biology by (Miz"
C12-2097,D11-1025,1,0.888857,"le to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks such as information retrieval and extraction (Teufel, 2001; Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). This comes, however, with a heavy cost of requiring thousands of manually annotated sentences to achieve good performance. Even the weakly supervised approach by (Guo and Korhonen, 2011) requires hundreds of annotated sentences for optimal performance. In this paper we focus on a primarily unsupervised approach to inferring information structure which avoids the high annotation cost of the supervised approaches. The only previous work on this topic that we are aware of is that of (Varge et al., 2012) who proposed a simple word-level Latent Dirichlet Allocation (LDA) model to the task, assuming that the phenomenon is mostly lexical. As we show in this paper, the information structure of scientific documents is governed by a number of additional factors, which calls for a more"
C12-2097,W10-1913,1,0.871954,"riments the algorithm provably finds the exact MAP assignment. 996 We compare the predictions of our model to those of argumentative zoning (AZ) – a widely used information structure scheme (Teufel and Moens, 2002) where the core categories are argued to be domain-independent and which has been used to analyse texts in various disciplines such as computational linguistics (Teufel and Moens, 2002), law (Hachey and Grover, 2006), biology (Mizuta et al., 2006) and chemistry (Teufel et al., 2009). We experiment with the only publicly available AZ corpus: the corpus of 792 biomedical abstracts by (Guo et al., 2010) which provides AZ annotations for 7886 sentences. Our experimental evaluation shows that the model outperforms traditional algorithms for both the unsupervised and the transductive setups. by a large margin Our results show that it is possible to infer high quality knowledge about the information structure of scientific documents even when only little or no human annotation effort is involved. 2 Previous Work Machine Learning for Information Structure Nearly all previous work on automatic detection of information structure has relied on supervised algorithms and, consequently, on corpora cons"
C12-2097,I08-1050,0,0.607355,"G 2012, Mumbai, December 2012. 995 1 Introduction Information structure of scientific literature (i.e. the way scientists communicate their ideas, methods, results, conclusions, and so forth, to their audience) has been a topic of intense research within different disciplines (Taboada and Mann, 2006; Argamon et al., 2008; Deane et al., 2008; Lungen et al., 2010). Within Natural Language Processing (NLP), various schemes have been proposed for describing the information structure of scientific documents. These have been based on, for example, section names found in documents (Lin et al., 2006; Hirohata et al., 2008), rhetorical or argumentative zones (AZ) of sentences (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative aspects of scientific information (Shatkay et al., 2008) or core scientific concepts (Liakata et al., 2010). Previous works have shown that it is possible to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks such as information retri"
C12-2097,liakata-etal-2010-corpora,0,0.121932,"earch within different disciplines (Taboada and Mann, 2006; Argamon et al., 2008; Deane et al., 2008; Lungen et al., 2010). Within Natural Language Processing (NLP), various schemes have been proposed for describing the information structure of scientific documents. These have been based on, for example, section names found in documents (Lin et al., 2006; Hirohata et al., 2008), rhetorical or argumentative zones (AZ) of sentences (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative aspects of scientific information (Shatkay et al., 2008) or core scientific concepts (Liakata et al., 2010). Previous works have shown that it is possible to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks such as information retrieval and extraction (Teufel, 2001; Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). This comes, however, with a heavy cost of requiring thousands of manually annotated sentences to achieve good performa"
C12-2097,W06-3309,0,0.686947,"es 995–1006, COLING 2012, Mumbai, December 2012. 995 1 Introduction Information structure of scientific literature (i.e. the way scientists communicate their ideas, methods, results, conclusions, and so forth, to their audience) has been a topic of intense research within different disciplines (Taboada and Mann, 2006; Argamon et al., 2008; Deane et al., 2008; Lungen et al., 2010). Within Natural Language Processing (NLP), various schemes have been proposed for describing the information structure of scientific documents. These have been based on, for example, section names found in documents (Lin et al., 2006; Hirohata et al., 2008), rhetorical or argumentative zones (AZ) of sentences (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative aspects of scientific information (Shatkay et al., 2008) or core scientific concepts (Liakata et al., 2010). Previous works have shown that it is possible to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks s"
C12-2097,N12-1008,1,0.8603,"ully unsupervised and transductive learning scenarios, depending on how much prior knowledge about the task is actually available. Although the transductive learning scenario can realistically occur when developing new corpora or applications, it has not been addressed in previous work on our task. Corpus level Inference A number of recent models have obtained improved performance by sharing information between sentences and documents in large text collections (Sutton and McCallum, 2004; Taskar et al., 2002; Bunescu and Mooney, 2004; Finkel et al., 2005; Gupta et al., 2010; Rush et al., 2012; Reichart and Barzilay, 2012; Ganchev et al., 2010; Gillenwater et al., 2010; Mann and McCallum, 2010; Liang et al., 2009; Roth and Yih, 2005). We follow these works and model inter-sentence similarity across multiple scientific documents. To the best of our knowledge, this is the first model for the AZ classification task that explicitly shares information among sentences in different documents. 3 Model Given a set of scientific documents our goal is to assign each sentence in these documents into a category that represents its role in the information structure of the document. As our data is biomedical, we use the vers"
C12-2097,D12-1131,1,0.849641,"y applied to both fully unsupervised and transductive learning scenarios, depending on how much prior knowledge about the task is actually available. Although the transductive learning scenario can realistically occur when developing new corpora or applications, it has not been addressed in previous work on our task. Corpus level Inference A number of recent models have obtained improved performance by sharing information between sentences and documents in large text collections (Sutton and McCallum, 2004; Taskar et al., 2002; Bunescu and Mooney, 2004; Finkel et al., 2005; Gupta et al., 2010; Rush et al., 2012; Reichart and Barzilay, 2012; Ganchev et al., 2010; Gillenwater et al., 2010; Mann and McCallum, 2010; Liang et al., 2009; Roth and Yih, 2005). We follow these works and model inter-sentence similarity across multiple scientific documents. To the best of our knowledge, this is the first model for the AZ classification task that explicitly shares information among sentences in different documents. 3 Model Given a set of scientific documents our goal is to assign each sentence in these documents into a category that represents its role in the information structure of the document. As our data i"
C12-2097,J02-4002,0,0.925022,"ientific literature (i.e. the way scientists communicate their ideas, methods, results, conclusions, and so forth, to their audience) has been a topic of intense research within different disciplines (Taboada and Mann, 2006; Argamon et al., 2008; Deane et al., 2008; Lungen et al., 2010). Within Natural Language Processing (NLP), various schemes have been proposed for describing the information structure of scientific documents. These have been based on, for example, section names found in documents (Lin et al., 2006; Hirohata et al., 2008), rhetorical or argumentative zones (AZ) of sentences (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative aspects of scientific information (Shatkay et al., 2008) or core scientific concepts (Liakata et al., 2010). Previous works have shown that it is possible to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks such as information retrieval and extraction (Teufel, 2001; Teufel and Moens, 2002; Mizuta et al., 200"
C12-2097,D09-1155,0,0.28462,"communicate their ideas, methods, results, conclusions, and so forth, to their audience) has been a topic of intense research within different disciplines (Taboada and Mann, 2006; Argamon et al., 2008; Deane et al., 2008; Lungen et al., 2010). Within Natural Language Processing (NLP), various schemes have been proposed for describing the information structure of scientific documents. These have been based on, for example, section names found in documents (Lin et al., 2006; Hirohata et al., 2008), rhetorical or argumentative zones (AZ) of sentences (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative aspects of scientific information (Shatkay et al., 2008) or core scientific concepts (Liakata et al., 2010). Previous works have shown that it is possible to classify sentences in scientific documents according to the categories of such schemes (e.g. the Background, Method, Results and Conclusions categories of the AZ scheme) using supervised methods. These methods perform very well and their output has proved useful for important tasks such as information retrieval and extraction (Teufel, 2001; Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007"
C12-2097,varga-etal-2012-unsupervised,0,0.435568,"extraction (Teufel, 2001; Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). This comes, however, with a heavy cost of requiring thousands of manually annotated sentences to achieve good performance. Even the weakly supervised approach by (Guo and Korhonen, 2011) requires hundreds of annotated sentences for optimal performance. In this paper we focus on a primarily unsupervised approach to inferring information structure which avoids the high annotation cost of the supervised approaches. The only previous work on this topic that we are aware of is that of (Varge et al., 2012) who proposed a simple word-level Latent Dirichlet Allocation (LDA) model to the task, assuming that the phenomenon is mostly lexical. As we show in this paper, the information structure of scientific documents is governed by a number of additional factors, which calls for a more expressive model. We propose a more sophisticated and flexible model capable of integrating different types of task knowledge, depending on the knowledge available in a real-life situation. We investigate two scenarios: (1) the fully unsupervised scenario where no manually annotated sentences are available; and (2) th"
C12-2097,W04-1202,0,\N,Missing
C12-2097,L10-1000,0,\N,Missing
C12-2109,P06-4020,0,0.0159027,"e.g. speed up would select for MACHINES, or VEHICLES, rather than CHANGE (the target domain), whereas the ones used literally for the target domain, e.g. facilitate would select for PROCESSES (including CHANGE). We therefore expect that selecting the verbs whose preferences the noun in the metaphorical expression matches best should allow us to filter out non-literalness. We automatically acquired selectional preference (SP) distributions of the candidate substitutes (for subject-verb and verb-object relations) from the British National Corpus (BNC) (Burnard, 2007) parsed by the RASP parser (Briscoe et al., 2006). We obtained SP classes by clustering the 2000 most frequent nouns in the BNC into 200 clusters using the algorithm of Sun and Korhonen (2009). We quantified selectional preferences using the association measure proposed by Resnik (1993). It represents SPs as the difference between the posterior distribution of noun classes in a particular relation with the verb and their prior distribution in that syntactic position irrespective of the identity of the verb. This difference then defines the selectional preference strength (SPS) of the verb, quantified in terms of Kullback-Leibler divergence a"
C12-2109,E03-1034,0,0.209562,"Missing"
C12-2109,D08-1094,0,0.0272439,"restricted text. The recent metaphor paraphrasing approach of Shutova (2010) was designed with this requirement in mind and used statistical methods, but still relied on the WordNet (Fellbaum, 1998) database to generate the initial set of paraphrases. In this paper, we take the metaphor paraphrasing task a step further and present a fully unsupervised approach to this problem. In our method, candidate substitutes for the metaphorical term are generated using a vector space model. Vector space models have been previously used in the general lexical substitution task (Mitchell and Lapata, 2008; Erk and Padó, 2008, 2009; Thater et al., 2009, 2010; Erk and Padó, 2010; Van de Cruys et al., 2011). However, (to the best of our knowledge) they have not yet been deployed in tasks involving figurative meaning transfers, such as interpretation of metonymy or metaphor. In this paper, we address this problem and apply a vector space model of word meaning in context to metaphor paraphrasing, appropriately adapting it to the task. In comparison to lexical substitution, metaphor paraphrasing presents an additional challenge, namely that of discriminating between literal and metaphorical substitutes. Shutova (2010)"
C12-2109,W09-0208,0,0.178835,"Missing"
C12-2109,P10-2017,0,0.0156901,"roach of Shutova (2010) was designed with this requirement in mind and used statistical methods, but still relied on the WordNet (Fellbaum, 1998) database to generate the initial set of paraphrases. In this paper, we take the metaphor paraphrasing task a step further and present a fully unsupervised approach to this problem. In our method, candidate substitutes for the metaphorical term are generated using a vector space model. Vector space models have been previously used in the general lexical substitution task (Mitchell and Lapata, 2008; Erk and Padó, 2008, 2009; Thater et al., 2009, 2010; Erk and Padó, 2010; Van de Cruys et al., 2011). However, (to the best of our knowledge) they have not yet been deployed in tasks involving figurative meaning transfers, such as interpretation of metonymy or metaphor. In this paper, we address this problem and apply a vector space model of word meaning in context to metaphor paraphrasing, appropriately adapting it to the task. In comparison to lexical substitution, metaphor paraphrasing presents an additional challenge, namely that of discriminating between literal and metaphorical substitutes. Shutova (2010) used a selectional preference-based model for this pu"
C12-2109,J91-1003,0,0.870472,"ird sentence in general domain text contains a metaphorical expression). Due to this high frequency usage, a system capable of recognizing and interpreting metaphorical expressions in unrestricted text would become an invaluable component of many semantics-oriented NLP applications. The majority of previous computational approaches to metaphor rely on manually created knowledge and thus operate on a limited domain and are expensive to build and extend. Handcoded knowledge has proved useful for both metaphor identification, i.e. distinguishing between literal and metaphorical language in text (Fass, 1991; Martin, 1990; Krishnakumaran and Zhu, 2007; Gedigian et al., 2006) and metaphor interpretation, i.e. identifying the intended literal meaning of a metaphorical expression (Fass, 1991; Martin, 1990; Narayanan, 1997; Barnden and Lee, 2002). However, to be applicable in a real-world setting a metaphor processing system needs to be able to identify and interpret metaphorical expressions in unrestricted text. The recent metaphor paraphrasing approach of Shutova (2010) was designed with this requirement in mind and used statistical methods, but still relied on the WordNet (Fellbaum, 1998) database"
C12-2109,W06-3506,0,0.659038,"cal expression). Due to this high frequency usage, a system capable of recognizing and interpreting metaphorical expressions in unrestricted text would become an invaluable component of many semantics-oriented NLP applications. The majority of previous computational approaches to metaphor rely on manually created knowledge and thus operate on a limited domain and are expensive to build and extend. Handcoded knowledge has proved useful for both metaphor identification, i.e. distinguishing between literal and metaphorical language in text (Fass, 1991; Martin, 1990; Krishnakumaran and Zhu, 2007; Gedigian et al., 2006) and metaphor interpretation, i.e. identifying the intended literal meaning of a metaphorical expression (Fass, 1991; Martin, 1990; Narayanan, 1997; Barnden and Lee, 2002). However, to be applicable in a real-world setting a metaphor processing system needs to be able to identify and interpret metaphorical expressions in unrestricted text. The recent metaphor paraphrasing approach of Shutova (2010) was designed with this requirement in mind and used statistical methods, but still relied on the WordNet (Fellbaum, 1998) database to generate the initial set of paraphrases. In this paper, we take"
C12-2109,W07-0103,0,0.212135,"main text contains a metaphorical expression). Due to this high frequency usage, a system capable of recognizing and interpreting metaphorical expressions in unrestricted text would become an invaluable component of many semantics-oriented NLP applications. The majority of previous computational approaches to metaphor rely on manually created knowledge and thus operate on a limited domain and are expensive to build and extend. Handcoded knowledge has proved useful for both metaphor identification, i.e. distinguishing between literal and metaphorical language in text (Fass, 1991; Martin, 1990; Krishnakumaran and Zhu, 2007; Gedigian et al., 2006) and metaphor interpretation, i.e. identifying the intended literal meaning of a metaphorical expression (Fass, 1991; Martin, 1990; Narayanan, 1997; Barnden and Lee, 2002). However, to be applicable in a real-world setting a metaphor processing system needs to be able to identify and interpret metaphorical expressions in unrestricted text. The recent metaphor paraphrasing approach of Shutova (2010) was designed with this requirement in mind and used statistical methods, but still relied on the WordNet (Fellbaum, 1998) database to generate the initial set of paraphrases."
C12-2109,nivre-etal-2006-maltparser,0,0.024482,"The model uses non-negative matrix factorization (NMF) (Lee and Seung, 2000) in order to find latent dimensions, using the minimization of the Kullback-Leibler divergence as an objective function. A more detailed description of the factorization model can be found in Van de Cruys et al. (2011). Our paraphrase generation model has been trained on part of the UKWaC corpus (Baroni et al., 2009), covering about 500M words. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006), so that dependency triples could be extracted. Using the latent distributions yielded by our factorization model, it is now possible to compute the meaning vector for a particular word in context, and subsequently the most similar words to this meaning vector, which will be our candidate paraphrases. Intuitively, the contextual features of the word (i.e. the dependency-based context features) will highlight the important semantic dimensions of the particular instance, creating a probability distribution over latent factors p(z|d j ). Using this probability distribution, a new probability dis"
C12-2109,P10-1045,0,0.0249184,"Missing"
C12-2109,J07-2002,0,0.0213779,"o observe the behavior of the system, and the remaining 52 constituted the test set. 11 of them were subject-verb constructions and 41 were verb-direct object constructions. 3.2 Baseline system The baseline system is also unsupervised and incorporates two methods: that of generating most similar substitutes for the metaphorical verb regardless of its context and a method for their re-ranking based on the likelihood of their co-occurrence with the noun in the metaphorical expression. Thus a list of most similar substitutes is first generated using a standard dependencybased vector space model (Padó and Lapata, 2007). The likelihood of a paraphrase is then calculated as a joint probability of the candidate substitutes and the noun in the context as follows: f (v) f (v, n) f (v, n) L v = P(v, n)) = P(v) · P(n|v) = P · =P (5) f (v) f (v ) k k k f (vk ) where f (v, n) is the frequency of the co-occurrence of the substitute with the context and P k f (vk ) is the total number of verbs in the corpus. 3.3 Evaluation method and results We evaluated the paraphrases with the aid of human judges and against a human-created gold standard in two different experimental settings. Setting 1 Human judges were presented w"
C12-2109,N10-1147,1,0.578853,"d knowledge has proved useful for both metaphor identification, i.e. distinguishing between literal and metaphorical language in text (Fass, 1991; Martin, 1990; Krishnakumaran and Zhu, 2007; Gedigian et al., 2006) and metaphor interpretation, i.e. identifying the intended literal meaning of a metaphorical expression (Fass, 1991; Martin, 1990; Narayanan, 1997; Barnden and Lee, 2002). However, to be applicable in a real-world setting a metaphor processing system needs to be able to identify and interpret metaphorical expressions in unrestricted text. The recent metaphor paraphrasing approach of Shutova (2010) was designed with this requirement in mind and used statistical methods, but still relied on the WordNet (Fellbaum, 1998) database to generate the initial set of paraphrases. In this paper, we take the metaphor paraphrasing task a step further and present a fully unsupervised approach to this problem. In our method, candidate substitutes for the metaphorical term are generated using a vector space model. Vector space models have been previously used in the general lexical substitution task (Mitchell and Lapata, 2008; Erk and Padó, 2008, 2009; Thater et al., 2009, 2010; Erk and Padó, 2010; Van"
C12-2109,shutova-teufel-2010-metaphor,1,0.73581,"sters, pages 1121–1130, COLING 2012, Mumbai, December 2012. 1121 1 Introduction Metaphor has traditionally been viewed as an artistic device that lends vividness and distinction to its author’s style. This view was first challenged by Lakoff and Johnson (1980), who claimed that it is a productive phenomenon that operates at the level of mental processes. Humans often use metaphor to describe abstract concepts through reference to more concrete experiences. Being a characteristic property of human thought and communication, metaphor becomes an important problem for natural language processing. Shutova and Teufel (2010) have shown in an empirical study that the use of metaphor is ubiquitous in natural language text (according to their data, on average every third sentence in general domain text contains a metaphorical expression). Due to this high frequency usage, a system capable of recognizing and interpreting metaphorical expressions in unrestricted text would become an invaluable component of many semantics-oriented NLP applications. The majority of previous computational approaches to metaphor rely on manually created knowledge and thus operate on a limited domain and are expensive to build and extend."
C12-2109,D09-1067,1,0.816623,"domain, e.g. facilitate would select for PROCESSES (including CHANGE). We therefore expect that selecting the verbs whose preferences the noun in the metaphorical expression matches best should allow us to filter out non-literalness. We automatically acquired selectional preference (SP) distributions of the candidate substitutes (for subject-verb and verb-object relations) from the British National Corpus (BNC) (Burnard, 2007) parsed by the RASP parser (Briscoe et al., 2006). We obtained SP classes by clustering the 2000 most frequent nouns in the BNC into 200 clusters using the algorithm of Sun and Korhonen (2009). We quantified selectional preferences using the association measure proposed by Resnik (1993). It represents SPs as the difference between the posterior distribution of noun classes in a particular relation with the verb and their prior distribution in that syntactic position irrespective of the identity of the verb. This difference then defines the selectional preference strength (SPS) of the verb, quantified in terms of Kullback-Leibler divergence as follows. SR (v) = D(P(c|v)||P(c)) = X P(c|v) log c P(c|v) P(c) , (3) where P(c) is the prior probability of the noun class, P(c|v) is the pos"
C12-2109,W09-2506,0,0.0152106,"t metaphor paraphrasing approach of Shutova (2010) was designed with this requirement in mind and used statistical methods, but still relied on the WordNet (Fellbaum, 1998) database to generate the initial set of paraphrases. In this paper, we take the metaphor paraphrasing task a step further and present a fully unsupervised approach to this problem. In our method, candidate substitutes for the metaphorical term are generated using a vector space model. Vector space models have been previously used in the general lexical substitution task (Mitchell and Lapata, 2008; Erk and Padó, 2008, 2009; Thater et al., 2009, 2010; Erk and Padó, 2010; Van de Cruys et al., 2011). However, (to the best of our knowledge) they have not yet been deployed in tasks involving figurative meaning transfers, such as interpretation of metonymy or metaphor. In this paper, we address this problem and apply a vector space model of word meaning in context to metaphor paraphrasing, appropriately adapting it to the task. In comparison to lexical substitution, metaphor paraphrasing presents an additional challenge, namely that of discriminating between literal and metaphorical substitutes. Shutova (2010) used a selectional preferen"
C12-2109,P10-1097,0,0.0504015,"Missing"
C12-2109,N03-1033,0,0.00418949,"endency-based feature vector of the word accordingly. The model uses non-negative matrix factorization (NMF) (Lee and Seung, 2000) in order to find latent dimensions, using the minimization of the Kullback-Leibler divergence as an objective function. A more detailed description of the factorization model can be found in Van de Cruys et al. (2011). Our paraphrase generation model has been trained on part of the UKWaC corpus (Baroni et al., 2009), covering about 500M words. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006), so that dependency triples could be extracted. Using the latent distributions yielded by our factorization model, it is now possible to compute the meaning vector for a particular word in context, and subsequently the most similar words to this meaning vector, which will be our candidate paraphrases. Intuitively, the contextual features of the word (i.e. the dependency-based context features) will highlight the important semantic dimensions of the particular instance, creating a probability distribution over latent factors p(z|d j ). Using thi"
C12-2109,W00-1308,0,0.00730431,"ar context, and adapt the dependency-based feature vector of the word accordingly. The model uses non-negative matrix factorization (NMF) (Lee and Seung, 2000) in order to find latent dimensions, using the minimization of the Kullback-Leibler divergence as an objective function. A more detailed description of the factorization model can be found in Van de Cruys et al. (2011). Our paraphrase generation model has been trained on part of the UKWaC corpus (Baroni et al., 2009), covering about 500M words. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006), so that dependency triples could be extracted. Using the latent distributions yielded by our factorization model, it is now possible to compute the meaning vector for a particular word in context, and subsequently the most similar words to this meaning vector, which will be our candidate paraphrases. Intuitively, the contextual features of the word (i.e. the dependency-based context features) will highlight the important semantic dimensions of the particular instance, creating a probability distribution over latent fac"
C12-2109,D11-1094,1,0.516995,"Missing"
C12-2109,P09-2019,0,0.0790482,"Missing"
C12-2109,P08-1028,0,\N,Missing
C12-3023,P07-2009,0,0.0285208,"ning. The idea of active learning is to create a high-performance classifier but to minimize the cost of annotation. On the server side we tested the most popular classifiers including Naive Bayes classifier, Support Vector Machines (SVM), Maximum Entropy Model, Conditional Random Fields, among many others, and SVM is so far the best classifier for this task. The features listed below were used for classification. Most of them have been successfully used in recent related work (Teufel and Moens, 2002; Mullen et al., 2005; Merity et al., 2009; Guo et al., 2011b). The C&C POS tagger and parser (Curran et al., 2007) was used for extracting syntactic features such as grammatical relations (GR) from each sentence. Section. Normalized section names (Introduction, Methods, Results, Discussion). Location in article/section/paragraph. Each article/section/paragraph was divided into ten equal parts. Location was defined by the parts where the sentence begins and ends. Citation. The number of citations in a sentence (0, 1 or more). Table and Figure. The number of referred tables and figures in a sentence (0, 1 or more). N-gram. Any unigrams and bigrams in the corpus (an n-gram feature equals 1 if it is observed"
C12-3023,W10-1913,1,0.75623,"information about methods and results and also provide a comparison against other peoples’ work). An automatic analysis of the information category of each sentence is therefore important and can be useful for both natural language processing tasks as well as for scientists e.g. conducting literature review. Different approaches have been developed for determining the information structure (aka. discourse, rhetorical, argumentative or conceptual structure) of scientific publications (Teufel and Moens, 2002; Mizuta et al., 2006; Shatkay et al., 2008; Teufel et al., 2009; Liakata et al., 2010; Guo et al., 2010). Some of this work has proved helpful for tasks such as information retrieval, information extraction, and summarization (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). Most existing approaches are based on supervised learning and require large amounts of annotated data which limits their applicability to different domains . Recently (Guo et al., 2011b) has shown that weakly supervised learning (especially active learning) works well for determining the information structure of biomedical abstracts. This work is interesting since it can facilitate easi"
C12-3023,D11-1025,1,0.706692,"ka. discourse, rhetorical, argumentative or conceptual structure) of scientific publications (Teufel and Moens, 2002; Mizuta et al., 2006; Shatkay et al., 2008; Teufel et al., 2009; Liakata et al., 2010; Guo et al., 2010). Some of this work has proved helpful for tasks such as information retrieval, information extraction, and summarization (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). Most existing approaches are based on supervised learning and require large amounts of annotated data which limits their applicability to different domains . Recently (Guo et al., 2011b) has shown that weakly supervised learning (especially active learning) works well for determining the information structure of biomedical abstracts. This work is interesting since it can facilitate easier porting of the techniques to new tasks. However, also approaches based on weak supervision require data annotation in real-world applications. Moreover, simulation of active learning (such as that conducted by (Guo et al., 2011b) who used a fully annotated corpus from which they restored the labels of selected sentences in each iteration) is not practical but real-time interactive annotati"
C12-3023,liakata-etal-2010-corpora,0,0.0184916,"on section may include information about methods and results and also provide a comparison against other peoples’ work). An automatic analysis of the information category of each sentence is therefore important and can be useful for both natural language processing tasks as well as for scientists e.g. conducting literature review. Different approaches have been developed for determining the information structure (aka. discourse, rhetorical, argumentative or conceptual structure) of scientific publications (Teufel and Moens, 2002; Mizuta et al., 2006; Shatkay et al., 2008; Teufel et al., 2009; Liakata et al., 2010; Guo et al., 2010). Some of this work has proved helpful for tasks such as information retrieval, information extraction, and summarization (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). Most existing approaches are based on supervised learning and require large amounts of annotated data which limits their applicability to different domains . Recently (Guo et al., 2011b) has shown that weakly supervised learning (especially active learning) works well for determining the information structure of biomedical abstracts. This work is interesting since it"
C12-3023,W09-3603,0,0.0197104,"ning. This process can be repeated many times that is called active learning. The idea of active learning is to create a high-performance classifier but to minimize the cost of annotation. On the server side we tested the most popular classifiers including Naive Bayes classifier, Support Vector Machines (SVM), Maximum Entropy Model, Conditional Random Fields, among many others, and SVM is so far the best classifier for this task. The features listed below were used for classification. Most of them have been successfully used in recent related work (Teufel and Moens, 2002; Mullen et al., 2005; Merity et al., 2009; Guo et al., 2011b). The C&C POS tagger and parser (Curran et al., 2007) was used for extracting syntactic features such as grammatical relations (GR) from each sentence. Section. Normalized section names (Introduction, Methods, Results, Discussion). Location in article/section/paragraph. Each article/section/paragraph was divided into ten equal parts. Location was defined by the parts where the sentence begins and ends. Citation. The number of citations in a sentence (0, 1 or more). Table and Figure. The number of referred tables and figures in a sentence (0, 1 or more). N-gram. Any unigrams"
C12-3023,D09-1067,1,0.794821,"article/section/paragraph. Each article/section/paragraph was divided into ten equal parts. Location was defined by the parts where the sentence begins and ends. Citation. The number of citations in a sentence (0, 1 or more). Table and Figure. The number of referred tables and figures in a sentence (0, 1 or more). N-gram. Any unigrams and bigrams in the corpus (an n-gram feature equals 1 if it is observed in the sentence and 0 if not; the rest of the features are defined in a similar way). Verb. Any verbs in the corpus. Verb Class. Verbs are grouped into 60 categories by spectral clustering (Sun and Korhonen, 2009). Each category corresponds to a feature. Tense and Voice. Tense and voice indicated by the POS tag of main verbs and auxiliary verbs. e.g. have|VBZ be|VBN __|VBN indicates present perfect tense, passive voice. Grammatical Relation. Subject (ncsubj), direct object (dobj), indirect object (iobj) and second object (obj2) relations for verbs. e.g. (ncsubj observed difference obj). 186 Subj/Obj. The subjects/objects appearing with any verbs in the corpus. We implemented a number of query strategies for SVM-based active learning, including least confident sampling (Lewis and Gale, 1994), margin sam"
C12-3023,J02-4002,0,0.593882,"nterest, many sections tend to include different types of information (e.g. the Discussion section may include information about methods and results and also provide a comparison against other peoples’ work). An automatic analysis of the information category of each sentence is therefore important and can be useful for both natural language processing tasks as well as for scientists e.g. conducting literature review. Different approaches have been developed for determining the information structure (aka. discourse, rhetorical, argumentative or conceptual structure) of scientific publications (Teufel and Moens, 2002; Mizuta et al., 2006; Shatkay et al., 2008; Teufel et al., 2009; Liakata et al., 2010; Guo et al., 2010). Some of this work has proved helpful for tasks such as information retrieval, information extraction, and summarization (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). Most existing approaches are based on supervised learning and require large amounts of annotated data which limits their applicability to different domains . Recently (Guo et al., 2011b) has shown that weakly supervised learning (especially active learning) works well for determining"
C12-3023,D09-1155,0,0.125929,"on (e.g. the Discussion section may include information about methods and results and also provide a comparison against other peoples’ work). An automatic analysis of the information category of each sentence is therefore important and can be useful for both natural language processing tasks as well as for scientists e.g. conducting literature review. Different approaches have been developed for determining the information structure (aka. discourse, rhetorical, argumentative or conceptual structure) of scientific publications (Teufel and Moens, 2002; Mizuta et al., 2006; Shatkay et al., 2008; Teufel et al., 2009; Liakata et al., 2010; Guo et al., 2010). Some of this work has proved helpful for tasks such as information retrieval, information extraction, and summarization (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). Most existing approaches are based on supervised learning and require large amounts of annotated data which limits their applicability to different domains . Recently (Guo et al., 2011b) has shown that weakly supervised learning (especially active learning) works well for determining the information structure of biomedical abstracts. This work is"
C12-3023,W04-1202,0,\N,Missing
C14-2017,W10-1913,1,0.844671,"tructure can accelerate it considerably (Guo et al., 2011). The model of information structure incorporated in CRAB 2.0 is based on argumentative zoning (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel, 2010), whereby the text of a scientific abstract (or article) is segmented into blocks of sentences that carry a specific rhetorical function and combine to communicate the argument the authors wish to convey to the reader. The markup scheme used in our system labels each sentence with one of seven categories: background, objective, method, result, conclusion, related work and future work (Guo et al., 2010). The CRAB system incorporates preprocessing (lemmatisation, POS tagging, parsing) with the C&C toolkit3 and information structure markup with an SVM classifier that labels sentences according to a combination of lexical, syntactic and discourse features (Guo et al., 2011). The classifier has been trained on an annotated dataset of 1,000 CRA abstracts (Guo et al., 2010). The automatic information structure markup is used to support browsing of the set of abstracts assigned a label of interest by the semantic classifier; e.g., the user can inspect all abstracts labelled genotoxic (Figure 3). Ea"
C14-2017,J02-4002,0,0.0270979,"can also request a histogram visualisation (Figure 2b), which is produced through a call to the statistical software R.2 2.3 Literature browsing The risk assessment workflow involves close reading of relevant abstracts to identify specific information about methods, experimental details, results and conclusions. While it is not feasible to automate this process, we have shown that automatic markup and visualisation of abstracts’ information structure can accelerate it considerably (Guo et al., 2011). The model of information structure incorporated in CRAB 2.0 is based on argumentative zoning (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel, 2010), whereby the text of a scientific abstract (or article) is segmented into blocks of sentences that carry a specific rhetorical function and combine to communicate the argument the authors wish to convey to the reader. The markup scheme used in our system labels each sentence with one of seven categories: background, objective, method, result, conclusion, related work and future work (Guo et al., 2010). The CRAB system incorporates preprocessing (lemmatisation, POS tagging, parsing) with the C&C toolkit3 and information structure markup with an SVM classifie"
C16-1123,Q16-1031,0,0.0570178,"ons of atomic features, this work presents a hierarchical architecture that enables discarding chosen feature combinations. This allows the model to integrate prior typological knowledge, while ignoring uninformative combinations of typological and dependency features. At the same time, it capitalises on the automatisation of feature construction inherent to tensor models to generate combinations of informative typology-based features, further enhancing the added value of typological priors. Another successful integration of externally-defined typological information in parsing is the work of Ammar et al. (2016). They present a multilingual parser trained on a concatenation of syntactic treebanks of multiple languages. To reduce the adverse impact of contradicting syntactic information in treebanks of typologically distinct languages, while still maintaining the benefits of additional training data for cross-linguistically consistent syntactic patterns, the parser encodes a language-specific bias for each given input language. This bias is based on the identity of the language and its WBO features as used in (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015). Differently from p"
C16-1123,D14-1034,1,0.783502,"ly carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed"
C16-1123,W13-2710,0,0.0153812,"g to word order. The predicted typological features, when evaluated against 1301 WALS, achieve high accuracy. This method not only extends WALS word order documentation to hundreds of new languages, but also quantifies the frequency of different word orders across languages – information that is not available in manually crafted typological repositories. Typological information can also be extracted from Interlinear Glossed Text (IGT). Such resources contain morphological segmentation, glosses and English translations of example sentences collected by field linguists. Lewis and Xia (2008) and Bender et al. (2013) demonstrate that IGT can be used to extract typological information relating to word order, case systems and determiners for a variety of languages. Another line of work seeks to increase the coverage of typological information using existing information in typological databases. Daum´e III and Campbell (2007) and Bakker (2008) use existing WALS features to learn typological implications of the kind pioneered by Greenberg (1963). Such rules can then be used to predict unknown feature values for new languages. Georgi et al. (2010) use documented WALS features to cluster languages, and subseque"
C16-1123,P10-1131,0,0.177593,"nder, 2011). For example, it can be used to define the similarity between two languages with respect to the linguistic information one hopes to transfer; it can also help to define the optimal degree, level and method of transfer. For example, direct transfer of POS tagging is more likely to succeed when languages are sufficiently similar in terms of morphology in particular (Hana et al., 2004; Wisniewski et al., 2014). Typological information has been used to guide language transfer mostly in the areas of part-of-speech tagging and parsing, e.g. (Cohen and Smith, 2009; McDonald et al., 2011; Berg-Kirkpatrick and Klein, 2010; Naseem et al., 2012; T¨ackstr¨om et al., 2013). Section 4 surveys such works in more detail. Multilingual Joint Learning Another approach involves learning information for multiple languages simultaneously, with the idea that the languages will be able to support each other (Snyder, 2010; Navigli and Ponzetto, 2012). This can help in the challenging but common scenario where none of the languages involved has adequate resources. This applies even with English, where annotations needed for training basic tools are primarily available only for newspaper texts and a handful of other domains. In"
C16-1123,W14-1603,1,0.869447,"alues for new languages. Georgi et al. (2010) use documented WALS features to cluster languages, and subsequently predict new feature values using nearest-neighbour projection. A classifier-based approach for predicting new feature values from documented WALS information is presented in (Takamura et al., 2016). Coke et al. (2016) predict word order typological features by combining documented typological and genealogical features with the multilingual alignment approach discussed above. An alternative approach for learning typological information uses English as a Second Language (ESL) texts (Berzak et al., 2014). This work demonstrates that morphosyntactic typological similarities between languages are largely preserved in second language structural usage. It leverages this observation to approximate typological similarities between languages directly from ESL usage patterns and further utilise these similarities for nearest neighbor prediction of typological features. The method evaluates competitively compared to baselines in the spirit of (Georgi et al., 2010) which rely on existing typological documentation of the target language for determining its nearest neighbors. In addition, a number of stu"
C16-1123,K15-1010,1,0.845013,"nguage model is trained on several languages using a shared phonological inventory. The model is conditioned on the identity of the language at hand, as well as its phonological features obtained from a concatenation of phonological features from WALS, PHOIBLE and Ethnologue, extracted from URIEL. The resulting model subsumes and outperforms monolingually trained models for phone sequence prediction. Deri and Knight (2016) use URIEL to obtain phone and language similarity metrics, which are used for adjusting Grapheme to Phoneme (G2P) models from resource rich to resource poor languages. 1303 Berzak et al. (2015) use typological classifications to study language learning. Formalizing the theory of “Contrastive Analysis” which aims to analyse learning difficulties in a foreign language by comparing native and foreign language structures, they build a regression model that predicts language-specific grammatical error distributions by comparing typological features in the native and foreign languages. 5 Typological Information and NLP: What’s Next? § 4.2 surveyed the current uses of typological information in NLP. Here we discuss several future research avenues that might benefit from tighter integration"
C16-1123,P07-1036,0,0.0446587,"respect to gold training labels. The inference step is a natural place for encoding external knowledge through constraints. It biases the prediction of the model to agree with external knowledge, which, in turn, affects both the training process and the final model prediction. As typological information often reflects tendencies rather than strict rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of"
C16-1123,D14-1082,0,0.0308785,"e type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012; Hermann and Blunsom, 2014; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evalua"
C16-1123,N09-1009,0,0.190277,"jority of other languages they are lacking altogether. Since resource creation is expensive and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: languag"
C16-1123,D11-1005,0,0.0221498,"ifferent languages. The field of linguistic typology offers valuable resources for nearing both of these theoretical ideals: it studies and classifies world’s languages according to their structural and functional features, with the aim of explaining both the common properties and the structural diversity of languages. Many of the current popular solutions to multilingual NLP: transfer of information from resource-rich to resource-poor languages (Pad´o and Lapata, 2005; Khapra et al., 2011; Das and Petrov, 2011; T¨ackstr¨om et al., 2012, inter alia), joint multilingual learning (Snyder, 2010; Cohen et al., 2011; Navigli and Ponzetto, 2012, inter alia), and development of universal models (de Marneffe et al., 2014; Nivre et al., 2016, inter alia), either assume or explicitly make use of information related to linguistic typology. While previous work has recognised the role of linguistic typology (Bender, 2011), no systematic survey of typological information resources and their use in NLP to date has been published. Given the growing need for multilingual NLP and the increased use of typological information in recent work, such a survey would be highly valuable in guiding further development. This pa"
C16-1123,W02-1001,0,0.0226129,"r open challenges for typologically-driven NLP is the construction of principled mechanisms for the integration of typological knowledge in machine learning-based algorithms. Here, we briefly discuss a few traditional machine learning frameworks which support encoding of expert information, and as such hold promise for integrating typological information in NLP. Encoding typological knowledge into machine learning requires mechanisms that can bias learning (parameter estimation) and inference (prediction) of the model towards predefined knowledge. Algorithms such as the structured perceptron (Collins, 2002) and structured SVM (Taskar et al., 2004) iterate between an inference step and a parameter update step with respect to gold training labels. The inference step is a natural place for encoding external knowledge through constraints. It biases the prediction of the model to agree with external knowledge, which, in turn, affects both the training process and the final model prediction. As typological information often reflects tendencies rather than strict rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Inde"
C16-1123,J81-4005,0,0.764144,"Missing"
C16-1123,P11-1061,0,0.0239916,"ardless of language-specific variation; ii) comprehensive systematisation of all possible variation in different languages. The field of linguistic typology offers valuable resources for nearing both of these theoretical ideals: it studies and classifies world’s languages according to their structural and functional features, with the aim of explaining both the common properties and the structural diversity of languages. Many of the current popular solutions to multilingual NLP: transfer of information from resource-rich to resource-poor languages (Pad´o and Lapata, 2005; Khapra et al., 2011; Das and Petrov, 2011; T¨ackstr¨om et al., 2012, inter alia), joint multilingual learning (Snyder, 2010; Cohen et al., 2011; Navigli and Ponzetto, 2012, inter alia), and development of universal models (de Marneffe et al., 2014; Nivre et al., 2016, inter alia), either assume or explicitly make use of information related to linguistic typology. While previous work has recognised the role of linguistic typology (Bender, 2011), no systematic survey of typological information resources and their use in NLP to date has been published. Given the growing need for multilingual NLP and the increased use of typological info"
C16-1123,P07-1009,0,0.67981,"Missing"
C16-1123,de-marneffe-etal-2014-universal,0,0.088799,"Missing"
C16-1123,P16-1038,0,0.0439992,"al POS tagger. Another application area which benefited from integration of typological knowledge are phonological models of text. In (Tsvetkov et al., 2016) a multilingual neural phoneme-based language model is trained on several languages using a shared phonological inventory. The model is conditioned on the identity of the language at hand, as well as its phonological features obtained from a concatenation of phonological features from WALS, PHOIBLE and Ethnologue, extracted from URIEL. The resulting model subsumes and outperforms monolingually trained models for phone sequence prediction. Deri and Knight (2016) use URIEL to obtain phone and language similarity metrics, which are used for adjusting Grapheme to Phoneme (G2P) models from resource rich to resource poor languages. 1303 Berzak et al. (2015) use typological classifications to study language learning. Formalizing the theory of “Contrastive Analysis” which aims to analyse learning difficulties in a foreign language by comparing native and foreign language structures, they build a regression model that predicts language-specific grammatical error distributions by comparing typological features in the native and foreign languages. 5 Typologica"
C16-1123,N15-1184,0,0.0326505,"Missing"
C16-1123,C10-1044,0,0.311097,"see Table 1 for feature coverage of other typological databases). The integration of information from different databases is challenging due to differences in feature taxonomies as well as information overlap across repositories. Furthermore, available typological classifications contain different feature types, including nominal, ordinal and interval variables, and features that mix several types of values. This property hinders systematic and efficient encoding of such features in NLP models – a problem which thus far has only received a partial solution in the form of feature binarisation (Georgi et al., 2010). Further, typological databases are constructed manually using limited resources, and do not contain information on the distribution of feature values within a given language. This results in incomplete feature characterisations, as well as inaccurate generalisations. For example, WALS encodes only the dominant noun-adjective ordering for French, although in some cases this language also permits the adjective-noun ordering. Other aspects of typological databases may require feature pruning and preprocessing prior to use. For example, some features in WALS, such as feature 81B “Languages with"
C16-1123,P11-1149,1,0.703578,"and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss t"
C16-1123,N13-1113,1,0.819373,"d, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such a"
C16-1123,W04-3229,0,0.0329235,"uage structure), despite their great diversity. Captured in typological classifications at the level of generalisation useful 1299 for NLP, such information can be used to support multilingual NLP in a variety of ways (Bender, 2011). For example, it can be used to define the similarity between two languages with respect to the linguistic information one hopes to transfer; it can also help to define the optimal degree, level and method of transfer. For example, direct transfer of POS tagging is more likely to succeed when languages are sufficiently similar in terms of morphology in particular (Hana et al., 2004; Wisniewski et al., 2014). Typological information has been used to guide language transfer mostly in the areas of part-of-speech tagging and parsing, e.g. (Cohen and Smith, 2009; McDonald et al., 2011; Berg-Kirkpatrick and Klein, 2010; Naseem et al., 2012; T¨ackstr¨om et al., 2013). Section 4 surveys such works in more detail. Multilingual Joint Learning Another approach involves learning information for multiple languages simultaneously, with the idea that the languages will be able to support each other (Snyder, 2010; Navigli and Ponzetto, 2012). This can help in the challenging but common"
C16-1123,P14-1006,0,0.0284841,"ual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012; Hermann and Blunsom, 2014; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually improves the alignment betwee"
C16-1123,P11-1057,0,0.0519073,"natural language, regardless of language-specific variation; ii) comprehensive systematisation of all possible variation in different languages. The field of linguistic typology offers valuable resources for nearing both of these theoretical ideals: it studies and classifies world’s languages according to their structural and functional features, with the aim of explaining both the common properties and the structural diversity of languages. Many of the current popular solutions to multilingual NLP: transfer of information from resource-rich to resource-poor languages (Pad´o and Lapata, 2005; Khapra et al., 2011; Das and Petrov, 2011; T¨ackstr¨om et al., 2012, inter alia), joint multilingual learning (Snyder, 2010; Cohen et al., 2011; Navigli and Ponzetto, 2012, inter alia), and development of universal models (de Marneffe et al., 2014; Nivre et al., 2016, inter alia), either assume or explicitly make use of information related to linguistic typology. While previous work has recognised the role of linguistic typology (Bender, 2011), no systematic survey of typological information resources and their use in NLP to date has been published. Given the growing need for multilingual NLP and the increased u"
C16-1123,C12-1089,0,0.0412053,"Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012; Hermann and Blunsom, 2014; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually im"
C16-1123,I08-2093,0,0.266592,"gical information relating to word order. The predicted typological features, when evaluated against 1301 WALS, achieve high accuracy. This method not only extends WALS word order documentation to hundreds of new languages, but also quantifies the frequency of different word orders across languages – information that is not available in manually crafted typological repositories. Typological information can also be extracted from Interlinear Glossed Text (IGT). Such resources contain morphological segmentation, glosses and English translations of example sentences collected by field linguists. Lewis and Xia (2008) and Bender et al. (2013) demonstrate that IGT can be used to extract typological information relating to word order, case systems and determiners for a variety of languages. Another line of work seeks to increase the coverage of typological information using existing information in typological databases. Daum´e III and Campbell (2007) and Bakker (2008) use existing WALS features to learn typological implications of the kind pioneered by Greenberg (1963). Such rules can then be used to predict unknown feature values for new languages. Georgi et al. (2010) use documented WALS features to cluste"
C16-1123,P08-1099,0,0.0349195,"hrough constraints. It biases the prediction of the model to agree with external knowledge, which, in turn, affects both the training process and the final model prediction. As typological information often reflects tendencies rather than strict rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingua"
C16-1123,W12-0209,0,0.0637077,"Missing"
C16-1123,D11-1006,0,0.132738,"l., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a trans"
C16-1123,N16-1018,0,0.0371221,"Missing"
C16-1123,D10-1120,0,0.0614546,"pervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald"
C16-1123,P12-1066,0,0.127638,"e used to define the similarity between two languages with respect to the linguistic information one hopes to transfer; it can also help to define the optimal degree, level and method of transfer. For example, direct transfer of POS tagging is more likely to succeed when languages are sufficiently similar in terms of morphology in particular (Hana et al., 2004; Wisniewski et al., 2014). Typological information has been used to guide language transfer mostly in the areas of part-of-speech tagging and parsing, e.g. (Cohen and Smith, 2009; McDonald et al., 2011; Berg-Kirkpatrick and Klein, 2010; Naseem et al., 2012; T¨ackstr¨om et al., 2013). Section 4 surveys such works in more detail. Multilingual Joint Learning Another approach involves learning information for multiple languages simultaneously, with the idea that the languages will be able to support each other (Snyder, 2010; Navigli and Ponzetto, 2012). This can help in the challenging but common scenario where none of the languages involved has adequate resources. This applies even with English, where annotations needed for training basic tools are primarily available only for newspaper texts and a handful of other domains. In some areas of NLP, e"
C16-1123,D12-1128,0,0.183062,"The field of linguistic typology offers valuable resources for nearing both of these theoretical ideals: it studies and classifies world’s languages according to their structural and functional features, with the aim of explaining both the common properties and the structural diversity of languages. Many of the current popular solutions to multilingual NLP: transfer of information from resource-rich to resource-poor languages (Pad´o and Lapata, 2005; Khapra et al., 2011; Das and Petrov, 2011; T¨ackstr¨om et al., 2012, inter alia), joint multilingual learning (Snyder, 2010; Cohen et al., 2011; Navigli and Ponzetto, 2012, inter alia), and development of universal models (de Marneffe et al., 2014; Nivre et al., 2016, inter alia), either assume or explicitly make use of information related to linguistic typology. While previous work has recognised the role of linguistic typology (Bender, 2011), no systematic survey of typological information resources and their use in NLP to date has been published. Given the growing need for multilingual NLP and the increased use of typological information in recent work, such a survey would be highly valuable in guiding further development. This paper provides such a survey f"
C16-1123,Q16-1030,0,0.052552,"uages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually improves the alignment between the induced WEs and the meaning of the participating words in each of the involved languages. Naturally, as these models become more established and better understood, the challenge of external knowledge encoding becomes more prominent. Recent work has examined the ability to map from word embeddings to interpretable typological representations (Qian et al., 2016). Furthermore, a number of works (Faruqui et al., 2015; Rothe and Sch¨utze, 2015; Osborne et al., 2016; Mrkˇsi´c et al., 2016) proposed means through which external knowledge from structured knowledge bases and specialised linguistic resources can be encoded in these models. The success of these works suggests that more extensive integration of external linguistic knowledge in general, and typological knowledge in particular, is likely to play a key role in the future development of WE representations. 1304 Can NLP Support Typology Construction? As discussed in §4, typological resources are commonly constructed manually by linguists. Despite the progress made in recent years in the digitisatio"
C16-1123,P15-2034,0,0.163715,"Missing"
C16-1123,H05-1108,0,0.170353,"Missing"
C16-1123,petrov-etal-2012-universal,0,0.245026,"2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperfo"
C16-1123,P16-1140,0,0.0255051,"rieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually improves the alignment between the induced WEs and the meaning of the participating words in each of the involved languages. Naturally, as these models become more established and better understood, the challenge of external knowledge encoding becomes more prominent. Recent work has examined the ability to map from word embeddings to interpretable typological representations (Qian et al., 2016). Furthermore, a number of works (Faruqui et al., 2015; Rothe and Sch¨utze, 2015; Osborne et al., 2016; Mrkˇsi´c et al., 2016) proposed means through which external knowledge from structured knowledge bases and specialised linguistic resources can be encoded in these models. The success of these works suggests that more extensive integration of external linguistic knowledge in general, and typological knowledge in particular, is likely to play a key role in the future development of WE representations. 1304 Can NLP Support Typology Construction? As discussed in §4, typological resources are co"
C16-1123,N12-1008,1,0.912594,"t constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal feature"
C16-1123,D10-1067,1,0.759201,"es they are lacking altogether. Since resource creation is expensive and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual"
C16-1123,D11-1001,0,0.0132249,"mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs)"
C16-1123,P15-2040,0,0.0407258,"linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperforms unsupervised learning, it does not achieve optimal performance. One potential reason for this is that the tagset used by a POS tagger may not fit a target language which exhibits significantly different morphological features to a source language for which the tagset was initially developed (Petrov et al., 2012). Although parallel data can be used to giv"
C16-1123,P15-1173,0,0.0417279,"Missing"
C16-1123,D10-1001,0,0.014986,"t rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learnin"
C16-1123,D12-1131,1,0.832511,"raints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued"
C16-1123,P11-2120,0,0.0651153,"fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperforms unsupervised learning, it does not achieve optimal performance. One potential reason for this is that the tagset used by a POS tagger may not fit a target language which exhibits significantly different morphological features to a source language for which the tagset was initially developed (Petrov et al., 2012). Although para"
C16-1123,song-xia-2014-modern,0,0.0275956,"structural usage. It leverages this observation to approximate typological similarities between languages directly from ESL usage patterns and further utilise these similarities for nearest neighbor prediction of typological features. The method evaluates competitively compared to baselines in the spirit of (Georgi et al., 2010) which rely on existing typological documentation of the target language for determining its nearest neighbors. In addition, a number of studies learned typological information tailored to the particular task and data at hand (i.e. task-based development). For example, Song and Xia (2014) process Ancient Chinese using Modern Chinese parsing resources. They manually identify and address statistical patterns in variation between monolingual corpora in each language, and ultimately optimise the model performance by selectively using only the Modern Chinese features which correspond to Ancient Chinese features. Although automatically-learned typological classifications have not been used frequently to date, they hold great promise for extending the use of typological information in NLP. Furthermore, such work offers an additional axis of interaction between linguistic typology and"
C16-1123,D11-1117,0,0.0144064,"ce creation is expensive and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of unive"
C16-1123,N12-1052,0,0.0971877,"Missing"
C16-1123,N13-1126,0,0.219991,"Missing"
C16-1123,L16-1011,0,0.044567,"increase the coverage of typological information using existing information in typological databases. Daum´e III and Campbell (2007) and Bakker (2008) use existing WALS features to learn typological implications of the kind pioneered by Greenberg (1963). Such rules can then be used to predict unknown feature values for new languages. Georgi et al. (2010) use documented WALS features to cluster languages, and subsequently predict new feature values using nearest-neighbour projection. A classifier-based approach for predicting new feature values from documented WALS information is presented in (Takamura et al., 2016). Coke et al. (2016) predict word order typological features by combining documented typological and genealogical features with the multilingual alignment approach discussed above. An alternative approach for learning typological information uses English as a Second Language (ESL) texts (Berzak et al., 2014). This work demonstrates that morphosyntactic typological similarities between languages are largely preserved in second language structural usage. It leverages this observation to approximate typological similarities between languages directly from ESL usage patterns and further utilise th"
C16-1123,N16-1161,0,0.058165,"nological Modeling and Language Learning Besides dependency parsing, several other areas have started integrating typological information in various forms. A number of such works revolve around the task of POS tagging. For example, in Zhang et al. (2012), the previously discussed WBO features were used to inform mappings from language-specific to a universal POS tagset. In (Zhang et al., 2016), WBO feature values are used to evaluate the quality of a multilingual POS tagger. Another application area which benefited from integration of typological knowledge are phonological models of text. In (Tsvetkov et al., 2016) a multilingual neural phoneme-based language model is trained on several languages using a shared phonological inventory. The model is conditioned on the identity of the language at hand, as well as its phonological features obtained from a concatenation of phonological features from WALS, PHOIBLE and Ethnologue, extracted from URIEL. The resulting model subsumes and outperforms monolingually trained models for phone sequence prediction. Deri and Knight (2016) use URIEL to obtain phone and language similarity metrics, which are used for adjusting Grapheme to Phoneme (G2P) models from resource"
C16-1123,P10-1040,0,0.0148561,"he modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012; Hermann and Blunsom, 2014; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word"
C16-1123,D14-1187,0,0.0449846,"Missing"
C16-1123,I08-3008,0,0.365977,"ransfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperforms unsupervised learning, it does not achieve optimal performance. One potential reason for this is that the tagset used by a POS tagger may not fit a target language which exhibits significantly different morphological features to a source language for which the tagset was initially develope"
C16-1123,D15-1213,0,0.349778,"al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperforms unsupervised learning,"
C16-1123,D12-1125,1,0.921529,"been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petro"
C16-1123,N16-1156,0,0.153027,"databases reviewed in §2; and ii) automatic learning. The two 1300 methods have been used independently and in combination, and both are based on the assumption (be it explicit or implicit) that typological relations may be fruitfully used in NLP. Manual Extraction from Linguistic Resources Manually crafted linguistic resources – in particular the WALS database – have been the most commonly used sources of typological information in NLP. To date, syntactic parsing (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ammar et al., 2016) and POS tagging (Zhang et al., 2012; Zhang et al., 2016) were the predominant areas for integration of structural information from such databases. In the context of these tasks, the most frequently used features related to word ordering according to coarse syntactic categories. Additional areas with emerging research which leverages externally-extracted typological features are phonological modeling (Tsvetkov et al., 2016; Deri and Knight, 2016) and language learning (Berzak et al., 2015). While information obtained from typological databases has been successfully integrated in several NLP tasks, a number of challenges remain. Perhaps the most cruc"
C16-1123,L16-1262,0,\N,Missing
C16-1220,W02-2203,0,0.0832599,"er et al., 2016). Table 4.1 shows the distribution of 1,580 abstracts and sentences for each of the hallmark categories. The inter-annotator agreement is k = 0.81. Hallmark # Abstracts # Sentences PS 462 993 GS 242 468 CD 430 883 RI 115 295 A 143 357 IM 291 667 GI 333 771 PI 240 520 CE 105 213 ID 108 226 Table 1: Distribution of data for the ten hallmarks. 4.2 Handcrafted supervised model We employ a fully supervised handcrafted baseline for this task, classifying using binary classifiers for each hallmark category. Sentences are first tokenised and part-of-speech tagged using the C&C tagger (Clark, 2002) trained on biomedical texts. The text is lemmatised using BioLemmatizer (Liu et al., 2012) and grammatical relations are extracted using the C&C Parser. The parser was trained using molecular biology annotations (Rimell and Clark, 2009). Finally, named entities are extracted from parsed data using ABNER (Settles, 2005), trained on the NLPBA and BioCreative corpora (Leitner et al., 2010). We experimented with several types of handcrafted features for hallmark classification, chosen based on their inclusion in other state-of-the-art biomedical text classification systems. Only the first five ar"
C16-1220,W10-1913,1,0.821517,"gument by segmenting a document into several zones, such as: “Objective”, “Background”, “Method”, “Result”, and “Conclusion”. This task differs from Task 1 in that the objective is to classify scientific text according to generic labels (i.e., unrelated to domain-specific knowledge) and the focus is on a different classification features, such as the position of the text in the document and the author’s writing style. For example, the “Objective” zone of the argument generally appears very early in the article using an active voice. 5.1 Data We evaluate using an expert-annotated dataset from (Guo et al., 2010) comprising of 1000 PubMed abstracts relevant to cancer biology. The dataset consists of 7985 labelled sentences, with an interannotator agreement of k = 0.85. There are five mutually non-exclusive classes, described together with their frequencies in Table 5.1. Class Objective (OBJ) Background (BKG) Method (METH) Result (RES) Conclusion (CON) Description The background and the aim of the research The circumstances pertaining to the current work The way to achieve the goal The principal findings Analysis, discussion and the main conclusions # Abstracts 744 692 640 889 859 # Sentences 812 1517"
C16-1220,C12-3023,1,0.812301,"epigenetic alterations (Marusyk et al., 2012), this framework provides an organizing principle to simplify the complexity of cancer biological processes (Baker et al., 2016). 4.1 Data Baker et al. (2016) acquired a collection of PubMed abstracts using a set of search terms representative for each of the 10 hallmarks. The terms and their synonyms appearing in Hanahan and Weinberg (2000) and Hanahan and Weinberg (2011) were employed along with additional ones selected by a team of cancer researchers. Annotation was conducted by experts in cancer research, using the annotation tool described in Guo et al. (2012). Annotations are assigned at a sentence-level: a sentence is annotated if contains clear evidence relating to one or several hallmarks (Baker et al., 2016). Table 4.1 shows the distribution of 1,580 abstracts and sentences for each of the hallmark categories. The inter-annotator agreement is k = 0.81. Hallmark # Abstracts # Sentences PS 462 993 GS 242 468 CD 430 883 RI 115 295 A 143 357 IM 291 667 GI 333 771 PI 240 520 CE 105 213 ID 108 226 Table 1: Distribution of data for the ten hallmarks. 4.2 Handcrafted supervised model We employ a fully supervised handcrafted baseline for this task, cla"
C16-1220,reschke-etal-2014-event,0,0.0197833,"small labelled datasets. There are works that target small labelled data text classification in sparse domains using techniques such as active learning (Guo et al., 2013; Figueroa et al., 2012; Nissim et al., 2015). The idea of active learning is to reduce annotation effort by iteratively selecting the most informative instances to be labelled by interactively querying an expert. Although good accuracy can be achieved, the approach relies on expert knowledge and interaction, and may still require feature engineering. Other works tackle the sparsity of labelled data using distant supervision (Reschke et al., 2014; Vivaldi and Rodr´ıguez, 2015). Here, a classifier is trained using data labelled automatically using approximate heuristics rather than annotators. However, due to the assumptions and bias that are inherent in such labelling heuristics, this may result in lower performance. The work presented in this paper differs from the above as it focuses on learning embeddings for sparse domains with small labelled datasets; moreover, we focus on utilizing these embeddings specifically for text classification. 3 Approach This section first describes the Distributed Memory model (Section 3.1), and then e"
C16-1220,D09-1067,1,0.806461,"feature employs all words occurring in input texts. We lemmatise the words in order to reduce sparsity. Noun bigrams: Noun bigrams are used because they can be useful in capturing two word-concepts in texts (e.g., Gene silencing). Grammatical relations: we use the dobj (direct object), ncsubj (non-clausal subject), and iobj (indirect object) relations, plus the head and dependent words in relations. Verb classes: verb classes group semantically similar verbs together, abstracting away from individual words when faced with data sparsity. We used the hierarchical classification of 399 verbs by Sun and Korhonen (2009). Named entities: domain-specific concepts, providing another way to group bags of words into meaningful categories. We use five types which are particularly relevant for cancer research: Proteins, DNA, RNA, Cell Line, and Cell Type. Medical Subject Headings (MeSH): is a comprehensive controlled vocabulary for indexing journal articles and books in the life sciences. Most abstracts in our dataset contain an associated list of MeSH terms which we employ as features. Chemicals list: a total of 3,021 associated chemicals (manually annotated). We use these as features, since processes involved wit"
C16-1220,J02-4002,0,0.0425339,"ehensive controlled vocabulary for indexing journal articles and books in the life sciences. Most abstracts in our dataset contain an associated list of MeSH terms which we employ as features. Chemicals list: a total of 3,021 associated chemicals (manually annotated). We use these as features, since processes involved with hallmarks might involve similar chemicals. 5 Task 2: Rhetorical text classification Rhetorical text classification (also known as information structure analysis) segments scientific text into information categories. One such classification technique is argumentative zoning (Teufel and Moens, 2002) which captures the rhetorical progression of the scientific argument by segmenting a document into several zones, such as: “Objective”, “Background”, “Method”, “Result”, and “Conclusion”. This task differs from Task 1 in that the objective is to classify scientific text according to generic labels (i.e., unrelated to domain-specific knowledge) and the focus is on a different classification features, such as the position of the text in the document and the author’s writing style. For example, the “Objective” zone of the argument generally appears very early in the article using an active voice"
C16-1220,P14-1074,0,0.0310738,"ta. 2 Related Work Embedded distributed representations have been used widely for document and sentence classification. For example, Huang et al. (2014) learn document-level embeddings using word-level embeddings as input. Yan et al. (2015) learn document-embeddings by combing a Deep Boltzmann Machine and a Deep Belief Network. Bhatia et al. (2015) learn embeddings for large multi-label classification in situations where the label set is extremely large. Liu et al. (2015) use latent topic models to learn a topic from each word, and then learn an embedding based on both the topic and the word. Yogatama and Smith (2014) use structured regularizers based on parse trees, topics, and hierarchical word clusters, as well as hierarchical sparse coding for regularization using stochastic proximal methods (Yogatama et al., 2015). All of these works have been trained and evaluated on general domains such as newswire rather than on sparse domains with small labelled datasets. There are works that target small labelled data text classification in sparse domains using techniques such as active learning (Guo et al., 2013; Figueroa et al., 2012; Nissim et al., 2015). The idea of active learning is to reduce annotation eff"
C98-2242,A97-1052,0,\N,Missing
D09-1067,D08-1007,0,0.0943275,"exical resources. It is based on clustering argument head data in the grammatical relations associated with verbs. We describe our features in section 2 and the clustering methods in section 3. Experimental evaluation and results are reported in sections 4 and 5, respectively. Section 6 provides discussion and describes related work, and section 7 concludes. 2 lectional restrictions, can be assigned to the majority of Levin classes, as demonstrated by VerbNet (Kipper-Schuler, 2005). SP acquisition from undisambiguated corpus data is arguably challenging (Brockmann and Lapata, 2003; Erk, 2007; Bergsma et al., 2008). It is especially challenging in the context of verb classification where SP models are needed for specific syntactic slots for which the data may be sparse, and the resulting feature vectors integrating both syntactic and semantic features may be high dimensional. However, we wanted to investigate whether better results could be obtained if the features were optimised for richness, the feature extraction for accuracy, and a clustering method capable of dealing with the resulting high dimensional feature space was employed. 2.1 We adopted a recent SCF acquisition system which has proved more"
D09-1067,W02-1016,0,0.826636,"Missing"
D09-1067,P06-4020,0,0.16092,"we wanted to investigate whether better results could be obtained if the features were optimised for richness, the feature extraction for accuracy, and a clustering method capable of dealing with the resulting high dimensional feature space was employed. 2.1 We adopted a recent SCF acquisition system which has proved more accurate than previous comparable systems2 but which has not been employed for verb clustering before: the system of Preiss et al. (2007). This system tags, lemmatizes and parses corpus data using the current version of the RASP (Robust Accurate Statistical Parsing) toolkit (Briscoe et al., 2006), and on the basis of resulting grammatical relations (GRs) assigns each occurrence of a verb to one of 168 verbal SCFs classes3 . The system provides a filter which can be used to remove adjuncts from the resulting lexicon. We do not employ this filter since adjuncts have proved informative for verb classification (Sun et al., 2008; Joanis et al., 2008). However, we do frequency-based thresholding to minimise the noise (e.g. erroneous scfs) and sparse data in verb classification and to ensure that only features supported by several verbs are used in classification: we only consider SCFs and G"
D09-1067,E03-1034,0,0.608483,"ploit WordNet (Miller, 1995) or other lexical resources. It is based on clustering argument head data in the grammatical relations associated with verbs. We describe our features in section 2 and the clustering methods in section 3. Experimental evaluation and results are reported in sections 4 and 5, respectively. Section 6 provides discussion and describes related work, and section 7 concludes. 2 lectional restrictions, can be assigned to the majority of Levin classes, as demonstrated by VerbNet (Kipper-Schuler, 2005). SP acquisition from undisambiguated corpus data is arguably challenging (Brockmann and Lapata, 2003; Erk, 2007; Bergsma et al., 2008). It is especially challenging in the context of verb classification where SP models are needed for specific syntactic slots for which the data may be sparse, and the resulting feature vectors integrating both syntactic and semantic features may be high dimensional. However, we wanted to investigate whether better results could be obtained if the features were optimised for richness, the feature extraction for accuracy, and a clustering method capable of dealing with the resulting high dimensional feature space was employed. 2.1 We adopted a recent SCF acquisi"
D09-1067,P06-2012,0,0.159883,"PPs (F2). This feature parameterizes SCF s for prepositions. 640 3 Clustering methods Spectral clustering We use two clustering methods: (i) pairwise clustering (PC) which obtained the best performance in comparison with several other methods in recent work on biomedical verb clustering (Korhonen et al., 2008), and (ii) a method which is new to the task (and to the best of our knowledge, to NLP): a variation of spectral clustering which exploits the MNCut algorithm (Meila and Shi, 2001) (SPEC). Spectral clustering has been shown to be effective for high dimensional and non-convex data in NLP (Chen et al., 2006) and it has been applied to German verb clustering by Brew and Schulte im Walde (2002). However, previous work has used Ng et al. (2002)’s algorithm, while we adopt the MNCut algorithm. The latter has shown a wider applicability (von Luxburg, 2007; Verma and Meila, 2003) and it can be justified from the random walk view, which has a clear probabilistic interpretation. Clustering groups a given set of items (verbs in our experiment) V = {vn }N n=1 into a disjoint partition of K classes I = {Ik }K k=1 . Both our algorithms take a similarity matrix as input. We construct this from the skew diverg"
D09-1067,P07-1028,0,0.250427,"or other lexical resources. It is based on clustering argument head data in the grammatical relations associated with verbs. We describe our features in section 2 and the clustering methods in section 3. Experimental evaluation and results are reported in sections 4 and 5, respectively. Section 6 provides discussion and describes related work, and section 7 concludes. 2 lectional restrictions, can be assigned to the majority of Levin classes, as demonstrated by VerbNet (Kipper-Schuler, 2005). SP acquisition from undisambiguated corpus data is arguably challenging (Brockmann and Lapata, 2003; Erk, 2007; Bergsma et al., 2008). It is especially challenging in the context of verb classification where SP models are needed for specific syntactic slots for which the data may be sparse, and the resulting feature vectors integrating both syntactic and semantic features may be high dimensional. However, we wanted to investigate whether better results could be obtained if the features were optimised for richness, the feature extraction for accuracy, and a clustering method capable of dealing with the resulting high dimensional feature space was employed. 2.1 We adopted a recent SCF acquisition system"
D09-1067,P08-1057,0,0.108553,"s verb classification results WordNet (Miller, 1995) and Germanet (Kunze and Lemnitzer, 2002) classes as SP models. Joanis (2002) obtained no improvement over syntactic features, whereas Schulte im Walde (2006) obtained insignificant improvement. Korhonen et al. (2008) combined SPs with SCFs when clustering biomedical verbs. The SPs were acquired automatically from syntactic slots of SCF s (not from GR s as in our experiment) using PC clustering. A small improvement was obtained using LPs extracted from the same syntactic slots, but the SP clusters offered no improvement. Recently, Schulte im Walde et al. (2008) proposed an interesting SP acquisition method which involves combining EM training and the MDL principle for an verb classification incorporating SPs. However, no comparison against purely syntactic features is provided. In our experiment, we obtained a considerable improvement over syntactic features, despite using a fully unsupervised approach to both verb clustering and SP acquisition. In addition to the rich, syntactic-semantic feature sets, our good results can be attributed to the clustering technique capable of dealing with them. The potential of spectral clustering for the task was re"
D09-1067,korhonen-etal-2006-large,1,0.790042,"numerous and could be addressed by developing more balanced SP models across different GR s. 6 T1 Li et al. 2008 Joanis et al. 2008 Stevenson et al. 2003 SPEC T2 Sun et al. 2008 ´ S´eaghdha et al. 2008 O SPEC Method supervised supervised semi-supervised unsupervised unsupervised supervised unsupervised supervised unsupervised Result 66.3 58.4 29 31 57.55 62.50 51.6 67.3 80.35 Table 5: Previous verb classification results WordNet (Miller, 1995) and Germanet (Kunze and Lemnitzer, 2002) classes as SP models. Joanis (2002) obtained no improvement over syntactic features, whereas Schulte im Walde (2006) obtained insignificant improvement. Korhonen et al. (2008) combined SPs with SCFs when clustering biomedical verbs. The SPs were acquired automatically from syntactic slots of SCF s (not from GR s as in our experiment) using PC clustering. A small improvement was obtained using LPs extracted from the same syntactic slots, but the SP clusters offered no improvement. Recently, Schulte im Walde et al. (2008) proposed an interesting SP acquisition method which involves combining EM training and the MDL principle for an verb classification incorporating SPs. However, no comparison against purely s"
D09-1067,C08-1057,1,0.906281,"CFs in a similar fashion as LPs are combined with SCFs in F9-F11: F5: F4 with COs (F1). The SCF and CO feature vectors are concatenated. F12-F14: as F9-F11 but SPs (20 clusters from 200 argument heads) are used instead of LPs F6: F4 with the tense of the verb. The frequency of verbal POS tags is calculated specific to each SCF. F15-F17: as F9-F11 but SPs (30 clusters from 500 argument heads) are used instead of LPs All the other feature sets include information about SCFs which have been widely employed in verb classification, e.g. (Schulte im Walde, 2006; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008). F4-F7 include basic SCF information and/or refine it with additional information which has proved useful in previous works: 5 This feature was included to enable comparing the contribution of the recent SCF system to that of an older, comparable system which was used for constructing the VALEX lexicon. F7: F4 with PPs (F2). This feature parameterizes SCF s for prepositions. 640 3 Clustering methods Spectral clustering We use two clustering methods: (i) pairwise clustering (PC) which obtained the best performance in comparison with several other methods in recent work on biomedical verb clust"
D09-1067,W09-0210,1,0.822355,"Missing"
D09-1067,P08-1050,0,0.281443,"vious work. 639 F8: Basic SCF feature corresponding to F4 but extracted from the VALEX lexicon (Korhonen et al., 2006)5 . for experimentation, both shallow and deep syntactic and semantic features. As described below, some of the feature types have been employed in previous works and some are novel. 2.2 The following 9 feature sets are novel. They build on F7, refining it further. F9-F11 refine F7 with information about LPs: Feature sets The first feature set F1 includes information about the lexical context (co-occurrences) of verbs which has proved useful for supervised verb classification (Li and Brew, 2008): F9: F7 with F3 (subject only) F10: F7 with F3 (object only) F1: Co-occurrence (CO): We adopt the best method of Li and Brew (2008) where collocations are extracted from the four words immediately preceding and following a lemmatized verb. Stop words are removed prior to extraction, and the 600 most frequent resulting COs are kept. F11: F7 with F3 (subject, object, indirect object) F12-17 refine F7 with SPs. We adopt a fully unsupervised approach to SP acquisition. We acquire the SPs by 1. taking the GR relations (subject, object, indirect object) associated with verbs, F2-F3 provide informat"
D09-1067,P08-1063,0,0.0553152,"Missing"
D09-1067,C08-1082,0,0.134773,"Missing"
D09-1067,P07-1115,1,0.570326,")). Disappointingly, semantic features have not yielded significant additional improvement, although they play a key role in manual and theoretical work on verb classification and could thus be expected to offer a considerable contribution to classification performance. Since the accuracy of automatic verb classification shows room for improvement, we further investigate the potential of semantic features – verb selectional preferences (SPs) – for the task. We introduce a novel approach to verb clustering which involves the use of (i) a recent subcategorization frame (SCF) acquisition system (Preiss et al., 2007) which produces rich lexical, SCF and syntactic data, (ii) novel syntactic-semantic feature sets extracted from this data which incorporate a variety of linguistic information, including SP s, and (iii) a new variation of spectral clusterIn previous research in automatic verb classification, syntactic features have proved the most useful features, although manual classifications rely heavily on semantic features. We show, in contrast with previous work, that considerable additional improvement can be obtained by using semantic features in automatic classification: verb selectional preferences"
D09-1067,W04-3213,0,\N,Missing
D09-1067,W03-0410,0,\N,Missing
D09-1067,J06-2001,0,\N,Missing
D09-1067,kunze-lemnitzer-2002-germanet,0,\N,Missing
D11-1025,P07-2009,0,0.0712628,"|x, θ) = Z(x) exp( j θj Fj (y, x)), where Fj (y, x) is a real-valued feature function of the states and the observations; θj is the weight of Fj , and Z(x) is a normalization factor. The θ parameters can be learned using the L-BFGS algorithm (Nocedal, 1980). We used Mallet software (McCallum, 2002) for CRF experiments. These features were extracted from the corpus using a number of tools. A tokenizer was used to detect the boundaries of sentences and to separate punctuation from adjacent words e.g. in complex biomedical terms such as 2-amino-3,8-diethylimidazo[4,5f]quinoxaline. The C&C tools (Curran et al., 2007) trained on biomedical literature were employed for POS tagging, lemmatization and parsing. The lemma output was used for creating Word, Bi-gram and Verb features. The GR output was used for creating the GR, Subj, Obj and Voice features. The ”obj” marker in a subject relation indicates passive voice (e.g. (ncsubj observed 14 difference 5 obj)). The verb classes were acquired automatically from the corpus using the unsupervised spectral clustering method of (Sun and Korhonen, 2009). To control the number of features we lemmatized the lexical items for all the features, and removed the words 3.2"
D11-1025,W10-1913,1,0.502157,"lations in the corpus. e.g. (ncsubj observed 14 difference 5 obj). The value of this feature equals 1 if it occurs in a particular sentence (and 0 if not). • Subj and Obj. The subjects and objects appearing with any verbs in the corpus (extracted from above GRs). • Voice. The voice of verbs (active or passive) in the corpus. 3.2 Machine learning methods Support Vector Machines (SVM) and Conditional Random Fields (CRF) have proved the best performing fully supervised methods in most recent works on information structure, e.g. (Teufel and Moens, 2002; Mullen et al., 2005; Hirohata et al., 2008; Guo et al., 2010). We therefore implemented these methods as well as weakly supervised variations of them: active SVM with and without self-training, transductive SVM and semi-supervised CRF. 3.2.1 Supervised methods SVM constructs hyperplanes in a multidimensional space to separate data points of different classes. Good separation is achieved by the hyperplane that has the largest distance from the nearest data points of any class. The hyperplane has the form w · x − b = 0, where w is its normal vector. We want to maximize the distance from the hyperplane to the data points, or the distance between two parall"
D11-1025,I08-1050,0,0.185889,"s obtained, or the conclusions drawn by authors. Similarly, many Natural Language Processing (NLP) tasks focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scien273 Thierry Poibeau LaTTiCe, UMR8094 tific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive"
D11-1025,P06-1027,0,0.231964,"u) ) Subject to y (l) (w · x(l) − b) ≥ 1, y (u) (w · x(u) − b) ≥ 1 , y (u) ∈ {−1, 1}, where x(u) is unlabeled data and y (u) the estimate of its label. The problem can be solved by using the CCCP algorithm (Collobert et al., 2006). We used UniverSVM software (Sinz, 2011) for TSVM experiments. Semi-supervised CRF (SSCRF) can be implemented with entropy regularization (ER). It extends the objective function on Labeled data P (l) (l) log additional term PL P p(y |x(u) , θ) with an (u) , θ) log p(y|x , θ) to minimize U Y p(y|x the conditional entropy of the model’s predictions on U nlabeled data (Jiao et al., 2006; Mann and Mccal277 lum, 2007). We used Mallet software (McCallum, 2002) for SSCRF experiments. 4 Experimental evaluation 4.1 Evaluation methods We evaluated the ML results in terms of accuracy, precision, recall, and F-measure against manual AZ annotations in the corpus: acc = no. of correctly classif ied sentences total no. of sentences in the corpus p= no. of sentences correctly identif ied as Classi total no. of sentences identif ied as Classi r= no. of sentences correctly identif ied as Classi total no. of sentences in Classi f= 2∗p∗r p+r We used 10-fold cross validation for all the metho"
D11-1025,liakata-etal-2010-corpora,0,0.499936,"ave been proposed for sentence-based classification of scien273 Thierry Poibeau LaTTiCe, UMR8094 tific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks. A potential solution to this bottleneck is to develop techniques based on weakly-supervised ML. Relying on a small amount of labeled data and a large"
D11-1025,W06-3309,0,0.0568061,"study, the results obtained, or the conclusions drawn by authors. Similarly, many Natural Language Processing (NLP) tasks focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scien273 Thierry Poibeau LaTTiCe, UMR8094 tific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing a"
D11-1025,N07-2028,0,0.0309862,"Missing"
D11-1025,W09-3603,0,0.0989553,"Missing"
D11-1025,W10-0104,0,0.0620553,"Missing"
D11-1025,D09-1067,1,0.212661,"m adjacent words e.g. in complex biomedical terms such as 2-amino-3,8-diethylimidazo[4,5f]quinoxaline. The C&C tools (Curran et al., 2007) trained on biomedical literature were employed for POS tagging, lemmatization and parsing. The lemma output was used for creating Word, Bi-gram and Verb features. The GR output was used for creating the GR, Subj, Obj and Voice features. The ”obj” marker in a subject relation indicates passive voice (e.g. (ncsubj observed 14 difference 5 obj)). The verb classes were acquired automatically from the corpus using the unsupervised spectral clustering method of (Sun and Korhonen, 2009). To control the number of features we lemmatized the lexical items for all the features, and removed the words 3.2.2 Weakly-supervised methods and GRs with fewer than 2 occurrences and bi-grams Active SVM (ASVM) starts with a small amount of with fewer than 5 occurrences. labeled data, and iteratively chooses a proportion of 276 unlabeled data for which SVM has less confidence to be labeled (the labels can be restored from the original corpus) and used in the next round of learning, i.e. active learning. Query strategies based on the structure of SVM are frequently employed (Tong and Koller,"
D11-1025,J02-4002,0,0.317771,"guage Processing (NLP) tasks focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scien273 Thierry Poibeau LaTTiCe, UMR8094 tific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks. A potential s"
D11-1025,D09-1155,0,0.518778,"raction of specific types of information in documents only. To date, a number of approaches have been proposed for sentence-based classification of scien273 Thierry Poibeau LaTTiCe, UMR8094 tific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of these classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008), while others are based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The best of current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (Teufel and Moens, 2002; Mizuta et al., 2006; Tbahriti et al., 2006; Ruch et al., 2007). However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different scientific domains and tasks. A potential solution to this bottleneck is to develop te"
D11-1025,L10-1000,0,\N,Missing
D11-1094,W06-1669,0,0.0172616,"Missing"
D11-1094,D10-1113,0,0.737717,"nventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined sense; instead, the original meaning representation of a word is adapted ‘on the fly’, according to – and specifically tailored for – the particular context in which it appears. To be able to do so, we build a factorization model in which words, together with their window-based context words and their dependency 1012 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1012–1022, c Edinburgh, Scotland, U"
D11-1094,D09-1046,0,0.0573743,"unity’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined sense; instead, the original meaning representation of a word is adapted ‘on the fly’, according to – and specifically tailored for – the particular context in which it appears. To be able to do so, we build a factorization model in whic"
D11-1094,D08-1094,0,0.580303,"Missing"
D11-1094,W09-0208,0,0.459823,"Missing"
D11-1094,P10-2017,0,0.264572,"Missing"
D11-1094,P09-1002,0,0.0201734,"ay for equal work! The meaning of work is quite different in both sentences. In sentence (1), work refers to the product of a creative act, viz. a painting. In sentence (2), it refers to labour carried out as a source of income. The NLP community’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a p"
D11-1094,W10-2803,0,0.0181118,"The meaning of work is quite different in both sentences. In sentence (1), work refers to the product of a creative act, viz. a painting. In sentence (2), it refers to labour carried out as a source of income. The NLP community’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined s"
D11-1094,S07-1029,0,0.0630491,"ch better results than Dinu and Lapata’s approach. The results indicate that our approach, after vector adaptation in context, is still able to provide accurate similarity calculations across the complete word space. While other algorithms are able to rank candidate substitutes at the expense of accurate similarity calculations, our approach is able to do both. This is one of the important advantages of our approach. For reasons of comparison, we also included the scores of the best performing models that participated in the SEMEVAL 2007 lexical substitution task (KU (Yuret, 2007) and IRST 2 (Giuliano et al., 2007), which got the best scores for Rbest and P10 , respectively). These models reach better scores compared to our models. Note, however, that all participants of the SEMEVAL 2007 lexical substitution task relied on a predefined sense inventory (i.e. WordNet, or a machine readable thesaurus). Our system, on the other hand, induces paraphrases in a fully unsupervised way. To our knowledge, this is the first time a fully unsupervised system is tested on the paraphrase induction task. model n v a r vectordep 31.66 23.53 29.91 38.43 NMF context 33.73?? 31.40 33.37? 25.21? 25.97?? 25.99?? 28.58 20.56"
D11-1094,N06-2015,0,0.031274,"carried out as a source of income. The NLP community’s standard answer to the ambiguity problem has always been some flavour of word sense disambiguation (WSD), which in its standard form boils down to choosing the best-possible fit from a pre-defined sense inventory. In recent years, it has become clear that this is in fact a very hard task to solve for computers and humans alike (Ide and Wilks, 2006; Erk et al., 2009; Erk, 2010). With these findings in mind, researchers have started looking at different methods to tackle language’s ambiguity, ranging from coarser-grained sense inventories (Hovy et al., 2006) and graded sense assignment (Erk and McCarthy, 2009), over word sense induction (Sch¨utze, 1998; Pantel and Lin, 2002; Agirre et al., 2006), to the computation of individual word meaning in context (Erk and Pad´o, 2008; Thater et al., 2010; Dinu and Lapata, 2010). This research inscribes itself in the same line of thought, in which the meaning disambiguation of a word is not just the assignment of a pre-defined sense; instead, the original meaning representation of a word is adapted ‘on the fly’, according to – and specifically tailored for – the particular context in which it appears. To be"
D11-1094,P98-2127,0,0.0919097,"portant dimensions that come out of the SVD are said to represent latent semantic dimensions, according to which nouns and documents can be represented more efficiently. Our model also applies a factorization technique (albeit a different one) in order to find a reduced semantic space. The nature of a word’s context is a determining factor in the kind of the semantic similarity that is induced. A broad context window (e.g. a paragraph or document) yields broad, topical similarity, whereas a small context window yields tight, synonym-like similarity. This has lead a number of researchers (e.g. Lin (1998)) to use the dependency relations that a particular word takes part in as context features. An overview of dependency-based semantic space models is given in Pad´o and Lapata (2007). A number of researchers have exploited the no1013 tion of context to differentiate between the different senses of a word in an unsupervised way (a task labeled word sense induction or WSI). Sch¨utze (1998) proposed a context-clustering approach, in which context vectors are created for the different instances of a particular word, and those contexts are grouped into a number of clusters, representing the differen"
D11-1094,S07-1009,0,0.372725,"d context words. 2 In this case, the sets of context features contain only one item, so the average probability distribution of the sets is just the latent probability distribution of their respective item. 1016 Evaluation In this section, we present a thorough evaluation of the method described above, and compare it with related methods for meaning computation in context. In order to test the applicability of the method to multiple languages, we present evaluation results for both English and French. 4.1 Datasets For English, we make use of the SEMEVAL 2007 English Lexical Substitution task (McCarthy and Navigli, 2007; McCarthy and Navigli, 2009). The task’s goal is to find suitable substitutes for a target word in a particular context. The complete data set contains 200 target words (about 50 for each part of speech, viz. nouns, verbs, adjectives, and adverbs). Each target word occurs in 10 different sentences, which yields a total of 2000 sentences. Five annotators provided suitable substitutes for each target word in the different contexts. For French, we developed a small-scale lexical substitution task ourselves, closely following the guidelines of the original English task. We manually selected 10 am"
D11-1094,nivre-etal-2006-maltparser,0,0.0298749,"the FRWaC corpus (Baroni et al., 2009). Four different native French speakers were then asked to provide suitable substitutes for the nouns in context.3 3 The task is provided as supplementary material to this paper; it is also available from the first author’s website. 4.2 Implementational details 4.3 The model for English has been trained on part of the UKW a C corpus (Baroni et al., 2009), covering about 500M words. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank4 , so that dependency triples could be extracted. The sentences of the English lexical substitution task have been tagged, lemmatized and parsed in the same way. The model for French has been trained on the French version of Wikipedia (± 100M words), parsed with the FRMG parser (Villemonte de La Clergerie, 2010) for French. For English, we built different models for each part of speech (nouns, verbs, adjectives and adverbs), which yields four models in tota"
D11-1094,J07-2002,0,0.290981,"Missing"
D11-1094,J98-1004,0,0.428432,"Missing"
D11-1094,W09-2506,0,0.186895,"Missing"
D11-1094,P10-1097,0,0.433017,"Missing"
D11-1094,W00-1308,0,0.317109,"Missing"
D11-1094,N03-1033,0,0.0105463,"for each noun we selected 10 different sentences from the FRWaC corpus (Baroni et al., 2009). Four different native French speakers were then asked to provide suitable substitutes for the nouns in context.3 3 The task is provided as supplementary material to this paper; it is also available from the first author’s website. 4.2 Implementational details 4.3 The model for English has been trained on part of the UKW a C corpus (Baroni et al., 2009), covering about 500M words. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank4 , so that dependency triples could be extracted. The sentences of the English lexical substitution task have been tagged, lemmatized and parsed in the same way. The model for French has been trained on the French version of Wikipedia (± 100M words), parsed with the FRMG parser (Villemonte de La Clergerie, 2010) for French. For English, we built different models for each part of speech (nouns, verbs, adjectiv"
D11-1094,C08-1117,1,0.916057,"Missing"
D11-1094,C00-2137,0,0.0278975,"and the latter one paraphrase induction. In the next paragraphs, we will describe the actual evaluation measures that have been used for both approaches. Paraphrase ranking Following Dinu and Lapata (2010), we compare the ranking produced by our model with the gold standard ranking using Kendall’s τb (which is adjusted for ties). For reasons of comparison, we also compute general average precision (GAP, Kishida (2005)), which was used by Erk and Pad´o (2010) and Thater et al. (2010) to evaluate their rankings. Differences between models are tested for significance using stratified shuffling (Yeh, 2000), using a standard number of 10000 iterations. We compare the results for paraphrase ranking to two different baselines. The first baseline is a random one, in which the gold standard is compared to an arbitrary ranking. The second baseline is a dependency-based vector space model that does not take the context of the particular instance into account (and thus returns the same ranking for each instance of the target word). This is a fairly competitive baseline, as noted by other researchers (Erk and Pad´o, 2008; Thater et al., 2009; Dinu and Lapata, 2010). Paraphrase induction To evaluate the"
D11-1094,S07-1044,0,0.0207272,"dels are able to reach much better results than Dinu and Lapata’s approach. The results indicate that our approach, after vector adaptation in context, is still able to provide accurate similarity calculations across the complete word space. While other algorithms are able to rank candidate substitutes at the expense of accurate similarity calculations, our approach is able to do both. This is one of the important advantages of our approach. For reasons of comparison, we also included the scores of the best performing models that participated in the SEMEVAL 2007 lexical substitution task (KU (Yuret, 2007) and IRST 2 (Giuliano et al., 2007), which got the best scores for Rbest and P10 , respectively). These models reach better scores compared to our models. Note, however, that all participants of the SEMEVAL 2007 lexical substitution task relied on a predefined sense inventory (i.e. WordNet, or a machine readable thesaurus). Our system, on the other hand, induces paraphrases in a fully unsupervised way. To our knowledge, this is the first time a fully unsupervised system is tested on the paraphrase induction task. model n v a r vectordep 31.66 23.53 29.91 38.43 NMF context 33.73?? 31.40 33.37?"
D11-1094,P08-1028,0,\N,Missing
D11-1094,C98-2122,0,\N,Missing
D11-1095,P06-4020,0,0.208943,"Missing"
D11-1095,P03-1068,0,0.0551819,"Missing"
D11-1095,P04-2007,0,0.302954,"y have mostly focussed on acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008). The method has also been popular in the related task of noun clusProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011). We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006). This graph-based, probabilistic clustering algorithm has some clear advantages over AGG (e.g. it delays the decision on a verb’"
D11-1095,C94-1042,0,0.345492,"Missing"
D11-1095,N06-2015,0,0.0272985,"omerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification. 1 Introduction A variety of verb classifications have been built to support NLP tasks. These include syntactic and semantic classifications, as well as ones which integrate aspects of both (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Hovy et al., 2006). Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness. One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic 1023 and semantic properties of verbs. Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Da"
D11-1095,C08-1057,1,0.919993,"Missing"
D11-1095,P08-1050,0,0.222726,"emantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required. In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen ´ S´eaghdha and Copestake, 2008; Vlaet al., 2008; O chos et al., 2009). The best of such approaches have yielded promising results. However, they have mostly focussed on acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been"
D11-1095,W06-1664,0,0.0242755,"ering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008). The method has also been popular in the related task of noun clusProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011). We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006). This graph-based, probabilistic clustering algorithm has some clear advantages over AGG (e.g. it delays the decision on a verb’s cluster membership at any level until a full graph is available, minimising the problem of error propagation) and it has been shown to perform better than several other hierarchical clustering methods in recent comparisons (Yu et al., 2006). The method has been applied to the identification of social network communities (Lin et"
D11-1095,C08-1082,0,0.0546149,"Missing"
D11-1095,J05-1004,0,0.0987891,"utperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification. 1 Introduction A variety of verb classifications have been built to support NLP tasks. These include syntactic and semantic classifications, as well as ones which integrate aspects of both (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Hovy et al., 2006). Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness. One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic 1023 and semantic properties of verbs. Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translat"
D11-1095,P07-1115,1,0.851331,"relations associated with the verb in parsed data. D: B with SCFs parameterized for objects appearing in grammatical relations associated with the verb in parsed data. These features are purely syntactic. Although semantic features – verb selectional preferences – proved the best (when used in combination with syntactic features) in the recent work of Sun and Korhonen (2009), we left such features for future work because we noticed that different levels of classification are likely to require semantic features at different granularities. We extracted the syntactic features using the system of Preiss et al. (2007). The system tags, lemmatizes and parses corpus data using the RASP (Robust Accurate Statistical Parsing toolkit (Briscoe et al., 2006)), and on the basis of the resulting grammatical relations, assigns each occurrence of a verb as a member of one of the 168 verbal SCFs. We parameterized the SCFs as described above using the information provided by the system. 1025 3.2 Clustering We introduce the agglomerative clustering (AGG) and Hierarchical Graph Factorization Clustering (HGFC) methods in the following two subsections, respectively. The subsequent two subsections present our extensions to H"
D11-1095,D07-1043,0,0.136598,"ets described in section 2 and evaluated them both quantitatively and qualitatively, as described in the subsequent sections. 4.1 where nij is the size of the intersection between class i and cluster j. We used normalized mutual information (NMI) and F-Score (F) to evaluate hierarchical clustering results on T2 and T3. NMI measures the amount of statistical information shared by two random variables representing the clustering result and the goldstandard labels. Given random variables A and B: F= 2 · mPUR · ACC mPUR + ACC F is not suitable for comparing results with different cluster numbers (Rosenberg and Hirschberg, 2007). Therefore, we only report NMI when the number of classes in clustering and gold-standard is substantially different. Finally, we supplemented quantitative evaluation with qualitative evaluation of clusters produced by different methods. 4.2 Quantitative evaluation We first evaluated AGG and the basic (unconstrained) HGFC on the small flat test set T1. The verbs in DOM-CLUSTi ACC = i=1 main purpose of this evaluation was to compare the number of verbs The formula of Radj is (Hubert and Arabie, 1985): results of our methods against previously published results on the same test set. The number"
D11-1095,J06-2001,0,0.618589,"including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008). Because verbs change their meaning and behaviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required. In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen ´ S´eaghdha and Copestake, 2008; Vlaet al., 2008; O chos et al., 2009). The best of such approaches have yielded promising results. However, they have mostly focussed on acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchica"
D11-1095,P02-1029,0,0.195611,"Missing"
D11-1095,W03-0410,0,0.591012,"chos et al., 2009). The best of such approaches have yielded promising results. However, they have mostly focussed on acquiring and evaluating flat classifications. Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008). The method has also been popular in the related task of noun clusProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011). We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006). This graph-based, probabilistic clustering algori"
D11-1095,D09-1067,1,0.893622,"umber of clusters manually. The second is addition of soft constraints to guide the clustering performance (Vlachos et al., 2009). This is useful for situations where a partial (e.g. a flat) verb classification is available and the goal is to extend it. Adopting a set of lexical and syntactic features which have performed well in previous works, we compare the performance of the two methods on test sets extracted from Levin and VerbNet. When evaluated on a flat clustering task, HGFC outperforms AGG and performs very similarly with the best flat clustering method reported on the same test set (Sun and Korhonen, 2009). When evaluated on a hierarchical task, HGFC performs considerably better than AGG at all levels of gold standard classification. The constrained version of HGFC performs the best, as expected, demonstrating the usefulness of soft constraints for extending partial classifications. Our qualitative analysis shows that HGFC is capable of detecting novel information not included in our gold standards. The unconstrained version can be used to acquire novel classifications from scratch while the constrained version can be used to extend existing ones with additional class members, classes and level"
D11-1095,C96-2212,0,0.105341,"vin-style clustering. We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008). The method has also been popular in the related task of noun clusProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011). We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006). This graph-based, probabilistic clustering algorithm has some clear advantages over AGG (e.g. it delays the decision on a verb’s cluster membership at any level until a full graph is available, minimising the problem of error propagation) and it has been shown to perform better than several other hierarchical clustering methods in recent comparisons (Yu et al., 2006). The method has been applied to the identification of social networ"
D11-1095,W09-0210,1,0.943807,". The method has been applied to the identification of social network communities (Lin et al., 2008), but has not been used (to the best of our knowledge) in NLP before. We modify HGFC with a new tree extraction algorithm which ensures a more consistent result, and we propose two novel extensions to it. The first is a method for automatically determining the tree structure (i.e. number of clusters to be produced for each level of the hierarchy). This avoids the need to predetermine the number of clusters manually. The second is addition of soft constraints to guide the clustering performance (Vlachos et al., 2009). This is useful for situations where a partial (e.g. a flat) verb classification is available and the goal is to extend it. Adopting a set of lexical and syntactic features which have performed well in previous works, we compare the performance of the two methods on test sets extracted from Levin and VerbNet. When evaluated on a flat clustering task, HGFC outperforms AGG and performs very similarly with the best flat clustering method reported on the same test set (Sun and Korhonen, 2009). When evaluated on a hierarchical task, HGFC performs considerably better than AGG at all levels of gold"
D11-1095,W04-3213,0,\N,Missing
D11-1095,P98-1013,0,\N,Missing
D11-1095,C98-1013,0,\N,Missing
D11-1095,P08-1063,0,\N,Missing
D11-1095,kunze-lemnitzer-2002-germanet,0,\N,Missing
D11-1097,D10-1115,0,0.0392896,"yntactic dependencies. Thater et al. (2010) develop this geometric approach further using a space of second-order distributional vectors that represent the words typically co-occurring with the contexts in which a word typically appears. The primary concern of these authors is to model the effect of context on word meaning; the work we present in this paper uses similar intuitions in a probabilistic modelling framework. A parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra (Smolensky, 1990; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer ´ S´eaghdha and Dumais, 1997). More recently, O (2010) and Ritter et al. (2010"
D11-1097,P06-4020,0,0.0648851,"sing Spearman’s ρ). Both Mitchell and Lapata and Erk and Pad´o (2008) split the data into a development portion and a test portion, the development portion consisting of the judgements of six annotators; in order to compare our results with previous research we use the same data split. To evaluate performance, the predictions made by a model are compared to the judgements of each annotator in turn (using ρ) and the resulting per-annotator ρ values are averaged. 4.2 Models All models were trained on the written section of the British National Corpus (around 90 million words), parsed with RASP (Briscoe et al., 2006). The BNC was also used by Mitchell and Lapata (2008) and Erk and Pad´o (2008); as the ML08 dataset was compiled using words appearing more than 50 times in the BNC, there are no coverage problems caused by data sparsity. We trained LDA models for the grammatical relations v:ncsubj:n and n:ncsubj−1 :v No optimisation Optimised on dev Erk and Pad´o (2008) Model C→T T→C T↔C C→T T→C T↔C Mult SVS PARA S IM 0.24 0.34 0.36 0.39 0.33 0.39 0.24 0.35 0.41 0.41 0.37 0.41 0.24 0.27 Table 1: Performance (average ρ) on the ML08 test set and used these to create predictors of type C → T and T → C, respectiv"
D11-1097,E09-1013,0,0.0344854,"3), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. 1048 As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al., 2007). 3 3.1 Models Latent variable context models In this paper we consider generative models of lexical choice that assign a probability to a particular word appearing in a given linguistic context. In particular, we follow recent work (Dinu and Lapata, ´ S´eaghdha, 2010; Ritter et al., 2010) in as2010; O suming a latent variable model that associates contexts with distributions over a shared set of variables and associates each variable with a distribution over the vocabulary of word types: X P (w|c) = P (w|z)P (z|c)"
D11-1097,D10-1113,0,0.109712,"ngdom Anna.Korhonen@cl.cam.ac.uk phrase the body/corpse deliberated the motion. . . ?” and “how similar are the phrases the body deliberated the motion and the corpse rotted?”. In this paper we focus on answering questions of the former type and investigate models that describe the effect of syntactic context on the meaning of a single word. The work described in this paper uses probabilistic latent variable models to describe patterns of syntactic interaction, building on the selectional prefer´ S´eaghdha (2010) and Ritter et al. ence models of O (2010) and the lexical substitution models of Dinu and Lapata (2010). We propose novel methods for incorporating information about syntactic context in models of lexical choice, yielding a probabilistic analogue to dependency-based models of contextual similarity. Our models attain state-of-the-art performance on two evaluation datasets: a set of sentence similarity judgements collected by Mitchell and Lapata (2008) and the dataset of the English Lexical Substitution Task (McCarthy and Navigli, 2009). In view of the well-established effectiveness of dependency-based distributional semantics and of probabilistic frameworks for semantic inference, we expect that"
D11-1097,D08-1094,0,0.171636,"Missing"
D11-1097,W11-0114,0,0.281803,"ypically used, have a long history in Natural Language Processing (Sp¨arck Jones, 1964; Harper, 1965). Such models still constitute one of the most popular approaches to lexical semantics, with many proven applications. Much work in distributional semantics treats words as non-contextualised units; the models that are constructed can answer questions such as “how similar are the words body and corpse?” but do not capture the way the syntactic context in which a word appears can affect its interpretation. Recent developments (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010; Grefenstette et al., 2011) have aimed to address compositionality of meaning in terms of distributional semantics, leading to new kinds of questions such as “how similar are the usages of the words body and corpse in the 1047 Anna Korhonen Computer Laboratory University of Cambridge United Kingdom Anna.Korhonen@cl.cam.ac.uk phrase the body/corpse deliberated the motion. . . ?” and “how similar are the phrases the body deliberated the motion and the corpse rotted?”. In this paper we focus on answering questions of the former type and investigate models that describe the effect of syntactic context on the meaning of a si"
D11-1097,C65-1010,0,0.486648,"resulting models capture the effects of context on the interpretation of a word and in particular its effect on the appropriateness of replacing that word with a potentially related one. Evaluating our techniques on two datasets, we report performance above the prior state of the art for estimating sentence similarity and ranking lexical substitutes. 1 Introduction Distributional models of lexical semantics, which assume that aspects of a word’s meaning can be related to the contexts in which that word is typically used, have a long history in Natural Language Processing (Sp¨arck Jones, 1964; Harper, 1965). Such models still constitute one of the most popular approaches to lexical semantics, with many proven applications. Much work in distributional semantics treats words as non-contextualised units; the models that are constructed can answer questions such as “how similar are the words body and corpse?” but do not capture the way the syntactic context in which a word appears can affect its interpretation. Recent developments (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010; Grefenstette et al., 2011) have aimed to address compositionality of meaning in terms of distribution"
D11-1097,P10-1116,0,0.0242497,"t Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. 1048 As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al., 2007). 3 3.1 Models Latent variable context models In this paper we consider generative models of lexical choice that assign a probability to a particular word appearing in a given linguistic context. In particular, we follow recent work (Dinu and Lapata, ´ S´eaghdha, 2010; Ritter et al., 2010) in as2010; O suming a latent variable model that associates contexts with distributions over a shared set of variables and associates each variable with a distribution over the vocab"
D11-1097,P08-1028,0,0.78476,"word’s meaning can be related to the contexts in which that word is typically used, have a long history in Natural Language Processing (Sp¨arck Jones, 1964; Harper, 1965). Such models still constitute one of the most popular approaches to lexical semantics, with many proven applications. Much work in distributional semantics treats words as non-contextualised units; the models that are constructed can answer questions such as “how similar are the words body and corpse?” but do not capture the way the syntactic context in which a word appears can affect its interpretation. Recent developments (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010; Grefenstette et al., 2011) have aimed to address compositionality of meaning in terms of distributional semantics, leading to new kinds of questions such as “how similar are the usages of the words body and corpse in the 1047 Anna Korhonen Computer Laboratory University of Cambridge United Kingdom Anna.Korhonen@cl.cam.ac.uk phrase the body/corpse deliberated the motion. . . ?” and “how similar are the phrases the body deliberated the motion and the corpse rotted?”. In this paper we focus on answering questions of the former type and investigate model"
D11-1097,S10-1052,0,0.0292408,"ults; in particular, it tended to rank common words highly in most contexts.6 As before we compiled training data by extracting target-context cooccurrences from a text corpus. In addition to the parsed BNC described above we used a corpus of Wikipedia text consisting of over 45 million sentences (almost 1 billion words) parsed using the fast Combinatory Categorial Grammar (CCG) parser described by Clark et al. (2009). The depen5 We use the software package available at http://www. nlpado.de/˜sebastian/sigf.html. 6 Favouring more general words may indeed make sense in some paraphrasing tasks (Nulty and Costello, 2010). 1053 dency representation produced by this parser is interoperable with the RASP dependency format. In order to focus our models on semantically discriminative information and make inference more tractable we ignored all parts of speech other than nouns, verbs, adjectives, prepositions and adverbs. Stopwords and words of fewer than three characters were removed. We also removed the very frequent but semantically weak lemmas be and have. We compare two classes of context models: models learned from window-based contexts and models learned from syntactic dependency contexts. For the syntactic"
D11-1097,P10-1045,1,0.916884,"Missing"
D11-1097,J07-2002,0,0.0966089,"Missing"
D11-1097,P93-1024,0,0.680563,"this paper uses similar intuitions in a probabilistic modelling framework. A parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra (Smolensky, 1990; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer ´ S´eaghdha and Dumais, 1997). More recently, O (2010) and Ritter et al. (2010) reformulated Rooth et al.’s approach in a Bayesian framework using models related to Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the co"
D11-1097,D10-1114,0,0.0556038,"iour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer ´ S´eaghdha and Dumais, 1997). More recently, O (2010) and Ritter et al. (2010) reformulated Rooth et al.’s approach in a Bayesian framework using models related to Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. 1048 As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al., 2007). 3 3.1 Models Latent variable context mo"
D11-1097,P10-1044,0,0.0632602,"nd Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer ´ S´eaghdha and Dumais, 1997). More recently, O (2010) and Ritter et al. (2010) reformulated Rooth et al.’s approach in a Bayesian framework using models related to Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of polysemous words. 1048 As described in Section 3 below, Dinu and Lapata (2010) propose an LDA-based model for lexical substitution; the techniques presented in this paper can be viewed as a generalisation of theirs. Topic models have a"
D11-1097,P99-1014,0,0.0666044,"tuitions in a probabilistic modelling framework. A parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra (Smolensky, 1990; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer ´ S´eaghdha and Dumais, 1997). More recently, O (2010) and Ritter et al. (2010) reformulated Rooth et al.’s approach in a Bayesian framework using models related to Latent Dirichlet Allocation (Blei et al., 2003), demonstrating that this “topic modelling” architecture is a very good fit for capturing selectional preferences. Reisinger and Mooney (2010) investigate nonparametric Bayesian models for teasing apart the context distributions of p"
D11-1097,P10-1093,0,0.107586,"s corresponding to different syntactic dependencies. Thater et al. (2010) develop this geometric approach further using a space of second-order distributional vectors that represent the words typically co-occurring with the contexts in which a word typically appears. The primary concern of these authors is to model the effect of context on word meaning; the work we present in this paper uses similar intuitions in a probabilistic modelling framework. A parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra (Smolensky, 1990; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Grefenstette et al., 2011). This nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications. Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al. (1993) and Rooth et al. (1999). Latent variable models are also conceptually similar to non-probabilistic dimensionality reduction techniques such as Latent Semantic Analysis (Landauer ´ S´eaghdha and Dumais, 1997). More recently, O ("
D11-1097,P10-1097,0,0.128197,"Missing"
D11-1097,C00-2137,0,0.131697,"The cat is fierce v:ncsubj:n P REPOSITIONS : n:ncmod:i  The cat in the hat O n:prep in:n ⇒  The cat in the hat i:dobj:n Table 3: Dependency graph preprocessing measure that was used in the original Lexical Substitution Task; this measure assigns credit for the proportion of the first 10 proposed paraphrases that are present in the gold standard and in the context of ranking attested substitutes it is unclear how to obtain non-trivial results for target words with 10 or fewer possible substitutes. We calculate statistical significance of performance differences using stratified shuffling (Yeh, 2000).5 5.2 Models We apply the models developed in Section 3.1 to the Lexical Substitution Task dataset using dependencyand window-based context information. Here we only use the S IM predictor type. PARA did not give satisfactory results; in particular, it tended to rank common words highly in most contexts.6 As before we compiled training data by extracting target-context cooccurrences from a text corpus. In addition to the parsed BNC described above we used a corpus of Wikipedia text consisting of over 45 million sentences (almost 1 billion words) parsed using the fast Combinatory Categorial Gr"
D14-1032,N10-1011,0,0.121314,"tively than alternatives. We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation. 1 Introduction Multi-modal models that learn semantic representations from both language and information about the perceptible properties of concepts were originally motivated by parallels with human word learning (Andrews et al., 2009) and evidence that many concepts are grounded in perception (Barsalou and Wiemer-Hastings, 2005). The perceptual information in such models is generally mined directly from images (Feng and Lapata, 2010; Bruni et al., 2012) or from data collected in psychological studies (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). 1 Contributors to the USF dataset (Nelson et al., 2004). 255 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255–265, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics to decrease as target concepts become more abstract. Indeed, for the most abstract concepts of all, language-only models still provide the most effective learning mechanism. Finally, we investigate the optimum q"
D14-1032,P12-1015,0,0.0580315,"s. We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation. 1 Introduction Multi-modal models that learn semantic representations from both language and information about the perceptible properties of concepts were originally motivated by parallels with human word learning (Andrews et al., 2009) and evidence that many concepts are grounded in perception (Barsalou and Wiemer-Hastings, 2005). The perceptual information in such models is generally mined directly from images (Feng and Lapata, 2010; Bruni et al., 2012) or from data collected in psychological studies (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). 1 Contributors to the USF dataset (Nelson et al., 2004). 255 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255–265, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics to decrease as target concepts become more abstract. Indeed, for the most abstract concepts of all, language-only models still provide the most effective learning mechanism. Finally, we investigate the optimum quantity and type of p"
D14-1032,P12-1092,0,0.0443642,"by how well they reflect free association scores, an empirical measure of cognitive conceptual proximity. The University of South Florida Norms (USF) (Nelson et al., 2004) contain free association scores for over 40,000 concept pairs, and have been widely used in NLP to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Each concept that we extract from the USF database has also been rated for conceptual concreteness on a Likert scale of 1-7 by at least 10 human annotators. Following previous studies (Huang et al., 2012; Silberer and Lapata, 2012), we measure the (Spearman ρ) correlation between association scores and the cosine similarity of vector representations. We create separate abstract and concrete concept lists by ranking the USF concepts according to concreteness and sampling at random from the first and fourth quartiles respectively. We also introduce a complementary noun/verb dichotomy, Results and Discussion Our experiments were designed to answer four questions, outlined in the following subsections: (1) Which model architectures perform best at combining information pertinent to multiple modal"
D14-1032,D12-1130,0,0.179974,"performance of multi-modal models and for theories of abstract conceptual representation. 1 Introduction Multi-modal models that learn semantic representations from both language and information about the perceptible properties of concepts were originally motivated by parallels with human word learning (Andrews et al., 2009) and evidence that many concepts are grounded in perception (Barsalou and Wiemer-Hastings, 2005). The perceptual information in such models is generally mined directly from images (Feng and Lapata, 2010; Bruni et al., 2012) or from data collected in psychological studies (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). 1 Contributors to the USF dataset (Nelson et al., 2004). 255 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255–265, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics to decrease as target concepts become more abstract. Indeed, for the most abstract concepts of all, language-only models still provide the most effective learning mechanism. Finally, we investigate the optimum quantity and type of perceptual input for such models. Between the most concrete concepts, which"
D14-1032,P14-2135,1,0.5637,"Missing"
D14-1032,P14-1068,0,0.272666,"Missing"
D14-1032,C94-1103,0,0.0243319,"large (10) Table 1: Concepts identified in images in the ESP Game (left) and features produced for concepts by human annotators in the CSLB dataset (with feature strength, max=30). Wikipedia text, split into sentences and with punctuation removed. on the intuition that information propagation may occur differently from noun to noun or from noun to verb (because of their distinct structural relationships in sentences). POS-tags are not assigned as part of the USF data, so we draw the noun/verb distinction based on the majority POS-tag of USF concepts in the lemmatized British National Corpus (Leech et al., 1994). The abstract/concrete and noun/verb dichotomies yield four distinct concept lists. For consistency, the concrete noun list is filtered so that each concrete noun concept w has a perceptual representation b(w) in both PESP and PCSLB . For the four resulting concept lists C (concrete/abstract, noun/verb), a corresponding set of evaluation pairs {(w1 , w2 ) ∈ U SF : w1 , w2 ∈ C} is extracted (see Table 3 for details). 2.2 3 Concept 1 abdomen (6.83) throw (4.05) hope (1.18) egg (5.79) Concept 2 stomach (6.04) ball (6.08) glory (3.53) milk (6.66) Assoc. 0.566 0.234 0.192 0.012 Table 2: Example co"
D14-1032,P13-1056,0,0.0812557,"Missing"
D14-1032,P10-1040,0,0.0186667,"describing how our multi-modal architecture encodes and integrates perceptual information, we first describe the underlying corpus-based representation learning model. Language-only Model Our multi-modal architecture builds on the continuous log-linear skipgram language model proposed by Mikolov et al. (2013). This model learns lexical representations in a similar way to neural-probabilistic language models (NPLM) but without a non-linear hidden layer, a simplification that facilitates the efficient learning of large vocabularies of dense representations, generally referred to as embeddings (Turian et al., 2010). Embeddings learned by the model achieve state-of-the-art performance on several evaluations including sentence completion and analogy modelling (Mikolov et al., 2013). For each word type w in the vocabulary V , the model learns both a ‘target-embedding’ rw ∈ Rd and a ‘context-embedding’ rˆw ∈ Rd such that, given a target word, its ability to predict nearby context words is maximized. The probability of seeing context word c given target w is defined as: We train our model on running-text language and two sources of perceptual descriptors for concrete nouns: the ESPGame dataset of annotated i"
D14-1032,D13-1115,0,0.622134,"Missing"
D14-1032,Q14-1023,1,\N,Missing
D14-1034,P05-1038,0,0.0670223,"ter Laboratory Technion, IIT Computer Laboratory University of Cambridge Haifa, Israel University of Cambridge sb895@cam.ac.uk roiri@ie.technion.ac.il alk23@cam.ac.uk Abstract (2) Indirect Transitive: [They]NP [distinguished]VP [between [me and you]ADVP ]PP . (3) Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves an"
D14-1034,N10-1083,0,0.0166289,"Missing"
D14-1034,W05-0621,0,0.0203224,", Israel University of Cambridge sb895@cam.ac.uk roiri@ie.technion.ac.il alk23@cam.ac.uk Abstract (2) Indirect Transitive: [They]NP [distinguished]VP [between [me and you]ADVP ]PP . (3) Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicate"
D14-1034,A97-1052,0,0.0293914,"appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and the"
D14-1034,chesley-salmon-alt-2006-automatic,0,0.0290533,"riation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited. We propose a new unsupervised, Markov Random Field"
D14-1034,D10-1088,0,0.0322158,"Missing"
D14-1034,N09-1009,0,0.0432093,"Missing"
D14-1034,W02-0907,1,0.450511,"ial argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for N"
D14-1034,de-marneffe-etal-2006-generating,0,0.0159356,"iss et al., 2007; Lippincott et al., 2012; Van de Cruys et al., 2012; Reichart and Korhonen, 2013) and other languages, including French (Messiant, 2008), Italian (Lenci et al., 2008), Turkish (Uzun et al., 2008), Japanese (Kawahara and Kurohashi, 2010) and Chinese (Han et al., 2008). The prominent input to these systems are grammatical relations (GRs) which express binary dependencies between words (e.g. direct and indirect objects, various types of complements and conjunctions). These are generated by some parsers (e.g. (Briscoe et al., 2006)) and can be extracted from the output of others (De-Marneffe et al., 2006). Two representative systems for English are the Cambridge system (Preiss et al., 2007) and the BioLexicon system which was used to acquire a substantial lexicon for biomedicine (Venturi et al., 2009). These systems extract GRs at the verb instance level from the output of a parser: the RASP general-language unlexicalized parser3 (Briscoe et al., 2006) and the lexicalized Enju parser tuned to the biomedical domain (Miyao and Tsujii, 2005), respectively. They generate potential SCFs by mapping GRs to a predefined SCF inventory using a set of manually developed rules (the Cambridge system) or by"
D14-1034,P11-1112,0,0.0154919,"f manually developed rules (the Cambridge system) or by simply considering the sets of GRs including verbs in question as potential SCFs (BioLexicon). Finally, a type level lexicon 2 (Lippincott et al., 2012) does not use a parser, but the syntactic frames induced by the system do not capture sets of arguments for verbs, so are not SCFs in a traditional sense. 3 A so-called unlexicalized parser is a parser trained without explicit SCF annotations. 279 vised parsers (e.g. grammatical relations). Finally, a number of recent works addressed related tasks such as argument role clustering for SRL (Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titvo and Klementiev, 2012) in an unsupervised manner. While these works differ from ours in the task (clustering arguments rather than verbs) and the level of supervision (applying a supervised parser), like us they analyze the verb argument structure at the instance level. is built through noisy frame filtering (based on frequencies or on external resources and annotations), which aims to remove errors from parsing and argument-adjunct distinction. Clearly, these systems require extensive manual work: a-priori definition of an SCF inventory and rules, manually anno"
D14-1034,D11-1122,0,0.00964019,"f manually developed rules (the Cambridge system) or by simply considering the sets of GRs including verbs in question as potential SCFs (BioLexicon). Finally, a type level lexicon 2 (Lippincott et al., 2012) does not use a parser, but the syntactic frames induced by the system do not capture sets of arguments for verbs, so are not SCFs in a traditional sense. 3 A so-called unlexicalized parser is a parser trained without explicit SCF annotations. 279 vised parsers (e.g. grammatical relations). Finally, a number of recent works addressed related tasks such as argument role clustering for SRL (Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titvo and Klementiev, 2012) in an unsupervised manner. While these works differ from ours in the task (clustering arguments rather than verbs) and the level of supervision (applying a supervised parser), like us they analyze the verb argument structure at the instance level. is built through noisy frame filtering (based on frequencies or on external resources and annotations), which aims to remove errors from parsing and argument-adjunct distinction. Clearly, these systems require extensive manual work: a-priori definition of an SCF inventory and rules, manually anno"
D14-1034,I05-1006,0,0.0219923,"Missing"
D14-1034,lenci-etal-2008-unsupervised,0,0.0223961,"epending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited. We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is designed to address t"
D14-1034,S13-1035,0,0.0202436,"Missing"
D14-1034,P12-1044,1,0.932598,"three different frames, the difference between which is not evident when considering the phrase structure categorization: (1) Direct Transitive: [They]NP [distinguished]VP [the mast]NP [of [ships on the horizon ]NP ]PP . 1 The verb similarity dataset used for the evaluation of our model is publicly available at ie.technion.ac.il/∼roiri/. 278 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 278–289, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics that use modern machine learning techniques (Debowski, 2009; Lippincott et al., 2012; Van de Cruys et al., 2012; Reichart and Korhonen, 2013) use supervised parsers to pre-process the data2 . Supervised parsers are notoriously sensitive to domain variation (Lease and Charniak, 2005). As annotation of data for each new domain is unrealistic, current SCF systems suffer from poor portability. This problem is compounded for the many systems that employ manually developed SCF rules because rules are inherently ignorant to domain-specific preferences. The few SCF studies that focused on specific domains (e.g. biomedicine) have reported poor performance due to these reasons (Rimell"
D14-1034,P05-1012,0,0.0116371,"ed to be the concatenation of that verb’s basic feature representation with the basic representations of the words in a size 2 window around the represented verb. The final feature representation for the i-th verb instance in our dataset is therefore defined to be vi = [w−2 , w−1 , vbi , w+1 , w+2 ], where w−k and w+k are the basic feature representations of the words in distance −k or +k from the i-th verb instance in its sentence, and vbi is the basic feature representation of that verb instance. Our basic feature representation is inspired from the feature representation of the MST parser (McDonald et al., 2005) except that in the parser the features represent a directed edge in the complete directed graph defined over the words in a sentence that is to be parsed, while our features are generated for word n-grams. Particularly, our feature set is a concatenation of two sets derived from the MST set described in Table 1 of (McDonald et al., 2005) in the following way: (1) In both sets the parent word in the parser’s set is replaced with the represented word; (2) In one set every child word in the parser’s set is replaced by the word to the left of the represented word and in the other set it is replac"
D14-1034,han-etal-2000-handling,0,0.0214665,"]VP [between [me and you]ADVP ]PP . (3) Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong"
D14-1034,messiant-etal-2008-lexschem,1,0.834325,"ir probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited. We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is d"
D14-1034,ienco-etal-2008-automatic,0,0.0242489,"s themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited. We propose a new unsupervised, Markov Random Field-based model for SCF"
D14-1034,W07-2416,0,0.0160652,". For our model we estimated the verb pair similarity using the Tanimato similarity score for binary vectors: P Xi ∧ Yi T (X, Y ) = Pi i xi ∨ Yi For the baseline model, where the features are collocation counts, we used the standard cosine similarity. Our second baseline is identical to our model, except that: (a) the data is parsed with the Stanford parser (version 3.3.0, (Klein and Manning, 2003)) which was trained with sections 2-21 of the WSJ corpus; (b) the phrase structure output of the parser is transformed to the CoNLL dependency format using the official CoNLL 2007 conversion script (Johansson and Nugues, 2007); and then (c) the SCF of each verb instance is inferred using the rule-based system used by (Reichart and Korhonen, 2013). The vector space representation for each verb is then created using the process we described for our model and the same holds for vector comparison. This baseline allows direct comparison of frames induced by our SCF model with those derived from a supervised parser’s output. We computed the Pearson correlation between the scores of each of the models and the human scores. The results demonstrate the superiority of our model in predicting verb similarity: the correlation"
D14-1034,P05-1011,0,0.0119094,"s types of complements and conjunctions). These are generated by some parsers (e.g. (Briscoe et al., 2006)) and can be extracted from the output of others (De-Marneffe et al., 2006). Two representative systems for English are the Cambridge system (Preiss et al., 2007) and the BioLexicon system which was used to acquire a substantial lexicon for biomedicine (Venturi et al., 2009). These systems extract GRs at the verb instance level from the output of a parser: the RASP general-language unlexicalized parser3 (Briscoe et al., 2006) and the lexicalized Enju parser tuned to the biomedical domain (Miyao and Tsujii, 2005), respectively. They generate potential SCFs by mapping GRs to a predefined SCF inventory using a set of manually developed rules (the Cambridge system) or by simply considering the sets of GRs including verbs in question as potential SCFs (BioLexicon). Finally, a type level lexicon 2 (Lippincott et al., 2012) does not use a parser, but the syntactic frames induced by the system do not capture sets of arguments for verbs, so are not SCFs in a traditional sense. 3 A so-called unlexicalized parser is a parser trained without explicit SCF annotations. 279 vised parsers (e.g. grammatical relations"
D14-1034,kawahara-kurohashi-2010-acquiring,0,0.0111722,"omain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited. We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is designed to address these problems. The system relies on supervised POS tagging rather than"
D14-1034,W05-1002,0,0.0343409,"Cambridge sb895@cam.ac.uk roiri@ie.technion.ac.il alk23@cam.ac.uk Abstract (2) Indirect Transitive: [They]NP [distinguished]VP [between [me and you]ADVP ]PP . (3) Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question ("
D14-1034,P03-1054,0,0.0724575,"tation, to the best of our knowledge). Second, in order to compare the output of our system to a rule-based SCF system that utilizes a supervised syntactic parser, we turn to a task-based evaluation. We aim to predict the degree of similarity between verb pairs and, following (Pado and Lapata, 2007) , we do so using a syntactic-based vector space model (VSM). We construct three VSMs - (a) one that derives features from our clusters; (b) one whose features come from the output of a state-of-the-art verb type level, rule based, SCF system (Reichart and Korhonen, 2013) that uses a modern parser (Klein and Manning, 2003); and (c) a standard lexical VSM. Below we show that our system compares favorably in both evaluations. Data. We experimented with two datasets taken from different domains: labor legislation and environment (Quochi et al., 2012). These datasets were created through web crawling followed by domain filtering. Each sentence in both datasets may contain multiple verbs but only one target verb has been manually annotated with a SCF. The labour legislation domain dataset contains 4415 annotated verb instances (and hence also ˆ (i, j) W i∈A,j∈B Using this definition, the normalized link ratio of A a"
D14-1034,J05-3003,0,0.0822047,"Missing"
D14-1034,W00-1325,1,0.687278,"orization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooc"
D14-1034,J07-2002,0,0.0118427,"comparison to previous work, is therefore challenging. We therefore evaluate our system in two ways. First, we compare its output, as well as the output of a number of clustering baselines, to the gold standard annotation of corpora from two different domains (the only publicly available ones with instance level SCF annotation, to the best of our knowledge). Second, in order to compare the output of our system to a rule-based SCF system that utilizes a supervised syntactic parser, we turn to a task-based evaluation. We aim to predict the degree of similarity between verb pairs and, following (Pado and Lapata, 2007) , we do so using a syntactic-based vector space model (VSM). We construct three VSMs - (a) one that derives features from our clusters; (b) one whose features come from the output of a state-of-the-art verb type level, rule based, SCF system (Reichart and Korhonen, 2013) that uses a modern parser (Klein and Manning, 2003); and (c) a standard lexical VSM. Below we show that our system compares favorably in both evaluations. Data. We experimented with two datasets taken from different domains: labor legislation and environment (Quochi et al., 2012). These datasets were created through web crawl"
D14-1034,E12-1003,0,0.011159,") or by simply considering the sets of GRs including verbs in question as potential SCFs (BioLexicon). Finally, a type level lexicon 2 (Lippincott et al., 2012) does not use a parser, but the syntactic frames induced by the system do not capture sets of arguments for verbs, so are not SCFs in a traditional sense. 3 A so-called unlexicalized parser is a parser trained without explicit SCF annotations. 279 vised parsers (e.g. grammatical relations). Finally, a number of recent works addressed related tasks such as argument role clustering for SRL (Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titvo and Klementiev, 2012) in an unsupervised manner. While these works differ from ours in the task (clustering arguments rather than verbs) and the level of supervision (applying a supervised parser), like us they analyze the verb argument structure at the instance level. is built through noisy frame filtering (based on frequencies or on external resources and annotations), which aims to remove errors from parsing and argument-adjunct distinction. Clearly, these systems require extensive manual work: a-priori definition of an SCF inventory and rules, manually annotated sentences for training a supervised parser, SCF"
D14-1034,P07-1115,1,0.78629,"d positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the grammatical dependency output of supervised statistical parsers. Even the handful of recent systems Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level. These systems suffer from poor portability across domains and their benefit for NLP tasks that involve"
D14-1034,N03-1033,0,0.0421755,"Missing"
D14-1034,P13-1085,1,0.752952,"h is not evident when considering the phrase structure categorization: (1) Direct Transitive: [They]NP [distinguished]VP [the mast]NP [of [ships on the horizon ]NP ]PP . 1 The verb similarity dataset used for the evaluation of our model is publicly available at ie.technion.ac.il/∼roiri/. 278 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 278–289, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics that use modern machine learning techniques (Debowski, 2009; Lippincott et al., 2012; Van de Cruys et al., 2012; Reichart and Korhonen, 2013) use supervised parsers to pre-process the data2 . Supervised parsers are notoriously sensitive to domain variation (Lease and Charniak, 2005). As annotation of data for each new domain is unrealistic, current SCF systems suffer from poor portability. This problem is compounded for the many systems that employ manually developed SCF rules because rules are inherently ignorant to domain-specific preferences. The few SCF studies that focused on specific domains (e.g. biomedicine) have reported poor performance due to these reasons (Rimell et al., 2013). Another limitation of most current SCF sys"
D14-1034,C12-1165,1,0.836868,"Missing"
D14-1034,W09-1121,1,0.889241,"Missing"
D14-1034,C12-1141,1,0.830797,"e to the MRF’s potential functions and then compute the approximate MAP solution of the resulted model using the MPLP algorithm. Noising was done by adding an  term to the lambda values described in section 3.2 7 . This protocol results in a set of cluster (label) assignments for the involved verb instances, which we treat as an ensemble of experts from which a final, high quality, solution is to be induced. The basic idea in ensemble learning is that if several experts independently cluster together two verb instances, our belief that these verbs belong in the same cluster should increase. (Reichart et al., 2012) implemented this idea through the kway normalized cut clustering algorithm (Yu and ˆ = Shi, 2003). Its input is an undirected graph G ˆ W ˆ ) where Vˆ is the set of vertexes, E ˆ is (Vˆ , E, ˆ the set of edges and W is a non-negative and symmetric edge weight matrix. To apply this model ˆ from to our task, we construct the input graph G the labelings (frame assignments) contained in the ensemble. The graph vertexes Vˆ correspond to the verb instances and the (i, j)-th entry of the matrix ˆ is the number of ensemble members that assign W the same label to the i-th and j-th verb instances. For"
D14-1034,P13-1058,0,0.0130749,"Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF informatio"
D14-1034,W07-1516,0,0.0236007,"Missing"
D14-1034,P98-2184,0,0.0333799,"clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et al., 2013), Section 4). Because of the strong impact of domain variation, SCF information is best acquired automatically. Existing data-driven SCF induction systems, however, do not port well between domains. Most existing systems rely on handwritten rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or simple cooccurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010) applied to the gramm"
D14-1034,D07-1043,0,0.0244332,"Missing"
D14-1034,D12-1131,1,0.394151,"st part of the algorithm generates an excessive number of clusters, and M was then tuned so that these clusters are merged to the desired number of clusters. The λ function, used to measure the similarity between two verbs, is designed to bias the instances of the same verb type to have a higher similarity score. Algorithm 1 therefore tends to assign such instances to the same cluster. In our experiments that was always the case for this algorithm. High Cardinality Verb Sets Potentials. This set of potentials aims to bias larger sets of verb instances to share the same SCF. It is inspired by (Rush et al., 2012) who demonstrated, that syntactic structures that appear at the same syntactic context, in terms of the surrounding POS tags, tend to manifest similar syntactic behavior. While they demonstrated the usefulness of their method for dependency parsing and POS tagging, we implement it for higher level SCFs. We identified syntactic contexts that imply similar SCFs for verb instances appearing inside them. 4 All hyperparameters that require gold-standard annotation for tuning, were tuned using held-out data (Section 4). 5 The values in practice are S = 0.43 for labour legislation and S = 0.38 for en"
D14-1034,J06-2001,0,0.0270007,"am.ac.uk Abstract (2) Indirect Transitive: [They]NP [distinguished]VP [between [me and you]ADVP ]PP . (3) Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincot"
D14-1034,D11-1095,1,0.850079,"ract (2) Indirect Transitive: [They]NP [distinguished]VP [between [me and you]ADVP ]PP . (3) Ditransitive: [They]NP [distinguished]VP [him]NP [from [the other boys]NP ]PP. As SCFs describe the syntactic realization of the verbal predicate-argument structure, they are highly valuable for a variety of NLP tasks. For example, verb subcategorization information has proven useful for tasks such as parsing (Carroll and Fang, 2004; Arun and Keller, 2005; Cholakov and van Noord, 2010), semantic role labeling (Bharati et al., 2005; Moschitti and Basili, 2005), verb clustering, (Schulte im Walde, 2006; Sun and Korhonen, 2011) and machine translation (hye Han et al., 2000; Hajiˇc et al., 2002; Weller et al., 2013). SCF induction is challenging. The argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and adjuncts can appear frequently in potential argument head positions (Korhonen et al., 2000). SCFs are also highly sensitive to domain variation so that both the frames themselves and their probabilities vary depending on the meaning and behavior of predicates in the domain in question (e.g. (Roland and Jurafsky, 1998; Lippincott et al., 2010; Rimell et"
D14-1034,W10-1612,0,\N,Missing
D14-1034,P06-4020,0,\N,Missing
D16-1235,N09-1003,0,0.316379,"rb similarity due to their small size or narrow coverage of verbs. In particular, a number of word pair evaluation sets are prominent in the distributional semantics 1 In some existing evaluation sets pairs are scored for relatedness which has some overlap with similarity. SimVerb-3500 focuses on similarity as this is a more focused semantic relation that seems to yield a higher agreement between human annotators. For a broader discussion see (Hill et al., 2015). 2174 literature. Representative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively). Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena. Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015). Two datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al., 2015). These datasets, however, still contain a limited number of verb pairs (134 a"
D16-1235,P98-1013,0,0.378783,"s similar in their (morpho-)syntactic and semantic properties (e.g. BREAK verbs, sharing the VN class 45.1, and the top-level VN class 45).6 The basic overview of the VerbNet structure already suggests that measuring verb similarity is far from trivial as it revolves around a complex interplay between various semantic and syntactic properties. The wide coverage of VN in SimVerb-3500 assures the wide coverage of distinct verb groups/classes and their related linguistic phenomena. Finally, VerbNet enables further connections of SimVerb-3500 to other important lexical resources such as FrameNet (Baker et al., 1998), WordNet (Miller, 1995), and PropBank (Palmer et al., 2005) through the sets of mappings created by the SemLink project initiative (Loper et al., 2007).7 all. For each such class we sampled additional verb types until the class was represented by 3 or 4 member verbs (chosen randomly).8 Following that, we sampled at least 2 verb pairs for each previously ’under-represented’ VN class by pairing 2 member verbs from each such class. This procedure resulted in 81 additional pairs, now 3,153 in total. (Step 4) Finally, to complement this set with a sample of entirely unassociated pairs, we followed"
D16-1235,D14-1034,1,0.389034,"resentative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively). Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena. Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015). Two datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al., 2015). These datasets, however, still contain a limited number of verb pairs (134 and 222, respectively), making them unrepresentative of the rich variety of verb semantic phenomena. In this paper we provide a remedy for this problem by presenting a more comprehensive and representative verb pair evaluation resource. 3 The SimVerb-3500 Data Set Design Motivation Hill et al. (2015) argue that comprehensive highquality evaluation resources have to satisfy the following three criteria: (C1) Representative (the resource covers the full range of concepts occurring in n"
D16-1235,P14-1023,0,0.0211925,"rage of all the other raters. SimVerb-3500 obtains ρ = 0.84 (IAA-1) and ρ = 0.86 (IAA-2), a very good agreement compared to other benchmarks (see Tab. 2). Vector Space Models We compare the performance of prominent representation models on SimVerb-3500. We include: (1) unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) with various contexts (BOW = bag of words; DEPS = dependency contexts) as in Levy and Goldberg (2014), the symmetric-pattern based vectors by Schwartz et al. (2015), and count-based PMIweighted vectors (Baroni et al., 2014); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use sparse binary vectors built from linguistic resources (NonEval set IAA-1 IAA-2 A LL T EXT WS IM (203) S IM L EX (999) 0.67 0.65 0.67 0.78 0.79 SGNS-BOW 0.74 Paragram+CF 0.79 SGNS-BOW 0.56 SymPat+SGNS SL-222 (222) 0.72 - 0.73 Paragram+CF 0.58 SymPat S IM V ERB (3500) 0.84 0.86 0.63 Paragram+CF 0.36 SGNS-DEPS Table 2: An overview of word similarity evaluation benchmarks. A LL is the current best reported score on each data set across all models (including the models that exploit curated knowledge"
D16-1235,P15-2076,0,0.0948827,"SGNS-BOW 0.74 Paragram+CF 0.79 SGNS-BOW 0.56 SymPat+SGNS SL-222 (222) 0.72 - 0.73 Paragram+CF 0.58 SymPat S IM V ERB (3500) 0.84 0.86 0.63 Paragram+CF 0.36 SGNS-DEPS Table 2: An overview of word similarity evaluation benchmarks. A LL is the current best reported score on each data set across all models (including the models that exploit curated knowledge bases and hand-crafted lexical resources, see supplementary material). T EXT denotes the best reported score for a model that learns solely on the basis of distributional information. All scores are Spearman’s ρ correlations. Distributional, (Faruqui and Dyer, 2015)), and vectors fine-tuned to a paraphrase database (Paragram, (Wieting et al., 2015)) further refined using linguistic constraints (Paragram+CF, (Mrkši´c et al., 2016)). Descriptions of these models are in the supplementary material. Comparison to SimLex-999 (SL-222) 170 pairs from SL-222 also appear in SimVerb-3500. The correlation between the two data sets calculated on the shared pairs is ρ = 0.91. This proves, as expected, that the ratings are consistent across the two data sets. Tab. 3 shows a comparison of models’ performance on SimVerb-3500 against SL-222. Since the number of evaluation"
D16-1235,N15-1184,0,0.117935,"Missing"
D16-1235,W16-2506,0,0.0632045,"oying additional databases or linguistic resources. The performance of the best scoring Paragram+CF model is even on par with the IAA-1 of 0.72. The same model obtains the highest score on SV-3500 (ρ = 0.628), with a clear gap to IAA-1 of 0.84. We attribute these differences in 2178 performance largely to SimVerb-3500 being a more extensive and diverse resource in terms of verb pairs. Development Set A common problem in scored word pair datasets is the lack of a standard split to development and test sets. Previous works often optimise models on the entire dataset, which leads to overfitting (Faruqui et al., 2016) or use custom splits, e.g., 10-fold cross-validation (Schwartz et al., 2015), which make results incomparable with others. The lack of standard splits stems mostly from small size and poor coverage – issues which we have solved with SimVerb-3500. Our development set contains 500 pairs, selected to ensure a broad coverage in terms of similarity ranges (i.e., non-similar and highly similar pairs, as well as pairs of medium similarity are represented) and top-level VN classes (each class is represented by at least 1 member verb). The test set includes the remaining 3,000 verb pairs. The performa"
D16-1235,J15-4004,1,0.907142,"emantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993). Verbs play a key role at almost every level of linguistic analysis. Information related to their predicate argument structure can benefit many NLP tasks (e.g. parsing, semantic role labelling, information extraction) and applications (e.g. machine translation, One factor behind the lack of more nuanced word representation learning methods is the scarcity of satisfactory ways to evaluate or analyse representations of particular word types. Resources such as MEN (Bruni et al., 2014), Rare Words (Luong et al., 2013) and SimLex-999 (Hill et al., 2015) focus either on words from a single class or small samples of different word types, with automatic approaches already reaching or surpassing the inter-annotator agreement ceiling. Consequently, for word classes such as verbs, whose semantics is critical for language understanding, it is practically impossible to achieve statistically robust analyses and comparisons between different 2173 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173–2182, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics representation lear"
D16-1235,kipper-etal-2004-extending,0,0.102139,"Missing"
D16-1235,P14-2050,0,0.13367,"idual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. SimVerb-3500 obtains ρ = 0.84 (IAA-1) and ρ = 0.86 (IAA-2), a very good agreement compared to other benchmarks (see Tab. 2). Vector Space Models We compare the performance of prominent representation models on SimVerb-3500. We include: (1) unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) with various contexts (BOW = bag of words; DEPS = dependency contexts) as in Levy and Goldberg (2014), the symmetric-pattern based vectors by Schwartz et al. (2015), and count-based PMIweighted vectors (Baroni et al., 2014); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use sparse binary vectors built from linguistic resources (NonEval set IAA-1 IAA-2 A LL T EXT WS IM (203) S IM L EX (999) 0.67 0.65 0.67 0.78 0.79 SGNS-BOW 0.74 Paragram+CF 0.79 SGNS-BOW 0.56 SymPat+SGNS SL-222 (222) 0.72 - 0.73 Paragram+CF 0.58 SymPat S IM V ERB (3500) 0.84 0.86 0.63 Paragram+CF 0.36 SGNS-DEPS Table 2: An overview of word similarity evaluation benchmarks. A LL"
D16-1235,W13-3512,0,0.701559,"play a rich range of syntactic and semantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993). Verbs play a key role at almost every level of linguistic analysis. Information related to their predicate argument structure can benefit many NLP tasks (e.g. parsing, semantic role labelling, information extraction) and applications (e.g. machine translation, One factor behind the lack of more nuanced word representation learning methods is the scarcity of satisfactory ways to evaluate or analyse representations of particular word types. Resources such as MEN (Bruni et al., 2014), Rare Words (Luong et al., 2013) and SimLex-999 (Hill et al., 2015) focus either on words from a single class or small samples of different word types, with automatic approaches already reaching or surpassing the inter-annotator agreement ceiling. Consequently, for word classes such as verbs, whose semantics is critical for language understanding, it is practically impossible to achieve statistically robust analyses and comparisons between different 2173 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173–2182, c Austin, Texas, November 1-5, 2016. 2016 Association for Computatio"
D16-1235,N16-1018,0,0.138051,"Missing"
D16-1235,D07-1042,0,0.100538,"Missing"
D16-1235,J05-1004,0,0.62211,"to verb semantics research, we introduce SimVerb-3500 – an extensive intrinsic evaluation resource that is unprecedented in both size and coverage. SimVerb-3500 includes 827 verb types from the University of South Florida Free Association Norms (USF) (Nelson et al., 2004), and at least 3 member verbs from each of the 101 top-level VerbNet classes (Kipper et al., 2008). This coverage enables researchers to better understand the complex diversity of syntactic-semantic verb behaviours, and provides direct links to other established semantic resources such as WordNet (Miller, 1995) and PropBank (Palmer et al., 2005). Moreover, the large standardised development and test sets in SimVerb-3500 allow for principled tuning of hyperparameters, a critical aspect of achieving strong performance with the latest representation learning architectures. In § 2, we discuss previous evaluation resources targeting verb similarity. We present the new SimVerb-3500 data set along with our design choices and the pair selection process in § 3, while the annotation process is detailed in § 4. In § 5 we report the performance of a diverse range of popular representation learning architectures, together with benchmark performan"
D16-1235,D14-1162,0,0.0789657,"Missing"
D16-1235,D10-1114,0,0.0113012,"g models. One clear conclusion is that distributional models trained on raw text (e.g. SGNS) perform very poorly on low frequency and highly polysemous verbs. This degradation in performance can be partially mitigated by focusing models on more principled distributional contexts, such as those defined by symmetric patterns. More generally, the finding suggests that, in order to model the diverse spectrum of verb semantics, we may require algorithms that are better suited to fast learning from few examples (Lake et al., 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney, 2010b; Vilnis and McCallum, 2015). In future work we aim to apply such methods to the task of verb acquisition. Beyond the preliminary conclusions from these initial analyses, the benefit of SimLex-3500 will become clear as researchers use it to probe the relationship between architectures, algorithms and representation quality for a wide range of verb classes. Better understanding of how to represent the full diversity of verbs should in turn yield improved methods for encoding and interpreting the facts, propositions, relations and events that constitute much of the important information in lang"
D16-1235,N10-1013,0,0.0390423,"g models. One clear conclusion is that distributional models trained on raw text (e.g. SGNS) perform very poorly on low frequency and highly polysemous verbs. This degradation in performance can be partially mitigated by focusing models on more principled distributional contexts, such as those defined by symmetric patterns. More generally, the finding suggests that, in order to model the diverse spectrum of verb semantics, we may require algorithms that are better suited to fast learning from few examples (Lake et al., 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney, 2010b; Vilnis and McCallum, 2015). In future work we aim to apply such methods to the task of verb acquisition. Beyond the preliminary conclusions from these initial analyses, the benefit of SimLex-3500 will become clear as researchers use it to probe the relationship between architectures, algorithms and representation quality for a wide range of verb classes. Better understanding of how to represent the full diversity of verbs should in turn yield improved methods for encoding and interpreting the facts, propositions, relations and events that constitute much of the important information in lang"
D16-1235,K15-1026,1,0.904651,"v et al., 2013; Pennington et al., 2014; Faruqui et al., 2015). These representations (or embeddings) typically contain powerful features that are applicable to many language applications (Collobert and Weston, 2008; Turian et al., 2010). Nevertheless, the predominant approaches to distributed representation learning apply a single learning algorithm and representational form for all words in a vocabulary. This is despite evidence that applying different learning algorithms to word types such as nouns, adjectives and verbs can significantly increase the ultimate usefulness of representations (Schwartz et al., 2015). Introduction Verbs are famously both complex and variable. They express the semantics of an event as well the relational information among participants in that event, and they display a rich range of syntactic and semantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993). Verbs play a key role at almost every level of linguistic analysis. Information related to their predicate argument structure can benefit many NLP tasks (e.g. parsing, semantic role labelling, information extraction) and applications (e.g. machine translation, One factor behind the lack of more nuanced word represent"
D16-1235,P14-1068,0,0.0149921,"are scored for relatedness which has some overlap with similarity. SimVerb-3500 focuses on similarity as this is a more focused semantic relation that seems to yield a higher agreement between human annotators. For a broader discussion see (Hill et al., 2015). 2174 literature. Representative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively). Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena. Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015). Two datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al., 2015). These datasets, however, still contain a limited number of verb pairs (134 and 222, respectively), making them unrepresentative of the rich variety of verb semantic phenomena. In this paper we provide a remedy for this problem by presenting a more comprehensive and representative verb pa"
D16-1235,P10-1040,0,0.0211468,"methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning. 1 Numerous algorithms for acquiring word representations from text and/or more structured knowledge bases have been developed in recent years (Mikolov et al., 2013; Pennington et al., 2014; Faruqui et al., 2015). These representations (or embeddings) typically contain powerful features that are applicable to many language applications (Collobert and Weston, 2008; Turian et al., 2010). Nevertheless, the predominant approaches to distributed representation learning apply a single learning algorithm and representational form for all words in a vocabulary. This is despite evidence that applying different learning algorithms to word types such as nouns, adjectives and verbs can significantly increase the ultimate usefulness of representations (Schwartz et al., 2015). Introduction Verbs are famously both complex and variable. They express the semantics of an event as well the relational information among participants in that event, and they display a rich range of syntactic and"
D16-1235,Q15-1025,0,0.0741812,"gram+CF 0.58 SymPat S IM V ERB (3500) 0.84 0.86 0.63 Paragram+CF 0.36 SGNS-DEPS Table 2: An overview of word similarity evaluation benchmarks. A LL is the current best reported score on each data set across all models (including the models that exploit curated knowledge bases and hand-crafted lexical resources, see supplementary material). T EXT denotes the best reported score for a model that learns solely on the basis of distributional information. All scores are Spearman’s ρ correlations. Distributional, (Faruqui and Dyer, 2015)), and vectors fine-tuned to a paraphrase database (Paragram, (Wieting et al., 2015)) further refined using linguistic constraints (Paragram+CF, (Mrkši´c et al., 2016)). Descriptions of these models are in the supplementary material. Comparison to SimLex-999 (SL-222) 170 pairs from SL-222 also appear in SimVerb-3500. The correlation between the two data sets calculated on the shared pairs is ρ = 0.91. This proves, as expected, that the ratings are consistent across the two data sets. Tab. 3 shows a comparison of models’ performance on SimVerb-3500 against SL-222. Since the number of evaluation pairs may influence the results, we ideally want to compare sets of equal size for"
D16-1235,C10-1011,0,\N,Missing
D16-1235,N03-1033,0,\N,Missing
D16-1235,W08-1301,0,\N,Missing
D16-1235,C98-1013,0,\N,Missing
D16-1235,P06-1038,0,\N,Missing
D16-1235,petrov-etal-2012-universal,0,\N,Missing
D16-1235,N13-1092,0,\N,Missing
D16-1235,P15-2070,0,\N,Missing
D16-1235,N16-1060,1,\N,Missing
D16-1235,P16-2084,1,\N,Missing
D16-1235,C12-1059,0,\N,Missing
D16-1235,P13-2109,0,\N,Missing
D16-1239,P16-1231,0,0.0433984,"Missing"
D16-1239,P16-1070,1,0.828628,"notating challenging syntactic structures arising from grammatical errors. The students also annotated individually six 3 The annotation bias and quality results reported in sections 3 and 4 use the original learner sentences, which contain grammatical errors. These results were replicated on the error corrected versions of the sentences. practice batches of 20-30 sentences from the English Web Treebank (EWT) (Silveira et al., 2014) and FCE corpora, and resolved annotation disagreements during group meetings. Following the training period, the students annotated a treebank of learner English (Berzak et al., 2016) over a period of five months, three of which as a full time job. During this time, the students continued attending weekly meetings in which further annotation challenges were discussed and resolved. The annotation was carried out for sentences from the FCE dataset, where both the original and error corrected versions of each sentence were annotated and reviewed. In the course of the annotation project, each annotator completed approximately 800 sentence annotations, and a similar number of sentence reviews. The annotations and reviews were done in the same format used in this study. With res"
D16-1239,de-marneffe-etal-2006-generating,0,0.143689,"Missing"
D16-1239,de-marneffe-etal-2014-universal,0,0.0631645,"Missing"
D16-1239,W10-1807,0,0.064206,"Missing"
D16-1239,W13-3711,0,0.130455,"Missing"
D16-1239,P14-1130,0,0.0526912,"Missing"
D16-1239,J93-2004,0,0.112779,"ion. At the same time, human agreement on these annotations provides an indicator for the difficulty of the task, and can be instrumental for estimating upper limits for the performance obtainable by computational methods. Linguistic gold standards are often constructed using pre-existing annotations, generated by automatic tools. The output of such tools is then manually corrected by human annotators to produce the gold standard. The justification for this annotation methodology was first introduced in a set of experiments on POS tag annotation conducted as part of the Penn Treebank project (Marcus et al., 1993). In this study, the authors concluded that tagger-based annotations are not only much faster to obtain, but also more consistent and of higher quality compared to annotations from scratch. Following the Penn Treebank, syntactic annotation projects for various languages, including German (Brants et al., 2002), French (Abeill´e et al., 2003), Arabic (Maamouri et al., 2004) and many others, were annotated using automatic tools as a starting point. Despite the widespread use of this annotation pipeline, there is, to our knowledge, little prior work on syntactic annotation quality and on the relia"
D16-1239,P13-2109,0,0.0327121,"ng annotation and review tasks. The fifth participant is left out to perform the quality ranking task described in section 4. The first participant annotates the sentence from scratch, and a second participant reviews this an2218 a quality assessment task described in section 4, which requires to rank the three gold standards in cases of disagreement. notation. The overall agreement of the reviewers with the annotators is 98.24 POS, 97.16 UAS, 96.3 LA and 94.81 LAS. The next two participants review parser outputs. One participant reviews an annotation generated by the Turbo tagger and parser (Martins et al., 2013). The other participant reviews the output of the Stanford tagger (Toutanova et al., 2003) and RBG parser (Lei et al., 2014). The taggers and parsers were trained on the gold annotations of the EWT UD treebank, version 1.1. Both parsers use predicted POS tags for the FCE sentences. Assigning the reviews to the human annotations yields a human based gold standard for each sentence called “Human Gold”. Assigning the reviews to the tagger and parser outputs yields two parserbased gold standards, “Turbo Gold” and “RBG Gold”. We chose the Turbo-Turbo and StanfordRBG tagger-parser pairs as these too"
D16-1239,P14-2083,0,0.0227559,"newswire. Higher agreement rates could in principle be obtained through further annotator training, refinement and revision of annotation guidelines, as well as additional automatic validation tests for the annotations. Nonetheless, we believe that our estimates reliably reflect a realistic scenario of expert syntactic annotation. The obtained agreement rates call for a more extensive examination of annotator disagreements on parsing and tagging. Recent work in this area has already proposed an analysis of expert annotator disagreements for POS tagging in the absence of annotation guidelines (Plank et al., 2014). Our annotations will enable conducting such studies for annotation with guidelines, and support extending this line of investigation to annotations of syntactic dependencies. As a first step towards this goal, we plan to carry out an in-depth analysis of disagreement in the collected data, characterize the main sources of inconsistent annotation and subsequently formulate further strategies for improving annotation accuracy. We believe that better understanding of human disagreements and their relation to disagreements between humans and parsers will also contribute to advancing evaluation m"
D16-1239,rambow-etal-2002-dependency,0,0.110505,"Missing"
D16-1239,P11-1067,0,0.0220179,"ation to annotations of syntactic dependencies. As a first step towards this goal, we plan to carry out an in-depth analysis of disagreement in the collected data, characterize the main sources of inconsistent annotation and subsequently formulate further strategies for improving annotation accuracy. We believe that better understanding of human disagreements and their relation to disagreements between humans and parsers will also contribute to advancing evaluation methodologies for POS tagging and syntactic parsing in NLP, an important topic that has received only limited attention thus far (Schwartz et al., 2011; Plank et al., 2015). Finally, since the release of the Penn Treebank in 1992, it has been serving as the standard benchmark for English parsing evaluation. Over the past few years, improvements in parsing performance on this dataset were obtained in small increments, and are commonly reported without a linguistic analysis of the improved predictions. As dependency parsing performance on English newswire may be reaching 2223 human expert agreement, not only new evaluation practices, but also more attention to noisier domains and other languages may be in place. Acknowledgments We thank our te"
D16-1239,silveira-etal-2014-gold,0,0.0447238,"Missing"
D16-1239,W13-2304,0,0.0431071,"Missing"
D16-1239,N03-1033,0,0.109567,"ranking task described in section 4. The first participant annotates the sentence from scratch, and a second participant reviews this an2218 a quality assessment task described in section 4, which requires to rank the three gold standards in cases of disagreement. notation. The overall agreement of the reviewers with the annotators is 98.24 POS, 97.16 UAS, 96.3 LA and 94.81 LAS. The next two participants review parser outputs. One participant reviews an annotation generated by the Turbo tagger and parser (Martins et al., 2013). The other participant reviews the output of the Stanford tagger (Toutanova et al., 2003) and RBG parser (Lei et al., 2014). The taggers and parsers were trained on the gold annotations of the EWT UD treebank, version 1.1. Both parsers use predicted POS tags for the FCE sentences. Assigning the reviews to the human annotations yields a human based gold standard for each sentence called “Human Gold”. Assigning the reviews to the tagger and parser outputs yields two parserbased gold standards, “Turbo Gold” and “RBG Gold”. We chose the Turbo-Turbo and StanfordRBG tagger-parser pairs as these tools obtain comparable performance on standard evaluation benchHuman Gold Turbo Gold Error R"
D16-1239,P15-1032,0,0.022944,"Missing"
D16-1239,P11-1019,0,0.0376631,"d metrics Unlabeled Attachment Score (UAS) and Label Accuracy (LA) to measure accuracy of head attachment and dependency labels. We also utilize the standard parsing metric Labeled Attachment Score (LAS), which takes into account both dependency arcs and dependency labels. In all our parsing and agreement experiments, we exclude punctuation tokens from the evaluation. 2.4 Corpora We use sentences from two publicly available datasets, covering two different genres. The first corpus, used in the experiments in sections 3 and 4, is the First Certificate in English (FCE) Cambridge Learner Corpus (Yannakoudakis et al., 2011). This dataset contains essays authored by upperintermediate level English learners3 . The second corpus is the WSJ part of the Penn Treebank (WSJ PTB) (Marcus et al., 1993). Since its release, this dataset has been the most commonly used resource for training and evaluation of English parsers. Our experiment on inter-annotator agreement in section 5 uses a random subset of the sentences in section 23 of the WSJ PTB, which is traditionally reserved for tagging and parsing evaluation. 2.5 Annotators We recruited five students at MIT as annotators. Three of the students are linguistics majors an"
D17-1270,E17-1088,0,0.0338438,"into cross-lingual BABEL synsets (and is currently available for 271 languages). The wide and steadily growing coverage of languages in BabelNet means that our proposed framework promises to support the transfer of VerbNet-style information to numerous target languages (with increasingly high accuracy). To establish that the proposed transfer approach is in fact independent of the chosen cross-lingual information source, we also experiment with another cross-lingual dictionary: PanLex (Kamholz et al., 2014), which was used in prior work on crosslingual word vector spaces (Duong et al., 2016; Adams et al., 2017). This dictionary currently covers around 1,300 language varieties with over 12 million expressions, thus offering support also for low-resource transfer settings.7 VerbNet constraints are extracted from the English VerbNet class structure in a straightforward manner. For each class V N i from the 273 VerbNet classes, we simply take the set of all ni verbs CLi = {v1,i , v2,i , . . . , vni ,i } associated with that class, including its subclasses, and generate all unique pairs (vk , vl ) so that vk , vl ∈ CLi and vk 6= vl . Example VerbNet pairwise constraints are shown in Tab. 1. Note that Ver"
D17-1270,W13-3520,0,0.0596742,"language: training/test data and constraints. Coverage refers to the percentage of test verbs represented in the target language vocabularies. vectors on ptWaC (Wagner Filho et al., 2016), FI vectors on fiWaC (Ljubeši´c et al., 2016), and PL vectors on the Araneum Polonicum Maius Web corpus (Benko, 2014). Note that we do not utilise any VerbNet-specific knowledge in the target language to induce and further specialise these word vectors. Source EN vectors were taken directly from the work of Levy and Goldberg (2014): they are trained with SGNS on the cleaned and tokenised Polyglot Wikipedia (Al-Rfou et al., 2013) containing ∼75M sentences, ∼1.7B word tokens and a vocabulary of ∼180k words after lowercasing and frequency cut-off. To measure the importance of the starting source language space as well as to test if syntactic knowledge on the source side may be propagated to the target space, we test two variant EN vector spaces: SGNS with (a) BOW contexts and the window size 2 (SGNS-BOW2); and (b) dependencybased contexts (SGNS-DEPS) (Padó and Lapata, 2007; Levy and Goldberg, 2014). Linguistic Constraints We experiment with the following constraint types: (a) monolingual synonymy constraints in each tar"
D17-1270,aparicio-etal-2008-ancora,0,0.173239,"f generalisation for many NLP tasks, VerbNet has been used to support semantic role labelling (Swier and Stevenson, 2004; Giuglea and Moschitti, 2006), semantic parsing (Shi and Mihalcea, 2005), word sense disambiguation (Brown et al., 2011), discourse parsing (Subba and Di Eugenio, 2009), information extraction (Mausam et al., 2012), text mining applications (Lippincott et al., 2013; Rimell et al., 2013), research into human language acquisition (Korhonen, 2010), and other tasks. This benefit for English NLP has motivated the development of VerbNets for languages such as Spanish and Catalan (Aparicio et al., 2008), Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). However, end-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which aim to learn verb classes automatically offer an attractive alternative. However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012, i.a.), ranging from accu"
D17-1270,P98-1013,0,0.896649,"verse languages with high accuracy. The practical usefulness of VerbNet style classification both within and across languages has been limited by the fact that few languages boast resources similar to the English VerbNet. Some VerbNets have been developed completely manually from scratch, aiming to capture properties specific to the language in question, e.g., the resources for Spanish and Catalan (Aparicio et al., 2008), 3 The usefulness of VerbNet is further accentuated by available mappings (Loper et al., 2007) to a number of other verb resources such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998), and PropBank (Palmer et al., 2005). 4 For example, Levin (1993) notes that verbs in Warlpiri manifest analogous behavior to English with respect to the conative alternation. In another example, Polish verbs have the same patterns as EN verbs in terms of the middle construction. 2548 Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). Other VerbNets were created semi-automatically, with the help of other lexical resources, e.g., for French (Pradet et al., 2014) and Brazilian Portuguese (Scarton and Aluısio, 2012). These approaches involved substantial amounts of specialised lin"
D17-1270,D14-1034,1,0.854054,"rs: SGNS-BOW2. lingual information again leads to large gains with the cross-lingual transfer model, as is evident from Fig. 3. This suggests that the proposed approach does not depend on a particular source of information - it can be used with any general-purpose bilingual dictionary. We mark slight improvements for 3/6 target languages when comparing the results with the ones from Fig. 2a. The new state-of-the-art F-1 scores are 0.79 for FR and 0.74 for PT. Verb Classification vs. Semantic Similarity An interesting question originating from prior work on verb representation learning, e.g., (Baker et al., 2014) touches upon the correlation between verb classification and semantic similarity. Due to the availability of VerbNet constraints and a recent similarity evaluation set (SimVerb-3500; it contains human similarity ratings for 3,500 verb pairs) (Gerz et al., 2016), we perform the analysis on English: the results are summarised in Tab. 4. They clearly indicate that cross-lingual synonymy constraints are useful for both relationship types (compare the scores with XLing), with strong gains over the nonspecialised distributional space. However, the inclusion of VerbNet information, while boosting cl"
D17-1270,P14-1023,0,0.0382331,"ssing, pages 2546–2558 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics even typologically diverse languages. Based on this finding, we propose an automatic approach which exploits readily available annotations for English to facilitate efficient, large-scale development of VerbNets for a wide set of target languages. Recently, unsupervised methods for inducing distributed word vector space representations or word embeddings (Mikolov et al., 2013a) have been successfully applied to a plethora of NLP tasks (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014, i.a.). These methods offer an elegant way to learn directly from large corpora, bypassing the feature engineering step and the dependence on mature NLP pipelines (e.g., POS taggers, parsers, extraction of subcategorisation frames). In this work, we demonstrate how these models can be used to support automatic verb class induction. Moreover, we show that these models offer the means to exploit inherent cross-lingual links in VerbNet-style classification in order to guide the development of new classifications for resource-lean languages. To the best of our knowledge, this proposition has not"
D17-1270,D16-1136,0,0.0430305,"e which groups words into cross-lingual BABEL synsets (and is currently available for 271 languages). The wide and steadily growing coverage of languages in BabelNet means that our proposed framework promises to support the transfer of VerbNet-style information to numerous target languages (with increasingly high accuracy). To establish that the proposed transfer approach is in fact independent of the chosen cross-lingual information source, we also experiment with another cross-lingual dictionary: PanLex (Kamholz et al., 2014), which was used in prior work on crosslingual word vector spaces (Duong et al., 2016; Adams et al., 2017). This dictionary currently covers around 1,300 language varieties with over 12 million expressions, thus offering support also for low-resource transfer settings.7 VerbNet constraints are extracted from the English VerbNet class structure in a straightforward manner. For each class V N i from the 273 VerbNet classes, we simply take the set of all ni verbs CLi = {v1,i , v2,i , . . . , vni ,i } associated with that class, including its subclasses, and generate all unique pairs (vk , vl ) so that vk , vl ∈ CLi and vk 6= vl . Example VerbNet pairwise constraints are shown in"
D17-1270,ehrmann-etal-2014-representing,0,0.0341319,"have a similar effect on verb classification, which relies on the similarity in syntactic-semantic properties of verbs within a class. In summary, we explore three important questions in this paper: (Q1) Given their fundamental dependence on the distributional hypothesis, to what extent can unsupervised methods for inducing vector spaces facilitate the automatic induction of VerbNet-style verb classes across different languages? (Q2) Can one boost verb classification for lowerresource languages by exploiting general-purpose cross-lingual resources such as BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014) or bilingual dictionaries such as PanLex (Kamholz et al., 2014) to construct better word vector spaces for these languages? (Q3) Based on the stipulated cross-linguistic validity of VerbNet-style classification, can one exploit rich sets of readily available annotations in one language (e.g., the full English VerbNet) to automatically bootstrap the creation of VerbNets for other languages? In other words, is it possible to exploit a cross-lingual vector space to transfer VerbNet knowledge from a resource-rich to a resource-lean language? To investigate Q1, we induce standard distributional ve"
D17-1270,P12-1090,0,0.597422,"Spanish and Catalan (Aparicio et al., 2008), Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). However, end-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which aim to learn verb classes automatically offer an attractive alternative. However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012, i.a.), ranging from accurate parsers to pre-compiled subcategorisation frames (Schulte im Walde, 2006; Li and Brew, 2008; Messiant, 2008). Such methods are limited to a small set of resource-rich languages. It has been argued that VerbNet-style classification has a strong cross-lingual element (Jackendoff, 1992; Levin, 1993). In support of this argument, Majewska et al. (2017) have shown that English VerbNet has high translatability across different, 2546 1 http://verbs.colorado.edu/∼mpalmer/projects/verbnet.html Proceedings of the 2017 Conference on Empirical Methods in Natural Language Pro"
D17-1270,N15-1184,0,0.157158,"Missing"
D17-1270,D16-1235,1,0.899174,"Missing"
D17-1270,P06-1117,0,0.0515276,"f, 1972; Gruber, 1976; Levin, 1993, inter alia). Lexical resources which capture the variability of verbs are instrumental for many Natural Language Processing (NLP) applications. One of the richest verb resources currently available for English is VerbNet (Kipper et al., 2000; Kipper, 2005).1 Based on the work of Levin (1993), this largely hand-crafted taxonomy organises verbs into classes on the basis of their shared syntacticsemantic behaviour. Providing a useful level of generalisation for many NLP tasks, VerbNet has been used to support semantic role labelling (Swier and Stevenson, 2004; Giuglea and Moschitti, 2006), semantic parsing (Shi and Mihalcea, 2005), word sense disambiguation (Brown et al., 2011), discourse parsing (Subba and Di Eugenio, 2009), information extraction (Mausam et al., 2012), text mining applications (Lippincott et al., 2013; Rimell et al., 2013), research into human language acquisition (Korhonen, 2010), and other tasks. This benefit for English NLP has motivated the development of VerbNets for languages such as Spanish and Catalan (Aparicio et al., 2008), Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). However, end-to-end manual resource development using Levin"
D17-1270,J15-4004,1,0.774578,"ffer the means to exploit inherent cross-lingual links in VerbNet-style classification in order to guide the development of new classifications for resource-lean languages. To the best of our knowledge, this proposition has not been investigated in previous work. There has been little work on assessing the suitability of embeddings for capturing rich syntacticsemantic phenomena. One challenge is their reliance on the distributional hypothesis (Harris, 1954), which coalesces fine-grained syntacticsemantic relations between words into a broad relation of semantic relatedness (e.g., coffee:cup) (Hill et al., 2015; Kiela et al., 2015). This property has an adverse effect when word embeddings are used in downstream tasks such as spoken language understanding (Kim et al., 2016a,b) or dialogue state tracking (Mrkši´c et al., 2016, 2017a). It could have a similar effect on verb classification, which relies on the similarity in syntactic-semantic properties of verbs within a class. In summary, we explore three important questions in this paper: (Q1) Given their fundamental dependence on the distributional hypothesis, to what extent can unsupervised methods for inducing vector spaces facilitate the automatic"
D17-1270,W02-1016,0,0.184628,"Missing"
D17-1270,W11-0110,0,0.0374549,"verbs are instrumental for many Natural Language Processing (NLP) applications. One of the richest verb resources currently available for English is VerbNet (Kipper et al., 2000; Kipper, 2005).1 Based on the work of Levin (1993), this largely hand-crafted taxonomy organises verbs into classes on the basis of their shared syntacticsemantic behaviour. Providing a useful level of generalisation for many NLP tasks, VerbNet has been used to support semantic role labelling (Swier and Stevenson, 2004; Giuglea and Moschitti, 2006), semantic parsing (Shi and Mihalcea, 2005), word sense disambiguation (Brown et al., 2011), discourse parsing (Subba and Di Eugenio, 2009), information extraction (Mausam et al., 2012), text mining applications (Lippincott et al., 2013; Rimell et al., 2013), research into human language acquisition (Korhonen, 2010), and other tasks. This benefit for English NLP has motivated the development of VerbNets for languages such as Spanish and Catalan (Aparicio et al., 2008), Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). However, end-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English"
D17-1270,P06-1017,0,0.0515279,"understand) (accept, reject) (repent, rue) (reject, discourage) (encourage, discourage) (reject, discourage) (disprefer, understand) Given the initial distributional or specialised collection of target language vectors Vt , we apply an offthe-shelf clustering algorithm on top of these vectors in order to group verbs into classes. Following prior work (Brew and Schulte im Walde, 2002; Sun and Korhonen, 2009; Sun et al., 2010), we employ the MNCut spectral clustering algorithm (Meila and Shi, 2001), which has wide applicability in similar NLP tasks which involve high-dimensional feature spaces (Chen et al., 2006; von Luxburg, 2007; Scarton et al., 2014, i.a.). Again, following prior work (Sun et al., 2010, 2013), we estimate the number of clusters KClust using the self-tuning method of Zelnik-Manor and Perona (2004). This algorithm finds the optimal number by minimising a cost function based on the eigenvector structure of the word similarity matrix. We refer the reader to the relevant literature for further details. Table 1: Example pairwise ATTRACT constraints extracted from three VerbNet classes in English. a) cross-lingual (translation) links between languages, and b) available VerbNet annotation"
D17-1270,kamholz-etal-2014-panlex,0,0.0919158,"e similarity in syntactic-semantic properties of verbs within a class. In summary, we explore three important questions in this paper: (Q1) Given their fundamental dependence on the distributional hypothesis, to what extent can unsupervised methods for inducing vector spaces facilitate the automatic induction of VerbNet-style verb classes across different languages? (Q2) Can one boost verb classification for lowerresource languages by exploiting general-purpose cross-lingual resources such as BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014) or bilingual dictionaries such as PanLex (Kamholz et al., 2014) to construct better word vector spaces for these languages? (Q3) Based on the stipulated cross-linguistic validity of VerbNet-style classification, can one exploit rich sets of readily available annotations in one language (e.g., the full English VerbNet) to automatically bootstrap the creation of VerbNets for other languages? In other words, is it possible to exploit a cross-lingual vector space to transfer VerbNet knowledge from a resource-rich to a resource-lean language? To investigate Q1, we induce standard distributional vector spaces (Mikolov et al., 2013b; Levy and Goldberg, 2014) fro"
D17-1270,P14-1097,0,0.0547583,"ferring lexical resources from resource-rich to resourcepoor languages using general-purpose cross-lingual dictionaries and bilingual vector spaces as means of transfer within a semantic specialisation framework. However, we believe that the proposed basic framework may be upgraded and extended across several research paths in future work. First, in the current work we have operated with standard single-sense/single-prototype representations, thus effectively disregarding the problem of verb polysemy. While several polysemy-aware verb classification models for English were developed recently (Kawahara et al., 2014; Peterson et al., 2016), the current lack of polysemyaware evaluation sets in other languages impedes this line of research. Evaluation issues aside, one idea for future work is to use the ATTRACT-R EPEL specialisation framework for sense-aware crosslingual transfer relying on recently developed multisense/prototype word representations (Neelakantan et al., 2014; Pilehvar and Collier, 2016, inter alia). Another challenge is to apply the idea from this work to enable cross-lingual transfer of other structured lexical resources available in English such as FrameNet (Baker et al., 1998), PropBan"
D17-1270,D15-1242,0,0.299366,"Missing"
D17-1270,W16-1607,0,0.152578,"Missing"
D17-1270,D12-1048,0,0.0181924,"chest verb resources currently available for English is VerbNet (Kipper et al., 2000; Kipper, 2005).1 Based on the work of Levin (1993), this largely hand-crafted taxonomy organises verbs into classes on the basis of their shared syntacticsemantic behaviour. Providing a useful level of generalisation for many NLP tasks, VerbNet has been used to support semantic role labelling (Swier and Stevenson, 2004; Giuglea and Moschitti, 2006), semantic parsing (Shi and Mihalcea, 2005), word sense disambiguation (Brown et al., 2011), discourse parsing (Subba and Di Eugenio, 2009), information extraction (Mausam et al., 2012), text mining applications (Lippincott et al., 2013; Rimell et al., 2013), research into human language acquisition (Korhonen, 2010), and other tasks. This benefit for English NLP has motivated the development of VerbNets for languages such as Spanish and Catalan (Aparicio et al., 2008), Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). However, end-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which"
D17-1270,kipper-etal-2006-extending,1,0.70061,"ss V N i from the 273 VerbNet classes, we simply take the set of all ni verbs CLi = {v1,i , v2,i , . . . , vni ,i } associated with that class, including its subclasses, and generate all unique pairs (vk , vl ) so that vk , vl ∈ CLi and vk 6= vl . Example VerbNet pairwise constraints are shown in Tab. 1. Note that VerbNet classes in practice contain verb instances standing in a variety of lexical relations, including synonyms, antonyms, troponyms, hypernyms, and the class membership is determined on the basis of connections between the syntactic patterns and the underlying semantic relations (Kipper et al., 2006, 2008). 7 Similar to BabelNet, the translations in PanLex were derived from various sources such as glossaries, dictionaries, and automatic inference from other languages. This results in a high-coverage lexicon containing a certain amount of noise. 3 Clustering Algorithm Experimental Setup Languages We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI). All statistics regarding the source and size of training and test data, and linguistic constraints for each target language are summarised in Tab. 2. Automa"
D17-1270,P14-2050,0,0.473593,"anLex (Kamholz et al., 2014) to construct better word vector spaces for these languages? (Q3) Based on the stipulated cross-linguistic validity of VerbNet-style classification, can one exploit rich sets of readily available annotations in one language (e.g., the full English VerbNet) to automatically bootstrap the creation of VerbNets for other languages? In other words, is it possible to exploit a cross-lingual vector space to transfer VerbNet knowledge from a resource-rich to a resource-lean language? To investigate Q1, we induce standard distributional vector spaces (Mikolov et al., 2013b; Levy and Goldberg, 2014) from large monolingual corpora in English and six target languages. As expected, the results obtained with this straightforward approach show positive trends, but at the same time reveal its limitations for all the languages involved. Therefore, the focus of our work shifts to Q2 and Q3. The problem of inducing VerbNetoriented embeddings is framed as vector space specialisation using the available external resources: BabelNet or PanLex, and (English) VerbNet. Formalised as an instance of post-processing semantic specialisation approaches (Faruqui et al., 2015; Mrkši´c et al., 2016), our proce"
D17-1270,P08-1050,0,0.0262482,"nd-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which aim to learn verb classes automatically offer an attractive alternative. However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012, i.a.), ranging from accurate parsers to pre-compiled subcategorisation frames (Schulte im Walde, 2006; Li and Brew, 2008; Messiant, 2008). Such methods are limited to a small set of resource-rich languages. It has been argued that VerbNet-style classification has a strong cross-lingual element (Jackendoff, 1992; Levin, 1993). In support of this argument, Majewska et al. (2017) have shown that English VerbNet has high translatability across different, 2546 1 http://verbs.colorado.edu/∼mpalmer/projects/verbnet.html Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2546–2558 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics even"
D17-1270,W14-0405,0,0.0376627,"Missing"
D17-1270,K16-1006,0,0.0361737,"ype word representations (Neelakantan et al., 2014; Pilehvar and Collier, 2016, inter alia). Another challenge is to apply the idea from this work to enable cross-lingual transfer of other structured lexical resources available in English such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbKB (Wijaya and Mitchell, 2016). Other potential research avenues include porting the approach to other typologically diverse languages and truly low-resource settings (e.g., with only limited amounts of parallel data), as well as experiments with other distributional spaces, e.g. (Melamud et al., 2016). Further refinements of the specialisation and clustering algorithms may also result in improved verb class induction. 5 Conclusion We have presented a novel cross-lingual transfer model which enables the automatic induction of VerbNet-style verb classifications across multiple languages. The transfer is based on a word vector space specialisation framework, utilised to directly model the assumption of cross-linguistic validity of VerbNet-style classifications. Our results indicate strong improvements in verb classification accuracy across all six target languages explored. All automatically"
D17-1270,P02-1027,0,0.0579786,"rate that a constraints-driven fine-tuning framework can specialise word embeddings to reflect VerbNet-style relations which rely not only on verb sense similarity, but also on similarity in syntax, selectional preferences, and diathesis alternations. arguments.3 The current version of VerbNet (v3.2) contains 8,537 distinct English verbs grouped into 273 VerbNet main classes. The inter-relatedness of syntactic behaviour and meaning of verbs is not limited to English (Levin, 1993). The basic meaning components underlying verb classes are said to be cross-linguistically valid (Jackendoff, 1992; Merlo et al., 2002)4 and therefore the classification has a strong cross-lingual dimension. A recent investigation of Majewska et al. (2017) show that it is possible to manually translate VerbNet classes and class members to different, typologically diverse languages with high accuracy. The practical usefulness of VerbNet style classification both within and across languages has been limited by the fact that few languages boast resources similar to the English VerbNet. Some VerbNets have been developed completely manually from scratch, aiming to capture properties specific to the language in question, e.g., the"
D17-1270,P08-3010,0,0.032561,"source development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which aim to learn verb classes automatically offer an attractive alternative. However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012, i.a.), ranging from accurate parsers to pre-compiled subcategorisation frames (Schulte im Walde, 2006; Li and Brew, 2008; Messiant, 2008). Such methods are limited to a small set of resource-rich languages. It has been argued that VerbNet-style classification has a strong cross-lingual element (Jackendoff, 1992; Levin, 1993). In support of this argument, Majewska et al. (2017) have shown that English VerbNet has high translatability across different, 2546 1 http://verbs.colorado.edu/∼mpalmer/projects/verbnet.html Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2546–2558 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics even typologically div"
D17-1270,N16-1018,1,0.903553,"Missing"
D17-1270,P17-1163,1,0.690358,"Missing"
D17-1270,Q17-1022,1,0.879777,"Missing"
D17-1270,N16-1060,0,0.0177637,"ised in Tab. 2. Automatic approaches to verb class induction have been tried out in prior work for FR and PT. To the best of our knowledge, our cross-lingual study is the first aiming to generalise an automatic induction method to more languages using an underlying methodology which is language-pair independent. Initial Vector Space: Training Data and Setup All target language vectors were trained on large monolingual running text using the same setup: 300-dimensional word vectors, the frequency cutoff set to 100, bag-of-words (BOW) contexts, and the window size of 2 (Levy and Goldberg, 2014; Schwartz et al., 2016). All tokens were lowercased, and all numbers were converted to a placeholder symbol &lt;NUM&gt;.8 FR and IT word vectors were trained on the standard frWaC and itWaC corpora (Baroni et al., 2009), and vectors for other target languages were trained on the corpora of similar style and size: HR vectors were trained on the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), PT 8 Other SGNS parameters were also set to standard values (Baroni et al., 2014; Vuli´c and Korhonen, 2016): 15 epochs, 15 negative samples, global learning rate: .025, subsampling rate: 1e − 4. Similar trends in results persist with d"
D17-1270,D14-1113,0,0.0300192,"t work we have operated with standard single-sense/single-prototype representations, thus effectively disregarding the problem of verb polysemy. While several polysemy-aware verb classification models for English were developed recently (Kawahara et al., 2014; Peterson et al., 2016), the current lack of polysemyaware evaluation sets in other languages impedes this line of research. Evaluation issues aside, one idea for future work is to use the ATTRACT-R EPEL specialisation framework for sense-aware crosslingual transfer relying on recently developed multisense/prototype word representations (Neelakantan et al., 2014; Pilehvar and Collier, 2016, inter alia). Another challenge is to apply the idea from this work to enable cross-lingual transfer of other structured lexical resources available in English such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbKB (Wijaya and Mitchell, 2016). Other potential research avenues include porting the approach to other typologically diverse languages and truly low-resource settings (e.g., with only limited amounts of parallel data), as well as experiments with other distributional spaces, e.g. (Melamud et al., 2016). Further refinements of the"
D17-1270,C08-1082,0,0.0356657,"lassification approach requires an initial gold standard (Sun et al., 2010): these have been developed for FR (Sun et al., 2010), PT (Scarton et al., 2014), IT, PL, HR, and FI (Majewska et al., 2017). They were created using the methodology of Sun et al. (2010), based on the EN gold standard of Sun et al. (2008) which contains 17 fine-grained Levin classes with 12 member verbs each. For instance, the class PUT-9.1 in French contains verbs such as accrocher, déposer, mettre, répartir, réintégrer, etc. Evaluation Measures We use standard evaluation measures from prior work on verb clustering (Ó Séaghdha and Copestake, 2008; Sun and Korhonen, 2009; Sun et al., 2010; Falk et al., 2012, i.a.). The mean precision of induced verb clusters labelled modified purity (M P UR) is computed as: P M P UR = C∈Clust,nprev(C) &gt;1 nprev(C) #test_verbs (4) Here, each cluster C from the set of all KClust induced clusters Clust is associated with its prevalent class/cluster from the gold standard, and the number of verbs in an induced cluster C taking this prevalent class is labelled nprev(C) . All other verbs not taking the prevalent class are considered errors.10 #test_verbs denotes the total number of test verb instances. The se"
D17-1270,J07-2002,0,0.0510243,"EN vectors were taken directly from the work of Levy and Goldberg (2014): they are trained with SGNS on the cleaned and tokenised Polyglot Wikipedia (Al-Rfou et al., 2013) containing ∼75M sentences, ∼1.7B word tokens and a vocabulary of ∼180k words after lowercasing and frequency cut-off. To measure the importance of the starting source language space as well as to test if syntactic knowledge on the source side may be propagated to the target space, we test two variant EN vector spaces: SGNS with (a) BOW contexts and the window size 2 (SGNS-BOW2); and (b) dependencybased contexts (SGNS-DEPS) (Padó and Lapata, 2007; Levy and Goldberg, 2014). Linguistic Constraints We experiment with the following constraint types: (a) monolingual synonymy constraints in each target language extracted from BabelNet (Mono-Syn); (b) cross-lingual ENTARGET constraints from BabelNet; (c) crosslingual EN-TARGET constraints plus EN VerbNet constraints (see Sect. 2.1 and Fig. 1). Unless stated otherwise, we use BabelNet as the default source of cross-lingual constraints for (b) and (c). Vector Space Specialisation The PARAGRAM model’s parameters are adopted directly from prior work (Wieting et al., 2015) without any additional"
D17-1270,J05-1004,0,0.580512,"The practical usefulness of VerbNet style classification both within and across languages has been limited by the fact that few languages boast resources similar to the English VerbNet. Some VerbNets have been developed completely manually from scratch, aiming to capture properties specific to the language in question, e.g., the resources for Spanish and Catalan (Aparicio et al., 2008), 3 The usefulness of VerbNet is further accentuated by available mappings (Loper et al., 2007) to a number of other verb resources such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998), and PropBank (Palmer et al., 2005). 4 For example, Levin (1993) notes that verbs in Warlpiri manifest analogous behavior to English with respect to the conative alternation. In another example, Polish verbs have the same patterns as EN verbs in terms of the middle construction. 2548 Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). Other VerbNets were created semi-automatically, with the help of other lexical resources, e.g., for French (Pradet et al., 2014) and Brazilian Portuguese (Scarton and Aluısio, 2012). These approaches involved substantial amounts of specialised linguistic and translation work. Finall"
D17-1270,S16-2012,0,0.0669443,"es from resource-rich to resourcepoor languages using general-purpose cross-lingual dictionaries and bilingual vector spaces as means of transfer within a semantic specialisation framework. However, we believe that the proposed basic framework may be upgraded and extended across several research paths in future work. First, in the current work we have operated with standard single-sense/single-prototype representations, thus effectively disregarding the problem of verb polysemy. While several polysemy-aware verb classification models for English were developed recently (Kawahara et al., 2014; Peterson et al., 2016), the current lack of polysemyaware evaluation sets in other languages impedes this line of research. Evaluation issues aside, one idea for future work is to use the ATTRACT-R EPEL specialisation framework for sense-aware crosslingual transfer relying on recently developed multisense/prototype word representations (Neelakantan et al., 2014; Pilehvar and Collier, 2016, inter alia). Another challenge is to apply the idea from this work to enable cross-lingual transfer of other structured lexical resources available in English such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005),"
D17-1270,D16-1174,0,0.0350193,"Missing"
D17-1270,pradet-etal-2014-adapting,0,0.0400988,"Missing"
D17-1270,D07-1043,0,0.0306227,"from English (as a resource-rich language) to the resource-lean target language (providing an answer to question Q3, Sect. 1). These improvements are visible across all target languages, empirically demonstrating the cross-lingual nature of VerbNetstyle classifications. Second, using cross-lingual constraints alone (XLing) yields strong gains over initial distributional spaces (answering Q1 and Q2). Fig. 2 also shows that cross-lingual similarity constraints are more beneficial than the monolingual ones, despite a larger total number of the monolin11 We have also experimented with V-measure (Rosenberg and Hirschberg, 2007), another standard evaluation measure which balances between homogeneity (precision) and completeness (recall); we do not report these scores for brevity as similar trends in results are observed. gual constraints in each language (see Tab. 2). This suggests that such cross-lingual similarity links are strong implicit indicators of class membership. Namely, target language words which map to the same source language word are likely to be synonyms and consequently end up in the same verb class in the target language. However, the crosslingual links are even more useful as means for transferring"
D17-1270,J06-2001,0,0.0498789,"). However, end-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which aim to learn verb classes automatically offer an attractive alternative. However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012, i.a.), ranging from accurate parsers to pre-compiled subcategorisation frames (Schulte im Walde, 2006; Li and Brew, 2008; Messiant, 2008). Such methods are limited to a small set of resource-rich languages. It has been argued that VerbNet-style classification has a strong cross-lingual element (Jackendoff, 1992; Levin, 1993). In support of this argument, Majewska et al. (2017) have shown that English VerbNet has high translatability across different, 2546 1 http://verbs.colorado.edu/∼mpalmer/projects/verbnet.html Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2546–2558 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computationa"
D17-1270,N09-1064,0,0.05965,"Missing"
D17-1270,D09-1067,1,0.895498,"relearn) (read, study) (cram, relearn) (read, assimilate) (learn, assimilate) (read, relearn) (break, dissolve) (crash, crush) (shatter, split) (break, rip) (crack, smash) (shred, splinter) (snap, tear) (accept, understand) (accept, reject) (repent, rue) (reject, discourage) (encourage, discourage) (reject, discourage) (disprefer, understand) Given the initial distributional or specialised collection of target language vectors Vt , we apply an offthe-shelf clustering algorithm on top of these vectors in order to group verbs into classes. Following prior work (Brew and Schulte im Walde, 2002; Sun and Korhonen, 2009; Sun et al., 2010), we employ the MNCut spectral clustering algorithm (Meila and Shi, 2001), which has wide applicability in similar NLP tasks which involve high-dimensional feature spaces (Chen et al., 2006; von Luxburg, 2007; Scarton et al., 2014, i.a.). Again, following prior work (Sun et al., 2010, 2013), we estimate the number of clusters KClust using the self-tuning method of Zelnik-Manor and Perona (2004). This algorithm finds the optimal number by minimising a cost function based on the eigenvector structure of the word similarity matrix. We refer the reader to the relevant literature"
D17-1270,P13-2129,1,0.893049,"Missing"
D17-1270,C10-1119,1,0.355161,"genio, 2009), information extraction (Mausam et al., 2012), text mining applications (Lippincott et al., 2013; Rimell et al., 2013), research into human language acquisition (Korhonen, 2010), and other tasks. This benefit for English NLP has motivated the development of VerbNets for languages such as Spanish and Catalan (Aparicio et al., 2008), Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). However, end-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which aim to learn verb classes automatically offer an attractive alternative. However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012, i.a.), ranging from accurate parsers to pre-compiled subcategorisation frames (Schulte im Walde, 2006; Li and Brew, 2008; Messiant, 2008). Such methods are limited to a small set of resource-rich languages. It has been argued that VerbNet-style classification has a strong cross-lingual element"
D17-1270,P10-1040,0,0.115591,"n Empirical Methods in Natural Language Processing, pages 2546–2558 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics even typologically diverse languages. Based on this finding, we propose an automatic approach which exploits readily available annotations for English to facilitate efficient, large-scale development of VerbNets for a wide set of target languages. Recently, unsupervised methods for inducing distributed word vector space representations or word embeddings (Mikolov et al., 2013a) have been successfully applied to a plethora of NLP tasks (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014, i.a.). These methods offer an elegant way to learn directly from large corpora, bypassing the feature engineering step and the dependence on mature NLP pipelines (e.g., POS taggers, parsers, extraction of subcategorisation frames). In this work, we demonstrate how these models can be used to support automatic verb class induction. Moreover, we show that these models offer the means to exploit inherent cross-lingual links in VerbNet-style classification in order to guide the development of new classifications for resource-lean languages. To the bes"
D17-1270,P16-2084,1,0.915141,"Missing"
D17-1270,Q15-1025,0,0.366752,"t encodes the cross-linguistic validity of Levin-style verb classifications into the vector-space specialisation framework (Sect. 2.1) driven by linguistic constraints. A standard clustering algorithm is then run on top of the VerbNetspecialised representations using vector dimensions as features to learn verb clusters (Sect. 2.2). Our approach attains state-of-the-art verb classification performance across all six target languages. 2.1 Vector Space Specialisation Specialisation Model Our departure point is a state-of-the-art specialisation model for fine-tuning vector spaces termed PARAGRAM (Wieting et al., 2015).5 The PARAGRAM procedure injects similarity constraints between word pairs in order to make their vector space representations more similar; we term these the ATTRACT constraints. Let V = Vs t Vt be the vocabulary consisting of the source language and target language vocabularies Vs and Vt , respectively. Let C be the set of word pairs standing in desirable lexical relations; these include: 1) verb pairs from the same VerbNet class (e.g. (en_transport, en_transfer) from verb class SEND -11.1); and 2) the cross-lingual synonymy 5 The original PARAGRAM model as well as other finetuning models f"
D17-1270,N16-1096,0,0.0305242,"lack of polysemyaware evaluation sets in other languages impedes this line of research. Evaluation issues aside, one idea for future work is to use the ATTRACT-R EPEL specialisation framework for sense-aware crosslingual transfer relying on recently developed multisense/prototype word representations (Neelakantan et al., 2014; Pilehvar and Collier, 2016, inter alia). Another challenge is to apply the idea from this work to enable cross-lingual transfer of other structured lexical resources available in English such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbKB (Wijaya and Mitchell, 2016). Other potential research avenues include porting the approach to other typologically diverse languages and truly low-resource settings (e.g., with only limited amounts of parallel data), as well as experiments with other distributional spaces, e.g. (Melamud et al., 2016). Further refinements of the specialisation and clustering algorithms may also result in improved verb class induction. 5 Conclusion We have presented a novel cross-lingual transfer model which enables the automatic induction of VerbNet-style verb classifications across multiple languages. The transfer is based on a word vect"
D18-1026,W13-3520,0,0.0562518,"− Xs ||F = UV&gt; W UΣV&gt; = SVD(Xt X&gt; s ) (7) where ||· ||F is the Frobenius norm. After mapping the original target embeddings into the shared space with this method, we post-specialize them with the function outlined in §2.2, learnt on the source language. This yields the specialized target vectors ˆ t = G(W ˆ Xt ; θG ). Y 3 Experimental Setup Distributional Vectors. We estimate the robustness of adversarial post-specialization by experimenting with three widely used collections of distributional English vectors. 1) SGNS - W 2 vectors are trained on the cleaned and tokenized Polyglot Wikipedia (Al-Rfou et al., 2013) using Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) by Levy and Goldberg (2014) with bag-ofwords contexts (window size is 2). 2) GLOVE - CC are GloVe vectors trained on the Common Crawl (Pennington et al., 2014). 3) FASTTEXT are vectors trained on Wikipedia with a SGNS variant that builds word vectors by summing the vectors of their constituent character n-grams (Bojanowski et al., 2017). All vectors are 300-dimensional.4 Constraints and Initial Specialization. We experiment with the sets of linguistic constraints used in prior work (Zhang et al., 2014; Ono et al., 2015; Vuli"
D18-1026,P14-2131,0,0.06586,"Missing"
D18-1026,Q17-1010,0,0.552254,"xical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the semantic properties of pre"
D18-1026,D14-1082,0,0.00851156,"plification. We report consistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move bey"
D18-1026,P16-1156,0,0.0192147,"straints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017). By design, these methods fine-tune only vectors of words seen in external resources. Vuli´c et al. (2018) suggest that specializing the full vocabulary is beneficial for downstream applications. Comparing to their work, we show that a more sophisticated adversarial post-specia"
D18-1026,N15-1184,0,0.2529,"Missing"
D18-1026,N13-1092,0,0.0607536,"Missing"
D18-1026,D16-1235,1,0.92545,"Missing"
D18-1026,P15-2011,1,0.950221,"nstream NLP applications (Faruqui, 2016). Such models are versatile as they can be applied to arbitrary distributional spaces, but they have a major drawback: they locally update only vectors of words present in linguistic constraints (i.e., seen words), whereas vectors of all other (i.e., unseen) words remain intact (see Figure 1). Both authors equally contributed to this work. 1 For instance, it is difficult to discern synonyms from antonyms in distributional vector spaces: this has a negative impact on language understanding tasks such as statistical dialog modeling or text simplification (Glavaš and Štajner, 2015; Faruqui et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016) 282 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 282–293 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Source Target posed model in two downstream tasks: lexical text simplification and dialog state tracking. Finally, we demonstrate that, by coupling our adversarial specialization model with any unsupervised model for inducing bilingual vector spaces, such as the algorithm proposed by Conneau et al. (2018), we can successfully per"
D18-1026,W14-4337,0,0.114879,"Missing"
D18-1026,J15-4004,1,0.949832,"P), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the semantic properties of pre-trained vectors and specialize the distributional spaces for a particular relation, e.g., synonymy (i.e., true similarity) (Faruqui et al., 2015; Mrkši´c et al.,"
D18-1026,P14-2075,0,0.121359,"Missing"
D18-1026,N15-1070,0,0.0340618,"lexical resources. An overview of the proposed methodology from this section is provided in Figure 1. 2.1 Initial Specialization Linguistic Constraints. Adopting the nomenclature from Mrkši´c et al. (2017), post-processing models are generally guided by two broad sets of constraints: 1) ATTRACT constraints specify which words should be close to each other in the finetuned vector space (e.g. synonyms like graceful and amiable); 2) REPEL constraints describe which words should be pulled away from each other (e.g. antonyms like innocent and sinful). Earlier postprocessors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015) operate only with ATTRACT constraints, and are thus not suited to model both aspects contributing to the specialization process. We first outline the state-of-the-art ATTRACTREPEL specialization model (Mrkši´c et al., 2017) 283 which leverages both sets of constraints. Here, we again stress two important aspects relevant to our post-specialization model: a) all initial specialization models fine-tune only representations for the subspace of words seen in the external constraints, while all other words remain unaffected by specialization; b) post-specialization is not ti"
D18-1026,D15-1242,0,0.222314,"Missing"
D18-1026,P17-1163,1,0.900293,"Missing"
D18-1026,P14-2050,0,0.556242,"onal space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the"
D18-1026,P15-1145,0,0.126883,"ss, in the long run, these transfer results hold promise to support the specialization of vector spaces even for resource-lean languages, and their applications. 5 Related Work Vector Space Specialization. Specialization methods embed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrkši´c et"
D18-1026,Q17-1022,1,0.887228,"Missing"
D18-1026,P16-2074,0,0.299861,"twork (i.e., the generator). We show that the proposed adversarial model yields state-of-the-art performance on standard word similarity benchmarks, outperforming the post-specialization model of Vuli´c et al. (2018). We further demonstrate the effectiveness of the proMethodology The post-specialization procedure (Vuli´c et al., 2018) is a two-step process. First, a subspace of vectors for words observed in external resources is fine-tuned using any off-the-shelf specialization model, such as the original retrofitting model (Faruqui et al., 2015), counter-fitting (Mrkši´c et al., 2016), dLCE (Nguyen et al., 2016), or state-of-theart ATTRACT- REPEL ( AR ) specialization (Mrkši´c et al., 2017; Vuli´c et al., 2017). We outline the initial specialization algorithms in §2.1. In the second step, the initial specialization is propagated to the entire vocabulary, including words not observed in the resources, relying on an adversarial architecture augmented with a distance loss. This adversarial post-specialization model, compatible with any specialization model, is described in §2.2. Finally, in §2.3, we introduce a cross-lingual zero-shot specialization model which transfers the specialization to a target l"
D18-1026,K16-1006,0,0.0212333,"onsistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervise"
D18-1026,N15-1100,0,0.536844,"(Al-Rfou et al., 2013) using Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) by Levy and Goldberg (2014) with bag-ofwords contexts (window size is 2). 2) GLOVE - CC are GloVe vectors trained on the Common Crawl (Pennington et al., 2014). 3) FASTTEXT are vectors trained on Wikipedia with a SGNS variant that builds word vectors by summing the vectors of their constituent character n-grams (Bojanowski et al., 2017). All vectors are 300-dimensional.4 Constraints and Initial Specialization. We experiment with the sets of linguistic constraints used in prior work (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018). These constraints, extracted from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009), comprise a total of 1,023,082 synonymy/ATTRACT word pairs and 380,873 antonymy/REPEL pairs. Note that the sets of constraints cover only a fraction of the full distributional vocabulary, providing direct motivation for post-specialization methods 4 3 See the recent survey papers on cross-lingual word embeddings and their typology (Upadhyay et al., 2016; Vuli´c and Korhonen, 2016; Ruder et al., 2017) 286 Experiments with other standard word vectors, such as (Melamud et al., 2"
D18-1026,N16-1118,0,0.0700653,"onsistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervise"
D18-1026,Q16-1030,0,0.390812,"results hold promise to support the specialization of vector spaces even for resource-lean languages, and their applications. 5 Related Work Vector Space Specialization. Specialization methods embed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al"
D18-1026,D14-1162,0,0.0930116,"a full target distributional space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et"
D18-1026,P15-1173,0,0.0417519,"ed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017). By design, these methods fine-tune only vectors of words seen in external resources. Vuli´c et al. (2018) suggest that specializing the full vocabulary is beneficial for downstream applica"
D18-1026,K15-1026,0,0.222497,"ess has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the semantic properties of pre-trained vectors and specialize the distributional spaces for a particular relation, e.g., synonymy (i.e., true similarity) (Faruqui et al., 2015; Mrkši´c et al., 2017) or hypernymy (Nic"
D18-1026,P16-1157,0,0.102759,"Missing"
D18-1026,N18-1048,1,0.24712,"Missing"
D18-1026,P16-1024,1,0.923484,"Missing"
D18-1026,N18-1103,1,0.824775,"Missing"
D18-1026,P17-1006,1,0.860576,"Missing"
D18-1026,K17-1013,1,0.898667,"Missing"
D18-1026,E17-1042,1,0.840593,"Missing"
D18-1026,Q15-1025,0,0.637092,"n overview of the proposed methodology from this section is provided in Figure 1. 2.1 Initial Specialization Linguistic Constraints. Adopting the nomenclature from Mrkši´c et al. (2017), post-processing models are generally guided by two broad sets of constraints: 1) ATTRACT constraints specify which words should be close to each other in the finetuned vector space (e.g. synonyms like graceful and amiable); 2) REPEL constraints describe which words should be pulled away from each other (e.g. antonyms like innocent and sinful). Earlier postprocessors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015) operate only with ATTRACT constraints, and are thus not suited to model both aspects contributing to the specialization process. We first outline the state-of-the-art ATTRACTREPEL specialization model (Mrkši´c et al., 2017) 283 which leverages both sets of constraints. Here, we again stress two important aspects relevant to our post-specialization model: a) all initial specialization models fine-tune only representations for the subspace of words seen in the external constraints, while all other words remain unaffected by specialization; b) post-specialization is not tied to ATTRACT- REPEL in"
D18-1026,P14-2089,0,0.199538,"he different ways concepts are lexicalized across languages, as studied by semantic typology (Ponti et al., 2018). Nonetheless, in the long run, these transfer results hold promise to support the specialization of vector spaces even for resource-lean languages, and their applications. 5 Related Work Vector Space Specialization. Specialization methods embed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, ou"
D18-1026,D14-1161,0,0.205358,"Polyglot Wikipedia (Al-Rfou et al., 2013) using Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) by Levy and Goldberg (2014) with bag-ofwords contexts (window size is 2). 2) GLOVE - CC are GloVe vectors trained on the Common Crawl (Pennington et al., 2014). 3) FASTTEXT are vectors trained on Wikipedia with a SGNS variant that builds word vectors by summing the vectors of their constituent character n-grams (Bojanowski et al., 2017). All vectors are 300-dimensional.4 Constraints and Initial Specialization. We experiment with the sets of linguistic constraints used in prior work (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018). These constraints, extracted from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009), comprise a total of 1,023,082 synonymy/ATTRACT word pairs and 380,873 antonymy/REPEL pairs. Note that the sets of constraints cover only a fraction of the full distributional vocabulary, providing direct motivation for post-specialization methods 4 3 See the recent survey papers on cross-lingual word embeddings and their typology (Upadhyay et al., 2016; Vuli´c and Korhonen, 2016; Ruder et al., 2017) 286 Experiments with other standard word vectors, such as"
D18-1026,P17-1179,0,0.031446,"distance can be formulated as the mean squared error between the input and the target (Pathak et al., 2016), their feature maps (Li and Wand, 2016), both (Zhu et al., 2016), or a loss calculated on feature maps of a deep convolutional network (Ledig et al., 2017). In the textual domain, adversarial models have been proven to support domain adaptation (Ganin et al., 2016) and language transfer (Chen et al., 2016) by learning domain/language-invariant latent features. Adversarial training also powers unsupervised mapping between monolingual vector spaces to learn cross-lingual word embeddings (Zhang et al., 2017; Conneau et al., 2018). In this work, we show how to apply adversarial techniques to the problem of vector specialization, which has a substantial impact on language understanding tasks. 6 Conclusion and Future Work We have presented adversarial post-specialization, a novel model supported by adversarial training which specializes word vectors for the full vocabulary of the input distributional vector space, including words unseen in external lexical resources. We have also introduced a method for zero-shot specialization of word vectors in languages without any external resources. The benefi"
D18-1029,E17-1088,0,0.119612,"e standard fixed-vocabulary assumption. MIN=5: only words with corpus frequency above 5 are retained in the final fixed vocabulary V ; 10K: V comprises the 10k most frequent words. improves the perplexity measure, it actually makes the models less useful, especially in morphologically rich languages, as exemplified in Table 1. Our goal is to get a clear picture on how different typological features and the corresponding corpus frequency distributions affect LM performance, without the influence of the unrealistic fixed-vocabulary assumption. Therefore, we work in the full-vocabulary LM setup (Adams et al., 2017; Grave et al., 2017). This means that we explicitly decide to retain also infrequent words in the modeled data: V contains all words occurring at least once in the training set, only unseen words from test data are treated as OOVs. We believe that this setup leads to an evaluation that pinpoints the crucial limitations of standard LM architectures.2 Why Not Open Vocabulary Setup? Recent neural LM architectures have also focused on handling large vocabularies and unseen words using character-aware modeling (Luong and Manning, 2016; Jozefowicz et al., 2016; Kawakami et al., 2017, inter alia). T"
D18-1029,W13-3520,0,0.0177648,"nly for a fraction of the world’s languages. Second, these data are biased because their features may not stem from the underlying distribution, i.e., from what is naturally possible/frequent, but rather 319 can be inherited by genealogical relatedness or borrowed by areal proximity (Bakker, 2010). To mitigate these biases, theoretical works resorted to stratification approaches, where each subgroup of related languages is sampled independently. maximizing their diversity (Dryer, 1989, inter alia). We perform our selection in the same spirit. We start from the Polyglot Wikipedia (PW) project (Al-Rfou et al., 2013) which provides cleaned and tokenised Wikipedia data in 40 languages. However, the majority of the PW languages are similar from the perspective of genealogy (26/40 are Indo-European), geography (28/40 are Western European), and typology (26/40 are fusional). Consequently, the PW set is not a representative sample of the world’s languages. To amend this limitation, we source additional languages with the data coming from the same domain, Wikipedia, considering candidates in descending order of corpus size cleaned and preprocessed by the Polyglot tokeniser (Al-Rfou et al., 2013). Since fusional"
D18-1029,K17-2002,0,0.0277995,"sis (1949), and it cannot be guaranteed for resource-poor languages where obtaining sufficient monolingual data is also a challenge (Adams et al., 2017). Therefore, another solution is to resort to other sources of information which are not purely contextual/distributional. For instance, a promising line of current and future research is to (learn to) exploit subword-level patterns captured in an unsupervised manner (Pinter et al., 2017; Herbelot and Baroni, 2017) or integrate existing morphological generation and inflection tools and regularities (Cotterell et al., 2015; Vuli´c et al., 2017; Bergmanis et al., 2017) into language models to reduce data sparsity, and improve language modeling for morphologically rich languages. For instance, a recent enhancement of the Char-CNN-LSTM language model that enforces similarity between parameters of morphologically related words leads to large perplexity gains across a large number of languages, with the most prominent gains reported for morphologically complex languages (Gerz et al., 2018). Given the recent success and improved performance with LM-based pre-training methodology (Peters et al., 2018; Howard and Ruder, 2018) across a wide variety of syntactic and"
D18-1029,D08-1078,0,0.02949,"rts preliminary results from prior work (Botha and Blunsom, 2014; Adams et al., 2017; Cotterell et al., 2018), and is also backed by insights from linguistic theory on variance of language complexity in general and variance of morphological complexity in specific (McWhorter, 2001; Evans and Levinson, 2009). More broadly and along the same line, earlier research in statistical machine translation (SMT) has also shown that typological factors such as the amount of reordering, the morphological complexity, as well as genealogical relatedness of languages are crucial in predicting success in SMT (Birch et al., 2008; Paul et al., 2009; Daiber, 2018). Our results indicate that the artificial fixed323 5 Unfortunately no values are available in WALS for the feature of flexivity besides a limited domain. vocabulary assumption from prior work produces overly optimistic perplexity scores, and its limitation is even more pronounced in morphologically rich languages, which inherently contain a large number of infrequent words due to their productive morphological systems. The typical solution to collect more data (Jozefowicz et al., 2016; Kawakami et al., 2017) mitigates this effect to a certain extent, but stil"
D18-1029,D15-1042,0,0.0182386,"ically diverse languages is still missing. The novel dataset we discuss in this paper aims at bridging this gap (see §4). Multilingual Language Modeling A language model computes a probability distribution over sequences of word tokens, and is typically trained to maximise the likelihood of word input sequences. The LM objective is expressed as: P (w1 , ...wn ) = Y P (wi |w1 , ...wi−1 ) (1) i wi is a word token with the index i in the sequence. LM is considered a central task in NLP and language understanding, with applications in speech recognition (Mikolov et al., 2010), text summarisation (Filippova et al., 2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marc"
D18-1029,Q18-1032,1,0.863129,"Missing"
D18-1029,P17-1109,0,0.0426337,", as shown in §4. 4 Data Selection of Languages. Our selection of test languages is guided by the following goals: a) we have to ensure the coverage of typological properties from §3, and b) we want to analyse a large set of languages which extends and surpasses other work in the LM literature (see §2). Since cross-lingual NLP aims at modeling extant languages rather than possible languages (including, e.g., extinct ones), creating a balanced sample is challenging. In fact, attested languages, intended as a random variable, are extremely sparse and not independent-and-identically-distributed (Cotterell and Eisner, 2017). First, available and reliable data exist only for a fraction of the world’s languages. Second, these data are biased because their features may not stem from the underlying distribution, i.e., from what is naturally possible/frequent, but rather 319 can be inherited by genealogical relatedness or borrowed by areal proximity (Bakker, 2010). To mitigate these biases, theoretical works resorted to stratification approaches, where each subgroup of related languages is sampled independently. maximizing their diversity (Dryer, 1989, inter alia). We perform our selection in the same spirit. We star"
D18-1029,P13-2121,0,0.0214948,"Missing"
D18-1029,N18-2085,0,0.180154,"et statistical models in terms of variation in language structures (Ponti et al., 2017). ∗ Both authors equally contributed to this work. In order to evaluate how cross-lingual structural variation hinders the design of effective generalpurpose algorithms, we propose the task of language modeling (LM) as a testbed. In particular, we opt for a full-vocabulary setup where no word encountered at training time is treated as an unknown symbol, in order to a) ensure a fair comparison across languages with different word frequency rates and b) avoid setting an arbitrary threshold on vocabulary size (Cotterell et al., 2018). Although there has recently been a tendency towards expanding test language samples, the datasets considered in previous works (Botha and Blunsom, 2014; Vania and Lopez, 2017; Kawakami et al., 2017; Cotterell et al., 2018) are not entirely adequate yet to represent the typological variation and to ground cross-lingual generalisations empirically. Hence, we test several LM architectures (including n-gram, neural, and character-aware models) on a novel and wider set of 50 languages sampled according to stratification principles. Through this large-scale multilingual analysis, we shed new light"
D18-1029,D17-1030,0,0.0229719,"n to collect more data (Jozefowicz et al., 2016; Kawakami et al., 2017) mitigates this effect to a certain extent, but stills suffers from the Zipfian hypothesis (1949), and it cannot be guaranteed for resource-poor languages where obtaining sufficient monolingual data is also a challenge (Adams et al., 2017). Therefore, another solution is to resort to other sources of information which are not purely contextual/distributional. For instance, a promising line of current and future research is to (learn to) exploit subword-level patterns captured in an unsupervised manner (Pinter et al., 2017; Herbelot and Baroni, 2017) or integrate existing morphological generation and inflection tools and regularities (Cotterell et al., 2015; Vuli´c et al., 2017; Bergmanis et al., 2017) into language models to reduce data sparsity, and improve language modeling for morphologically rich languages. For instance, a recent enhancement of the Char-CNN-LSTM language model that enforces similarity between parameters of morphologically related words leads to large perplexity gains across a large number of languages, with the most prominent gains reported for morphologically complex languages (Gerz et al., 2018). Given the recent s"
D18-1029,K15-1017,0,0.0360047,", but stills suffers from the Zipfian hypothesis (1949), and it cannot be guaranteed for resource-poor languages where obtaining sufficient monolingual data is also a challenge (Adams et al., 2017). Therefore, another solution is to resort to other sources of information which are not purely contextual/distributional. For instance, a promising line of current and future research is to (learn to) exploit subword-level patterns captured in an unsupervised manner (Pinter et al., 2017; Herbelot and Baroni, 2017) or integrate existing morphological generation and inflection tools and regularities (Cotterell et al., 2015; Vuli´c et al., 2017; Bergmanis et al., 2017) into language models to reduce data sparsity, and improve language modeling for morphologically rich languages. For instance, a recent enhancement of the Char-CNN-LSTM language model that enforces similarity between parameters of morphologically related words leads to large perplexity gains across a large number of languages, with the most prominent gains reported for morphologically complex languages (Gerz et al., 2018). Given the recent success and improved performance with LM-based pre-training methodology (Peters et al., 2018; Howard and Ruder"
D18-1029,P18-1031,0,0.0568651,"word token with the index i in the sequence. LM is considered a central task in NLP and language understanding, with applications in speech recognition (Mikolov et al., 2010), text summarisation (Filippova et al., 2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German"
D18-1029,P17-1137,0,0.184332,"ders the design of effective generalpurpose algorithms, we propose the task of language modeling (LM) as a testbed. In particular, we opt for a full-vocabulary setup where no word encountered at training time is treated as an unknown symbol, in order to a) ensure a fair comparison across languages with different word frequency rates and b) avoid setting an arbitrary threshold on vocabulary size (Cotterell et al., 2018). Although there has recently been a tendency towards expanding test language samples, the datasets considered in previous works (Botha and Blunsom, 2014; Vania and Lopez, 2017; Kawakami et al., 2017; Cotterell et al., 2018) are not entirely adequate yet to represent the typological variation and to ground cross-lingual generalisations empirically. Hence, we test several LM architectures (including n-gram, neural, and character-aware models) on a novel and wider set of 50 languages sampled according to stratification principles. Through this large-scale multilingual analysis, we shed new light on the current limitations of standard LM models and offer support to further developments in multilingual NLP. In particular, we demonstrate that the previous fixedvocabulary assumption in fact ign"
D18-1029,2005.mtsummit-papers.11,0,0.0412215,"gium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ing a short overview of multilingual LM and its possible setups (§2), we describe the cross-lingual variation in morphological systems and propose a novel typologically diverse dataset for LM in §3. We outline the data in §4 and benchmarked language models in §5. Finally, we discuss the results in light of linguistic typology in §6. 2 To the best of our knowledge, the largest datasets used in previous work are from (Müller et al., 2012; Cotterell et al., 2018) and amount to 21 languages from the Europarl data (Koehn, 2005). Despite the large coverage of languages, these sets are still restricted only to the languages of the European Union. On the other hand, the most typologically diverse dataset thus far was released by Vania and Lopez (2017). It includes 10 languages representing some morphological systems. This short survey of related work demonstrates a clear tendency towards extending LM evaluation to other languages, abandoning English-centric assumptions, and focusing on language-agnostic LM architectures. However, a comprehensive evaluation set that systematically covers a wide and balanced spectrum of"
D18-1029,P16-1100,0,0.0493417,"Missing"
D18-1029,P11-1015,0,0.00869074,"representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German, and Russian from the 2013 Workshop on Statistical Machine Translation (WMT) data (Bojar et al., 2013). Kim et al. (2016) reuse these datasets and add Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Kawakami et al. (2017) evaluate on 7 European languages using Wikipedia data, including Finnish. Fixed vs"
D18-1029,J93-2004,0,0.0652462,"2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German, and Russian from the 2013 Workshop on Statistical Machine Translation (WMT) data (Bojar et al., 2013). Kim et al. (2016) reuse these datasets and add Arabic. Ling et al. (2015) evaluate on English, Portuguese,"
D18-1029,D16-1209,0,0.0331062,"Missing"
D18-1029,N12-1043,0,0.0266202,"18 Conference on Empirical Methods in Natural Language Processing, pages 316–327 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ing a short overview of multilingual LM and its possible setups (§2), we describe the cross-lingual variation in morphological systems and propose a novel typologically diverse dataset for LM in §3. We outline the data in §4 and benchmarked language models in §5. Finally, we discuss the results in light of linguistic typology in §6. 2 To the best of our knowledge, the largest datasets used in previous work are from (Müller et al., 2012; Cotterell et al., 2018) and amount to 21 languages from the Europarl data (Koehn, 2005). Despite the large coverage of languages, these sets are still restricted only to the languages of the European Union. On the other hand, the most typologically diverse dataset thus far was released by Vania and Lopez (2017). It includes 10 languages representing some morphological systems. This short survey of related work demonstrates a clear tendency towards extending LM evaluation to other languages, abandoning English-centric assumptions, and focusing on language-agnostic LM architectures. However, a"
D18-1029,C16-1123,1,0.845009,"Missing"
D18-1029,N09-2056,0,0.100996,"lts from prior work (Botha and Blunsom, 2014; Adams et al., 2017; Cotterell et al., 2018), and is also backed by insights from linguistic theory on variance of language complexity in general and variance of morphological complexity in specific (McWhorter, 2001; Evans and Levinson, 2009). More broadly and along the same line, earlier research in statistical machine translation (SMT) has also shown that typological factors such as the amount of reordering, the morphological complexity, as well as genealogical relatedness of languages are crucial in predicting success in SMT (Birch et al., 2008; Paul et al., 2009; Daiber, 2018). Our results indicate that the artificial fixed323 5 Unfortunately no values are available in WALS for the feature of flexivity besides a limited domain. vocabulary assumption from prior work produces overly optimistic perplexity scores, and its limitation is even more pronounced in morphologically rich languages, which inherently contain a large number of infrequent words due to their productive morphological systems. The typical solution to collect more data (Jozefowicz et al., 2016; Kawakami et al., 2017) mitigates this effect to a certain extent, but stills suffers from the"
D18-1029,N18-1202,0,0.182197,".wi−1 ) (1) i wi is a word token with the index i in the sequence. LM is considered a central task in NLP and language understanding, with applications in speech recognition (Mikolov et al., 2010), text summarisation (Filippova et al., 2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech"
D18-1029,D17-1010,0,0.0372516,". The typical solution to collect more data (Jozefowicz et al., 2016; Kawakami et al., 2017) mitigates this effect to a certain extent, but stills suffers from the Zipfian hypothesis (1949), and it cannot be guaranteed for resource-poor languages where obtaining sufficient monolingual data is also a challenge (Adams et al., 2017). Therefore, another solution is to resort to other sources of information which are not purely contextual/distributional. For instance, a promising line of current and future research is to (learn to) exploit subword-level patterns captured in an unsupervised manner (Pinter et al., 2017; Herbelot and Baroni, 2017) or integrate existing morphological generation and inflection tools and regularities (Cotterell et al., 2015; Vuli´c et al., 2017; Bergmanis et al., 2017) into language models to reduce data sparsity, and improve language modeling for morphologically rich languages. For instance, a recent enhancement of the Char-CNN-LSTM language model that enforces similarity between parameters of morphologically related words leads to large perplexity gains across a large number of languages, with the most prominent gains reported for morphologically complex languages (Gerz et al"
D18-1029,S17-1003,1,0.846376,"Missing"
D18-1029,E17-2025,0,0.0127292,"ternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German, and Russian from the 2013 Workshop on Statistical Machine Translation (WMT) data (Bojar et al., 2013). Kim et al. (2016) reuse these datasets and add Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Kawakami et al. (2017) evaluate on 7 European languages using Wikipedia data, including Finnish. Fixed vs. Full Vocabulary Setup. A majority of language models rely on the fixed-vocabulary assumption: they use a spe"
D18-1029,D15-1044,0,0.0129894,"is still missing. The novel dataset we discuss in this paper aims at bridging this gap (see §4). Multilingual Language Modeling A language model computes a probability distribution over sequences of word tokens, and is typically trained to maximise the likelihood of word input sequences. The LM objective is expressed as: P (w1 , ...wn ) = Y P (wi |w1 , ...wi−1 ) (1) i wi is a word token with the index i in the sequence. LM is considered a central task in NLP and language understanding, with applications in speech recognition (Mikolov et al., 2010), text summarisation (Filippova et al., 2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and"
D18-1029,P17-1184,0,0.256638,"tructural variation hinders the design of effective generalpurpose algorithms, we propose the task of language modeling (LM) as a testbed. In particular, we opt for a full-vocabulary setup where no word encountered at training time is treated as an unknown symbol, in order to a) ensure a fair comparison across languages with different word frequency rates and b) avoid setting an arbitrary threshold on vocabulary size (Cotterell et al., 2018). Although there has recently been a tendency towards expanding test language samples, the datasets considered in previous works (Botha and Blunsom, 2014; Vania and Lopez, 2017; Kawakami et al., 2017; Cotterell et al., 2018) are not entirely adequate yet to represent the typological variation and to ground cross-lingual generalisations empirically. Hence, we test several LM architectures (including n-gram, neural, and character-aware models) on a novel and wider set of 50 languages sampled according to stratification principles. Through this large-scale multilingual analysis, we shed new light on the current limitations of standard LM models and offer support to further developments in multilingual NLP. In particular, we demonstrate that the previous fixedvocabulary"
D18-1029,P17-1006,1,0.88688,"Missing"
D18-1029,P16-1125,0,0.0132887,"form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German, and Russian from the 2013 Workshop on Statistical Machine Translation (WMT) data (Bojar et al., 2013). Kim et al. (2016) reuse these datasets and add Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Kawakami et al. (2017) evaluate on 7 European languages using Wikipedia data, including Finnish. Fixed vs. Full Vocabulary Setup. A majority of language models rely on"
D19-1125,W17-5506,0,0.0356549,"process is expensive and time-consuming, requiring domain and expert knowledge (Asri et al., 2017). Due to this restriction, existing datasets for task-oriented dialogue are several orders of magnitude smaller compared to general open-domain dialogue corpora (Lowe et al., 2015; Henderson et al., 2019). Arguably, one of the most challenging parts of dialogue modelling is maintaining an interpretable internal memory over crucial domain concepts (Williams et al., 2016). Although there is increasing research effort to learn Dialogue State Tracking (DST) jointly with the text generation component (Eric et al., 2017; Wu et al., 2019), the most effective models use it as an intermediate signal (Wen et al., 2017; Lei et al., 2018). The difficulty of state tracking has made this task a driving force behind most of the Dialog System Technology Challenges in recent years (Henderson et al., 2014; Kim et al., 2017). In this paper, we reduce the reliance of taskoriented dialogue systems on data collection by leveraging semi-supervised training (Chapelle et al., 2009). Two approaches are investigated and evaluated for providing an improved training signal to the dialogue state tracking component in an end-to-end"
D19-1125,W10-4334,0,0.0889143,"Missing"
D19-1125,P18-1133,0,0.0505559,"en et al., 2015). Recent progress in sequence-to-sequence (seq2seq) modelling has enabled the development of fully neural end-to-end architectures, allowing for different components to be optimized jointly in order to share information (Wen et al., 2017; Zhao et al., 2017; Budzianowski and Vuli´c, 2019). Dialogue systems benefit greatly from optimizing on detailed annotations, such as turn-level dialogue state labels (Henderson et al., 2014) or dialogue actions (Rieser and Lemon, 2011), with end-to-end architectures still relying on intermediate labels in order to obtain satisfactory results (Lei et al., 2018). Collecting these labels is often the bottleneck in dataset creation, as the process is expensive and time-consuming, requiring domain and expert knowledge (Asri et al., 2017). Due to this restriction, existing datasets for task-oriented dialogue are several orders of magnitude smaller compared to general open-domain dialogue corpora (Lowe et al., 2015; Henderson et al., 2019). Arguably, one of the most challenging parts of dialogue modelling is maintaining an interpretable internal memory over crucial domain concepts (Williams et al., 2016). Although there is increasing research effort to le"
D19-1125,W15-4640,0,0.030933,"on detailed annotations, such as turn-level dialogue state labels (Henderson et al., 2014) or dialogue actions (Rieser and Lemon, 2011), with end-to-end architectures still relying on intermediate labels in order to obtain satisfactory results (Lei et al., 2018). Collecting these labels is often the bottleneck in dataset creation, as the process is expensive and time-consuming, requiring domain and expert knowledge (Asri et al., 2017). Due to this restriction, existing datasets for task-oriented dialogue are several orders of magnitude smaller compared to general open-domain dialogue corpora (Lowe et al., 2015; Henderson et al., 2019). Arguably, one of the most challenging parts of dialogue modelling is maintaining an interpretable internal memory over crucial domain concepts (Williams et al., 2016). Although there is increasing research effort to learn Dialogue State Tracking (DST) jointly with the text generation component (Eric et al., 2017; Wu et al., 2019), the most effective models use it as an intermediate signal (Wen et al., 2017; Lei et al., 2018). The difficulty of state tracking has made this task a driving force behind most of the Dialog System Technology Challenges in recent years (Hen"
D19-1125,D15-1166,0,0.00765738,"rrectly recognize the mentioned slot-value pairs in the user utterance and to maintain the updated dialogue (belief) state. Let i, j and k denote the index of domain, slot and value. As depicted at the top of Figure 1, the user utterance w1 :wL at turn t is first encoded by the BiLSTM to obtain the hidden states ht1:L . The encoding of the slot-value pair svkij is the output of the affine layer that takes the concatenation of the embeddings of domain i, slot j and value k as the input. The context vector aij k is then computed by the attention mechanism, denoted as attn in Figure 1, following Luong et al. (2015): el = sim(hl , svkij ) aij k = L ∑ el hl , (1) (2) l=1 where l is the word index of the user utterance and sim denotes any function that calculates the similarity of two vectors. We adopt here the dot product function, following Mrkši´c et al. (2017); Zhong et al. (2018); Ramadan et al. (2018). The ij ij similarity score sij k between ak and svk is then computed to see whether the slot-value pair svkij is mentioned in the utterance. The mentioned pair should have higher similarity score to its context vector than those which are not mentioned. The softmax layer is then applied to form the pro"
D19-1125,P17-1163,0,0.0997314,"Missing"
D19-1125,P18-2069,1,0.913302,"Missing"
D19-1125,P17-1061,0,0.0312897,"ts or providing information to visitors in a new city (Raux et al., 2005). Most current industry-oriented systems rely on modular, domain-focused frameworks (Young et al., 2013; Sarikaya et al., 2016), with separate components for user understanding (Henderson et al., 2014), decision making (Gaši´c et al., 2010) and system answer generation (Wen et al., 2015). Recent progress in sequence-to-sequence (seq2seq) modelling has enabled the development of fully neural end-to-end architectures, allowing for different components to be optimized jointly in order to share information (Wen et al., 2017; Zhao et al., 2017; Budzianowski and Vuli´c, 2019). Dialogue systems benefit greatly from optimizing on detailed annotations, such as turn-level dialogue state labels (Henderson et al., 2014) or dialogue actions (Rieser and Lemon, 2011), with end-to-end architectures still relying on intermediate labels in order to obtain satisfactory results (Lei et al., 2018). Collecting these labels is often the bottleneck in dataset creation, as the process is expensive and time-consuming, requiring domain and expert knowledge (Asri et al., 2017). Due to this restriction, existing datasets for task-oriented dialogue are sev"
D19-1125,P18-1135,0,0.0479066,"by the BiLSTM to obtain the hidden states ht1:L . The encoding of the slot-value pair svkij is the output of the affine layer that takes the concatenation of the embeddings of domain i, slot j and value k as the input. The context vector aij k is then computed by the attention mechanism, denoted as attn in Figure 1, following Luong et al. (2015): el = sim(hl , svkij ) aij k = L ∑ el hl , (1) (2) l=1 where l is the word index of the user utterance and sim denotes any function that calculates the similarity of two vectors. We adopt here the dot product function, following Mrkši´c et al. (2017); Zhong et al. (2018); Ramadan et al. (2018). The ij ij similarity score sij k between ak and svk is then computed to see whether the slot-value pair svkij is mentioned in the utterance. The mentioned pair should have higher similarity score to its context vector than those which are not mentioned. The softmax layer is then applied to form the probability distribution pinf ij for each informable slot sinf ij , where the predicted value is the value with 1274 the highest probability. The same attention mechanism is used for each requestable slot reqr to decide whether the user has asked for the slot in the current"
D19-1125,D15-1199,0,0.0220238,"Missing"
D19-1125,E17-1042,0,0.0922133,"Missing"
D19-1226,Q17-1010,0,0.0982944,"revious state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog"
D19-1226,S17-2001,0,0.0533888,"Missing"
D19-1226,P19-1070,1,0.721523,"Missing"
D19-1226,P15-2011,1,0.869561,"Missing"
D19-1226,W16-2501,1,0.82432,", word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrkˇsi´c et al., 2016; Kim et al., 2016b). Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018) or lexical entailment (Nguyen et al., 2017; Vuli´c and Mrkˇsi´c, 2018) over other types of semantic association in the word vector space. The best-performing sp"
D19-1226,J15-4004,1,0.915895,"Missing"
D19-1226,P14-2075,0,0.0774785,"pecialization on LS, we employ ˇ Light-LS (Glavaˇs and Stajner, 2015), a languageagnostic LS tool that makes simplifications based on word similarities in a given vector space. The quality of similarity-based information encoded in the vector space encode is thus expected to directly correlate with the performance of Light-LS. We use LS datasets for Italian (IT) (Tonelli et al., 2016), Spanish (ES) (Saggion et al., 2015; Saggion, 2017), and Portuguese (PT) (Hartmann et al., 2018) to evaluate the specialized spaces in those languages. We rely on the standard LS evaluation metric of Accuˇ racy (Horn et al., 2014; Glavaˇs and Stajner, 2015): it quantifies both the quality and frequency of replacements as a number of correct simplifications divided by the total number of complex words. Results and Analysis. The results are reported in Table 3. As shown in previous work (Vuli´c et al., 2018; Ponti et al., 2018), retrofitting (CLSRI-AR) and the cross-lingual post-specialization transfer (X-PS) are substantially better in the LS task than the original distributional space. However, our full CLSRI-PS model results in substantial boosts in the 2213 LS any additional input for the lexical prediction step (i."
D19-1226,N15-1184,0,0.302339,"Missing"
D19-1226,D18-1330,0,0.0952862,"Missing"
D19-1226,W19-4310,1,0.811783,"Missing"
D19-1226,D15-1242,0,0.242713,"Missing"
D19-1226,W16-1607,0,0.262024,"Missing"
D19-1226,N18-2029,1,0.870858,"Missing"
D19-1226,P18-1004,1,0.752226,"Missing"
D19-1226,N16-1018,0,0.139801,"Missing"
D19-1226,C18-1205,0,0.107783,"and translation of incorrect senses of Ls words. We thus subsequently refine the noisy set of target constraints by having a state-of-the-art neural model for lexico-semantic relation prediction (Glavaˇs and Vuli´c, 2018a), trained on the Ls constraints, discern valid from invalid Lt constraints. Following that, we perform monolingual retrofitting and post-specialization in the target language Lt , as outlined in § 3.2. The Lt distributional vectors can be specialized with the cleaned Lt constraints using any off-the-shelf retrofitting model (Faruqui et al., 2015; Mrkˇsi´c et al., 2016; 2208 Lengerich et al., 2018, inter alia). In this work we opt for the best-performing retrofitting model ATTRACT- REPEL ( AR ) (Mrkˇsi´c et al., 2017; Vuli´c et al., 2017b). AR specializes only the words seen in the cleaned Lt constraints. As the final step, we generalize AR’s specialization to the entire target vocabulary with a post-specialization model (Ponti et al., 2018) that learns the global specialization function from pairs of distributional and ARspecialized vectors of words from Lt constraints. A visual summary of our transfer model is presented in Figure 1. Our proposed CLSRI specialization conceptually diff"
D19-1226,P14-2050,0,0.0346927,"og state tracking, and semantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., betwe"
D19-1226,C18-1172,0,0.0157486,"ext simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the latter yield better downstream performance (Mrkˇsi´c et al., 2016). Moreover, while the joint models are tightly coupled to a concrete word embedding objective, retrofitting models can be applied on top"
D19-1226,P15-1145,0,0.0602024,"ntic relation (e.g., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical"
D19-1226,N16-1118,0,0.020871,". The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can br"
D19-1226,P18-2018,1,0.880048,"Missing"
D19-1226,Q17-1022,1,0.892103,"Missing"
D19-1226,L18-1381,0,0.208203,"Missing"
D19-1226,C16-1123,1,0.872206,"Missing"
D19-1226,N15-1100,0,0.0535031,"., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the l"
D19-1226,Q16-1030,0,0.0603068,"rity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the latter yield better dow"
D19-1226,D14-1162,0,0.0845732,"mantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensi"
D19-1226,N18-1202,0,0.0147967,"ialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation s"
D19-1226,J19-3005,1,0.873216,"Missing"
D19-1226,P17-1163,0,0.0403913,"Missing"
D19-1226,D18-1026,1,0.750703,"Missing"
D19-1226,D18-1299,0,0.171899,"Missing"
D19-1226,Q15-1025,0,0.0752505,"(Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrkˇsi´c et al., 2016; Kim et al., 2016b). Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018) or lexical entailment (Nguyen et al., 2017; Vuli´c and Mrkˇsi´c, 2018) over other types of semantic association in the word vector space. The best-performing specialization models (cf. Mrkˇsi´c et al. 2017; Ponti et al. 2018) are executed as vector space post-processors. In short, these techniques force the distributional vectors to conform to external linguistic constraints (e.g., synonymy, meronymy, lexical entailment) extracted from structured external resources (e.g., WordNet, BabelNet) to emphasize the particular relation. As post-processors th"
D19-1226,K15-1026,1,0.801861,"ally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrkˇsi´c et al., 2016; Kim et al., 2016b). Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015;"
D19-1226,S17-2016,0,0.0528375,"Missing"
D19-1226,P18-1072,1,0.903026,"Missing"
D19-1226,P14-2089,0,0.035718,"d vectors; semantic specialization of such spaces for a particular lexicosemantic relation (e.g., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of"
D19-1226,D14-1161,0,0.211456,"Missing"
D19-1226,P18-1135,0,0.0126936,"ble 2. Several findings emerge from the results. First, as already confirmed in prior work (Vuli´c et al., 2018; Ponti et al., 2018), vectors specialized for semantic similarity are indeed important for DST: we observe improvements with all specialized vectors. The highest gains are observed with the full CSLRIPS model. This confirms two main intuitions: 1) our proposed specialization transfer via lexical induction in the target language is more robust than 15 Note that the original NBT framework in the English DST task has been recently surpassed by more intricate taskspecific architectures (Zhong et al., 2018; Ren et al., 2018), but its lightweight design coupled with its strong dependence on input word vectors still makes it a convenient means to evaluate the effects of different specialization methods. Lexical Simplification Lexical simplification (LS) aims to automatically replace complex words (i.e., specialized terms, words used less frequently and known to fewer speakers) with their simpler in-context synonyms: the simplified text must be grammatical and retain the meaning of the original text. Lexical simplification critically depends on discerning semantic similarity from other types of se"
D19-1226,W17-0228,0,0.123249,"Missing"
D19-1226,N18-1048,1,0.885485,"Missing"
D19-1226,N18-1103,1,0.913929,"Missing"
D19-1226,D17-1270,1,0.902499,"Missing"
D19-1226,P17-1006,1,0.89768,"Missing"
D19-1226,E17-1042,0,0.0520418,"Missing"
D19-1288,W18-5412,0,0.214596,"d stored it in the form of attribute–value features in publicly accessible databases (Croft, 2002; Dryer and Haspelmath, 2013). The usage of such features to inform neural NLP models is still scarce, partly because the evidence in favor of their effectiveness is mixed (Ponti et al., 2018, 2019). In this work, we propose a way to distantly supervise the model with this side information effectively. We extend our non-conditional language models outlined in §3 (BARE) to a series of variants conditioned on language-specific properties, inspired by Östling and Tiedemann (2017) and Platanios et al. (2018). A fundamental difference from these previous works, however, is that they learn such properties in an end-to-end fashion from the data in a joint multilingual learning setting. Obviously, this is not feasible for the zeroshot setting and unreliable for the few-shot setting. Rather, we represent languages with their typological feature vector, which we assume to be readily available both for both training and held-out languages. Let t` ∈ [0, 1]f be a vector of f typological features for language ` ∈ T t E. We reinterpret the conditional language models within the Bayesian framework by estimat"
D19-1288,N16-1161,0,0.0861779,"shing the problem to its most complex formulation, zero-shot inference, and in taking into account the largest sample of languages for language modeling to date. In addition to those considered in our work, there are also alternative methods to condition language models on features. Kalchbrenner and Blunsom (2013) used encoded features as additional biases in recurrent layers. Kiros et al. (2014) put forth a log-bilinear model that allows for a ‘multiplicative interaction’ between hidden representations and input features (such as images). With a similar device, but a different gating method, Tsvetkov et al. (2016) trained a phoneme-level joint multilingual model of words conditioned on typological features from Moran et al. (2014). The use of the Laplace method for neural transfer learning has been proposed by Kirkpatrick et al. (2017), inspired by synaptic consolidation in neuroscience, with the aim to avoid catastrophic forgetting. Kochurov et al. (2018) tackled the problem of continuous learning by approximating the posterior probabilities through stochastic variational inference. Ritter et al. (2018) substitute diagonal Laplace approximation with a Kronecker factored method, leading to better uncer"
D19-1288,Q19-1040,0,0.125021,"rld’s languages, we hope that these findings will help broaden the scope of applications for language technology. 1 Introduction With the success of recurrent neural networks and other black-box models on core NLP tasks, such as language modeling, researchers have turned their attention to the study of the inductive bias such neural models exhibit (Linzen et al., 2016; Marvin and Linzen, 2018; Ravfogel et al., 2018). A number of natural questions have been asked. For example, do recurrent neural language models learn syntax (Marvin and Linzen, 2018)? Do they map onto grammaticality judgments (Warstadt et al., 2019)? However, as Ravfogel et al. (2019) note, “[m]ost of the work so far has focused on English.” Moreover, these studies have almost always focused on training scenarios where a large number of in-language sentences are available. In this work, we aim to find a prior distribution over network parameters that generalize well to new human languages. The recent vein of research on the inductive biases of neural nets implicitly assumes a uniform (unnormalizable) prior over the space of neural network parameters (Ravfogel et al., 2019, inter alia). In contrast, we take a Bayesian-updating approach: F"
D19-1449,E17-1088,0,0.0317067,"ble 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces which scored above a PanLex-predefined threshold. As in prior work (Conneau et al., 2018a; Glavaš et al., 2019), we then reserve the 5K pairs created from the more frequent L1 words for training, while the next 2K pairs are used for test. Smaller training dictionaries (1K and 500 pairs) are created by again"
D19-1449,W13-3520,0,0.0499261,"LWEs for similar languages in the first place: we can harvest cheap supervision here, e.g., cognates. The main motivation behind unsupervised approaches is to support dissimilar and resourcepoor language pairs for which supervision cannot be guaranteed. Domain Differences. Finally, we also verify that UNSUPERVISED CLWEs still cannot account for domain differences when training monolingual vectors. We rely on the probing test of Søgaard et al. (2018): 300-dim fastText vectors are trained on 1.1M sentences on three corpora: 1) EuroParl.v7 (Koehn, 2005) (parliamentary proceedings); 2) Wikipedia (Al-Rfou et al., 2013), and 3) EMEA (Tiedemann, 2009) (medical), and BLI evaluation for three language pairs is conducted on standard MUSE BLI test sets (Conneau et al., 2018a). The results, summarized in Figure 4, reveal that UN SUPERVISED methods are able to yield a good solution only when there is no domain mismatch and for the pair with two most similar languages (English-Spanish), again questioning their robustness and portability to truly low-resource and more challenging setups. Weakly supervised methods (|D0 |= 500 or D0 seeded with identical strings), in contrast, yield good solutions for all setups. 5 Fur"
D19-1449,D18-1062,0,0.0195462,"seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their m"
D19-1449,2020.lrec-1.495,0,0.468497,"Missing"
D19-1449,P18-1073,0,0.0976736,"e induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dict"
D19-1449,J82-2005,0,0.673463,"Missing"
D19-1449,D16-1136,0,0.0311352,"tive agglutinative agglutinative agglutinative introflexive agglutinative isolating agglutinative agglutinative fusional fusional isolating agglutinative BG CA EO ET EU FI HE HU ID KA KO LT NO TH TR Table 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces which scored above a PanLex-predefined threshold. As in prior work (Conneau et al., 2018a; Glavaš et al.,"
D19-1449,E14-1049,0,0.0557935,"l., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019"
D19-1449,C10-3010,0,0.0486842,"c Uralic Austronesian Kartvelian Koreanic IE: Baltic IE: Germanic Kra-Dai Turkic fusional fusional agglutinative agglutinative agglutinative agglutinative introflexive agglutinative isolating agglutinative agglutinative fusional fusional isolating agglutinative BG CA EO ET EU FI HE HU ID KA KO LT NO TH TR Table 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces"
D19-1449,D18-1029,1,0.891637,"Missing"
D19-1449,W16-1614,0,0.118504,"et al. (2018) and further verified by Glavaš et al. (2019) and Doval et al. (2019), the language pair at hand can have a huge impact on CLWE induction: the adversarial method of Conneau et al. (2018a) often gets stuck in poor local optima and yields degenerate solutions for distant language pairs such as English-Finnish. More recent CLWE methods (Artetxe et al., 2018b; Mohiuddin and Joty, 2019) focus on mitigating this robustness issue. However, they still rely on one critical assumption which leads them to degraded performance for distant language pairs: they assume approximate isomorphism (Barone, 2016; Søgaard et al., 2018) between monolingual embedding spaces to learn the initial seed dictionary. In other words, they assume very similar geometric constellations between two monolingual spaces: due to the Zipfian phenomena in language (Zipf, 1949) such near-isomorphism can be satisfied only for similar languages and for similar domains used for training monolingual vectors. This property is reflected in the results reported in Table 3, the number of unsuccessful setups in Table 4, as well as later in Figure 4. For instance, the largest number of unsuccessful BLI setups with the UNSUPERVISED"
D19-1449,P19-1070,1,0.828323,"Missing"
D19-1449,Q17-1010,0,0.186535,"rature. These two properties will facilitate analyses between (dis)similar language pairs and offer a comprehensive set of evaluation setups that test the robustness and portability of fully unsupervised CLWEs. The final list of 15 diverse test languages is provided in Table 2, and includes samples from different languages types and families. We run BLI evaluations for all language pairs in both directions, for a total of 15×14=210 BLI setups. Monolingual Embeddings. We use the 300-dim vectors of Grave et al. (2018) for all 15 languages, pretrained on Common Crawl and Wikipedia with fastText (Bojanowski et al., 2017).7 We trim all 5 While BLI is an intrinsic task, as discussed by Glavaš et al. (2019) it is a strong indicator of CLWE quality also for downstream tasks: relative performance in the BLI task correlates well with performance in cross-lingual information retrieval (Litschko et al., 2018) or natural language inference (Conneau et al., 2018b). More importantly, it also provides a means to analyze whether a CLWE method manages to learn anything meaningful at all, and can indicate “unsuccessful” CLWE induction (e.g., when BLI performance is similar to a random baseline): detecting such CLWEs is espe"
D19-1449,D14-1082,0,0.0593674,"uage pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods. 1 Introduction and Motivation The wide use and success of monolingual word embeddings in NLP tasks (Turian et al., 2010; Chen and Manning, 2014) has inspired further research focus on the induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Lit"
D19-1449,N15-1157,0,0.0457001,"lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries t"
D19-1449,D18-1024,0,0.0746406,"typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can rough"
D19-1449,L18-1550,0,0.0283189,"nguage pairs and offer new evaluation data which extends and surpasses other work in the CLWE literature. These two properties will facilitate analyses between (dis)similar language pairs and offer a comprehensive set of evaluation setups that test the robustness and portability of fully unsupervised CLWEs. The final list of 15 diverse test languages is provided in Table 2, and includes samples from different languages types and families. We run BLI evaluations for all language pairs in both directions, for a total of 15×14=210 BLI setups. Monolingual Embeddings. We use the 300-dim vectors of Grave et al. (2018) for all 15 languages, pretrained on Common Crawl and Wikipedia with fastText (Bojanowski et al., 2017).7 We trim all 5 While BLI is an intrinsic task, as discussed by Glavaš et al. (2019) it is a strong indicator of CLWE quality also for downstream tasks: relative performance in the BLI task correlates well with performance in cross-lingual information retrieval (Litschko et al., 2018) or natural language inference (Conneau et al., 2018b). More importantly, it also provides a means to analyze whether a CLWE method manages to learn anything meaningful at all, and can indicate “unsuccessful” CL"
D19-1449,P15-1119,0,0.0322898,"transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui"
D19-1449,D18-1269,0,0.0727556,"eak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-tra"
D19-1449,N19-1188,1,0.675442,"Missing"
D19-1449,N18-2085,0,0.0180662,"nslations for a test set of source language words. Its lightweight nature allows us to conduct a comprehensive evaluation across a large number of language pairs.5 Since BLI is cast as a ranking task, following Glavaš et al. (2019) we use mean average precision (MAP) as the main evaluation metric: in our BLI setup with only one correct translation for each “query” word, MAP is equal to mean reciprocal rank (MRR).6 (Selection of) Language Pairs. Our selection of test languages is guided by the following goals: a) following recent initiatives in other NLP research (e.g., for language modeling) (Cotterell et al., 2018; Gerz et al., 2018), we aim to ensure the coverage of different genealogical and typological language properties, and b) we aim to analyze a large set of language pairs and offer new evaluation data which extends and surpasses other work in the CLWE literature. These two properties will facilitate analyses between (dis)similar language pairs and offer a comprehensive set of evaluation setups that test the robustness and portability of fully unsupervised CLWEs. The final list of 15 diverse test languages is provided in Table 2, and includes samples from different languages types and families."
D19-1449,E17-1102,1,0.919319,"Missing"
D19-1449,D18-1043,0,0.222539,"languages and language pairs. However, the first attempts at fully unsupervised CLWE induction failed exactly for these use cases, as shown by Søgaard et al. (2018). Therefore, the follow-up work aimed to improve the robustness of unsupervised CLWE induction by introducing more robust self-learning procedures (Artetxe et al., 2018b; Kementchedjhieva et al., 2018). Besides increased robustness, recent work claims that fully unsupervised projection-based CLWEs can even match or surpass their supervised counterparts (Conneau et al., 2018a; Artetxe et al., 2018b; Alvarez-Melis and Jaakkola, 2018; Hoshen and Wolf, 2018; Heyman et al., 2019). In this paper, we critically examine these claims on robustness and improved performance of unsupervised CLWEs by running a large-scale evaluation in the bilingual lexicon induction (BLI) task on 15 languages (i.e., 210 languages pairs, see Table 2 in §3). The languages were selected to represent different language families and morphological types, as we argue that fully unsupervised CLWEs have been designed to support exactly these setups. However, we show that even the most robust unsupervised CLWE method (Artetxe et al., 2018b) still fails for a large number of langu"
D19-1449,N19-1386,0,0.228386,"Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can roughly be described by three main components: C1) unsupervised extraction of a seed dictionary; C2) a"
D19-1449,kamholz-etal-2014-panlex,0,0.516415,"Kartvelian Koreanic IE: Baltic IE: Germanic Kra-Dai Turkic fusional fusional agglutinative agglutinative agglutinative agglutinative introflexive agglutinative isolating agglutinative agglutinative fusional fusional isolating agglutinative BG CA EO ET EU FI HE HU ID KA KO LT NO TH TR Table 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces which scored above a Pa"
D19-1449,D18-1047,0,0.0484142,"solute BLI scores for distant pairs (see Table 4 and results in the supplemental material). Unsupervised approaches even exploit the assumption twice as their seed extraction is fully based on the topological similarity. Future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (Zhang et al., 2019) by distinguishing between language-specific and language-pairinvariant subspaces; 2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of Nakashole (2018); 3) similar to Vuli´c and Korhonen (2016) and Lubin et al. (2019) “denoisify” seed lexicons during the self-learning procedure. For instance, keeping only mutual/symmetric nearest neighbour as in FULL + SL + SYM can be seen as a form of rudimentary denoisifying: it is indicative to see that the best overall performance in this work is reported with that model configuration. Further, the most important contributions of unsupervised CLWE models are, in fact, the improved and more robust self-learning procedures (component C2) and technical enhancements (component C3). In this work we have demon"
D19-1449,K18-1021,0,0.0602112,"an inherently interesting research topic per se. Nonetheless, the main practical motivation for developing such approaches in the first place is to facilitate the construction of multilingual NLP tools and widen the access to language technology for resource-poor languages and language pairs. However, the first attempts at fully unsupervised CLWE induction failed exactly for these use cases, as shown by Søgaard et al. (2018). Therefore, the follow-up work aimed to improve the robustness of unsupervised CLWE induction by introducing more robust self-learning procedures (Artetxe et al., 2018b; Kementchedjhieva et al., 2018). Besides increased robustness, recent work claims that fully unsupervised projection-based CLWEs can even match or surpass their supervised counterparts (Conneau et al., 2018a; Artetxe et al., 2018b; Alvarez-Melis and Jaakkola, 2018; Hoshen and Wolf, 2018; Heyman et al., 2019). In this paper, we critically examine these claims on robustness and improved performance of unsupervised CLWEs by running a large-scale evaluation in the bilingual lexicon induction (BLI) task on 15 languages (i.e., 210 languages pairs, see Table 2 in §3). The languages were selected to represent different language fam"
D19-1449,P19-1492,0,0.0933756,"Missing"
D19-1449,D18-1101,0,0.0167767,"t al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can roughly be described by three main components: C1) unsupe"
D19-1449,C12-1089,0,0.180369,"n a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and com"
D19-1449,2005.mtsummit-papers.11,0,0.0217419,"more, one could argue that we do not need unsupervised CLWEs for similar languages in the first place: we can harvest cheap supervision here, e.g., cognates. The main motivation behind unsupervised approaches is to support dissimilar and resourcepoor language pairs for which supervision cannot be guaranteed. Domain Differences. Finally, we also verify that UNSUPERVISED CLWEs still cannot account for domain differences when training monolingual vectors. We rely on the probing test of Søgaard et al. (2018): 300-dim fastText vectors are trained on 1.1M sentences on three corpora: 1) EuroParl.v7 (Koehn, 2005) (parliamentary proceedings); 2) Wikipedia (Al-Rfou et al., 2013), and 3) EMEA (Tiedemann, 2009) (medical), and BLI evaluation for three language pairs is conducted on standard MUSE BLI test sets (Conneau et al., 2018a). The results, summarized in Figure 4, reveal that UN SUPERVISED methods are able to yield a good solution only when there is no domain mismatch and for the pair with two most similar languages (English-Spanish), again questioning their robustness and portability to truly low-resource and more challenging setups. Weakly supervised methods (|D0 |= 500 or D0 seeded with identical"
D19-1449,P15-1165,0,0.159533,"Missing"
D19-1449,P18-1072,1,0.810814,"Missing"
D19-1449,D18-1549,0,0.0740195,"Missing"
D19-1449,P10-1040,0,0.0777116,"e languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods. 1 Introduction and Motivation The wide use and success of monolingual word embeddings in NLP tasks (Turian et al., 2010; Chen and Manning, 2014) has inspired further research focus on the induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vu"
D19-1449,P16-1024,1,0.918618,"Missing"
D19-1449,N19-1045,0,0.0138207,"the supplemental material). Unsupervised approaches even exploit the assumption twice as their seed extraction is fully based on the topological similarity. Future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (Zhang et al., 2019) by distinguishing between language-specific and language-pairinvariant subspaces; 2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of Nakashole (2018); 3) similar to Vuli´c and Korhonen (2016) and Lubin et al. (2019) “denoisify” seed lexicons during the self-learning procedure. For instance, keeping only mutual/symmetric nearest neighbour as in FULL + SL + SYM can be seen as a form of rudimentary denoisifying: it is indicative to see that the best overall performance in this work is reported with that model configuration. Further, the most important contributions of unsupervised CLWE models are, in fact, the improved and more robust self-learning procedures (component C2) and technical enhancements (component C3). In this work we have demonstrated that these components can be equally applied to weakly sup"
D19-1449,D17-1270,1,0.904042,"Missing"
D19-1449,N15-1104,0,0.525331,"Missing"
D19-1449,P19-1307,0,0.0605721,"tance, the underlying assumption of all projection-based methods (both supervised and unsupervised) is the topological similarity between monolingual spaces, which is why standard simple linear projections result in lower absolute BLI scores for distant pairs (see Table 4 and results in the supplemental material). Unsupervised approaches even exploit the assumption twice as their seed extraction is fully based on the topological similarity. Future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (Zhang et al., 2019) by distinguishing between language-specific and language-pairinvariant subspaces; 2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of Nakashole (2018); 3) similar to Vuli´c and Korhonen (2016) and Lubin et al. (2019) “denoisify” seed lexicons during the self-learning procedure. For instance, keeping only mutual/symmetric nearest neighbour as in FULL + SL + SYM can be seen as a form of rudimentary denoisifying: it is indicative to see that the best overall performance in this work is reported with that model configuration. F"
D19-1449,N16-1156,0,0.0208315,"tilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned seve"
D19-1449,D18-1022,1,0.928371,"ardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-ling"
E17-1016,W13-3520,0,0.0812908,"Missing"
E17-1016,W16-2519,0,0.0207043,"system and the ground truth ranking. This protocol, however, is not directly applicable to the USF test data. First, the evaluated relation of WA is asymmetric, and the pairs (X, Y ) and (Y, X) may differ dramatically in their WA scores (see the difference in FSG and BSG values from Tab. 1). Second, instead of one global list of pairs, the data comprises a series of ranked lists conditioned on the cue/normed word wc (see Tab. 1 again). Finally, unlike with SimLex999 or MEN scores where it is difficult to interpret “what a similarity/relatedness of 7.69 exactly means” (Batchkarov et al., 2016; Avraham and Goldberg, 2016), the USF FSG scores have a direct meaningful interpretation (i.e., F SG = #P/#G). To fully capture all aspects of the ground truth USF data set, an evaluation protocol should ideally be based not only on response rankings, but also on the actual scores, i.e., the association strength. In this paper, we propose and investigate two different families of evaluation metrics on the USF data: Sect. 3.1 discusses rank correlation evaluation metrics inspired by recent work on the evaluation of vector space models in distributional semantics (Bruni et al., 2014; Hill et al., 2015; Vuli´c et al., 165 2"
E17-1016,P14-1023,0,0.0648873,"t topic from the set of M topics induced from the corpus data (using LDA). We label this model LDA-assoc. The probability scores P (wr |toi ) select words that are highly descriptive for each particular topic. P (toi |wc ) scores are computed as in prior work, by assuming topic independence and applying Bayes’ rule on the LDA output per-topic word distributions P (·|toi ) (Steyvers and Griffiths, 2007; Vuli´c and Moens, 2013).10 We train LDA with 1,000 topics using suggested parameters (Griffiths et al., 2007). Count-Based Models We evaluate the best performing reduced count-based model from (Baroni et al., 2014). We label this model count-ppmi500d.11 For a more detailed description of the model’s training data and setup we refer the reader to the original work and supplementary material. Vector Space Models We also compare the performance of prominent representation models on the WA USF task. We include: (1) unsupervised models that learn from distributional information in text, including Glove (Pennington et al., 2014) with d = 50 and d = 300 dimensions (glove-6B-50d and glove-6B-300d), the skip-gram negative-sampling (SGNS) 300dimensional vectors (Mikolov et al., 2013) with various contexts (bow ="
E17-1016,W16-2502,0,0.0876675,"ntation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variability (Batchkarov et al., 2016) and does not account for the fact that human similarity judgements are asymmetric by nature (Tversky, 1977). What is more, given that humans perform linguistic comparison"
E17-1016,E14-1049,0,0.0405544,"re (Steyvers et al., 2004; Griffiths et al., 2007; Steyvers and Griffiths, 2007).9 The following quantitative model of word association has been proposed (Griffiths et al., 2007): 167 9 Griffiths et al. (2007) also experimented with LSA (Landauer and Dumais, 1997) and found that their LDA-based approach consistently outperformed LSA-based approaches. P (wr |wc ) = M X P (wr |toi )P (toi |wc ) paragram-300d, (Wieting et al., 2015)) further refined using linguistic constraints (paragram+cf300d, (Mrkˇsi´c et al., 2016)); (3) Multilingual embedding models from Luong et al. (2015) (biskip256d) and Faruqui and Dyer (2014) (bicca-512d). More detailed descriptions of all VSM models are available in the listed papers and supplementary material attached to this work. (10) i=1 where wc is a cue word, wr ∈ V r any concept from the search space, and toi is the ith latent topic from the set of M topics induced from the corpus data (using LDA). We label this model LDA-assoc. The probability scores P (wr |toi ) select words that are highly descriptive for each particular topic. P (toi |wc ) scores are computed as in prior work, by assuming topic independence and applying Bayes’ rule on the LDA output per-topic word dist"
E17-1016,N15-1184,0,0.085062,"Missing"
E17-1016,P06-1038,0,0.0664548,"Missing"
E17-1016,N13-1092,0,0.0711201,"Missing"
E17-1016,D16-1235,1,0.876283,"Missing"
E17-1016,W09-3207,0,0.142675,"Missing"
E17-1016,C16-1175,0,0.135609,"Missing"
E17-1016,D14-1032,1,0.858335,"fter post-processing, the repository contains ~5K queries, and ~70,000 (cue, response) pairs, making it one of the largest semantic evaluation databases available (by contrast, the largest word pair scoring data sets in NLP, SimVerb and MEN, contain 3,500 and 3,000 word pairs respectively). This new resource enables comprehensive quantitative studies of WA and may be used to guide the future development of representation learning architectures. While parts of the USF data set have been used for evaluation in NLP before (Michelbacher et al., 2007; Silberer and Lapata, 2012; Kiela et al., 2014; Hill and Korhonen, 2014, inter alia), we conduct the first full study regarding the evaluation on the quantitative WA task. We compare a wide variety of different semantic representation models, discuss various evaluation metrics and analyse the links between word association and semantic similarity and relatedness. In summary, the main contributions of this paper are as follows:3 (C1) We present an end-to-end evaluation framework for the WA task, and provide new evaluation metrics and detailed guidelines for evaluating semantic models on the WA task. (C2) We conduct a systematic study and comparison of current stat"
E17-1016,J15-4004,1,0.929697,"its direct analogy with information retrieval problems, (3) evaluate various state-of-the-art representation models on this task, and (4) discuss the relationship between WA and prior evaluations of semantic representation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces in"
E17-1016,W16-2513,0,0.248291,"imilarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variability (Batchkarov et al., 2016) and does not account for the fact that human similarity judgements are asymmetric by nature (Tversky, 1977). What is more, given that humans perform linguistic comparisons between concepts on a subc"
E17-1016,W14-1503,1,0.866108,"xp. IV. Exp. III: Window Size In the next experiment, we analysed the effect of the window size on models’ ability to capture similarity, relatedness, and association. We train the sgns-pw-bow model (d = 300) with varying window sizes in the interval [1, 30]. The results on similarity (SimLex-999), relatedness (MEN), and WA benchmarks (USF) are presented in Fig. 1(a)-1(b). It is clear that using larger windows deteriorates the performance on SimLex-999 as the focus of the model is shifted from functional to topical similarity. This shift has been detected in prior work on vector space models (Kiela and Clark, 2014). However, we also observe a similar trend with MEN scores, although an opposite effect was expected, which questions the ability of MEN to accurately evaluate relatedness. The opposite effect is, however, visible with the WA evaluation, where it is evident that larger win170 dows (leading to topical similarity) lead to better WA estimates. This also provides the first hint that WA and semantic similarity capture two completely distinct semantic phenomena. Exp. IV: WA vs. Similarity vs. Relatedness We delve deeper into this conjecture by computing correlations between model rankings on the WA"
E17-1016,P14-2135,1,0.843608,"son et al., 2004). After post-processing, the repository contains ~5K queries, and ~70,000 (cue, response) pairs, making it one of the largest semantic evaluation databases available (by contrast, the largest word pair scoring data sets in NLP, SimVerb and MEN, contain 3,500 and 3,000 word pairs respectively). This new resource enables comprehensive quantitative studies of WA and may be used to guide the future development of representation learning architectures. While parts of the USF data set have been used for evaluation in NLP before (Michelbacher et al., 2007; Silberer and Lapata, 2012; Kiela et al., 2014; Hill and Korhonen, 2014, inter alia), we conduct the first full study regarding the evaluation on the quantitative WA task. We compare a wide variety of different semantic representation models, discuss various evaluation metrics and analyse the links between word association and semantic similarity and relatedness. In summary, the main contributions of this paper are as follows:3 (C1) We present an end-to-end evaluation framework for the WA task, and provide new evaluation metrics and detailed guidelines for evaluating semantic models on the WA task. (C2) We conduct a systematic study and c"
E17-1016,D15-1242,1,0.871942,"Missing"
E17-1016,Y13-1013,0,0.0480397,"d investigate whether the difference originates from inadequate evaluation data and protocols (see Fig. 1(a)-1(b) again), or whether the difference is fundamental. 6 Conclusion and Future Work In future work, we plan to test the portability of the evaluation protocol and apply it to other repositories of word association data in English (De Deyne et al., 2016), as well as in other languages, using existing WA tables in, e.g., German (Schulte im Walde et al., 2008), Dutch (De Deyne and Storms, 2008; Brysbaert et al., 2014), Italian (Guida and Lenci, 2007), Japanese (Joyce, 2005), or Cantonese (Kwong, 2013).15 In another line of future work, we will experiment with other “cognitively plausible” evaluation data such as N400 (Kutas and Federmeier, 2011; Ettinger et al., 2016), and will analyse the similarities and differences between WA and other such “cognitive” evaluation protocols, as the one relying on semantic priming (SPP) (Hutchison et al., 2013; Ettinger and Linzen, 2016). All evaluation scripts and detailed guidelines related to this work are freely available at: github.com/cambridgeltl/wa-eval/ Acknowledgments This work is supported by ERC Consolidator Grant LEXICAL (no 648909). The auth"
E17-1016,P14-2050,0,0.0672836,"e detailed description of the model’s training data and setup we refer the reader to the original work and supplementary material. Vector Space Models We also compare the performance of prominent representation models on the WA USF task. We include: (1) unsupervised models that learn from distributional information in text, including Glove (Pennington et al., 2014) with d = 50 and d = 300 dimensions (glove-6B-50d and glove-6B-300d), the skip-gram negative-sampling (SGNS) 300dimensional vectors (Mikolov et al., 2013) with various contexts (bow = bag-of-words; deps = dependency contexts) as in (Levy and Goldberg, 2014) and (Schwartz et al., 2015) (sgns-pw-bow-w2, sgns-pw-bow-w5, sgns-pw-deps, sgns-8b-boww2), and the symmetric-pattern based vectors by Schwartz et al. (2015) (sympat-500d); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use vectors finetuned to a paraphrase database (paragram-25d, 10 The generative model closely resembles the actual process in the human brain (Griffiths et al., 2007) - when we generate responses, we first tend to associate that word with a related semantic/cognitive concept, i.e., a latent topic (the factor P (toi |wc )), and the"
E17-1016,W13-3512,0,0.0696612,"t capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variability (Batchkarov et al., 2016) and does not account for the fact that human similarity judgements are asymmetric by nature (Tversky, 1977). What is more, given that humans perform linguistic comparisons between concepts on a subconscious level (Kutas and Federmeier, 2011), it is at least debatable whether current similarity/relatedness evaluation sets fully capture the implicit relational structure underlying human language representation"
E17-1016,W15-1521,0,0.0259935,"ng, rooted in the psychology literature (Steyvers et al., 2004; Griffiths et al., 2007; Steyvers and Griffiths, 2007).9 The following quantitative model of word association has been proposed (Griffiths et al., 2007): 167 9 Griffiths et al. (2007) also experimented with LSA (Landauer and Dumais, 1997) and found that their LDA-based approach consistently outperformed LSA-based approaches. P (wr |wc ) = M X P (wr |toi )P (toi |wc ) paragram-300d, (Wieting et al., 2015)) further refined using linguistic constraints (paragram+cf300d, (Mrkˇsi´c et al., 2016)); (3) Multilingual embedding models from Luong et al. (2015) (biskip256d) and Faruqui and Dyer (2014) (bicca-512d). More detailed descriptions of all VSM models are available in the listed papers and supplementary material attached to this work. (10) i=1 where wc is a cue word, wr ∈ V r any concept from the search space, and toi is the ith latent topic from the set of M topics induced from the corpus data (using LDA). We label this model LDA-assoc. The probability scores P (wr |toi ) select words that are highly descriptive for each particular topic. P (toi |wc ) scores are computed as in prior work, by assuming topic independence and applying Bayes’ r"
E17-1016,P04-1003,0,0.0426851,"ystematic study and comparison of current state-of-the-art representation learning architectures on the WA task. (C3) We present a systematic quantitative analysis of the connections between the models’ performance on the subconscious WA task and their performance on benchmarking similarity and relatedness evaluation sets. 2 Motivation: Association and USF Implicit Cognitive Measures: Means of Semantic Evaluation? Several studies have shown clear correspondence between implicit cognitive measures (most notably semantic priming) and semantic relations encountered in vector space models (VSMs) (McDonald and Brew, 2004; Jones et al., 2006; Pad´o and Lapata, 2007; Herda˘gdelen et al., 2009), suggesting that some of the implicit relation structure in the human brain is already reflected in current statistical models of meaning. These findings encouraged Ettinger and Linzen (2016) to propose a preliminary evaluation framework based on semantic priming experiments (Meyer and Schvaneveldt, 1971).4 They demonstrate the feasibility of such an evaluation using a subconscious language processing task. They use the online database of the Semantic Priming Project (SPP), which compiles priming data for over 6,000 word"
E17-1016,N16-1118,0,0.0350172,"Missing"
E17-1016,N16-1018,0,0.0262075,"Missing"
E17-1016,J07-2002,0,0.137456,"Missing"
E17-1016,P15-2070,0,0.0485682,"Missing"
E17-1016,D14-1162,0,0.0961134,"and Moens, 2013).10 We train LDA with 1,000 topics using suggested parameters (Griffiths et al., 2007). Count-Based Models We evaluate the best performing reduced count-based model from (Baroni et al., 2014). We label this model count-ppmi500d.11 For a more detailed description of the model’s training data and setup we refer the reader to the original work and supplementary material. Vector Space Models We also compare the performance of prominent representation models on the WA USF task. We include: (1) unsupervised models that learn from distributional information in text, including Glove (Pennington et al., 2014) with d = 50 and d = 300 dimensions (glove-6B-50d and glove-6B-300d), the skip-gram negative-sampling (SGNS) 300dimensional vectors (Mikolov et al., 2013) with various contexts (bow = bag-of-words; deps = dependency contexts) as in (Levy and Goldberg, 2014) and (Schwartz et al., 2015) (sgns-pw-bow-w2, sgns-pw-bow-w5, sgns-pw-deps, sgns-8b-boww2), and the symmetric-pattern based vectors by Schwartz et al. (2015) (sympat-500d); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use vectors finetuned to a paraphrase database (paragram-25d, 10 The genera"
E17-1016,D15-1036,0,0.0358539,"with information retrieval problems, (3) evaluate various state-of-the-art representation models on this task, and (4) discuss the relationship between WA and prior evaluations of semantic representation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variabili"
E17-1016,K15-1026,0,0.0399438,"model’s training data and setup we refer the reader to the original work and supplementary material. Vector Space Models We also compare the performance of prominent representation models on the WA USF task. We include: (1) unsupervised models that learn from distributional information in text, including Glove (Pennington et al., 2014) with d = 50 and d = 300 dimensions (glove-6B-50d and glove-6B-300d), the skip-gram negative-sampling (SGNS) 300dimensional vectors (Mikolov et al., 2013) with various contexts (bow = bag-of-words; deps = dependency contexts) as in (Levy and Goldberg, 2014) and (Schwartz et al., 2015) (sgns-pw-bow-w2, sgns-pw-bow-w5, sgns-pw-deps, sgns-8b-boww2), and the symmetric-pattern based vectors by Schwartz et al. (2015) (sympat-500d); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use vectors finetuned to a paraphrase database (paragram-25d, 10 The generative model closely resembles the actual process in the human brain (Griffiths et al., 2007) - when we generate responses, we first tend to associate that word with a related semantic/cognitive concept, i.e., a latent topic (the factor P (toi |wc )), and then, after establishing the co"
E17-1016,D12-1130,0,0.0310874,"s (Nelson et al., 2000; Nelson et al., 2004). After post-processing, the repository contains ~5K queries, and ~70,000 (cue, response) pairs, making it one of the largest semantic evaluation databases available (by contrast, the largest word pair scoring data sets in NLP, SimVerb and MEN, contain 3,500 and 3,000 word pairs respectively). This new resource enables comprehensive quantitative studies of WA and may be used to guide the future development of representation learning architectures. While parts of the USF data set have been used for evaluation in NLP before (Michelbacher et al., 2007; Silberer and Lapata, 2012; Kiela et al., 2014; Hill and Korhonen, 2014, inter alia), we conduct the first full study regarding the evaluation on the quantitative WA task. We compare a wide variety of different semantic representation models, discuss various evaluation metrics and analyse the links between word association and semantic similarity and relatedness. In summary, the main contributions of this paper are as follows:3 (C1) We present an end-to-end evaluation framework for the WA task, and provide new evaluation metrics and detailed guidelines for evaluating semantic models on the WA task. (C2) We conduct a sy"
E17-1016,W16-2521,0,0.0197121,"ast debatable whether current similarity/relatedness evaluation sets fully capture the implicit relational structure underlying human language representation and understanding. As evidenced by recent workshops on evaluation of semantic representations1 , the community appears to recognise that current evaluation methods are inadequate. To fill in this gap, recent work has proposed using subconscious cognitive measures of semantic connection instead, as a proxy for measuring the ability of statistical models to tackle various problems in human language understanding (Ettinger and Linzen, 2016; Søgaard, 2016; Mandera et al., 2017). Motivated by these insights, this work proposes an evaluation framework based on the word association (WA) task, firmly rooted in and described by the psychology literature, e.g., Nelson et al. (2000) and Griffiths et al. (2007)2 . Word associations, provided as simple (cue, response) concept pairs, are naturally asymmetric: they tend to be given as a repository of ranked lists of concepts col1 E.g. RepEval, https://sites.google.com/site/repevalacl16/ The WA task is a free-association task, in which participants are asked to produce the first word that came into their"
E17-1016,D15-1243,0,0.0212972,"eval problems, (3) evaluate various state-of-the-art representation models on this task, and (4) discuss the relationship between WA and prior evaluations of semantic representation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variability (Batchkarov et al.,"
E17-1016,N13-1011,1,0.811134,"Missing"
I08-2107,A97-1052,0,0.0605696,"s to the class in question (we verified this according to the method described in (Korhonen et al., 2006)). As VALEX was designed to maximise coverage most test verbs had 1000-9000 occurrences in the lexicon. 3 Syntactic Features We employed as features distributions of SCFs specific to given verbs. We extracted them from the recent VALEX (Korhonen et al., 2006) lexicon which provides SCF frequency information for 6,397 English verbs. VALEX was acquired automatically from five large corpora and the Web (using up to 10,000 occurrences per verb) using the subcategorization acquisition system of Briscoe and Carroll (1997). The system incorporates RASP, a domainindependent robust statistical parser (Briscoe and Carroll, 2002), and a SCF classifier which identifies 163 verbal SCFs. The basic SCFs abstract over lexically-governed particles and prepositions and predicate selectional preferences. We used the noisy unfiltered version of VALEX which includes 33 SCFs per verb on average1 . Some are genuine SCFs but some express adjuncts (e.g. I sang in the party could be SCF PP). A lexical entry for each verb and SCF combination provides e.g. the frequency of the entry (in active and passive) in corpora, the POS tags"
I08-2107,briscoe-carroll-2002-robust,0,0.0268927,"). As VALEX was designed to maximise coverage most test verbs had 1000-9000 occurrences in the lexicon. 3 Syntactic Features We employed as features distributions of SCFs specific to given verbs. We extracted them from the recent VALEX (Korhonen et al., 2006) lexicon which provides SCF frequency information for 6,397 English verbs. VALEX was acquired automatically from five large corpora and the Web (using up to 10,000 occurrences per verb) using the subcategorization acquisition system of Briscoe and Carroll (1997). The system incorporates RASP, a domainindependent robust statistical parser (Briscoe and Carroll, 2002), and a SCF classifier which identifies 163 verbal SCFs. The basic SCFs abstract over lexically-governed particles and prepositions and predicate selectional preferences. We used the noisy unfiltered version of VALEX which includes 33 SCFs per verb on average1 . Some are genuine SCFs but some express adjuncts (e.g. I sang in the party could be SCF PP). A lexical entry for each verb and SCF combination provides e.g. the frequency of the entry (in active and passive) in corpora, the POS tags of verb tokens, the argument heads in argument positions, and the prepositions in PP slots. We experiment"
I08-2107,J01-3003,0,0.0320527,"ical information as a side-effect of the acquisition process. Most work on automatic classification has focussed on verbs which are typically the main predicates in sentences. Syntactic features have proved the most informative in verb classification. Experiments have been reported using both (i) deep syntactic features (e.g. subcategorization frames (SCFs)) extracted using parsers and subcategorisation acquisition systems (Schulte im Walde, 2000; Korhonen et al., 2003; Schulte im Walde, 2006) and (ii) shallow ones (e.g. NPs/PPs preceding/following verbs) extracted using taggers and chunkers (Merlo and Stevenson, 2001; Joanis et al., 2007). (i) correspond closely with features used for manual classification (Levin, 1993). They have proved successful in the classification of German (Schulte im Walde, 2006) and English verbs (Korhonen et al., 2003). Yet promising results have also been reported when using (ii) for English verb classification (Merlo and Stevenson, 2001; Joanis et al., 2007). This may indicate that (i) are optimal for the task when combined with additional syntactic information from (ii). We investigate this matter by experimenting with a new, rich feature set which incorporates information ab"
I08-2107,C00-2094,0,0.0209278,"ine learning technology, to automatically induce lexical classes from corpus data with promising accuracy (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006; Joanis et al., 2007). This research is interesting, since lexical classifications, when tailored to the application and domain in question, can provide an effective means to deal with a number of important NLP tasks (e.g. parsing, word sense disambiguation, semantic role labeling), as well as enhance performance in many applications (e.g. information extraction, question-answering, machine translation) (Dorr, 1997; Prescher et al., 2000; Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005). 769 Large-scale exploitation of lexical classes in realworld or domain-sensitive tasks has not been possible because existing manually built classifications are incomprehensive. They are expensive to extend and do not incorporate important statistical information about the likelihood of different classes for words. Automatic classification is a better alternative. It is cost-effective and gathers statistical information as a side-effect of the acquisition process. Most work on automatic classification has focussed on verbs which"
I08-2107,C00-2108,0,0.0920078,"Missing"
I08-2107,J06-2001,0,0.0529436,"rent classes for words. Automatic classification is a better alternative. It is cost-effective and gathers statistical information as a side-effect of the acquisition process. Most work on automatic classification has focussed on verbs which are typically the main predicates in sentences. Syntactic features have proved the most informative in verb classification. Experiments have been reported using both (i) deep syntactic features (e.g. subcategorization frames (SCFs)) extracted using parsers and subcategorisation acquisition systems (Schulte im Walde, 2000; Korhonen et al., 2003; Schulte im Walde, 2006) and (ii) shallow ones (e.g. NPs/PPs preceding/following verbs) extracted using taggers and chunkers (Merlo and Stevenson, 2001; Joanis et al., 2007). (i) correspond closely with features used for manual classification (Levin, 1993). They have proved successful in the classification of German (Schulte im Walde, 2006) and English verbs (Korhonen et al., 2003). Yet promising results have also been reported when using (ii) for English verb classification (Merlo and Stevenson, 2001; Joanis et al., 2007). This may indicate that (i) are optimal for the task when combined with additional syntactic in"
I08-2107,P03-1009,1,0.894268,"tion about the likelihood of different classes for words. Automatic classification is a better alternative. It is cost-effective and gathers statistical information as a side-effect of the acquisition process. Most work on automatic classification has focussed on verbs which are typically the main predicates in sentences. Syntactic features have proved the most informative in verb classification. Experiments have been reported using both (i) deep syntactic features (e.g. subcategorization frames (SCFs)) extracted using parsers and subcategorisation acquisition systems (Schulte im Walde, 2000; Korhonen et al., 2003; Schulte im Walde, 2006) and (ii) shallow ones (e.g. NPs/PPs preceding/following verbs) extracted using taggers and chunkers (Merlo and Stevenson, 2001; Joanis et al., 2007). (i) correspond closely with features used for manual classification (Levin, 1993). They have proved successful in the classification of German (Schulte im Walde, 2006) and English verbs (Korhonen et al., 2003). Yet promising results have also been reported when using (ii) for English verb classification (Merlo and Stevenson, 2001; Joanis et al., 2007). This may indicate that (i) are optimal for the task when combined wit"
I08-2107,korhonen-etal-2006-large,1,0.943489,"nto 48 broad and 192 fine-grained classes according to their participation in 79 alternations involving NP and PP complements. We selected 17 fine-grained classes and 12 member verbs per class (table 2) for experimentation. The small test set enabled us to evaluate our results thoroughly. The classes were selected to (i) include both syntactically and semantically similar and different classes (to vary the difficulty of the classification task), and to (ii) have enough member verbs whose predominant sense belongs to the class in question (we verified this according to the method described in (Korhonen et al., 2006)). As VALEX was designed to maximise coverage most test verbs had 1000-9000 occurrences in the lexicon. 3 Syntactic Features We employed as features distributions of SCFs specific to given verbs. We extracted them from the recent VALEX (Korhonen et al., 2006) lexicon which provides SCF frequency information for 6,397 English verbs. VALEX was acquired automatically from five large corpora and the Web (using up to 10,000 occurrences per verb) using the subcategorization acquisition system of Briscoe and Carroll (1997). The system incorporates RASP, a domainindependent robust statistical parser ("
I08-2107,W04-3213,0,\N,Missing
J13-2003,P10-1024,0,0.0334597,"Missing"
J13-2003,W03-1402,0,0.011637,"General-domain lexical resources often include information about metaphorical word senses, although unsystematically and without any accompanying semantic annotation. For example, WordNet2 (Fellbaum 1998) contains the comprehension sense of grasp, deﬁned as “get the meaning of something,” and the reading sense of skim, deﬁned as “read superﬁcially.” A great deal of metaphorical senses are absent from the current version of WordNet, however. A number of researchers have advocated the necessity of systematic inclusion and mark-up of metaphorical senses in such general¨ domain lexical resources (Alonge and Castelli 2003; Lonneker and Eilts 2004) and claim that this would be beneﬁcial for the computational modeling of metaphor. Metaphor processing systems could then either use this knowledge or be evaluated against it. ¨ Lonneker (2004) mapped the senses from EuroWordNet3 to the Hamburg Metaphor ¨ ¨ Database (Lonneker 2004; Reining and Lonneker-Rodman 2007) containing examples of metaphorical expressions in German and French. Currently no explicit information about metaphor is integrated into WordNet for English, however. Although consistent inclusion in WordNet is in principle possible for conventional metap"
J13-2003,andersen-etal-2008-bnc,0,0.0860829,"Missing"
J13-2003,N03-1003,0,0.0247979,"Missing"
J13-2003,P01-1008,0,0.111162,"Missing"
J13-2003,D08-1007,0,0.382018,"Missing"
J13-2003,E06-1042,0,0.245004,"Missing"
J13-2003,W02-1016,0,0.208896,"Missing"
J13-2003,P06-4020,0,0.335837,"Missing"
J13-2003,E03-1034,0,0.184107,"Missing"
J13-2003,N06-1003,0,0.0359736,"Missing"
J13-2003,E99-1042,0,0.140698,"Missing"
J13-2003,P06-2012,0,0.147165,"Missing"
J13-2003,J90-1003,0,0.263128,"Missing"
J13-2003,J07-4004,0,0.0410609,"Missing"
J13-2003,W09-1108,0,0.0380368,"Missing"
J13-2003,W09-1109,0,0.0249527,"Missing"
J13-2003,D09-1046,0,0.0183457,"Missing"
J13-2003,J91-1003,0,0.724298,"solving non-literal meanings via analogical comparisons, the development of a complete and computationally practical account of this phenomenon is a challenging and complex task. Despite the importance of metaphor for NLP systems dealing with semantic interpretation, its automatic processing has received little attention in contemporary NLP, and is far from being a solved problem. The majority of computational approaches to metaphor still exploit ideas articulated two or three decades ago (Wilks 1978; Lakoff and Johnson 1980). They often rely on task-speciﬁc hand-coded knowledge (Martin 1990; Fass 1991; Narayanan 1997, 1999; Barnden and Lee 2002; Feldman and Narayanan 2004; Agerri et al. 2007) and reduce the task to reasoning about a limited domain or a subset of phenomena (Gedigian et al. 2006; Krishnakumaran and Zhu 2007). So far there has been no robust statistical system operating on unrestricted text. State-of-the-art accurate parsing (Klein and Manning 2003; Briscoe, Carroll, and Watson 2006; Clark and Curran 2007), however, as well as recent work on computational lexical semantics (Schulte im Walde 2006; 303 Computational Linguistics Volume 39, Number 2 Mitchell and Lapata 2008; Davi"
J13-2003,J83-3004,0,0.520817,"given context. Selectional restrictions are the semantic constraints that a predicate places onto its arguments. Consider the following example. (17) a. My aunt always drinks her tea on the terrace. b. My car drinks gasoline. (Wilks 1978) The verb drink normally requires a grammatical subject of type ANIMATE and a grammatical object of type LIQUID, as in Example (17a). Therefore, drink taking a car as a subject in (17b) is an anomaly, which, according to Wilks, indicates a metaphorical use of drink. Although Wilks’s idea inspired a number of computational experiments on metaphor recognition (Fass and Wilks 1983; Fass 1991; Krishnakumaran and Zhu 2007), it is important to note that in practice this approach has a number of limitations. Firstly, there are other kinds of non-literalness or anomaly in language that cause a violation of semantic norm, such as metonymies. Thus the method would overgenerate. Secondly, there are kinds of metaphor that do not represent a violation of selectional restrictions (i.e., the approach may also undergenerate). This would happen, for example, when highly conventionalized metaphorical word senses are more frequent than the original literal senses. Due to their frequen"
J13-2003,W06-3506,0,0.716069,"e the importance of metaphor for NLP systems dealing with semantic interpretation, its automatic processing has received little attention in contemporary NLP, and is far from being a solved problem. The majority of computational approaches to metaphor still exploit ideas articulated two or three decades ago (Wilks 1978; Lakoff and Johnson 1980). They often rely on task-speciﬁc hand-coded knowledge (Martin 1990; Fass 1991; Narayanan 1997, 1999; Barnden and Lee 2002; Feldman and Narayanan 2004; Agerri et al. 2007) and reduce the task to reasoning about a limited domain or a subset of phenomena (Gedigian et al. 2006; Krishnakumaran and Zhu 2007). So far there has been no robust statistical system operating on unrestricted text. State-of-the-art accurate parsing (Klein and Manning 2003; Briscoe, Carroll, and Watson 2006; Clark and Curran 2007), however, as well as recent work on computational lexical semantics (Schulte im Walde 2006; 303 Computational Linguistics Volume 39, Number 2 Mitchell and Lapata 2008; Davidov, Reichart, and Rappoport 2009; Erk and McCarthy ´ S´eaghdha 2010) open up 2009; Sun and Korhonen 2009; Abend and Rappoport 2010; O many avenues for the creation of such a system. This is the n"
J13-2003,P93-1023,0,0.193506,"Missing"
J13-2003,W09-2905,0,0.0174715,"Missing"
J13-2003,J98-1002,0,0.203057,"Missing"
J13-2003,N06-1058,0,0.0156241,"Missing"
J13-2003,kingsbury-palmer-2002-treebank,0,0.530837,"Missing"
J13-2003,P03-1054,0,0.0074496,"Missing"
J13-2003,N10-1017,0,0.0115698,"Missing"
J13-2003,korhonen-etal-2006-large,1,0.429092,"Missing"
J13-2003,W03-1601,0,0.0832078,"Missing"
J13-2003,W07-0103,0,0.857218,"taphor for NLP systems dealing with semantic interpretation, its automatic processing has received little attention in contemporary NLP, and is far from being a solved problem. The majority of computational approaches to metaphor still exploit ideas articulated two or three decades ago (Wilks 1978; Lakoff and Johnson 1980). They often rely on task-speciﬁc hand-coded knowledge (Martin 1990; Fass 1991; Narayanan 1997, 1999; Barnden and Lee 2002; Feldman and Narayanan 2004; Agerri et al. 2007) and reduce the task to reasoning about a limited domain or a subset of phenomena (Gedigian et al. 2006; Krishnakumaran and Zhu 2007). So far there has been no robust statistical system operating on unrestricted text. State-of-the-art accurate parsing (Klein and Manning 2003; Briscoe, Carroll, and Watson 2006; Clark and Curran 2007), however, as well as recent work on computational lexical semantics (Schulte im Walde 2006; 303 Computational Linguistics Volume 39, Number 2 Mitchell and Lapata 2008; Davidov, Reichart, and Rappoport 2009; Erk and McCarthy ´ S´eaghdha 2010) open up 2009; Sun and Korhonen 2009; Abend and Rappoport 2010; O many avenues for the creation of such a system. This is the niche the presented work is int"
J13-2003,S01-1009,0,0.016863,"Missing"
J13-2003,P98-2127,0,0.122342,"Missing"
J13-2003,C88-1081,0,0.672595,"B3 0FD, UK. E-mail: {Ekaterina.Shutova, Simone.Teufel, Anna.Korhonen}@cl.cam.ac.uk. Submission received: 28 July 2011; revised submission received: 21 April 2012; accepted for publication: 31 May 2012. doi:10.1162/COLI a 00124 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 2 Metaphors arise when one concept is viewed in terms of the properties of another. Humans often use metaphor to describe abstract concepts through reference to more concrete or physical experiences. Some examples of metaphor include the following. (1) How can I kill a process? (Martin 1988) (2) Hillary brushed aside the accusations. (3) I invested myself fully in this research. (4) And then my heart with pleasure ﬁlls, And dances with the daffodils. (“I wandered lonely as a cloud,” William Wordsworth, 1804) Metaphorical expressions may take a great variety of forms, ranging from conventional metaphors, which we produce and comprehend every day, for example, those in Examples (1)–(3), to poetic and novel ones, such as Example (4). In metaphorical expressions, seemingly unrelated features of one concept are attributed to another concept. In Example (1), a computational process is"
J13-2003,J04-1002,0,0.937327,"but also those of nouns and adjectives. As opposed to previous approaches that modeled metaphorical reasoning starting from a hand-crafted description and applying it to explain the data, we aim to design a statistical model that captures regular patterns of metaphoricity in a large corpus and thus generalizes to unseen examples. Compared to labor-intensive manual efforts, this approach is more robust and, being nearly unsupervised, cost-effective. In contrast to previous statistical approaches, which addressed metaphors of a speciﬁc topic or did not consider linguistic metaphor at all (e.g., Mason 2004), the proposed method covers all metaphors in principle, can be applied to unrestricted text, and can be adapted to different domains and genres. 306 Shutova, Teufel, and Korhonen Statistical Metaphor Processing Our ﬁrst experiment is concerned with the identiﬁcation of metaphorical expressions in unrestricted text. Starting from a small set of metaphorical expressions, the system learns the analogies involved in their production in a minimally supervised way. It generalizes over the exempliﬁed analogies by means of verb and noun clustering (i.e., the identiﬁcation of groups of similar concept"
J13-2003,W02-0816,0,0.0401881,"Missing"
J13-2003,S07-1009,0,0.0372132,"Missing"
J13-2003,P79-1016,0,0.200731,"Missing"
J13-2003,C88-2088,0,0.631192,"Missing"
J13-2003,P08-1028,0,0.0173268,"Missing"
J13-2003,T87-1040,0,0.278653,"hrasing gold standard by asking human subjects (not previously exposed to system output) to produce their own literal paraphrases for metaphorical verbs. The system paraphrasing was then also evaluated against this gold standard. 2. Theoretical and Computational Background 2.1 Metaphor and Polysemy Theorists of metaphor distinguish between two kinds of metaphorical language: novel (or poetic) metaphors (i.e., those that are imaginative), and conventionalized metaphors 307 Computational Linguistics Volume 39, Number 2 (i.e., those that are used as a part of an ordinary discourse). According to Nunberg (1987), all metaphors emerge as novel, but over time they become part of general usage and their rhetorical effect vanishes, resulting in conventionalized metaphors. Following Orwell (1946), Nunberg calls such metaphors “dead” and claims that they are not psychologically distinct from literally used terms. The scheme described by Nunberg demonstrates how metaphorical associations capture patterns governing polysemy, namely, the capacity of a word to have multiple meanings. Over time some of the aspects of the target domain are added to the meaning of a term in the source domain, resulting in a (meta"
J13-2003,P10-1045,0,0.109223,"Missing"
J13-2003,P93-1024,0,0.173268,"Missing"
J13-2003,peters-peters-2000-lexicalised,0,0.461657,"Missing"
J13-2003,I05-5010,0,0.0684057,"Missing"
J13-2003,P07-1115,1,0.843382,"Missing"
J13-2003,W09-4204,0,0.0354842,"Missing"
J13-2003,W04-3219,0,0.100268,"Missing"
J13-2003,W07-0102,0,0.288064,"Missing"
J13-2003,D10-1114,0,0.0232497,"Missing"
J13-2003,P99-1014,0,0.479383,"Missing"
J13-2003,P10-1093,0,0.0529917,"Missing"
J13-2003,J06-2001,0,0.0415072,"Missing"
J13-2003,W03-1609,0,0.0627777,"Missing"
J13-2003,N10-1147,1,0.808786,"Missing"
J13-2003,C10-1113,1,0.830833,"Missing"
J13-2003,shutova-teufel-2010-metaphor,1,0.842088,"associated with the act of killing. In Example (2) Hillary is not literally cleaning the space by sweeping accusations. Instead, the accusations lose their validity in that situation, in other words Hillary rejects them. The verbs brush aside and reject both entail the resulting disappearance of their object, which is the shared salient property that makes it possible for this analogy to be lexically expressed as a metaphor. Characteristic of all areas of human activity (from poetic to ordinary to scientiﬁc) and thus of all types of discourse, metaphor becomes an important problem for NLP. As Shutova and Teufel (2010) have shown in an empirical study, the use of conventional metaphor is ubiquitous in natural language text (according to their data, on average every third sentence in general-domain text contains a metaphorical expression). This makes metaphor processing essential for automatic text understanding. For example, an NLP application which is unaware that a “leaked report” is a “disclosed report” and not, for example, a “wet report,” would fail further semantic processing of the piece of discourse in which this phrase appears. A system capable of recognizing and interpreting metaphorical expressio"
J13-2003,D09-1067,1,0.593576,"Missing"
J13-2003,D11-1095,1,0.868196,"Missing"
J13-2003,D11-1094,1,0.352036,"Missing"
J13-2003,C08-1119,0,0.183734,"Missing"
J13-2003,P09-2019,0,0.0327936,"Missing"
J13-2003,P08-1089,0,0.0260576,"Missing"
J13-2003,P09-1094,0,0.02327,"Missing"
J13-2003,N06-1057,0,0.0107955,"Missing"
J13-2003,W09-0208,0,\N,Missing
J13-2003,C98-2122,0,\N,Missing
J14-3005,W99-0901,0,0.158715,"y-based methods build in an assumption that the lexical hierarchy chosen is the universally “correct” one and they will not perform as well when faced with data that violates the hierarchy or contains unknown words. A further issue faced by these models is that the resources they rely on require significant effort to create and will not always be available to model data in a new language or a new domain. Resnik (1993) proposes a measure of associational strength between a predicate and WordNet classes based on the empirical distribution of words of each class (and their hyponyms) in a corpus. Abney and Light (1999) conceptualize the process of generating an argument for a predicate in terms of a Markovian random walk from the hierarchy’s root to a leaf node and choosing the word associated with that leaf node. Ciaramita and Johnson (2000) likewise treat WordNet as defining the structure of a probabilistic graphical model, in this case a Bayesian network. Li and Abe (1998) and Clark and Weir (2002) both describe models in which a predicate “cuts” the hierarchy at an appropriate level of generalization, such that all classes below the cut are considered appropriate arguments (whether observed in data or n"
J14-3005,D08-1007,0,0.0481401,"Missing"
J14-3005,S12-1023,0,0.0404612,"Missing"
J14-3005,P06-4020,0,0.0988249,"Missing"
J14-3005,E09-1013,0,0.0304074,"Px (z)Py (z) (61) z One could alternatively use similarities derived from probabilistic divergences such ´ S´eaghdha and as the Jensen–Shannon Divergence or the L1 distance (Lee 1999; O Copestake 2008). 3.6 Related Work As related earlier, non-Bayesian mixture or latent-variable approaches to co-occurrence modeling were proposed by Pereira, Tishby, and Lee (1993) and Rooth et al. (1999). Blitzer, Globerson, and Pereira (2005) describe a co-occurrence model based on a different kind of distributed latent-variable architecture similar to that used in the literature on neural language models. Brody and Lapata (2009) use the clustering effects of LDA to perform word sense induction. Vlachos, Korhonen, and Ghahramani (2009) use non-parametric Bayesian methods to cluster verbs according to their co-occurrences with subcategorization frames. Reisinger and Mooney (2010, 2011) have also investigated Bayesian methods for lexical semantics in a spirit similar to that adopted here. Reisinger and Mooney (2010) describe a “tiered clustering” model that, like L EX LDA, mixes a cluster-based preference model with a predicate-specific distribution over 609 Computational Linguistics Volume 40, Number 3 words; however,"
J14-3005,P10-1046,0,0.0140815,"lish Gigaword Corpus. Being an order of magnitude smaller, the BNC required less pruning; we divided all counts in the BNC/Win5 by 5 and left the BNC/Syn corpus unaltered. Type/token statistics for the resulting sets of observations are given in Table 4. 4.2 Evaluating Selectional Preference Models Various approaches have been suggested in the literature for evaluating selectional preference models. One popular method is “pseudo-disambiguation,” in which a system must distinguish between actually occurring and randomly generated predicate– argument combinations (Pereira, Tishby, and Lee 1993; Chambers and Jurafsky 2010). In a similar vein, probabilistic topic models are often evaluated by measuring the probability they assign to held-out data; held-out likelihood has also been used for evaluation in a task involving selectional preferences (Schulte im Walde et al. 2008). These two approaches take a “language modeling” approach in which model quality is identified with the ability to predict the distribution of co-occurrences in unseen text. Although this metric should certainly correlate with the semantic quality of the model, it may also be affected by frequency and other idiosyncratic aspects of language u"
J14-3005,C00-1028,0,0.118823,"Missing"
J14-3005,J02-2003,0,0.259264,"or a new domain. Resnik (1993) proposes a measure of associational strength between a predicate and WordNet classes based on the empirical distribution of words of each class (and their hyponyms) in a corpus. Abney and Light (1999) conceptualize the process of generating an argument for a predicate in terms of a Markovian random walk from the hierarchy’s root to a leaf node and choosing the word associated with that leaf node. Ciaramita and Johnson (2000) likewise treat WordNet as defining the structure of a probabilistic graphical model, in this case a Bayesian network. Li and Abe (1998) and Clark and Weir (2002) both describe models in which a predicate “cuts” the hierarchy at an appropriate level of generalization, such that all classes below the cut are considered appropriate arguments (whether observed in data or not) and all classes above the cut are considered inappropriate. In this article we focus on purely distributional models that do not rely on manually constructed lexical resources; therefore we do not revisit the models described in this section subsequently, except as a basis for empirical comparison. ´ S´eaghdha and Korhonen (2012) do investigate a number of Bayesian preference O model"
J14-3005,C65-1003,0,0.248588,"exposition. In Section 4 we indicate which experimental results have previously been reported. 588 ´ S´eaghdha and Korhonen O Probabilistic Distributional Semantics You shall know a word by the company it keeps. (Frith 1957) In Natural Language Processing (NLP), the term distributional semantics encompasses a broad range of methods that identify the semantic properties of a word or other linguistic unit with its patterns of co-occurrence in a corpus of textual data. The potential for learning semantic knowledge from text was recognized very early in the development of NLP (Sp¨arck Jones 1964; Cordier 1965; Harper 1965), but it is with the technological developments of the past twenty years that this data-driven approach to semantics has become dominant. Distributional approaches may use a representation based on vector spaces, on graphs, or (like this article) on probabilistic models, but they all share the common property of estimating their parameters from empirically observed co-occurrences. The basic unit of distributional semantics is the co-occurrence: an observation of a word appearing in a particular context. The definition is a general one: We may be interested in all kinds of words,"
J14-3005,D10-1113,0,0.0857955,"Missing"
J14-3005,P07-1028,0,0.033807,"ions. S(w1 , w2 ) is a set of comparison words that may depend on w1 or w2 , or neither: Essen and Steinbiss (1992) use the entire vocabulary, whereas Dagan, Lee, and Pereira use a fixed number of the most similar words to w2 , provided their similarity value is above a threshold t. While originally proposed for language modeling—the task of estimating the probability of a sequence of words—these methods require only trivial alteration to estimate co-occurrence probabilities for predicates and arguments, as was noted early on by Grishman and Sterling (1993) and Dagan, Lee, and Pereira (1999). Erk (2007) and Erk, Pado, ´ and Pado´ (2010) build on this prior work to develop an “exemplarbased” selectional preference model called EPP. In the EPP model, the set of comparison words is the set of words observed for the predicate p in the training corpus, denoted Seenargs(p): SelprefEPP (a|p) =  a ∈Seenargs(p) weight(a|p)sim(a , a)  a ∈Seenargs(p) weight(a |p)  (3) The co-occurrence strength weight(a|p) may simply be normalized co-occurrence frequency; alternatively a statistical association measure such as pointwise mutual information may be used. As before, sim(a, a ) may be any similari"
J14-3005,D08-1094,0,0.116935,"Missing"
J14-3005,J10-4007,0,0.0592267,"presents our experimental results on four data sets. Section 5 concludes and sketches promising research directions for the future. 2. Background and Related Work 2.1 Distributional Semantics The distributional approach to semantics is often traced back to the so-called “distributional hypothesis” put forward by mid-century linguists such as Zellig Harris and J.R. Frith: If we consider words or morphemes A and B to be more different in meaning than A and C, then we will often find that the distributions of A and B are more different than the distributions of A and C. (Harris 1954) ´ S´eaghdha (2010) and O ´ S´eaghdha and Korhonen (2011), 1 We build on previous work published in O adding new models and evaluation experiments as well as a comprehensive exposition. In Section 4 we indicate which experimental results have previously been reported. 588 ´ S´eaghdha and Korhonen O Probabilistic Distributional Semantics You shall know a word by the company it keeps. (Frith 1957) In Natural Language Processing (NLP), the term distributional semantics encompasses a broad range of methods that identify the semantic properties of a word or other linguistic unit with its patterns of co-occurrence in"
J14-3005,H92-1021,0,0.532925,"Missing"
J14-3005,P07-1035,0,0.0606253,"Missing"
J14-3005,J02-3001,0,0.0442459,"edictions of a computational model correlate with judgments collected from human behavioral data, the assumption is that the model itself shares some properties with human linguistic knowledge and is in some sense a “good” semantic model. More practically, NLP researchers have shown that selectional preference knowledge is useful for downstream applications, including metaphor detection (Shutova 2010), identification of non-compositional multiword 590 ´ S´eaghdha and Korhonen O Probabilistic Distributional Semantics expressions (McCarthy, Venkatapathy, and Joshi 2007), semantic role labeling (Gildea and Jurafsky 2002; Zapirain, Agirre, and M`arquez 2009; Zapirain et al. 2010), word sense disambiguation (McCarthy and Carroll 2003), and parsing (Zhou et al. 2011). 2.2.2 The “Counting” Approach. The simplest way to estimate the plausibility of a predicate–argument combination from a corpus is to count the number of times that combination appears, on the assumptions that frequency correlates with plausibility and that given enough data the resulting estimates will be relatively accurate. For example, Keller and Lapata (2003) estimate predicate–argument plausibilities by submitting appropriate queries to a Web"
J14-3005,D11-1129,0,0.0272539,"typically, some transformation of the raw counts. Mitchell and Lapata (2008, 2010) present a very general vector-space framework in which to consider the problem of combining the semantic representations of cooccurring words. Given pre-computed word vectors vw , vw , their combination p is provided by a function g that may also depend on syntax R and background knowledge K: p = g(vw , vw , R, K) (5) Mitchell and Lapata investigate a number of functions that instantiate Equation (5), finding that elementwise multiplication is a simple and consistently effective choice: pi = vwi · vw i 4 cf. Grefenstette and Sadrzadeh (2011), Socher et al. (2011) 594 (6) ´ S´eaghdha and Korhonen O Probabilistic Distributional Semantics The motivation for this “disambiguation by multiplication” is that lexical vectors are sparse and the multiplication operation has the effect of sending entries not supported in both vw and vw towards zero while boosting entries that have high weights in both vectors. The elementwise multiplication approach assumes that all word vectors are in the same space. For a syntactic co-occurrence model, this is often not the case: The contexts for a verb and a noun may have no dependency labels in common"
J14-3005,H93-1050,0,0.139733,"ction; Dagan, Lee, and Pereira (1999) investigate a number of options. S(w1 , w2 ) is a set of comparison words that may depend on w1 or w2 , or neither: Essen and Steinbiss (1992) use the entire vocabulary, whereas Dagan, Lee, and Pereira use a fixed number of the most similar words to w2 , provided their similarity value is above a threshold t. While originally proposed for language modeling—the task of estimating the probability of a sequence of words—these methods require only trivial alteration to estimate co-occurrence probabilities for predicates and arguments, as was noted early on by Grishman and Sterling (1993) and Dagan, Lee, and Pereira (1999). Erk (2007) and Erk, Pado, ´ and Pado´ (2010) build on this prior work to develop an “exemplarbased” selectional preference model called EPP. In the EPP model, the set of comparison words is the set of words observed for the predicate p in the training corpus, denoted Seenargs(p): SelprefEPP (a|p) =  a ∈Seenargs(p) weight(a|p)sim(a , a)  a ∈Seenargs(p) weight(a |p)  (3) The co-occurrence strength weight(a|p) may simply be normalized co-occurrence frequency; alternatively a statistical association measure such as pointwise mutual information may be u"
J14-3005,C65-1010,0,0.191709,"Section 4 we indicate which experimental results have previously been reported. 588 ´ S´eaghdha and Korhonen O Probabilistic Distributional Semantics You shall know a word by the company it keeps. (Frith 1957) In Natural Language Processing (NLP), the term distributional semantics encompasses a broad range of methods that identify the semantic properties of a word or other linguistic unit with its patterns of co-occurrence in a corpus of textual data. The potential for learning semantic knowledge from text was recognized very early in the development of NLP (Sp¨arck Jones 1964; Cordier 1965; Harper 1965), but it is with the technological developments of the past twenty years that this data-driven approach to semantics has become dominant. Distributional approaches may use a representation based on vector spaces, on graphs, or (like this article) on probabilistic models, but they all share the common property of estimating their parameters from empirically observed co-occurrences. The basic unit of distributional semantics is the co-occurrence: an observation of a word appearing in a particular context. The definition is a general one: We may be interested in all kinds of words, or only a part"
J14-3005,J03-3005,0,0.359533,"mantics expressions (McCarthy, Venkatapathy, and Joshi 2007), semantic role labeling (Gildea and Jurafsky 2002; Zapirain, Agirre, and M`arquez 2009; Zapirain et al. 2010), word sense disambiguation (McCarthy and Carroll 2003), and parsing (Zhou et al. 2011). 2.2.2 The “Counting” Approach. The simplest way to estimate the plausibility of a predicate–argument combination from a corpus is to count the number of times that combination appears, on the assumptions that frequency correlates with plausibility and that given enough data the resulting estimates will be relatively accurate. For example, Keller and Lapata (2003) estimate predicate–argument plausibilities by submitting appropriate queries to a Web search engine and counting the number of “hits” returned. To estimate the frequency with which the verb drink takes beer as a direct object, Keller and Lapata’s method uses the query <drink|drinks|drank|drunk|drinking a|the|∅ beer|beers>; to estimate the frequency with which tasty modifies pizza the query is simply <tasty pizza|pizzas>. Where desired, these joint frequency counts can be normalized by unigram hit counts to estimate conditional probabilities such as P(pizza|tasty). The main advantages of this"
J14-3005,P99-1004,0,0.562352,"and Pantel 2010). Although it is natural to view the aggregate of co-occurrence counts for a word as constituting a vector, it is equally natural to view it as defining a probability distribution. When normalized to have unit sum, fw parameterizes a discrete distribution giving the conditional probability of observing a particular context given that we observe w. The contents of the vector-space modeler’s toolkit generally have probabilistic analogs: similarity and dissimilarity can be computed using measures from information theory such as the Kullback–Leibler or Jensen–Shannon divergences (Lee 1999); the effects of clustering and dimensionality reduction can be achieved through the use of latent variable models (see Section 3.2.2). Additionally, Bayesian priors on parameter distributions provide a flexible toolbox for performing regularization and incorporating prior information in learning. A further advantage of the probabilistic framework is that it is often straightforward to extend existing models to account for additional structure in the data, or to tie together parameters for shared statistical strength, while maintaining guarantees of well-normalized behavior thanks to the laws"
J14-3005,J98-2002,0,0.0557603,"ata in a new language or a new domain. Resnik (1993) proposes a measure of associational strength between a predicate and WordNet classes based on the empirical distribution of words of each class (and their hyponyms) in a corpus. Abney and Light (1999) conceptualize the process of generating an argument for a predicate in terms of a Markovian random walk from the hierarchy’s root to a leaf node and choosing the word associated with that leaf node. Ciaramita and Johnson (2000) likewise treat WordNet as defining the structure of a probabilistic graphical model, in this case a Bayesian network. Li and Abe (1998) and Clark and Weir (2002) both describe models in which a predicate “cuts” the hierarchy at an appropriate level of generalization, such that all classes below the cut are considered appropriate arguments (whether observed in data or not) and all classes above the cut are considered inappropriate. In this article we focus on purely distributional models that do not rely on manually constructed lexical resources; therefore we do not revisit the models described in this section subsequently, except as a basis for empirical comparison. ´ S´eaghdha and Korhonen (2012) do investigate a number of B"
J14-3005,D07-1072,0,0.026751,"ictive one-word window or a notion of syntax that ignores lexical or dependency-label effects; for example, knowing that the head of a noun is a verb is far less informative than knowing that the noun is the direct object of eat. More generally, there is a connection between the models developed here and latent-variable models used for parsing (e.g., Petrov et al. 2006). In such models each latent state corresponds to a “splitting” of a part-of-speech label so as to produce a finer-grained grammar and tease out intricacies of word–rule “co-occurrence.” Finkel, Grenager, and Manning (2007) and Liang et al. (2007) propose a non-parametric Bayesian treatment of state splitting. This is very similar to the motivation behind an LDA-style selectional preference model. One difference is that the parsing model must explain the parse tree structure as well as the choice of lexical items; another is that in the selectional preference models described here each head–dependent relation is treated as an independent observation (though this could be changed). These differences allow our selectional preference models to be trained efficiently on large corpora and, by focusing on lexical choice rather than syntax, t"
J14-3005,J03-4004,0,0.0478889,"is that the model itself shares some properties with human linguistic knowledge and is in some sense a “good” semantic model. More practically, NLP researchers have shown that selectional preference knowledge is useful for downstream applications, including metaphor detection (Shutova 2010), identification of non-compositional multiword 590 ´ S´eaghdha and Korhonen O Probabilistic Distributional Semantics expressions (McCarthy, Venkatapathy, and Joshi 2007), semantic role labeling (Gildea and Jurafsky 2002; Zapirain, Agirre, and M`arquez 2009; Zapirain et al. 2010), word sense disambiguation (McCarthy and Carroll 2003), and parsing (Zhou et al. 2011). 2.2.2 The “Counting” Approach. The simplest way to estimate the plausibility of a predicate–argument combination from a corpus is to count the number of times that combination appears, on the assumptions that frequency correlates with plausibility and that given enough data the resulting estimates will be relatively accurate. For example, Keller and Lapata (2003) estimate predicate–argument plausibilities by submitting appropriate queries to a Web search engine and counting the number of “hits” returned. To estimate the frequency with which the verb drink take"
J14-3005,D07-1039,0,0.0605062,"Missing"
J14-3005,P08-1028,0,0.208893,"they are applicable to evaluation scenarios that involve representing binary co-occurrences. 2.3.2 Vector-Space Models. As described in Section 2.1, the vector-space approach to distributional semantics casts word meanings as vectors of real numbers and uses linear algebra operations to compare and combine these vectors. A word w is represented by a vector vw that models aspects of its distribution in the training corpus; the elements of this vector may be co-occurrence counts (in which case it is the same as the frequency vector fw ) or, more typically, some transformation of the raw counts. Mitchell and Lapata (2008, 2010) present a very general vector-space framework in which to consider the problem of combining the semantic representations of cooccurring words. Given pre-computed word vectors vw , vw , their combination p is provided by a function g that may also depend on syntax R and background knowledge K: p = g(vw , vw , R, K) (5) Mitchell and Lapata investigate a number of functions that instantiate Equation (5), finding that elementwise multiplication is a simple and consistently effective choice: pi = vwi · vw i 4 cf. Grefenstette and Sadrzadeh (2011), Socher et al. (2011) 594 (6) ´ S´eaghdha"
J14-3005,D10-1020,0,0.0655508,"Missing"
J14-3005,P10-1045,1,0.917208,"Missing"
J14-3005,C08-1082,1,0.86626,"Missing"
J14-3005,D11-1097,1,0.894016,"ults on four data sets. Section 5 concludes and sketches promising research directions for the future. 2. Background and Related Work 2.1 Distributional Semantics The distributional approach to semantics is often traced back to the so-called “distributional hypothesis” put forward by mid-century linguists such as Zellig Harris and J.R. Frith: If we consider words or morphemes A and B to be more different in meaning than A and C, then we will often find that the distributions of A and B are more different than the distributions of A and C. (Harris 1954) ´ S´eaghdha (2010) and O ´ S´eaghdha and Korhonen (2011), 1 We build on previous work published in O adding new models and evaluation experiments as well as a comprehensive exposition. In Section 4 we indicate which experimental results have previously been reported. 588 ´ S´eaghdha and Korhonen O Probabilistic Distributional Semantics You shall know a word by the company it keeps. (Frith 1957) In Natural Language Processing (NLP), the term distributional semantics encompasses a broad range of methods that identify the semantic properties of a word or other linguistic unit with its patterns of co-occurrence in a corpus of textual data. The potentia"
J14-3005,S12-1025,1,0.712017,"Bayesian network. Li and Abe (1998) and Clark and Weir (2002) both describe models in which a predicate “cuts” the hierarchy at an appropriate level of generalization, such that all classes below the cut are considered appropriate arguments (whether observed in data or not) and all classes above the cut are considered inappropriate. In this article we focus on purely distributional models that do not rely on manually constructed lexical resources; therefore we do not revisit the models described in this section subsequently, except as a basis for empirical comparison. ´ S´eaghdha and Korhonen (2012) do investigate a number of Bayesian preference O models that incorporate WordNet classes and structure, finding that such models outperform previously proposed WordNet-based models and perform comparably to the distributional Bayesian models presented here. 2.3 Measuring Similarity in Context 2.3.1 Motivation. A fundamental idea in semantics is that the meaning of a word is disambiguated and modulated by the context in which it appears. The word body clearly has a different sense in each of the following text fragments: 1. Depending on the present position of the planetary body in its orbital"
J14-3005,J07-2002,0,0.181591,"Missing"
J14-3005,N07-1071,0,0.0348887,"nguistic knowledge is the predicate, by which we mean a word or other symbol that combines with one or more arguments to produce a composite representation with a composite meaning (by the principle of compositionality). The archetypal predicate is a verb; for example, transitive drink takes two noun arguments as subject and object, with which it combines to form a basic sentence. However, the concept is a general one, encompassing other word classes as well as more abstract items such as semantic relations (Yao et al. 2011), semantic frames (Erk, Pado, ´ and Pado´ 2010), and inference rules (Pantel et al. 2007). The asymmetric distinction between predicate and argument is analogous to that between context and word in the more general distributional framework. It is intuitive that a particular predicate will be more compatible with some semantic argument classes than with others. For example, the subject of drink is typically an animate entity (human or animal) and the object of drink is typically a beverage. The subject of eat is also typically an animate entity but its object is typically a foodstuff. The noun modified by the adjective tasty is also typically a foodstuff, whereas the noun modified"
J14-3005,P93-1024,0,0.6988,"Missing"
J14-3005,P06-1055,0,0.00750652,"ctic topic model” that makes topic selection conditional on both the document’s topic distribution and on the topic of the word’s parent in a dependency tree. Although these models do represent a form of local context, they either use a very restrictive one-word window or a notion of syntax that ignores lexical or dependency-label effects; for example, knowing that the head of a noun is a verb is far less informative than knowing that the noun is the direct object of eat. More generally, there is a connection between the models developed here and latent-variable models used for parsing (e.g., Petrov et al. 2006). In such models each latent state corresponds to a “splitting” of a part-of-speech label so as to produce a finer-grained grammar and tease out intricacies of word–rule “co-occurrence.” Finkel, Grenager, and Manning (2007) and Liang et al. (2007) propose a non-parametric Bayesian treatment of state splitting. This is very similar to the motivation behind an LDA-style selectional preference model. One difference is that the parsing model must explain the parse tree structure as well as the choice of lexical items; another is that in the selectional preference models described here each head–de"
J14-3005,D10-1114,0,0.0172977,"esian mixture or latent-variable approaches to co-occurrence modeling were proposed by Pereira, Tishby, and Lee (1993) and Rooth et al. (1999). Blitzer, Globerson, and Pereira (2005) describe a co-occurrence model based on a different kind of distributed latent-variable architecture similar to that used in the literature on neural language models. Brody and Lapata (2009) use the clustering effects of LDA to perform word sense induction. Vlachos, Korhonen, and Ghahramani (2009) use non-parametric Bayesian methods to cluster verbs according to their co-occurrences with subcategorization frames. Reisinger and Mooney (2010, 2011) have also investigated Bayesian methods for lexical semantics in a spirit similar to that adopted here. Reisinger and Mooney (2010) describe a “tiered clustering” model that, like L EX LDA, mixes a cluster-based preference model with a predicate-specific distribution over 609 Computational Linguistics Volume 40, Number 3 words; however, their model does not encourage sharing of classes between different predicates. Reisinger and Mooney (2011) propose a very interesting variant of the latentvariable approach in which different kinds of contextual behavior can be explained by different “"
J14-3005,D11-1130,0,0.013584,"honen, and Ghahramani (2009) use non-parametric Bayesian methods to cluster verbs according to their co-occurrences with subcategorization frames. Reisinger and Mooney (2010, 2011) have also investigated Bayesian methods for lexical semantics in a spirit similar to that adopted here. Reisinger and Mooney (2010) describe a “tiered clustering” model that, like L EX LDA, mixes a cluster-based preference model with a predicate-specific distribution over 609 Computational Linguistics Volume 40, Number 3 words; however, their model does not encourage sharing of classes between different predicates. Reisinger and Mooney (2011) propose a very interesting variant of the latentvariable approach in which different kinds of contextual behavior can be explained by different “views,” each of which has its own distribution over latent variables; this model can give more interpretable classes than LDA for higher settings of |Z|. Some extensions of the LDA topic model incorporate local as well as document context to explain lexical choice. Griffiths et al. (2004) combine LDA and a hidden Markov model (HMM) in a single model structure, allowing each word to be drawn from either the document’s topic distribution or a latent HM"
J14-3005,P10-1044,0,0.225199,"Missing"
J14-3005,P99-1014,0,0.411377,"nce data in the context of noun clustering by Pereira, Tishby, and Lee (1993). They suggest a factorization of a noun n’s distribution over verbs v as P(v|n) =  P(v|z)P(z|n) (24) z which is equivalent to Equation (23) when we take n as the predicate and v as the argument, in effect defining an inverse selectional preference model. Pereira, Tishby, & Lee also observe that given certain assumptions Equation (24) can be written more symmetrically as P(v, n) =  P(v|z)P(n|z)P(z) (25) z The distributions P(v|z), P(n|z), and P(z) are estimated by an optimization procedure based on Maximum Entropy. Rooth et al. (1999) propose a much simpler Expectation Maximization (EM) procedure for estimating the parameters of Equation (25). 599 Computational Linguistics Volume 40, Number 3 3.3 Bayesian Models for Binary Co-occurrences Combining the latent variable co-occurrence model (23) with the use of Dirichlet priors naturally leads to Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003). Often described as a “topic model,” LDA is a model of document content that assumes each document is generated from a mixture of multinomial distributions or “topics.” Topics are shared across documents and correspond to t"
J14-3005,P08-1057,0,0.0755145,"th weight(a|p) may simply be normalized co-occurrence frequency; alternatively a statistical association measure such as pointwise mutual information may be used. As before, sim(a, a ) may be any similarity measure defined on members of A. One advantage of this and other similarity-based models is that the corpus used to estimate similarity need not be the same as that used to estimate predicate–argument co-occurrence, which is useful when the corpus labeled with these co-occurrences is small (e.g., a corpus labeled with FrameNet frames). 2.2.4 Discriminative Models. Bergsma, Lin, and Goebel (2008) cast selectional preference acquisition as a supervised learning problem to which a discriminatively trained classifier such as a Support Vector Machine (SVM) can be applied. To produce training data for a predicate, they pair “positive” arguments that were observed for that predicate in the training corpus and have an association with that predicate above a specified threshold (measured by mutual information) with randomly selected “negative” arguments of similar frequency that do not occur with the predicate or fall below the association threshold. Given this training data, a classifier can"
J14-3005,N10-1147,0,0.0506824,"Missing"
J14-3005,P06-1124,0,0.344668,"behavior to the Dirichlet distribution prior but is “non-parametric” in the sense of varying the size of its support according to the data; in the context of mixture modeling, a Dirichlet process prior allows the number of mixture components to be learned rather than fixed in advance. The Pitman–Yor process is a generalization of the Dirichlet process that is better suited to learning powerlaw distributions. This makes it particularly suitable for language modeling where the Dirichlet distribution or Dirichlet process would not produce a long enough tail due to their preference for sparsity (Teh 2006). On the other hand, Dirichlet-like behavior may be preferable in semantic modeling, where we expect, for example, predicate–class and class–argument distributions to be sparse. 3.2.2 The Latent Variable Assumption. In probabilistic modeling, latent variables are random variables whose values are not provided by the input data. As a result, their 598 ´ S´eaghdha and Korhonen O Probabilistic Distributional Semantics values must be inferred at the same time as the model parameters on the basis of the training data and model structure. The latent variable concept is a very general one that is use"
J14-3005,P10-1097,0,0.0812861,"Missing"
J14-3005,I11-1127,0,0.235408,"Missing"
J14-3005,P11-1145,0,0.0186752,"nt of state splitting. This is very similar to the motivation behind an LDA-style selectional preference model. One difference is that the parsing model must explain the parse tree structure as well as the choice of lexical items; another is that in the selectional preference models described here each head–dependent relation is treated as an independent observation (though this could be changed). These differences allow our selectional preference models to be trained efficiently on large corpora and, by focusing on lexical choice rather than syntax, to home in on purely semantic information. Titov and Klementiev (2011) extend the idea of latent-variable distributional modeling to do “unsupervised semantic parsing” and reason about classes of semantically similar lexicalized syntactic fragments. 4. Experiments 4.1 Training Corpora In our experiments we use two training corpora: BNC the written component of the British National Corpus,9 comprising around 90 million words. The corpus was tagged for part of speech, lemmatized, and parsed with the RASP toolkit (Briscoe, Carroll, and Watson 2006). 9 http://www.natcorp.ox.ac.uk/. 610 ´ S´eaghdha and Korhonen O Probabilistic Distributional Semantics C OORDINATION :"
J14-3005,W09-0210,1,0.847652,"Missing"
J14-3005,J05-4002,0,0.0163646,"|, where C is the vocabulary of contexts. As such, fw is amenable to computations known from linear algebra. We can compare co-occurrence vectors for different words with a similarity function such as the cosine measure or a dissimilarity function such as Euclidean distance; we can cluster neighboring vectors; we can project a matrix of co-occurrence counts onto a low-dimensional subspace; and so on. This is perhaps the most popular approach to distributional semantics and there are many good general overviews covering the possibilities and applications of the vector space model (Curran 2003; Weeds and Weir 2005; Pado´ and Lapata 2007; Turney and Pantel 2010). Although it is natural to view the aggregate of co-occurrence counts for a word as constituting a vector, it is equally natural to view it as defining a probability distribution. When normalized to have unit sum, fw parameterizes a discrete distribution giving the conditional probability of observing a particular context given that we observe w. The contents of the vector-space modeler’s toolkit generally have probabilistic analogs: similarity and dissimilarity can be computed using measures from information theory such as the Kullback–Leibler"
J14-3005,D11-1135,0,0.0254556,"40, Number 3 2.2 Selectional Preferences 2.2.1 Motivation. A fundamental concept in linguistic knowledge is the predicate, by which we mean a word or other symbol that combines with one or more arguments to produce a composite representation with a composite meaning (by the principle of compositionality). The archetypal predicate is a verb; for example, transitive drink takes two noun arguments as subject and object, with which it combines to form a basic sentence. However, the concept is a general one, encompassing other word classes as well as more abstract items such as semantic relations (Yao et al. 2011), semantic frames (Erk, Pado, ´ and Pado´ 2010), and inference rules (Pantel et al. 2007). The asymmetric distinction between predicate and argument is analogous to that between context and word in the more general distributional framework. It is intuitive that a particular predicate will be more compatible with some semantic argument classes than with others. For example, the subject of drink is typically an animate entity (human or animal) and the object of drink is typically a beverage. The subject of eat is also typically an animate entity but its object is typically a foodstuff. The noun"
J14-3005,C00-2137,0,0.0836838,"ween their topic distributions P(z|n) and the target word topic distribution P(z|o). No Similarity A model that ranks substitutes n by their context-conditioned probability P(n|C) only; this is essentially a language-modeling approach using syntactic “bigrams.” We report baseline results for the T ↔ C syntactic model, but performance is similar with other co-occurrence types. Predictions for the LDA models are averaged over five runs for each setting of |Z| in the range {600, 800, 1000, 1200}. In order to test statistical significance of differences between models we use stratified shuffling (Yeh 2000).23 4.5.3 Results. Table 15 presents results on the Lexical Substitution Task data set.24 As expected, the window-based LDA models attain good coverage but worse performance than the syntactic models. The combined model Win5 + T ↔ C trained on BNC+WIKI gives the best scores (GAP = 49.5, τb = 0.23). Every combined model gives a statistically significant improvement (p < 0.01) over the corresponding window-based Win5 model. Our TFP11 reimplementation of Thater, Furstenau, ¨ and Pinkal (2011) has slightly less than complete coverage, and performs worse than almost all combined LDA models. To comp"
J14-3005,P09-2019,0,0.0397333,"Missing"
J14-3005,N10-1058,0,0.0354032,"Missing"
J14-3005,P11-1156,0,0.157495,"erties with human linguistic knowledge and is in some sense a “good” semantic model. More practically, NLP researchers have shown that selectional preference knowledge is useful for downstream applications, including metaphor detection (Shutova 2010), identification of non-compositional multiword 590 ´ S´eaghdha and Korhonen O Probabilistic Distributional Semantics expressions (McCarthy, Venkatapathy, and Joshi 2007), semantic role labeling (Gildea and Jurafsky 2002; Zapirain, Agirre, and M`arquez 2009; Zapirain et al. 2010), word sense disambiguation (McCarthy and Carroll 2003), and parsing (Zhou et al. 2011). 2.2.2 The “Counting” Approach. The simplest way to estimate the plausibility of a predicate–argument combination from a corpus is to count the number of times that combination appears, on the assumptions that frequency correlates with plausibility and that given enough data the resulting estimates will be relatively accurate. For example, Keller and Lapata (2003) estimate predicate–argument plausibilities by submitting appropriate queries to a Web search engine and counting the number of “hits” returned. To estimate the frequency with which the verb drink takes beer as a direct object, Kelle"
J14-3005,D09-1001,0,\N,Missing
J14-3005,P92-1053,0,\N,Missing
J15-4004,P14-2131,0,0.0210427,"Missing"
J15-4004,P14-1023,0,0.903009,"Missing"
J15-4004,J10-4006,0,0.00593156,"hine translation systems, which aim to define mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established application (He et al. 2008; Marton, Callison-Burch, and Resnik 2009). Moreover, since, as we establish, similarity is a cognitively complex operation that can require rich, structured conceptual knowledge to compute accurately, similarity estimation constitutes an effective proxy evaluation for general-purpose representation-learning models whose ultimate application is variable or unknown (Collobert and Weston 2008; Baroni and Lenci 2010). As we show in Section 2, the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity. In particular, in both WS353 and MEN, pairs of words with associated meaning, such as coffee and cup (rating = 6.810), telephone and communication (7.510), or movie and theater (7.710), receive a high rating regardless of whether or not their constituents are similar. Thus, the utility of such resources to the development and application of similarity models is limited, a problem exacerbated by the fact that many researchers appear unaware of what"
J15-4004,W14-2402,0,0.0140987,"Missing"
J15-4004,P13-2010,0,0.0350814,"Missing"
J15-4004,P06-4018,0,0.0582656,"Missing"
J15-4004,P12-1015,0,0.639717,"rhonen}@ cl.cam.ac.uk. ∗∗ Technion, Israel Institute of Technology, Haifa, Israel. E-mail: roiri@ie.technion.ac.il. Submission received: 25 July 2014; revised submission received: 10 June 2015; accepted for publication: 31 August 2015. doi:10.1162/COLI a 00237 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 4 cup are rated as more “similar” than pairs such as car and train, which share numerous common properties (function, material, dynamic behavior, wheels, windows, etc.). Such anomalies also exist in other gold standards such as the MEN data set (Bruni et al. 2012a). As a consequence, these evaluations effectively penalize models for learning the evident truth that coffee and cup are dissimilar. Although clearly different, coffee and cup are very much related. The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977)."
J15-4004,J06-1003,0,0.0176138,"n pairs such as car and train, which share numerous common properties (function, material, dynamic behavior, wheels, windows, etc.). Such anomalies also exist in other gold standards such as the MEN data set (Bruni et al. 2012a). As a consequence, these evaluations effectively penalize models for learning the evident truth that coffee and cup are dissimilar. Although clearly different, coffee and cup are very much related. The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977). At its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et a"
J15-4004,P14-1024,0,0.0230115,"Missing"
J15-4004,P08-1088,0,0.00473907,"ese are precisely the sort of pairs that are not contained in existing evaluation gold standards. Table 1 lists the USF noun pairs with the lowest similarity scores overall, and also those with the largest additive discrepancy between association strength and similarity. 2.1.1 Association and Similarity in NLP. As noted in the Introduction, the similarity/association distinction is not only of interest to researchers in psychology or linguistics. Models of similarity are particularly applicable to various NLP tasks, such as lexical resource building, semantic parsing, and machine translation (Haghighi et al. 2008; He et al. 2008; Marton, Callison-Burch, and Resnik 2009; Beltagy, Erk, and Mooney 2014). Models of association, on the other hand, may be better suited to tasks such as wordsense disambiguation (Navigli 2009), and applications such as text classification (Phan, Nguyen, and Horiguchi 2008) in which the target classes correspond to topical domains such as agriculture or sport (Rose, Stevenson, and Whitehead 2002). Much recent research in distributional semantics does not distinguish between association and similarity in a principled way (see, e.g., Reisinger and Mooney 2010b; Huang et al. 2012"
J15-4004,D08-1011,0,0.00927383,"its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et al. 2006). Machine translation systems, which aim to define mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established application (He et al. 2008; Marton, Callison-Burch, and Resnik 2009). Moreover, since, as we establish, similarity is a cognitively complex operation that can require rich, structured conceptual knowledge to compute accurately, similarity estimation constitutes an effective proxy evaluation for general-purpose representation-learning models whose ultimate application is variable or unknown (Collobert and Weston 2008; Baroni and Lenci 2010). As we show in Section 2, the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reflect similarity. In particular, in both WS353 and M"
J15-4004,Q14-1023,1,0.576245,"Missing"
J15-4004,P12-1092,0,0.688971,"erformance or inter-annotator agreement (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4) refer to MEN and/or WS-353 as “similarity data sets.” Others evaluate on both these association-based and genuine similarity-based gold standards with no reference to the fact that they measure different things (Medelyan et al. 2009; Li et al. 2014). 666 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Motivated by these observations, in Section 3 we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity. SimLex-999 was produced by 500 paid native English speakers, rec"
J15-4004,W14-1503,0,0.0286789,"so, we evaluate the models on the SimLex999 subsets of adjectives, nouns, and verbs, as well as on abstract and concrete subsets and subsets of more and less strongly associated pairs (Sections 5.2.2–5.2.4). As part of these analyses, we confirm the hypothesis (Agirre et al. 2009; Levy and Goldberg 2014) that models learning from input informed by dependency parsing, rather than simple running-text input, yield improved similarity estimation and, specifically, clearer distinction between similarity and association. In contrast, we find no evidence for a related hypothesis (Agirre et al. 2009; Kiela and Clark 2014) that smaller context windows improve the ability of models to capture similarity. We do, however, observe clear differences in model performance on the distinct concept types included in SimLex-999. Taken together, these experiments demonstrate the benefit of the diversity of concepts 2 www.mturk.com/. 667 Computational Linguistics Volume 41, Number 4 included in SimLex-999; it would not have been possible to derive similar insights by evaluating based on existing gold standards. We conclude by discussing how observations such as these can guide future research into distributional semantic mo"
J15-4004,P14-2135,1,0.451794,"Missing"
J15-4004,C94-1103,0,0.0904133,"Missing"
J15-4004,P14-2050,0,0.7627,"ty of lexical concepts in general, current models achieve notably lower scores on SimLex-999 than on existing gold standard evaluations, and well below the SimLex-999 inter-human agreement ceiling. Finally, we explore ways in which distributional models might improve on this performance in similarity modeling. To do so, we evaluate the models on the SimLex999 subsets of adjectives, nouns, and verbs, as well as on abstract and concrete subsets and subsets of more and less strongly associated pairs (Sections 5.2.2–5.2.4). As part of these analyses, we confirm the hypothesis (Agirre et al. 2009; Levy and Goldberg 2014) that models learning from input informed by dependency parsing, rather than simple running-text input, yield improved similarity estimation and, specifically, clearer distinction between similarity and association. In contrast, we find no evidence for a related hypothesis (Agirre et al. 2009; Kiela and Clark 2014) that smaller context windows improve the ability of models to capture similarity. We do, however, observe clear differences in model performance on the distinct concept types included in SimLex-999. Taken together, these experiments demonstrate the benefit of the diversity of concep"
J15-4004,N15-1098,0,0.0323029,"Missing"
J15-4004,P06-1129,0,0.00844882,"st 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977). At its strongest, the similarity relation is exemplified by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et al. 2006). Machine translation systems, which aim to define mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established application (He et al. 2008; Marton, Callison-Burch, and Resnik 2009). Moreover, since, as we establish, similarity is a cognitively complex operation that can require rich, structured conceptual knowledge to compute accurately, similarity estimation constitutes an effective proxy evaluation for general-purpose representation-learning models whose ultimate application is variable or unknown (Collobert and Weston 2"
J15-4004,W13-3512,0,0.933535,"Missing"
J15-4004,D09-1040,0,0.00699259,"Missing"
J15-4004,D07-1042,0,0.0859608,"Missing"
J15-4004,D10-1114,0,0.00897274,"(Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4) refer to MEN and/or WS-353 as “similarity data sets.” Others evaluate on both these association-based and genuine similarity-based gold standards with no reference to the fact that they measure different things (Medelyan et al. 2009; Li et al. 2014). 666 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Motivated by these observations, in Section 3 we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity. SimLex-999 was produced by 500 paid native English speakers, recruited via Amazon Mechanical Turk,2 who were ask"
J15-4004,N10-1013,0,0.285524,"(Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Mooney (2010b, page 4) refer to MEN and/or WS-353 as “similarity data sets.” Others evaluate on both these association-based and genuine similarity-based gold standards with no reference to the fact that they measure different things (Medelyan et al. 2009; Li et al. 2014). 666 Hill, Reichart, and Korhonen SimLex-999: Evaluating Semantic Models Motivated by these observations, in Section 3 we present SimLex-999, a gold standard resource for evaluating the ability of models to reflect similarity. SimLex-999 was produced by 500 paid native English speakers, recruited via Amazon Mechanical Turk,2 who were ask"
J15-4004,rose-etal-2002-reuters,0,0.0253268,"Missing"
J15-4004,P14-1068,0,0.566786,"Missing"
J15-4004,P10-1040,0,0.144364,"Missing"
J15-4004,W99-0502,0,0.073997,"of what their evaluation resources actually measure.1 Although certain smaller gold standards—those of Rubenstein and Goodenough (1965) (RG) and Agirre et al. (2009) (WS-Sim)—do focus clearly on similarity, these resources suffer from other important limitations. For instance, as we show, and as is also the case for WS-353 and MEN, state-of-the-art models have reached the average performance of a human annotator on these evaluations. It is common practice in NLP to define the upper limit for automated performance on an evaluation as the average human performance or inter-annotator agreement (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same confidence as their manually created equivalents. 1 For instance, Huang et al. (2012, pages 1, 4, 10) and Reisinger and Moon"
J15-4004,P94-1019,0,\N,Missing
J15-4004,N04-3012,0,\N,Missing
J15-4004,N09-1003,0,\N,Missing
J15-4004,W13-2609,1,\N,Missing
J17-4004,N09-1003,0,0.567913,"alculated the average of all ratings from the accepted raters (≥ 10) for each word pair. The score was finally scaled linearly from the 0–6 to the 0–10 interval as also done by Hill, Reichart, and Korhonen (2015). 799 Computational Linguistics Volume 43, Number 4 Table 4 A comparison of HyperLex IAA with several prominent crowdsourced semantic similarity/ relatedness evaluation benchmarks that also provide scores for word pairs. Numbers in parentheses refer to the total number of word pairs in each evaluation set. Benchmark IAA-1 IAA-2 W ORD S IM (353) (Finkelstein et al. 2002) WS-S IM (203) (Agirre et al. 2009) S IM L EX (999) (Hill, Reichart, and Korhonen 2015) 0.611 0.756 0.667 0.651 0.673 0.778 H YPER L EX (2616) 0.854 0.864 H YPER L EX : N OUNS (2163) H YPER L EX : V ERBS (453) 0.854 0.855 0.864 0.862 5. Analysis Inter-Annotator Agreement. We report two different inter-annotator agreement (IAA) measures. IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters. This is a common choice in previous data collection in distributional semantics (Padó, Padó, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014; Hill, Reichart, and Korhonen 2015). A c"
J17-4004,W13-3520,0,0.0197215,"Missing"
J17-4004,P15-1104,0,0.0243363,"ocates the use of more sophisticated learning algorithms in future work. Another path of research work could investigate how to exploit more training data from resources other than HyperLex to yield improved graded LE models. 7.4 Further Discussion: Specializing Semantic Spaces Following the growing interest in word representation learning, this work also touches upon the ideas of vector/semantic space specialization: A desirable property of representation models is their ability to steer their output vector spaces according to explicit linguistic and dictionary knowledge (Yu and Dredze 2014; Astudillo et al. 2015; Faruqui 823 Computational Linguistics Volume 43, Number 4 et al. 2015; Liu et al. 2015; Wieting et al. 2015; Mrkši´c et al. 2016; Vuli´c et al. 2017, inter alia). Previous work showed that it is possible to build vector spaces specialized for capturing different lexical relations (e.g., antonymy [Yih, Zweig, and Platt 2012; Ono, Miwa, and Sasaki 2015]) or distinguishing between similarity and relatedness (Kiela, Hill, and Clark 2015). Yet, it is to be seen how to build a representation model specialized for the graded LE relation. An analogy with (graded) semantic similarity is appropriate h"
J17-4004,E12-1004,0,0.210541,"derlines the lexical entailment (LE) relation. Simply put, an instantiation of a member concept such as a cat entails the existence of an animal. This lexical entailment in turns governs many cases of phrasal and sentential entailment: If we know that a cat is in the garden, we can quickly and intuitively conclude that an animal is in the garden, too.1 Because of this fundamental connection to language understanding, the automatic detection and modeling of lexical entailment has been an area of much focus in natural language processing (Bos and Markert 2005; Dagan, Glickman, and Magnini 2006; Baroni et al. 2012; Beltagy et al. 2013, inter alia). The ability to effectively detect and model both lexical and phrasal entailment in a human-like way may be critical for numerous related applications, such as question answering, information retrieval, information extraction, and text summarization and generation (Androutsopoulos and Malakasiotis 2010). For instance, in order to answer a question such as “Which mammal has a strong bite?”, a question-answering system has to know that a jaguar or a grizzly bear are types of mammals, whereas a crocodile or a piranha are not. Although inspired to some extent by"
J17-4004,P14-1023,0,0.108108,"Missing"
J17-4004,J10-4006,0,0.0102161,"-up regarding training data, their parameter settings, and other modeling choices. DEMs and SLQS. Directional entailment measures DEM1 –DEM4 and both SLQS variants (i.e., SLQS–B ASIC and SLQS–S IM) are based on the cleaned, tokenized, and lowercased Polyglot Wikipedia (Al-Rfou, Perozzi, and Skiena 2013). We have used two set-ups for the induction of word representations, the only difference being that in Set-up 1 context/feature vectors are extracted from the Polyglot Wiki directly based on bigram co-occurrence counts, whereas in Set-up 2, these vectors are extracted from the T YPE DM tensor (Baroni and Lenci 2010) as in the original work of Lenci and Benotto (2012).27 Both set-ups use the positive LMI weighting calculated on syntactic co-occurrence links between each word and its context word (Gulordava and Baroni 2011): LMI(w1 , w2 ) = 1 ,w2 )∗Total C(w1 , w2 ) ∗ log2 C(w C(w1 )C(w2 ) , where C(w) is the unigram count in the Polyglot Wiki for the word w, C(w1 , w2 ) is the dependency based co-occurrence count of the two tokens w1 and w2 , namely (w1 , (dep_rel, w2 )), and Total is the number of all such tuples. The Polyglot Wiki was parsed with Universal Dependencies (Nivre et al. 2015) as in the work"
J17-4004,W11-2501,0,0.0591396,"owing mapping: fgraded : (X, Y) → R+ 0 (4) fgraded outputs the strength of the lexical entailment relation s ∈ R+ 0 . By adopting the graded LE paradigm, HyperLex thus measures the degree of lexical entailment between words X and Y constituting the order-sensitive pair (X, Y). From another perspective, it measures the typicality and graded membership of the instance X for the class/category Y. From the relational similarity viewpoint (Jurgens et al. 2012; Zhila et al. 2013), it also measures the prototypicality of the pair (X, Y) for the LE relation. 3.1.2 Evaluation Sets BLESS. Introduced by Baroni and Lenci (2011), the original BLESS evaluation set includes 200 concrete English nouns as target concepts (i.e., X-s from the pairs (X, Y)), equally divided between animate and inanimate entities. A total of 175 concepts were extracted from the McRae feature norms data set (McRae et al. 2005), and the remaining 25 were selected manually by the authors. These concepts were then paired to 8,625 7 The terms intension and extension assume classical intensional and extensional definitions of a concept (van Benthem and ter Meulen 1996; Baronett 2012). ~ ), difference (Y ~ −X ~ ), or element-wise multiplication ~ ⊕"
J17-4004,W16-2502,0,0.0167661,"ave been largely overlooked in the 824 Vuli´c et al. HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment representation learning literature. Notable exceptions building word embeddings for LE have appeared only recently (see the work of Vendrov et al. [2016] and a short overview in Section 7.4), but a comprehensive evaluation resource for intrinsic evaluation of such LE embeddings is still missing. There is a pressing need to improve, broaden, and introduce new evaluation protocols and data sets for representation learning architectures (Schnabel et al. 2015; Tsvetkov et al. 2015; Batchkarov et al. 2016; Faruqui et al. 2016; Yaghoobzadeh and Schütze 2016, inter alia).36 We believe that one immediate application of HyperLex is its use as a comprehensive, wide-coverage large evaluation set for representationlearning architectures focused on the fundamental TYPE - OF taxonomic relation. Data Mining: Extending Knowledge Bases. Ontologies and knowledge bases such as WordNet, Yago, or DBPedia are useful resources in a variety of applications such as text generation, question answering, information retrieval, or for simply providing structured knowledge to users. Because they typically suffer from"
J17-4004,S13-1002,0,0.0247821,"entailment (LE) relation. Simply put, an instantiation of a member concept such as a cat entails the existence of an animal. This lexical entailment in turns governs many cases of phrasal and sentential entailment: If we know that a cat is in the garden, we can quickly and intuitively conclude that an animal is in the garden, too.1 Because of this fundamental connection to language understanding, the automatic detection and modeling of lexical entailment has been an area of much focus in natural language processing (Bos and Markert 2005; Dagan, Glickman, and Magnini 2006; Baroni et al. 2012; Beltagy et al. 2013, inter alia). The ability to effectively detect and model both lexical and phrasal entailment in a human-like way may be critical for numerous related applications, such as question answering, information retrieval, information extraction, and text summarization and generation (Androutsopoulos and Malakasiotis 2010). For instance, in order to answer a question such as “Which mammal has a strong bite?”, a question-answering system has to know that a jaguar or a grizzly bear are types of mammals, whereas a crocodile or a piranha are not. Although inspired to some extent by theories of human sem"
J17-4004,I13-1095,0,0.26422,"exical entailment. In addition to the use of HyperLex as a new evaluation set, we believe that the introduction of graded LE will have implications on how the distributional hypothesis (Harris 1954) is exploited in distributional models targeting taxonomic relations in particular (Rubinstein et al. 2015; Shwartz, Goldberg, and Dagan 2016; Roller and Erk 2016, inter alia). Further, a tight connection of LE with the broader phrase-/sentence-level task of recognizing lexical entailment (Dagan, Glickman, and Magnini 2006; Dagan et al. 2013) should lead to further implications for text generation (Biran and McKeown 2013), metaphor detection (Mohler et al. 2013), question answering (Sacaleanu et al. 2008), paraphrasing (Androutsopoulos and Malakasiotis 2010), and so forth. Representation Learning. Prior work in representation learning has mostly focused on the relations of semantic similarity and relatedness, as evidenced by the surge in interest in evaluation of word embeddings on data sets such as SimLex-999, WordSim-353, MEN (Bruni, Tran, and Baroni 2014), Rare Words (Luong, Socher, and Manning 2013), and so on. This strong focus towards similarity and relatedness means that other fundamental semantic relat"
J17-4004,D15-1075,0,0.076472,"Missing"
J17-4004,P15-2001,0,0.111058,"Missing"
J17-4004,W09-0215,0,0.0685026,"ive than their hyponyms (Murphy 2003), which is also reflected in less specific contexts for hypernyms. Unsupervised (distributional) models of lexical entailment were instigated by the early work of Hearst (1992) on prototypicality patterns (e.g., the pattern “X such as Y” indicates that Y is a hyponym of X). The current unsupervised models typically replace the symmetric cosine similarity measure that works well for semantic similarity computations (Bullinaria and Levy 2007; Mikolov et al. 2013a) with an asymmetric similarity measure optimized for entailment (Weeds, Weir, and McCarthy 2004; Clarke 2009; Kotlerman et al. 2010; Lenci and Benotto 2012; Herbelot and Ganesalingam 2013; Santus et al. 2014). Supervised models, on the other hand, learn the asymmetric operator from a training set, differing mostly in the feature selection to represent each candidate pair of words (Baroni et al. 2012; Fu et al. 2014; Rimell 2014; Roller, Erk, and Boleda 2014; Weeds et al. 2014; Fu et al. 2015; Roller and Erk 2016; Shwartz, Goldberg, and Dagan 2016).8 An overview of the supervised techniques also discussing their main shortcomings is provided by Levy et al. (2015); a thorough discussion of differences"
J17-4004,D10-1107,0,0.0153209,"this will in turn incentivize research into language technology that both reflects human semantic memory more faithfully and interprets and models linguistic entailment more effectively. 2. Graded Lexical Entailment Note on Terminology. Because of dual and inconsistent use in prior work, in this work we use the term lexical entailment (LE) in its stricter definition. It refers precisely to the taxonomical asymmetric hyponymy–hypernymy relation, also known as IS - A, or TYPE - OF relation (Hearst 1992; Snow, Jurafsky, and Ng 2004; Weeds, Weir, and McCarthy 2004; Pantel and Pennacchiotti 2006; Do and Roth 2010, inter alia), e.g., snake is a TYPE - OF animal, computer is a TYPE - OF machine. This is different from an alternative definition (Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010; Turney and Mohammad 2015) as substitutable lexical entailment: This relation holds for a pair of words (X, Y) if a possible meaning of one word (i.e., X) entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. This definition is looser and more general than the TYPE - OF definition, as it also encompasses other lexical relations such as synonymy, met"
J17-4004,N15-1184,0,0.154452,"Missing"
J17-4004,P15-2076,0,0.0297851,"ons are interlinked. We test the following benchmarking semantic similarity models: (1) Unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) (Mikolov et al. 2013b) with various contexts (BOW = bag of words; DEPS = dependency contexts) as described by Levy and Goldberg (2014); and (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. We evaluate models that currently hold the peak scores in word similarity tasks: sparse binary vectors built from linguistic resources (N ON -D ISTRIBUTIONAL [Faruqui and Dyer 2015]), vectors fine-tuned to a paraphrase database (PARAGRAM [Wieting et al. 2015]), and further refined using linguistic constraints (PARAGRAM +CF [Mrkši´c et al. 2016]). Because these models are not the main focus of this work, the reader is referred to the relevant literature for detailed descriptions. 6.8 Gaussian Embeddings An alternative approach to learning word embeddings was proposed by Vilnis and McCallum (2015). They represent words as Gaussian densities rather than points in the embedding space. Each concept X is represented as a multivariate K-dimensional Gaussian parameterized as N"
J17-4004,W16-2506,0,0.23782,"nnotations: HyperLex may also be used in the standard format of previous LE evaluation sets (see Table 1) for detection and directionality evaluation protocols (see later in Section 7.2). Second, a typical way to evaluate word representation quality at present is by judging the similarity of representations assigned to similar words. The most popular semantic similarity evaluation sets such as SimLex-999 or SimVerb-3500 consist of word pairs with similarity ratings produced by human annotators. HyperLex is the first resource that can be used for the intrinsic evaluation (Schnabel et al. 2015; Faruqui et al. 2016) of LE-based vector space models (Vendrov et al. 2016); see later in Section 6.6. Encouraged by high inter-annotator agreement scores and evident large gaps between the human and system performance (see Section 7), we believe that HyperLex will guide the development of a new generation of representation-learning architectures that induce hypernymy/LE-specialized word representations, as opposed to the current ubiquitous word representations targeting exclusively semantic similarity and/or relatedness (see later the discussion in Sections 7.4 and 8). Finally, HyperLex provides a wide coverage o"
J17-4004,P14-1113,0,0.0669191,"X). The current unsupervised models typically replace the symmetric cosine similarity measure that works well for semantic similarity computations (Bullinaria and Levy 2007; Mikolov et al. 2013a) with an asymmetric similarity measure optimized for entailment (Weeds, Weir, and McCarthy 2004; Clarke 2009; Kotlerman et al. 2010; Lenci and Benotto 2012; Herbelot and Ganesalingam 2013; Santus et al. 2014). Supervised models, on the other hand, learn the asymmetric operator from a training set, differing mostly in the feature selection to represent each candidate pair of words (Baroni et al. 2012; Fu et al. 2014; Rimell 2014; Roller, Erk, and Boleda 2014; Weeds et al. 2014; Fu et al. 2015; Roller and Erk 2016; Shwartz, Goldberg, and Dagan 2016).8 An overview of the supervised techniques also discussing their main shortcomings is provided by Levy et al. (2015); a thorough discussion of differences between unsupervised and supervised entailment models is provided by Turney and Mohammad (2015). Why is HyperLex Different? In short, regardless of the chosen methodology, the evaluation protocols (directionality or detection) may be straightforwardly translated into binary decision problems: (1) distinguish"
J17-4004,P05-1014,0,0.141677,"re exists a lexical entailment relation between two words (X, Y), and then, if the relation holds, it has to predict its directionality (i.e., the correct hypernym). The following mapping is defined by the joint detection and directionality function fdet+dir : fdet+dir : (X, Y) → {−1, 0, 1} (3) fdet+dir maps to 1 when (X, Y) stand in a lexical entailment relation and Y is the hypernym, to −1 if X is the hypernym, and to 0 if X and Y stand in some other lexical relation or no relation. Standard Modeling Approaches. These decisions are typically based on the distributional inclusion hypothesis (Geffet and Dagan 2005) or a lexical generality measure (Herbelot and Ganesalingam 2013). The intuition supporting the former is that the class (i.e., extension) denoted by a hyponym is included in the class denoted by the hypernym, and therefore hyponyms are expected to occur in a subset of the contexts of their hypernyms. The intuition supporting the latter hints that typical characteristics constituting the intension (i.e., concept) expressed by a hypernym (e.g., move or eat for the concept animal) are 787 Computational Linguistics Volume 43, Number 4 semantically more general than the characteristics forming the"
J17-4004,D16-1235,1,0.310735,"Missing"
J17-4004,gheorghita-pierrel-2012-towards,0,0.064616,"Missing"
J17-4004,W16-2507,0,0.00428272,"hat ground language in the physical world (Silberer and Lapata 2012, 2014; Bruni, Tran, and Baroni 2014, inter alia). Future work might also investigate attaching graded LE scores to large hierarchical image databases such as ImageNet (Deng et al. 2009; Russakovsky et al. 2015). 9. Conclusions Although the ultimate test of semantic models is their usefulness in downstream applications, the research community is still in need of wide-coverage comprehensive gold standard resources for intrinsic evaluation (Camacho-Collados, Pilehvar, and Navigli 2015; Schnabel et al. 2015; Tsvetkov et al. 2015; Gladkova and Drozd 2016; Hashimoto, Alvarez-Melis, and Jaakkola 2016, inter alia). Such resources can measure the general quality of the representations learned by semantic models, prior to their integration in end-to-end systems. We have presented HyperLex, a large wide-coverage gold standard resource for the evaluation of semantic representations targeting the lexical relation of graded lexical entailment (LE), also known as hypernymy-hyponymy or TYPE - OF relation, a relation which is fundamental in construction and understanding of concept hierarchies, that is, semantic taxonomies. Given that the problem of conc"
J17-4004,W11-2508,0,0.00981994,"d on the cleaned, tokenized, and lowercased Polyglot Wikipedia (Al-Rfou, Perozzi, and Skiena 2013). We have used two set-ups for the induction of word representations, the only difference being that in Set-up 1 context/feature vectors are extracted from the Polyglot Wiki directly based on bigram co-occurrence counts, whereas in Set-up 2, these vectors are extracted from the T YPE DM tensor (Baroni and Lenci 2010) as in the original work of Lenci and Benotto (2012).27 Both set-ups use the positive LMI weighting calculated on syntactic co-occurrence links between each word and its context word (Gulordava and Baroni 2011): LMI(w1 , w2 ) = 1 ,w2 )∗Total C(w1 , w2 ) ∗ log2 C(w C(w1 )C(w2 ) , where C(w) is the unigram count in the Polyglot Wiki for the word w, C(w1 , w2 ) is the dependency based co-occurrence count of the two tokens w1 and w2 , namely (w1 , (dep_rel, w2 )), and Total is the number of all such tuples. The Polyglot Wiki was parsed with Universal Dependencies (Nivre et al. 2015) as in the work of Vuli´c and Korhonen (2016).28 The context vocabulary (i.e., words w2 ) is restricted to the 10K most frequent words in the Polyglot Wiki. The same two set-ups were used for the SLQS model. We also use frequ"
J17-4004,Q16-1020,0,0.0114068,"Missing"
J17-4004,C92-2082,0,0.478737,"ans perceive the concepts of typicality and graded membership within the graded LE relation. We hope that this will in turn incentivize research into language technology that both reflects human semantic memory more faithfully and interprets and models linguistic entailment more effectively. 2. Graded Lexical Entailment Note on Terminology. Because of dual and inconsistent use in prior work, in this work we use the term lexical entailment (LE) in its stricter definition. It refers precisely to the taxonomical asymmetric hyponymy–hypernymy relation, also known as IS - A, or TYPE - OF relation (Hearst 1992; Snow, Jurafsky, and Ng 2004; Weeds, Weir, and McCarthy 2004; Pantel and Pennacchiotti 2006; Do and Roth 2010, inter alia), e.g., snake is a TYPE - OF animal, computer is a TYPE - OF machine. This is different from an alternative definition (Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010; Turney and Mohammad 2015) as substitutable lexical entailment: This relation holds for a pair of words (X, Y) if a possible meaning of one word (i.e., X) entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. This definition is looser and mo"
J17-4004,S10-1006,0,0.0353884,"ystem has to predict the relation directionality, that is, which word is the hypernym and which word is the hyponym. More formally, the following mapping is defined by the directionality function fdir : fdir : (X, Y) → {−1, 1} (1) fdir simply maps to 1 when Y is the hypernym, and to −1 otherwise. (ii) Entailment Detection. The system has to predict whether there exists a lexical entailment relation between two words, or the words stand in some other relation (synonymy, meronymy–holonymy, causality, no relation, etc.). A more detailed overview of lexical relations is available in related work (Hendrickx et al. 2010; Jurgens et al. 2012; Vylomova et al. 2016). The following mapping is defined by the detection function fdet : fdet : (X, Y) → {0, 1} (2) fdet simply maps to 1 when (X, Y) stand in a lexical entailment relation, irrespective to the actual directionality of the relation, and to 0 otherwise. (iii) Entailment Detection and Directionality. This recently proposed evaluation protocol (Weeds et al. 2014; Kiela et al. 2015) combines (i) and (ii). The system first has to detect whether there exists a lexical entailment relation between two words (X, Y), and then, if the relation holds, it has to predi"
J17-4004,P13-2078,0,0.08496,"s (X, Y), and then, if the relation holds, it has to predict its directionality (i.e., the correct hypernym). The following mapping is defined by the joint detection and directionality function fdet+dir : fdet+dir : (X, Y) → {−1, 0, 1} (3) fdet+dir maps to 1 when (X, Y) stand in a lexical entailment relation and Y is the hypernym, to −1 if X is the hypernym, and to 0 if X and Y stand in some other lexical relation or no relation. Standard Modeling Approaches. These decisions are typically based on the distributional inclusion hypothesis (Geffet and Dagan 2005) or a lexical generality measure (Herbelot and Ganesalingam 2013). The intuition supporting the former is that the class (i.e., extension) denoted by a hyponym is included in the class denoted by the hypernym, and therefore hyponyms are expected to occur in a subset of the contexts of their hypernyms. The intuition supporting the latter hints that typical characteristics constituting the intension (i.e., concept) expressed by a hypernym (e.g., move or eat for the concept animal) are 787 Computational Linguistics Volume 43, Number 4 semantically more general than the characteristics forming the intension7 of its hyponyms (e.g., bark or has tail for the conce"
J17-4004,J15-4004,1,0.787959,"Missing"
J17-4004,N15-1070,0,0.0250832,"Missing"
J17-4004,S12-1047,0,0.0528398,"Missing"
J17-4004,D14-1005,1,0.757367,"Missing"
J17-4004,D15-1242,1,0.0670611,"k as more typical instances of the class to communicate than concepts such as to touch, or to pray. In Section 6 we then turn our attention to Q2: We evaluate the performance of a wide range of LE detection or measurement approaches. This review covers: (i) distributional models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [Levy et al. 2015; Shw"
J17-4004,P14-2135,1,0.798609,"ndency, using sets of images associated with each concept word as returned by Google’s image search. The intuition is that the set of images returned for the broader concept animal will consist of pictures of different kinds of animals, that is, exhibiting greater visual variability and lesser concept specificity; on the other hand, the set of images for bird will consist of pictures of different birds, and the set for owl will mostly consist only of images of owls. The generality of a set of n images for each concept X is then computed. The first model relies on the image dispersion measure (Kiela et al. 2014). It is the average −→ −→ pairwise cosine distance between all image representations23 {iX,1 , . . . , iX,n } for X: id(X) = 2 n(n − 1) X j &lt; k ≤n − → −→ 1 − cos(iX,j , iX,k ) (11) Another similar measure instead of calculating the pairwise distance calculates the −→ −→ distance to the centroid − µ→ X of {iX,1 , . . . , iX,n }: 1 cent(X) = n X 1≤j≤n − → → 1 − cos(iX,j , − µX ) (12) Final Model. The following formula summarizes the visual model for ungraded LE directionality and detection that we also test in graded evaluations: ( sθ (X, Y) = 1− 0 f (X)+α f (Y) ~ ≥θ ~ Y) if cos(X, otherwise (13"
J17-4004,P15-2020,1,0.704519,"Missing"
J17-4004,N15-1016,0,0.0344693,"Missing"
J17-4004,S12-1012,0,0.0892939,"in human judgments. For instance, graded LE scores indicate that humans rate concepts such as to talk or to speak as more typical instances of the class to communicate than concepts such as to touch, or to pray. In Section 6 we then turn our attention to Q2: We evaluate the performance of a wide range of LE detection or measurement approaches. This review covers: (i) distributional models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim"
J17-4004,W14-1610,0,0.0265556,"of substitutable LE (see Section 2). Baroni et al. (2012). The N1  N2 evaluation set contains 2,770 nominal concept pairs, with 1,385 pairs labeled as positive examples (i.e., 1 or entails) (Baroni et al. 2012). The remaining 1,385 pairs labeled as negatives were created by inverting the positive pairs and randomly matching concepts from the positive pairs. The pairs and annotations were extracted automatically from WordNet and then validated manually by the authors (e.g., the abstract concepts with a large number of hyponyms such as entity or object were removed from the pool of concepts). Levy et al. (2014). A similar data set for the standard LE evaluation may be extracted from manually annotated entailment graphs of subject–verb–object tuples (i.e., propositions) (Levy, Dagan, and Goldberger 2014): Noun LEs were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the proposition-level entailment to the word level. This data set was built for the medical domain and adopts the looser definition of substitutable LE. Custom Evaluation Sets. A plethora of relevant work on ungraded LE do not rely on established evaluation resources, but simply extrac"
J17-4004,P14-2050,0,0.56899,"overs: (i) distributional models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [Levy et al. 2015; Shwartz, Goldberg, and Dagan 2016]) so that HyperLex can be used for supervised learning. We therefore evaluate several prominent supervised LE architectures (Baroni et al. 2012; Roller, Erk, and Boleda 2014; Weeds et al. 2014, inter alia). Although we o"
J17-4004,N15-1098,0,0.247759,"s (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [Levy et al. 2015; Shwartz, Goldberg, and Dagan 2016]) so that HyperLex can be used for supervised learning. We therefore evaluate several prominent supervised LE architectures (Baroni et al. 2012; Roller, Erk, and Boleda 2014; Weeds et al. 2014, inter alia). Although we observe interesting differences in the models, our findings indicate clearly that none of the currently available models or approaches accurately model the relation of graded LE reflected in human subjects. This study therefore calls for new paradigms and solutions capable of capturing the gradual nature of semantic relations such as hypernymy"
J17-4004,P98-2127,0,0.0625748,"tes the geometrical average of WeedsPrec (DEM1 ) or any other asymmetric measure (e.g., APinc from Kotlerman et al. [2010]) and the symmetric 21 Note that, unlike with similarity scores, the score now refers to an asymmetric relation stemming from the question “Is X a type of Y” for the word pair (X, Y). Therefore, the scores for two reverse pairs (X, Y) and (Y, X) should be different; see also Table 8. 806 Vuli´c et al. HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment similarity sim(X, Y) between X and Y, measured by cosine (Weeds, Weir, and McCarthy 2004), or the Lin measure (Lin 1998) as in the balAPinc measure of Kotlerman et al. (2010): DEM2 (X, Y) = DEM1 (X, Y) · sim(X, Y) (6) ClarkeDE (DEM3 ). A close variation of DEM1 was proposed by Clarke (2009): P DEM3 (X, Y) = ft∈FeatX ∩FeatY P min(wX ( ft), wY ( ft)) ft∈FeatX wX ( ft) (7) InvCL (DEM4 ). A variation of DEM3 was introduced by Lenci and Benotto (2012). It takes into account both the inclusion of context features of X in context features of Y and non-inclusion of features of Y in features of X.22 DEM4 (X, Y) = p DEM3 (X, Y) · (1 − DEM3 (Y, X)) (8) 6.2 Generality Measures Another related view towards the TYPE - OF rel"
J17-4004,P15-1145,0,0.0413618,"rch work could investigate how to exploit more training data from resources other than HyperLex to yield improved graded LE models. 7.4 Further Discussion: Specializing Semantic Spaces Following the growing interest in word representation learning, this work also touches upon the ideas of vector/semantic space specialization: A desirable property of representation models is their ability to steer their output vector spaces according to explicit linguistic and dictionary knowledge (Yu and Dredze 2014; Astudillo et al. 2015; Faruqui 823 Computational Linguistics Volume 43, Number 4 et al. 2015; Liu et al. 2015; Wieting et al. 2015; Mrkši´c et al. 2016; Vuli´c et al. 2017, inter alia). Previous work showed that it is possible to build vector spaces specialized for capturing different lexical relations (e.g., antonymy [Yih, Zweig, and Platt 2012; Ono, Miwa, and Sasaki 2015]) or distinguishing between similarity and relatedness (Kiela, Hill, and Clark 2015). Yet, it is to be seen how to build a representation model specialized for the graded LE relation. An analogy with (graded) semantic similarity is appropriate here: It was recently demonstrated that vector space models specializing for similarity a"
J17-4004,W13-3512,0,0.0649624,"Missing"
J17-4004,W13-0904,0,0.0158973,"HyperLex as a new evaluation set, we believe that the introduction of graded LE will have implications on how the distributional hypothesis (Harris 1954) is exploited in distributional models targeting taxonomic relations in particular (Rubinstein et al. 2015; Shwartz, Goldberg, and Dagan 2016; Roller and Erk 2016, inter alia). Further, a tight connection of LE with the broader phrase-/sentence-level task of recognizing lexical entailment (Dagan, Glickman, and Magnini 2006; Dagan et al. 2013) should lead to further implications for text generation (Biran and McKeown 2013), metaphor detection (Mohler et al. 2013), question answering (Sacaleanu et al. 2008), paraphrasing (Androutsopoulos and Malakasiotis 2010), and so forth. Representation Learning. Prior work in representation learning has mostly focused on the relations of semantic similarity and relatedness, as evidenced by the surge in interest in evaluation of word embeddings on data sets such as SimLex-999, WordSim-353, MEN (Bruni, Tran, and Baroni 2014), Rare Words (Luong, Socher, and Manning 2013), and so on. This strong focus towards similarity and relatedness means that other fundamental semantic relations such as lexical entailment have been"
J17-4004,N16-1018,0,0.00692386,"Missing"
J17-4004,P17-1163,0,0.00734502,"Missing"
J17-4004,D14-1113,0,0.0146062,"that a promising step in that direction are neural net–inspired approaches to LE proposed recently (Vilnis and McCallum 2015; Vendrov et al. 2016), mostly because of their conceptual distinction from other distributional modeling approaches complemented with their modeling adaptability and flexibility. In addition, in order to model hierarchical semantic knowledge more accurately, in future work we may require algorithms that are better suited to fast learning from few examples (Lake et al. 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney 2010b; Neelakantan et al. 2014; Jauhar, Dyer, and Hovy 2015; Šuster, Titov, and van Noord 2016). Despite the abundance of reported experiments and analyses in this work, we have only scratched the surface in terms of the possible analyses with HyperLex and use of such models as components of broader phrase-level and sentence-level textual entailment systems, as well as in other applications, as quickly surveyed in Section 8. Beyond the preliminary conclusions from these initial analyses, we believe that the benefits of HyperLex will become evident as researchers use it to probe the relationship between architectures, algor"
J17-4004,N15-1100,0,0.118597,"Missing"
J17-4004,D07-1042,0,0.061095,"Missing"
J17-4004,P06-1015,0,0.0188982,"raded LE relation. We hope that this will in turn incentivize research into language technology that both reflects human semantic memory more faithfully and interprets and models linguistic entailment more effectively. 2. Graded Lexical Entailment Note on Terminology. Because of dual and inconsistent use in prior work, in this work we use the term lexical entailment (LE) in its stricter definition. It refers precisely to the taxonomical asymmetric hyponymy–hypernymy relation, also known as IS - A, or TYPE - OF relation (Hearst 1992; Snow, Jurafsky, and Ng 2004; Weeds, Weir, and McCarthy 2004; Pantel and Pennacchiotti 2006; Do and Roth 2010, inter alia), e.g., snake is a TYPE - OF animal, computer is a TYPE - OF machine. This is different from an alternative definition (Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010; Turney and Mohammad 2015) as substitutable lexical entailment: This relation holds for a pair of words (X, Y) if a possible meaning of one word (i.e., X) entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. This definition is looser and more general than the TYPE - OF definition, as it also encompasses other lexical relations suc"
J17-4004,D16-1244,0,0.00552891,"(graded) semantic similarity is appropriate here: It was recently demonstrated that vector space models specializing for similarity and scoring high on SimLex-999 and SimVerb-3500 are able to boost performance of statistical systems in language understanding tasks such as dialogue state tracking (Mrkši´c et al. 2016, 2017; Vuli´c et al. 2017). We assume that the specification of what the degree of LE means for each individual pair may also boost performance of statistical end-to-end systems in another language understanding task in future work: natural language inference (Bowman et al. 2015; Parikh et al. 2016; Agi´c and Schluter 2017). Owing to their adaptability and versatility, representation architectures inspired by neural networks (e.g., Mrkši´c et al. 2016; Vendrov et al. 2016) seem to be a promising avenue for future modeling work on graded lexical entailment in both unsupervised and supervised settings, despite their low performance on the graded LE task at present. 8. Application Areas: A Quick Overview The proposed data set should have an immediate impact in the cognitive science research, providing means to analyze the effects of typicality and gradience in concept representations (Hamp"
J17-4004,N04-3012,0,0.0574719,"Missing"
J17-4004,W14-1608,0,0.0448142,"Missing"
J17-4004,D10-1114,0,0.031074,"ark IAA-1 IAA-2 W ORD S IM (353) (Finkelstein et al. 2002) WS-S IM (203) (Agirre et al. 2009) S IM L EX (999) (Hill, Reichart, and Korhonen 2015) 0.611 0.756 0.667 0.651 0.673 0.778 H YPER L EX (2616) 0.854 0.864 H YPER L EX : N OUNS (2163) H YPER L EX : V ERBS (453) 0.854 0.855 0.864 0.862 5. Analysis Inter-Annotator Agreement. We report two different inter-annotator agreement (IAA) measures. IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters. This is a common choice in previous data collection in distributional semantics (Padó, Padó, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014; Hill, Reichart, and Korhonen 2015). A complementary measure would smooth individual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. It arguably serves as a better “upper bound” than IAA-1 for the performance of automatic systems. HyperLex obtains ρ = 0.854 (IAA-1) and ρ = 0.864 (IAA-2), a very good agreement compared to other prominent crowdsourced benchmarks for semantic evaluation which also used word pair scoring (see Table 4).19 We also report IAAs over differe"
J17-4004,N10-1013,0,0.0779609,"ark IAA-1 IAA-2 W ORD S IM (353) (Finkelstein et al. 2002) WS-S IM (203) (Agirre et al. 2009) S IM L EX (999) (Hill, Reichart, and Korhonen 2015) 0.611 0.756 0.667 0.651 0.673 0.778 H YPER L EX (2616) 0.854 0.864 H YPER L EX : N OUNS (2163) H YPER L EX : V ERBS (453) 0.854 0.855 0.864 0.862 5. Analysis Inter-Annotator Agreement. We report two different inter-annotator agreement (IAA) measures. IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters. This is a common choice in previous data collection in distributional semantics (Padó, Padó, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014; Hill, Reichart, and Korhonen 2015). A complementary measure would smooth individual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. It arguably serves as a better “upper bound” than IAA-1 for the performance of automatic systems. HyperLex obtains ρ = 0.854 (IAA-1) and ρ = 0.864 (IAA-2), a very good agreement compared to other prominent crowdsourced benchmarks for semantic evaluation which also used word pair scoring (see Table 4).19 We also report IAAs over differe"
J17-4004,N13-1008,0,0.0139297,"but simply extract ad hoc LE evaluation data using distant supervision from readily available semantic resources and knowledge bases such as WordNet (Miller 1995), DBPedia (Auer et al. 2007), Freebase (Tanon et al. 2016), Yago (Suchanek, Kasneci, and Weikum 2007), or dictionaries (Gheorghita and Pierrel 2012). Although plenty of the custom evaluation sets are available online, there is a clear tendency to construct a new custom data set in every subsequent paper that uses the same evaluation protocol for ungraded LE. A standard practice (Snow, Jurafsky, and Ng 2004, 2006; Bordes et al. 2011; Riedel et al. 2013; Socher et al. 2013; Weeds et al. 2014; Shwartz, Goldberg, and Dagan 2016; Vendrov et al. 2016, inter alia) is to extract positive and negative pairs by coupling concepts that are directly related in at least one of the resources. Only pairs standing in an unambiguous hypernymy/LE relation, according to the set of indicators from Table 2, are annotated as positive examples (i.e., again 1 or entailing, Table 1) (Shwartz et al. 2015). All other pairs standing in other relations are taken as negative instances. Using related rather than random concept pairs as negative instances enables detectio"
J17-4004,E14-1054,0,0.125239,"unsupervised models typically replace the symmetric cosine similarity measure that works well for semantic similarity computations (Bullinaria and Levy 2007; Mikolov et al. 2013a) with an asymmetric similarity measure optimized for entailment (Weeds, Weir, and McCarthy 2004; Clarke 2009; Kotlerman et al. 2010; Lenci and Benotto 2012; Herbelot and Ganesalingam 2013; Santus et al. 2014). Supervised models, on the other hand, learn the asymmetric operator from a training set, differing mostly in the feature selection to represent each candidate pair of words (Baroni et al. 2012; Fu et al. 2014; Rimell 2014; Roller, Erk, and Boleda 2014; Weeds et al. 2014; Fu et al. 2015; Roller and Erk 2016; Shwartz, Goldberg, and Dagan 2016).8 An overview of the supervised techniques also discussing their main shortcomings is provided by Levy et al. (2015); a thorough discussion of differences between unsupervised and supervised entailment models is provided by Turney and Mohammad (2015). Why is HyperLex Different? In short, regardless of the chosen methodology, the evaluation protocols (directionality or detection) may be straightforwardly translated into binary decision problems: (1) distinguishing between h"
J17-4004,D16-1234,0,0.0924128,"Missing"
J17-4004,C14-1097,0,0.050532,"Missing"
J17-4004,P15-2119,0,0.0320914,"ven distributional LE models. In current binary evaluation protocols targeting ungraded LE detection and directionality, even simple methods modeling lexical generality are able to yield very accurate predictions. However, our preliminary analysis in Section 7.2 demonstrates their fundamental limitations for graded lexical entailment. In addition to the use of HyperLex as a new evaluation set, we believe that the introduction of graded LE will have implications on how the distributional hypothesis (Harris 1954) is exploited in distributional models targeting taxonomic relations in particular (Rubinstein et al. 2015; Shwartz, Goldberg, and Dagan 2016; Roller and Erk 2016, inter alia). Further, a tight connection of LE with the broader phrase-/sentence-level task of recognizing lexical entailment (Dagan, Glickman, and Magnini 2006; Dagan et al. 2013) should lead to further implications for text generation (Biran and McKeown 2013), metaphor detection (Mohler et al. 2013), question answering (Sacaleanu et al. 2008), paraphrasing (Androutsopoulos and Malakasiotis 2010), and so forth. Representation Learning. Prior work in representation learning has mostly focused on the relations of semantic similarity and"
J17-4004,C08-3008,0,0.0239186,"eve that the introduction of graded LE will have implications on how the distributional hypothesis (Harris 1954) is exploited in distributional models targeting taxonomic relations in particular (Rubinstein et al. 2015; Shwartz, Goldberg, and Dagan 2016; Roller and Erk 2016, inter alia). Further, a tight connection of LE with the broader phrase-/sentence-level task of recognizing lexical entailment (Dagan, Glickman, and Magnini 2006; Dagan et al. 2013) should lead to further implications for text generation (Biran and McKeown 2013), metaphor detection (Mohler et al. 2013), question answering (Sacaleanu et al. 2008), paraphrasing (Androutsopoulos and Malakasiotis 2010), and so forth. Representation Learning. Prior work in representation learning has mostly focused on the relations of semantic similarity and relatedness, as evidenced by the surge in interest in evaluation of word embeddings on data sets such as SimLex-999, WordSim-353, MEN (Bruni, Tran, and Baroni 2014), Rare Words (Luong, Socher, and Manning 2013), and so on. This strong focus towards similarity and relatedness means that other fundamental semantic relations such as lexical entailment have been largely overlooked in the 824 Vuli´c et al."
J17-4004,E14-4008,0,0.0900705,"at humans rate concepts such as to talk or to speak as more typical instances of the class to communicate than concepts such as to touch, or to pray. In Section 6 we then turn our attention to Q2: We evaluate the performance of a wide range of LE detection or measurement approaches. This review covers: (i) distributional models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test split"
J17-4004,W15-4208,0,0.0243423,"ty evaluations (Santus et al. 2014; Kiela et al. 2015), only the LE subset is used. Note that the original BLESS data are always presented with the hyponym first, so gold annotations are implicitly provided here. Second, for detection evaluations (Roller, Erk, and Boleda 2014; Santus et al. 2014; Levy et al. 2015), the pairs from the LE subset are taken as positive pairs, and all the remaining pairs are considered negative pairs. That way, the evaluation data effectively measure a model’s ability to predict the positive LE relation. Another evaluation data set based on BLESS was introduced by Santus et al. (2015). Following the standard annotation scheme, it comprises 7,429 noun pairs in total, and 1,880 LE pairs in particular, covering a wider range of relations than BLESS (i.e., the data set now includes synonymy and antonymy pairs). Adaptations of the original BLESS evaluation set were proposed recently. First, relying on its LE subset, Weeds et al. (2014) created another data set called WBLESS (Kiela et al. 2015) consisting of 1,976 concept pairs in total. Only (X, Y) pairs where Y is the hypernym are annotated as positive examples. It also contains reversed LE pairs (i.e., X is the hypernym), coh"
J17-4004,D15-1036,0,0.282274,"onverted to ungraded annotations: HyperLex may also be used in the standard format of previous LE evaluation sets (see Table 1) for detection and directionality evaluation protocols (see later in Section 7.2). Second, a typical way to evaluate word representation quality at present is by judging the similarity of representations assigned to similar words. The most popular semantic similarity evaluation sets such as SimLex-999 or SimVerb-3500 consist of word pairs with similarity ratings produced by human annotators. HyperLex is the first resource that can be used for the intrinsic evaluation (Schnabel et al. 2015; Faruqui et al. 2016) of LE-based vector space models (Vendrov et al. 2016); see later in Section 6.6. Encouraged by high inter-annotator agreement scores and evident large gaps between the human and system performance (see Section 7), we believe that HyperLex will guide the development of a new generation of representation-learning architectures that induce hypernymy/LE-specialized word representations, as opposed to the current ubiquitous word representations targeting exclusively semantic similarity and/or relatedness (see later the discussion in Sections 7.4 and 8). Finally, HyperLex prov"
J17-4004,K15-1026,0,0.0344989,"Missing"
J17-4004,P16-1226,0,0.365672,"Missing"
J17-4004,K15-1018,0,0.0116299,"ata set in every subsequent paper that uses the same evaluation protocol for ungraded LE. A standard practice (Snow, Jurafsky, and Ng 2004, 2006; Bordes et al. 2011; Riedel et al. 2013; Socher et al. 2013; Weeds et al. 2014; Shwartz, Goldberg, and Dagan 2016; Vendrov et al. 2016, inter alia) is to extract positive and negative pairs by coupling concepts that are directly related in at least one of the resources. Only pairs standing in an unambiguous hypernymy/LE relation, according to the set of indicators from Table 2, are annotated as positive examples (i.e., again 1 or entailing, Table 1) (Shwartz et al. 2015). All other pairs standing in other relations are taken as negative instances. Using related rather than random concept pairs as negative instances enables detection experiments. We adopt a similar construction principle regarding wide coverage of different lexical relations in HyperLex. This decision will support a variety of interesting analyses related to graded LE and other relations. Table 2 Indicators of LE/hypernymy relation in structured semantic resources. Resource Relation WordNet Wikidata DBPedia Yago instance hypernym, hypernym subclass of, instance of type subclass of 790 Vuli´c e"
J17-4004,E17-1007,0,0.189385,"Missing"
J17-4004,D12-1130,0,0.0201593,"Missing"
J17-4004,P14-1068,0,0.0145115,"353) (Finkelstein et al. 2002) WS-S IM (203) (Agirre et al. 2009) S IM L EX (999) (Hill, Reichart, and Korhonen 2015) 0.611 0.756 0.667 0.651 0.673 0.778 H YPER L EX (2616) 0.854 0.864 H YPER L EX : N OUNS (2163) H YPER L EX : V ERBS (453) 0.854 0.855 0.864 0.862 5. Analysis Inter-Annotator Agreement. We report two different inter-annotator agreement (IAA) measures. IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters. This is a common choice in previous data collection in distributional semantics (Padó, Padó, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014; Hill, Reichart, and Korhonen 2015). A complementary measure would smooth individual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. It arguably serves as a better “upper bound” than IAA-1 for the performance of automatic systems. HyperLex obtains ρ = 0.854 (IAA-1) and ρ = 0.864 (IAA-2), a very good agreement compared to other prominent crowdsourced benchmarks for semantic evaluation which also used word pair scoring (see Table 4).19 We also report IAAs over different groups of pairs accordin"
J17-4004,P06-1101,0,0.0237483,"Missing"
J17-4004,Q14-1017,0,0.00703476,"image captioning can be seen as special cases of a partial order over unified visual– semantic hierarchies (Deselaers and Ferrari 2011; Vendrov et al. 2016), see also Figure 6. For instance, image captions may be seen as abstractions of images, and they can be expressed at various levels in the hierarchy. The same image may be abstracted as, for example, A boy and a girl walking their dog, People walking their dog, People walking, A boy, a girl, and a dog, Children with a dog, Children with an animal. LE might prove helpful in research on image captioning (Hodosh, Young, and Hockenmaier 2013; Socher et al. 2014; Bernardi et al. 2016) or cross-modal information retrieval (Pereira et al. 2014) based on such visual–semantic hierarchies, but it is yet to be seen whether the knowledge of gradience and prototypicality may contribute to image captioning systems. Image generality is closely linked to semantic generality, as is evident from recent work (Deselaers and Ferrari 2011; Kiela et al. 2015). The data set could also be very useful in evaluating models that ground language in the physical world (Silberer and Lapata 2012, 2014; Bruni, Tran, and Baroni 2014, inter alia). Future work might also investiga"
J17-4004,D15-1243,0,0.035761,"s lexical entailment have been largely overlooked in the 824 Vuli´c et al. HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment representation learning literature. Notable exceptions building word embeddings for LE have appeared only recently (see the work of Vendrov et al. [2016] and a short overview in Section 7.4), but a comprehensive evaluation resource for intrinsic evaluation of such LE embeddings is still missing. There is a pressing need to improve, broaden, and introduce new evaluation protocols and data sets for representation learning architectures (Schnabel et al. 2015; Tsvetkov et al. 2015; Batchkarov et al. 2016; Faruqui et al. 2016; Yaghoobzadeh and Schütze 2016, inter alia).36 We believe that one immediate application of HyperLex is its use as a comprehensive, wide-coverage large evaluation set for representationlearning architectures focused on the fundamental TYPE - OF taxonomic relation. Data Mining: Extending Knowledge Bases. Ontologies and knowledge bases such as WordNet, Yago, or DBPedia are useful resources in a variety of applications such as text generation, question answering, information retrieval, or for simply providing structured knowledge to users. Because the"
J17-4004,J06-3003,0,0.0147319,"r free variance in the collected data in terms of their quantity and representative concept pairs. In addition, the distinction is often not evident for verb concepts. We leave further developments with respect to the two related phenomena of typicality and vagueness for future work, and refer the interested reader to the aforementioned literature. However, we already provide preliminary qualitative analyses in this article (see Section 5) suggesting that both phenomena are captured in graded LE ratings. Relation to Relational Similarity. A strand of related research on relational similarity (Turney 2006; Jurgens et al. 2012) also assigns the score s to a pair of concepts (X, Y). Note that there exists a fundamental difference between relational similarity and graded LE. In the latter, s refers to the degree of the LE relation in the (X, Y) pair, that is, to the levels of typicality and graded membership of the instance X for the class Y, whereas the former quantifies the typicality of the pair (X, Y) for some fixed lexical relation class R (Bejar, Chaffin, and Embretson 1991; Vylomova et al. 2016), for example, to what degree the pair (snake, animal) reflects a typical LE relation or a typic"
J17-4004,N16-1160,0,0.0325268,"Missing"
J17-4004,P16-2084,1,0.882326,"Missing"
J17-4004,P17-1006,1,0.87338,"Missing"
J17-4004,P16-1158,0,0.11102,"ded LE ratings. Relation to Relational Similarity. A strand of related research on relational similarity (Turney 2006; Jurgens et al. 2012) also assigns the score s to a pair of concepts (X, Y). Note that there exists a fundamental difference between relational similarity and graded LE. In the latter, s refers to the degree of the LE relation in the (X, Y) pair, that is, to the levels of typicality and graded membership of the instance X for the class Y, whereas the former quantifies the typicality of the pair (X, Y) for some fixed lexical relation class R (Bejar, Chaffin, and Embretson 1991; Vylomova et al. 2016), for example, to what degree the pair (snake, animal) reflects a typical LE relation or a typical synonymy relation.5 Graded LE vs. Semantic Similarity. A plethora of current evaluations in NLP and representation learning almost exclusively focus on semantic similarity and relatedness. Semantic similarity as quantified by, for example, SimLex-999 or SimVerb-3500 (Gerz et al. 2016) may be redefined as graded synonymy relation. The graded scores there, in fact, refer to the strength of the synonymy relation between any pair of concepts (X, Y). One could say that semantic similarity aims to answ"
J17-4004,C14-1212,0,0.55778,"Missing"
J17-4004,W03-1011,0,0.0185238,"s well. We closely follow the work from Lenci and Benotto (2012) in the presentation. Let FeatX denote the set of distributional features ft for a concept word X, and let wX ( ft) refer to the weight of the feature ft for X. The most common choices for the weighting function in traditional count-based distributional models are positive variants of pointwise mutual information (PMI) (Bullinaria and Levy 2007) and local mutual information (LMI) (Evert 2008). WeedsPrec (DEM1 ). This DEM quantifies the weighted inclusion of the features of a concept word X within the features of a concept word Y (Weeds and Weir 2003; Weeds, Weir, and McCarthy 2004; Kotlerman et al. 2010): P DEM1 (X, Y) = ft∈FeatX ∩FeatY P ft∈FeatX wX ( ft) wX ( ft) (5) WeedsSim (DEM2 ). It computes the geometrical average of WeedsPrec (DEM1 ) or any other asymmetric measure (e.g., APinc from Kotlerman et al. [2010]) and the symmetric 21 Note that, unlike with similarity scores, the score now refers to an asymmetric relation stemming from the question “Is X a type of Y” for the word pair (X, Y). Therefore, the scores for two reverse pairs (X, Y) and (Y, X) should be different; see also Table 8. 806 Vuli´c et al. HyperLex: A Large-Scale Ev"
J17-4004,C04-1146,0,0.49462,"Missing"
J17-4004,Q15-1025,0,0.5743,"l models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [Levy et al. 2015; Shwartz, Goldberg, and Dagan 2016]) so that HyperLex can be used for supervised learning. We therefore evaluate several prominent supervised LE architectures (Baroni et al. 2012; Roller, Erk, and Boleda 2014; Weeds et al. 2014, inter alia). Although we observe interesting di"
J17-4004,P94-1019,0,0.205773,"in three variant WN-based models: (1) WN–B ASIC: fWN returns a score denoting how similar two concepts are, based on the shortest path that connects the concepts in the WN taxonomy. (2) WN–LC H: Leacock–Chodorow similarity function (Leacock and Chodorow 1998) returns a score denoting how similar two concepts are, based on their shortest connecting path (as above) and the maximum depth of the taxonomy in which the concepts occur. The score is then − log(path/2 · depth), where path is the shortest connecting path length and depth the taxonomy depth. (3) WN–W U P: Wu–Palmer similarity function (Wu and Palmer 1994; Pedersen, Patwardhan, and Michelizzi 2004) returns a score denoting how similar two concepts are, based on their depth in the taxonomy and that of their most specific ancestor node. Note that all three WN-based similarity measures are not well-suited for graded LE experiments by their design: For example, they will rank direct co-hyponyms as more similar than distant hyponymy–hypernymy pairs. 6.6 Order Embeddings Following trends in semantic similarity (or graded synonymy computations, see Section 2 again), Vendrov et al. (2016) have recently demonstrated that it is possible to construct a v"
J17-4004,D12-1111,0,0.0202343,"Missing"
J17-4004,Q14-1006,0,0.0151611,"in semantic similarity (or graded synonymy computations, see Section 2 again), Vendrov et al. (2016) have recently demonstrated that it is possible to construct a vector space or a word embedding model that specializes in the lexical entailment relation, rather than in the more popular similarity/synonymy relation. The model is then applied to a variety of tasks including ungraded LE detection and directionality. 809 Computational Linguistics Volume 43, Number 4 Figure 6 A slice of the visual–semantic hierarchy. The toy example is taken from Vendrov et al. (2016), inspired by the resource of Young et al. (2014). The order embedding model exploits the partial order structure of a visual–semantic hierarchy (see Figure 6) by learning a mapping which is not distance-preserving but order-preserving between the visual–semantic hierarchy and a partial order over the embedding space. It learns a mapping from a partially ordered set (U, U ) into a partially ordered embedding space (V, V ): the ordering of a pair in U is then based on the ordering in the embedding space. The chosen embedding space is the reversed product order on RN + , defined by the conjunction of total orders on each coordinate: ~ ~ Y X"
J17-4004,P14-2089,0,0.0202901,"iminary analysis advocates the use of more sophisticated learning algorithms in future work. Another path of research work could investigate how to exploit more training data from resources other than HyperLex to yield improved graded LE models. 7.4 Further Discussion: Specializing Semantic Spaces Following the growing interest in word representation learning, this work also touches upon the ideas of vector/semantic space specialization: A desirable property of representation models is their ability to steer their output vector spaces according to explicit linguistic and dictionary knowledge (Yu and Dredze 2014; Astudillo et al. 2015; Faruqui 823 Computational Linguistics Volume 43, Number 4 et al. 2015; Liu et al. 2015; Wieting et al. 2015; Mrkši´c et al. 2016; Vuli´c et al. 2017, inter alia). Previous work showed that it is possible to build vector spaces specialized for capturing different lexical relations (e.g., antonymy [Yih, Zweig, and Platt 2012; Ono, Miwa, and Sasaki 2015]) or distinguishing between similarity and relatedness (Kiela, Hill, and Clark 2015). Yet, it is to be seen how to build a representation model specialized for the graded LE relation. An analogy with (graded) semantic simi"
J17-4004,N13-1120,0,0.0100078,"ther relations. HyperLex, on the other hand, targets a different type of evaluation. The graded entailment function fgraded defines the following mapping: fgraded : (X, Y) → R+ 0 (4) fgraded outputs the strength of the lexical entailment relation s ∈ R+ 0 . By adopting the graded LE paradigm, HyperLex thus measures the degree of lexical entailment between words X and Y constituting the order-sensitive pair (X, Y). From another perspective, it measures the typicality and graded membership of the instance X for the class/category Y. From the relational similarity viewpoint (Jurgens et al. 2012; Zhila et al. 2013), it also measures the prototypicality of the pair (X, Y) for the LE relation. 3.1.2 Evaluation Sets BLESS. Introduced by Baroni and Lenci (2011), the original BLESS evaluation set includes 200 concrete English nouns as target concepts (i.e., X-s from the pairs (X, Y)), equally divided between animate and inanimate entities. A total of 175 concepts were extracted from the McRae feature norms data set (McRae et al. 2005), and the remaining 25 were selected manually by the authors. These concepts were then paired to 8,625 7 The terms intension and extension assume classical intensional and exten"
J17-4004,J09-3004,0,0.039597,"inguistic entailment more effectively. 2. Graded Lexical Entailment Note on Terminology. Because of dual and inconsistent use in prior work, in this work we use the term lexical entailment (LE) in its stricter definition. It refers precisely to the taxonomical asymmetric hyponymy–hypernymy relation, also known as IS - A, or TYPE - OF relation (Hearst 1992; Snow, Jurafsky, and Ng 2004; Weeds, Weir, and McCarthy 2004; Pantel and Pennacchiotti 2006; Do and Roth 2010, inter alia), e.g., snake is a TYPE - OF animal, computer is a TYPE - OF machine. This is different from an alternative definition (Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010; Turney and Mohammad 2015) as substitutable lexical entailment: This relation holds for a pair of words (X, Y) if a possible meaning of one word (i.e., X) entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. This definition is looser and more general than the TYPE - OF definition, as it also encompasses other lexical relations such as synonymy, metonymy, meronymy, and so forth.3 Definitions. The classical definition of ungraded lexical entailment is as follows: Given a concept word pair (X, Y), Y is a hypernym"
J17-4004,H05-1079,0,\N,Missing
J17-4004,W07-1401,0,\N,Missing
J17-4004,C98-2122,0,\N,Missing
J19-3005,P13-2037,0,0.0744461,"Missing"
J19-3005,W17-0401,0,0.0483167,"Missing"
J19-3005,P15-2044,0,0.0358947,"Missing"
J19-3005,W14-4203,0,0.0512933,"Missing"
J19-3005,P15-1040,0,0.0140754,"k is word sense disambiguation, as senses can be propagated from multilingual word graphs (Silberer and Ponzetto 2010) by bootstrapping from a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WordEmbedding-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016). Another task where lexical semantics is crucial is sentiment analysis, for similar reasons: Bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, and Sebastiani 2015; Ziser and Reichart 2018). Moreover, sentiment 587 Computational Linguistics Volume 45, Number 3 analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli´c, and Korhonen 2017). Finally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al."
J19-3005,Q16-1031,0,0.0249656,"Missing"
J19-3005,D18-1549,0,0.0165078,"da et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, and Sebastiani 2015; Ziser and Reichart 2018). Moreover, sentiment 587 Computational Linguistics Volume 45, Number 3 analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli´c, and Korhonen 2017). Finally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al. 2018) set-ups. In fact, the degree of fusion between roots and inflectional/derivative morphemes impacts the type/token ratio of texts, and consequently their rate of infrequent words. Moreover, the ambiguity of mapping between form and meaning of morphemes determines the usefulness of injecting character-level information (Gerz et al. 2018a, 2018b). This variation has to be taken into account in both language transfer and multilingual joint learning. As a final note, we stress that the addition of new features does not concern just future work, but also the existing typology-savvy methods, which c"
J19-3005,D17-1011,0,0.193671,"ases. In this article, we provide an extensive survey of typologically informed NLP methods to date, including the more recent neural approaches not previously surveyed in this area. We consider the impact of typological (including both structural and semantic) information on system performance and discuss the optimal sources for such information. Traditionally, typological information has been obtained from hand-crafted databases and, therefore, it tends to be coarse-grained and incomplete. Recent research has focused on inferring typological information automatically from multilingual data (Asgari and Schütze 2017, inter alia), with the specific purpose of obtaining a more complete and finer-grained set of feature values. We survey these techniques and discuss ways to integrate their predictions into the current NLP algorithms. To the best of our knowledge, this has not yet been covered in the existing literature. In short, the key questions our paper addresses can be summarized as follows: (i) Which NLP tasks and applications can benefit from typology? (ii) What are the advantages and limitations of currently available typological databases? Can data-driven inference of typological features offer an a"
J19-3005,D08-1014,0,0.0572004,"Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3). Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1(c), a source sentence is machine translated into a target language (Banea et al. 2008), or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translated documents can also be used to generate multilingual sentence representations, which facilitate language transfer (Zhou, Wan, and Xiao 2016). Some of these methods are hampered by their resource requirements. In fact, annotation projection and translation need parallel texts to align words and train translation systems, respectively (Agi´c, Hovy, and Søgaard 2015). Moreover, comparisons of stateof-the-art algorithms revealed that model"
J19-3005,W09-0106,0,0.0830635,"ction The world’s languages may share universal features at a deep, abstract level, but the structures found in real-world, surface-level texts can vary significantly. This crosslingual variation has challenged the development of robust, multilingually applicable Natural Language Processing (NLP) technology, and as a consequence, existing NLP is still largely limited to a handful of resource-rich languages. The architecture design, training, and hyper-parameter tuning of most current algorithms are far from being language-agnostic, and often inadvertently incorporate language-specific biases (Bender 2009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das"
J19-3005,P07-1036,0,0.126415,"Missing"
J19-3005,Q18-1039,0,0.0162199,"al. 2012). The same ideas could be exploited in deep learning algorithms. We have seen in § 3.2 that multilingual joint models combine both shared and language-dependent parameters in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, similar to what Chen et al. (2018) implemented by predicting language identity. Recently, Hu et al. (2016a, 2016b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similar to CODL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models. A particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representa"
J19-3005,C16-1298,0,0.0281595,"Missing"
J19-3005,P11-1061,0,0.244408,"009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das and Petrov 2011; Täckström, McDonald, and Uszkoreit 2012, inter alia). Some multilingual applications, such as Neural Machine Translation and Information Retrieval, have been facilitated by learning joint models that learn from several languages (Ammar et al. 2016; Johnson et al. 2017, inter alia) or via multilingual distributed representations of words and sentences (Mikolov, Le, and Sutskever 2013, inter alia). Such techniques can lead to significant improvements in performance and parameter efficiency over monolingual baselines (Pappas and Popescu-Belis 2017). Another, highly promising source of informati"
J19-3005,P07-1009,0,0.17624,"Missing"
J19-3005,P16-1038,0,0.174612,"16) selected 190 binarized phonological features from URIEL (Littel, Mortensen, and Levin 2016). These features encoded the presence of single segments, classes of segments, minimal contrasts in a language inventory, and the number of segments in a class. For instance, they record whether a language allows two sounds to differ only in voicing, such as /t/ and /d/. Finally, a small number of experiments adopted the entire feature inventory of typological databases, without any sort of pre-selection. In particular, Agi´c (2017) and Ammar et al. (2016) extracted all the features in WALS, whereas Deri and Knight (2016) extracted all the features in URIEL. Schone and Jurafsky (2001) did not resort to basic typological features, but rather to “several hundred [implicational universals] applicable to syntax” drawn from the Universal Archive (Plank and Filiminova 1996). Typological attributes that are extracted from typological databases are typically represented as feature vectors in which each dimension encodes a feature value. This feature representation is often binarized (Georgi, Xia, and Lewis 2010): For each possible value v of each database attribute a, a new feature is created with value 1 if it corres"
J19-3005,P15-2139,0,0.0321535,"ver pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Bambara on the left and Warlpiri on the right). Parameter sharing, however, does not necessarily imply parameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Universals Figure 2 In multilingual joint learning, representations can be private or shared acros"
J19-3005,D15-1040,0,0.041461,"ver pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Bambara on the left and Warlpiri on the right). Parameter sharing, however, does not necessarily imply parameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Universals Figure 2 In multilingual joint learning, representations can be private or shared acros"
J19-3005,D16-1136,0,0.0403964,"Missing"
J19-3005,D12-1001,0,0.018952,"Missing"
J19-3005,P17-2093,0,0.047937,"Missing"
J19-3005,N15-1184,0,0.0387115,"et al. (2016a, 2016b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similar to CODL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models. A particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representation learning (§ 3.3). In this domain, a number of works (Faruqui et al. 2015; Rothe and Schütze 2015; Mrkši´c et al. 2016; Osborne, Narayan, and Cohen 2016) have proposed means through which external knowledge sourced from linguistic resources (such as WordNet, BabelNet, or lists of morphemes) can be encoded in word embeddings. Among the state-of-the-art specialization methods ATTRACT- REPEL (Mrkši´c et al. 2017; Vuli´c et al. 2017) pushes together or pulls apart 589 Computational Linguistics Volume 45, Number 3 vector pairs according to relational constraints, while preserving the relationship between words in the original space and possibly propagating the specializ"
J19-3005,C10-1044,0,0.0863316,"Missing"
J19-3005,Q18-1032,1,0.878475,"Missing"
J19-3005,D18-1029,1,0.881238,"Missing"
J19-3005,N15-1157,0,0.0210384,"Missing"
J19-3005,C16-1002,0,0.0416532,"ching scenarios (Adel, Vu, and Schultz 2013). In fact, multilingual joint learning improves over pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Bambara on the left and Warlpiri on the right). Parameter sharing, however, does not necessarily imply parameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Univ"
J19-3005,P15-1119,0,0.0603047,"Missing"
J19-3005,P16-1228,0,0.0498272,"Missing"
J19-3005,D16-1173,0,0.0523125,"Missing"
J19-3005,P11-1057,0,0.205918,"gure 1(a), a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly between corresponding words and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer"
J19-3005,C12-1089,0,0.0473073,"Missing"
J19-3005,P13-1117,0,0.0959291,"Missing"
J19-3005,P11-2055,0,0.04082,"Missing"
J19-3005,I08-2093,0,0.0406793,"ions. Similarly, Zhang et al. (2016) transfer PoS annotation with a model transfer technique relying on multilingual embeddings, created through monolingual mapping (see § 3.3). After the projection, they predict feature values with a multiclass support vector machine using PoS tag n-gram features. Finally, typological information can be extracted from Interlinear Glossed Texts (IGT). Such collections of example sentences are collated by linguists and contain grammatical glosses with morphological information. These can guide alignment between the example sentence and its English translation. Lewis and Xia (2008) and Bender et al. (2013) project chunking information from English and train context free grammars on target languages. After collapsing identical rules, they arrange them by frequency and infer word order features. 574 Ponti et al. Modeling Language Variation and Universals Unsupervised Propagation Morphosyntactic Annotation Table 2 An overview of the strategies for prediction of typological features. Author Details Requirements Liu (2010) Lewis and Xia (2008) Treebank count IGT projection Treebank IGT, source chunker 20 97 Bender et al. (2013) IGT projection IGT, source chunker 31 Östling ("
J19-3005,P13-3022,0,0.408676,"k-based language vector WALS 2,150 whole WALS whole whole PoS tag data set 27,824 phonology, morphology, syntax Logistic regression WALS whole whole Bayesian + feature and language interactions Feed-forward Neural Network Genealogy and WALS 2,607 whole Coke, King, and Radev (2016) Littel, Mortensen, and Levin (2016) Berzak, Reichart, and Katz (2014) Supervised Learning Malaviya, Neubig, and Littell (2017) Bjerva and Augenstein (2018) Takamura, Nagata, and Kawasaki (2016) Murawaki (2017) Wang and Eisner (2017) Cotterell and Eisner (2017) Cross-lingual distribution Daumé III and Campbell (2007) Lu (2013) Wälchli and Cysouw (2012) Asgari and Schütze (2017) Roy et al. (2014) Determinant Point Process with neural features Implication universals Automatic discovery Sentence edit distance Pivot alignment Correlations in counts and entropy Genealogy and WALS Genealogy and WALS ESL texts NMT data set WALS, tagger, synthetic treebanks WALS Genealogy and WALS Genealogy and WALS Multi-parallel texts, pivot Multi-parallel texts, pivot None Languages Features word order word and morpheme order, determiners word order and case alignment 986 word order 6 word order 325 whole word order and passive whole 14"
J19-3005,W15-1521,0,0.0349482,"Missing"
J19-3005,D17-1268,0,0.142382,"Missing"
J19-3005,P08-1099,0,0.0527456,"hallenge: How can the output of the model be biased to agree with the constraints while the efficiency of the search procedure is kept? In this article we do not answer this question directly but rather survey a number of approaches that succeed in dealing with it. Because linear models have been prominent in NLP research for a much longer time, it is not surprising that frameworks for the integration of soft constraints into these models are much more developed. The approaches proposed for this purpose include posterior regularization (PR) (Ganchev et al. 2010), generalized expectation (GE) (Mann and McCallum 2008), constraint-driven learning (CODL) (Chang, Ratinov, and Roth 2007), dual decomposition (DD) (Globerson and Jaakkola 2007; Komodakis, Paragios, and Tziritas 2011), and Bayesian modeling (Cohen 2016). These techniques use different types of knowledge encoding—for example, PR uses expectation constraints on the posterior parameter distribution, GE prefers parameter settings where the model’s distribution on unsupervised data matches a predefined target distribution, CODL enriches existing statistical models with Integer Linear Programming constraints, and in Bayesian modeling a prior distributio"
J19-3005,P05-1012,0,0.0516887,"Missing"
J19-3005,Q17-1022,1,0.921061,"Missing"
J19-3005,N16-1018,0,0.029262,"Missing"
J19-3005,I17-1046,0,0.464172,"ea, and typology-based Nearest Neighbors English as a Second Language–based Nearest Neighbors Task-based language vector Task-based language vector WALS 2,150 whole WALS whole whole PoS tag data set 27,824 phonology, morphology, syntax Logistic regression WALS whole whole Bayesian + feature and language interactions Feed-forward Neural Network Genealogy and WALS 2,607 whole Coke, King, and Radev (2016) Littel, Mortensen, and Levin (2016) Berzak, Reichart, and Katz (2014) Supervised Learning Malaviya, Neubig, and Littell (2017) Bjerva and Augenstein (2018) Takamura, Nagata, and Kawasaki (2016) Murawaki (2017) Wang and Eisner (2017) Cotterell and Eisner (2017) Cross-lingual distribution Daumé III and Campbell (2007) Lu (2013) Wälchli and Cysouw (2012) Asgari and Schütze (2017) Roy et al. (2014) Determinant Point Process with neural features Implication universals Automatic discovery Sentence edit distance Pivot alignment Correlations in counts and entropy Genealogy and WALS Genealogy and WALS ESL texts NMT data set WALS, tagger, synthetic treebanks WALS Genealogy and WALS Genealogy and WALS Multi-parallel texts, pivot Multi-parallel texts, pivot None Languages Features word order word and morpheme"
J19-3005,P12-1066,0,0.286336,"Missing"
J19-3005,D10-1120,0,0.0346019,"Tziritas 2011), and Bayesian modeling (Cohen 2016). These techniques use different types of knowledge encoding—for example, PR uses expectation constraints on the posterior parameter distribution, GE prefers parameter settings where the model’s distribution on unsupervised data matches a predefined target distribution, CODL enriches existing statistical models with Integer Linear Programming constraints, and in Bayesian modeling a prior distribution is defined on the model parameters. PR has already been used for incorporating universal linguistic knowledge into an unsupervised parsing model (Naseem et al. 2010). In the future, it could be extended to typological knowledge, which is a good fit for soft constraints. As another option, Bayesian modeling sets prior probability distributions according to the relationships encoded in typological features (Schone and Jurafsky 2001). Finally, DD has been applied to multi-task learning, which paves the way for typological knowledge encoding through a multi-task architecture in which one of the tasks is the actual NLP application and the other is the data-driven prediction of typological features. In fact, a modification of this architecture has already been"
J19-3005,W11-2124,0,0.0331622,"rameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Universals Figure 2 In multilingual joint learning, representations can be private or shared across languages. Tied parameters are shown as neurons with identical color. Image adapted from Fang and Cohn (2017), representing multilingual PoS tagging for Bambara (left) and Warlpiri (right). (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016; Östling and Tiedemann 2017) or neural machine translation tasks (Ha, Niehues, and Waibel 2016; Johnson et al. 2017). Ammar et al. (2016) instead used language vector"
J19-3005,C16-1123,1,0.918794,"Missing"
J19-3005,Q16-1030,0,0.164108,"Missing"
J19-3005,P15-2034,0,0.172911,"syntactically annotated texts. For example, word order features can be calculated by counting the average direction of dependency relations or constituency hierarchies (Liu 2010). Consider the tree of a sentence in Welsh from Bender et al. (2013) in Figure 6. The relative order of verb– subject, and verb–object can be deduced from the position of the relevant nodes VBD, NNS , and NNO (highlighted). Morphosyntactic annotation is often unavailable for resource-lean languages. In such cases, it can be projected from a source language to a target language through language transfer. For instance, Östling (2015) projects source morphosyntactic annotation directly to several languages through a multilingual word alignment. After the alignment and projection, word order features are calculated by the average direction of dependency relations. Similarly, Zhang et al. (2016) transfer PoS annotation with a model transfer technique relying on multilingual embeddings, created through monolingual mapping (see § 3.3). After the projection, they predict feature values with a multiclass support vector machine using PoS tag n-gram features. Finally, typological information can be extracted from Interlinear Gloss"
J19-3005,E17-2102,0,0.0715416,"g multilingual PoS tagging for Bambara (left) and Warlpiri (right). (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016; Östling and Tiedemann 2017) or neural machine translation tasks (Ha, Niehues, and Waibel 2016; Johnson et al. 2017). Ammar et al. (2016) instead used language vectors as a prior for language identity or typological features. In § 5.2, we discuss ways in which typological knowledge is used to balance private and shared neural network components and provide informative input language vectors. In § 6.3, we argue that language vectors do not need to be limited to features extracted from typological databases, but should also include automatically induced typological information (Malaviya, Neubig, and Littell 2017, see § 4.3"
J19-3005,H05-1108,0,0.0802046,"cific biases (Bender 2009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das and Petrov 2011; Täckström, McDonald, and Uszkoreit 2012, inter alia). Some multilingual applications, such as Neural Machine Translation and Information Retrieval, have been facilitated by learning joint models that learn from several languages (Ammar et al. 2016; Johnson et al. 2017, inter alia) or via multilingual distributed representations of words and sentences (Mikolov, Le, and Sutskever 2013, inter alia). Such techniques can lead to significant improvements in performance and parameter efficiency over monolingual baselines (Pappas and Popescu-Belis 2017). Another, highly promisin"
J19-3005,I17-1102,0,0.0522275,"Missing"
J19-3005,P18-1142,1,0.820943,"Missing"
J19-3005,D18-1026,1,0.847658,"Missing"
J19-3005,S17-1003,1,0.801184,"Missing"
J19-3005,N12-1008,1,0.844552,". As another option, Bayesian modeling sets prior probability distributions according to the relationships encoded in typological features (Schone and Jurafsky 2001). Finally, DD has been applied to multi-task learning, which paves the way for typological knowledge encoding through a multi-task architecture in which one of the tasks is the actual NLP application and the other is the data-driven prediction of typological features. In fact, a modification of this architecture has already been applied to minimally supervised learning and domain adaptation with soft (non-typological) constraints (Reichart and Barzilay 2012; Rush et al. 2012). The same ideas could be exploited in deep learning algorithms. We have seen in § 3.2 that multilingual joint models combine both shared and language-dependent parameters in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, simila"
J19-3005,P15-2040,0,0.0202231,"ribution of each language and example. The selection is typically carried out through general language similarity metrics. For instance, Deri and Knight (2016) base their selection on the URIEL language typology database, considering information about genealogical, geographic, syntactic, and phonetic properties. This facilitates language transfer of grapheme-to-phoneme models, by guiding the choice of source languages and aligning phoneme inventories. Metrics for source selection can also be extracted in a data-driven fashion, without explicit reference to structured taxonomies. For instance, Rosa and Zabokrtsky (2015) estimate the Kullback–Leibler divergence between PoS trigram distributions for delexicalized parser transfer. In order to approximate the divergence in syntactic structures between languages, Ponti et al. (2018a) utilize the Jaccard distance between morphological feature sets and the tree edit distance of delexicalized dependency parses of translationally equivalent sentences. A priori and bottom–up approaches can also be combined. For delexicalized parser transfer, Agi´c (2017) relies on a weighted sum of distances based on (1) the PoS divergence defined by Rosa and Zabokrtsky (2015); (2) th"
J19-3005,P15-1173,0,0.0316864,"Missing"
J19-3005,P18-1084,1,0.891535,"Missing"
J19-3005,C14-1098,0,0.0637796,"Missing"
J19-3005,D12-1131,1,0.808979,"Missing"
J19-3005,S10-1027,0,0.0116844,"s-lingual information about frame semantics can be extracted, for example, from the Valency Patterns Leipzig database (ValPaL). Typological information regarding lexical semantics patterns can further assist various NLP tasks by providing information about translationally equivalent words across languages. Such information is provided in databases such as the World Loanword Database (WOLD), the Intercontinental Dictionary Series (IDS), and the Automated Similarity Judgment Program (ASJP). One example task is word sense disambiguation, as senses can be propagated from multilingual word graphs (Silberer and Ponzetto 2010) by bootstrapping from a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WordEmbedding-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016). Another task where lexical semantics is crucial is sentiment analysis, for similar reasons: Bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, a"
J19-3005,P08-1084,0,0.26666,"ure design, training, and hyper-parameter tuning of most current algorithms are far from being language-agnostic, and often inadvertently incorporate language-specific biases (Bender 2009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das and Petrov 2011; Täckström, McDonald, and Uszkoreit 2012, inter alia). Some multilingual applications, such as Neural Machine Translation and Information Retrieval, have been facilitated by learning joint models that learn from several languages (Ammar et al. 2016; Johnson et al. 2017, inter alia) or via multilingual distributed representations of words and sentences (Mikolov, Le, and Sutskever 2013, inter alia). Such techniques can"
J19-3005,P11-2120,0,0.0488046,"hand, the accuracy of PoS-based metrics deteriorates easily in scenarios with scarce amounts of data. Source language selection is a special case of source language weighting where weights are one-hot vectors. However, weights can also be gradient and consist of real numbers. Søgaard and Wulff (2012) adapt delexicalized parsers by weighting every 584 Ponti et al. Modeling Language Variation and Universals training instance based on the inverse of the Hamming distance between typological (or genealogical) features in source and target languages. An equivalent bottom–up approach is developed by Søgaard (2011), who weighs source language sentences based on the perplexity between their coarse PoS tags and the predictions of a sequential model trained on the target language. Alternatively, the lack of target annotated data can be alleviated by synthesizing new examples, thus boosting the variety and amount of the source data. For instance, the Galactic Dependency Treebanks stem from real trees whose nodes have been permuted probabilistically, according to the word orders of nouns and verbs in other languages (Wang and Eisner 2016). Synthetic trees improve the performance of model transfer for parsing"
J19-3005,C12-2115,0,0.139636,"from the practice of discarding features that are not discriminative, when they are identical for all the languages in the sample. Another group of studies used more comprehensive feature sets. The feature set of Daiber, Stanojevi´c, and Sima’an (2016) included not only WALS word order features but also nominal categories (e.g., “Conjunctions and Universal Quantifiers”) and nominal syntax (e.g., “Possessive Classification”). Berzak, Reichart, and Katz (2015) considered all features from WALS associated with morphosyntax and pruned out the redundant ones, resulting in a total of 119 features. Søgaard and Wulff (2012) utilized all the 571 Computational Linguistics Volume 45, Number 3 Figure 4 Feature sets used in a sample of typologically informed experiments for dependency parsing. The numbers refer to WALS ordering (Dryer and Haspelmath 2013). features in WALS with the exception of phonological features. Tsvetkov et al. (2016) selected 190 binarized phonological features from URIEL (Littel, Mortensen, and Levin 2016). These features encoded the presence of single segments, classes of segments, minimal contrasts in a language inventory, and the number of segments in a class. For instance, they record whet"
J19-3005,N13-1126,0,0.051696,"Missing"
J19-3005,N12-1052,0,0.116447,"Missing"
J19-3005,P15-1150,0,0.0238395,"Missing"
J19-3005,L16-1011,0,0.120366,"Missing"
J19-3005,W15-2137,0,0.099488,"Missing"
J19-3005,P12-1068,0,0.0507951,"d labor. Furthermore, the immense range of possible tasks and languages makes the aim of a complete coverage unrealistic. One solution to this problem explored by the research community abandons the use of annotated resources altogether and instead focuses on unsupervised learning. This class of methods infers probabilistic models of the observations given some latent variables. In other words, it unravels the hidden structures within unlabeled text data. Although these methods have been used extensively for multilingual applications (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011; Titov and Klementiev 2012, inter alia), their performance tends to lag behind the more linguistically informed supervised learning approaches (Täckström, McDonald, and Nivre 2013). Moreover, they have been rarely combined with typological knowledge. For these reasons, we do not review them in this section. Other promising ways to overcome data scarcity include transferring models or data from resource-rich to resource-poor languages (§ 3.1) or learning joint models from annotated examples in multiple languages (§ 3.2) in order to leverage language interdependencies. Early approaches of this kind have relied on univers"
J19-3005,N16-1161,0,0.0936611,"hn (2017), representing multilingual PoS tagging for Bambara (left) and Warlpiri (right). (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016; Östling and Tiedemann 2017) or neural machine translation tasks (Ha, Niehues, and Waibel 2016; Johnson et al. 2017). Ammar et al. (2016) instead used language vectors as a prior for language identity or typological features. In § 5.2, we discuss ways in which typological knowledge is used to balance private and shared neural network components and provide informative input language vectors. In § 6.3, we argue that language vectors do not need to be limited to features extracted from typological databases, but should also include automatically induced typological information (Malaviya, Neubig"
J19-3005,P16-1157,0,0.0178939,"§ 4.3). 3.3 Multilingual Representation Learning The multilingual algorithms reviewed in § 3.1 and § 3.2 are facilitated by dense realvalued vector representations of words, known as multilingual word embeddings. These can be learned from corpora and provide pivotal lexical features to several downstream NLP applications. In multilingual word embeddings, similar words (regardless of the actual language) obtain similar representations. Various methods to generate multilingual word embeddings have been developed. We follow the classification proposed by Ruder (2018), and we refer the reader to Upadhyay et al. (2016) for an empirical comparison. 567 Computational Linguistics Volume 45, Number 3 Monolingual mapping generates independent monolingual representations and subsequently learns a linear map between a source language and a target language based on a bilingual lexicon (Mikolov, Le, and Sutskever 2013) or in an unsupervised fashion through adversarial networks (Conneau et al. 2017). Alternatively, both spaces can be cast into a new, lower-dimensional space through canonical correlation analysis based on dictionaries (Ammar et al. 2016) or word alignments (Guo et al. 2015). Pseudo-cross-lingual appro"
J19-3005,P11-2084,1,0.843653,"Missing"
J19-3005,P15-2118,1,0.902813,"Missing"
J19-3005,P17-1006,1,0.822412,"Missing"
J19-3005,Q16-1035,0,0.0476954,"source and target languages. An equivalent bottom–up approach is developed by Søgaard (2011), who weighs source language sentences based on the perplexity between their coarse PoS tags and the predictions of a sequential model trained on the target language. Alternatively, the lack of target annotated data can be alleviated by synthesizing new examples, thus boosting the variety and amount of the source data. For instance, the Galactic Dependency Treebanks stem from real trees whose nodes have been permuted probabilistically, according to the word orders of nouns and verbs in other languages (Wang and Eisner 2016). Synthetic trees improve the performance of model transfer for parsing when the source is chosen in a supervised way (performance on target development data) and in an unsupervised way (coverage of target PoS sequences). Rather than generating new synthetic data, Ponti et al. (2018a) leverage typological features to pre-process treebanks in order to reduce their variation in language transfer tasks. In particular, they adapt source trees to the typology of a target language with respect to several constructions in a rule-based fashion. For instance, relative clauses in Arabic (Afro–Asiatic) w"
J19-3005,Q17-1011,0,0.141548,"-based Nearest Neighbors English as a Second Language–based Nearest Neighbors Task-based language vector Task-based language vector WALS 2,150 whole WALS whole whole PoS tag data set 27,824 phonology, morphology, syntax Logistic regression WALS whole whole Bayesian + feature and language interactions Feed-forward Neural Network Genealogy and WALS 2,607 whole Coke, King, and Radev (2016) Littel, Mortensen, and Levin (2016) Berzak, Reichart, and Katz (2014) Supervised Learning Malaviya, Neubig, and Littell (2017) Bjerva and Augenstein (2018) Takamura, Nagata, and Kawasaki (2016) Murawaki (2017) Wang and Eisner (2017) Cotterell and Eisner (2017) Cross-lingual distribution Daumé III and Campbell (2007) Lu (2013) Wälchli and Cysouw (2012) Asgari and Schütze (2017) Roy et al. (2014) Determinant Point Process with neural features Implication universals Automatic discovery Sentence edit distance Pivot alignment Correlations in counts and entropy Genealogy and WALS Genealogy and WALS ESL texts NMT data set WALS, tagger, synthetic treebanks WALS Genealogy and WALS Genealogy and WALS Multi-parallel texts, pivot Multi-parallel texts, pivot None Languages Features word order word and morpheme order, determiners word"
J19-3005,D18-1215,0,0.0199401,"ltilingual joint models combine both shared and language-dependent parameters in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, similar to what Chen et al. (2018) implemented by predicting language identity. Recently, Hu et al. (2016a, 2016b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similar to CODL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models. A particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representation learning (§ 3.3). In this domain, a number of works (Faruqui et al. 2015; Rothe and Schütze 2015; Mr"
J19-3005,Q14-1005,0,0.0236961,"et al. (2005). In its original formulation, as illustrated in Figure 1(a), a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly between corresponding words and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompati"
J19-3005,D14-1187,0,0.0145363,"text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly between corresponding words and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-indepe"
J19-3005,W14-1613,0,0.0141525,"ns and subsequently learns a linear map between a source language and a target language based on a bilingual lexicon (Mikolov, Le, and Sutskever 2013) or in an unsupervised fashion through adversarial networks (Conneau et al. 2017). Alternatively, both spaces can be cast into a new, lower-dimensional space through canonical correlation analysis based on dictionaries (Ammar et al. 2016) or word alignments (Guo et al. 2015). Pseudo-cross-lingual approaches merge words with contexts of other languages and generate representations based on this mixed corpus. Substitutions are based on Wiktionary (Xiao and Guo 2014) or machine translation (Gouws and Søgaard 2015; Duong et al. 2016). Moreover, the mixed corpus can be produced by randomly shuffling words between aligned documents in two languages (Vuli´c and Moens 2015). Cross-lingual training approaches jointly learn embeddings from parallel corpora and enforce cross-lingual constraints. This involves minimizing the distance of the hidden sentence representations of the two languages (Hermann and Blunsom 2014) or decoding one from the other (Lauly, Boulanger, and Larochelle 2013), possibly adding a correlation term to the loss (Chandar et al. 2014). Joint"
J19-3005,H01-1035,0,0.123357,"Missing"
J19-3005,I08-3008,0,0.0176835,"). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3). Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1(c), a source sentence is machine translat"
J19-3005,C16-1044,0,0.0230991,"Missing"
J19-3005,D15-1213,0,0.0647972,"Missing"
J19-3005,N16-1156,0,0.0228005,"Missing"
J19-3005,D12-1125,1,0.809705,"orical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3). Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1(c), a source sentence is machine translated into a target language (Banea et al. 2008), or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translat"
J19-3005,P16-1133,0,0.0606511,"Missing"
J19-3005,D18-1022,1,0.844622,"om a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WordEmbedding-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016). Another task where lexical semantics is crucial is sentiment analysis, for similar reasons: Bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, and Sebastiani 2015; Ziser and Reichart 2018). Moreover, sentiment 587 Computational Linguistics Volume 45, Number 3 analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli´c, and Korhonen 2017). Finally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al. 2018) set-ups. In fact, the degree of fusion between roots and inflectional/derivative morphemes impacts the type/token ratio of texts, and co"
J19-3005,Q17-1024,0,\N,Missing
J19-3005,D18-1269,0,\N,Missing
jiang-etal-2014-native,N12-1033,0,\N,Missing
jiang-etal-2014-native,P03-1054,0,\N,Missing
jiang-etal-2014-native,C12-1027,0,\N,Missing
jiang-etal-2014-native,D11-1148,0,\N,Missing
jiang-etal-2014-native,W07-0602,0,\N,Missing
jiang-etal-2014-native,W13-1706,0,\N,Missing
K17-1013,W13-3520,0,0.0499698,"eled conjll. If both are used, the label is simply conj=conjlr+conjll.7 Consequently, the individual context bags we use in all experiments are: subj, obj, comp, nummod, appos, nmod, acl, amod, prep, adv, compound, conjlr, conjll. 4.2 Training and Evaluation We run the algorithm for context configuration selection only once, with the SGNS training setup described below. Our main evaluation setup is presented below, but the learned configurations are tested in additional setups, detailed in Sect. 5. Training Data Our training corpus is the cleaned and tokenised English Polyglot Wikipedia data (Al-Rfou et al., 2013),8 consisting of approxi7 Given the coordination structure boys and girls, conjlr training pairs are (boys, girls_conj), (girls, boys_conj −1 ), while conjll pairs are (boys, girls_conj), (girls, boys_conj). 8 https://sites.google.com/site/rmyeid/projects/polyglot 116 mately 75M sentences and 1.7B word tokens. The Wikipedia data were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the state-of-the art TurboTagger (Martins et al., 2013).9 The parser was trained using default settings (SVM MIRA with 20 iterations, no further parameter tuning) on the TRAIN + DEV portion of t"
K17-1013,P14-2131,0,0.0737811,"v et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still th"
K17-1013,P14-1023,0,0.0288632,"alian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and ve"
K17-1013,C10-1011,0,0.0100926,"nj), (girls, boys_conj −1 ), while conjll pairs are (boys, girls_conj), (girls, boys_conj). 8 https://sites.google.com/site/rmyeid/projects/polyglot 116 mately 75M sentences and 1.7B word tokens. The Wikipedia data were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the state-of-the art TurboTagger (Martins et al., 2013).9 The parser was trained using default settings (SVM MIRA with 20 iterations, no further parameter tuning) on the TRAIN + DEV portion of the UD treebank annotated with UPOS tags. The data were then parsed with UD using the graph-based Mate parser v3.61 (Bohnet, 2010)10 with standard settings on TRAIN + DEV of the UD treebank. Evaluation We experiment with the verb pair (222 pairs), adjective pair (111 pairs), and noun pair (666 pairs) portions of SimLex-999. We report Spearman’s ρ correlation between the ranks derived from the scores of the evaluated models and the human scores. Our evaluation setup is borrowed from Levy et al. (2015): we perform 2-fold cross-validation, where the context configurations are optimised on a development set, separate from the unseen test data. Unless stated otherwise, the reported scores are always the averages of the 2 runs"
K17-1013,D14-1082,0,0.0395504,"at the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, rea"
K17-1013,W14-1618,0,0.0925589,"le features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the ind"
K17-1013,P06-1038,1,0.647241,"sequential position of each context word. Given the example from Fig. 1, POSIT with the window size 2 extracts the following contexts for discovers: Australian_-2, scientist_-1, stars_+2, with_+1. - DEPS-All: All dependency links without any context selection, extracted from dependency-parsed data with prepositional arc collapsing. - COORD: Coordination-based contexts are used as fast lightweight contexts for improved representations of adjectives and verbs (Schwartz et al., 2016). This is in fact the conjlr context bag, a subset of DEPS-All. - SP: Contexts based on symmetric patterns (SPs, (Davidov and Rappoport, 2006; Schwartz et al., 2015)). For example, if the word X and the word 9 10 Context Group Adj Verb Noun conjlr (A+N+V) obj (N+V) prep (N+V) amod (A+N) compound (N) adv (V) nummod (-) 0.415 -0.028 0.188 0.479 -0.124 0.197 -0.142 0.281 0.309 0.344 0.058 -0.019 0.342 -0.065 0.401 0.390 0.387 0.398 0.416 0.104 0.029 Table 1: 2-fold cross-validation results for an illustrative selection of individual context bags. Results are presented for the noun, verb and adjective subsets of SimLex-999. Values in parentheses denote the class-specific initial pools to which each context is selected based on its ρ sc"
K17-1013,de-marneffe-etal-2014-universal,0,0.0293418,"Missing"
K17-1013,W08-1301,0,0.125848,"Missing"
K17-1013,N15-1184,0,0.0343743,"searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scopre stelle con telescopio nmod amod Australian nsubj scientist dobj discovers case stars with telescope prep:with Figure 1: Ext"
K17-1013,C12-1059,0,0.0242772,"ed in Sect. 5.1 are useful when SGNS is trained in another English setup (Schwartz et al., 2016), with more training data and other annotation and parser choices, while evaluation is still performed on SimLex-999. In this setup the training corpus is the 8B words corpus generated by the word2vec script.13 A preprocessing step now merges common word pairs and triplets to expression tokens (e.g., Bilbo_Baggins). The corpus is parsed with labelled Stanford dependencies (de Marneffe and Manning, 2008) using the Stanford POS Tagger (Toutanova et al., 2003) and the stack version of the MALT parser (Goldberg and Nivre, 2012). SGNS preprocessing and parameters are also replicated; we now 13 Table 6: Results on the A/V/N SimLex-999 subsets, and on the entire set (All) in the setup from Schwartz et al. (2016). d = 500. BEST-* are again the best class-specific configs returned by Alg. 1. train 500-dim embeddings as in prior work.14 Results are presented in Tab. 6. The imported class-specific configurations, computed using a much smaller corpus (Sect. 5.1), again outperform competitive baseline context types for adjectives and nouns. The BEST-VERBS configuration is outscored by SP, but the margin is negligible. We als"
K17-1013,D15-1242,0,0.0909931,"bs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case do"
K17-1013,Q15-1016,0,0.665137,"trate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, S"
K17-1013,N15-1142,0,0.0798687,"an the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scop"
K17-1013,D15-1161,0,0.115421,"an the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scop"
K17-1013,P15-1145,0,0.038685,"ore useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Sci"
K17-1013,P13-2109,0,0.0606688,"Missing"
K17-1013,P13-2017,0,0.0177128,"rch algorithm over the configuration space. Context Configuration Space We focus on the configuration space based on dependency-based contexts (DEPS) (Padó and Lapata, 2007; Utt and Padó, 2014). We choose this space due to multiple reasons. First, dependency structures are known to be very useful in capturing functional relations between words, even if these relations are long distance. Second, they have been proven useful in learning word embeddings (Levy and Goldberg, 2014a; Melamud et al., 2016). Finally, owing to the recent development of the Universal Dependencies (UD) annotation scheme (McDonald et al., 2013; Nivre et al., 2016)1 it is possible to reason over dependency structures in a multilingual manner (e.g., Fig. 1). Consequently, a search algorithm in such DEPS-based configuration space can be developed for multiple languages based on the same design principles. Indeed, in this work we show that the optimal configurations for English translate to improved representations in two additional languages, German and Italian. And so, given a (UD-)parsed training corpus, for each target word w with modifiers m1 , . . . , mk and a head h, the word w is paired with context elements m1 _r1 , . . . , mk"
K17-1013,N15-1050,0,0.0247132,"its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended the comparison to more languages, reaching similar conclusions. Schwartz et al. (2016), showed that symmetric patterns are useful as contexts for V and A si"
K17-1013,N16-1118,0,0.353877,"ill considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when l"
K17-1013,P14-2050,0,0.0802645,"le features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the ind"
K17-1013,Q17-1022,1,0.874088,"Missing"
K17-1013,J07-2002,0,0.313892,"Word representation models typically train on (word, context) pairs. Traditionally, most models use bag-of-words (BOW) contexts, which represent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended"
K17-1013,D14-1162,0,0.0756258,"figurations across languages: these configurations improve the SGNS performance when trained with German or Italian corpora and evaluated on class-specific subsets of the multilingual SimLex-999 (Leviant and Reichart, 2015), without any language-specific tuning. 2 Related Work Word representation models typically train on (word, context) pairs. Traditionally, most models use bag-of-words (BOW) contexts, which represent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic"
K17-1013,petrov-etal-2012-universal,0,0.227836,"path of the best-scoring lower-level configuration even if its score is lower than that of its origin. As we do not observe any significant improvement with this variant, we opt for the faster and simpler one. 5 https://bitbucket.org/yoavgo/word2vecf 6 SGNS for all models was trained using stochastic gradient descent and standard settings: 15 negative samples, global learning rate: 0.025, subsampling rate: 1e − 4, 15 epochs. Universal Dependencies as Labels The adopted UD scheme leans on the universal Stanford dependencies (de Marneffe et al., 2014) complemented with the universal POS tagset (Petrov et al., 2012). It is straightforward to “translate” previous annotation schemes to UD (de Marneffe et al., 2014). Providing a consistently annotated inventory of categories for similar syntactic constructions across languages, the UD scheme facilitates representation learning in languages other than English, as shown in (Vuli´c and Korhonen, 2016; Vuli´c, 2017). Individual Context Bags Standard post-parsing steps are performed in order to obtain an initial list of individual context bags for our algorithm: (1) Prepositional arcs are collapsed ((Levy and Goldberg, 2014a; Vuli´c and Korhonen, 2016), see Fig."
K17-1013,P93-1034,0,0.74216,"Missing"
K17-1013,K15-1026,1,0.948135,"for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when learning representations for nouns (N). In thi"
K17-1013,N16-1060,1,0.312769,"). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when learning representations for nouns (N). In this work, we propose a simple yet effective framework for selecting context configurations, which yields improved representations for verbs, adjectives, and nouns. We s"
K17-1013,N03-1033,0,0.0161394,"ining Setup We first test whether the context configurations learned in Sect. 5.1 are useful when SGNS is trained in another English setup (Schwartz et al., 2016), with more training data and other annotation and parser choices, while evaluation is still performed on SimLex-999. In this setup the training corpus is the 8B words corpus generated by the word2vec script.13 A preprocessing step now merges common word pairs and triplets to expression tokens (e.g., Bilbo_Baggins). The corpus is parsed with labelled Stanford dependencies (de Marneffe and Manning, 2008) using the Stanford POS Tagger (Toutanova et al., 2003) and the stack version of the MALT parser (Goldberg and Nivre, 2012). SGNS preprocessing and parameters are also replicated; we now 13 Table 6: Results on the A/V/N SimLex-999 subsets, and on the entire set (All) in the setup from Schwartz et al. (2016). d = 500. BEST-* are again the best class-specific configs returned by Alg. 1. train 500-dim embeddings as in prior work.14 Results are presented in Tab. 6. The imported class-specific configurations, computed using a much smaller corpus (Sect. 5.1), again outperform competitive baseline context types for adjectives and nouns. The BEST-VERBS co"
K17-1013,P10-1040,0,0.0566914,"ning time. Our results generalise: we show that the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not"
K17-1013,Q14-1020,0,0.0199341,"ified by similar adjectives”. In another example, “two verbs are similar if they are used as predicates of similar nominal subjects” (the nsubj and nsubjpass dependency relations). First, we have to define an expressive context configuration space that contains potential training configurations and is effectively decomposed so that useful configurations may be sought algorithmically. We can then continue by designing a search algorithm over the configuration space. Context Configuration Space We focus on the configuration space based on dependency-based contexts (DEPS) (Padó and Lapata, 2007; Utt and Padó, 2014). We choose this space due to multiple reasons. First, dependency structures are known to be very useful in capturing functional relations between words, even if these relations are long distance. Second, they have been proven useful in learning word embeddings (Levy and Goldberg, 2014a; Melamud et al., 2016). Finally, owing to the recent development of the Universal Dependencies (UD) annotation scheme (McDonald et al., 2013; Nivre et al., 2016)1 it is possible to reason over dependency structures in a multilingual manner (e.g., Fig. 1). Consequently, a search algorithm in such DEPS-based conf"
K17-1013,E17-2065,1,0.823839,"Missing"
K17-1013,P16-2084,1,0.761929,"Missing"
K17-1013,Q15-1025,0,0.0546541,"dependency link, are more useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope n"
K17-1013,D12-1086,0,0.0288956,"epresent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended the comparison to more languages, reaching similar conclusions. Schwartz et al. (2016), showed that symmetric patterns are useful as"
K17-1013,P14-2089,0,0.0430435,"ctures, a particular dependency link, are more useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers"
K19-1004,N19-1391,0,0.318028,"ly context-aware dataset for evaluating cross-lingual embeddings on the word level is Bilingual Contextual Word Similarity (BCWS) (Chi and Chen, 2018). It challenges a system to predict similarity scores between cross-lingual word pairs with sentential context provided in both languages. However, BCWS does not explicitly test for the retrieval of meaning-equivalent cross-lingual contextualized embeddings, which is explicitly tested in our test. Also, BCWS is only available for one language pair: English-Chinese. Another task used for evaluating contextualized embeddings is Sentence Retrieval (Aldarmaki and Diab, 2019): given a query source sentence, the task is to retrieve the corresponding parallel sentence in the target language. Sentences can be represented as averages of contextualized embeddings of their constituent words. As the task does not explicitly evaluate at the word level, even if a system cannot accurately capture polysemy, it can rely on other words in the sentence to retrieve the correct parallel sentence. Therefore, Sentence Retrieval may lead to superficially high scores. Cross-lingual Word Embeddings. We conduct our experiments using a popular projection-based approach that learns an or"
K19-1004,D18-1027,0,0.0469943,"e also experimented with directly aligning embeddings obtained from the BERT multilingual model, which is a joint model trained with the same model parameters with shared subword vocabulary (Devlin et al., 2019). This means that identical words in two different languages will obtain the same embeddings. W = arg min kW S − T k2 s.t. W T W = I. (1) W The closed-form solution can be found by solving the orthogonal Procrustes problem (Sch¨onemann, 1966) as follows: T S T = U ΣV T ; W = U V T (2) We also optionally apply a post-processing Meeting-in-the-Middle (MIM) technique, recently proposed by Doval et al. (2018). It first calculates the average of each dictionary item representation in a pair after the orthogonal mapping: we denote the matrix U as the matrix where each column is such an average vector. Then, it finds a linear mapping M from both the source language (denoted as Ms ) and the target language (Mt ) after the previous step of orthogonal mapping to minimize the distance to U via a closed-form solution. Equation (3) formulates how to find Ms , and we do the same from target to source. Ms = arg min kMs W S − U k2 (3) Ms We apply the orthogonal mapping and MIM both on static embeddings (for b"
K19-1004,D16-1250,0,0.220628,"ponding parallel sentence in the target language. Sentences can be represented as averages of contextualized embeddings of their constituent words. As the task does not explicitly evaluate at the word level, even if a system cannot accurately capture polysemy, it can rely on other words in the sentence to retrieve the correct parallel sentence. Therefore, Sentence Retrieval may lead to superficially high scores. Cross-lingual Word Embeddings. We conduct our experiments using a popular projection-based approach that learns an orthogonal mapping between pretrained embeddings (Xing et al., 2015; Artetxe et al., 2016). The orthogonality of the mapping is crucial as it preserves monolingual invariance and is empirically proven to be more robust (Smith et al., 2017; Xing et al., 2015). This projection-based method can be applied post-hoc on pretrained monolingual embeddings with an exact analytical solution. Moreover, its performance is often competitive to that of jointly trained crosslingual models using additional bilingual signals in the form of parallel or comparable corpora (Ruder et al., 2019; Glavaˇs et al., 2019). However, projection-based cross-lingual embeddings are still predominantly concerned w"
K19-1004,N13-1073,0,0.110811,"Missing"
K19-1004,Q17-1010,0,0.460088,"n a dictionary with item pairs from source and target languages (si , ti ), and matrices S and T that contain the vector representations corresponding to the item pairs in the columns, we follow the standard practice (Glavaˇs et al., 2019) to find an orthogonal alignment matrix W that minimizes the distance between the transformed matrix W S and T . For improved performance, following Artetxe et al. (2016), we normalize and mean center the embeddings in S and T . The mapping is as follows: Methods Monolingual Contextualized Embeddings Compared to static word embeddings (Mikolov et al., 2013b; Bojanowski et al., 2017), more recent contextualized embeddings provide dynamic representations for a word in context as hidden layers in a deep neural network. They are typically obtained by unsupervised pretraining based on language modeling objectives (Devlin et al., 2019; Yang et al., 2019). The underlying contextualized method in our study is the pretrained BERTbase cased model1 (Devlin et al., 2019). BERT is trained using a transformer architecture (Vaswani et al., 2017) with masked language modelling (MLM) and next sentence prediction (NSP) tasks. MLM predicts the vocabulary id of a randomly masked word in a s"
K19-1004,P19-1070,1,0.84668,"Missing"
K19-1004,P13-1133,0,0.0433986,"e pair are aligned. Therefore, the final contexts for the source and target word in the word pair are indeed non-parallel. The use of non-parallel contexts here is crucial because when we perform the token retrieval task, parallel contexts can be superficially retrieved by simply matching the contexts rather than repreCollecting Translation Pairs. We select a representative set of query words from WordNet (Miller, 1998) (one unique word per WordNet synset). For each source word, we retrieve its WordNet senses and the corresponding translations in the target language from Multilingual WordNet (Bond and Foster, 2013). As WordNet senses are too fine-grained, we collapse senses into clusters if they contain the same translation for the source word. For example, “uniform” has five WordNet senses which are translated into four distinct Chinese words: 制服(the clothes worn by a particular group), 一致(the translation of two senses: consistent and undifferenti4 Notice the senses are different thus contexts are needed to find the pair corresponding to the same meaning. 37 senting the words in context appropriately. We empirically verified that a simplistic context average baseline outperforms contextualized word emb"
K19-1004,P12-1092,0,0.177851,"Missing"
K19-1004,S17-2002,0,0.031663,"uation. The core differences between the three tasks are illustrated in the following examples below: Evaluation of (Contextualized) Cross-lingual Embeddings. The traditional task to evaluate cross-lingual embeddings is Bilingual Dictionary Induction (BDI) (Vuli´c and Moens, 2013; Mikolov et al., 2013a; Gouws et al., 2015): given a source query word, the task is to retrieve the translation word in the target language. The test words in BDI are out-of-context and polysemy cannot be addressed properly. The same issue is found in another relevant lexical task, Cross-lingual Semantic Similarity. (Camacho-Collados et al., 2017). (1) Cross-lingual Word Sense Disambigution: source query: the national [coach] of the Irish teams ... answer: allenatore (Italian); Fußbaltrainer; Nationaltrainer; Trainer (German); entrenador(Spanish) ... (2) Cross-lingual Lexical Substitution : source query: She looked as [severely] as she could muster at Draco. answer: rigurosamente, seriamente (3) BTSR: source query: The reflections included in this document are linked to discussions with many colleagues and friends, in the present [tense]. 34 3.2 answer: Scott Peterson meti´o la pata elfondo y us´o el [tiempo] pasado mientras afirmaba q"
K19-1004,W09-2413,0,0.043201,"trieval task. Second, Schuster et al. (2019) introduce anchor embeddings as the average of contextualized embeddings of a word to perform alignment for contextualized models, and show its effectiveness in cross-lingual dependency parsing. These two studies are not directly comparable, whereas our paper provides a comprehensive and systematic comparison of various methods for learning cross-lingual contextualized embeddings and introduces a new and more challenging evaluation task. Cross-lingual Word Sense Disambiguation. Our new task is also related to Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2009): given a source language word in context, a system needs to provide the correct sense labels as clustered translation words in a number of target languages. Another related task is Cross-lingual Lexical Substitution (Sinha et al., 2009): the model must provide plausible target language translations for the source language lexical item in the source language context. In contrast, our BTSR task: (1) directly evaluates token-level word representations without the need to predict sense labels from a sense inventory and (2) it contextualizes both the source query and the target candidates ensuring"
K19-1004,K18-2005,0,0.0169691,"to serve as anchors for the alignment using static dictionaries, or we use parallel sentences as dictionary items to directly align contextualized word representations on the token level. We discuss this in what follows. 1 To produce the contextualized representation for a word in context, we average the 12 hidden layers of the word’s subword representations in BERT and then average the subword representations as input for the cross-lingual alignment. We leave other ways to extract the representations for future work. 2 We have also experimented with ELMo in lieu of BERT (Peters et al., 2018; Che et al., 2018). However, as we reach similar conclusions in terms of relative performance, while BERT-based cross-lingual embeddings outperform their ELMo-based counterparts in absolute terms, we do not report ELMo’s results for brevity. It should be noted that these pretrained models used different training data. 3.3 Alignment Levels We explore aligning contextualized models on two levels: type-level and token-level. Type-level word representation refers to static word representation that assigns one fixed embedding to a word. All the traditional word embedding models (e.g., skipgram, CBOW, fastText) provi"
K19-1004,D18-1025,0,0.354342,"R task highlights the merits of different alignment methods. In particular, we find that using context average type-level alignment is effective in transferring monolingual contextualized embeddings cross-lingually especially in non-parallel contexts, and at the same time improves the monolingual space. Furthermore, aligning independently trained models yields better performance than aligning multilingual embeddings with shared vocabulary. 1 We evaluate the methods on a variety of contextaware tasks. Besides two previously established evaluation tasks (1) Bilingual Contextual Word Similarity (Chi and Chen, 2018) and (2) Sentence Retrieval (Conneau et al., 2017), we introduce a new task: Bilingual Token-level Sense Retrieval (BTSR). It is more challenging than the alternatives as it requires the accurate cross-lingual retrieval of contextualized words on the token level which are disambiguated both in the source and the target language using non-parallel contexts. We provide BTSR task data and run evaluations on two language pairs: English–Chinese (EN–ZH) and English–Spanish (EN–ES). The data and guidelines can be found at: https://github.com/ qianchu/BTSR Introduction Contextualized embeddings have b"
K19-1004,N19-1386,0,0.0335393,"al invariance and is empirically proven to be more robust (Smith et al., 2017; Xing et al., 2015). This projection-based method can be applied post-hoc on pretrained monolingual embeddings with an exact analytical solution. Moreover, its performance is often competitive to that of jointly trained crosslingual models using additional bilingual signals in the form of parallel or comparable corpora (Ruder et al., 2019; Glavaˇs et al., 2019). However, projection-based cross-lingual embeddings are still predominantly concerned with static word embeddings (Glavaˇs et al., 2019; Vuli´c et al., 2019; Mohiuddin and Joty, 2019). Learning crosslingual contextualized embeddings is still a large unexplored area with only two concurrent papers at the moment. First, Aldarmaki and Diab (2019) adopt the same projection-based approach as our paper to align contextualized embeddings on the token-level using parallel data. They find that context-aware mapping using parallel data outperforms context-independent mappings from static dictionaries on a parallel Sentence Retrieval task. Second, Schuster et al. (2019) introduce anchor embeddings as the average of contextualized embeddings of a word to perform alignment for contextu"
K19-1004,N19-1423,0,0.296476,"eval (BTSR). It is more challenging than the alternatives as it requires the accurate cross-lingual retrieval of contextualized words on the token level which are disambiguated both in the source and the target language using non-parallel contexts. We provide BTSR task data and run evaluations on two language pairs: English–Chinese (EN–ZH) and English–Spanish (EN–ES). The data and guidelines can be found at: https://github.com/ qianchu/BTSR Introduction Contextualized embeddings have been shown to achieve superior performance compared to static word embeddings in English (Peters et al., 2018; Devlin et al., 2019). Despite recent efforts to better understand their multilingual variants (Pires et al., 2019), leveraging these available pretrained contextualized embeddings to learn cross-lingual contextualized embeddings is still an under-explored area: past cross-lingual embedding alignment methods have mainly focused on static embeddings (Ruder et al., 2019). In this paper, we introduce a first study that investigates and compares different ways of aligning the pretrained contextualized embeddings. In particular, we make the comparisons focused on the following properties: (1) aligning contextualOur mai"
K19-1004,N19-1392,0,0.0656437,"Missing"
K19-1004,D14-1113,0,0.123895,"Missing"
K19-1004,D19-1449,1,0.859792,"Missing"
K19-1004,N13-1011,1,0.878435,"Missing"
K19-1004,N18-1202,0,0.319663,"ken-level Sense Retrieval (BTSR). It is more challenging than the alternatives as it requires the accurate cross-lingual retrieval of contextualized words on the token level which are disambiguated both in the source and the target language using non-parallel contexts. We provide BTSR task data and run evaluations on two language pairs: English–Chinese (EN–ZH) and English–Spanish (EN–ES). The data and guidelines can be found at: https://github.com/ qianchu/BTSR Introduction Contextualized embeddings have been shown to achieve superior performance compared to static word embeddings in English (Peters et al., 2018; Devlin et al., 2019). Despite recent efforts to better understand their multilingual variants (Pires et al., 2019), leveraging these available pretrained contextualized embeddings to learn cross-lingual contextualized embeddings is still an under-explored area: past cross-lingual embedding alignment methods have mainly focused on static embeddings (Ruder et al., 2019). In this paper, we introduce a first study that investigates and compares different ways of aligning the pretrained contextualized embeddings. In particular, we make the comparisons focused on the following properties: (1) alig"
K19-1004,N15-1104,0,0.0932394,"Missing"
K19-1004,P19-1493,0,0.0378202,"gual retrieval of contextualized words on the token level which are disambiguated both in the source and the target language using non-parallel contexts. We provide BTSR task data and run evaluations on two language pairs: English–Chinese (EN–ZH) and English–Spanish (EN–ES). The data and guidelines can be found at: https://github.com/ qianchu/BTSR Introduction Contextualized embeddings have been shown to achieve superior performance compared to static word embeddings in English (Peters et al., 2018; Devlin et al., 2019). Despite recent efforts to better understand their multilingual variants (Pires et al., 2019), leveraging these available pretrained contextualized embeddings to learn cross-lingual contextualized embeddings is still an under-explored area: past cross-lingual embedding alignment methods have mainly focused on static embeddings (Ruder et al., 2019). In this paper, we introduce a first study that investigates and compares different ways of aligning the pretrained contextualized embeddings. In particular, we make the comparisons focused on the following properties: (1) aligning contextualOur main findings are as follows. (1) Using the average of the contextualized word representations as"
K19-1004,N19-1162,0,0.117939,"ddings are still predominantly concerned with static word embeddings (Glavaˇs et al., 2019; Vuli´c et al., 2019; Mohiuddin and Joty, 2019). Learning crosslingual contextualized embeddings is still a large unexplored area with only two concurrent papers at the moment. First, Aldarmaki and Diab (2019) adopt the same projection-based approach as our paper to align contextualized embeddings on the token-level using parallel data. They find that context-aware mapping using parallel data outperforms context-independent mappings from static dictionaries on a parallel Sentence Retrieval task. Second, Schuster et al. (2019) introduce anchor embeddings as the average of contextualized embeddings of a word to perform alignment for contextualized models, and show its effectiveness in cross-lingual dependency parsing. These two studies are not directly comparable, whereas our paper provides a comprehensive and systematic comparison of various methods for learning cross-lingual contextualized embeddings and introduces a new and more challenging evaluation task. Cross-lingual Word Sense Disambiguation. Our new task is also related to Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2009): given a source lan"
K19-1004,W09-2412,1,0.746156,"se two studies are not directly comparable, whereas our paper provides a comprehensive and systematic comparison of various methods for learning cross-lingual contextualized embeddings and introduces a new and more challenging evaluation task. Cross-lingual Word Sense Disambiguation. Our new task is also related to Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2009): given a source language word in context, a system needs to provide the correct sense labels as clustered translation words in a number of target languages. Another related task is Cross-lingual Lexical Substitution (Sinha et al., 2009): the model must provide plausible target language translations for the source language lexical item in the source language context. In contrast, our BTSR task: (1) directly evaluates token-level word representations without the need to predict sense labels from a sense inventory and (2) it contextualizes both the source query and the target candidates ensuring full sense disambiguation. The core differences between the three tasks are illustrated in the following examples below: Evaluation of (Contextualized) Cross-lingual Embeddings. The traditional task to evaluate cross-lingual embeddings"
K19-1004,tian-etal-2014-um,0,0.0293443,"urce word. For example, “uniform” has five WordNet senses which are translated into four distinct Chinese words: 制服(the clothes worn by a particular group), 一致(the translation of two senses: consistent and undifferenti4 Notice the senses are different thus contexts are needed to find the pair corresponding to the same meaning. 37 senting the words in context appropriately. We empirically verified that a simplistic context average baseline outperforms contextualized word embeddings in a variant of our task which relies on parallel contexts. We set aside 1M parallel sentences from the UMCorpus (Tian et al., 2014) (EN–ZH) and the WMT13 news dataset (Bojar et al., 2013) (EN–ES) for extracting the sentence contexts. We end up with 14,604 distinct word pairs with contexts extracted for EN–ZH, and 9,623 pairs for EN–ES. target candidate. We experiment with 20k target candidates and 200k target candidates. 5 Experiments Training Setup. To test the effects of corpora size on the induction of the cross-lingual alignment, we vary the size of the parallel corpus from 100 up to 200k parallel sentences in the UMCorpus and the WMT13 corpus. Word alignment was produced by IBM Model 2 using Fastalign (Dyer et al., 2"
K19-1021,E17-1088,0,0.0541429,"data). Following that, we motivate our selection of test languages and outline the subword-informed representation methods compared in our evaluation. Types of Data Scarcity. The majority of languages in the world still lack basic language technology, and progress in natural language processing is largely hindered by the lack of annotated task data that can guide machine learning models (Agi´c et al., 2016; Ponti et al., 2018). However, many languages face another challenge: the lack of large unannotated text corpora that can be used to induce useful general features such as word embeddings (Adams et al., 2017; Fang and Cohn, 2017; Ponti et al., 2018):2 i.e. WE data. The absence of data has over the recent years materialized the proxy fallacy. That is, methods tailored for low-resource languages are typically tested only by proxy, simulating low-data regimes exclusively on resource-rich languages (Agi´c et al., 2017). While this type of evaluation is useful for analyzing the main properties of the intended lowresource methods in controlled in vitro conditions, a complete evaluation should also provide results on true low-resource languages in vivo. In this paper we therefore conduct both types of e"
K19-1021,Q16-1022,0,0.0419405,"Missing"
K19-1021,P17-2093,0,0.0157947,"at, we motivate our selection of test languages and outline the subword-informed representation methods compared in our evaluation. Types of Data Scarcity. The majority of languages in the world still lack basic language technology, and progress in natural language processing is largely hindered by the lack of annotated task data that can guide machine learning models (Agi´c et al., 2016; Ponti et al., 2018). However, many languages face another challenge: the lack of large unannotated text corpora that can be used to induce useful general features such as word embeddings (Adams et al., 2017; Fang and Cohn, 2017; Ponti et al., 2018):2 i.e. WE data. The absence of data has over the recent years materialized the proxy fallacy. That is, methods tailored for low-resource languages are typically tested only by proxy, simulating low-data regimes exclusively on resource-rich languages (Agi´c et al., 2017). While this type of evaluation is useful for analyzing the main properties of the intended lowresource methods in controlled in vitro conditions, a complete evaluation should also provide results on true low-resource languages in vivo. In this paper we therefore conduct both types of evaluation. Note that"
K19-1021,E17-2040,0,0.0287154,"Missing"
K19-1021,C18-1139,0,0.0675297,"Missing"
K19-1021,E17-2067,0,0.0188003,"word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representa"
K19-1021,Q17-1010,0,0.661817,"tity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of"
K19-1021,D18-1366,0,0.0341798,"translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more"
K19-1021,D17-1078,0,0.0241869,"ecific Data: Task Data. The maximum number of training instances for all languages is again provided in Table 1. As before, for 4 languages we simulate low-resource settings by taking only a sample of the available task data: for FGET we work with 200, 2K or 20K training instances which roughly correspond to training regimes of different data availability, while we select 300,3 1K, and 10K sentences for NER and MGET. Again, for the remaining 12 languages, we use all the available data to run the experiments. We adopt existing data splits into training, development, and test portions for MTAG (Cotterell and Heigold, 2017), and random splits for FGET (Heinzerling and Strube, 2018; Zhu et al., 2019) and NER (Pan et al., 2017). 219 3 With a smaller number of instances (e.g., 100), NER and model training was unstable and resulted in near-zero performance across multiple runs. MGET A large number of data points for scarcity simulations allow us to trace how performance on the three tasks varies in relation to the availability of WE data versus task data, and what data source is more important for the final performance. Embedding Training Setup. When training our subword-informed representations, we argue that keepi"
K19-1021,N18-2085,0,0.0176097,"ithout any available data (Kornai, 2013; Ponti et al., 2018) is a challenge left for future work. (Selection of) Languages. Both sources of data scarcity potentially manifest in degraded task performance for low-resource languages: our goal is to analyze the extent to which these factors affect downstream tasks across morphologically diverse language types that naturally come with varying data sizes to train their respective embeddings and task-based models. Our selection of test languages is therefore guided by the following goals: a) following recent initiatives (e.g. in language modeling) (Cotterell et al., 2018; Gerz et al., 2018), we aim to ensure coverage of different genealogical and typological properties; b) we aim to cover low-resource languages with varying amounts of available WE data and task-specific data. We select 16 languages in total spanning 4 broad In what follows, we further motivate our work by analyzing two different sources of data scarcity: 217 2 For instance, as of April 2019, Wikipedia is available only in 304 out of the estimated 7,000 existing languages. Agglutinative EMB FGET NER MTAG BERT Fusional Introflexive Isolating BM BXR MYV TE TR ZU EN FO GA GOT MT RUE AM HE YO ZH 4"
K19-1021,N15-1140,0,0.04425,"Missing"
K19-1021,Q18-1003,0,0.0380649,"Missing"
K19-1021,D18-1029,1,0.899767,"Missing"
K19-1021,L18-1550,0,0.0288291,"ubword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. In"
K19-1021,L18-1473,1,0.706967,". Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e."
K19-1021,P82-1020,0,0.744642,"Missing"
K19-1021,C18-1216,0,0.0212074,"its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representation learning focused on low-resource set"
K19-1021,P18-1007,0,0.0258865,"t al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in"
K19-1021,N16-1030,0,0.0378679,"d-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data spars"
K19-1021,N19-1423,0,0.646923,"at the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the le"
K19-1021,D18-1549,0,0.0214746,"ning the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on variou"
K19-1021,N19-1154,0,0.0223364,"nd taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-in"
K19-1021,P13-1149,0,0.0307702,"leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by p"
K19-1021,E14-2006,0,0.124496,"r (iii) both, we analyse how different data regimes affect the final task performance. 2) We experiment with 16 languages representing 4 diverse morphological types, with a focus on truly low-resource languages such as Zulu, Rusyn, Buryat, or Bambara. 3) We experiment with a variety of subword-informed representation architectures, where the focus is on unsupervised, widely portable language-agnostic methods such as the ones based on character n-grams (Luong and Manning, 2016; Bojanowski et al., 2017), Byte Pair Encodings (BPE) (Sennrich et al., 2016; Heinzerling and Strube, 2018), Morfessor (Smit et al., 2014), or BERT-style pretraining and fine-tuning (Devlin et al., 2019) which relies on WordPieces (Wu et al., 2016). We demonstrate that by tuning subword-informed models in low-resource settings we can obtain substantial gains over subwordagnostic models such as skip-gram with negative sampling (Mikolov et al., 2013) across the board. The main goal of this study is to identify viable and effective subword-informed approaches for truly low-resource languages and offer modeling guidance in relation to the target task, the language at hand, and the (un)availability of general and/or task-specific tra"
K19-1021,W18-1205,0,0.0212108,"e word itself can be appended to the subword sequence and embedded into the subword space in order to incorporate word-level information (Bojanowski et al., 2017). To encode subword order, s can be further enriched by a trainable position embedding p. We use addition to combine subword and position embeddings, namely s := s + p, which has become the de-facto standard method to encode positional information (Gehring et al., 2017; Vaswani et al., 2017; Devlin et al., 2019). Finally, the subword embedding sequence is passed to a composition function, which computes the final word representation. Li et al. (2018) and Zhu et al. (2019) have empirically verified that composition by simple addition, among other more complex composition functions, is a robust choice. Therefore, we use addition in all our experiments. Similar to Bojanowski et al. (2017); Zhu et al. (2019), we adopt skip-gram with negative sampling 218 Component Segmentation Option Morfessor BPE char n-gram Label morf bpeX charn Word token exclusion inclusion ww+ Position embedding exclusion additive pp+ addition add Composition function resentations, and can benefit from the information. Table 2: Components for constructing subwordinformed"
K19-1021,P16-1100,0,0.0620352,"Missing"
K19-1021,P17-1178,0,0.10163,"before, for 4 languages we simulate low-resource settings by taking only a sample of the available task data: for FGET we work with 200, 2K or 20K training instances which roughly correspond to training regimes of different data availability, while we select 300,3 1K, and 10K sentences for NER and MGET. Again, for the remaining 12 languages, we use all the available data to run the experiments. We adopt existing data splits into training, development, and test portions for MTAG (Cotterell and Heigold, 2017), and random splits for FGET (Heinzerling and Strube, 2018; Zhu et al., 2019) and NER (Pan et al., 2017). 219 3 With a smaller number of instances (e.g., 100), NER and model training was unstable and resulted in near-zero performance across multiple runs. MGET A large number of data points for scarcity simulations allow us to trace how performance on the three tasks varies in relation to the availability of WE data versus task data, and what data source is more important for the final performance. Embedding Training Setup. When training our subword-informed representations, we argue that keeping hyper-parameters fixed across different data points will possibly result in underfitting for larger d"
K19-1021,N18-1202,0,0.0352715,"representation method. Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the me"
K19-1021,D18-1169,0,0.130939,"a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words"
K19-1021,D17-1010,0,0.0259855,"2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowle"
K19-1021,P16-2067,0,0.096051,"Missing"
K19-1021,P16-1162,0,0.314176,"well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018)"
K19-1021,P17-1184,0,0.0193316,"ed from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representation learning focused o"
K19-1021,D15-1083,0,0.0434509,"Missing"
K19-1021,C18-1153,0,0.0422342,"Missing"
K19-1021,D18-1059,0,0.0518042,"Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representation learning focused on low-resource setups. Our study centers on the following"
K19-1021,N19-1097,1,0.876896,"Missing"
kipper-etal-2006-extending,kingsbury-palmer-2002-treebank,1,\N,Missing
kipper-etal-2006-extending,J88-2003,0,\N,Missing
kipper-etal-2006-extending,C94-1042,0,\N,Missing
kipper-etal-2006-extending,C00-2094,0,\N,Missing
kipper-etal-2006-extending,W02-0907,1,\N,Missing
kipper-etal-2006-extending,W04-2606,1,\N,Missing
kipper-etal-2006-extending,W02-1016,0,\N,Missing
kipper-etal-2006-extending,P98-1013,0,\N,Missing
kipper-etal-2006-extending,C98-1013,0,\N,Missing
kipper-etal-2006-extending,P87-1027,0,\N,Missing
korhonen-etal-2006-large,J87-3002,1,\N,Missing
korhonen-etal-2006-large,W98-1505,0,\N,Missing
korhonen-etal-2006-large,A00-2034,0,\N,Missing
korhonen-etal-2006-large,rose-etal-2002-reuters,0,\N,Missing
korhonen-etal-2006-large,C94-1042,0,\N,Missing
korhonen-etal-2006-large,W02-0907,1,\N,Missing
korhonen-etal-2006-large,A97-1052,1,\N,Missing
korhonen-etal-2006-large,J93-2002,0,\N,Missing
korhonen-etal-2006-large,P05-1038,0,\N,Missing
korhonen-etal-2006-large,P03-1007,1,\N,Missing
korhonen-etal-2006-large,P04-2007,0,\N,Missing
korhonen-etal-2006-large,briscoe-carroll-2002-robust,1,\N,Missing
korhonen-etal-2006-large,P03-1002,0,\N,Missing
korhonen-etal-2006-large,P03-1009,1,\N,Missing
korhonen-etal-2006-large,P02-1029,0,\N,Missing
korhonen-etal-2006-large,J03-4004,0,\N,Missing
korhonen-etal-2006-large,W02-2014,1,\N,Missing
L18-1153,W15-0102,0,0.0258175,"ameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such"
L18-1153,P98-1013,0,0.0943859,"antic clustering, multilingual NLP 1. Introduction With the recent advances in automatic lexical acquisition, the need for high-quality evaluation resources is ever growing. Due to the pivotal role played by verbs in sentence structure, the problem of creation of verbal classifications has attracted a lot of attention in natural language processing (NLP). Different approaches to creation of verbal classifications have been proposed, varying with regard to the guiding criteria by which the class architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). Howev"
L18-1153,P13-1133,0,0.0212106,"re intentionally restricted, so as to avoid imposing any preconceived semantic categories or classification structure onto the annotators and elicit possibly spontaneous similarity judgments, such discrepancies in detecting ambiguity are inevitable. In order to have more control over which sense of a given verb is taken into consideration in the clustering task, word senses rather than word forms would have to be provided at the start of the task. Such a set-up would also allow comparison of the elicited classes with the existing multilingual sense inventories, like Open Multilingual WordNet (Bond and Foster, 2013) or BabelNet (Navigli and Ponzetto, 2012). Since the aim of the present study was to elicit judgments on basic word forms, without any guidance as to the different word senses available, such comparisons are beyond the scope of this study; however, in future work we intend to extend this analysis and compare our findings against the resources available. 5. Conclusion We have presented the first cross-lingual analysis and evaluation of semantic clustering of verbs by non-expert human annotators. The inter-annotator agreement scores reported for English, Polish, and Croatian are encouraging and"
L18-1153,P12-1090,0,0.0217686,"al verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such systems relies on the availability of gold standard classes, and these are still lacking for a great majority of languages. In light of these challenges and the high demand for verbal resources, this paper investigates whether semantic verb classes can be reliably acquired from non-expert native speakers based solely on verb semantics and following simple instructions, which, to th"
L18-1153,D16-1235,1,0.9085,"Missing"
L18-1153,C94-1042,0,0.65147,"c lexical acquisition, the need for high-quality evaluation resources is ever growing. Due to the pivotal role played by verbs in sentence structure, the problem of creation of verbal classifications has attracted a lot of attention in natural language processing (NLP). Different approaches to creation of verbal classifications have been proposed, varying with regard to the guiding criteria by which the class architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and sy"
L18-1153,J15-4004,1,0.623982,"ints of view, for example, lend and borrow, which differ along only one dimension of meaning, that is, the direction of the action (the object of the verb either travels away from the participant (A lends something to B) or towards the participant (B borrows something from A)), and are essentially identical with regard to all other features, which makes them appear semantically close. 4.2. Semantic Similarity versus Relatedness The importance of distinguishing between the concepts of semantic similarity (e.g. cup and mug) and relatedness (e.g. coffee and cup) has been noted in the literature (Hill et al., 2015), and the analysis of our data provides more evidence illustrating the influence of loose association on how humans conceptualize similarity between words, and the difficulty of keeping similarity and relatedness apart. In all three languages we can observe instances of what can be described as a ‘storyline approach’ to judging semantic similarity and verb classification. This is particularly noticeable in Croatian classifications, where several classes formed by the annotators group verbs describing quite different actions, linked via loose thematic ties: (1) marry, conquer, approach, move, w"
L18-1153,S13-2049,0,0.415023,"lusters, and 1 We used the Fuzzy B-Cubed implementation of Jurgens and Klapaftis (2013) but did not associate the clusters with weights, and therefore the metric is equivalent to that of Amig´o et al. (2009). 953 Average B-Cubed English Polish Croatian All 0.262 0.338 0.172 0.205 1c1inst All-instances, One class 0.0 0.069 Table 3: The average B-Cubed F-score (i.e. harmonic mean of B-Cubed precision and recall) calculated for all possible pairings of annotators, for each language individually and across the three languages, and for two SemEval baselines: 1c1inst and All-instances, One class by Jurgens and Klapaftis (2013) to fuzzy clusters, used to evaluate the performance of Word Sense Induction systems in SemEval tasks (Jurgens and Klapaftis, 2013). The B-Cubed metrics (B-Cubed precision and recall) compare two clusterings (say, X and Y) at the item level: for an item i, precision measures how many items sharing a cluster with i in clustering X are placed in its cluster in clustering Y; whereas B-Cubed recall measures how many items sharing a cluster with i in Y are also placed in its cluster in X, with the final B-Cubed score equivalent to the harmonic mean of the two values. In our task, rather than compar"
L18-1153,P14-1097,0,0.0151587,"urrently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such systems relies on the availability of gold standard classes, and these are still lacking for a great majority of languages. In light of these challenges and the high demand for verbal resources, this paper investigates whether semantic verb classes can be reliably acquired from non-expert native speakers based solely on verb semantics and following simple instructions, which, to the best of our knowledge"
L18-1153,korhonen-etal-2006-large,1,0.648455,"for high-quality evaluation resources is ever growing. Due to the pivotal role played by verbs in sentence structure, the problem of creation of verbal classifications has attracted a lot of attention in natural language processing (NLP). Different approaches to creation of verbal classifications have been proposed, varying with regard to the guiding criteria by which the class architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challeng"
L18-1153,J05-1004,0,0.143765,"NLP 1. Introduction With the recent advances in automatic lexical acquisition, the need for high-quality evaluation resources is ever growing. Due to the pivotal role played by verbs in sentence structure, the problem of creation of verbal classifications has attracted a lot of attention in natural language processing (NLP). Different approaches to creation of verbal classifications have been proposed, varying with regard to the guiding criteria by which the class architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resour"
L18-1153,S16-2012,0,0.0165565,"s NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such systems relies on the availability of gold standard classes, and these are still lacking for a great majority of languages. In light of these challenges and the high demand for verbal resources, this paper investigates whether semantic verb classes can be reliably acquired from non-expert native speakers based solely on verb semantics and following simple instructions, which, to the best of our knowledge, is the first evaluation of this approach. D"
L18-1153,W11-2112,0,0.0252591,"ss architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the"
L18-1153,D12-1048,0,0.0248176,"tic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trai"
L18-1153,C10-1119,1,0.80849,"the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such systems relies on the availability of gold standard classes, and these are still lacking for a great majority of languages. In light of these challenges and the high demand for verbal resources, this paper investigates whether semantic verb classes can be reliably acquired from non-expert native speakers based solely on verb semantics and following simple instruc"
L18-1153,D17-1270,1,0.883378,"Missing"
L18-1153,W11-0110,0,0.0318037,", prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate t"
messiant-etal-2008-lexschem,schulte-im-walde-2002-subcategorisation,0,\N,Missing
messiant-etal-2008-lexschem,poibeau-messiant-2008-still,1,\N,Missing
messiant-etal-2008-lexschem,W98-1114,0,\N,Missing
messiant-etal-2008-lexschem,W00-1325,1,\N,Missing
messiant-etal-2008-lexschem,A97-1052,0,\N,Missing
messiant-etal-2008-lexschem,J93-2002,0,\N,Missing
messiant-etal-2008-lexschem,P05-1038,0,\N,Missing
messiant-etal-2008-lexschem,P93-1032,0,\N,Missing
messiant-etal-2008-lexschem,P02-1029,0,\N,Missing
messiant-etal-2008-lexschem,P07-1115,1,\N,Missing
messiant-etal-2008-lexschem,sagot-etal-2006-lefff,0,\N,Missing
messiant-etal-2008-lexschem,chesley-salmon-alt-2006-automatic,0,\N,Missing
messiant-etal-2008-lexschem,korhonen-etal-2006-large,1,\N,Missing
N13-1113,J96-1002,0,0.0249814,"Missing"
N13-1113,P07-1036,0,0.490984,"lude the presumed DNA adduct of BA and might thus be slightly overestimated. The verb “calculated” usually indicates the “Method” class, but, when accompanied by the modal verb “might”, it is more likely to imply that authors are interpreting their own results (i.e. the “Conclusion” class in AZ). This can be explicitly encoded in the model through a target distribution for sentences containing certain modal verbs. Recent work has shown that explicit declaration of domain and expert knowledge can be highly useful for structured NLP tasks such as parsing, POS tagging and information extraction (Chang et al., 2007; Mann and McCallum, 2008; Ganchev et al., 2010). These works have encoded expert knowledge through constraints, with different frameworks differing in the type of constraints and the inference and learning algorithms used. We build on the Generalized Expectation (GE) framework (Mann and McCallum, 2007) which encodes expert knowledge through a preference (i.e. soft) constraints for parameter settings for which the predicted label distribution matches a target distribution. In order to integrate domain knowledge with a features-based model, we develop a simple taxonomy of constraints (i.e. desi"
N13-1113,C12-1041,1,0.945708,"ul even when no labeled data is available. 1 Introduction Techniques that enable automatic analysis of the information structure of scientific articles can help scientists identify information of interest in the growing volume of scientific literature. For example, classification of sentences according to argumentative zones (AZ) – an information structure scheme that is applicable across scientific domains (Teufel et al., 2009) – can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)). Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Liakata et al. (2012) in biochemistry and chemistry with per-class F-scores ranging from .18"
N13-1113,P07-2009,0,0.220165,"Missing"
N13-1113,W10-1913,1,0.883021,"me that is applicable across scientific domains (Teufel et al., 2009) – can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)). Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Liakata et al. (2012) in biochemistry and chemistry with per-class F-scores ranging from .18 to .76. We propose a novel approach to this task in which traditional feature-based models are augmented with explicit declarative expert and domain knowledge, and apply it to sentence-based AZ. We explore two sources of declarative knowledge for our task - discourse and lexical. One way to utilize discourse knowledge is to guide the model predictions"
N13-1113,D11-1025,1,0.868095,"Missing"
N13-1113,I08-1050,0,0.150893,"ve zones (AZ) – an information structure scheme that is applicable across scientific domains (Teufel et al., 2009) – can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)). Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Liakata et al. (2012) in biochemistry and chemistry with per-class F-scores ranging from .18 to .76. We propose a novel approach to this task in which traditional feature-based models are augmented with explicit declarative expert and domain knowledge, and apply it to sentence-based AZ. We explore two sources of declarative knowledge for our task - discourse and lexical. One way to utilize discourse know"
N13-1113,P06-1027,0,0.0287614,"DIFF FUT 16.9 74.8 0.5 4.0 16.9 17.9 0.6 1.4 68.9 1.5 22.3 5.9 0.2 12.1 63.5 0.8 0.1 0.1 2.4 2.8 13.2 0.2 2.1 1.1 34.8 5.4 97.5 11.7 0.7 4.3 0.1 0.2 1.1 13.3 0.2 0.7 Table 5: Performance of baselines on the Discussion section. BKG PROB METH RES CON CN Full supervision SVM .56 0 0 0 .84 MaxEnt .55 .08 0 0 .84 Light supervision with 150 labeled sentence SVM .26 0 0 0 .80 TSVM .25 .04 .04 .03 .33 MaxEnt .25 0 0 0 .80 MaxEnt+ER .23 0 0 0 .80 .35 .38 DIFF FUT 0 0 0 0 .05 0 14 .06 .10 0 .07 0 0 .02 0 0 ductive SVM (TSVM) and semi-supervised MaxEnt based on Entropy Regularization (ER) (Vapnik, 1998; Jiao et al., 2006). SVM and MaxEnt have proved successful in information structure analysis (e.g. (Merity et al., 2009; Guo et al., 2011)) but, to the best of our knowledge, their semi-supervised versions have not been used for AZ of full articles. Parameter tuning The boundaries of the reference probabilities (ak and bk in Equation (8)) were defined and optimized on the development data which consists of one third of the corpus. We considered six types of boundaries: Fairly High for 1, High for [0.9,1), Medium High for [0.5,0.9), Medium Low for [0.1,0.5), Low for [0,0.1), and Fairly Low for 0. Evaluation We ev"
N13-1113,liakata-etal-2010-corpora,0,0.0380636,"Previous work Information structure analysis The information structure of scientific documents (e.g. journal articles, abstracts, essays) can be analyzed in terms of patterns of topics, functions or relations observed in multi-sentence scientific text. Computational approaches have mainly focused on analysis based on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Hachey and Grover, 2006; Teufel et al., 2009), discourse structure (Burstein et al., 2003; Webber et al., 2011), qualitative dimensions (Shatkay et al., 2008), scientific claims (Blake, 2009), scientific concepts (Liakata et al., 2010) and information status (Markert et al., 2012). Most existing methods for analyzing scientific text according to information structure use full supervision in the form of thousands of manually annotated sentences (Teufel and Moens, 2002; Burstein et al., 2003; Mizuta et al., 2006; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012; Markert et al., 2012). Because manual annotation is prohibitively expensive, approaches based on light supervision are now emerging for the task, including those based on active learning and self-training (Guo et al., 2011) and unsupervised methods (Varga"
N13-1113,W06-3309,0,0.0827281,"ing to argumentative zones (AZ) – an information structure scheme that is applicable across scientific domains (Teufel et al., 2009) – can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)). Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Liakata et al. (2012) in biochemistry and chemistry with per-class F-scores ranging from .18 to .76. We propose a novel approach to this task in which traditional feature-based models are augmented with explicit declarative expert and domain knowledge, and apply it to sentence-based AZ. We explore two sources of declarative knowledge for our task - discourse and lexical. One way to"
N13-1113,P08-1099,0,0.645249,"A adduct of BA and might thus be slightly overestimated. The verb “calculated” usually indicates the “Method” class, but, when accompanied by the modal verb “might”, it is more likely to imply that authors are interpreting their own results (i.e. the “Conclusion” class in AZ). This can be explicitly encoded in the model through a target distribution for sentences containing certain modal verbs. Recent work has shown that explicit declaration of domain and expert knowledge can be highly useful for structured NLP tasks such as parsing, POS tagging and information extraction (Chang et al., 2007; Mann and McCallum, 2008; Ganchev et al., 2010). These works have encoded expert knowledge through constraints, with different frameworks differing in the type of constraints and the inference and learning algorithms used. We build on the Generalized Expectation (GE) framework (Mann and McCallum, 2007) which encodes expert knowledge through a preference (i.e. soft) constraints for parameter settings for which the predicted label distribution matches a target distribution. In order to integrate domain knowledge with a features-based model, we develop a simple taxonomy of constraints (i.e. desired class distributions)"
N13-1113,P12-1084,0,0.0883378,"Missing"
N13-1113,W09-3603,0,0.0236176,".2 0.2 2.1 1.1 34.8 5.4 97.5 11.7 0.7 4.3 0.1 0.2 1.1 13.3 0.2 0.7 Table 5: Performance of baselines on the Discussion section. BKG PROB METH RES CON CN Full supervision SVM .56 0 0 0 .84 MaxEnt .55 .08 0 0 .84 Light supervision with 150 labeled sentence SVM .26 0 0 0 .80 TSVM .25 .04 .04 .03 .33 MaxEnt .25 0 0 0 .80 MaxEnt+ER .23 0 0 0 .80 .35 .38 DIFF FUT 0 0 0 0 .05 0 14 .06 .10 0 .07 0 0 .02 0 0 ductive SVM (TSVM) and semi-supervised MaxEnt based on Entropy Regularization (ER) (Vapnik, 1998; Jiao et al., 2006). SVM and MaxEnt have proved successful in information structure analysis (e.g. (Merity et al., 2009; Guo et al., 2011)) but, to the best of our knowledge, their semi-supervised versions have not been used for AZ of full articles. Parameter tuning The boundaries of the reference probabilities (ak and bk in Equation (8)) were defined and optimized on the development data which consists of one third of the corpus. We considered six types of boundaries: Fairly High for 1, High for [0.9,1), Medium High for [0.5,0.9), Medium Low for [0.1,0.5), Low for [0,0.1), and Fairly Low for 0. Evaluation We evaluated the precision, recall and F-score for each category, using a standard ten-fold cross-validat"
N13-1113,C12-2097,1,0.831417,"on status (Markert et al., 2012). Most existing methods for analyzing scientific text according to information structure use full supervision in the form of thousands of manually annotated sentences (Teufel and Moens, 2002; Burstein et al., 2003; Mizuta et al., 2006; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012; Markert et al., 2012). Because manual annotation is prohibitively expensive, approaches based on light supervision are now emerging for the task, including those based on active learning and self-training (Guo et al., 2011) and unsupervised methods (Varga et al., 2012; Reichart and Korhonen, 2012). Unfortunately, these approaches do not reach the performance level of fully supervised models, let alone exceed it. Our novel method addresses this problem. Declarative knowledge and constraints Previous work has shown that incorporating declarative constraints into feature-based machine learning models works well in many NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2008; Bellare et al., 2009; Ganchev et al., 2010). Such constraints can be used in a semi-supervised or unsupervised fashion. For example, (Mann and McCallum, 2008) shows that using CRF in conjunction wit"
N13-1113,D09-1067,1,0.887348,"Missing"
N13-1113,J02-4002,0,0.828448,"tperforms lightly supervised feature-based models, showing that our approach can be useful even when no labeled data is available. 1 Introduction Techniques that enable automatic analysis of the information structure of scientific articles can help scientists identify information of interest in the growing volume of scientific literature. For example, classification of sentences according to argumentative zones (AZ) – an information structure scheme that is applicable across scientific domains (Teufel et al., 2009) – can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)). Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Li"
N13-1113,D09-1155,0,0.347904,"of existing fully and lightly supervised models. Even a fully unsupervised version of this model outperforms lightly supervised feature-based models, showing that our approach can be useful even when no labeled data is available. 1 Introduction Techniques that enable automatic analysis of the information structure of scientific articles can help scientists identify information of interest in the growing volume of scientific literature. For example, classification of sentences according to argumentative zones (AZ) – an information structure scheme that is applicable across scientific domains (Teufel et al., 2009) – can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)). Unfortunately, the performa"
N13-1113,varga-etal-2012-unsupervised,0,0.0937764,"2010) and information status (Markert et al., 2012). Most existing methods for analyzing scientific text according to information structure use full supervision in the form of thousands of manually annotated sentences (Teufel and Moens, 2002; Burstein et al., 2003; Mizuta et al., 2006; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012; Markert et al., 2012). Because manual annotation is prohibitively expensive, approaches based on light supervision are now emerging for the task, including those based on active learning and self-training (Guo et al., 2011) and unsupervised methods (Varga et al., 2012; Reichart and Korhonen, 2012). Unfortunately, these approaches do not reach the performance level of fully supervised models, let alone exceed it. Our novel method addresses this problem. Declarative knowledge and constraints Previous work has shown that incorporating declarative constraints into feature-based machine learning models works well in many NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2008; Bellare et al., 2009; Ganchev et al., 2010). Such constraints can be used in a semi-supervised or unsupervised fashion. For example, (Mann and McCallum, 2008) shows tha"
N13-1134,D10-1115,0,0.796735,"often attributed to Frege, is the principle that states that the meaning of a complex expression is a function of the meaning of its parts and the way those parts are (syntactically) combined (Frege, 1892). It is the fundamental principle that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words. Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2012). However, the absolute gains of the systems remain a bit unclear, and a simple method of composition – vector multiplication – often seems to produce the best results (Blacoe and Lapata, 2012). In this paper, we present a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We use our method to model the composition of subject verb object triples. The method consis"
N13-1134,D12-1050,0,0.0738527,"that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words. Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2012). However, the absolute gains of the systems remain a bit unclear, and a simple method of composition – vector multiplication – often seems to produce the best results (Blacoe and Lapata, 2012). In this paper, we present a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We use our method to model the composition of subject verb object triples. The method consists of two steps. First, we compute a latent factor model for nouns from standard co-occurrence data. Next, the latent factors are used to induce a latent model of three-way subject verb object interactions. Our model has been evaluated"
N13-1134,J90-1003,0,0.141423,"As a baseline, we compute the non-contextualized 1148 similarity score for target verb and landmark. The upper bound is provided by Grefenstette and Sadrzadeh (2011a), based on interannotator agreement. 5.2 Implementational details All models have been constructed using the UKWAC corpus (Baroni et al., 2009), a 2 billion word corpus automatically harvested from the web. From this data, we accumulate the input matrix V for our first NMF step. We use the 10K most frequent nouns, crossclassified by the 2K most frequent context words.7 Matrix V is weighted using pointwise mutual information (PMI, Church and Hanks (1990)). A parsed version of the corpus is available, which has been parsed with MaltParser (Nivre et al., 2006). We use this version in order to extract our svo triples. From these triples, we construct our tensor X, using 1K verbs × 10K subjects × 10K objects. Note once again that the subject and object instances in the second step are exactly the same as the noun instances in the first step. Tensor X has been weighted using a three-way extension of PMI, following equation 10 (Van de Cruys, 2011). pmi3(x, y, z) = log p(x, y, z) p(x)p(y)p(z) (10) We set K = 300 as our number of latent factors. The"
N13-1134,D10-1113,0,0.0448751,"43 it changes the meaning of neighbouring words and phrases. Closely related to the work on compositionality is research on the computation of word meaning in context. Erk and Pad´o (2008, 2009) make use of selectional preferences to express the meaning of a word in context; the meaning of a word in the presence of an argument is computed by multiplying the word’s vector with a vector that captures the inverse selectional preferences of the argument. Thater et al. (2009, 2010) extend the approach based on selectional preferences by incorporating second-order co-occurrences in their model. And Dinu and Lapata (2010) propose a probabilistic framework that models the meaning of words as a probability distribution over latent factors. This allows them to model contextualized meaning as a change in the original sense distribution. Dinu and Lapata use non-negative matrix factorization (NMF) to induce latent factors. Similar to their work, our model uses NMF – albeit in a slightly different configuration – as a first step towards our final factorization model. In general, latent models have proven to be useful for the modeling of word meaning. One of the best known latent models of semantics is Latent Semantic"
N13-1134,D08-1094,0,0.461001,"Missing"
N13-1134,W09-0208,0,0.0859558,"Missing"
N13-1134,N10-3005,0,0.0464449,"different configuration – as a first step towards our final factorization model. In general, latent models have proven to be useful for the modeling of word meaning. One of the best known latent models of semantics is Latent Semantic Analysis (Landauer and Dumais, 1997), which uses singular value decomposition in order to automatically induce latent factors from term-document matrices. Another well known latent model of meaning, which takes a generative approach, is Latent Dirichlet Allocation (Blei et al., 2003). Tensor factorization has been used before for the modeling of natural language. Giesbrecht (2010) describes a tensor factorization model for the construction of a distributional model that is sensitive to word order. And Van de Cruys (2010) uses a tensor factorization model in order to construct a three-way selectional preference model of verbs, subjects, and objects. Our underlying tensor factorization – Tucker decomposition – is the same as Giesbrecht’s; and similar to Van de Cruys (2010), we construct a latent model of verb, subject, and object interactions. The way our model is constructed, however, is significantly different. The former research does not use any syntactic information"
N13-1134,D11-1129,0,0.488078,"Missing"
N13-1134,W11-2507,0,0.0582789,"Missing"
N13-1134,nivre-etal-2006-maltparser,0,0.0282329,"bound is provided by Grefenstette and Sadrzadeh (2011a), based on interannotator agreement. 5.2 Implementational details All models have been constructed using the UKWAC corpus (Baroni et al., 2009), a 2 billion word corpus automatically harvested from the web. From this data, we accumulate the input matrix V for our first NMF step. We use the 10K most frequent nouns, crossclassified by the 2K most frequent context words.7 Matrix V is weighted using pointwise mutual information (PMI, Church and Hanks (1990)). A parsed version of the corpus is available, which has been parsed with MaltParser (Nivre et al., 2006). We use this version in order to extract our svo triples. From these triples, we construct our tensor X, using 1K verbs × 10K subjects × 10K objects. Note once again that the subject and object instances in the second step are exactly the same as the noun instances in the first step. Tensor X has been weighted using a three-way extension of PMI, following equation 10 (Van de Cruys, 2011). pmi3(x, y, z) = log p(x, y, z) p(x)p(y)p(z) (10) We set K = 300 as our number of latent factors. The value was chosen as a trade-off between a model that is both rich enough, and does not require an excessiv"
N13-1134,D12-1110,0,0.502234,"states that the meaning of a complex expression is a function of the meaning of its parts and the way those parts are (syntactically) combined (Frege, 1892). It is the fundamental principle that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words. Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2012). However, the absolute gains of the systems remain a bit unclear, and a simple method of composition – vector multiplication – often seems to produce the best results (Blacoe and Lapata, 2012). In this paper, we present a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We use our method to model the composition of subject verb object triples. The method consists of two steps. First, we compute a latent"
N13-1134,W09-2506,0,0.0281007,"Missing"
N13-1134,P10-1097,0,0.359266,"Missing"
N13-1134,W11-1303,1,0.86549,"Missing"
N13-1134,C00-2137,0,0.0112051,"he target verb in composition (system meets criterion) to the non-contextualized semantics of the landmark verb (visit). Note that the scores presented in this evaluation (including the baseline score) are significantly higher than the scores presented in Grefenstette and Sadrzadeh (2011b). This is not surprising, since the corpus we use – UKWAC – is an order of magnitude larger than the corpus used in their research – the British National Corpus (BNC). Presumably, the scores are also favoured by our weighting measure. 8 p < 0.01; model differences have been tested using stratified shuffling (Yeh, 2000). 1149 In this paper, we presented a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We used our method to model the composition of subject verb object combinations. The method consists of two steps. First, we compute a latent factor model for nouns from standard co-occurrence data. Next, the latent factors are used to induce a latent model of three-way subject verb object interactions, represented by a"
N13-1134,P08-1028,0,\N,Missing
N16-1162,S14-2010,0,0.0519061,". (2015): a logistic regression classifier is trained on top of sentence representations, with 10-fold cross-validation used when a train-test split is not pre-defined. 3.2 Unsupervised Evaluations We also measure how well representation spaces reflect human intuitions of the semantic sentence relatedness, by computing the cosine distance between vectors for the two sentences in each test pair, and correlating these distances with gold-standard human judgements. The SICK dataset (Marelli et al., 1371 2014) consists of 10,000 pairs of sentences and relatedness judgements. The STS 2014 dataset (Agirre et al., 2014) consists of 3,750 pairs and ratings from six linguistic domains. Example ratings are shown in Table 2. All available pairs are used for testing apart from the 500 SICK ‘trial’ pairs, which are held-out for tuning hyperparameters (representation size of log-bilinear models, and noise parameters in SDAE). The optimal settings on this task are then applied to both supervised and unsupervised evaluations. 4 Results Performance of the models on the supervised evaluations (grouped according to the data required by their objective) is shown in Table 3. Overall, SkipThought vectors perform best on th"
N16-1162,2014.lilt-9.5,0,0.0108545,"so propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance. 1 Introduction Distributed representations - dense real-valued vectors that encode the semantics of linguistic units are ubiquitous in today’s NLP research. For singlewords or word-like entities, there are established ways to acquire such representations from naturally occurring (unlabelled) training data based on comparatively task-agnostic objectives (such as predicting adjacent words). These methods are well understood empirically (Baroni et al., 2014b) and theoretically (Levy and Goldberg, 2014). The best word representation spaces reflect consistently-observed aspects of human conceptual organisation (Hill et al., 2015b), and can be added as features to improve the performance of numerous language processing systems (Collobert et al., 2011). By contrast, there is comparatively little consensus on the best ways to learn distributed representations of phrases or sentences.1 With the advent of deeper language processing techniques, it is relatively common for models to represent phrases or sentences as continuous-valued vectors. Examples in"
N16-1162,P14-1023,0,0.643214,"so propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance. 1 Introduction Distributed representations - dense real-valued vectors that encode the semantics of linguistic units are ubiquitous in today’s NLP research. For singlewords or word-like entities, there are established ways to acquire such representations from naturally occurring (unlabelled) training data based on comparatively task-agnostic objectives (such as predicting adjacent words). These methods are well understood empirically (Baroni et al., 2014b) and theoretically (Levy and Goldberg, 2014). The best word representation spaces reflect consistently-observed aspects of human conceptual organisation (Hill et al., 2015b), and can be added as features to improve the performance of numerous language processing systems (Collobert et al., 2011). By contrast, there is comparatively little consensus on the best ways to learn distributed representations of phrases or sentences.1 With the advent of deeper language processing techniques, it is relatively common for models to represent phrases or sentences as continuous-valued vectors. Examples in"
N16-1162,C04-1051,0,0.805554,"g with human relatedness judgements - unspervised evaluation (Hill et al., 2015a; Baroni et al., 2014b; Levy et al., 2015). The former setting reflects a scenario in which representations are used to inject general knowledge (sometimes considered as pre-training) into a supervised model. The latter pertains to applications in which the sentence representation space is used for direct comparisons, lookup or retrieval. Here, we apply and compare both evaluation paradigms. 3.1 Supervised Evaluations Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Voorhees, 2002). We follow the procedure (and code) of Kiros et al. (2015): a logistic regression classifier is trained on top of sentence representations, with 10-fold cross-validation used when a train-test split is not pre-defined. 3.2 Unsupervised Evaluations We also measure how well representation spaces reflect human intuitions of the semantic sentence rel"
N16-1162,J15-4004,1,0.722961,"Distributed representations - dense real-valued vectors that encode the semantics of linguistic units are ubiquitous in today’s NLP research. For singlewords or word-like entities, there are established ways to acquire such representations from naturally occurring (unlabelled) training data based on comparatively task-agnostic objectives (such as predicting adjacent words). These methods are well understood empirically (Baroni et al., 2014b) and theoretically (Levy and Goldberg, 2014). The best word representation spaces reflect consistently-observed aspects of human conceptual organisation (Hill et al., 2015b), and can be added as features to improve the performance of numerous language processing systems (Collobert et al., 2011). By contrast, there is comparatively little consensus on the best ways to learn distributed representations of phrases or sentences.1 With the advent of deeper language processing techniques, it is relatively common for models to represent phrases or sentences as continuous-valued vectors. Examples include machine translation (Sutskever et al., 2014), image captioning (Mao et al., 2015) and dialogue systems (Serban et al., 2015). While it has been observed informally tha"
N16-1162,P15-1162,0,0.0615922,"Missing"
N16-1162,D13-1090,0,0.0341145,"ttings on this task are then applied to both supervised and unsupervised evaluations. 4 Results Performance of the models on the supervised evaluations (grouped according to the data required by their objective) is shown in Table 3. Overall, SkipThought vectors perform best on three of the six evaluations, the BOW DictRep model with pretrained word embeddings performs best on two, and the SDAE on one. SDAEs perform notably well on the paraphrasing task, going beyond SkipThought by three percentage points and approaching stateof-the-art performance of models designed specifically for the task (Ji and Eisenstein, 2013). SDAE is also consistently better than SAE, which aligns with other findings that adding noise to AEs produces richer representations (Vincent et al., 2008). Results on the unsupervised evaluations are shown in Table 4. The same DictRep model performs best on four of the six STS categories (and overall) and is joint-top performer on SICK. Of the models trained on raw text, simply adding CBOW word vectors works best on STS. The best performing raw text model on SICK is FastSent, which achieves almost identical performance to CPHRASE’s state-of-the-art performance for a distributed model (Pham"
N16-1162,P14-1062,0,0.00809071,"ral language models that compute sentence representations from unlabelled, naturally-ocurring data, as with the predominant methods for word representations.2 Likewise, we do not focus on ‘bottom up’ models where phrase or sentence representations are built from fixed mathe proposed bymatical operations on word vectors (although we do consider a canonical case - see CBOW below); these were already compared by Milajevs et al. (2014). Most space is devoted to our novel approaches, and we refer the 2 This excludes innovative supervised sentence-level architectures including (Socher et al., 2011; Kalchbrenner et al., 2014) and many others. 1368 reader to the original papers for more details of existing models. 2.1 Existing Models Trained on Text SkipThought Vectors For consecutive sentences Si−1 , Si , Si+1 in some document, the SkipThought model (Kiros et al., 2015) is trained to predict target sentences Si−1 and Si+1 given source sentence Si . As with all sequence-to-sequence models, in training the source sentence is ‘encoded’ by a Recurrent Neural Network (RNN) (with Gated Recurrent uUnits (Cho et al., 2014)) and then ‘decoded’ into the two target sentences in turn. Importantly, because RNNs employ a single"
N16-1162,Q15-1016,0,0.0319935,"ion. A man is jumping into a full pool. /5 4 3.8 3.6 1.6 4.4 0.4 1.7 Table 2: Example sentence pairs and ‘similarity’ ratings from the unsupervised evaluations used in this study. 3 Evaluating Sentence Representations In previous work, distributed representations of language were evaluated either by measuring the effect of adding representations as features in some classification task - supervised evaluation (Collobert et al., 2011; Mikolov et al., 2013a; Kiros et al., 2015) - or by comparing with human relatedness judgements - unspervised evaluation (Hill et al., 2015a; Baroni et al., 2014b; Levy et al., 2015). The former setting reflects a scenario in which representations are used to inject general knowledge (sometimes considered as pre-training) into a supervised model. The latter pertains to applications in which the sentence representation space is used for direct comparisons, lookup or retrieval. Here, we apply and compare both evaluation paradigms. 3.1 Supervised Evaluations Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivi"
N16-1162,marelli-etal-2014-sick,0,0.0889193,"Missing"
N16-1162,D14-1079,0,0.0295207,"t of most current language understanding systems. We address this issue with a systematic comparison of cutting-edge methods for learning distributed representations of sentences. We focus on methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains. We also propose two new phrase or sentence representation learning objectives - Sequential Denoising Autoencoders (SDAEs) 1 See the contrasting conclusions in (Mitchell and Lapata, 2008; Clark and Pulman, 2007; Baroni et al., 2014a; Milajevs et al., 2014) among others. 1367 Proceedings of NAACL-HLT 2016, pages 1367–1377, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics and FastSent, a sentence-level log-bilinear bag-ofwords model. We compare all methods on two types of task - supervised and unsupervised evaluations - reflecting different ways in which representations are ultimately to be used. In the former setting, a classifier or regression model is applied to representations and trained with task-specific labelled data, while in the latter, representation spaces are directly queried using cosine dist"
N16-1162,P08-1028,0,0.0164017,"owledge (or ‘common sense’) (Norman, 1972) that is a critical missing part of most current language understanding systems. We address this issue with a systematic comparison of cutting-edge methods for learning distributed representations of sentences. We focus on methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains. We also propose two new phrase or sentence representation learning objectives - Sequential Denoising Autoencoders (SDAEs) 1 See the contrasting conclusions in (Mitchell and Lapata, 2008; Clark and Pulman, 2007; Baroni et al., 2014a; Milajevs et al., 2014) among others. 1367 Proceedings of NAACL-HLT 2016, pages 1367–1377, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics and FastSent, a sentence-level log-bilinear bag-ofwords model. We compare all methods on two types of task - supervised and unsupervised evaluations - reflecting different ways in which representations are ultimately to be used. In the former setting, a classifier or regression model is applied to representations and trained with task-specific labelled data, while in th"
N16-1162,P04-1035,0,0.083734,"cts a scenario in which representations are used to inject general knowledge (sometimes considered as pre-training) into a supervised model. The latter pertains to applications in which the sentence representation space is used for direct comparisons, lookup or retrieval. Here, we apply and compare both evaluation paradigms. 3.1 Supervised Evaluations Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Voorhees, 2002). We follow the procedure (and code) of Kiros et al. (2015): a logistic regression classifier is trained on top of sentence representations, with 10-fold cross-validation used when a train-test split is not pre-defined. 3.2 Unsupervised Evaluations We also measure how well representation spaces reflect human intuitions of the semantic sentence relatedness, by computing the cosine distance between vectors for the two sentences in each test pair, and correlating these distances with gold-standa"
N16-1162,P05-1015,0,0.0593828,"evaluation (Hill et al., 2015a; Baroni et al., 2014b; Levy et al., 2015). The former setting reflects a scenario in which representations are used to inject general knowledge (sometimes considered as pre-training) into a supervised model. The latter pertains to applications in which the sentence representation space is used for direct comparisons, lookup or retrieval. Here, we apply and compare both evaluation paradigms. 3.1 Supervised Evaluations Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Voorhees, 2002). We follow the procedure (and code) of Kiros et al. (2015): a logistic regression classifier is trained on top of sentence representations, with 10-fold cross-validation used when a train-test split is not pre-defined. 3.2 Unsupervised Evaluations We also measure how well representation spaces reflect human intuitions of the semantic sentence relatedness, by computing the cosine distance between"
N16-1162,P15-1094,0,0.0499898,"stimate sentence representations s for arbitrary sentences based on the vw . This additional computation is reflected in the higher encoding time in Table 1 (TE). ILSVRC 2014 object recognition task (Russakovsky et al., 2014). Multi-modal distributed representations can be encoded by feeding test sentences forward through the trained model. Bottom-Up Methods We train CBOW and SkipGram word embeddings (Mikolov et al., 2013b) on the same text corpus as the SkipThought and ParagraphVector models, and compose by elementwise addition as per Mitchell and Lapata (2010).4 We also compare to C-PHRASE (Pham et al., 2015), an approach that exploits a (supervised) parser to infer distributed semantic representations based on a syntactic parse of sentences. C-PHRASE achieves state-of-the-art results for distributed representations on several evaluations used in this study.5 NMT We consider the sentence representations learned by neural MT models. These models have identical architecture to SkipThought, but are trained on sentence-aligned translated texts. We used a standard architecture (Cho et al., 2014) on all available En-Fr and En-De data from the 2015 Workshop on Statistical MT (WMT).7 Non-Distributed Basel"
N16-1162,W15-2701,0,0.0147471,"an others in this study. 6 https://www.cl.cam.ac.uk/˜fh295/. Definitions from the training data matching those in the WordNet STS 2014 evaluation (used in this study) were excluded. 5 1369 2.3 Novel Text-Based Models We introduce two new approaches designed to address certain limitations with the existing models. 7 www.statmt.org/wmt15/translation-task.html FastSent The performance of SkipThought vectors shows that rich sentence semantics can be inferred from the content of adjacent sentences. The model could be said to exploit a type of sentence-level Distributional Hypothesis (Harris, 1954; Polajnar et al., 2015). Nevertheless, like many deep neural language models, SkipThought is very slow to train (see Table 1). FastSent is a simple additive (log-bilinear) sentence model designed to exploit the same signal, but at much lower computational expense. Given a BOW representation of some sentence in context, the model simply predicts adjacent sentences (also represented as BOW) . More formally, FastSent learns a source uw and target vw embedding for each word in the model vocabulary. For a training example Si−1 , Si , Si+1 of consecutive sentences, Si is represented as the sum P of its source embeddings s"
N16-1162,D11-1014,0,0.0156144,"lysis, we compare neural language models that compute sentence representations from unlabelled, naturally-ocurring data, as with the predominant methods for word representations.2 Likewise, we do not focus on ‘bottom up’ models where phrase or sentence representations are built from fixed mathe proposed bymatical operations on word vectors (although we do consider a canonical case - see CBOW below); these were already compared by Milajevs et al. (2014). Most space is devoted to our novel approaches, and we refer the 2 This excludes innovative supervised sentence-level architectures including (Socher et al., 2011; Kalchbrenner et al., 2014) and many others. 1368 reader to the original papers for more details of existing models. 2.1 Existing Models Trained on Text SkipThought Vectors For consecutive sentences Si−1 , Si , Si+1 in some document, the SkipThought model (Kiros et al., 2015) is trained to predict target sentences Si−1 and Si+1 given source sentence Si . As with all sequence-to-sequence models, in training the source sentence is ‘encoded’ by a Recurrent Neural Network (RNN) (with Gated Recurrent uUnits (Cho et al., 2014)) and then ‘decoded’ into the two target sentences in turn. Importantly,"
N18-1048,W13-3520,0,0.199805,"Missing"
N18-1048,D16-1250,0,0.180495,"places word vectors from X0s with their approximations, i.e., f -mapped vectors.2 Objective Functions As mentioned, the N seen words xi ∈ Vs in fact serve as our “pseudotranslation” pairs supporting the learning of a crossspace mapping function. In practice, in its highlevel formulation, our mapping problem is equivalent to those encountered in the literature on crosslingual word embeddings where the goal is to learn a shared cross-lingual space given monolingual vector spaces in two languages and N1 translation pairs (Mikolov et al., 2013a; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Artetxe et al., 2016, 2017; Conneau et al., 2017; Ruder et al., 2017). In our setup, the standard objective based on L2 -penalised 2 We have empirically confirmed the intuition that the first variant is superior to this alternative. We do not report the actual quantitative comparison for brevity. 518 swish swish swish ... swish Xd = Xs ∪ Xu attract-repel X0s ∪ Xu (distributional) (specialised: seen) xi (d=300) mapping c0 Xf = X0s ∪ X u (specialised final: all) (a) High-level illustration Xu x&apos;i,h2 (d2=512) Hidden 1 Hidden 2 ... ... ... ... ... ...... ... ... ... ... ... ... x&apos;i,h1 (d1=512) Input ... ... ... ... T"
N18-1048,P17-1042,0,0.0514536,"Missing"
N18-1048,P14-2131,0,0.0446942,"ector and the correct target vector to have a maximum cosine similarity. We do not report the results with this variant as, although it outscores the MSE-style objective, it was consistently outperformed by the MM objective. 4 For further details regarding the architectures and training setup of the used vector collections, we refer the reader to the original papers. Additional experiments with other word vectors, e.g., with CONTEXT 2 VEC (Melamud et al., 2016a) (which uses bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for context modeling), and with dependency-word based embeddings (Bansal et al., 2014; Melamud et al., 2016b) lead to similar results and same conclusions. 5 We have experimented with another set of constraints used in prior work (Zhang et al., 2014; Ono et al., 2015), reaching similar conclusions: these were extracted from WordNet (Fellbaum, 1998) and Roget (Kipfer, 2009), and comprise 1,023,082 synonymy pairs and 380,873 antonymy pairs. PN  omitted only before the final output layer to enable full-range predictions (see Fig. 1b again). The choices of non-linear activation and initialisation are guided by recent recommendations from the literature. First, we use swish (Ramac"
N18-1048,Q17-1010,0,0.59926,"cialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often done as a post-processing (sometimes referred"
N18-1048,D14-1082,0,0.0136967,". This approach, applicable to any postprocessing model, yields considerable gains over the initial specialisation models both in intrinsic word similarity tasks, and in two downstream tasks: dialogue state tracking and lexical text simplification. The positive effects persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of inte"
N18-1048,N15-1184,0,0.37829,"Missing"
N18-1048,N13-1092,0,0.108384,"Missing"
N18-1048,D16-1235,1,0.913856,"Missing"
N18-1048,D17-1185,1,0.901185,"Missing"
N18-1048,P15-2011,1,0.915838,"Missing"
N18-1048,P16-1156,0,0.0292948,"these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). Such post-processing models are popular because they offer a portable, flexible, and light-weight approach to incorporating external knowledge into arbitrary vector spaces, yielding state-of-the-art results on language understanding tasks (Faruqui et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017b). Existing post-processing models, however, suffer from a major limitation. Their modus operandi is to enrich the distributional information with external knowledge only if such knowledge is present in a lexical resource. This means that they update an"
N18-1048,W14-4337,0,0.149535,"Missing"
N18-1048,J15-4004,1,0.950289,"demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often done as a post-processing (sometimes referred to as retrofitting) step: input word vectors are fine-tuned to satisfy linguistic constraints extracted from lexical resources such as WordNet or BabelNet (Faruqui et al., 2015; Mrkši´c et al."
N18-1048,P14-2075,0,0.105851,".10 For a complex word, L IGHT-LS considers the most similar words from the vector space as simplification candidates. Candidates are ranked according to several features, indicating simplicity and fitness for the context (semantic relatedness to the context of the complex word). The substitution is made if the best candidate is simpler than the original word. By providing vector spaces post-specialised for semantic similarity to L IGHT-LS, we expect to more often replace complex words with their true synonyms. We evaluate L IGHT-LS performance in the all setup on the LS benchmark compiled by Horn et al. (2014), who crowdsourced 50 manual simplifications for each complex word. As in prior work, we evaluate performance with the following metrics: 1) Accurracy (Acc.) is the number of correct simplifications made (i.e., the system made the simplification and its substitution is found in the list of crowdsourced substitutions), divided by the total number of indicated complex words; 2) Changed (Ch.) is the percentage of indicated complex words 10 https://github.com/codogogo/lightls Conclusion and Future Work We have presented a novel post-processing model, termed post-specialisation, that specialises wo"
N18-1048,D15-1242,0,0.251526,"Missing"
N18-1048,D15-1032,0,0.0200176,"can thus replace the linear map with a nonlinear function f : Rdim → Rdim . The non-linear mapping, illustrated by Fig. 1b, is implemented as a deep feed-forward fully-connected neural network (DFFN) with H hidden layers and non-linear activations. This variant is called NONLINEAR - MSE. Another variant objective is the contrastive margin-based ranking loss with negative sampling (MM) similar to the original ATTRACT- REPEL objective, used in other applications in prior work (e.g., for cross-modal mapping) (Weston et al., 2011; Frome et al., 2013; Lazaridou et al., 2015; c0 i = f (xi ) denote Kummerfeld et al., 2015). Let x the predicted vector for the word xi ∈ Vs , and let x0 i refer to the “true” vector of xi in the specialised space X0s after the AR specialisation procedure. The MM loss is then defined as follows: JMM = N X k   X  0  0 , x0 0 c c τ δmm − cos x i i + cos x i , x j i=1 j6=i where cos is the cosine similarity measure, δmm is the margin, and k is the number of negative samples. The objective tries to learn the mapping f so c0 i is by the specified that each predicted vector x margin δmm closer to the correct target vector x0 i than to any other of k target vectors x0 j serving as nega"
N18-1048,P15-1027,0,0.481288,"utput of the initial specialisation procedure and replaces word vectors from X0s with their approximations, i.e., f -mapped vectors.2 Objective Functions As mentioned, the N seen words xi ∈ Vs in fact serve as our “pseudotranslation” pairs supporting the learning of a crossspace mapping function. In practice, in its highlevel formulation, our mapping problem is equivalent to those encountered in the literature on crosslingual word embeddings where the goal is to learn a shared cross-lingual space given monolingual vector spaces in two languages and N1 translation pairs (Mikolov et al., 2013a; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Artetxe et al., 2016, 2017; Conneau et al., 2017; Ruder et al., 2017). In our setup, the standard objective based on L2 -penalised 2 We have empirically confirmed the intuition that the first variant is superior to this alternative. We do not report the actual quantitative comparison for brevity. 518 swish swish swish ... swish Xd = Xs ∪ Xu attract-repel X0s ∪ Xu (distributional) (specialised: seen) xi (d=300) mapping c0 Xf = X0s ∪ X u (specialised final: all) (a) High-level illustration Xu x&apos;i,h2 (d2=512) Hidden 1 Hidden 2 ... ... ... ... ... ...... ... ... ... ."
N18-1048,P14-2050,0,0.108038,"nguages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often"
N18-1048,K16-1006,0,0.283738,"ble to any postprocessing model, yields considerable gains over the initial specialisation models both in intrinsic word similarity tasks, and in two downstream tasks: dialogue state tracking and lexical text simplification. The positive effects persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation"
N18-1048,N16-1118,0,0.193378,"ble to any postprocessing model, yields considerable gains over the initial specialisation models both in intrinsic word similarity tasks, and in two downstream tasks: dialogue state tracking and lexical text simplification. The positive effects persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation"
N18-1048,P17-1163,1,0.902962,"Missing"
N18-1048,P15-2130,1,0.814001,"Missing"
N18-1048,Q15-1016,0,0.0670043,"e importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often done as a post-proc"
N18-1048,Q17-1022,1,0.895295,"Missing"
N18-1048,P15-1145,0,0.186188,"ated Work and Motivation Vector Space Specialisation A standard approach to incorporating external and background knowledge into word vector spaces is to pull the representations of similar words closer together and to push words in undesirable relations (e.g., antonyms) away from each other. Some models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et"
N18-1048,P16-2074,0,0.109936,"17). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). Such post-processing models are popular because they offer a portable, flexible, and light-weight approach to incorporating external knowledge into arbitrary vector spaces, yielding state-of-the-art results on language understanding tasks (Faruqui et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017b). Existing post-processing models, however, suffer from a major limitation. Their modus operandi is to enrich the distributional information with external knowledge only if such knowledge is present in a le"
N18-1048,N15-1100,0,0.578375,"vation Vector Space Specialisation A standard approach to incorporating external and background knowledge into word vector spaces is to pull the representations of similar words closer together and to push words in undesirable relations (e.g., antonyms) away from each other. Some models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe"
N18-1048,Q16-1030,0,0.371651,"e Specialisation A standard approach to incorporating external and background knowledge into word vector spaces is to pull the representations of similar words closer together and to push words in undesirable relations (e.g., antonyms) away from each other. Some models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wie"
N18-1048,P15-2070,0,0.0619664,"Missing"
N18-1048,D14-1162,0,0.106855,"s persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic speci"
N18-1048,P15-1173,0,0.0539443,", 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). Such post-processing models are popular because they offer a portable, flexible, and light-weight approach to incorporating external knowledge into arbitrary vector spaces, yielding state-of-the-art results on language understanding tasks (Faruqui et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017b). Existing post-processing models, however, suffer from a major limitation. Their modus operandi is to enrich the distributional information with external knowledg"
N18-1048,K15-1026,0,0.190252,"s a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often done as a post-processing (sometimes referred to as retrofitting) step: input word vectors are fine-tuned to satisfy linguistic constraints extracted from lexical resources such as WordNet or BabelNet (Faruqui et al., 2015; Mrkši´c et al., 2017). The use of exte"
N18-1048,P16-2084,1,0.896635,"Missing"
N18-1048,P16-1024,1,0.911463,"Missing"
N18-1048,N18-1103,1,0.857227,"Missing"
N18-1048,D17-1270,1,0.894634,"Missing"
N18-1048,P17-1006,1,0.878454,"Missing"
N18-1048,E17-1042,1,0.811334,"Missing"
N18-1048,Q15-1025,0,0.28346,"e models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). Such post-processing models are popular because they offer a portable, flexible, and light-weight approach to incorporating external knowledge into arbit"
N18-1048,P14-2089,0,0.217393,"spaces for English and for three test languages (English, German, Italian), verifying the robustness of our approach. 2 Related Work and Motivation Vector Space Specialisation A standard approach to incorporating external and background knowledge into word vector spaces is to pull the representations of similar words closer together and to push words in undesirable relations (e.g., antonyms) away from each other. Some models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge f"
N18-1048,D14-1161,0,0.248362,"tive, it was consistently outperformed by the MM objective. 4 For further details regarding the architectures and training setup of the used vector collections, we refer the reader to the original papers. Additional experiments with other word vectors, e.g., with CONTEXT 2 VEC (Melamud et al., 2016a) (which uses bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for context modeling), and with dependency-word based embeddings (Bansal et al., 2014; Melamud et al., 2016b) lead to similar results and same conclusions. 5 We have experimented with another set of constraints used in prior work (Zhang et al., 2014; Ono et al., 2015), reaching similar conclusions: these were extracted from WordNet (Fellbaum, 1998) and Roget (Kipfer, 2009), and comprise 1,023,082 synonymy pairs and 380,873 antonymy pairs. PN  omitted only before the final output layer to enable full-range predictions (see Fig. 1b again). The choices of non-linear activation and initialisation are guided by recent recommendations from the literature. First, we use swish (Ramachandran et al., 2017; Elfwing et al., 2017) as nonlinearity, defined as swish(x) = x · sigmoid(βx). We fix β = 1 as suggested by Ramachandran et al. (2017).6 Second"
N19-1097,D17-1010,0,0.108153,"d word segmentation. 1 Introduction Word representations are central to a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Jia and Liang, 2016; Ammar et al., 2016; Goldberg, 2017; Peters et al., 2018; Kudo, 2018, inter alia). Standard word representation models are based on the distributional hypothesis (Harris, 1954) and induce representations from large unlabeled corpora using word co-occurrence statistics (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). However, as pointed out by recent work (Bojanowski et al., 2017; Vania and Lopez, 2017; Pinter et al., 2017; Chaudhary et al., 2018; Zhao et al., 2018), mapping a finite set of word types into corresponding word representations limits the capacity of these models to learn beyond distributional information, which leads to several fundamental limitations. The standard approaches ignore the internal structure of words, that is, the syntactic or semantic composition from subwords or morphemes to words, and are incapable of parameter sharing at the level of subword units. Assigning only a single vector to each word causes the data sparsity problem, especially in resource-poor settings where huge amounts"
N19-1097,C14-1015,0,0.020782,"ion δ(•) dishonestly Figure 1: Illustration of the general framework for learning subword-informed word representations, with the focus on two crucial components: 1) segmentation of words and 2) subword embedding composition. By varying the two components, and optionally including or excluding position embeddings from the computations, we obtain a wide spectrum of different subwordinformed configurations used in the study (see §2). Our word-level representation model in this work is skipgram (on the top layer of the figure), but it can be replaced by any other distributional word-level model. Qiu et al., 2014; Cotterell and Sch¨utze, 2015; Wieting et al., 2016; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Pinter et al., 2017; Cotterell and Sch¨utze, 2018). First, the models differ in the chosen method for segmenting words into subwords. The methods range from fully supervised approaches (Cotterell and Sch¨utze, 2015) to e.g. unsupervised approaches based on BPE (Heinzerling and Strube, 2018). Second, another crucial aspect is the subword composition function used to obtain word embeddings from the embeddings of each word’s constituent subword units. Despite a steadily increasing interest in"
N19-1097,W13-3512,0,0.493012,"e words (Gerz et al., 2018). Although potentially useful information on word relationships is hidden in their internal subwordlevel structure,1 subword-agnostic word representation models do not take these structure features into account and are effectively unable to represent rare words accurately, or unseen words at all. Therefore, there has been a surge of interest in subword-informed word representation architectures aiming to address these gaps. A large number of architectures has been proposed in related research, and they can be clustered over the two main axes (Lazaridou et al., 2013; Luong et al., 2013; 1 For example, nouns in Finnish have 15 cases and 3 plural forms; Spanish verbs may contain over 40 inflected forms, sharing the lemma and taking up standard suffixes. 912 Proceedings of NAACL-HLT 2019, pages 912–932 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Skip-gram Skip-gram word embedding context embedding They Composition function op f(•) make op op honest ly position embedding subword embedding dis Segmentation δ(•) dishonestly Figure 1: Illustration of the general framework for learning subword-informed word representations, with t"
N19-1097,I17-1007,0,0.0208649,"arsing task. The two best configurations are selected according to LAS. Dependency Parsing Next, we use the syntactic dependency parsing task to analyze the importance of subword information for syntactically-driven downstream applications. For all test languages, we rely on the standard Universal Dependencies treebanks (UD v2.2; Nivre et al. (2016)). We use subword-informed word embeddings from different configurations to initialize the deep biaffine parser of Dozat and Manning (2017) which has shown competitive performance in shared tasks (Dozat et al., 2017) and among other parsing models (Ma and Hovy, 2017; Shi et al., 2017; Ma et al., Fine-Grained Entity Typing The task is to map entities, which could comprise more than one entity token, to predefined entity types (Yaghoobzadeh and Sch¨utze, 2015). It is a suitable semi-semantic task to test our subword models, as the subwords of entities usually carry some semantic information from which the entity types can be inferred. For example, Lincolnshire will belong to /location/county as -shire is a suffix that strongly indicates a location. We rely on an entity typing dataset of Heinzerling and Strube (2018) built for over 250 languages by obtainin"
N19-1097,P18-1130,0,0.0252072,"Missing"
N19-1097,P16-1162,0,0.120272,"ach word, apart from generating Sw , it also outputs the corresponding morphotactic tags Tw .5 In §2.3 we discuss how to incorporate information from Tw into subword representations. Morfessor Morfessor (Smit et al., 2014) denotes a family of generative probabilistic models for unsupervised morphological segmentation used, among other applications, to learn morphologicallyaware word embeddings (Luong et al., 2013). BPE Byte Pair Encoding (BPE; Gage (1994)) is a simple data compression algorithm. It has become a de facto standard for providing subword information in neural machine translation (Sennrich et al., 2016). The input word is initially split into a sequence of characters, with each unique character denoted as a byte. BPE then iteratively replaces the most common pair of consecutive bytes with a new byte that does not occur within the data, and the number of iterations can be set in advance to control the granularity of the byte combinations. An example output for all three methods is shown in Table 1. Note that a standard practice in subword-informed models is to also insert the entire word token into Sw (Bojanowski et al., 2017).6 This is, however, again an optional step and we evaluate configu"
N19-1097,L18-1008,0,0.0229894,"ertion if |Sw |&gt; 1. For CHIPMUNK, a generic tag word is added to the sequence Tw . CHIPMUNK configurations without the use of Tw to analyze its contribution.7 After generating Sw , an optional step is to have a learnable position embedding sequence Pw further operate on Sw to encode the order information. Similar to Ws , the definition of the position embedding matrix Wp also varies: for Morfessor and BPE, we use the absolute positions of subwords in the sequence Sw , whereas for CHIPMUNK morphotactic tags are encoded directly as positions. Finally, following prior work (Gehring et al., 2017; Mikolov et al., 2018), we use addition and element-wise multiplication between each subword vector s from Sw and the corresponding position vector p from Pw to compute each entry r for the final sequence of subword vectors Rw : r=s+p 2.4 or r = s p. (2) Composition Functions A composition function fΘ is then applied to the sequence of subword embeddings Rw to compute the final word embedding w. We investigate three composition functions: 1) addition, 2) single-head and 3) multi-head self-attention (Vaswani et al., 2017; Lin et al., 2017).8 Addition is used in the original fastText model of Bojanowski et al. (2017)"
N19-1097,Q17-1022,1,0.87147,"Missing"
N19-1097,L16-1262,0,0.0896054,"Missing"
N19-1097,D14-1162,0,0.0980802,"ations based on unsupervised segmentation (e.g., BPE, Morfessor) are sometimes comparable to or even outperform the ones based on supervised word segmentation. 1 Introduction Word representations are central to a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Jia and Liang, 2016; Ammar et al., 2016; Goldberg, 2017; Peters et al., 2018; Kudo, 2018, inter alia). Standard word representation models are based on the distributional hypothesis (Harris, 1954) and induce representations from large unlabeled corpora using word co-occurrence statistics (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). However, as pointed out by recent work (Bojanowski et al., 2017; Vania and Lopez, 2017; Pinter et al., 2017; Chaudhary et al., 2018; Zhao et al., 2018), mapping a finite set of word types into corresponding word representations limits the capacity of these models to learn beyond distributional information, which leads to several fundamental limitations. The standard approaches ignore the internal structure of words, that is, the syntactic or semantic composition from subwords or morphemes to words, and are incapable of parameter sharing at the level of subword units"
N19-1097,N18-1202,0,0.0318293,"ndency parsing, fine-grained entity typing) for 5 languages representing 3 language types. Our main results clearly indicate that there is no “one-sizefits-all” configuration, as performance is both language- and task-dependent. We also show that configurations based on unsupervised segmentation (e.g., BPE, Morfessor) are sometimes comparable to or even outperform the ones based on supervised word segmentation. 1 Introduction Word representations are central to a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Jia and Liang, 2016; Ammar et al., 2016; Goldberg, 2017; Peters et al., 2018; Kudo, 2018, inter alia). Standard word representation models are based on the distributional hypothesis (Harris, 1954) and induce representations from large unlabeled corpora using word co-occurrence statistics (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). However, as pointed out by recent work (Bojanowski et al., 2017; Vania and Lopez, 2017; Pinter et al., 2017; Chaudhary et al., 2018; Zhao et al., 2018), mapping a finite set of word types into corresponding word representations limits the capacity of these models to learn beyond distributional information, which"
N19-1097,Q15-1016,0,\N,Missing
N19-1097,J15-4004,1,\N,Missing
N19-1097,P13-1149,0,\N,Missing
N19-1097,E14-2006,0,\N,Missing
N19-1097,D14-1082,0,\N,Missing
N19-1097,K15-1017,0,\N,Missing
N19-1097,Q16-1023,0,\N,Missing
N19-1097,D16-1157,0,\N,Missing
N19-1097,Q17-1010,0,\N,Missing
N19-1097,Q18-1003,0,\N,Missing
N19-1097,E17-2067,0,\N,Missing
N19-1097,K17-3002,0,\N,Missing
N19-1097,W18-1205,0,\N,Missing
N19-1097,C18-1323,0,\N,Missing
N19-1097,D18-1029,1,\N,Missing
N19-1097,D18-1059,0,\N,Missing
N19-1097,D15-1083,0,\N,Missing
N19-1097,N15-1140,0,\N,Missing
N19-1097,W17-0228,0,\N,Missing
N19-1354,P15-1034,0,0.137901,"Missing"
N19-1354,K18-2005,0,0.262058,"n dependency parsing required careful feature engineering (McDonald et al., 2005b; Koo et al., 2008), this has become less of a concern in recent years with the emergence of deep neural networks (Kiperwasser and Goldberg, 2016; Dozat et al., 2017). Nonetheless, an accurate parser still requires a large amount of labeled data for training, which is costly to obtain, while the lack of data often causes overfitting and poor generalization. Several approaches for parsing in the small data regime have been proposed. These include augmenting input data with pretrained embedding (Dozat et al., 2017; Che et al., 2018), leveraging unannotated data via semi-supervised learning (Corro and Titov, 2018), predicting based on a pool of high probability trees (Niculae et al., 2018; Keith et al., 2018), and transferring annotation or model across languages (Agic et al., 2016; Lacroix et al., 2016; Rasooli and Collins, 2017). Despite the empirical success of these approaches, an inherent problem still holds: The maximum likelihood parameter estimation (MLE) in deep neural networks (DNNs) introduces statistical challenges at both estimation (training), due to the risk of overfitting, and at test time as the model ign"
N19-1354,P17-1110,0,0.0225826,"connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter, first we generate the maximum scoring parse using the arcfactored decomposition (McDonald et al., 2005a) and dynamic progra"
N19-1354,Q16-1022,0,0.0247513,"theless, an accurate parser still requires a large amount of labeled data for training, which is costly to obtain, while the lack of data often causes overfitting and poor generalization. Several approaches for parsing in the small data regime have been proposed. These include augmenting input data with pretrained embedding (Dozat et al., 2017; Che et al., 2018), leveraging unannotated data via semi-supervised learning (Corro and Titov, 2018), predicting based on a pool of high probability trees (Niculae et al., 2018; Keith et al., 2018), and transferring annotation or model across languages (Agic et al., 2016; Lacroix et al., 2016; Rasooli and Collins, 2017). Despite the empirical success of these approaches, an inherent problem still holds: The maximum likelihood parameter estimation (MLE) in deep neural networks (DNNs) introduces statistical challenges at both estimation (training), due to the risk of overfitting, and at test time as the model ignores the uncertainty around the estimated parameters. When training data is small these challenges are more pronounced. The Bayesian paradigm provides a statistical framework which addresses both challenges by (i) including prior knowledge to guide the"
N19-1354,K17-3002,0,0.139222,"assumption between the arcs, the parsing is done via a dynamic programming solution that finds the dependency parse ? ∗ such that, ( ) ∑ ∗ ? = argmax SCORE(? ) = ARC -SCORE (?, ?) . ? (?,?)∈? Next, given ? ∗ , for each arc (?, ?) ∈ ? ∗ the LABEL -SCORE (?, ?, ?) is computed as: ( ) ′ ′(???−ℎ???) ′(???−???) ′ ? ×tanh(? ?? +? ?? +? ) [?], ⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟ ⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟ head specialized ?? modifier specialized ?? where ? is a dependency relation type, ? ∈ {?? }??=1 , ? ′ is (? × ℎ), ?′ is (? × 1), and ? ′(.) is (ℎ × ?). The label for each dependency arc (?, ?) is then chosen by a max operation. Dozat et al. (2017) proposed an extension of the BiLSTM parser by replacing the non-linear transformation of ARC-SCORE and LABEL -SCORE with a linear transformation (BiAFFINE). This was further extended by Che et al. (2018) who utilized contextualized word embeddings (Peters et al., 2018). Both extensions showed success in dependency parsing shared tasks (Zeman et al., 2017, 2018). Our Neural Parser Architecture Our network architecture extends the BiLSTM model with an additional BiLSTM layer and input signals. While our 3510 architecture is not the core contribution of this paper, we aim to implement our BNNP o"
N19-1354,P18-1128,1,0.834955,"shared task on parsing to Universal Dependencies (UD) (Zeman et al., 2017).5 5.1 Experimental Setup We use the UDPipe baseline outputs for segmentation and POS tagging of the raw test data (released along with the raw test data). While segmentation and POS errors substantially impact the quality of the final predicted parse, their exploration is beyond our scope. Our evaluation metric is Labeled Attachment Score (LAS), computed by the shared task evaluation script. Statistical significance, when mentioned, is computed over 20 runs, via the Kolmogorov-Smirnov test (Reimers and Gurevych, 2017; Dror et al., 2018) with ? = 0.01. Mono-Lingual Experiments We experiment with Persian (fa), Korean (ko), Russian (ru), Turkish (tr), Vietnamese (vi) and Irish (ga), all with less than 5? training sentences (Table 1). For comparison we report the scores published by the top system of the CoNLL 2017 shared task, B iAFFINE (Dozat et al., 2017), noting the following differences between their input and output and ours. The BiAFFINE parser: (i) uses the UDPipe outputs for segmentation but corrects POS errors before parsing, (ii) includes both language specific and universal POS tags in the input layer while we only i"
N19-1354,C96-1058,0,0.122623,"oodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter, first we generate the maximum scoring parse using the arcfactored decomposition (McDonald et al., 2005a) and dynamic programming (Eisner, 1996). This can be done concurrently for all samples, resulting in a running time identical to the non-Bayesian approach. For each labelled edge, we replace its score in the ARC-SCORE matrix with its occurrence count in the collection of sampled trees and infer the final tree using counts as scores. The predicted structure is then passed to the label predictor, which assigns labels to the edges (§2). This decoding approach, while selecting the global structure with the highest probability under the approximate posterior, could potentially allow for additional corrections of the highest scoring tree"
N19-1354,N09-1037,0,0.0193894,"1997) lends itself as a natural choice for low-resource settings as it aims at leveraging the commonality between tasks to improve their performance in the absence of sufficient amount of training data. This framework hence naturally complements Bayesian modeling in dealing with the challenges of the small data regime. We couple our BNNP with POS tagging due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning"
N19-1354,P17-1030,0,0.146707,"rameters which offers the desired degree of uncertainty by exploring the posterior space during inference. However, this solution comes with a high computational cost, specifically in DNNs, and is often replaced by regularization techniques such as dropout (Srivastava et al., 2014) as well as ensemble learning and prediction averaging (Liu et al., 2018; Che et al., 2018). Bayesian neural networks (BNNs) have attracted some attention (Welling and Teh, 2011; HernándezLobato and Adams, 2015; Li et al., 2016; Gong et al., 2018). Yet, its current application to NLP is limited to language modeling (Gan et al., 2017), and BNNs have not been developed for structured 3509 Proceedings of NAACL-HLT 2019, pages 3509–3519 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics prediction tasks such as dependency parsing. In this paper we aim to close this gap and propose the first BNN for dependency parsing (BNNP). To address the costs of inference step, we apply an efficient sampling procedure via stochastic gradient Langevin dynamics (SGLD) (Welling and Teh, 2011). At training, samples from the posterior distribution of the parser parameters are generated via controlled"
N19-1354,P96-1024,0,0.194908,"and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter, first we generate the maximum scoring parse using the arcfactored decomposition (McDonald et al., 2005a) and dynamic programming (Eisner, 1996). This can be done concurrently for all samples, resulting in a running time identical to the non-Bayesian approach. For each labelled edge, we replace its score in the ARC-SCORE matrix with its occurrence count in the collection of sampled trees and infer the final tree using counts as scores. The predicted structure is then passed to the label predictor, which assigns labels to the edges (§2). This decoding approach, wh"
N19-1354,D17-1206,0,0.0277421,"respectively. To train the discriminator, a sum of the cross-entropy losses for ?1∶? is used (denoted by ???? ). As the parameters of the discriminators are being updated, the gradient signals to minimize the discriminator’s error are backpropagated with an opposite sign to the shared BiLSTM layer, which adversarially encourages the shared BiLSTM to fool the discriminator. Our training schedule alternates between the two modes, in one mode optimizing the shared and taskspecific parameters based on ????? and ??? (in 4 We also tried layer-wise placements of tasks (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) and the results were slightly worse. Details are omitted for space reason. 3513 random order), while in the other mode optimizing ???? which includes the shared and discriminatorspecific parameters. 5 Experiments and Results We experiment with mono-lingual and cross-lingual dependency parsing using the treebanks of the CoNLL 2017 shared task on parsing to Universal Dependencies (UD) (Zeman et al., 2017).5 5.1 Experimental Setup We use the UDPipe baseline outputs for segmentation and POS tagging of the raw test data (released along with the raw test data). While segmentation and POS errors su"
N19-1354,N18-1084,0,0.0478884,"Missing"
N19-1354,D17-1302,0,0.0199562,"due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter, first we generate the maximum scoring parse using the arcfactored decomposition (McDonald et al., 2005a)"
N19-1354,Q16-1023,0,0.0215492,"anji (kmr), Buriat (bxr), and Northern Sami (sme). We also report the results for each language, where the combination of training datasets for the rest of the languages (marked as +) was used for training. The cross-lingual experiments are done on delexicalized 5 For train and dev sets (1-1983), test set (1-2184), and pretrained embeddings (1-1989) see: https: //lindat.mff.cuni.cz/repository/xmlui/ handle/11234/{1-1983,1-2184,1-1989} parses after replacing the words with their Universal POS tags. Models and Baselines - Single-Task We consider the following models: BASE is the BiLSTM model of Kiperwasser and Goldberg (2016); BASE ++ extends BASE by having 2 layers of BiLSTM s and using 1 layer of character level BiLSTM (§2); + SHARED includes an additional BiLSTM (dashed box in Figure 1). We included this to provide a fair comparison (in terms of the number of parameters) with the multi-task experiments but we apply a higher dropout rate to resolve overfitting; ENSEMBLE denotes a collection of 9 + SHARED models each randomly initialized (Reimers and Gurevych, 2017) and trained for MLE with MBR (§3.3) applied for prediction; MAP denotes the + SHARED model optimized for MAP instead of MLE; + SGLD denotes Bayesian"
N19-1354,D18-1290,1,0.853481,"ow-resource settings as it aims at leveraging the commonality between tasks to improve their performance in the absence of sufficient amount of training data. This framework hence naturally complements Bayesian modeling in dealing with the challenges of the small data regime. We couple our BNNP with POS tagging due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Chara"
N19-1354,K17-1041,0,0.107917,"Missing"
N19-1354,P05-1012,0,0.175811,"2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter, first we generate the maximum scoring parse using the arcfactored decomposition (McDonald et al., 2005a) and dynamic programming (Eisner, 1996). This can be done concurrently for all samples, resulting in a running time identical to the non-Bayesian approach. For each labelled edge, we replace its score in the ARC-SCORE matrix with its occurrence count in the collection of sampled trees and infer the final tree using counts as scores. The predicted structure is then passed to the label predictor, which assigns labels to the edges (§2). This decoding approach, while selecting the global structure with the highest probability under the approximate posterior, could potentially allow for additiona"
N19-1354,P08-1068,0,0.11998,"Missing"
N19-1354,D16-1180,0,0.101046,"SCORE(? )− SCORE(? ) For label prediction, hinge loss with ? = 1 is used (denoted by ??? ). We refer to the parser loss as: ????? = ??? + ??? . Beyond MLE Training The point-estimate of DNN parameters is computationally efficient, but ignores the uncertainty around model parameters during learning. This results in an overconfidence over model predictions during the inference phase. A common generic practice to incorporate a degree of uncertainty is to consider an ensemble of models. Indeed, for dependency parsing ensemble learning has shown to improve accuracy (Surdeanu and Manning, 2010; Kuncoro et al., 2016; Che et al., 2018). However, ensembles are computationally demanding due to the large number of participating models. The de-facto approach to overcome this 2 Stacking BiLSTM s is believed to be helpful. In our case, the addition of a third layer led to overfitting. has been to randomly perturb the structure of the network for each training instance by switching off connections between the nodes, a practice known as dropConnect (Wan et al., 2013), or eliminating the nodes entirely, which is known as dropout (Srivastava et al., 2014). Hinton et al. (2012) demonstrated that dropping out a node"
N19-1354,N18-1202,0,0.0183951,"puted as: ( ) ′ ′(???−ℎ???) ′(???−???) ′ ? ×tanh(? ?? +? ?? +? ) [?], ⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟ ⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟ head specialized ?? modifier specialized ?? where ? is a dependency relation type, ? ∈ {?? }??=1 , ? ′ is (? × ℎ), ?′ is (? × 1), and ? ′(.) is (ℎ × ?). The label for each dependency arc (?, ?) is then chosen by a max operation. Dozat et al. (2017) proposed an extension of the BiLSTM parser by replacing the non-linear transformation of ARC-SCORE and LABEL -SCORE with a linear transformation (BiAFFINE). This was further extended by Che et al. (2018) who utilized contextualized word embeddings (Peters et al., 2018). Both extensions showed success in dependency parsing shared tasks (Zeman et al., 2017, 2018). Our Neural Parser Architecture Our network architecture extends the BiLSTM model with an additional BiLSTM layer and input signals. While our 3510 architecture is not the core contribution of this paper, we aim to implement our BNNP on a strong architecture. In §5.3 we demonstrate the contribution of these additions to our final results. In our BNNP, each word is represented as, ?? = ?(?? )◦?(?? )◦?(?  ? )◦?(?? ), where, similar to the BiAFFINE parser, ?(?? ) is a character-level representation of"
N19-1354,N16-1121,0,0.0202361,"e parser still requires a large amount of labeled data for training, which is costly to obtain, while the lack of data often causes overfitting and poor generalization. Several approaches for parsing in the small data regime have been proposed. These include augmenting input data with pretrained embedding (Dozat et al., 2017; Che et al., 2018), leveraging unannotated data via semi-supervised learning (Corro and Titov, 2018), predicting based on a pool of high probability trees (Niculae et al., 2018; Keith et al., 2018), and transferring annotation or model across languages (Agic et al., 2016; Lacroix et al., 2016; Rasooli and Collins, 2017). Despite the empirical success of these approaches, an inherent problem still holds: The maximum likelihood parameter estimation (MLE) in deep neural networks (DNNs) introduces statistical challenges at both estimation (training), due to the risk of overfitting, and at test time as the model ignores the uncertainty around the estimated parameters. When training data is small these challenges are more pronounced. The Bayesian paradigm provides a statistical framework which addresses both challenges by (i) including prior knowledge to guide the learning in the absenc"
N19-1354,E17-1005,0,0.0152841,"s Bayesian modeling in dealing with the challenges of the small data regime. We couple our BNNP with POS tagging due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter"
N19-1354,P14-2050,0,0.0695435,"Missing"
N19-1354,Q17-1020,0,0.0135469,"s a large amount of labeled data for training, which is costly to obtain, while the lack of data often causes overfitting and poor generalization. Several approaches for parsing in the small data regime have been proposed. These include augmenting input data with pretrained embedding (Dozat et al., 2017; Che et al., 2018), leveraging unannotated data via semi-supervised learning (Corro and Titov, 2018), predicting based on a pool of high probability trees (Niculae et al., 2018; Keith et al., 2018), and transferring annotation or model across languages (Agic et al., 2016; Lacroix et al., 2016; Rasooli and Collins, 2017). Despite the empirical success of these approaches, an inherent problem still holds: The maximum likelihood parameter estimation (MLE) in deep neural networks (DNNs) introduces statistical challenges at both estimation (training), due to the risk of overfitting, and at test time as the model ignores the uncertainty around the estimated parameters. When training data is small these challenges are more pronounced. The Bayesian paradigm provides a statistical framework which addresses both challenges by (i) including prior knowledge to guide the learning in the absence of sufficient data, and (i"
N19-1354,N18-1088,1,0.842192,"ore pronounced. The Bayesian paradigm provides a statistical framework which addresses both challenges by (i) including prior knowledge to guide the learning in the absence of sufficient data, and (ii) predicting under the full posterior distribution of model parameters which offers the desired degree of uncertainty by exploring the posterior space during inference. However, this solution comes with a high computational cost, specifically in DNNs, and is often replaced by regularization techniques such as dropout (Srivastava et al., 2014) as well as ensemble learning and prediction averaging (Liu et al., 2018; Che et al., 2018). Bayesian neural networks (BNNs) have attracted some attention (Welling and Teh, 2011; HernándezLobato and Adams, 2015; Li et al., 2016; Gong et al., 2018). Yet, its current application to NLP is limited to language modeling (Gan et al., 2017), and BNNs have not been developed for structured 3509 Proceedings of NAACL-HLT 2019, pages 3509–3519 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics prediction tasks such as dependency parsing. In this paper we aim to close this gap and propose the first BNN for dependency parsing (BNNP)"
N19-1354,P08-1098,1,0.655231,"ask learning (Caruana, 1997) lends itself as a natural choice for low-resource settings as it aims at leveraging the commonality between tasks to improve their performance in the absence of sufficient amount of training data. This framework hence naturally complements Bayesian modeling in dealing with the challenges of the small data regime. We couple our BNNP with POS tagging due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learni"
N19-1354,D17-1035,0,0.0397089,"treebanks of the CoNLL 2017 shared task on parsing to Universal Dependencies (UD) (Zeman et al., 2017).5 5.1 Experimental Setup We use the UDPipe baseline outputs for segmentation and POS tagging of the raw test data (released along with the raw test data). While segmentation and POS errors substantially impact the quality of the final predicted parse, their exploration is beyond our scope. Our evaluation metric is Labeled Attachment Score (LAS), computed by the shared task evaluation script. Statistical significance, when mentioned, is computed over 20 runs, via the Kolmogorov-Smirnov test (Reimers and Gurevych, 2017; Dror et al., 2018) with ? = 0.01. Mono-Lingual Experiments We experiment with Persian (fa), Korean (ko), Russian (ru), Turkish (tr), Vietnamese (vi) and Irish (ga), all with less than 5? training sentences (Table 1). For comparison we report the scores published by the top system of the CoNLL 2017 shared task, B iAFFINE (Dozat et al., 2017), noting the following differences between their input and output and ours. The BiAFFINE parser: (i) uses the UDPipe outputs for segmentation but corrects POS errors before parsing, (ii) includes both language specific and universal POS tags in the input l"
N19-1354,D10-1001,0,0.0434372,"computed as tanh(? ′′ ?? +?′′ )[?], ? where ? ∈ {?? }?=1 , ? ′′ is (? × ?), and ?′′ is (? × 1). To train the POS tagger, the cross-entropy loss is 1st BiLSTM is Multi-task learning (Caruana, 1997) lends itself as a natural choice for low-resource settings as it aims at leveraging the commonality between tasks to improve their performance in the absence of sufficient amount of training data. This framework hence naturally complements Bayesian modeling in dealing with the challenges of the small data regime. We couple our BNNP with POS tagging due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is t"
N19-1354,P16-2038,0,0.20852,"rk hence naturally complements Bayesian modeling in dealing with the challenges of the small data regime. We couple our BNNP with POS tagging due to the strong connection between the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Giv"
N19-1354,N10-1091,0,0.0172665,"??? = max 0, COST(? , ? )+ SCORE(? )− SCORE(? ) For label prediction, hinge loss with ? = 1 is used (denoted by ??? ). We refer to the parser loss as: ????? = ??? + ??? . Beyond MLE Training The point-estimate of DNN parameters is computationally efficient, but ignores the uncertainty around model parameters during learning. This results in an overconfidence over model predictions during the inference phase. A common generic practice to incorporate a degree of uncertainty is to consider an ensemble of models. Indeed, for dependency parsing ensemble learning has shown to improve accuracy (Surdeanu and Manning, 2010; Kuncoro et al., 2016; Che et al., 2018). However, ensembles are computationally demanding due to the large number of participating models. The de-facto approach to overcome this 2 Stacking BiLSTM s is believed to be helpful. In our case, the addition of a third layer led to overfitting. has been to randomly perturb the structure of the network for each training instance by switching off connections between the nodes, a practice known as dropConnect (Wan et al., 2013), or eliminating the nodes entirely, which is known as dropout (Srivastava et al., 2014). Hinton et al. (2012) demonstrated tha"
N19-1354,P16-1136,0,0.155752,"Missing"
N19-1354,N18-1123,0,0.0195712,"the two tasks (Rush et al., 2010) and the availability of joint training data in several languages (Zeman et al., 2017). While multi-task frameworks have shown success in some areas (Reichart et al., 2008; Finkel and Manning, 2009; Liu et al., 2016; Malca and Reichart, 2018), in our case we found that our two tasks interfered with each other and degraded the parser performance (see similar findings for other tasks at Søgaard and Goldberg (2016); Plank and Alonso (2017)). To minimize task interference, an approach shown effective (Ganin and Lempitsky, 2015; Kim et al., 2017; Chen et al., 2017; ZareMoodi and Haffari, 2018) is to implicitly guide the update signals during training via an adversarial procedure that avoids shared parameters contamination. We adapt this idea to our multi-task learning. … Multi-Task Learning Input Representation Character Emb. s 4 Character BiLSTM i As this solution is not computationally feasible, we use the sampled parameters and follow a procedure that minimizes the Bayes risk (MBR) (Goodman, 1996). Given each sampled parameter, first we generate the maximum scoring parse using the arcfactored decomposition (McDonald et al., 2005a) and dynamic programming (Eisner, 1996). This can"
N19-1354,K18-2001,0,0.0267615,"Missing"
N19-1354,K17-3001,0,0.356075,"⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟ head specialized ?? modifier specialized ?? where ? is a dependency relation type, ? ∈ {?? }??=1 , ? ′ is (? × ℎ), ?′ is (? × 1), and ? ′(.) is (ℎ × ?). The label for each dependency arc (?, ?) is then chosen by a max operation. Dozat et al. (2017) proposed an extension of the BiLSTM parser by replacing the non-linear transformation of ARC-SCORE and LABEL -SCORE with a linear transformation (BiAFFINE). This was further extended by Che et al. (2018) who utilized contextualized word embeddings (Peters et al., 2018). Both extensions showed success in dependency parsing shared tasks (Zeman et al., 2017, 2018). Our Neural Parser Architecture Our network architecture extends the BiLSTM model with an additional BiLSTM layer and input signals. While our 3510 architecture is not the core contribution of this paper, we aim to implement our BNNP on a strong architecture. In §5.3 we demonstrate the contribution of these additions to our final results. In our BNNP, each word is represented as, ?? = ?(?? )◦?(?? )◦?(?  ? )◦?(?? ), where, similar to the BiAFFINE parser, ?(?? ) is a character-level representation of the word ?? , generated by a BiLSTM :  ? (?1∶|? |)◦LSTM  ? (?|? |∶1 ). ?(?? ) = LSTM"
N19-1417,E17-1088,0,0.0358111,"Missing"
N19-1417,P16-1186,0,0.0524637,"Missing"
N19-1417,N18-2085,0,0.0189208,"annot reach the performance peaks of computationally much more expensive neural models, but a 2× decrease in perplexity comes at the cost of 15, 000× longer training time. Our work reveals that recent ?-gram LMs should be used as strong baselines, especially in resource-lean LM data and for morphologically rich languages. Additionally, ?-gram LMs offer a stringent way of dealing with Out-of-Vocabulary (OOVs) and rare words in a full vocabulary setting without relying on any pruning (Heafield, 2013). However, in neural LMs this remains an open question (Kawakami et al., 2017; Kim et al., 2016; Cotterell et al., 2018), while a common practice is pruning the training corpus and imposing closed vocabulary assumption (Mikolov et al., 2010) where rare words at training and unseen words at test are treated as an UNK token. We provide the mathematical underpinnings of ?-gram models and highlight how this popular treatment works in favor of neural LMs (in comparative studies), and enforces ?-gram LMs to perform much worse than their full potential. 2 ?-gram Language Models: Smoothing We now provide an overview of established smoothing techniques for ?-gram LMs and their recent extensions. Smoothing is typically a"
N19-1417,Q18-1032,1,0.913614,"d, a comparison between neural models and more recent developments in ?-gram models is neglected. In this paper, we examine the recent progress in ?-gram literature, running experiments on 50 languages covering all morphological language families. Experimental results illustrate that a simple extension of Modified KneserNey outperforms an LSTM language model on 42 languages while a word-level Bayesian ?gram LM (Shareghi et al., 2017) outperforms the character-aware neural model (Kim et al., 2016) on average across all languages, and its extension which explicitly injects linguistic knowledge (Gerz et al., 2018a) on 8 languages. Further experiments on larger Europarl datasets for 3 languages indicate that neural architectures are able to outperform computationally much cheaper ?-gram models: ?-gram training is up to 15, 000× quicker. Our experiments illustrate that standalone ?-gram models lend themselves as natural choices for resource-lean or morphologically rich languages, while the recent progress has significantly improved their accuracy. 1 Introduction Statistical language models (LMs) are the pivot for several natural language processing tasks where a model trained on a text corpus is require"
N19-1417,D18-1029,1,0.915172,"d, a comparison between neural models and more recent developments in ?-gram models is neglected. In this paper, we examine the recent progress in ?-gram literature, running experiments on 50 languages covering all morphological language families. Experimental results illustrate that a simple extension of Modified KneserNey outperforms an LSTM language model on 42 languages while a word-level Bayesian ?gram LM (Shareghi et al., 2017) outperforms the character-aware neural model (Kim et al., 2016) on average across all languages, and its extension which explicitly injects linguistic knowledge (Gerz et al., 2018a) on 8 languages. Further experiments on larger Europarl datasets for 3 languages indicate that neural architectures are able to outperform computationally much cheaper ?-gram models: ?-gram training is up to 15, 000× quicker. Our experiments illustrate that standalone ?-gram models lend themselves as natural choices for resource-lean or morphologically rich languages, while the recent progress has significantly improved their accuracy. 1 Introduction Statistical language models (LMs) are the pivot for several natural language processing tasks where a model trained on a text corpus is require"
N19-1417,P17-1137,0,0.028832,"arl datasets we find that ?-gram models cannot reach the performance peaks of computationally much more expensive neural models, but a 2× decrease in perplexity comes at the cost of 15, 000× longer training time. Our work reveals that recent ?-gram LMs should be used as strong baselines, especially in resource-lean LM data and for morphologically rich languages. Additionally, ?-gram LMs offer a stringent way of dealing with Out-of-Vocabulary (OOVs) and rare words in a full vocabulary setting without relying on any pruning (Heafield, 2013). However, in neural LMs this remains an open question (Kawakami et al., 2017; Kim et al., 2016; Cotterell et al., 2018), while a common practice is pruning the training corpus and imposing closed vocabulary assumption (Mikolov et al., 2010) where rare words at training and unseen words at test are treated as an UNK token. We provide the mathematical underpinnings of ?-gram models and highlight how this popular treatment works in favor of neural LMs (in comparative studies), and enforces ?-gram LMs to perform much worse than their full potential. 2 ?-gram Language Models: Smoothing We now provide an overview of established smoothing techniques for ?-gram LMs and their"
N19-1417,2005.mtsummit-papers.11,0,0.281847,"Missing"
N19-1417,N18-1192,0,0.0432216,"Missing"
N19-1417,J93-2004,0,0.0648147,"Missing"
N19-1417,D16-1094,1,0.945485,"lity to a given sequence ?1 ?2 ...?? (denoted by ?? ). This probability indicates how likely 1 ? is for ?1 to belong to the corpus and is decomposed into conditional probabilities of words given their ∏ ?−1 preceding contexts as ? (?? )= ? ?=1 ? (?? |?1 ). 1 In ?-gram LMs the unbounded conditional probabilities ? (?? |??−1 ) are approximated by imposing 1 a finite-order Markov assumption, ? (?? |??−1 )≈ 1 ?−1 ? (?? |??−?+1 ). Several smoothing techniques address the statistical sparsity issue for computing the conditional probabilities (Kneser and Ney, 1995; Chen and Goodman, 1999; Teh, 2006; Shareghi et al., 2016a), while others avoided the above approximation with unbounded hierarchical nonparametric Bayesian frameworks (Wood et al., 2011; Shareghi et al., 2017). Alternatively, neural LMs compute ? (?? |??−1 ) 1 via recurrent neural units which, in theory, are capable of encoding an unbounded context ??−1 . In 1 recent years, neural LMs have become the prominent class of language modeling and have established state-of-the-art results on almost all sufficiently large benchmarks (Melis et al., 2018; Yang et al., 2018). While outperforming ?-grams in terms of predictive accuracy, the computational short"
N19-1417,Q16-1034,1,0.886349,"lity to a given sequence ?1 ?2 ...?? (denoted by ?? ). This probability indicates how likely 1 ? is for ?1 to belong to the corpus and is decomposed into conditional probabilities of words given their ∏ ?−1 preceding contexts as ? (?? )= ? ?=1 ? (?? |?1 ). 1 In ?-gram LMs the unbounded conditional probabilities ? (?? |??−1 ) are approximated by imposing 1 a finite-order Markov assumption, ? (?? |??−1 )≈ 1 ?−1 ? (?? |??−?+1 ). Several smoothing techniques address the statistical sparsity issue for computing the conditional probabilities (Kneser and Ney, 1995; Chen and Goodman, 1999; Teh, 2006; Shareghi et al., 2016a), while others avoided the above approximation with unbounded hierarchical nonparametric Bayesian frameworks (Wood et al., 2011; Shareghi et al., 2017). Alternatively, neural LMs compute ? (?? |??−1 ) 1 via recurrent neural units which, in theory, are capable of encoding an unbounded context ??−1 . In 1 recent years, neural LMs have become the prominent class of language modeling and have established state-of-the-art results on almost all sufficiently large benchmarks (Melis et al., 2018; Yang et al., 2018). While outperforming ?-grams in terms of predictive accuracy, the computational short"
N19-1417,Q18-1036,0,0.0215156,"l., 2018). While outperforming ?-grams in terms of predictive accuracy, the computational shortcomings of neural LMs are well-documented: Training neural LMs is computationally expensive to the point that running experiments on large data (≥ a few GiBs) is beyond the reach of academic research to this date (Chen et al., 2016; Patwary et al., 2018; Puri et al., 2018). 1 Similarly, querying is slower for neural LMs due to the required matrix-based operations, whereas most of the widely used ?-gram LM toolkits rely on a few hash lookups and much cheaper scalar-based operations (Liu et al., 2018; Tang and Lin, 2018). Nonetheless, it has been shown that the best predictive performance is still achieved by combining the two models via a basic interpolation or a mixture model (Jozefowicz et al., 2016; Neubig and Dyer, 2016): this indicates that the progress in ?-gram LM should eventually be reflected in improving the 1 For instance, ?-gram LMs could be trained on 32GiB of data on a single CPU with ∼32GiB of RAM in half a day (Shareghi et al., 2016b). A ballpark estimate for neural LMs, based on Puri et al. (2018), requires 26 Tesla V100 16GB GPUs to finish within the same amount of time while its financial"
N19-1417,D16-1124,0,0.0148028,"unning experiments on large data (≥ a few GiBs) is beyond the reach of academic research to this date (Chen et al., 2016; Patwary et al., 2018; Puri et al., 2018). 1 Similarly, querying is slower for neural LMs due to the required matrix-based operations, whereas most of the widely used ?-gram LM toolkits rely on a few hash lookups and much cheaper scalar-based operations (Liu et al., 2018; Tang and Lin, 2018). Nonetheless, it has been shown that the best predictive performance is still achieved by combining the two models via a basic interpolation or a mixture model (Jozefowicz et al., 2016; Neubig and Dyer, 2016): this indicates that the progress in ?-gram LM should eventually be reflected in improving the 1 For instance, ?-gram LMs could be trained on 32GiB of data on a single CPU with ∼32GiB of RAM in half a day (Shareghi et al., 2016b). A ballpark estimate for neural LMs, based on Puri et al. (2018), requires 26 Tesla V100 16GB GPUs to finish within the same amount of time while its financial cost is at least 100× higher. 4113 Proceedings of NAACL-HLT 2019, pages 4113–4118 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics state-of-the-art performance. I"
P03-1007,S01-1005,0,\N,Missing
P03-1007,J87-3002,0,\N,Missing
P03-1007,preiss-etal-2002-subcategorization,1,\N,Missing
P03-1007,S01-1034,0,\N,Missing
P03-1007,A94-1009,0,\N,Missing
P03-1007,C00-2100,0,\N,Missing
P03-1007,C94-1042,0,\N,Missing
P03-1007,W02-0815,1,\N,Missing
P03-1007,A97-1052,0,\N,Missing
P03-1007,J01-3001,0,\N,Missing
P03-1007,briscoe-carroll-2002-robust,0,\N,Missing
P03-1007,W00-0905,0,\N,Missing
P03-1007,W02-2014,1,\N,Missing
P03-1009,P87-1027,0,0.0444546,"dicated by number codes from the classifications of Levin, Dorr (the classes starting with 0) and Korhonen (the classes starting with A).3 The predominant sense is indicated by bold font. 3 Subcategorization Information We obtain our SCF data using the subcategorization acquisition system of Briscoe and Carroll (1997). We expect the use of this system to be beneficial: it employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses, and a comprehensive SCF classifier, which incorporates 163 SCF distinctions, a superset of those found in the ANLT (Boguraev et al., 1987) and COMLEX (Grishman et al., 1994) dictionaries. The SCFs abstract over specific lexicallygoverned particles and prepositions and specific predicate selectional preferences but include some derived semi-predictable bounded dependency constructions, such as particle and dative movement. 78 of these ‘coarse-grained’ SCFs appeared in our data. In addition, a set of 160 fine grained frames were employed. These were obtained by parameterizing two high frequency SCFs for prepositions: the simple PP and NP + PP frames. The scope was restricted to these two frames to prevent sparse data problems in c"
P03-1009,W02-1016,0,0.378744,"Missing"
P03-1009,A97-1052,0,0.847945,"Walde and Brew, 2002). In this paper, we focus on the particular task of classifying subcategorization frame (SCF) distributions in a semantically motivated manner. Previous research has demonstrated that clustering can be useful in inferring Levin-style semantic classes (Levin, 1993) from both English and German verb subcategorization information (Brew and Schulte im Walde, 2002; Schulte im Walde, 2000; Schulte im Walde and Brew, 2002). We propose a novel approach, which involves: (i) obtaining SCF frequency information from a lexicon extracted automatically using the comprehensive system of Briscoe and Carroll (1997) and (ii) applying a clustering mechanism to this information. We use clustering methods that process raw distributional data directly, avoiding complex preprocessing steps required by many advanced methods (e.g. Brew and Schulte im Walde (2002)). In contrast to earlier work, we give special emphasis to polysemy. Earlier work has largely ignored this issue by assuming a single gold standard class for each verb (whether polysemic or not). The relatively good clustering results obtained suggest that many polysemic verbs do have some predominating sense in corpus data. However, this sense can var"
P03-1009,briscoe-carroll-2002-robust,0,0.0197201,"ominant (most frequent) sense in WordNet. The polysemic one provides a comprehensive list of senses for each verb. The test verbs and their classes are shown in table 1. The classes are indicated by number codes from the classifications of Levin, Dorr (the classes starting with 0) and Korhonen (the classes starting with A).3 The predominant sense is indicated by bold font. 3 Subcategorization Information We obtain our SCF data using the subcategorization acquisition system of Briscoe and Carroll (1997). We expect the use of this system to be beneficial: it employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses, and a comprehensive SCF classifier, which incorporates 163 SCF distinctions, a superset of those found in the ANLT (Boguraev et al., 1987) and COMLEX (Grishman et al., 1994) dictionaries. The SCFs abstract over specific lexicallygoverned particles and prepositions and specific predicate selectional preferences but include some derived semi-predictable bounded dependency constructions, such as particle and dative movement. 78 of these ‘coarse-grained’ SCFs appeared in our data. In addition, a set of 160 fine grained frames were employed. These were"
P03-1009,P98-1046,0,0.0733281,"ottleneck and nearest neighbour methods. In contrast to previous work, we particularly focus on clustering polysemic verbs. A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data. 1 Introduction Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jackendoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)). While such classifications may not provide a means for full semantic inferencing, they can capture generalizations over a range of linguistic properties, and can therefore be used as a means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. ∗ This work was partly supported by UK EPSRC project GR/N36462/93: ‘Robust Accurate Statistical Parsing (RASP)’. Zvika Marx Interdisciplinary Center for Neural Computation, The Hebrew University Jerusalem, Israel zvim@cs.huji.ac.il Verb classifications have, in fact, been used to supp"
P03-1009,C96-1055,0,0.149849,"erties, and can therefore be used as a means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. ∗ This work was partly supported by UK EPSRC project GR/N36462/93: ‘Robust Accurate Statistical Parsing (RASP)’. Zvika Marx Interdisciplinary Center for Neural Computation, The Hebrew University Jerusalem, Israel zvim@cs.huji.ac.il Verb classifications have, in fact, been used to support many natural language processing (NLP) tasks, such as language generation, machine translation (Dorr, 1997), document classification (Klavans and Kan, 1998), word sense disambiguation (Dorr and Jones, 1996) and subcategorization acquisition (Korhonen, 2002). One attractive property of these classifications is that they make it possible, to a certain extent, to infer the semantics of a verb on the basis of its syntactic behaviour. In recent years several attempts have been made to automatically induce semantic verb classes from (mainly) syntactic information in corpus data (Joanis, 2002; Merlo et al., 2002; Schulte im Walde and Brew, 2002). In this paper, we focus on the particular task of classifying subcategorization frame (SCF) distributions in a semantically motivated manner. Previous researc"
P03-1009,C94-1042,0,0.12671,"assifications of Levin, Dorr (the classes starting with 0) and Korhonen (the classes starting with A).3 The predominant sense is indicated by bold font. 3 Subcategorization Information We obtain our SCF data using the subcategorization acquisition system of Briscoe and Carroll (1997). We expect the use of this system to be beneficial: it employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses, and a comprehensive SCF classifier, which incorporates 163 SCF distinctions, a superset of those found in the ANLT (Boguraev et al., 1987) and COMLEX (Grishman et al., 1994) dictionaries. The SCFs abstract over specific lexicallygoverned particles and prepositions and specific predicate selectional preferences but include some derived semi-predictable bounded dependency constructions, such as particle and dative movement. 78 of these ‘coarse-grained’ SCFs appeared in our data. In addition, a set of 160 fine grained frames were employed. These were obtained by parameterizing two high frequency SCFs for prepositions: the simple PP and NP + PP frames. The scope was restricted to these two frames to prevent sparse data problems in clustering. A SCF lexicon was acquir"
P03-1009,P98-1112,0,0.104513,"ure generalizations over a range of linguistic properties, and can therefore be used as a means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. ∗ This work was partly supported by UK EPSRC project GR/N36462/93: ‘Robust Accurate Statistical Parsing (RASP)’. Zvika Marx Interdisciplinary Center for Neural Computation, The Hebrew University Jerusalem, Israel zvim@cs.huji.ac.il Verb classifications have, in fact, been used to support many natural language processing (NLP) tasks, such as language generation, machine translation (Dorr, 1997), document classification (Klavans and Kan, 1998), word sense disambiguation (Dorr and Jones, 1996) and subcategorization acquisition (Korhonen, 2002). One attractive property of these classifications is that they make it possible, to a certain extent, to infer the semantics of a verb on the basis of its syntactic behaviour. In recent years several attempts have been made to automatically induce semantic verb classes from (mainly) syntactic information in corpus data (Joanis, 2002; Merlo et al., 2002; Schulte im Walde and Brew, 2002). In this paper, we focus on the particular task of classifying subcategorization frame (SCF) distributions in"
P03-1009,J01-3003,0,0.359533,"methods. In contrast to previous work, we particularly focus on clustering polysemic verbs. A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data. 1 Introduction Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jackendoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)). While such classifications may not provide a means for full semantic inferencing, they can capture generalizations over a range of linguistic properties, and can therefore be used as a means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. ∗ This work was partly supported by UK EPSRC project GR/N36462/93: ‘Robust Accurate Statistical Parsing (RASP)’. Zvika Marx Interdisciplinary Center for Neural Computation, The Hebrew University Jerusalem, Israel zvim@cs.huji.ac.il Verb classifications have, in fact, been used to support many natural language processing (NL"
P03-1009,P02-1027,0,0.0258664,"pport many natural language processing (NLP) tasks, such as language generation, machine translation (Dorr, 1997), document classification (Klavans and Kan, 1998), word sense disambiguation (Dorr and Jones, 1996) and subcategorization acquisition (Korhonen, 2002). One attractive property of these classifications is that they make it possible, to a certain extent, to infer the semantics of a verb on the basis of its syntactic behaviour. In recent years several attempts have been made to automatically induce semantic verb classes from (mainly) syntactic information in corpus data (Joanis, 2002; Merlo et al., 2002; Schulte im Walde and Brew, 2002). In this paper, we focus on the particular task of classifying subcategorization frame (SCF) distributions in a semantically motivated manner. Previous research has demonstrated that clustering can be useful in inferring Levin-style semantic classes (Levin, 1993) from both English and German verb subcategorization information (Brew and Schulte im Walde, 2002; Schulte im Walde, 2000; Schulte im Walde and Brew, 2002). We propose a novel approach, which involves: (i) obtaining SCF frequency information from a lexicon extracted automatically using the comprehensi"
P03-1009,W02-0815,1,0.815398,"Schulte im Walde (2002)). In contrast to earlier work, we give special emphasis to polysemy. Earlier work has largely ignored this issue by assuming a single gold standard class for each verb (whether polysemic or not). The relatively good clustering results obtained suggest that many polysemic verbs do have some predominating sense in corpus data. However, this sense can vary across corpora (Roland et al., 2000), and assuming a single sense is inadequate for an important group of medium and high frequency verbs whose distribution of senses in balanced corpus data is flat rather than zipfian (Preiss and Korhonen, 2002). To allow for sense variation, we introduce a new evaluation scheme against a polysemic gold standard. This helps to explain the results and offers a better insight into the potential and limitations of clustering undisambiguated SCF data semantically. We discuss our gold standards and the choice of test verbs in section 2. Section 3 describes the method for subcategorization acquisition and section 4 presents the approach to clustering. Details of the experimental evaluation are supplied in section 5. Section 6 concludes with directions for future work. 2 Semantic Verb Classes and Test Verbs"
P03-1009,W00-0905,0,0.0245098,"a clustering mechanism to this information. We use clustering methods that process raw distributional data directly, avoiding complex preprocessing steps required by many advanced methods (e.g. Brew and Schulte im Walde (2002)). In contrast to earlier work, we give special emphasis to polysemy. Earlier work has largely ignored this issue by assuming a single gold standard class for each verb (whether polysemic or not). The relatively good clustering results obtained suggest that many polysemic verbs do have some predominating sense in corpus data. However, this sense can vary across corpora (Roland et al., 2000), and assuming a single sense is inadequate for an important group of medium and high frequency verbs whose distribution of senses in balanced corpus data is flat rather than zipfian (Preiss and Korhonen, 2002). To allow for sense variation, we introduce a new evaluation scheme against a polysemic gold standard. This helps to explain the results and offers a better insight into the potential and limitations of clustering undisambiguated SCF data semantically. We discuss our gold standards and the choice of test verbs in section 2. Section 3 describes the method for subcategorization acquisitio"
P03-1009,P02-1029,0,0.387792,"cessing (NLP) tasks, such as language generation, machine translation (Dorr, 1997), document classification (Klavans and Kan, 1998), word sense disambiguation (Dorr and Jones, 1996) and subcategorization acquisition (Korhonen, 2002). One attractive property of these classifications is that they make it possible, to a certain extent, to infer the semantics of a verb on the basis of its syntactic behaviour. In recent years several attempts have been made to automatically induce semantic verb classes from (mainly) syntactic information in corpus data (Joanis, 2002; Merlo et al., 2002; Schulte im Walde and Brew, 2002). In this paper, we focus on the particular task of classifying subcategorization frame (SCF) distributions in a semantically motivated manner. Previous research has demonstrated that clustering can be useful in inferring Levin-style semantic classes (Levin, 1993) from both English and German verb subcategorization information (Brew and Schulte im Walde, 2002; Schulte im Walde, 2000; Schulte im Walde and Brew, 2002). We propose a novel approach, which involves: (i) obtaining SCF frequency information from a lexicon extracted automatically using the comprehensive system of Briscoe and Carroll ("
P03-1009,C00-2108,0,0.397949,"ic behaviour. In recent years several attempts have been made to automatically induce semantic verb classes from (mainly) syntactic information in corpus data (Joanis, 2002; Merlo et al., 2002; Schulte im Walde and Brew, 2002). In this paper, we focus on the particular task of classifying subcategorization frame (SCF) distributions in a semantically motivated manner. Previous research has demonstrated that clustering can be useful in inferring Levin-style semantic classes (Levin, 1993) from both English and German verb subcategorization information (Brew and Schulte im Walde, 2002; Schulte im Walde, 2000; Schulte im Walde and Brew, 2002). We propose a novel approach, which involves: (i) obtaining SCF frequency information from a lexicon extracted automatically using the comprehensive system of Briscoe and Carroll (1997) and (ii) applying a clustering mechanism to this information. We use clustering methods that process raw distributional data directly, avoiding complex preprocessing steps required by many advanced methods (e.g. Brew and Schulte im Walde (2002)). In contrast to earlier work, we give special emphasis to polysemy. Earlier work has largely ignored this issue by assuming a single"
P03-1009,W03-0410,0,0.0852586,"ons. 8 This yielded better results, which might indicate that the unfiltered “noisy” SCFs contain information which is valuable for the task. 5.2 Alg. Method = 1 K K P num. of correct pairs in ki · num. of pairs in ki i=1 |ki |−1 |ki |+1 . is the average proportion of all within-cluster pairs that are correctly co-assigned. It is multiplied by a factor that increases with cluster size. This factor compensates for a bias towards small clusters. Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003). We associate with each cluster its most prevalent semantic class, and denote the number of verbs in a cluster K that take its prevalent class by nprevalent (K). Verbs that do not take this class are considered as errors. Given our task, we are only interested in classes which contain two or more verbs. We therefore disregard those clusters where nprevalent (K) = 1. This leads us to define modified purity: APP P mPUR = nprevalent (ki ) nprevalent (ki )≥2 number of verbs . The modification we introduce to purity removes the bias towards the trivial configuration comprised of only singletons. 5"
P03-1009,C98-1046,0,\N,Missing
P03-1009,C98-1108,0,\N,Missing
P05-1076,P87-1027,1,0.735614,"can do it” “such a young man”; “so young a man” “the younger of them”; “the young” “he died young” “Young, he was plain in appearance” “When young, he was lonely” Figure 1: Fundamental adjectival frames ing such features as the mood of the complement (mandative, interrogative, etc.), preferences for particular prepositions and whether the subject is extraposed. Even ignoring preposition preference, there are more than 30 distinguishable adjectival SCFs. Some fairly extensive frame sets can be found in large syntax dictionaries, such as COMLEX (31 SCFs) (Wolff et al., 1998) and ANLT (24 SCFs) (Boguraev et al., 1987). While such resources are generally accurate, they are disappointingly incomplete: none of the proposed frame sets in the well-known resources subsumes the others, the coverage of SCF types for individual adjectives is low, and (accurate) information on the relative frequency of SCFs for each adjective is absent. The inadequacy of manually-created dictionaries and the difficulty of adequately enhancing and maintaining the information by hand was a central motivation for early research into automatic subcategorization acquisition. The focus heretofore has remained firmly on verb subcategorizat"
P05-1076,P91-1027,0,0.259506,"is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998b), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has been considerable research into acquisition of verb subcategorization, we are not aware of any systems built for adjectives. Although adjectives are syntactically less mul"
P05-1076,A97-1052,1,0.888084,"evelopment of successful parsing technology (e.g. (Carroll et al., 1998b), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has been considerable research into acquisition of verb subcategorization, we are not aware of any systems built for adjectives. Although adjectives are syntactically less multivalent than verbs, and although verb subcategorization distribution data appears to offer the greatest potential boost"
P05-1076,briscoe-carroll-2002-robust,1,0.848006,"c Ann Arbor, June 2005. 2005 Association for Computational Linguistics (from tagging to syntactic and semantic analysis). Automatic SCF acquisition techniques are particularly important for adjectives because extant syntax dictionaries provide very limited coverage of adjective subcategorization. In this paper we propose a method for automatic acquisition of adjectival SCFs from English corpus data. Our method has been implemented using a decision-tree classifier which tests for the presence of grammatical relations (GRs) in the output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe and Carroll, 2002). It uses a powerful taskspecific pattern-matching language which enables the frames to be classified hierarchically in a way that mirrors inheritance-based lexica. As reported later, the system is capable of detecting 30 SCFs with an accuracy comparable to that of best state-ofart verbal SCF acquisition systems (e.g. (Korhonen, 2002)). Additionally, we present a novel tool for linguistic annotation of SCFs in corpus data aimed at alleviating the process of obtaining training and test data for subcategorization acquisition. The tool incorporates an intuitive interface with the ability to signi"
P05-1076,C02-1013,1,0.909127,"Missing"
P05-1076,W98-1505,0,0.0759918,"exicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998b), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has been considerable research into acquisition of verb subcategorization, we are not aware of any systems built for adjectives. Although adjectives are syntactically less multivalent than verbs, and although verb subcategorization distribution data appears to offer th"
P05-1076,W98-1114,1,0.955193,"ed to genres and sublanguages. Such resources are critical for natural language processing (NLP), both for enhancing the performance of ∗ Part of this research was conducted while this author was at the University of Edinburgh Laboratory for Foundations of Computer Science. state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998b), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting compreh"
P05-1076,P04-2007,0,0.0223169,"ion (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has been considerable research into acquisition of verb subcategorization, we are not aware of any systems built for adjectives. Although adjectives are syntactically less multivalent than verbs, and although verb subcategorization distribution data appears to offer the greatest potential boost in parser performance, accurate and comprehensive knowledge of the many adjective SCFs can improve the accuracy of parsing at several levels 614 Proceedings of the 43rd Annual Meeting of the ACL, pages 614–621, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics (from t"
P05-1076,J92-3002,0,0.0440088,"some cases result in an inconsistent or misleading combination of GRs. The parser uses a scheme of GRs between lemmatised lexical heads (Carroll et al., 1998a; Briscoe et al., 2002). The relations are organized as a multipleinheritance subsumption hierarchy where each subrelation extends the meaning, and perhaps the argument structure, of its parents (figure 2). For descriptions and examples of each relation, see (Carroll et al., 1998a). The dependency relationships which the GRs embody correspond closely to the head-complement 1 Compare the cogent argument for a inheritance-based lexicon in (Flickinger and Nerbonne, 1992), much of which can be applied unchanged to the taxonomy of SCFs. 616 mod arg mod ncmod xmod cmod detmod arg aux subj or dobj subj ncsubj conj xsubj csubj comp obj dobj obj2 clausal iobj xcomp ccomp Figure 2: The GR hierarchy used by RASP 2 Obtaining Grammatical Relations In contrast to almost all earlier work, there was no filtering stage involved in SCF acquisition. The classifier was designed to operate with high precision, so filtering was less necessary. 2 SUBJECT NP 1 , 6 6 6 * "" 6 PVAL 6 6ADJ-COMPS 4 NP PP Figure 3: “for” 3 # Feature , 2 MOOD 6 6SUBJECT 4 OMISSION VP 3 3 7 7 to-infiniti"
P05-1076,C02-1122,0,0.0309122,"ture (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has been considerable research into acquisition of verb subcategorization, we are not aware of any systems built for adjectives. Although adjectives are syntactically less multivalent than verbs, and although verb subcategorization distribution data appears to offer the greatest potential boost in parser performance, accurate and comprehensive knowledge of the many adjective SCFs can improve the accuracy of parsing at several levels 614 Proceedings of the 43rd Annual Meeting of the ACL, pages 614–621, c Ann Arbor, June 2005. 2005 Association for Computational Ling"
P05-1076,P03-1009,1,0.870014,"ny application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has been considerable research into acquisition of verb subcategorization, we are not aware of any systems built for adjectives. Although adjectives are syntactically less multivalent than verbs, and although verb subcategorization distribution data appears to offer the greatest potential boost in parser performance, accurate and comprehensive knowledge of the many adjective SCFs can improve the accuracy of parsing at several levels 614 Proceedings of the 43rd Annual Meeting of t"
P05-1076,P02-1029,0,0.061693,"h for enhancing the performance of ∗ Part of this research was conducted while this author was at the University of Edinburgh Laboratory for Foundations of Computer Science. state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998b), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carro"
P05-1076,P03-1002,0,0.0585437,"puter Science. state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998b), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) (Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998b; Korhonen et al., 2003)), besides systems for a number of other languages (e.g. (Kawahara and Kurohashi, 2002; Ferrer, 2004)). While there has"
P05-1076,H91-1067,0,\N,Missing
P05-1076,P93-1032,0,\N,Missing
P06-1044,P03-1009,1,0.901237,"beling, and subcategorization acquisition (Dorr, 1997; Prescher et al., 2000; Korhonen, 2002). However, large-scale exploitation of the classes in real-world or domain-sensitive tasks has not been possible because the existing classifications, e.g. (Levin, 1993), are incomprehensive and unsuitable for specific domains. While manual classification of large numbers of words has proved difficult and time-consuming, recent research shows that it is possible to automatically induce lexical classes from corpus data with promising accuracy (Merlo and Stevenson, 2001; Brew and Schulte im Walde, 2002; Korhonen et al., 2003). A number of ML methods have been applied to classify words using features pertaining to mainly syntactic structure (e.g. statistical distributions of subcategorization frames (SCFs) or general patterns of syntactic behaviour, e.g. transitivity, passivisability) which have been extracted from corpora using e.g. part-of-speech tagging or robust statistical parsing techniques. Lexical classes, when tailored to the application and domain in question, can provide an effective means to deal with a number of natural language processing (NLP) tasks. While manual construction of such classes is diffi"
P06-1044,I05-1006,0,0.0156539,"een proteins and cell types from biomedical texts (Hirschman et al., 2002). Other tasks, such as the extraction of factual information, remain a bigger challenge. This is partly due to the challenging nature of biomedical texts. They are complex both in terms of syntax and semantics, containing complex nominals, modal subordination, anaphoric links, etc. Researchers have recently began to use deeper NLP techniques (e.g. statistical parsing) in the domain because they are not challenged by the complex structures to the same extent than shallow techniques (e.g. regular expression patterns) are (Lease and Charniak, 2005). However, deeper techniques require richer domain-specific lexical information for optimal performance than is provided by existing lexicons (e.g. UMLS). This is particularly important for verbs, which are central to the structure and meaning of sentences. Where the lexical information is absent, lexical classes can compensate for it or aid in obtaining it in the ways described in section 1. Consider e.g. the INDICATE and ACTIVATE verb classes in Figure 1. They capture the fact that their members are similar in terms of syntax and semantics: they have similar SCFs and selectional preferences,"
P06-1044,J01-3003,0,0.0761921,"ine translation, word sense disambiguation, semantic role labeling, and subcategorization acquisition (Dorr, 1997; Prescher et al., 2000; Korhonen, 2002). However, large-scale exploitation of the classes in real-world or domain-sensitive tasks has not been possible because the existing classifications, e.g. (Levin, 1993), are incomprehensive and unsuitable for specific domains. While manual classification of large numbers of words has proved difficult and time-consuming, recent research shows that it is possible to automatically induce lexical classes from corpus data with promising accuracy (Merlo and Stevenson, 2001; Brew and Schulte im Walde, 2002; Korhonen et al., 2003). A number of ML methods have been applied to classify words using features pertaining to mainly syntactic structure (e.g. statistical distributions of subcategorization frames (SCFs) or general patterns of syntactic behaviour, e.g. transitivity, passivisability) which have been extracted from corpora using e.g. part-of-speech tagging or robust statistical parsing techniques. Lexical classes, when tailored to the application and domain in question, can provide an effective means to deal with a number of natural language processing (NLP)"
P06-1044,W02-1016,0,0.385559,"Missing"
P06-1044,A97-1052,0,0.150056,"e calculated the Spearman rank correlation between the 1165 verbs which occurred in both corpora. The result was only a weak correlation: 0.37 ± 0.03. When the scope was restricted to the 100 most frequent verbs in the biomedical data, the correlation was 0.12 ± 0.10 which is only 1.2σ away from zero. The dissimilarity between the distributions is further indicated by the KullbackLeibler distance of 0.97. Table 1 illustrates some of these big differences by showing the list of 15 most frequent verbs in the two corpora. 3 pus data using the comprehensive subcategorization acquisition system of Briscoe and Carroll (1997) (Korhonen, 2002). The system incorporates RASP, a domain-independent robust statistical parser (Briscoe and Carroll, 2002), which tags, lemmatizes and parses data yielding complete though shallow parses and a SCF classifier which incorporates an extensive inventory of 163 verbal SCFs3 . The SCFs abstract over specific lexically-governed particles and prepositions and specific predicate selectional preferences. In our work, we parameterized two high frequency SCFs for prepositions (PP and NP + PP SCFs). No filtering of potentially noisy SCFs was done to provide clustering with as much informat"
P06-1044,C00-2094,0,0.407499,"level abstractions they can be used as a means to abstract away from individual words when required. They are also helpful in many operational contexts where lexical information must be acquired from small application-specific corpora. Their predictive power can help compensate for lack of data fully exemplifying the behavior of relevant words. Lexical verb classes have been used to support various (multilingual) tasks, such as computational lexicography, language generation, machine translation, word sense disambiguation, semantic role labeling, and subcategorization acquisition (Dorr, 1997; Prescher et al., 2000; Korhonen, 2002). However, large-scale exploitation of the classes in real-world or domain-sensitive tasks has not been possible because the existing classifications, e.g. (Levin, 1993), are incomprehensive and unsuitable for specific domains. While manual classification of large numbers of words has proved difficult and time-consuming, recent research shows that it is possible to automatically induce lexical classes from corpus data with promising accuracy (Merlo and Stevenson, 2001; Brew and Schulte im Walde, 2002; Korhonen et al., 2003). A number of ML methods have been applied to classify"
P06-1044,briscoe-carroll-2002-robust,0,0.13216,"k correlation: 0.37 ± 0.03. When the scope was restricted to the 100 most frequent verbs in the biomedical data, the correlation was 0.12 ± 0.10 which is only 1.2σ away from zero. The dissimilarity between the distributions is further indicated by the KullbackLeibler distance of 0.97. Table 1 illustrates some of these big differences by showing the list of 15 most frequent verbs in the two corpora. 3 pus data using the comprehensive subcategorization acquisition system of Briscoe and Carroll (1997) (Korhonen, 2002). The system incorporates RASP, a domain-independent robust statistical parser (Briscoe and Carroll, 2002), which tags, lemmatizes and parses data yielding complete though shallow parses and a SCF classifier which incorporates an extensive inventory of 163 verbal SCFs3 . The SCFs abstract over specific lexically-governed particles and prepositions and specific predicate selectional preferences. In our work, we parameterized two high frequency SCFs for prepositions (PP and NP + PP SCFs). No filtering of potentially noisy SCFs was done to provide clustering with as much information as possible. Approach We extended the system of Korhonen et al. (2003) with additional clustering techniques (introduce"
P07-1115,P87-1027,1,0.69118,"gure 3. Each instantiated GR in Figure 3 corresponds to one or more parts of the feature structure in Figure 2. xcomp( be[6] easy[8]) establishes be[6] as the head of the VP in which easy[8] occurs as a complement. The first (PP)-complement is for us, as indicated by ncmod(for[9] easy[8] we+[10]), with for as PFORM and we+ (us) as NP. The second complement is represented by xcomp(to[11] be+[6] comprehend[12]): a to-infinitive VP. The 914 SCF Frames The SCFs recognized by the classifier were obtained by manually merging the frames exemplified in the COMLEX Syntax (Grishman et al., 1994), ANLT (Boguraev et al., 1987) and/or NOMLEX (Macleod et al., 1997) dictionaries and including additional frames found by manual inspection of unclassifiable examples during development of the classifier. These consisted of e.g. some occurrences of phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some non-passivizable words with a surface direct object, and some rarer combinations of governed preposition and complementizer combinations. The frames were created so that they abstract over specific lexically-governed particles and prepositions and specific predicate selectiona"
P07-1115,P91-1027,0,0.149439,"orization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has foc"
P07-1115,A97-1052,1,0.926335,"f successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and although verb SCF distribution dat"
P07-1115,briscoe-carroll-2002-robust,1,0.485691,"coe and Carroll, 1997)) is to extract SCFs from parse trees, introducing an unnecessary dependence on the details of a particular parser. In our approach SCFs are extracted from GR s — representations of head-dependent relations which are more parser/grammar independent but at the appropriate level of abstraction for extraction of SCF s. A similar approach was recently motivated and explored by Yallop et al. (2005). A decision-tree classifier was developed for 30 adjectival SCF types which tests for the presence of GRs in the GR output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe and Carroll, 2002). The results reported with 9 test adjectives were promising (68.9 F-measure in detecting SCF types). Our acquisition process consists of four main steps: 1) extracting GRs from corpus data, 2) feeding the GR sets as input to a rule-based classifier which incrementally matches them with the corresponding SCF s, 3) building lexical entries from the classified data, and 4) filtering those entries to obtain a more accurate lexicon. The details of these steps are provided in the subsequent sections. b) Kim (VP persuaded (NP the judge) (Scomp that Sandy was present)) However, both a) and b) consist"
P07-1115,P06-4020,1,0.215678,"N one | judge). In this paper we present the first system for largescale acquisition of SCFs from English corpus data which can be used to acquire comprehensive lexicons for verbs, nouns and adjectives. The classifier incorporates 168 verbal, 37 adjectival and 31 nominal SCF distinctions. An improved acquisition technique is used which expands on the ideas Yallop et al. (2005) recently explored for a small experiment on adjectival SCF acquisition. It involves identifying SCF s on the basis of grammatical relations ( GR s) in the output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe et al., 2006). As detailed later, the system performs better with verbs than previous comparable state-of-art systems, achieving 68.9 F-measure in detecting SCF types. It achieves similarly good performance with nouns and adjectives (62.2 and 71.9 F-measure, respectively). Additionally, we have developed a tool for linguistic annotation of SCFs in corpus data aimed at alleviating the process of obtaining training and test data for subcategorization acquisition. The tool incorporates an intuitive interface with the ability to significantly reduce the number of frames presented to the user for each sentence."
P07-1115,W98-1505,0,0.06437,"tal for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and althoug"
P07-1115,W98-1114,1,0.889783,"eb, corpora of published text, etc.) is starting to produce large scale lexical resources which include frequency and usage information tuned to genres and sublanguages. Such resources are critical for natural language processing (NLP), both for enhancing the performance of state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detect"
P07-1115,C94-1042,0,0.187709,"r this sentence is shown in Figure 3. Each instantiated GR in Figure 3 corresponds to one or more parts of the feature structure in Figure 2. xcomp( be[6] easy[8]) establishes be[6] as the head of the VP in which easy[8] occurs as a complement. The first (PP)-complement is for us, as indicated by ncmod(for[9] easy[8] we+[10]), with for as PFORM and we+ (us) as NP. The second complement is represented by xcomp(to[11] be+[6] comprehend[12]): a to-infinitive VP. The 914 SCF Frames The SCFs recognized by the classifier were obtained by manually merging the frames exemplified in the COMLEX Syntax (Grishman et al., 1994), ANLT (Boguraev et al., 1987) and/or NOMLEX (Macleod et al., 1997) dictionaries and including additional frames found by manual inspection of unclassifiable examples during development of the classifier. These consisted of e.g. some occurrences of phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some non-passivizable words with a surface direct object, and some rarer combinations of governed preposition and complementizer combinations. The frames were created so that they abstract over specific lexically-governed particles and prepositions and"
P07-1115,W02-2014,1,0.876333,"type recall (the percentage of SCF types in the gold standard that the system proposes) and the F-measure which is the harmonic mean of type precision and recall. We also compared the similarity between the acquired unfiltered3 SCF distributions and gold standard SCF distributions using various measures of distributional similarity: the Spearman rank correlation (RC), Kullback-Leibler distance (KL), JensenShannon divergence (JS), cross entropy (CE), skew divergence (SD) and intersection (IS). The details of these measures and their application to subcategorization acquisition can be found in (Korhonen and Krymolowski, 2002). Finally, we recorded the total number of gold standard SCFs unseen in the system output, i.e. the type of false negatives which were never detected by the classifier. 3.4 Results Table 1 includes the average results for the 183 verbs. The first column shows the results for Briscoe and Carroll’s (1997) (B&C) system when this system is run with the original classifier but a more recent version of the parser (Briscoe and Carroll, 2002) and the same filtering technique as our new system (thresholding based on the relative frequencies of SCFs). The classifier of B&C system is comparable to our cl"
P07-1115,P03-1009,1,0.719202,"on which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and although verb SCF distribution data is likely to offer the greatest boost in parser performance, accurate and comprehensive knowledge of the many noun and adjective SCFs in English could improve the accuracy of parsing at"
P07-1115,korhonen-etal-2006-large,1,0.894004,"rization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which contains frame and frequency information for over 6,300 English verbs – the VALEX lexicon (Korhonen et al., 2006). While there has been considerable work in the area, most of it has focussed on verbs. Although verbs are the richest words in terms of subcategorization and although verb SCF distribution data is likely to offer the greatest boost in parser performance, accurate and comprehensive knowledge of the many noun and adjective SCFs in English could improve the accuracy of parsing at several levels (from tagging to syntactic and semantic analysis). Furthermore the selection of the correct analysis from the set returned by a parser which does not initially utilize fine-grained lexico-syntactic inform"
P07-1115,P02-1029,0,0.0669307,"ude frequency and usage information tuned to genres and sublanguages. Such resources are critical for natural language processing (NLP), both for enhancing the performance of state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks"
P07-1115,P03-1002,0,0.0578166,"performance of state-of-art statistical systems and for improving the portability of these systems between domains. One type of lexical information with particular importance for NLP is subcategorization. Access to an accurate and comprehensive subcategorization lexicon is vital for the development of successful parsing technology (e.g. (Carroll et al., 1998), important for many NLP tasks (e.g. automatic verb classification (Schulte im Walde and Brew, 2002)) and useful for any application which can benefit from information about predicate-argument structure (e.g. Information Extraction (IE) ((Surdeanu et al., 2003)). The first systems capable of automatically learning a small number of verbal subcategorization frames (SCFs) from unannotated English corpora emerged over a decade ago (Brent, 1991; Manning, 1993). Subsequent research has yielded systems for English (Carroll and Rooth, 1998; Briscoe and Carroll, 1997; Korhonen, 2002) capable of detecting comprehensive sets of SCFs with promising accuracy and demonstrated success in application tasks (e.g. (Carroll et al., 1998; Korhonen et al., 2003)). Recently, a large publicly available subcategorization lexicon was produced using such technology which co"
P07-1115,P05-1076,1,0.432878,"our results and future work, and section 5 concludes. a) Kim (VP believes (NP the evidence (Scomp that Sandy was present))) A common strategy in existing large-scale SCF acquisition systems (e.g. (Briscoe and Carroll, 1997)) is to extract SCFs from parse trees, introducing an unnecessary dependence on the details of a particular parser. In our approach SCFs are extracted from GR s — representations of head-dependent relations which are more parser/grammar independent but at the appropriate level of abstraction for extraction of SCF s. A similar approach was recently motivated and explored by Yallop et al. (2005). A decision-tree classifier was developed for 30 adjectival SCF types which tests for the presence of GRs in the GR output of the RASP (Robust Accurate Statistical Parsing) system (Briscoe and Carroll, 2002). The results reported with 9 test adjectives were promising (68.9 F-measure in detecting SCF types). Our acquisition process consists of four main steps: 1) extracting GRs from corpus data, 2) feeding the GR sets as input to a rule-based classifier which incrementally matches them with the corresponding SCF s, 3) building lexical entries from the classified data, and 4) filtering those en"
P07-1115,H91-1067,0,\N,Missing
P07-1115,P93-1032,0,\N,Missing
P12-1044,P10-1024,0,0.0769505,"nsupervised POS tagging (Finkel et al., 2007), and sampling methods allow efficient estimation of full joint distributions (Neal, 1993). The potential for joint inference of complementary information, such as syntactic verb and semantic argument classes, has a clear and interpretable way forward, in contrast to the pipelined methods described above. This was demonstrated in Andrew et al. (2004), where a Bayesian model was used to jointly induce syntactic and semantic classes for verbs, although that study relied on manually annotated data and a predefined SCF inventory and MLE. More recently, Abend and Rappoport (2010) trained ensemble classifiers to perform argumentadjunct disambiguation of PP complements, a task closely related to SCF acquisition. Their study employed unsupervised POS tagging and parsing, and measures of selectional preference and argument structure as complementary features for the classifier. 422 Finally, our task-based evaluation, verb clustering with Levin (1993)’s alternation classes as the gold standard, was previously conducted by Joanis and Stevenson (2003), Korhonen et al. (2008) and Sun and Korhonen (2009). 3 Methodology In this section we describe the basic components of our st"
P12-1044,W04-3220,0,0.0284521,"contrast, does not use a predefined SCF inventory, and can perform well without parsed input. Graphical models have been increasingly popular for a variety of tasks such as distributional semantics (Blei et al., 2003) and unsupervised POS tagging (Finkel et al., 2007), and sampling methods allow efficient estimation of full joint distributions (Neal, 1993). The potential for joint inference of complementary information, such as syntactic verb and semantic argument classes, has a clear and interpretable way forward, in contrast to the pipelined methods described above. This was demonstrated in Andrew et al. (2004), where a Bayesian model was used to jointly induce syntactic and semantic classes for verbs, although that study relied on manually annotated data and a predefined SCF inventory and MLE. More recently, Abend and Rappoport (2010) trained ensemble classifiers to perform argumentadjunct disambiguation of PP complements, a task closely related to SCF acquisition. Their study employed unsupervised POS tagging and parsing, and measures of selectional preference and argument structure as complementary features for the classifier. 422 Finally, our task-based evaluation, verb clustering with Levin (19"
P12-1044,P98-1013,0,0.213669,"ice for SCF acquisition. There are several SCF lexicons for general language, such as ANLT (Boguraev and Briscoe, 1987) and COMLEX (Grishman et al., 1994), that depend on manual work. VALEX (Preiss et al., 2007) provides SCF distributions for 6,397 verbs acquired from a parsed general language corpus via a system that relies on hand-crafted rules. There are also resources which provide information about both syntactic and semantic properties of verbs: VerbNet (Kipper et al., 2008) draws on several hand-built and semi-automatic sources to link the syntax and semantics of 5,726 verbs. FrameNet (Baker et al., 1998) provides semantic frames and annotated example sentences for 4,186 verbs. PropBank (Palmer et al., 2005) is a corpus where each verb is annotated for its arguments and their semantic roles, covering a total of 4,592 verbs. There are many language-specific SCF acquisition systems, e.g. for French (Messiant, 2008), Italian (Lenci et al., 2008), Turkish (Han et al., 2008) and Chinese (Han et al., 2008). These typically rely on language-specific knowledge, either directly through heuristics, or indirectly through parsing models trained on treebanks. Furthermore, some require labeled training inst"
P12-1044,P06-4020,0,0.561564,"-of-the-art performance without relying on error-prone unlexicalized or domain-specific lexicalized parsers. Third, we highlight a key advantage of our method compared to previous approaches: the ease of integrating and performing joint inference of additional syntactic and semantic information. We describe how we plan to exploit this in our future research. 421 2 Previous work Many state-of-the-art SCF acquisition systems take grammatical relations (GRs) as input. GRs express binary dependencies between lexical items, and many parsers produce them as output, with some variation in inventory (Briscoe et al., 2006; De Marneffe et al., 2006). For example, a subjectrelation like “ncsubj(HEAD, DEPENDENT)” expresses the fact that the lexical item referred to by HEAD (such as a present-tense verb) has the lexical item referred to by DEPENDENT as its subject (such as a singular noun). GR inventories include direct and indirect objects, complements, conjunctions, among other relations. The dependency relationships included in GRs correspond closely to the head-complement structure of SCFs, which is why they are the natural choice for SCF acquisition. There are several SCF lexicons for general language, such a"
P12-1044,W98-1114,0,0.0624779,"OMP), while “understand” is more likely to take just a direct object (NP). A comprehensive lexicon would also include semantic information about selectional preferences (or restrictions) on argument heads of verbs, diathesis alternations (i.e. semantically-motivated alternations between pairs of SCFs) and a mapping from surface frames to the underlying predicate-argument structure. Information about verb subcategorization is useful for tasks like information extraction (Cohen and Hunter, 2006; Rupp et al., 2010), verb clustering (Korhonen et al., 2006b; Merlo and Stevenson, 2001) and parsing (Carroll et al., 1998). In general, tasks that depend on predicate-argument structure can benefit from a high-quality SCF lexicon (Surdeanu et al., 2003). We present a novel approach for building verb subcategorization lexicons using a simple graphical model. In contrast to previous methods, we show how the model can be trained without parsed input or a predefined subcategorization frame inventory. Our method outperforms the state-of-the-art on a verb clustering task, and is easily trained on arbitrary domains. This quantitative evaluation is complemented by a qualitative discussion of verbs and their frames. We di"
P12-1044,de-marneffe-etal-2006-generating,0,0.0146204,"Missing"
P12-1044,P07-1035,0,0.0313481,"on of matching rules, and an unlexicalized parsing model. The BioLexicon system induces its SCF inventory automatically, but requires a lexicalized parsing model, rendering it more sensitive to domain variation. Both rely on a filtering stage that depends on external resources and/or gold standards to select top-performing thresholds. Our method, by contrast, does not use a predefined SCF inventory, and can perform well without parsed input. Graphical models have been increasingly popular for a variety of tasks such as distributional semantics (Blei et al., 2003) and unsupervised POS tagging (Finkel et al., 2007), and sampling methods allow efficient estimation of full joint distributions (Neal, 1993). The potential for joint inference of complementary information, such as syntactic verb and semantic argument classes, has a clear and interpretable way forward, in contrast to the pipelined methods described above. This was demonstrated in Andrew et al. (2004), where a Bayesian model was used to jointly induce syntactic and semantic classes for verbs, although that study relied on manually annotated data and a predefined SCF inventory and MLE. More recently, Abend and Rappoport (2010) trained ensemble c"
P12-1044,C94-1042,0,0.532739,"lk23@cam.ac.uk Introduction Subcategorization frames (SCFs) give a compact description of a verb’s syntactic preferences. These two sentences have the same sequence of lexical syntactic categories (VP-NP-SCOMP), but the first is a simple transitive (“X understood Y”), while the second is a ditransitive with a sentential complement (“X persuaded Y that Z”): 1. Kim (VP understood (NP the evidence (SCOMP that Sandy was present))) 2. Kim (VP persuaded (NP the judge) (SCOMP that Sandy was present)) Large, manually-constructed SCF lexicons mostly target general language (Boguraev and Briscoe, 1987; Grishman et al., 1994). However, in many domains verbs exhibit different syntactic behavior (Roland and Jurafsky, 1998; Lippincott et al., 2010). For example, the verb “develop” has specific usages in newswire, biomedicine and engineering that dramatically change its probability distribution over SCFs. In a few domains like biomedicine, the need for focused SCF lexicons has led to manually-built resources (Bodenreider, 2004). Such resources, however, are costly, prone to human error, and in domains where new lexical and syntactic constructs are frequently coined, quickly become obsolete (Cohen and Hunter, 2006). Da"
P12-1044,E03-1040,0,0.0297944,"lasses for verbs, although that study relied on manually annotated data and a predefined SCF inventory and MLE. More recently, Abend and Rappoport (2010) trained ensemble classifiers to perform argumentadjunct disambiguation of PP complements, a task closely related to SCF acquisition. Their study employed unsupervised POS tagging and parsing, and measures of selectional preference and argument structure as complementary features for the classifier. 422 Finally, our task-based evaluation, verb clustering with Levin (1993)’s alternation classes as the gold standard, was previously conducted by Joanis and Stevenson (2003), Korhonen et al. (2008) and Sun and Korhonen (2009). 3 Methodology In this section we describe the basic components of our study: feature sets, graphical model, inference, and evaluation. 3.1 Input and feature sets We tested several feature sets either based on, or approximating, the concept of grammatical relation described in section 2. Our method is agnostic regarding the exact definition of GR, and for example could use the Stanford inventory (De Marneffe et al., 2006) or even an entirely different lexico-syntactic formalism like CCG supertags (Curran et al., 2007). In this paper, we dist"
P12-1044,W00-1325,1,0.442784,"g of the Association for Computational Linguistics, pages 420–429, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics these problems by building lexicons tailored to new domains with less manual effort, and higher coverage and scalability. Unfortunately, high quality SCF lexicons are difficult to build automatically. The argument-adjunct distinction is challenging even for humans, many SCFs have no reliable cues in data, and some SCFs (e.g. those involving control such as type raising) rely on semantic distinctions. As SCFs follow a Zipfian distribution (Korhonen et al., 2000), many genuine frames are also low in frequency. State-of-theart methods for building data-driven SCF lexicons typically rely on parsed input (see section 2). However, the treebanks necessary for training a highaccuracy parsing model are expensive to build for new domains. Moreover, while parsing may aid the detection of some frames, many experiments have also reported SCF errors due to noise from parsing (Korhonen et al., 2006a; Preiss et al., 2007). Finally, many SCF acquisition methods operate with predefined SCF inventories. This subscribes to a single (often language or domain-specific) i"
P12-1044,korhonen-etal-2006-large,1,0.960271,"likely to take a direct object and sentential complement (NP-SCOMP), while “understand” is more likely to take just a direct object (NP). A comprehensive lexicon would also include semantic information about selectional preferences (or restrictions) on argument heads of verbs, diathesis alternations (i.e. semantically-motivated alternations between pairs of SCFs) and a mapping from surface frames to the underlying predicate-argument structure. Information about verb subcategorization is useful for tasks like information extraction (Cohen and Hunter, 2006; Rupp et al., 2010), verb clustering (Korhonen et al., 2006b; Merlo and Stevenson, 2001) and parsing (Carroll et al., 1998). In general, tasks that depend on predicate-argument structure can benefit from a high-quality SCF lexicon (Surdeanu et al., 2003). We present a novel approach for building verb subcategorization lexicons using a simple graphical model. In contrast to previous methods, we show how the model can be trained without parsed input or a predefined subcategorization frame inventory. Our method outperforms the state-of-the-art on a verb clustering task, and is easily trained on arbitrary domains. This quantitative evaluation is complemen"
P12-1044,P06-1044,1,0.928945,"likely to take a direct object and sentential complement (NP-SCOMP), while “understand” is more likely to take just a direct object (NP). A comprehensive lexicon would also include semantic information about selectional preferences (or restrictions) on argument heads of verbs, diathesis alternations (i.e. semantically-motivated alternations between pairs of SCFs) and a mapping from surface frames to the underlying predicate-argument structure. Information about verb subcategorization is useful for tasks like information extraction (Cohen and Hunter, 2006; Rupp et al., 2010), verb clustering (Korhonen et al., 2006b; Merlo and Stevenson, 2001) and parsing (Carroll et al., 1998). In general, tasks that depend on predicate-argument structure can benefit from a high-quality SCF lexicon (Surdeanu et al., 2003). We present a novel approach for building verb subcategorization lexicons using a simple graphical model. In contrast to previous methods, we show how the model can be trained without parsed input or a predefined subcategorization frame inventory. Our method outperforms the state-of-the-art on a verb clustering task, and is easily trained on arbitrary domains. This quantitative evaluation is complemen"
P12-1044,C08-1057,1,0.878809,"at study relied on manually annotated data and a predefined SCF inventory and MLE. More recently, Abend and Rappoport (2010) trained ensemble classifiers to perform argumentadjunct disambiguation of PP complements, a task closely related to SCF acquisition. Their study employed unsupervised POS tagging and parsing, and measures of selectional preference and argument structure as complementary features for the classifier. 422 Finally, our task-based evaluation, verb clustering with Levin (1993)’s alternation classes as the gold standard, was previously conducted by Joanis and Stevenson (2003), Korhonen et al. (2008) and Sun and Korhonen (2009). 3 Methodology In this section we describe the basic components of our study: feature sets, graphical model, inference, and evaluation. 3.1 Input and feature sets We tested several feature sets either based on, or approximating, the concept of grammatical relation described in section 2. Our method is agnostic regarding the exact definition of GR, and for example could use the Stanford inventory (De Marneffe et al., 2006) or even an entirely different lexico-syntactic formalism like CCG supertags (Curran et al., 2007). In this paper, we distinguish “true GRs” (tGRs"
P12-1044,lenci-etal-2008-unsupervised,0,0.25731,"ules. There are also resources which provide information about both syntactic and semantic properties of verbs: VerbNet (Kipper et al., 2008) draws on several hand-built and semi-automatic sources to link the syntax and semantics of 5,726 verbs. FrameNet (Baker et al., 1998) provides semantic frames and annotated example sentences for 4,186 verbs. PropBank (Palmer et al., 2005) is a corpus where each verb is annotated for its arguments and their semantic roles, covering a total of 4,592 verbs. There are many language-specific SCF acquisition systems, e.g. for French (Messiant, 2008), Italian (Lenci et al., 2008), Turkish (Han et al., 2008) and Chinese (Han et al., 2008). These typically rely on language-specific knowledge, either directly through heuristics, or indirectly through parsing models trained on treebanks. Furthermore, some require labeled training instances for supervised (Uzun et al., 2008) or semi-supervised (Han et al., 2008) learning algorithms. Two state-of-the-art data-driven systems for English verbs are those that produced VALEX, Preiss et al. (2007), and the BioLexicon (Venturi et al., 2009). The Preiss system extracts a verb instance’s GRs using the Rasp general-language unlexica"
P12-1044,A00-2034,0,0.0552597,"l ways to extend and improve the method. First, the independence assumptions between GRs in a given instance turned out to be too strong. To address this, we could give instances internal structure to capture conditional probability between generated GRs. Second, our results showed the conflation of several verbal aspects, most notably the syntactic and semantic. In a sense this is encouraging, as it motivates our most exciting future work: augmenting this simple model to explicitly capture complementary information such as distributional semantics (Blei et al., 2003), diathesis alternations (McCarthy, 2000) and ´ S´eaghdha, 2010). This selectional preferences (O study targeted high-frequency verbs, but the use of syntactic and semantic classes would also help with data sparsity down the road. These extensions would also call for a more comprehensive evaluation, averaging over several tasks, such as clustering by semantics, syntax, alternations and selectional preferences. In concrete terms, we plan to introduce latent variables corresponding to syntactic, semantic and alternation classes, that will determine a verb’s syntactic arguments, their semantic realization (i.e. selectional preferences),"
P12-1044,J01-3003,0,0.0283379,"object and sentential complement (NP-SCOMP), while “understand” is more likely to take just a direct object (NP). A comprehensive lexicon would also include semantic information about selectional preferences (or restrictions) on argument heads of verbs, diathesis alternations (i.e. semantically-motivated alternations between pairs of SCFs) and a mapping from surface frames to the underlying predicate-argument structure. Information about verb subcategorization is useful for tasks like information extraction (Cohen and Hunter, 2006; Rupp et al., 2010), verb clustering (Korhonen et al., 2006b; Merlo and Stevenson, 2001) and parsing (Carroll et al., 1998). In general, tasks that depend on predicate-argument structure can benefit from a high-quality SCF lexicon (Surdeanu et al., 2003). We present a novel approach for building verb subcategorization lexicons using a simple graphical model. In contrast to previous methods, we show how the model can be trained without parsed input or a predefined subcategorization frame inventory. Our method outperforms the state-of-the-art on a verb clustering task, and is easily trained on arbitrary domains. This quantitative evaluation is complemented by a qualitative discussi"
P12-1044,P08-3010,0,0.0885308,"t relies on hand-crafted rules. There are also resources which provide information about both syntactic and semantic properties of verbs: VerbNet (Kipper et al., 2008) draws on several hand-built and semi-automatic sources to link the syntax and semantics of 5,726 verbs. FrameNet (Baker et al., 1998) provides semantic frames and annotated example sentences for 4,186 verbs. PropBank (Palmer et al., 2005) is a corpus where each verb is annotated for its arguments and their semantic roles, covering a total of 4,592 verbs. There are many language-specific SCF acquisition systems, e.g. for French (Messiant, 2008), Italian (Lenci et al., 2008), Turkish (Han et al., 2008) and Chinese (Han et al., 2008). These typically rely on language-specific knowledge, either directly through heuristics, or indirectly through parsing models trained on treebanks. Furthermore, some require labeled training instances for supervised (Uzun et al., 2008) or semi-supervised (Han et al., 2008) learning algorithms. Two state-of-the-art data-driven systems for English verbs are those that produced VALEX, Preiss et al. (2007), and the BioLexicon (Venturi et al., 2009). The Preiss system extracts a verb instance’s GRs using the"
P12-1044,P05-1011,0,0.0780885,"eiss et al. (2007), and the BioLexicon (Venturi et al., 2009). The Preiss system extracts a verb instance’s GRs using the Rasp general-language unlexicalized parser (Briscoe et al., 2006) as input, and based on handcrafted rules, maps verb instances to a predefined inventory of 168 SCFs. Filtering is then performed to remove noisy frames, with methods ranging from a simple single threshold to SCF-specific hypothesis tests based on external verb classes and SCF inventories. The BioLexicon system extracts each verb instance’s GRs using the lexicalized Enju parser tuned to the biomedical domain (Miyao, 2005). Each unique GR-set considered a potential SCF, and an experimentally-determined threshold is used to filter low-frequency SCFs. Note that both methods require extensive manual work: the Preiss system involves the a priori definition of the SCF inventory, careful construction of matching rules, and an unlexicalized parsing model. The BioLexicon system induces its SCF inventory automatically, but requires a lexicalized parsing model, rendering it more sensitive to domain variation. Both rely on a filtering stage that depends on external resources and/or gold standards to select top-performing"
P12-1044,P10-1045,1,0.911898,"Missing"
P12-1044,J05-1004,0,0.0642713,"Briscoe, 1987) and COMLEX (Grishman et al., 1994), that depend on manual work. VALEX (Preiss et al., 2007) provides SCF distributions for 6,397 verbs acquired from a parsed general language corpus via a system that relies on hand-crafted rules. There are also resources which provide information about both syntactic and semantic properties of verbs: VerbNet (Kipper et al., 2008) draws on several hand-built and semi-automatic sources to link the syntax and semantics of 5,726 verbs. FrameNet (Baker et al., 1998) provides semantic frames and annotated example sentences for 4,186 verbs. PropBank (Palmer et al., 2005) is a corpus where each verb is annotated for its arguments and their semantic roles, covering a total of 4,592 verbs. There are many language-specific SCF acquisition systems, e.g. for French (Messiant, 2008), Italian (Lenci et al., 2008), Turkish (Han et al., 2008) and Chinese (Han et al., 2008). These typically rely on language-specific knowledge, either directly through heuristics, or indirectly through parsing models trained on treebanks. Furthermore, some require labeled training instances for supervised (Uzun et al., 2008) or semi-supervised (Han et al., 2008) learning algorithms. Two s"
P12-1044,P07-1115,1,0.861221,"n data, and some SCFs (e.g. those involving control such as type raising) rely on semantic distinctions. As SCFs follow a Zipfian distribution (Korhonen et al., 2000), many genuine frames are also low in frequency. State-of-theart methods for building data-driven SCF lexicons typically rely on parsed input (see section 2). However, the treebanks necessary for training a highaccuracy parsing model are expensive to build for new domains. Moreover, while parsing may aid the detection of some frames, many experiments have also reported SCF errors due to noise from parsing (Korhonen et al., 2006a; Preiss et al., 2007). Finally, many SCF acquisition methods operate with predefined SCF inventories. This subscribes to a single (often language or domain-specific) interpretation of subcategorization a priori, and ignores the ongoing debate on how this interpretation should be tailored to new domains and applications, such as the more prominent role of adjuncts in information extraction (Cohen and Hunter, 2006). In this paper, we describe and evaluate a novel probabilistic data-driven method for SCF acquisition aimed at addressing some of the problems with current approaches. In our model, a Bayesian network des"
P12-1044,P98-2184,0,0.107516,"erb’s syntactic preferences. These two sentences have the same sequence of lexical syntactic categories (VP-NP-SCOMP), but the first is a simple transitive (“X understood Y”), while the second is a ditransitive with a sentential complement (“X persuaded Y that Z”): 1. Kim (VP understood (NP the evidence (SCOMP that Sandy was present))) 2. Kim (VP persuaded (NP the judge) (SCOMP that Sandy was present)) Large, manually-constructed SCF lexicons mostly target general language (Boguraev and Briscoe, 1987; Grishman et al., 1994). However, in many domains verbs exhibit different syntactic behavior (Roland and Jurafsky, 1998; Lippincott et al., 2010). For example, the verb “develop” has specific usages in newswire, biomedicine and engineering that dramatically change its probability distribution over SCFs. In a few domains like biomedicine, the need for focused SCF lexicons has led to manually-built resources (Bodenreider, 2004). Such resources, however, are costly, prone to human error, and in domains where new lexical and syntactic constructs are frequently coined, quickly become obsolete (Cohen and Hunter, 2006). Datadriven methods for SCF acquisition can alleviate 420 Proceedings of the 50th Annual Meeting of"
P12-1044,D09-1067,1,0.950989,"annotated data and a predefined SCF inventory and MLE. More recently, Abend and Rappoport (2010) trained ensemble classifiers to perform argumentadjunct disambiguation of PP complements, a task closely related to SCF acquisition. Their study employed unsupervised POS tagging and parsing, and measures of selectional preference and argument structure as complementary features for the classifier. 422 Finally, our task-based evaluation, verb clustering with Levin (1993)’s alternation classes as the gold standard, was previously conducted by Joanis and Stevenson (2003), Korhonen et al. (2008) and Sun and Korhonen (2009). 3 Methodology In this section we describe the basic components of our study: feature sets, graphical model, inference, and evaluation. 3.1 Input and feature sets We tested several feature sets either based on, or approximating, the concept of grammatical relation described in section 2. Our method is agnostic regarding the exact definition of GR, and for example could use the Stanford inventory (De Marneffe et al., 2006) or even an entirely different lexico-syntactic formalism like CCG supertags (Curran et al., 2007). In this paper, we distinguish “true GRs” (tGRs), produced by a parser, and"
P12-1044,P03-1002,0,0.0856525,"ormation about selectional preferences (or restrictions) on argument heads of verbs, diathesis alternations (i.e. semantically-motivated alternations between pairs of SCFs) and a mapping from surface frames to the underlying predicate-argument structure. Information about verb subcategorization is useful for tasks like information extraction (Cohen and Hunter, 2006; Rupp et al., 2010), verb clustering (Korhonen et al., 2006b; Merlo and Stevenson, 2001) and parsing (Carroll et al., 1998). In general, tasks that depend on predicate-argument structure can benefit from a high-quality SCF lexicon (Surdeanu et al., 2003). We present a novel approach for building verb subcategorization lexicons using a simple graphical model. In contrast to previous methods, we show how the model can be trained without parsed input or a predefined subcategorization frame inventory. Our method outperforms the state-of-the-art on a verb clustering task, and is easily trained on arbitrary domains. This quantitative evaluation is complemented by a qualitative discussion of verbs and their frames. We discuss the advantages of graphical models for this task, in particular the ease of integrating semantic information about verbs and"
P12-1044,J87-3002,0,\N,Missing
P12-1044,C98-2179,0,\N,Missing
P12-1044,C98-1013,0,\N,Missing
P12-1044,P07-2009,0,\N,Missing
P12-1044,D07-1096,0,\N,Missing
P13-1085,D10-1088,0,0.100138,"Missing"
P13-1085,C12-1165,1,0.76043,"Missing"
P13-1085,D12-1065,0,0.0296961,"l Point Processes Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. Recently (Kulesza, 2012; Kulesza and Taskar, 864 2012c) introduced them to the machine learning community and demonstrated their usefulness for a variety of tasks including document summarization, image search, modeling non-overlapping human poses in images and video and automatically building timelines of important news stories (Kulesza and Taskar, 2010; Kulesza and Taskar, 2012a; Gillenwater et al., 2012; Kulesza and Taskar, 2012b). Below we provide a brief description of the framework, a comprehensive survey can be found in (Kulesza and Taskar, 2012c). Given a set of items Y = {y1 , . . . , yN }, a DPP P defines a probability measure on the set of all subsets of Y, 2Y . Kulesza and Taskar (2012c) restricted their discussion of DDPs to L-ensembles, where the probability of a subset Y ∈ Y is defined through a positive semi-definite matrix L indexed by the elements of Y: alternative models such as Markov Random Fields (MRFs), is efficient, theoretically and practically, for DPPs. 3.2 DPPs are p"
P13-1085,C94-1042,0,0.079275,"nd Mooney, 2011), distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The lin"
P13-1085,P98-1013,0,0.0257233,"larity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and"
P13-1085,N06-2015,0,0.0145547,"; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded"
P13-1085,ienco-etal-2008-automatic,0,0.257449,"e clusters induced by our model for the acquisition of the three information types. Our evaluation against a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler tha"
P13-1085,D07-1017,0,0.0310741,"ic information in SCF acquisition (Korhonen, 2002). We will address this problem in an unsupervised way: our approach is to consider SCFs together with semantic SPs through VCs which generalize over syntactically and semantically similar verbs. SP acquisition Considerable research has been conducted on SP acquisition, with a variety of unsupervised models proposed for this task that use no hand-crafted information during training. The latter approaches include latent variable mod´ S´eaghdha, 2010; Ritter and Etzioni, 2010; els (O Reisinger and Mooney, 2011), distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al.,"
P13-1085,W05-0621,0,0.485004,"Missing"
P13-1085,kawahara-kurohashi-2010-acquiring,0,0.296994,"inst a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler than traditional SCFs. Current approaches to SCF acquisition suffer from lack of semantic information which is neede"
P13-1085,A97-1052,0,0.393055,"hen defined over the joint SCF and SP kernel, this new algorithm can be used to induce VCs that are valuable for both tasks. We also contribute by evaluating the value of the clusters induced by our model for the acquisition of the three information types. Our evaluation against a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame indu"
P13-1085,P10-1045,0,0.0450577,"Missing"
P13-1085,C08-1057,1,0.831645,"These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of an unlexicalized parse"
P13-1085,J05-1004,0,0.114074,"at et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were"
P13-1085,W02-0907,1,0.837735,"SCF and SP kernel, this new algorithm can be used to induce VCs that are valuable for both tasks. We also contribute by evaluating the value of the clusters induced by our model for the acquisition of the three information types. Our evaluation against a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that ident"
P13-1085,P07-1115,1,0.940249,"ent structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation ´ S´eaghdha and (Dang, 2004; Thater et al., 2010; O Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the first unif"
P13-1085,D11-1130,0,0.13274,"Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation ´ S´eaghdha and (Dang, 2004; Thater et al., 2010; O Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the first unified framework for unsupervised learning of these"
P13-1085,P10-1044,0,0.025356,"ly syntax-driven acquisition process. Previous works have showed the benefit of hand-coded semantic information in SCF acquisition (Korhonen, 2002). We will address this problem in an unsupervised way: our approach is to consider SCFs together with semantic SPs through VCs which generalize over syntactically and semantically similar verbs. SP acquisition Considerable research has been conducted on SP acquisition, with a variety of unsupervised models proposed for this task that use no hand-crafted information during training. The latter approaches include latent variable mod´ S´eaghdha, 2010; Ritter and Etzioni, 2010; els (O Reisinger and Mooney, 2011), distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemant"
P13-1085,P99-1014,0,0.197007,"Missing"
P13-1085,lenci-etal-2008-unsupervised,0,0.143571,"uisition of the three information types. Our evaluation against a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler than traditional SCFs. Current approaches to S"
P13-1085,P08-1057,0,0.353164,"onstrating the benefit our approach. [show]VERB [no evidence to the usefulness of joint learning leaning for these tasks]DOBJ. Finally, VC induction involves clustering together verbs with similar meaning, reflected in similar SCFs and SPs. For example, ”show” in the above examples could get clustered together with ”demonstrate” and ”indicate”. Because these challenging tasks capture complementary information about predicate argument structure, they should be able to inform and support each other. Recently, researchers have begun to investigate the benefits of their joint learning. Schulte im Walde et al. (2008) integrated SCF and VC acquisition and used it for WordNet-based ´ S´eaghdha (2010) presented a SP classification. O “dual-topic” model for SPs that induces also verb clusters. Both works reported SP evaluation with promising results. Lippincott et al. (2012) presented a joint model for inducing simple syntactic frames and VCs. They reported high accuracy results on VCs. de Cruys et al. (2012) introduced a joint model for SCF and SP acquisition. They evaluated both the SCFs and SPs, obtaining reasonable result on both tasks. In this paper, we present the first unified framework for unsupervise"
P13-1085,P08-1050,0,0.100784,"in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of"
P13-1085,J06-2001,0,0.430591,"ion about predicateargument structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation ´ S´eaghdha and (Dang, 2004; Thater et al., 2010; O Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument stru"
P13-1085,P12-1044,1,0.794435,"Missing"
P13-1085,messiant-etal-2008-lexschem,1,0.890917,"y our model for the acquisition of the three information types. Our evaluation against a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler than traditional SCFs. Cur"
P13-1085,D09-1067,1,0.962363,"c, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of an unlexicalized parser. Our approach hence pr"
P13-1085,D11-1095,1,0.94025,"d van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation ´ S´eaghdha and (Dang, 2004; Thater et al., 2010; O Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the first unified framework for unsupervised learning of these three types of informat"
P13-1085,W05-1002,0,0.387461,"Missing"
P13-1085,J05-3003,0,0.243331,"Missing"
P13-1085,P10-1097,0,0.0501069,"Missing"
P13-1085,W09-0211,0,0.0578131,"Missing"
P13-1085,D11-1097,1,0.892213,"Missing"
P13-1085,W09-0210,1,0.724227,"ntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of an unlexicalized parser. Our approach hence provides a framework for"
P13-1085,P11-1156,0,0.0652817,"Missing"
P13-1085,W98-1505,0,\N,Missing
P13-1085,W10-1612,0,\N,Missing
P13-1085,P07-1028,0,\N,Missing
P13-1085,C98-1013,0,\N,Missing
P13-1085,P09-2019,0,\N,Missing
P13-1085,chesley-salmon-alt-2006-automatic,0,\N,Missing
P13-2129,P03-1009,1,0.893953,"Missing"
P13-2129,W02-1016,0,0.490118,"Missing"
P13-2129,A00-2034,1,0.911844,"argely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson ("
P13-2129,P06-4020,0,0.011323,"(j)) sup{p(a|v)} = 1 XX Z= min(fv (m), fv (n)) m 1 n A relaxation is used in mathematical optimization for relaxing the strict requirement, by either substituting it with an easier requirement or dropping it completely. 737 Frame pair NP+PPon NP+PPwith NP+PPon PPwith NP+PPon PPon NP+PPwith PPwith NP+PPwith PPon PPwith PPon NP+PPon NP+PPon NP+PPwith NP+PPwith PPwith PPwith PPon PPon Possible DA Locative Causative(with) Causative(on) ? ? ? - Frequency 30 0 30 0 30 0 40 30 0 30 occurrence of a verb as a member of one of the 168 SCF s on the basis of grammatical relations identified by the RASP (Briscoe et al., 2006) parser. We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008), and the 17 classes set in Sun et al. (2008). We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. Table 2: Example frame pair features for spray So we end up with a simple form: p(fv (i), fv (j)|v) ≈ Z −1 · min(fv (i), fv (j))"
P13-2129,J01-3003,0,0.300833,"chulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson (2010, 2011) used hierarchical Bayesian models on slot frequency data obtained from"
P13-2129,J02-3001,0,0.0664675,"1993)’s seminal book provides a manual inventory both of DAs and verb classes where membership is determined according to participation in these alternations. For example, most of the C OOK verbs (e.g. bake, cook, fry . . . ) can all take various DAs, such as the causative alternation, middle alternation and instrument subject alternation. In computational linguistics, work inspired by Levin’s classification has exploited the link between syntax and semantics for producing classifications of verbs. Such classifications are useful for a wide variety of purposes such as semantic role labelling (Gildea and Jurafsky, 2002), 736 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 736–741, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Frame NP+PPon NP+PPwith PPwith PPon Example sentence Jessica sprayed paint on the wall Jessica sprayed the wall with paint *The wall sprayed with paint Jessica sprayed paint on the wall Freq 40 30 0 30 decomposed as: p(fv (i), fv (j)|v)0 , p(fv (i)|v) · p(fv (j)|v) (1) We assume that SCFs are dependent as they are generated by the underlying meaning components (Levin and Hovav, 2006). The frame dependenc"
P13-2129,D11-1025,1,0.920283,"d Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson (2010, 2011) used hierarchical Bayesian models on slot frequency data obtained from childdirected speech parsed with a dependency parser to model"
P13-2129,J06-2001,0,0.371871,"ory University of Cambridge Cambridge, UK diana@dianamccarthy.co.uk alk23@cam.ac.uk Abstract predicting unseen syntax (Parisien and Stevenson, 2010), argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson ("
P13-2129,D09-1067,1,0.928296,"(Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the ov"
P13-2129,D11-1095,1,0.848268,"y of Cambridge Cambridge, UK diana@dianamccarthy.co.uk alk23@cam.ac.uk Abstract predicting unseen syntax (Parisien and Stevenson, 2010), argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (20"
P13-2129,W04-2605,0,0.563711,"tic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson (2010, 2011) used hierarchica"
P13-2129,W09-0210,1,0.907059,"Missing"
P13-2129,C10-1113,1,\N,Missing
P13-2129,P98-2247,1,\N,Missing
P13-2129,C98-2242,1,\N,Missing
P13-2129,P99-1051,0,\N,Missing
P13-2129,P07-1115,1,\N,Missing
P14-2118,P11-1040,0,0.0128329,"presentations from both linguistic and perceptual input, outperform text-only models on a range of evaluations. However, while multi-modal models acquire richer representations of concrete concepts, their ability to represent abstract concepts can be weaker than text-only models (Hill et al., 2013). A principled treatment of concreteness is thus likely to be important if the multi-modal approach is to prove effective on a wider range of concepts. In a similar vein, interest in subjectivity analysis is set to grow with interest in extracting sentiment and opinion from the web and social media (Benson et al., 2011). Moreover, given that humans seem to exploit both concreteness (Paivio, 1990) and subjectivity (Canestrelli et al., 2013) clues when processing language, it is likely that the same clues should benefit computational models aiming to replicate human-level performance in this area. We quantify the lexical subjectivity of adjectives using a corpus-based method, and show for the first time that it correlates with noun concreteness in large corpora. These cognitive dimensions together influence how word meanings combine, and we exploit this fact to achieve performance improvements on the semantic"
P14-2118,N12-2003,1,0.830787,"ves. We therefore extract the relative frequency of occurrence in such constructions. Non-nominality: Many adjectives also function as nouns (sweet cake vs. (boiled sweet). Unlike nouns, many adjectives are inherently subjective, and the number of adjectives in texts correlates with human judgements of their subjectivity (Hatzivassiloglou and Wiebe, 2000). We therefore extract the frequency with which concepts are tagged as adjectives relative to as nouns, on the 1 Available at http://crr.ugent.be/archives/1330. See e.g. (Wiebe and Riloff, 2011). 3 Several of the features here were applied by Hill (2012), to the task of ordering multiple-modifier strings. 2 726 assumption that ‘pure’ adjectives are on average more subjective than nominal-style adjectives. NonNominality Concreteness meets Subjectivity Demonstrable commonalities in how different people perceive the physical world suggest that concrete language may be more objective than abstract language (Langacker, 1997). Intuitively, adjectives ascribing physical properties (wooden shed) are more objective than those conveying abstract traits (suspicious man). Indeed, in certain cases the original, apparently objective, senses of polysemous a"
P14-2118,Y08-1023,0,0.0289199,"al., 2005) and cognitive representation (Hill et al., 2013) have also been observed. In linguistic conctructions, concreteness appears to influence compound and phrasal semantics (Traugott, 1985; Bowdle and Gentner, 2005; Turney et al., 2011). Together with the practical applications outlined in Section 1, these facts indicate the potential value of concreteness for models aiming to replicate human performance in language processing tasks. While automatic methods have been proposed for the quantification of lexical concreteness, they each rely on dictionaries or similar hand-coded resources (Kwong, 2008; Turney et al., 2011). We instead extract scores from a recently released dataset of lexical concepts rated on a 1-5 scale for concreteness by 20 annotators in a crowdsourcing experiment (Brysbaert et al., 2013).1 P P hotly P hot + hotly Comparability: Wiebe (2000) oberve that gradable are more likely to be subjective. Following Wiebe, we note that the existence of comparative forms for an adjective are indicative of gradability. We thus define comparability as the frequency of comparative or superlative forms relative to the frequency of the base form, e.g. P P hotter + hottest P P P hot + h"
P14-2118,C94-1103,0,0.0228783,"Missing"
P14-2118,P07-1123,0,0.0177718,"ther characteristic of gradable adjectives noted by Wiebe (2000) is that they admit degree modifiers (very/quite delicious). We therefore extract the relative frequency of occurrence with one of a hand-coded list of English degree modifiers. Subjectivity Subjectivity is the degree to which language is interpretable independently of the speaker’s perspective (Langacker, 2002). For example, the utterance he sits across the table is more subjective than he sits opposite Sam as its truth depends on the speaker’s position. Language may also be subjective because it conveys evaluations or opinions (Mihalcea et al., 2007). Computational applications of subjectivity, including sentiment analysis and information extraction, have focused largely on phrase or document meaning.2 In contrast, here we present six corpus-based features designed to quantify the lexical subjectivity of adjectives. The features Comparability and Modifiability are identified as predictors of subjectivity by Wiebe (2000). The remainder are motivated by corpus studies and/or observations from the theoretical literature.3 Predicativity: Bolinger (1967) proposed that subjective adjectives occur in predicative constructions (the cake is sweet)"
P14-2118,P12-1015,0,0.0521183,"Missing"
P14-2118,P04-1035,0,0.0128762,"Missing"
P14-2118,S13-1035,0,0.0262796,"ation is shown in Figure 1 (top). We first tested the relationship between concreteness and subjectivity with a correlation analysis over noun concepts. For each noun n we de4 Comparability Adverbiability In addressing (1), we extracted the 2,000 highestfrequency nouns from the Brysbaert et al. (2013) concreteness dataset. We denote by CON C(n) the mean concreteness rating for noun n. For the 24,908 adjectives that occur in some adjectivenoun pair with one of these nouns in the British National Corpus (BNC) (Leech et al., 1994), we extracted subjectivity features from the Google Books Corpus (Goldberg and Orwant, 2013). Since each of the six features takes values on [0, 1], we define the overall subjectivity of an adjective a with feature vector sa = [sa1 . . . sa6 ] as 6 X 71.2 Modifiability Analysis SU BJ(a) = 73.7 LeftTendency 1 X a s |An |a∈An where the bag An contains an adjective a for each occurrence of the pair (a, n) in the BNC. As hypothesized, CON C(n) was a significant predictor of the magnitude of the subjectivity profile (Pearson r = −0.421, p &lt; 0.01). This effect is illustrated in Figure 1 (bottom). To explore the relationship between concreteness, subjectivity and meaning, we plotted the 20,"
P14-2118,W03-1014,0,0.0792562,"Missing"
P14-2118,D13-1115,0,0.0441819,"Missing"
P14-2118,D11-1063,0,0.507125,"eir base form, e.g. Dimensions of meaning Concreteness A large and growing body of empirical evidence indicates clear differences between concrete concepts, such as donut or hotdog and abstract concepts, such as guilt or obesity. Concrete words are more easily learned, remembered and processed than abstract words (Paivio, 1991), while differences in brain activity (Binder et al., 2005) and cognitive representation (Hill et al., 2013) have also been observed. In linguistic conctructions, concreteness appears to influence compound and phrasal semantics (Traugott, 1985; Bowdle and Gentner, 2005; Turney et al., 2011). Together with the practical applications outlined in Section 1, these facts indicate the potential value of concreteness for models aiming to replicate human performance in language processing tasks. While automatic methods have been proposed for the quantification of lexical concreteness, they each rely on dictionaries or similar hand-coded resources (Kwong, 2008; Turney et al., 2011). We instead extract scores from a recently released dataset of lexical concepts rated on a 1-5 scale for concreteness by 20 annotators in a crowdsourcing experiment (Brysbaert et al., 2013).1 P P hotly P hot +"
P14-2118,P06-1134,0,0.0415544,"Missing"
P14-2118,H05-1044,0,0.0876193,"Missing"
P14-2118,C00-1044,0,\N,Missing
P14-2118,D12-1112,0,\N,Missing
P14-2118,W13-2609,1,\N,Missing
P14-2135,N09-1003,0,0.0572959,"Missing"
P14-2135,P12-1092,0,0.0431552,"e average pairwise cosine distance between all the image representations {w~1 . . . w~n } in the set of images for that concept: d(w) = X 1 w ~i · w~j 1− 2n(n − 1) i<j≤n |w ~i ||w~j | (1) We use an average pairwise distance-based metric because this emphasizes the total variation more than e.g. the mean distance from the centroid. In all experiments we set n = 50. Generating Visual Representations Visual vector representations for each image were obtained using the well-known bag of visual words (BoVW) approach (Sivic and Zisserman, 2003). BoVW obtains a vector representation for an 836 e.g. (Huang et al., 2012; Bruni et al., 2012)). As a complementary gold-standard, we use the University of South Florida Norms (USF) (Nelson et al., 2004). This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs. The USF norms have been used in many previous studies to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for concept"
P14-2135,Y08-1023,0,0.0218403,"vements on other tasks in semantic processing and representation. Table 1: Concepts with highest and lowest image dispersion scores in our evaluation set, and concreteness ratings from the USF dataset. the very abstract or very concrete concepts. As Table 1 illustrates, the concepts with the lowest dispersion in this sample are, without exception, highly concrete, and the concepts of highest dispersion are clearly very abstract. It should be noted that all previous approaches to the automatic measurement of concreteness rely on annotator ratings, dictionaries or manuallyconstructed resources. Kwong (2008) proposes a method based on the presence of hard-coded phrasal features in dictionary entries corresponding to each concept. By contrast, S´anchez et al. (2011) present an approach based on the position of word senses corresponding to each concept in the WordNet ontology (Fellbaum, 1999). Turney et al. (2011) propose a method that extends a large set of concreteness ratings similar to those in the USF dataset. The Turney et al. algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space. In contrast to each of"
P14-2135,C94-1103,0,0.022202,"et al., 2007). We resize the images in our dataset to 100x100 pixels and compute PHOW descriptors using VLFeat (Vedaldi and Fulkerson, 2008). The descriptors for the images were subsequently clustered using mini-batch k-means (Sculley, 2010) with k = 50 to obtain histograms of visual words, yielding 50-dimensional visual vectors for each of the images. Generating Linguistic Representations We extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al. (2013a), trained on the 100M word British National Corpus (Leech et al., 1994). This model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping (Mikolov et al., 2013b). 2.2 3 Improving Multi-Modal Representations We apply image dispersion-based filtering as follows: if both concepts in an evaluation pair have an image dispersion below a given threshold, both the linguistic and the visual representations are included. If not, in accordance with the Dual Coding Theory of human concept pro"
P14-2135,R11-1055,0,0.208132,"ments focus on multi-modal models that extract their perceptual input automatically from images. Image-based models more naturally mirror the process of human concept acquisition than those whose input derives from experimental datasets or expert annotation. They are also more scalable since high-quality tagged images are freely available in several web-scale image datasets. We use Google Images as our image source, and extract the first n image results for each concept word. It has been shown that images from Google yield higher-quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011). Other potential sources, such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (Von Ahn and Dabbish, 2004), either do not contain images for abstract concepts or do not contain sufficient images for the concepts in our evaluation sets. 2.1 Figure 1: Example images for a concrete (elephant – little diversity, low dispersion) and an abstract concept (happiness – greater diversity, high dispersion). Figure 2: Computation of PHOW descriptors using dense SIFT for levels l = 0 to l = 2 and the corresponding histogram representations (Bosch et al., 2007). Image Dispersion-Based Filtering Fol"
P14-2135,P12-1015,0,0.642782,"be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been 835 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835–841, c Baltimore, Maryland, USA, June"
P14-2135,D13-1115,0,0.470058,"Missing"
P14-2135,N10-1011,0,0.377871,"on image data, and can be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been 835 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835–841, c Baltimore"
P14-2135,D12-1130,0,0.48871,"lti-modal models to learn and represent word meanings. The method relies solely on image data, and can be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been 835 Proceedings of the 52nd Annual Meeting of the Associatio"
P14-2135,D11-1063,0,0.245846,"s spectrum), we observed a high correlation between abstractness and dispersion (Spearman ρ = 0.61, p < 0.001). On this more diverse sample, which reflects the range of concepts typically found in linguistic corpora, image dispersion is a particularly useful diagnostic for identifying Concreteness and Image Dispersion The filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts. Since research has demonstrated the applicability of concreteness to a range of other NLP tasks (Turney et al., 2011; Kwong, 2008), it is important to examine the connection between image dispersion and concreteness in more detail. 4.1 Quantifying Concreteness To evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary classification task based on the set of 100 concrete and 100 abstract concepts A∪C introduced in Section 2. By classifying con838 Concept shirt bed knife dress car ego nonsense memory potential know Image Dispersion .488 .495 .560 .578 .580 1.000 .999 .999 .997 .996 Conc. (USF) 6.05 5.91 6.08 6.59 6.35 1.93 1.90 1.78 1.90 2.70 racy of 7"
P14-2135,W13-2609,1,\N,Missing
P16-1024,W13-3520,0,0.0115101,"ts). We also compare with a benchmarking Type 1 model from sentence-aligned parallel data called BiCVM (Hermann and Blunsom, 2014b). Finally, a SGNS-based BWE model with the BNC+GT seed lexicon is taken as a baseline Type 4 model (Mikolov et al., 2013a).7 Training Data and Setup We use standard training data and suggested settings to obtain BWEs for all models involved in comparison. We retain the 100K most frequent words in each language for all models. To induce monolingual WE spaces, two monolingual SGNS models were trained on the cleaned and tokenized Wikipedias from the Polyglot website (Al-Rfou et al., 2013) using SGD with a global learning rate of 0.025. For BilBOWA, as in the original work (Gouws et al., 2015), the bilingual signal for the cross-lingual regularization is provided by the first 500K sentences from Europarl.v7 (Tiedemann, 2012). We use SGD with a global rate of 0.15.8 The window size is varied from 2 to 16 in steps of 2, and the best scoring model is always reported in all comparisons. BWESG was trained on the cleaned and tokenized document-aligned Wikipedias available online9 , SGD on pseudo-bilingual documents with a global rate 0.025. For BiCVM, we use the tool released by its"
P16-1024,P14-1023,0,0.0253315,"s shared bilingual word embedding space (further SBWES) from Gouws et al. (2015). range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). Several studies have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010). Yet the widely used skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is considered as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy and Goldberg, 2014b; Levy et al., 2015). Introduction Dense real-valued vector representations of words or word embeddings (WEs) have recently gained increasing popularity in natural language processing (NLP), serving as invaluable features in a broad Research interest has recently extended to bilingual word embeddings (BWEs). BWE learning models focus on the induction of a shared bilingual word embedding space (SBWES) where words from both languages are represented in a uniform language-independent manner such that similar words (regardless of the actual language) have similar represen"
P16-1024,D14-1082,0,0.0960485,"BLL) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models, all of which use more expensive bilingual signals. Effectively, we demonstrate that a SBWES may be induced by leveraging only a very weak bilingual signal (document alignments) along with monolingual data. 1 Monolingual vs Bilingual Figure 1: A toy example of a 3-dimensional monolingual vs shared bilingual word embedding space (further SBWES) from Gouws et al. (2015). range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). Several studies have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010). Yet the widely used skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is considered as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy and Goldberg, 2014b; Levy et al., 2015). Introduction Dense real-valued vector representations of words or word embeddings (WEs) have recently gained i"
P16-1024,D15-1131,0,0.323168,"or word-aligned parallel data as the only data source (Zou et al., 2013; Hermann and Blunsom, 2014a; Koˇciský et al., 2014; Hermann and Blunsom, 2014b; Chandar et al., 2014). In addition to an expensive bilingual signal (colliding with P2), these models do not leverage larger monolingual datasets for training (not satisfying P1). 248 (Type 2) Joint Bilingual Training: These models jointly optimize two monolingual objectives, with the cross-lingual objective acting as a cross-lingual regularizer during training (Klementiev et al., 2012; Gouws et al., 2015; Soyer et al., 2015; Shi et al., 2015; Coulmance et al., 2015). The idea may be summarized by the simplified formulation (Luong et al., 2015): γ(MonoS +MonoT )+δBi. The monolingual objectives M onoS and M onoT ensure that similar words in each language are assigned similar embeddings and aim to capture the semantic structure of each language, whereas the cross-lingual objective Bi ensures that similar words across languages are assigned similar embeddings. It ties the two monolingual spaces together into a SBWES (thus satisfying P1). Parameters γ and δ govern the influence of the monolingual and bilingual components.1 The main disadvantage of Type 2 mode"
P16-1024,E14-1049,0,0.782736,"word embeddings (BWEs). BWE learning models focus on the induction of a shared bilingual word embedding space (SBWES) where words from both languages are represented in a uniform language-independent manner such that similar words (regardless of the actual language) have similar representations (see Fig. 1). A variety of BWE learning models have been proposed, differing in the essential requirement of a bilingual signal necessary to construct such a SBWES (discussed later in Sect. 2). SBWES may be used to support many tasks, e.g., computing cross-lingual/multilingual semantic word similarity (Faruqui and Dyer, 2014), learning bilingual word lexicons (Mikolov et al., 2013a; Gouws et al., 2015; Vuli´c et al., 2016), cross-lingual entity linking (Tsai and Roth, 2016), 247 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247–257, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics parsing (Guo et al., 2015; Johannsen et al., 2015), machine translation (Zou et al., 2013), or crosslingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016). BWE models should have two desirable properties: (P1) leverage (large) monolin"
P16-1024,P04-1067,0,0.0334336,"., 2015; Ammar et al., 2016): (1) two separate non-aligned monolingual embedding spaces are induced using any monolingual WE learning model (SGNS is the typical choice), (2) given a seed lexicon of word translation pairs as the bilingual signal for training, a mapping function is learned which ties the two monolingual spaces together into a SBWES. All existing work on this class of models assumes that high-quality training seed lexicons are readily available. In reality, little is understood regarding what constitutes a high quality seed lexicon, even with “traditional” distributional models (Gaussier et al., 2004; Holmlund et al., 2005; Vuli´c and Moens, 2013). Therefore, in this work we ask whether BWE learning could be improved by making more intelligent choices when deciding over seed lexicon entries. In order to do this we delve deeper into the cross-lingual mapping problem by analyzing a spectrum of seed lexicons with respect to controllable parameters such as lexicon source, its size, translation method, and translation pair reliability. The contributions of this paper are as follows: (C1) We present a systematic study on the importance of seed lexicons for learning mapping functions between mon"
P16-1024,P15-1119,0,0.0520406,"e essential requirement of a bilingual signal necessary to construct such a SBWES (discussed later in Sect. 2). SBWES may be used to support many tasks, e.g., computing cross-lingual/multilingual semantic word similarity (Faruqui and Dyer, 2014), learning bilingual word lexicons (Mikolov et al., 2013a; Gouws et al., 2015; Vuli´c et al., 2016), cross-lingual entity linking (Tsai and Roth, 2016), 247 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247–257, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics parsing (Guo et al., 2015; Johannsen et al., 2015), machine translation (Zou et al., 2013), or crosslingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016). BWE models should have two desirable properties: (P1) leverage (large) monolingual training sets tied together through a bilingual signal, (P2) use as inexpensive bilingual signal as possible in order to learn a SBWES in a scalable and widely applicable manner across languages and domains. While we provide a classification of related work, that is, different BWE models according to these properties in Sect. 2.1, the focus of this work is on a po"
P16-1024,P14-1006,0,0.0839575,"propose a simple yet effective hybrid BWE model HYBWE that removes the need for readily available seed lexicons, and satisfies properties P1 and P2. HYBWE relies on an inexpensive seed lexicon of highly reliable word translation pairs obtained by a documentlevel BWE model (Vuli´c and Moens, 2016) from document-aligned comparable data. (C3) Using a careful pair selection process when constructing a seed lexicon, we show that in the BLL task HYBWE outperforms a BWE model of Mikolov et al. (2013a) which relies on readily available seed lexicons. HYBWE also outperforms state-of-the-art models of (Hermann and Blunsom, 2014b; Gouws et al., 2015) which require sentencealigned parallel data. 2 Learning SBWES using Seed Lexicons Given source and target language vocabularies V S and V T , all BWE models learn a representation of each word w ∈ V S t V T in a SBWES as a realvalued vector: w = [f1 , . . . , fd ], where fk ∈ R denotes the value for the k-th cross-lingual feature for w within a d-dimensional SBWES. Semantic similarity sim(w, v) between two words w, v ∈ V S t V T is then computed by applying a similarity function (SF), e.g. cosine (cos) on their representations in the SBWES: sim(w, v) = SF (w, v) = cos(w,"
P16-1024,P12-1092,0,0.0176666,"omputed by aggregating over all word embeddings for each cwj ∈ Con(w) using standard addition as the compositional operator (Mitchell and Lapata, 2008) which was proven a robust choice (Milajevs et al., 2014): Con(w) = cw1 + cw2 + . . . + cwr Table 3: Acc1 scores in the SWTC task. All seed lexicons contain 6K translation pairs, except for BNC+HYB+SYM (its sizes provided in parentheses). * denotes a statistically significant improvement over baselines and BNC+GT using McNemar’s statistical significance test with the Bonferroni correction, p &lt; 0.05. cross-lingual variant of the task proposed by Huang et al. (2012) which evaluates monolingual contextsensitive semantic similarity of words in sentential context, and it is also very related to cross-lingual lexical substitution (Mihalcea et al., 2010). To isolate the performance of each BWE induction model from the details of the SWTC setup, we use the same approach with all models: we opt for the SWTC framework proven to yield excellent results with BWEs in the SWTC task (Vuli´c and Moens, 2016). In short, the context bag Con(w) = {cw1 , . . . , cwr } is obtained by harvesting all r words that occur with w in the sentence. (4) where cwj is the embedding o"
P16-1024,D15-1245,0,0.0442718,"ement of a bilingual signal necessary to construct such a SBWES (discussed later in Sect. 2). SBWES may be used to support many tasks, e.g., computing cross-lingual/multilingual semantic word similarity (Faruqui and Dyer, 2014), learning bilingual word lexicons (Mikolov et al., 2013a; Gouws et al., 2015; Vuli´c et al., 2016), cross-lingual entity linking (Tsai and Roth, 2016), 247 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247–257, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics parsing (Guo et al., 2015; Johannsen et al., 2015), machine translation (Zou et al., 2013), or crosslingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016). BWE models should have two desirable properties: (P1) leverage (large) monolingual training sets tied together through a bilingual signal, (P2) use as inexpensive bilingual signal as possible in order to learn a SBWES in a scalable and widely applicable manner across languages and domains. While we provide a classification of related work, that is, different BWE models according to these properties in Sect. 2.1, the focus of this work is on a popular class of models lab"
P16-1024,C12-1089,0,0.086408,"n. (Type 1) Parallel-Only: This group of BWE models relies on sentence-aligned and/or word-aligned parallel data as the only data source (Zou et al., 2013; Hermann and Blunsom, 2014a; Koˇciský et al., 2014; Hermann and Blunsom, 2014b; Chandar et al., 2014). In addition to an expensive bilingual signal (colliding with P2), these models do not leverage larger monolingual datasets for training (not satisfying P1). 248 (Type 2) Joint Bilingual Training: These models jointly optimize two monolingual objectives, with the cross-lingual objective acting as a cross-lingual regularizer during training (Klementiev et al., 2012; Gouws et al., 2015; Soyer et al., 2015; Shi et al., 2015; Coulmance et al., 2015). The idea may be summarized by the simplified formulation (Luong et al., 2015): γ(MonoS +MonoT )+δBi. The monolingual objectives M onoS and M onoT ensure that similar words in each language are assigned similar embeddings and aim to capture the semantic structure of each language, whereas the cross-lingual objective Bi ensures that similar words across languages are assigned similar embeddings. It ties the two monolingual spaces together into a SBWES (thus satisfying P1). Parameters γ and δ govern the influence"
P16-1024,P14-2037,0,0.0693184,"Missing"
P16-1024,P15-1027,0,0.517691,", 2016). BWE models should have two desirable properties: (P1) leverage (large) monolingual training sets tied together through a bilingual signal, (P2) use as inexpensive bilingual signal as possible in order to learn a SBWES in a scalable and widely applicable manner across languages and domains. While we provide a classification of related work, that is, different BWE models according to these properties in Sect. 2.1, the focus of this work is on a popular class of models labeled Post-Hoc Mapping with Seed Lexicons. These models operate as follows (Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016): (1) two separate non-aligned monolingual embedding spaces are induced using any monolingual WE learning model (SGNS is the typical choice), (2) given a seed lexicon of word translation pairs as the bilingual signal for training, a mapping function is learned which ties the two monolingual spaces together into a SBWES. All existing work on this class of models assumes that high-quality training seed lexicons are readily available. In reality, little is understood regarding what constitutes a high quality seed lexicon, even with “traditional” distributional models (Gaussie"
P16-1024,P14-2050,0,0.219306,"rd embedding space (further SBWES) from Gouws et al. (2015). range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). Several studies have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010). Yet the widely used skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is considered as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy and Goldberg, 2014b; Levy et al., 2015). Introduction Dense real-valued vector representations of words or word embeddings (WEs) have recently gained increasing popularity in natural language processing (NLP), serving as invaluable features in a broad Research interest has recently extended to bilingual word embeddings (BWEs). BWE learning models focus on the induction of a shared bilingual word embedding space (SBWES) where words from both languages are represented in a uniform language-independent manner such that similar words (regardless of the actual language) have similar representations (see Fig. 1). A v"
P16-1024,Q15-1016,0,0.0387534,"r SBWES) from Gouws et al. (2015). range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). Several studies have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010). Yet the widely used skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is considered as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy and Goldberg, 2014b; Levy et al., 2015). Introduction Dense real-valued vector representations of words or word embeddings (WEs) have recently gained increasing popularity in natural language processing (NLP), serving as invaluable features in a broad Research interest has recently extended to bilingual word embeddings (BWEs). BWE learning models focus on the induction of a shared bilingual word embedding space (SBWES) where words from both languages are represented in a uniform language-independent manner such that similar words (regardless of the actual language) have similar representations (see Fig. 1). A variety of BWE learnin"
P16-1024,W15-1521,0,0.524887,"d Blunsom, 2014a; Koˇciský et al., 2014; Hermann and Blunsom, 2014b; Chandar et al., 2014). In addition to an expensive bilingual signal (colliding with P2), these models do not leverage larger monolingual datasets for training (not satisfying P1). 248 (Type 2) Joint Bilingual Training: These models jointly optimize two monolingual objectives, with the cross-lingual objective acting as a cross-lingual regularizer during training (Klementiev et al., 2012; Gouws et al., 2015; Soyer et al., 2015; Shi et al., 2015; Coulmance et al., 2015). The idea may be summarized by the simplified formulation (Luong et al., 2015): γ(MonoS +MonoT )+δBi. The monolingual objectives M onoS and M onoT ensure that similar words in each language are assigned similar embeddings and aim to capture the semantic structure of each language, whereas the cross-lingual objective Bi ensures that similar words across languages are assigned similar embeddings. It ties the two monolingual spaces together into a SBWES (thus satisfying P1). Parameters γ and δ govern the influence of the monolingual and bilingual components.1 The main disadvantage of Type 2 models is the costly parallel data needed for the bilingual signal (thus colliding"
P16-1024,W15-1501,0,0.0174094,"translation given the sentential context.12 Table 3 summarizes the results (Acc1 scores) in the SWTC task. NO-CONTEXT refers to the contextinsensitive majority baseline obtained by BNC+GT (i.e., it always chooses the most semantically similar translation candidate at the word type level). We also report the results of the best SWTC model from Vuli´c and Moens (2014). The results largely support the claims established with the BLL evaluation. An exter11 The same ranking of different models (with lower absolute scores) is observed when adapting the monolingual lexical substitution framework of Melamud et al. (2015) to the SWTC task as done by Vuli´c and Moens (2016). 12 The SWTC evaluation set is available online at: http://aclweb.org/anthology/attachments/D/D14/D141040.Attachment.zip 254 nal seed lexicon of BNC+GT may be safely replaced by an automatically induced inexpensive seed lexicon (as in HYBWE with BNC+HYB+SYM/ASYM). The best performing models are again BNC+HYB+SYM and HFQ+HYB+SYM. The comparison of ASYM and SYM lexicon variants further suggests that filtering translation pairs using the symmetry constraint again leads to consistent improvements, but stricter selection criteria with higher thre"
P16-1024,N16-1118,0,0.017269,"ent-level embedding space. The results in the tasks of (1) bilingual lexicon learning and (2) suggesting word translations in context demonstrate that – due to its careful selection of reliable translation pairs for seed lexicons – HYBWE outperforms benchmarking BWE induction models, all of which use more expensive bilingual signals for training. In future work, we plan to investigate other methods for seed pairs selection, settings with scarce resources (Agi´c et al., 2015; Zhang et al., 2016), other context types inspired by recent work in the monolingual settings (Levy and Goldberg, 2014a; Melamud et al., 2016), as well as model adaptations that can work with multi-word expressions. Encouraged by the excellent results, we also plan to test the portability of the approach to more language pairs, and other tasks and applications. Acknowledgments This work is supported by ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). The authors are grateful to Roi Reichart and the anonymous reviewers for their helpful comments and suggestions. 255 References Željko Agi´c, Dirk Hovy, and Anders Søgaard. 2015. If all you have is a bit of the Bible: Learning POS taggers for truly low-r"
P16-1024,S10-1002,0,0.0251863,"(Milajevs et al., 2014): Con(w) = cw1 + cw2 + . . . + cwr Table 3: Acc1 scores in the SWTC task. All seed lexicons contain 6K translation pairs, except for BNC+HYB+SYM (its sizes provided in parentheses). * denotes a statistically significant improvement over baselines and BNC+GT using McNemar’s statistical significance test with the Bonferroni correction, p &lt; 0.05. cross-lingual variant of the task proposed by Huang et al. (2012) which evaluates monolingual contextsensitive semantic similarity of words in sentential context, and it is also very related to cross-lingual lexical substitution (Mihalcea et al., 2010). To isolate the performance of each BWE induction model from the details of the SWTC setup, we use the same approach with all models: we opt for the SWTC framework proven to yield excellent results with BWEs in the SWTC task (Vuli´c and Moens, 2016). In short, the context bag Con(w) = {cw1 , . . . , cwr } is obtained by harvesting all r words that occur with w in the sentence. (4) where cwj is the embedding of the j-th context word, and Con(w) is the resulting embedding of the context bag Con(w). Finally, for each tj ∈ T C(w), the context-sensitive similarity with w is computed as: sim(w, tj"
P16-1024,D14-1079,0,0.0343167,"Missing"
P16-1024,P08-1028,0,0.0178393,"Missing"
P16-1024,C10-2174,0,0.110497,"Missing"
P16-1024,N10-1135,0,0.169773,"red lexicon pairs available (e.g., 100500)? (2) Can the Type 4 models profit from the inclusion of more seed lexicon pairs (e.g., more than 5K, even up to 40K-50K lexicon pairs)? Translation Pair Reliability When building seed lexicons through SBWES-1 (i.e., BNC+HYB and HFQ+HYB methods), it is possible to control for the reliability of translation pairs to be included in the final lexicon, with the idea that the use of only highly reliable pairs can potentially lead to an improved SBWES-2. A simple yet effective reliability reliability feature for translation pairs is the symmetry constraint (Peirsman and Padó, 2010; Vuli´c and Moens, 2013) : two words xi ∈ V S and yi ∈ V S are used as seed lexicon pairs only if they are mutual nearest neighbours given their representations in SBWES-1. The two variants of seed lexicons with only symmetric pairs are BNC+HYB+SYM and HFREQ+HYB+SYM. We also test the variants without the symmetry constraint (i.e., BNC+HYB+ASYM and HFQ+HYB+ASYM). Even more conservative reliability measures may be applied by exploiting the scores in the lists of translation candidates ranked by their similarity to the cue word xi . We investigate a symmetry constraint with a threshold: two word"
P16-1024,P15-2093,0,0.0579551,"tence-aligned and/or word-aligned parallel data as the only data source (Zou et al., 2013; Hermann and Blunsom, 2014a; Koˇciský et al., 2014; Hermann and Blunsom, 2014b; Chandar et al., 2014). In addition to an expensive bilingual signal (colliding with P2), these models do not leverage larger monolingual datasets for training (not satisfying P1). 248 (Type 2) Joint Bilingual Training: These models jointly optimize two monolingual objectives, with the cross-lingual objective acting as a cross-lingual regularizer during training (Klementiev et al., 2012; Gouws et al., 2015; Soyer et al., 2015; Shi et al., 2015; Coulmance et al., 2015). The idea may be summarized by the simplified formulation (Luong et al., 2015): γ(MonoS +MonoT )+δBi. The monolingual objectives M onoS and M onoT ensure that similar words in each language are assigned similar embeddings and aim to capture the semantic structure of each language, whereas the cross-lingual objective Bi ensures that similar words across languages are assigned similar embeddings. It ties the two monolingual spaces together into a SBWES (thus satisfying P1). Parameters γ and δ govern the influence of the monolingual and bilingual components.1 The main di"
P16-1024,D07-1070,0,0.030323,"uws et al., 2015).5 Baseline Models To induce SBWES-1, we resort to document-level embeddings of Vuli´c and Moens (2016) (Type 3). We also compare to results obtained directly by their model (BWESG) to measure the performance gains with HYBWE. To compare with a representative Type 2 model, we opt for the BilBOWA model of Gouws et al. (2015) due to its solid performance and robustness in the BLL task when trained on general-domain corpora such as Wikipedia (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets, as well as its public availabilliterature (Smith and Eisner, 2007; Tu and Honavar, 2012; Vuli´c and Moens, 2013), but we do not observe any significant gains when resorting to the more complex reliability estimates. 4 http://people.cs.kuleuven.be/~ivan.vulic/ 5 Similar trends are observed within a more lenient setting with Acc5 and Acc10 scores, but we omit these results for clarity and the fact that the actual BLL performance is best reflected in Acc1 scores (i.e., best translation only). ity.6 In short, BilBOWA combines the adapted SGNS for monolingual objectives together with a cross-lingual objective that minimizes the L2 -loss between the bag-of-word v"
P16-1024,tiedemann-2012-parallel,0,0.0315584,", 2013a).7 Training Data and Setup We use standard training data and suggested settings to obtain BWEs for all models involved in comparison. We retain the 100K most frequent words in each language for all models. To induce monolingual WE spaces, two monolingual SGNS models were trained on the cleaned and tokenized Wikipedias from the Polyglot website (Al-Rfou et al., 2013) using SGD with a global learning rate of 0.025. For BilBOWA, as in the original work (Gouws et al., 2015), the bilingual signal for the cross-lingual regularization is provided by the first 500K sentences from Europarl.v7 (Tiedemann, 2012). We use SGD with a global rate of 0.15.8 The window size is varied from 2 to 16 in steps of 2, and the best scoring model is always reported in all comparisons. BWESG was trained on the cleaned and tokenized document-aligned Wikipedias available online9 , SGD on pseudo-bilingual documents with a global rate 0.025. For BiCVM, we use the tool released by its authors10 and train on the whole Europarl.v7 for each language pair: we train an additive model, with hinge loss margin set to d (i.e., dimensionality) as in the original paper, batch size of 50, and noise parameter of 10. All BiCVM models"
P16-1024,N16-1072,0,0.0994238,"represented in a uniform language-independent manner such that similar words (regardless of the actual language) have similar representations (see Fig. 1). A variety of BWE learning models have been proposed, differing in the essential requirement of a bilingual signal necessary to construct such a SBWES (discussed later in Sect. 2). SBWES may be used to support many tasks, e.g., computing cross-lingual/multilingual semantic word similarity (Faruqui and Dyer, 2014), learning bilingual word lexicons (Mikolov et al., 2013a; Gouws et al., 2015; Vuli´c et al., 2016), cross-lingual entity linking (Tsai and Roth, 2016), 247 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247–257, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics parsing (Guo et al., 2015; Johannsen et al., 2015), machine translation (Zou et al., 2013), or crosslingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016). BWE models should have two desirable properties: (P1) leverage (large) monolingual training sets tied together through a bilingual signal, (P2) use as inexpensive bilingual signal as possible in order to learn a SBWES in a scalab"
P16-1024,D12-1121,0,0.0201055,"line Models To induce SBWES-1, we resort to document-level embeddings of Vuli´c and Moens (2016) (Type 3). We also compare to results obtained directly by their model (BWESG) to measure the performance gains with HYBWE. To compare with a representative Type 2 model, we opt for the BilBOWA model of Gouws et al. (2015) due to its solid performance and robustness in the BLL task when trained on general-domain corpora such as Wikipedia (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets, as well as its public availabilliterature (Smith and Eisner, 2007; Tu and Honavar, 2012; Vuli´c and Moens, 2013), but we do not observe any significant gains when resorting to the more complex reliability estimates. 4 http://people.cs.kuleuven.be/~ivan.vulic/ 5 Similar trends are observed within a more lenient setting with Acc5 and Acc10 scores, but we omit these results for clarity and the fact that the actual BLL performance is best reflected in Acc1 scores (i.e., best translation only). ity.6 In short, BilBOWA combines the adapted SGNS for monolingual objectives together with a cross-lingual objective that minimizes the L2 -loss between the bag-of-word vectors of parallel sen"
P16-1024,P10-1040,0,0.0247888,"pace. We perform bilingual lexicon learning (BLL) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models, all of which use more expensive bilingual signals. Effectively, we demonstrate that a SBWES may be induced by leveraging only a very weak bilingual signal (document alignments) along with monolingual data. 1 Monolingual vs Bilingual Figure 1: A toy example of a 3-dimensional monolingual vs shared bilingual word embedding space (further SBWES) from Gouws et al. (2015). range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). Several studies have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010). Yet the widely used skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is considered as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy and Goldberg, 2014b; Levy et al., 2015). Introduction Dense real-valued vector representations of word"
P16-1024,P16-1157,0,0.30501,"ion of each word w ∈ V S t V T in a SBWES as a realvalued vector: w = [f1 , . . . , fd ], where fk ∈ R denotes the value for the k-th cross-lingual feature for w within a d-dimensional SBWES. Semantic similarity sim(w, v) between two words w, v ∈ V S t V T is then computed by applying a similarity function (SF), e.g. cosine (cos) on their representations in the SBWES: sim(w, v) = SF (w, v) = cos(w, v). 2.1 Related Work: BWE Models and Bilingual Signals BWE models may be clustered into four different types according to bilingual signals used in training, and properties P1 and P2 (see Sect. 1). Upadhyay et al. (2016) provide a similar overview of recent bilingual embedding learning architectures regarding different bilingual signals required for the embedding induction. (Type 1) Parallel-Only: This group of BWE models relies on sentence-aligned and/or word-aligned parallel data as the only data source (Zou et al., 2013; Hermann and Blunsom, 2014a; Koˇciský et al., 2014; Hermann and Blunsom, 2014b; Chandar et al., 2014). In addition to an expensive bilingual signal (colliding with P2), these models do not leverage larger monolingual datasets for training (not satisfying P1). 248 (Type 2) Joint Bilingual Tr"
P16-1024,D13-1168,1,0.647672,"Missing"
P16-1024,D14-1040,1,0.726709,"Missing"
P16-1024,P16-2031,1,0.836232,"Missing"
P16-1024,N16-1156,0,0.165743,"n two monolingual embedding spaces using only highly reliable symmetric translation pairs from an inexpensive seed document-level embedding space. The results in the tasks of (1) bilingual lexicon learning and (2) suggesting word translations in context demonstrate that – due to its careful selection of reliable translation pairs for seed lexicons – HYBWE outperforms benchmarking BWE induction models, all of which use more expensive bilingual signals for training. In future work, we plan to investigate other methods for seed pairs selection, settings with scarce resources (Agi´c et al., 2015; Zhang et al., 2016), other context types inspired by recent work in the monolingual settings (Levy and Goldberg, 2014a; Melamud et al., 2016), as well as model adaptations that can work with multi-word expressions. Encouraged by the excellent results, we also plan to test the portability of the approach to more language pairs, and other tasks and applications. Acknowledgments This work is supported by ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). The authors are grateful to Roi Reichart and the anonymous reviewers for their helpful comments and suggestions. 255 References Želj"
P16-1024,D13-1141,0,0.123989,"ruct such a SBWES (discussed later in Sect. 2). SBWES may be used to support many tasks, e.g., computing cross-lingual/multilingual semantic word similarity (Faruqui and Dyer, 2014), learning bilingual word lexicons (Mikolov et al., 2013a; Gouws et al., 2015; Vuli´c et al., 2016), cross-lingual entity linking (Tsai and Roth, 2016), 247 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247–257, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics parsing (Guo et al., 2015; Johannsen et al., 2015), machine translation (Zou et al., 2013), or crosslingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016). BWE models should have two desirable properties: (P1) leverage (large) monolingual training sets tied together through a bilingual signal, (P2) use as inexpensive bilingual signal as possible in order to learn a SBWES in a scalable and widely applicable manner across languages and domains. While we provide a classification of related work, that is, different BWE models according to these properties in Sect. 2.1, the focus of this work is on a popular class of models labeled Post-Hoc Mapping with Seed Lexicons"
P16-1024,P15-2044,0,\N,Missing
P16-2084,W15-0105,0,0.0182753,"ersal NLP” initiative but also owing to the benchmarking evaluation sets for other languages beyond English (i.e., IT, DE) that have very recently become available, e.g., (Leviant and Reichart, 2015). We evaluate SGNS with different context types from sect. 2.1 across the three languages on two benchmarking tasks and datasets: (1) semantic similarity on SimLex-999 (Hill et al., 2015) translated and re-scored by native speakers in EN, DE, and IT (Leviant and Reichart, 2015), and (2) word analogies on the Google dataset (Mikolov et al., 2013a) made available in IT (Berardi et al., 2015) and DE (Köper et al., 2015) only recently. WE Induction: Data All the word representations in comparison are induced from the Polyglot Wikipedia data (Al-Rfou et al., 2013).4 UDEPS-ARC However, UDEPS-NAIVE also produces uninformative context pairs such as (telescope, with_case), and it does not specify the type of e.g. the nmod relation between discovers and telescope which are linked through the preposition with. Our intuition is that a simple post-hoc intervention into the UDEPS context extraction may yield even more focused contexts. UDEPSARC leans on the idea of arc collapsing from prior work (Levy and Goldberg, 201"
P16-2084,W13-3520,0,0.130322,"tly become available, e.g., (Leviant and Reichart, 2015). We evaluate SGNS with different context types from sect. 2.1 across the three languages on two benchmarking tasks and datasets: (1) semantic similarity on SimLex-999 (Hill et al., 2015) translated and re-scored by native speakers in EN, DE, and IT (Leviant and Reichart, 2015), and (2) word analogies on the Google dataset (Mikolov et al., 2013a) made available in IT (Berardi et al., 2015) and DE (Köper et al., 2015) only recently. WE Induction: Data All the word representations in comparison are induced from the Polyglot Wikipedia data (Al-Rfou et al., 2013).4 UDEPS-ARC However, UDEPS-NAIVE also produces uninformative context pairs such as (telescope, with_case), and it does not specify the type of e.g. the nmod relation between discovers and telescope which are linked through the preposition with. Our intuition is that a simple post-hoc intervention into the UDEPS context extraction may yield even more focused contexts. UDEPSARC leans on the idea of arc collapsing from prior work (Levy and Goldberg, 2014a; Melamud et al., 2016) that we now adjust to the UD annotation scheme. The difference to UDEPS-NAIVE is as follows: For each pair of words lin"
P16-2084,P14-2131,0,0.156916,"DTAL, University of Cambridge {iv250, alk23}@cam.ac.uk Abstract contexts (BOW). However, the underlying SGNS model is equally applicable to other context types. Recent comparative studies have demonstrated the usefulness of dependency-based contexts (DEPS) (Padó and Lapata, 2007) for the task. In comparison with BOW, syntactic contexts steer the induced semantic spaces towards functional similarity (e.g., tiger:cat) rather than towards topical similarity/relatedness (e.g., tiger:jungle). DEPS-based embeddings outperform the less informed BOW-based embeddings in a variety of similarity tasks (Bansal et al., 2014; Levy and Goldberg, 2014a; Hill et al., 2015; Melamud et al., 2016). However, these studies have all focused solely on English. A comparison extending to additional languages is required before any cross-lingual generalisations can be drawn. Following recent initiatives on languageagnostic and cross-linguistically consistent universal natural language processing (i.e., universal POS (UPOS) tagging and dependency (UD) parsing) (Nivre et al., 2015), this paper is concerned with two important questions: (Q1) Can one usefully replace the DEPS extraction pipeline optimised for tools developed for"
P16-2084,P14-1023,0,0.055475,"o consistent improvements across languages. 1 Introduction Dense real-valued distributed representations of words known as word embeddings (WEs) have become ubiquitous in NLP, serving as invaluable features in a broad range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is still considered the stateof-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original implementation of SGNS learns word representations from local bag-of-words 518 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 518–524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ful UDEPS contexts leads to consistent improvements across languages, especially in detecting functional similarity. This focused contribution is the first crosslinguistic comparison of different context types for learning word representations in three languages, reaching beyond English. It also"
P16-2084,P14-2050,0,0.0843096,"Cambridge {iv250, alk23}@cam.ac.uk Abstract contexts (BOW). However, the underlying SGNS model is equally applicable to other context types. Recent comparative studies have demonstrated the usefulness of dependency-based contexts (DEPS) (Padó and Lapata, 2007) for the task. In comparison with BOW, syntactic contexts steer the induced semantic spaces towards functional similarity (e.g., tiger:cat) rather than towards topical similarity/relatedness (e.g., tiger:jungle). DEPS-based embeddings outperform the less informed BOW-based embeddings in a variety of similarity tasks (Bansal et al., 2014; Levy and Goldberg, 2014a; Hill et al., 2015; Melamud et al., 2016). However, these studies have all focused solely on English. A comparison extending to additional languages is required before any cross-lingual generalisations can be drawn. Following recent initiatives on languageagnostic and cross-linguistically consistent universal natural language processing (i.e., universal POS (UPOS) tagging and dependency (UD) parsing) (Nivre et al., 2015), this paper is concerned with two important questions: (Q1) Can one usefully replace the DEPS extraction pipeline optimised for tools developed for English with a pipeline t"
P16-2084,W14-1618,0,0.0984323,"Cambridge {iv250, alk23}@cam.ac.uk Abstract contexts (BOW). However, the underlying SGNS model is equally applicable to other context types. Recent comparative studies have demonstrated the usefulness of dependency-based contexts (DEPS) (Padó and Lapata, 2007) for the task. In comparison with BOW, syntactic contexts steer the induced semantic spaces towards functional similarity (e.g., tiger:cat) rather than towards topical similarity/relatedness (e.g., tiger:jungle). DEPS-based embeddings outperform the less informed BOW-based embeddings in a variety of similarity tasks (Bansal et al., 2014; Levy and Goldberg, 2014a; Hill et al., 2015; Melamud et al., 2016). However, these studies have all focused solely on English. A comparison extending to additional languages is required before any cross-lingual generalisations can be drawn. Following recent initiatives on languageagnostic and cross-linguistically consistent universal natural language processing (i.e., universal POS (UPOS) tagging and dependency (UD) parsing) (Nivre et al., 2015), this paper is concerned with two important questions: (Q1) Can one usefully replace the DEPS extraction pipeline optimised for tools developed for English with a pipeline t"
P16-2084,Q15-1016,0,0.244474,"ents across languages. 1 Introduction Dense real-valued distributed representations of words known as word embeddings (WEs) have become ubiquitous in NLP, serving as invaluable features in a broad range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is still considered the stateof-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original implementation of SGNS learns word representations from local bag-of-words 518 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 518–524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ful UDEPS contexts leads to consistent improvements across languages, especially in detecting functional similarity. This focused contribution is the first crosslinguistic comparison of different context types for learning word representations in three languages, reaching beyond English. It also constitutes a first"
P16-2084,P13-2109,0,0.169567,"Missing"
P16-2084,P10-1131,0,0.0328108,"universal Stanford dependencies (de Marneffe et al., 2014) complemented with the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typologies) by capturing structural similarities across languages and by exploiting connections that exist naturally between them (Berg-Kirkpatrick and Klein, 2010; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012). Here, we test the ability of such a universal annotation scheme to encode potentially useful semantic knowledge crosslinguistically; in this case, to yield more informed UDEPS contexts for improved word embeddings. The extraction of UDEPS as the new variant of dependency-based contexts is completely language-agnostic on purpose: exactly the same procedure is followed for each language in comparison in order to make the representation learning framework completely universal. 2.1 nsubj tion of word2vec which is capable of learnin"
P16-2084,D11-1006,0,0.0299181,"e Marneffe et al., 2014) complemented with the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typologies) by capturing structural similarities across languages and by exploiting connections that exist naturally between them (Berg-Kirkpatrick and Klein, 2010; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012). Here, we test the ability of such a universal annotation scheme to encode potentially useful semantic knowledge crosslinguistically; in this case, to yield more informed UDEPS contexts for improved word embeddings. The extraction of UDEPS as the new variant of dependency-based contexts is completely language-agnostic on purpose: exactly the same procedure is followed for each language in comparison in order to make the representation learning framework completely universal. 2.1 nsubj tion of word2vec which is capable of learning from arbitrary (word,"
P16-2084,C10-1011,0,0.0884812,"rs_case_with−1 (Fig. 1). In addition, we remove the uninformative case arc and its associated contexts: (with, telescope_case−1 ), (telescope, with_case) from the training pairs. UPOS Tagging and UD Parsing The Wikipedia corpora were UPOS-tagged using a state-of-the art system TurboTagger (Martins et al., 2013).5 TurboTagger was trained using suggested settings without any further parameter fine-tuning (SVM MIRA with 20 iterations) on the TRAIN + DEV portion of the UD treebank annotated with UPOS tags. Following that, the Wikipedia data were UD-parsed6 using the graph-based Mate parser v3.61 (Bohnet, 2010)7 and the same regime: suggested settings on the TRAIN + DEV UD treebank portion.8 The performance of the models measured on the TEST portion of the UD treebanks is reported in Tab. 1. 4 https://sites.google.com/site/rmyeid/projects/polyglot http://www.cs.cmu.edu/ ark/TurboParser/ 6 Besides EN, DE, and IT, we also UPOS-tagged and UDparsed Wikipedias in NL, ES, and HR. We believe that the full UPOS-tagged and UD-parsed Wikipedias in six languages are a valuable asset for future research and we plan to make the resource publicly available at: http://ltl.mml.cam.ac.uk/resources/ 7 https://code.go"
P16-2084,P13-2017,0,0.028495,"ased contexts from UD parses (UDEPS) in English and Italian. Top: the example sentence in English taken from (Levy and Goldberg, 2014a), now UD-parsed. Middle: the same sentence in Italian, UD-parsed. Note the very similar structure of the two parses. Bottom: the intuition behind UDEPS-ARC. The uninformative shortrange case arc between with and telescope is removed, and another “pseudo-arc” now specifying the exact link type (i.e., case_with) between discovers and telescope is added. Universal Multilingual Resources The departure point in our experiments is the Universal Dependencies project (McDonald et al., 2013; Nivre et al., 2015) which develops crosslinguistically consistent treebank annotation.1 The annotation scheme leans on the universal Stanford dependencies (de Marneffe et al., 2014) complemented with the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typolo"
P16-2084,D14-1082,0,0.130132,"the universal DEPS (UDEPS) are useful for detecting functional similarity (e.g., verb similarity, solving syntactic analogies) among languages, but their advantage over BOW is not as prominent as previously reported on English. We also show that simple “post-parsing” filtering of useful UDEPS contexts leads to consistent improvements across languages. 1 Introduction Dense real-valued distributed representations of words known as word embeddings (WEs) have become ubiquitous in NLP, serving as invaluable features in a broad range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is still considered the stateof-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original implementation of SGNS learns word representations from local bag-of-words 518 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 518–524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Lin"
P16-2084,P15-1038,0,0.01382,"ted in Tab. 1. 4 https://sites.google.com/site/rmyeid/projects/polyglot http://www.cs.cmu.edu/ ark/TurboParser/ 6 Besides EN, DE, and IT, we also UPOS-tagged and UDparsed Wikipedias in NL, ES, and HR. We believe that the full UPOS-tagged and UD-parsed Wikipedias in six languages are a valuable asset for future research and we plan to make the resource publicly available at: http://ltl.mml.cam.ac.uk/resources/ 7 https://code.google.com/archive/p/mate-tools/ 8 We opted for the Mate parser due to its speed, simplicity, and state-of-the-art performance according to very recent parser evaluations (Choi et al., 2015). 5 3 Results with another context type relying on substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015) are omitted due to its subpar performance in our experiments as well as across a variety of semantic tasks in a recent Englishfocused study (Melamud et al., 2016). 520 Spearman’s ρ 0.40 0.35 0.40 0.35 0.30 0.35 0.30 0.25 0.30 0.25 0.20 0.25 0.20 UDEPS-NAIVE UDEPS-ARC BOW-2 POSIT-2 500 600 0.15 50 100 300 0.20 0.15 UDEPS-NAIVE UDEPS-ARC BOW-2 POSIT-2 500 600 0.10 50 100 300 d UDEPS-NAIVE UDEPS-ARC BOW-2 POSIT-2 0.14 50 100 d (a) English 300 500 600 d (b) German (c) Italian Figure 2:"
P16-2084,N15-1050,0,0.0876616,"Missing"
P16-2084,D11-1005,0,0.0193508,") complemented with the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typologies) by capturing structural similarities across languages and by exploiting connections that exist naturally between them (Berg-Kirkpatrick and Klein, 2010; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012). Here, we test the ability of such a universal annotation scheme to encode potentially useful semantic knowledge crosslinguistically; in this case, to yield more informed UDEPS contexts for improved word embeddings. The extraction of UDEPS as the new variant of dependency-based contexts is completely language-agnostic on purpose: exactly the same procedure is followed for each language in comparison in order to make the representation learning framework completely universal. 2.1 nsubj tion of word2vec which is capable of learning from arbitrary (word, context) pairs.2 Ke"
P16-2084,N16-1118,0,0.471559,"ontexts (BOW). However, the underlying SGNS model is equally applicable to other context types. Recent comparative studies have demonstrated the usefulness of dependency-based contexts (DEPS) (Padó and Lapata, 2007) for the task. In comparison with BOW, syntactic contexts steer the induced semantic spaces towards functional similarity (e.g., tiger:cat) rather than towards topical similarity/relatedness (e.g., tiger:jungle). DEPS-based embeddings outperform the less informed BOW-based embeddings in a variety of similarity tasks (Bansal et al., 2014; Levy and Goldberg, 2014a; Hill et al., 2015; Melamud et al., 2016). However, these studies have all focused solely on English. A comparison extending to additional languages is required before any cross-lingual generalisations can be drawn. Following recent initiatives on languageagnostic and cross-linguistically consistent universal natural language processing (i.e., universal POS (UPOS) tagging and dependency (UD) parsing) (Nivre et al., 2015), this paper is concerned with two important questions: (Q1) Can one usefully replace the DEPS extraction pipeline optimised for tools developed for English with a pipeline that relies on languageuniversal syntactic p"
P16-2084,de-marneffe-etal-2014-universal,0,0.0728399,"Missing"
P16-2084,P12-1066,0,0.0306358,"the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typologies) by capturing structural similarities across languages and by exploiting connections that exist naturally between them (Berg-Kirkpatrick and Klein, 2010; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012). Here, we test the ability of such a universal annotation scheme to encode potentially useful semantic knowledge crosslinguistically; in this case, to yield more informed UDEPS contexts for improved word embeddings. The extraction of UDEPS as the new variant of dependency-based contexts is completely language-agnostic on purpose: exactly the same procedure is followed for each language in comparison in order to make the representation learning framework completely universal. 2.1 nsubj tion of word2vec which is capable of learning from arbitrary (word, context) pairs.2 Keeping the representati"
P16-2084,J15-4004,1,0.900938,"am.ac.uk Abstract contexts (BOW). However, the underlying SGNS model is equally applicable to other context types. Recent comparative studies have demonstrated the usefulness of dependency-based contexts (DEPS) (Padó and Lapata, 2007) for the task. In comparison with BOW, syntactic contexts steer the induced semantic spaces towards functional similarity (e.g., tiger:cat) rather than towards topical similarity/relatedness (e.g., tiger:jungle). DEPS-based embeddings outperform the less informed BOW-based embeddings in a variety of similarity tasks (Bansal et al., 2014; Levy and Goldberg, 2014a; Hill et al., 2015; Melamud et al., 2016). However, these studies have all focused solely on English. A comparison extending to additional languages is required before any cross-lingual generalisations can be drawn. Following recent initiatives on languageagnostic and cross-linguistically consistent universal natural language processing (i.e., universal POS (UPOS) tagging and dependency (UD) parsing) (Nivre et al., 2015), this paper is concerned with two important questions: (Q1) Can one usefully replace the DEPS extraction pipeline optimised for tools developed for English with a pipeline that relies on langua"
P16-2084,J07-2002,0,0.0522346,"Missing"
P16-2084,petrov-etal-2012-universal,0,0.0602513,"om: the intuition behind UDEPS-ARC. The uninformative shortrange case arc between with and telescope is removed, and another “pseudo-arc” now specifying the exact link type (i.e., case_with) between discovers and telescope is added. Universal Multilingual Resources The departure point in our experiments is the Universal Dependencies project (McDonald et al., 2013; Nivre et al., 2015) which develops crosslinguistically consistent treebank annotation.1 The annotation scheme leans on the universal Stanford dependencies (de Marneffe et al., 2014) complemented with the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typologies) by capturing structural similarities across languages and by exploiting connections that exist naturally between them (Berg-Kirkpatrick and Klein, 2010; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012). Here, we test the ability of su"
P16-2084,K15-1026,0,0.197455,"e actual language: the claims made for English (i.e., DEPS ≥ BOW) do not extend to other languages (Q2). A comparison of results from Tab. 1 with the task evaluation also shows that excellent tagging and parsing results do not guarantee a strong task performance. The results over the verb subset of SimLex also reveal that claims established with English are not necessarily general and true with other languages. For instance, while it has been noted that modeling verb similarity is indeed a difficult problem in English as evidenced by lower correlation scores on SimLex (see Fig. 2(a) and e.g. (Schwartz et al., 2015)), verbs are apparently easier to model in Italian (Fig. 2(c)), and a real challenge in German, The results are consistent with prior work on the UD treebanks, e.g., (Tiedemann, 2015). Training Setup The SGNS preprocessing scheme for English was replicated from (Levy and Goldberg, 2014a) and extended to the other two languages: all tokens were converted to lowercase, and words and contexts that appeared less than 100 times were filtered. Exactly the same vocabularies were used with all context types (approx. 185K distinct EN words, 163K DE words, and 83K IT words). The word2vecf SGNS was train"
P16-2084,W15-2137,0,0.0267924,"nt tagging and parsing results do not guarantee a strong task performance. The results over the verb subset of SimLex also reveal that claims established with English are not necessarily general and true with other languages. For instance, while it has been noted that modeling verb similarity is indeed a difficult problem in English as evidenced by lower correlation scores on SimLex (see Fig. 2(a) and e.g. (Schwartz et al., 2015)), verbs are apparently easier to model in Italian (Fig. 2(c)), and a real challenge in German, The results are consistent with prior work on the UD treebanks, e.g., (Tiedemann, 2015). Training Setup The SGNS preprocessing scheme for English was replicated from (Levy and Goldberg, 2014a) and extended to the other two languages: all tokens were converted to lowercase, and words and contexts that appeared less than 100 times were filtered. Exactly the same vocabularies were used with all context types (approx. 185K distinct EN words, 163K DE words, and 83K IT words). The word2vecf SGNS was trained using standard settings: 15 epochs, 15 negative samples, global learning rate 0.025, subsampling rate 1e − 4. All WEs were trained with d = 50, 100, 300, 500, 600. BOW-based WEs we"
P16-2084,P10-1040,0,0.143642,"cific optimization. Our results suggest that the universal DEPS (UDEPS) are useful for detecting functional similarity (e.g., verb similarity, solving syntactic analogies) among languages, but their advantage over BOW is not as prominent as previously reported on English. We also show that simple “post-parsing” filtering of useful UDEPS contexts leads to consistent improvements across languages. 1 Introduction Dense real-valued distributed representations of words known as word embeddings (WEs) have become ubiquitous in NLP, serving as invaluable features in a broad range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is still considered the stateof-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original implementation of SGNS learns word representations from local bag-of-words 518 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 518–524, c Berlin, Germany, August 7"
P16-2084,D12-1086,0,0.182465,"Missing"
P16-2084,I08-3008,0,0.0595784,"ween with and telescope is removed, and another “pseudo-arc” now specifying the exact link type (i.e., case_with) between discovers and telescope is added. Universal Multilingual Resources The departure point in our experiments is the Universal Dependencies project (McDonald et al., 2013; Nivre et al., 2015) which develops crosslinguistically consistent treebank annotation.1 The annotation scheme leans on the universal Stanford dependencies (de Marneffe et al., 2014) complemented with the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typologies) by capturing structural similarities across languages and by exploiting connections that exist naturally between them (Berg-Kirkpatrick and Klein, 2010; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012). Here, we test the ability of such a universal annotation scheme to encode potentially useful semantic knowledge"
P17-1006,W16-1603,0,0.0323402,"ll and Schütze, 2015; Bhatia et al., 2016, i.a.). The key idea is to learn a morphological composition function (Lazaridou et al., 2013; Cotterell and Schütze, 2017) which synthesises the representation of a word given the representations of its constituent morphemes. Contrary to our work, these models typically coalesce all lexical relations. Another class of models, operating at the character level, shares a similar methodology: such models compose token-level representations from subcomponent embeddings (subwords, morphemes, or characters) (dos Santos and Zadrozny, 2014; Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016; Acknowledgments This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). RR is supported by the IntelICRI grant: Hybrid Models for Minimally Supervised Information Extraction from Conversations. The authors are grateful to the anonymous reviewers for their helpful suggestions. 64 References Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research 12:2493–2537. http://dl.acm.org/citation"
P17-1006,ehrmann-etal-2014-representing,0,0.0133747,"r verb conjugation (aspettare / aspettiamo); (3) regular formation of past participle (aspettare / aspettato); and (4) rules regarding grammatical gender (bianco / bianca). Besides these, another set of rules is used for German and Russian: (5) regular declension (e.g., asiatisch / asiatischem). Table 3: Vocabulary sizes and counts of ATTRACT (A) and R EPEL (R) constraints. constraints. These can be extracted from a variety of semantic databases such as WordNet (Fellbaum, 1998), the Paraphrase Database (Ganitkevitch et al., 2013; Pavlick et al., 2015), or BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014) as done in prior work (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2016, i.a.). In this work, we investigate another option: extracting constraints without curated knowledge bases in a spectrum of languages by exploiting inherent language-specific properties related to linguistic morphology. This relaxation ensures a wider portability of ATTRACTR EPEL to languages and domains without readily available or adequate resources. Extracting R EPEL Pairs As another source of implicit semantic signals, W also contains words which represent derivational antonyms: e.g., two words that d"
P17-1006,D14-1082,0,0.0120453,"roposed method does not require curated knowledge bases or gold lexicons. Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the Introduction Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 56 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1006 en_expensive costly costlier cheaper prohibitively pricey expensiveness costly costlier ruinously unaffordable de_teure teuren kostspielige aufwändige kostenintensive aufwendige teures teuren teurem teurer teurerer it_costoso dispendioso remu"
P17-1006,W14-4340,1,0.939514,"impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkši´c"
P17-1006,N15-1184,0,0.635015,"aduale lenti lente lenta veloce rapido en_book books memoir novel storybooks blurb booked rebook booking rebooked books de_buch sachbuch buches romandebüt büchlein pamphlet bücher büch büche büches büchen it_libro romanzo racconto volumetto saggio ecclesiaste libri libra librare libre librano Table 1: The nearest neighbours of three example words (expensive, slow and book) in English, German and Italian before (top) and after (bottom) morph-fitting. proliferation of word forms in morphologically rich languages. Formalised as an instance of the post-processing semantic specialisation paradigm (Faruqui et al., 2015; Mrkši´c et al., 2016), morphfitting is steered by a set of linguistic constraints derived from simple language-specific rules which describe (a subset of) morphological processes in a language. The constraints emphasise similarity on one side (e.g., by extracting morphological synonyms), and antonymy on the other (by extracting morphological antonyms), see Fig. 1 and Tab. 2. The key idea of the fine-tuning process is to pull synonymous examples described by the constraints closer together in the transformed vector space, while at the same time pushing antonymous examples away from each other"
P17-1006,E14-1049,0,0.198571,"contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). We also experiment with standard well-known distributional spaces in other languages (IT and DE ), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vuli´c and Korhonen, 2016a). 4 Intrinsic Evaluation: Word Similarity Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman’s rank correlation as the evaluation metric. First, we use the SimLex-999 dataset, as well as SimVerb-3500, a recent EN verb pair similarity dataset providing similarity ratings for 3,500 verb pairs.7 SimLex-"
P17-1006,N16-1077,0,0.0279301,"nally generating incorrect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a"
P17-1006,N15-1070,0,0.0942068,"approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016). Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them to better reflect true semantic similarity, and learning more accurate representations for low-frequency words. 7 Conclusion and Future Work We have presented a novel morph-fitting method which injects morpholog"
P17-1006,D15-1245,0,0.0180451,"require curated knowledge bases or gold lexicons. Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the Introduction Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 56 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1006 en_expensive costly costlier cheaper prohibitively pricey expensiveness costly costlier ruinously unaffordable de_teure teuren kostspielige aufwändige kostenintensive aufwendige teures teuren teurem teurer teurerer it_costoso dispendioso remunerativo redditizio risch"
P17-1006,E17-1049,0,0.024409,"rect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model whi"
P17-1006,D15-1242,0,0.404513,"Missing"
P17-1006,N13-1092,0,0.182887,"Missing"
P17-1006,D16-1235,1,0.903455,"Missing"
P17-1006,P13-1149,0,0.162129,"ig. 1). The final A and R constraint counts are given in Tab. 3. The full sets of rules are available as supplemental material. Extracting ATTRACT Pairs The core difference between inflectional and derivational morphology can be summarised in a few lines as follows: the former refers to a set of processes through which the word form expresses meaningful syntactic information, e.g., verb tense, without any change to the semantics of the word. On the other hand, the latter refers to the formation of new words with semantic shifts in meaning (Schone and Jurafsky, 2001; Haspelmath and Sims, 2013; Lazaridou et al., 2013; Zeller et al., 2013; Cotterell and Schütze, 2017). For the ATTRACT constraints, we focus on inflectional rather than on derivational morphology rules as the former preserve the full meaning of a word, modifying it only to reflect grammatical roles such as verb tense or case markers (e.g., (en_read, en_reads) or (de_katalanisch, de_katalanischer)). This choice is guided by our intent to fine-tune the original vector space in order to improve the embedded semantic relations. We define two rules for English, widely recognised as morphologically simple (Avramidis and Koehn, 2008; Cotterell et al"
P17-1006,W13-4066,0,0.0234804,", DE and IT are trained using four variants of the SGNS - LARGE vectors: 1) the initial distributional vectors; 2) morph-fixed vectors; 3) and 4) the two variants of morph-fitted vectors (see Sect. 3). As shown by Mrkši´c et al. (2017b), semantic specialisation of the employed word vectors benThe Dialogue State Tracking Challenge (DSTC) shared task series formalised the evaluation and provided labelled DST datasets (Henderson et al., 2014a,b; Williams et al., 2016). While a plethora of DST models are available based on, e.g., handcrafted rules (Wang et al., 2014) or conditional random fields (Lee and Eskenazi, 2013), the recent DST methodology has seen a shift towards neural62 0.45 0.45 0.85 0.85 SimLex 0.75 0.30 0.70 0.25 0.65 0.20 0.15 Distrib MFix MFit-A MFit-AR SimLex (Spearman’s ρ) SimLex (Spearman’s ρ) 0.35 0.40 0.60 0.35 0.75 0.30 0.70 0.25 0.65 0.20 0.15 (a) English 0.80 Distrib MFix MFit-A MFit-AR DST Performance (Joint) 0.80 DST Performance (Joint) 0.40 DST 0.60 (b) German 0.45 0.85 0.45 0.35 0.75 0.30 0.70 0.25 0.65 0.20 0.15 Distrib MFix MFit-A MFit-AR SimLex SimLex (Spearman’s ρ) SimLex (Spearman’s ρ) 0.80 DST Performance (Joint) 0.40 0.40 0.35 0.30 0.25 0.20 0.15 0.60 (c) Italian Distrib MF"
P17-1006,W14-4337,0,0.147086,"Missing"
P17-1006,W13-4065,0,0.0658382,"Missing"
P17-1006,Q17-1022,1,0.883478,"Missing"
P17-1006,E17-1001,0,0.0359627,"We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkši´c et al., 2017a). The NBT learns to compose these vectors into intermediate utterance and conte"
P17-1006,P15-1145,0,0.280709,"Missing"
P17-1006,W15-1521,0,0.150173,"= bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). We also experiment with standard well-known distributional spaces in other languages (IT and DE ), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vuli´c and Korhonen, 2016a). 4 Intrinsic Evaluation: Word Similarity Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman’s rank correlation as the evaluation metric. First, we use the SimLex-999 dataset, as well as SimVerb-3500, a recent EN verb pair similarity dataset providing similarity"
P17-1006,P16-2074,0,0.0671559,"ion into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016). Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them to better reflect true semantic similarity, and learning more accurate representations for low-frequency words. 7 Conclusion and Future Work We have presented a novel morph-fitting method which injects morphological knowledge in the form of linguistic co"
P17-1006,W13-3512,0,0.0917221,"ts of the post-processing specialisation algorithm and the constraint selection. Word Vectors and Morphology The use of morphological resources to improve the representations of morphemes and words is an active area of research. The majority of proposed architectures encode morphological information, provided either as gold standard morphological resources (SylakGlassman et al., 2015) such as CELEX (Baayen et al., 1995) or as an external analyser such as Morfessor (Creutz and Lagus, 2007), along with distributional information jointly at training time in the language modelling (LM) objective (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Schütze, 2015; Bhatia et al., 2016, i.a.). The key idea is to learn a morphological composition function (Lazaridou et al., 2013; Cotterell and Schütze, 2017) which synthesises the representation of a word given the representations of its constituent morphemes. Contrary to our work, these models typically coalesce all lexical relations. Another class of models, operating at the character level, shares a similar methodology: such models compose token-level representations from subcomponent embeddings (subwords, morphemes, or characters)"
P17-1006,Q16-1030,0,0.239528,"ty and evaluate morph-fitting in a well-defined downstream task where the artefacts of the distributional hypothesis are known to prompt statistical system failures. Related Work Semantic Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016). Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them"
P17-1006,K16-1006,0,0.171359,"al models: Common-Crawl GloVe (Pennington et al., 2014), SGNS vectors (Mikolov et al., 2013) with various contexts (BOW = bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). We also experiment with standard well-known distributional spaces in other languages (IT and DE ), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vuli´c and Korhonen, 2016a). 4 Intrinsic Evaluation: Word Similarity Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman’s rank correlation as the evaluation metric. First, w"
P17-1006,P15-2070,0,0.0998422,"Missing"
P17-1006,D14-1162,0,0.0854243,"each language are provided in Tab. 3.6 We label these collections of vectors SGNS - LARGE. all of our intrinsic and extrinsic experiments. Morph-fitting Variants We analyse two variants of morph-fitting: (1) using ATTRACT constraints only (MF IT-A), and (2) using both ATTRACT and R EPEL constraints (MF IT-AR). Other Starting Distributional Vectors We also analyse the impact of morph-fitting on other collections of well-known EN word vectors. These vectors have varying vocabulary coverage and are trained with different architectures. We test standard distributional models: Common-Crawl GloVe (Pennington et al., 2014), SGNS vectors (Mikolov et al., 2013) with various contexts (BOW = bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN"
P17-1006,E17-1029,0,0.030739,"sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkši´c et al., 2017a). The NBT learns to compose these vectors into intermediat"
P17-1006,P15-2130,1,0.859159,"Missing"
P17-1006,C14-1015,0,0.0339755,"rithm and the constraint selection. Word Vectors and Morphology The use of morphological resources to improve the representations of morphemes and words is an active area of research. The majority of proposed architectures encode morphological information, provided either as gold standard morphological resources (SylakGlassman et al., 2015) such as CELEX (Baayen et al., 1995) or as an external analyser such as Morfessor (Creutz and Lagus, 2007), along with distributional information jointly at training time in the language modelling (LM) objective (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Schütze, 2015; Bhatia et al., 2016, i.a.). The key idea is to learn a morphological composition function (Lazaridou et al., 2013; Cotterell and Schütze, 2017) which synthesises the representation of a word given the representations of its constituent morphemes. Contrary to our work, these models typically coalesce all lexical relations. Another class of models, operating at the character level, shares a similar methodology: such models compose token-level representations from subcomponent embeddings (subwords, morphemes, or characters) (dos Santos and Zadrozny, 2014; Ling et al"
P17-1006,N01-1024,0,0.0956175,"constraints such as (rispettosa, irrispettosi) (see Fig. 1). The final A and R constraint counts are given in Tab. 3. The full sets of rules are available as supplemental material. Extracting ATTRACT Pairs The core difference between inflectional and derivational morphology can be summarised in a few lines as follows: the former refers to a set of processes through which the word form expresses meaningful syntactic information, e.g., verb tense, without any change to the semantics of the word. On the other hand, the latter refers to the formation of new words with semantic shifts in meaning (Schone and Jurafsky, 2001; Haspelmath and Sims, 2013; Lazaridou et al., 2013; Zeller et al., 2013; Cotterell and Schütze, 2017). For the ATTRACT constraints, we focus on inflectional rather than on derivational morphology rules as the former preserve the full meaning of a word, modifying it only to reflect grammatical roles such as verb tense or case markers (e.g., (en_read, en_reads) or (de_katalanisch, de_katalanischer)). This choice is guided by our intent to fine-tune the original vector space in order to improve the embedded semantic relations. We define two rules for English, widely recognised as morphologically"
P17-1006,P17-1163,1,0.882418,"Missing"
P17-1006,K15-1026,1,0.916649,"Simple Language-Specific Rules Ivan Vuli´c1 , Nikola Mrkši´c1 , Roi Reichart2 Diarmuid Ó Séaghdha3 , Steve Young1 , Anna Korhonen1 1 2 3 University of Cambridge Technion, Israel Institute of Technology Apple Inc. {iv250,nm480,sjy11,alk23}@cam.ac.uk doseaghdha@apple.com roiri@ie.technion.ac.il Abstract 2011). Most prominent word representation techniques are grounded in the distributional hypothesis (Harris, 1954), relying on word co-occurrence information in large textual corpora (Curran, 2004; Turney and Pantel, 2010; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Schwartz et al., 2015, i.a.). Morphologically rich languages, in which “substantial grammatical information. . . is expressed at word level” (Tsarfaty et al., 2010), pose specific challenges for NLP. This is not always considered when techniques are evaluated on languages such as English or Chinese, which do not have rich morphology. In the case of distributional vector space models, morphological complexity brings two challenges to the fore: Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for lowfrequency word f"
P17-1006,N16-1060,1,0.847607,"analyse the impact of morph-fitting on other collections of well-known EN word vectors. These vectors have varying vocabulary coverage and are trained with different architectures. We test standard distributional models: Common-Crawl GloVe (Pennington et al., 2014), SGNS vectors (Mikolov et al., 2013) with various contexts (BOW = bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). We also experiment with standard well-known distributional spaces in other languages (IT and DE ), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vuli´c and Korhonen, 2016a). 4 Intrinsic Evaluation: Word Similarity"
P17-1006,N15-1186,0,0.023404,"ge-specific rules does come at a cost of occasionally generating incorrect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological v"
P17-1006,E17-1042,1,0.846566,"Missing"
P17-1006,Q15-1025,0,0.601637,"redite) (dressed, undressed) (similar, dissimilar) (formality, informality) (stabil, unstabil) (geformtes, ungeformt) (relevant, irrelevant) (abitata, inabitato) (realtà, irrealtà) (attuato, inattuato) words from the in-batch ATTRACT constraints to be closer to one another than to any other word in the current mini-batch. The second term pushes antonyms away from each other. If (xl , xr ) ∈ BR is the current minibatch of R EPEL constraints, this term can be expressed as follows: The ATTRACT-R EPEL model, proposed by Mrkši´c et al. (2017b), is an extension of the PARAGRAM procedure proposed by Wieting et al. (2015). It provides a generic framework for incorporating similarity (e.g. successful and accomplished) and antonymy constraints (e.g. nimble and clumsy) into pre-trained word vectors. Given the initial vector space and collections of ATTRACT and R EPEL constraints A and R, the model gradually modifies the space to bring the designated word vectors closer together or further apart. The method’s cost function consists of three terms. The first term pulls the ATTRACT examples (xl , xr ) ∈ A closer together. If BA denotes the current mini-batch of ATTRACT examples, this term can be expressed as: X Germ"
P17-1006,P16-1230,1,0.801978,"Missing"
P17-1006,D16-1157,0,0.0330579,"→ 66.3 (MF IT-AR), setting a new state-of-the-art score for both datasets. The morph-fixed vectors do not enhance DST performance, probably because fixing word vectors to their highest frequency inflectional form eliminates useful semantic content encoded in the original vectors. On the other hand, morph-fitting makes use of this information, supplementing it with semantic relations between different morphological forms. These conclusions are in line with the SimLex gains, where morph-fitting outperforms both distributional and morph-fixed vectors. 63 spaces for extrinsic tasks such as DST. 6 Wieting et al., 2016; Verwimp et al., 2017, i.a.). In contrast to prior work, our model decouples the use of morphological information, now provided in the form of inflectional and derivational rules transformed into constraints, from the actual training. This pipelined approach results in a simpler, more portable model. In spirit, our work is similar to Cotterell et al. (2016b), who formulate the idea of post-training specialisation in a generative Bayesian framework. Their work uses gold morphological lexicons; we show that competitive performance can be achieved using a non-exhaustive set of simple rules. Our"
P17-1006,P15-2111,0,0.0142836,"models from the literature in lieu of ATTRACT-R EPEL using the same set of “morphological” synonymy and antonymy constraints. We compare ATTRACT-R EPEL to the retrofitting model Further Discussion The simplicity of the used language-specific rules does come at a cost of occasionally generating incorrect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existi"
P17-1006,W10-1401,0,0.0148159,"rsity of Cambridge Technion, Israel Institute of Technology Apple Inc. {iv250,nm480,sjy11,alk23}@cam.ac.uk doseaghdha@apple.com roiri@ie.technion.ac.il Abstract 2011). Most prominent word representation techniques are grounded in the distributional hypothesis (Harris, 1954), relying on word co-occurrence information in large textual corpora (Curran, 2004; Turney and Pantel, 2010; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Schwartz et al., 2015, i.a.). Morphologically rich languages, in which “substantial grammatical information. . . is expressed at word level” (Tsarfaty et al., 2010), pose specific challenges for NLP. This is not always considered when techniques are evaluated on languages such as English or Chinese, which do not have rich morphology. In the case of distributional vector space models, morphological complexity brings two challenges to the fore: Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for lowfrequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for languag"
P17-1006,P10-1040,0,0.0585254,"ervation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the Introduction Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 56 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1006 en_expensive costly costlier cheaper prohibitively pricey expensiveness costly costlier ruinously unaffordable de_teure teuren kostspielige aufwändige kostenintensive aufwendige teures teuren teurem teurer teurerer it_costoso dispendioso remunerativo redditizio rischioso costosa costosa costose costosi dispendioso dispendiose en_slow fast slow"
P17-1006,P14-2089,0,0.265019,"nd naturally extends to constraints from other sources (e.g., WordNet) in future work. Another practical difference is that we focus on similarity and evaluate morph-fitting in a well-defined downstream task where the artefacts of the distributional hypothesis are known to prompt statistical system failures. Related Work Semantic Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016). Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological"
P17-1006,P13-1118,0,0.0513226,"Missing"
P17-1006,E17-1040,0,0.0904069,"Missing"
P17-1006,E17-2033,0,0.0793456,"uct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkši´c et al., 2017a). The NBT learns to compose these vectors into intermediate utterance and context representations. Th"
P17-1006,D13-1141,0,0.035066,"ns. Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the Introduction Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 56 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1006 en_expensive costly costlier cheaper prohibitively pricey expensiveness costly costlier ruinously unaffordable de_teure teuren kostspielige aufwändige kostenintensive aufwendige teures teuren teurem teurer teurerer it_costoso dispendioso remunerativo redditizio rischioso costosa costosa costose costosi dis"
P17-1006,P16-1024,1,0.914352,"Missing"
P17-1006,W13-3520,0,\N,Missing
P17-1006,J15-4004,1,\N,Missing
P17-1006,P08-1087,0,\N,Missing
P17-1006,P14-2050,0,\N,Missing
P17-1006,P14-2131,0,\N,Missing
P17-1006,D16-1047,0,\N,Missing
P17-1006,P16-1156,0,\N,Missing
P17-1006,P16-2084,1,\N,Missing
P17-1006,N15-1140,0,\N,Missing
P18-1142,W17-0401,0,0.422071,"Missing"
P18-1142,W14-4203,0,0.17361,"Missing"
P18-1142,P17-2021,0,0.0122502,"resource-rich to resource-poor languages using approaches such as annotation projection, model transfer, and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content words. This reduces the variation of dependency trees, because content words are crosslingually more stable than function words (Croft et al., 2017), and benefits semantically-oriented applications (de Marneffe et al., 2014)1 . Importantly, although UD is"
P18-1142,P16-1231,0,0.0152094,"port LAS scores using three different source languages: (1) the highestranked source according to the Jaccard index; (2) a source sampled from the middle of the list ranked by the Jaccard indices; (3) a very dissimilar language sampled from the bottom of the ranked list. The total number of sentences used for training corresponds to the smallest of the three source language treebanks in order to isolate the effect of treebank size on the final transfer results. We conduct experiments with two well-known transition-based parsers (Nivre, 2006): (1) DeSR (Attardi et al., 2007) and (2) SyntaxNet (Andor et al., 2016; Alberti et al., 2017). The two were selected as they represent two different architectures: the former is an SVM-based model with a polynomial kernel, whereas the latter is a feed-forward neural network with beam search based on conditional random fields. The results are evaluated in terms of LAS and UAS scores. Neural Machine Translation. For NMT, we examine whether the tree processing procedure from §2.3 can reduce anisomorphism between source and target language syntactic structures. We thus run NMT models in two settings: with and without the anisomorphism reduction procedure. For this e"
P18-1142,P17-1042,0,0.015014,"ovide a different forget gate ftk for each child. Hidden layers Hidden size Input size Batch size Epochs qt = σ (Wq xt + Uq ht−1 + bq ) (5) ct = ft ct−1 + it tanh (Wc xt + Uc ht−1 + bc ) (6) ht = ot tanh(ct ) (7) In our resource-lean cross-lingual scenario the language of the training data (English) differs from that of the target (Arabic). Since TreeLSTM is a lexicalised model, we employ multilingual word embeddings, such that the words of both languages lie in the shared cross-lingual semantic space. In particular, we map English into Arabic through the iterative Procustes method devised by Artetxe et al. (2017). The results are evaluated through the Pearson correlation and the Mean Squared Error (MSE) between predicted and golden labels. Learning rate Optimiser Dropout Results and Discussion Source Selection. The results for cross-lingual parser transfer with the DeSR parser are provided in Figure 3, while the results with SyntaxNet are provided as supplemental material as they follow the same trends. The selection of the source for Nematus (NMT) TreeLSTM (STS) 2 512 160 256 12 (greed); 10 (beam) 0.8 Adam 0.2 / 0.3 2 1000 280 80 Early stopping 1−4 AdaDelta 0.1 / 0.2 1 300 512 25 5 1−2 SGD 0 Table 1:"
P18-1142,D07-1119,0,0.0979575,"Missing"
P18-1142,S17-2001,0,0.0189177,"y annotated by SyntaxNet. The data for cross-lingual STS are chosen to resemble a real-world scenario with a resource-poor target language. The training data (9,709 sentence 8 http://universaldependencies.org/ Language names are substituted in this work by their corresponding ISO 639-1 codes. A table of names and codes is provided in the supplemental material. 10 http://opus.nlpl.eu/OpenSubtitles.php 1535 9 pairs) are in English, taken from the STS benchmark, the ensemble of all the datasets from SemEval 2012-2017 STS tasks. The test data (250 sentence pairs) come from Task 1 of SemEval 2017 (Cer et al., 2017); target language is Arabic.11 All the sentence pairs are associated with a label ranging from 0 (dissimilarity) to 5 (equivalence). 4 Methodology Cross-lingual Dependency Parsing. To assess if the anisomorphism metrics devised in §2.2 are reliable in finding compatible languages for knowledge transfer, we use the Jaccard index of the morphological feature sets as a criterion to choose source languages for cross-lingual parser transfer. We adopt the variant of delexicalised model transfer (Zeman and Resnik, 2008) for this task. This technique ignores lexicalised features and leverages only lan"
P18-1142,P16-1038,0,0.032079,"turned out to be useful for correcting programming scripts (Tai, 1979), evolution studies, and most notably accounting for transformations in constituency trees (Selkow, 1977). Although previous works were aware of the problem of anisomorphism in the context of syntax-based NLP applications (Ambati, 2008), to our knowledge we are the first to quantify it formally and to leverage it in cross-lingual NLP. For source selection, similarity metrics from prior work mostly relied on information stored in typological databases (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Deri and Knight, 2016). Otherwise, the metrics were derived empirically: they mostly concerned linear-order properties such as part-of-speech ngrams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). In domain adaptation, the selection also hinges upon topic models (Plank and Van Noord, 2011) or Bayesian Optimisation (Ruder and Plank, 2017). The metrics we defined in §2.2 are instead based on configurational properties of languages, and add another piece to the puzzle of source selection. The idea of tree processing dates back to the attempts to steer source towards target syntactic structures in statistical MT, although th"
P18-1142,N16-1024,0,0.0196463,"limited to simple reordering steps. Gildea (2003) proposed cloning operations to relocate subtrees. Other works learned rewrite patterns in an automatic fashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims at developing and evaluating models focused on specific constructions rather than languages as a whole (Rimell et al., 2009; Bender, 2011; Rimell et al., 2016). The gist is that current models have reached a plateau in performance because they excel with frequent and simple phenomena, but they still lag behind with respect to rarer or more complex constructions. Conclusions and Future Work We have demonstrated that syntactic structures differ across languages even in well-developed annotation schemes such as Universal Dependencies. This variation stems from morp"
P18-1142,P16-1078,0,0.0953751,"can be transferred from resource-rich to resource-poor languages using approaches such as annotation projection, model transfer, and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content words. This reduces the variation of dependency trees, because content words are crosslingually more stable than function words (Croft et al., 2017), and benefits semantically-oriented applications (de Marneffe et al., 2014)1"
P18-1142,P03-1011,0,0.115293,"they mostly concerned linear-order properties such as part-of-speech ngrams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). In domain adaptation, the selection also hinges upon topic models (Plank and Van Noord, 2011) or Bayesian Optimisation (Ruder and Plank, 2017). The metrics we defined in §2.2 are instead based on configurational properties of languages, and add another piece to the puzzle of source selection. The idea of tree processing dates back to the attempts to steer source towards target syntactic structures in statistical MT, although they were mostly limited to simple reordering steps. Gildea (2003) proposed cloning operations to relocate subtrees. Other works learned rewrite patterns in an automatic fashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims"
P18-1142,W15-2114,0,0.0138647,"ntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1531–1542 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics In this paper we address these two challenges. We propose the concept of isomorphism (i.e., identity of shapes: syntactic structures) and its opposite, anisomorphism, as a probe to measuring quantitatively the extent to which syntactic tree pairs are cross-lingually compatible. We assess the varia"
P18-1142,2007.mtsummit-papers.29,0,0.07936,"esian Optimisation (Ruder and Plank, 2017). The metrics we defined in §2.2 are instead based on configurational properties of languages, and add another piece to the puzzle of source selection. The idea of tree processing dates back to the attempts to steer source towards target syntactic structures in statistical MT, although they were mostly limited to simple reordering steps. Gildea (2003) proposed cloning operations to relocate subtrees. Other works learned rewrite patterns in an automatic fashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims at developing and evaluating models focused on specific constructions rather than languages as a whole (Rimell et al., 2009; Bender, 2011; Rimell et al., 2016). The gist is that current models have reached a plateau i"
P18-1142,de-marneffe-etal-2014-universal,0,0.071785,"Missing"
P18-1142,P12-1066,0,0.74434,"upport to cross-lingual transfer, it also supports monolingual applications with a quality comparable to languagespecific annotations (Vincze et al., 2017, inter alia). Despite the careful design of this resource, there are still substantial variations in morphological richness and strategies employed to express the same syntactic constructions across languages. These variations posit challenges for syntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers"
P18-1142,P15-2040,0,0.51718,"e to languagespecific annotations (Vincze et al., 2017, inter alia). Despite the careful design of this resource, there are still substantial variations in morphological richness and strategies employed to express the same syntactic constructions across languages. These variations posit challenges for syntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1531–1542 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguist"
P18-1142,D17-1038,0,0.034432,"our knowledge we are the first to quantify it formally and to leverage it in cross-lingual NLP. For source selection, similarity metrics from prior work mostly relied on information stored in typological databases (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Deri and Knight, 2016). Otherwise, the metrics were derived empirically: they mostly concerned linear-order properties such as part-of-speech ngrams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). In domain adaptation, the selection also hinges upon topic models (Plank and Van Noord, 2011) or Bayesian Optimisation (Ruder and Plank, 2017). The metrics we defined in §2.2 are instead based on configurational properties of languages, and add another piece to the puzzle of source selection. The idea of tree processing dates back to the attempts to steer source towards target syntactic structures in statistical MT, although they were mostly limited to simple reordering steps. Gildea (2003) proposed cloning operations to relocate subtrees. Other works learned rewrite patterns in an automatic fashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner ("
P18-1142,C16-1123,1,0.925498,"Missing"
P18-1142,E17-3017,0,0.0213831,"Missing"
P18-1142,P15-2034,0,0.123715,"Missing"
P18-1142,W16-2209,0,0.0151383,"15) implemented in the Nematus suite12 (Sennrich et al., 2017). The encoder is a bidirectional gated recurrent network. For each step i, the decoder predicts the next word in output by taking as input the current hidden state hi , the previous word wi−1 and a context vector, Pn i.e., a weighted sum of all the hidden states j=1 wj · h1 . The weights are learned by a multilayer perceptron that estimates the likelihood of the alignment between the predicted word and each of the input words: wi,j = P (a|yi , xj ). This model is enriched with additional linguistic features on input, as proposed by Sennrich and Haddow (2016). In particular, we select the following which are proven as useful in prior work, and also relevant to our experiment: word form, POS tag, and dependency relations. These features are concatenated and fed to the encoder. Tree processing from §2.3 affects these features (and consequently the sentence representation) by changing the initial tree structure. For instance, the original tree in Figure 2a and the processed one in Figure 2c would correspond to these feature sets: Original Preprocessed ladayhim¯a ⊕ N ⊕ ROOT him¯a ⊕ N ⊕ N SUBJ D UMMY ⊕ V ⊕ ROOT ‘aˇsy¯a‘u ⊕ N ⊕ D OBJ muˇstarakatun ⊕ A ⊕"
P18-1142,P02-1040,0,0.102066,"M-based model with a polynomial kernel, whereas the latter is a feed-forward neural network with beam search based on conditional random fields. The results are evaluated in terms of LAS and UAS scores. Neural Machine Translation. For NMT, we examine whether the tree processing procedure from §2.3 can reduce anisomorphism between source and target language syntactic structures. We thus run NMT models in two settings: with and without the anisomorphism reduction procedure. For this experiment we rely on a state-of-the-art syntax-aware NMT architecture. We report its performance by BLEU scores (Papineni et al., 2002). 11 http://alt.qcri.org/semeval2017/ task1/ In particular, we use an attentional encoder-decoder network that jointly learns to translate and align words (Bahdanau et al., 2015) implemented in the Nematus suite12 (Sennrich et al., 2017). The encoder is a bidirectional gated recurrent network. For each step i, the decoder predicts the next word in output by taking as input the current hidden state hi , the previous word wi−1 and a context vector, Pn i.e., a weighted sum of all the hidden states j=1 wj · h1 . The weights are learned by a multilayer perceptron that estimates the likelihood of th"
P18-1142,D09-1086,0,0.21974,"bstantial variations in morphological richness and strategies employed to express the same syntactic constructions across languages. These variations posit challenges for syntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1531–1542 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics In this paper we address these two challenges. We propose the concept of isomorphism (i.e., identity of shapes: syntactic str"
P18-1142,P11-1157,0,0.211875,"Missing"
P18-1142,W17-0903,1,0.791524,"Missing"
P18-1142,S17-1003,1,0.825881,"Missing"
P18-1142,Q17-1020,0,0.160289,"translation and cross-lingual sentence similarity, demonstrating the importance of syntactic structure compatibility for boosting cross-lingual transfer in NLP. 1 Introduction Linguistic information can be transferred from resource-rich to resource-poor languages using approaches such as annotation projection, model transfer, and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content words. This reduces the variatio"
P18-1142,D09-1085,0,0.0306106,"ashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims at developing and evaluating models focused on specific constructions rather than languages as a whole (Rimell et al., 2009; Bender, 2011; Rimell et al., 2016). The gist is that current models have reached a plateau in performance because they excel with frequent and simple phenomena, but they still lag behind with respect to rarer or more complex constructions. Conclusions and Future Work We have demonstrated that syntactic structures differ across languages even in well-developed annotation schemes such as Universal Dependencies. This variation stems from morphological and syntactic differences across languages. This phenomenon, which we have labeled as anismorphism, can challenge the transfer of knowledge from"
P18-1142,J16-4004,0,0.0130895,"he order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims at developing and evaluating models focused on specific constructions rather than languages as a whole (Rimell et al., 2009; Bender, 2011; Rimell et al., 2016). The gist is that current models have reached a plateau in performance because they excel with frequent and simple phenomena, but they still lag behind with respect to rarer or more complex constructions. Conclusions and Future Work We have demonstrated that syntactic structures differ across languages even in well-developed annotation schemes such as Universal Dependencies. This variation stems from morphological and syntactic differences across languages. This phenomenon, which we have labeled as anismorphism, can challenge the transfer of knowledge from one language to another. We have pro"
P18-1142,N13-1126,0,0.507114,"Missing"
P18-1142,P15-1150,0,0.0924878,"Missing"
P18-1142,W15-2137,0,0.089893,"for both machine translation and cross-lingual sentence similarity, demonstrating the importance of syntactic structure compatibility for boosting cross-lingual transfer in NLP. 1 Introduction Linguistic information can be transferred from resource-rich to resource-poor languages using approaches such as annotation projection, model transfer, and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content word"
P18-1142,P16-2069,0,0.0787408,"Missing"
P18-1142,E17-1034,0,0.0624443,"Missing"
P18-1142,E17-2065,1,0.884029,"Missing"
P18-1142,P16-2084,1,0.906217,"Missing"
P18-1142,Q16-1035,0,0.116694,", and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content words. This reduces the variation of dependency trees, because content words are crosslingually more stable than function words (Croft et al., 2017), and benefits semantically-oriented applications (de Marneffe et al., 2014)1 . Importantly, although UD is tailored to offer support to cross-lingual transfer, it also supports monolingual applications wit"
P18-1142,I08-3008,0,0.202796,"12-2017 STS tasks. The test data (250 sentence pairs) come from Task 1 of SemEval 2017 (Cer et al., 2017); target language is Arabic.11 All the sentence pairs are associated with a label ranging from 0 (dissimilarity) to 5 (equivalence). 4 Methodology Cross-lingual Dependency Parsing. To assess if the anisomorphism metrics devised in §2.2 are reliable in finding compatible languages for knowledge transfer, we use the Jaccard index of the morphological feature sets as a criterion to choose source languages for cross-lingual parser transfer. We adopt the variant of delexicalised model transfer (Zeman and Resnik, 2008) for this task. This technique ignores lexicalised features and leverages only language-independent features instead. For each language from a sample of 7 (typologically diverse) targets, we report LAS scores using three different source languages: (1) the highestranked source according to the Jaccard index; (2) a source sampled from the middle of the list ranked by the Jaccard indices; (3) a very dissimilar language sampled from the bottom of the ranked list. The total number of sentences used for training corresponds to the smallest of the three source language treebanks in order to isolate"
P18-1142,D15-1213,0,0.471468,"rts monolingual applications with a quality comparable to languagespecific annotations (Vincze et al., 2017, inter alia). Despite the careful design of this resource, there are still substantial variations in morphological richness and strategies employed to express the same syntactic constructions across languages. These variations posit challenges for syntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1531–1542 c Melbourne, Australia, July 15 -"
P18-1142,W07-0401,0,0.107951,"Missing"
P98-2247,A97-1052,0,0.376514,"in the respective frames can be combined to produce a cheaper model than that produced if the data is encoded separately.I. 1 Introduction Diathesis alternations are regular variations in the syntactic expressions of verbal arguments, for example The boy broke the window ~-. The window broke. Levin&apos;s (1993) investigation of alternations summarises the research done and demonstrates the utility of alternation information for classifying verbs. Some studies have recently recognised the potential for using diathesis alternations within automatic lexical acquisition (Ribas, 1995; Korhonen, 1997; Briscoe and Carroll, 1997). This paper shows how corpus data can be used to automatically detect which verbs undergo these alternations. Automatic acquisition avoids the costly overheads of a manual approach and allows for the fact that predicate behaviour varies between sublanguages, domains and across time. Subcategorization frames (SCFs) are acquired for each verb and 1This work was partially funded by CEC LE1 project &quot;SPARKLE&quot;. We also acknowledge support from UK EPSRC project &quot;PSET: Practical Simplification of English Text&quot;. 1493 a hand-crafted classification of diathesis alternations filters potential candidates"
preiss-etal-2002-subcategorization,J87-3002,1,\N,Missing
preiss-etal-2002-subcategorization,S01-1009,0,\N,Missing
preiss-etal-2002-subcategorization,S01-1031,0,\N,Missing
preiss-etal-2002-subcategorization,W98-1505,0,\N,Missing
preiss-etal-2002-subcategorization,C00-2100,0,\N,Missing
preiss-etal-2002-subcategorization,C94-1042,0,\N,Missing
preiss-etal-2002-subcategorization,A97-1052,1,\N,Missing
Q14-1023,D10-1115,0,0.083666,"Missing"
Q14-1023,P12-1015,0,0.1434,"also what the child perceives about the world around it when the word is heard. Learning the meaning of words requires not only a sensitivity to both linguistic and perceptual input, but also the ability to process and combine information from these modalities in a productive way. However, the majority of perceptual input for the models in these studies corresponds directly to concrete noun concepts, such as chocolate or cheeseburger, and the superiority of the multi-modal over the corpus-only approach has only been established when evaluations include such concepts (Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012). It is thus unclear if the multi-modal approach is effective for more abstract words, such as guilt or obesity. Indeed, since empirical evidence indicates differences in the representational frameworks of both concrete and abstract concepts (Paivio, 1991; Hill et al., 2013), and verb and noun concepts (Markman and Wisniewski, 1997), perceptual information may not fulfill the same role in the representation of the various concept types. This potential challenge to the multi-modal approach is of particular practical importance since"
Q14-1023,D08-1094,0,0.0586223,"Missing"
Q14-1023,N10-1011,0,0.314881,"a corresponding set of pairs {(w1 , w2 ) ∈ U SF : w1 , w2 ∈ L} is defined for evaluation. These details are summarized in Table 2. Evaluation lists, sets of pairs and USF scores are downloadable from our website. 3.3 Evaluation Methodology All models are evaluated by measuring correlations with the free-association scores in the USF dataset (Nelson et al., 2004). This dataset contains the freeassociation strength of over 150,000 word pairs.3 These data reflect the cognitive proximity of concepts and have been widely used in NLP as a goldstandard for computational models (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). For evaluation pairs (c1 , c2 ) we calculate the cosine similarity between our learned feature representations for c1 and c2 , a standard measure of the proximity of two vectors (Turney et al., 2010), and follow previous studies (Leong and Mihalcea, 2011; Huang et al., 2012) in using Spearman’s ρ as a measure of correlation between these values and our goldstandard.4 All representations in this section are combined by concatenation, since the present focus is not on combination methods.5 3 Free-association strength is measured by"
Q14-1023,J02-3001,0,0.00451227,"295 1716 66 127 221 Examples yacht, cup fear, respect cup, respect kiss, launch differ, obey kiss, differ Table 2: Evaluation sets used throughout. All nouns and all verbs are the union of abstract and concrete subsets and mixed abstract-concrete or concrete-abstract pairs. Table 1: Grammatical features for noun/verb concepts nouns, such as shiver or walk, often refer to processes rather than entities. To capture such effects, we count the frequency of occurrence with the POS categories ajdective, adverb, noun and verb. Grammatical Features Grammatical role is a strong predictor of semantics (Gildea and Jurafsky, 2002). For instance, the subject of transitive verbs is more likely to refer to an animate entity than a noun chosen at random. Syntactic context also predicts verb semantics (Kipper et al., 2008). We thus count the frequency of nouns in a range of (nonlexicalized) syntactic contexts, and of verbs in one of the six most common subcategorization-frame classes as defined in Van de Cruys et al. (2012). These contexts are detailed in Table 1. 3.2 Evaluation Sets We create evaluation sets of abstract and concrete concepts, and introduce a complementary dichotomy between nouns and verbs, the two POS cate"
Q14-1023,S13-1035,0,0.0149563,"ction 2). In light of these considerations, this paper addresses three questions: (1) Which information sources (modalities) are important for acquiring concepts of different types? (2) Can perceptual input be propagated effectively from concrete to more abstract words? (3) What is the best way to combine information from the different sources? We construct models that acquire semantic representations for four sets of concepts: concrete nouns, abstract nouns, concrete verbs and abstract verbs. The linguistic input to the models comes from the recently released Google Syntactic N-Grams Corpus (Goldberg and Orwant, 2013), from which a selection of linguistic features are extracted. Perceptual input is approximated by data from the McRae et al. (2005) norms, which encode perceptual properties of concrete nouns, and the ESPGame dataset (Von Ahn and Dabbish, 2004), which contains manually generated descriptions of 100,000 images. To address (1) we extract representations for each concept type from combinations of information sources. We first focus on different classes of linguistic features, before extending our models to the multi-modal context. While linguistic information overall effectively reflects the mea"
Q14-1023,P12-1092,0,0.0267663,"(Nelson et al., 2004). This dataset contains the freeassociation strength of over 150,000 word pairs.3 These data reflect the cognitive proximity of concepts and have been widely used in NLP as a goldstandard for computational models (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). For evaluation pairs (c1 , c2 ) we calculate the cosine similarity between our learned feature representations for c1 and c2 , a standard measure of the proximity of two vectors (Turney et al., 2010), and follow previous studies (Leong and Mihalcea, 2011; Huang et al., 2012) in using Spearman’s ρ as a measure of correlation between these values and our goldstandard.4 All representations in this section are combined by concatenation, since the present focus is not on combination methods.5 3 Free-association strength is measured by presenting subjects with a cue word and asking them to produce the first word they can think of that is associated with that cue word. 4 We consider Spearman’s ρ, a non-parametric ranking correlation, to be more appropriate than Pearson’s r for free association data, which is naturally skewed and non-continuous. 5 When combining multiple"
Q14-1023,J06-2001,0,0.0185032,"Missing"
Q14-1023,W10-0608,1,0.904359,"Missing"
Q14-1023,P14-2135,1,0.614518,"hese comparisons, the optimal combination method is selected in each case. 294 deed, conceptual concreteness appears to determine the degree to which perceptual input is beneficial, since representations of abstract verbs, the most abstract concepts in our experiments, were actually degraded by this additional information. One important contribution of this work is therefore an insight into when multi-modal models should or should not aim to combine and/or propagate perceptual input to ensure that optimal representations are learned. In this respect, our conclusions align with the findings of Kiela and Hill (2014), who take an explicitly visual approach to resolving the same question. Various methods for propagating and combining perceptual information with linguistic input were presented. We proposed ridge regression for inferring perceptual representations for abstract concepts, which proved more robust than alternatives across the range of concept types. This approach is particularly simple to implement, since it is based on an established statistical prodedure. In addition, we introduced weighted gram matrix combination for combining representations from distinct modalities of differing sparsity an"
Q14-1023,C94-1103,0,0.0597624,"Missing"
Q14-1023,I11-1162,0,0.064937,"or speaker intention, but also what the child perceives about the world around it when the word is heard. Learning the meaning of words requires not only a sensitivity to both linguistic and perceptual input, but also the ability to process and combine information from these modalities in a productive way. However, the majority of perceptual input for the models in these studies corresponds directly to concrete noun concepts, such as chocolate or cheeseburger, and the superiority of the multi-modal over the corpus-only approach has only been established when evaluations include such concepts (Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012). It is thus unclear if the multi-modal approach is effective for more abstract words, such as guilt or obesity. Indeed, since empirical evidence indicates differences in the representational frameworks of both concrete and abstract concepts (Paivio, 1991; Hill et al., 2013), and verb and noun concepts (Markman and Wisniewski, 1997), perceptual information may not fulfill the same role in the representation of the various concept types. This potential challenge to the multi-modal approach is of particular practi"
Q14-1023,P13-1085,1,0.80337,"our evaluations, perceptual input actually degrades representation quality. This highlights the 286 need to consider the concreteness of the target domain when constructing multi-modal models. To address (3), we present various means of combining information from different modalities. We propose weighted gram matrix combination, a technique in which representations of distinct modalities are mapped to a space of common dimension where coordinates reflect proximity to other concepts. This transformation, which has been shown to enhance semantic representations in the context of verbclustering (Reichart and Korhonen, 2013), reduces representation sparsity and facilitates a productbased combination that results in greater inter-modal dependency. Weighted gram matrix combination outperforms alternatives such as concatenation and Canonical Correlation Analysis (CCA) (Hardoon et al., 2004) when combining representations from two similarly rich information sources. In Section 3, we present experiments with linguistic features designed to address question (1). These analyses are extended to multi-modal models in Section 4, where we also address (2) and (3). We first discuss the relevance of concreteness and part-ofsp"
Q14-1023,D13-1115,0,0.270695,"Missing"
Q14-1023,D12-1130,0,0.0802291,"ersity of Cambridge fh295@cam.ac.uk roiri@ie.technion.ac.il alk23@cam.ac.uk Abstract Many computational semantic models represent words as real-valued vectors, encoding their relative frequency of occurrence in particular forms and contexts in linguistic corpora (Sahlgren, 2006; Turney et al., 2010). Motivated both by parallels with human language acquisition and by evidence that many word meanings are grounded in the perceptual system (Barsalou et al., 2003), recent research has explored the integration into text-based models of input that approximates the visual or other sensory modalities (Silberer and Lapata, 2012; Bruni et al., 2014). Such models can learn higher-quality semantic representations than conventional corpusonly models, as evidenced by a range of evaluations. Multi-modal models that learn semantic representations from both linguistic and perceptual input outperform language-only models on a range of evaluations, and better reflect human concept acquisition. Most perceptual input to such models corresponds to concrete noun concepts and the superiority of the multimodal approach has only been established when evaluating on such concepts. We therefore investigate which concepts can be effecti"
Q14-1023,P13-1056,0,0.0328046,"pairs. The path similarity between words w1 and w2 is the shortest distance between synsets of w1 and w2 in the WordNet taxonomy (Fellbaum, 1999), which correlates significantly with human judgements of concept similarity (Pedersen et al., 2004).10 The correlations with the USF data (left hand column, Table 4) of our linguistic-only models (ρ = 0.094 − 0.233) and best performing multi-modal models (on both concrete nouns, ρ = 0.397, and more abstract concepts, ρ = 0.095 − 0.301) were higher than the best comparable models described elsewhere (Feng and Lapata, 2010; Silberer and Lapata, 2012; Silberer et al., 2013).11 This confirms 10 Other widely-used evaluation gold-standards, such as WordSim 353 and the MEN dataset, do not contain a sufficient number of abstract concepts for the current purpose. 11 Feng and Lapata (2010) report ρ = .08 for language-only 293 both that the underlying linguistic space is of high quality and that the ESP and McRae perceptual input is similarly or more informative than the input applied in previous work. Consistent with previous studies, adding perceptual input improved the quality of concrete noun representations as measured against both USF and path similarity gold-stan"
Q14-1023,C12-1165,1,\N,Missing
Q15-1010,P14-1056,0,0.025493,"relations and their temporal spans and applied distant learning to find approximate instances for classifier training. A set of constraint templates specific to temporal learning were also specified. In contrast, we do not use manually specified guidance in constraint learning. Particularly, we construct constraints from latent variables (topics in topic modeling) estimated from raw text rather than applying maximum likelihood estimation over observed variables (fluents and temporal expressions) in labeled data. Our method is therefore less dependent on human supervision. Even more recently, (Anzaroot et al., 2014) presented a supervised dual-decomposition based method, in the context of citation field extraction, which automatically generates large families of constraints and learn their costs with a convex optimization objective during training. Our work is unsupervised, as opposed to their model which requires a manually annotated training corpus for constraint learning. Information Structure Analysis Various schemes have been proposed for analysing the information structure of scientific documents, in particular the patterns of topics, functions and relations at sentence level. Existing schemes incl"
Q15-1010,P07-1036,0,0.0874532,"f (Guo et al., 2013a) and those in the gold standard. Our work demonstrates the great potential of automatically induced declarative knowledge in both improving the performance of information structure analysis and reducing reliance of human supervision. 2 Previous Work Automatic Declarative Knowledge Induction Learning with declarative knowledge offers effective means of reducing human supervision and improving performance. This framework augments featurebased models with domain and expert knowledge in the form of, e.g., linear constraints, posterior probabilities and logical formulas (e.g. (Chang et al., 2007; Mann and McCallum, 2007; Mann and McCallum, 2008; Ganchev et al., 2010)). It has proven useful for many NLP tasks including unsupervised and semi-supervised POS tagging, parsing (Druck et al., 2008; Ganchev et al., 2010; Rush et al., 2012) and information extraction (Chang et al., 2007; Mann and McCallum, 2008; Reichart and Korhonen, 2012; Reichart and Barzilay, 2012). However, declarative knowledge is still created in a costly manual process. We propose inducing such knowledge directly from text with minimal human involvement. This idea could be applied to almost any NLP task. We apply it h"
Q15-1010,C12-1041,1,0.925037,"round or motivation of the research, the methods used, the experiments carried out, the observations on the results, or the author’s conclusions. Readers of scientific literature find information in IS-annotated articles much faster than in unannotated articles (Guo et al., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather limited: Liakata et al. (2012) reported perclass F-scores ranging from .53 to .76 in the biochemistry and chemistry domains and Guo et al. (2013a) reported substantially lower numbers for the challen"
Q15-1010,P07-2009,0,0.0246166,"Missing"
Q15-1010,P07-1094,0,0.0417074,"g 150 sentences from that section, as in the lightly supervised case in (Guo et al., 2013a) (MaxEnt); and (d) a baseline that assigns all the sentences in a given section to the most frequent gold-standard category of that section (Table 3). This baseline emulates the use of section names for information structure classification. Our constraints, which we use in the TopicGE and TopicGC models, are based on topics that are learned on the test corpus. While having access to the raw test text at training time is a standard assumption in many unsupervised NLP works (e.g. (Klein and Manning, 2004; Goldwater and Griffiths, 2007; Lang and Lapata, 2014)), it is important to quantify the extent to which our method depends on its access to the test set. We therefore constructed the TopicGE* model which is identical to TopicGE except that the topics are learned from another collection of 47 biomedical articles containing 9352 sentences. Like our test set, these articles are from the cancer risk assessment domain - all of them were published in the Toxicol. Sci. journal in the years 2009-2012 and were retrieved using the PubMed search engine with the key words “cancer risk assessment”. There is no overlap between this new"
Q15-1010,W10-1913,1,0.89922,"l., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather limited: Liakata et al. (2012) reported perclass F-scores ranging from .53 to .76 in the biochemistry and chemistry domains and Guo et al. (2013a) reported substantially lower numbers for the challenging Introduction and Discussion sections in biomedical domain. Guo et al. (2013a) recently applied the Generalized Expectation (GE) criterion (Mann and McCallum, 2007) to information structure analysis using expert knowledge in the form of discourse and lexical"
Q15-1010,D11-1025,1,0.811028,"a sentence) into a category that represents the information type it conveys. By information structure we refer to a particular type of discourse structure that focuses on the functional role of a unit in the discourse (Webber et al., 2011). For instance, in the scientific literature, the functional role of a sentence could be the background or motivation of the research, the methods used, the experiments carried out, the observations on the results, or the author’s conclusions. Readers of scientific literature find information in IS-annotated articles much faster than in unannotated articles (Guo et al., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et a"
Q15-1010,N13-1113,1,0.0921206,"2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather limited: Liakata et al. (2012) reported perclass F-scores ranging from .53 to .76 in the biochemistry and chemistry domains and Guo et al. (2013a) reported substantially lower numbers for the challenging Introduction and Discussion sections in biomedical domain. Guo et al. (2013a) recently applied the Generalized Expectation (GE) criterion (Mann and McCallum, 2007) to information structure analysis using expert knowledge in the form of discourse and lexical constraints. Their model produces promising results, especially for sections and categories where 131 Transactions of the Association for Computational Linguistics, vol. 3, pp. 131–143, 2015. Action Editor: Masaaki Nagata. c Submission batch: 10/2014; Revision batch 1/2015; Publish"
Q15-1010,I08-1050,0,0.0267098,"faster than in unannotated articles (Guo et al., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather limited: Liakata et al. (2012) reported perclass F-scores ranging from .53 to .76 in the biochemistry and chemistry domains and Guo et al. (2013a) reported substantially lower numbers for the challenging Introduction and Discussion sections in biomedical domain. Guo et al. (2013a) recently applied the Generalized Expectation (GE) criterion (Mann and McCallum, 2007) to information structure analysis using expert knowled"
Q15-1010,P04-1061,0,0.0511901,"a particular section using 150 sentences from that section, as in the lightly supervised case in (Guo et al., 2013a) (MaxEnt); and (d) a baseline that assigns all the sentences in a given section to the most frequent gold-standard category of that section (Table 3). This baseline emulates the use of section names for information structure classification. Our constraints, which we use in the TopicGE and TopicGC models, are based on topics that are learned on the test corpus. While having access to the raw test text at training time is a standard assumption in many unsupervised NLP works (e.g. (Klein and Manning, 2004; Goldwater and Griffiths, 2007; Lang and Lapata, 2014)), it is important to quantify the extent to which our method depends on its access to the test set. We therefore constructed the TopicGE* model which is identical to TopicGE except that the topics are learned from another collection of 47 biomedical articles containing 9352 sentences. Like our test set, these articles are from the cancer risk assessment domain - all of them were published in the Toxicol. Sci. journal in the years 2009-2012 and were retrieved using the PubMed search engine with the key words “cancer risk assessment”. There"
Q15-1010,J14-3006,0,0.0211183,"on, as in the lightly supervised case in (Guo et al., 2013a) (MaxEnt); and (d) a baseline that assigns all the sentences in a given section to the most frequent gold-standard category of that section (Table 3). This baseline emulates the use of section names for information structure classification. Our constraints, which we use in the TopicGE and TopicGC models, are based on topics that are learned on the test corpus. While having access to the raw test text at training time is a standard assumption in many unsupervised NLP works (e.g. (Klein and Manning, 2004; Goldwater and Griffiths, 2007; Lang and Lapata, 2014)), it is important to quantify the extent to which our method depends on its access to the test set. We therefore constructed the TopicGE* model which is identical to TopicGE except that the topics are learned from another collection of 47 biomedical articles containing 9352 sentences. Like our test set, these articles are from the cancer risk assessment domain - all of them were published in the Toxicol. Sci. journal in the years 2009-2012 and were retrieved using the PubMed search engine with the key words “cancer risk assessment”. There is no overlap between this new dataset and our test se"
Q15-1010,liakata-etal-2010-corpora,0,0.0234495,"sed, as opposed to their model which requires a manually annotated training corpus for constraint learning. Information Structure Analysis Various schemes have been proposed for analysing the information structure of scientific documents, in particular the patterns of topics, functions and relations at sentence level. Existing schemes include argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), discourse structure (Burstein et al., 2003; Webber et al., 2011), qualitative dimensions (Shatkay et al., 2008), scientific claims (Blake, 2009), scientific concepts (Liakata et al., 2010), and information status (Markert et al., 2012), among others. Most previous work on automatic analysis of information structure relies on supervised learning (Teufel and Moens, 2002; Burstein et al., 2003; Mizuta et al., 2006; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012; Markert et al., 2012). Given the prohibitive cost 133 of manual annotation, unsupervised and minimally supervised techniques such as clustering (Kiela et al., 2014) and topic modeling (Varga et al., 2012; ´ S´eaghdha and Teufel, 2014) are highly important. O However, the performance of such approaches shows a"
Q15-1010,W06-3309,0,0.0402377,"ted articles much faster than in unannotated articles (Guo et al., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather limited: Liakata et al. (2012) reported perclass F-scores ranging from .53 to .76 in the biochemistry and chemistry domains and Guo et al. (2013a) reported substantially lower numbers for the challenging Introduction and Discussion sections in biomedical domain. Guo et al. (2013a) recently applied the Generalized Expectation (GE) criterion (Mann and McCallum, 2007) to information structure analys"
Q15-1010,W04-1013,0,0.00604792,"Missing"
Q15-1010,P08-1099,0,0.565872,"tandard. Our work demonstrates the great potential of automatically induced declarative knowledge in both improving the performance of information structure analysis and reducing reliance of human supervision. 2 Previous Work Automatic Declarative Knowledge Induction Learning with declarative knowledge offers effective means of reducing human supervision and improving performance. This framework augments featurebased models with domain and expert knowledge in the form of, e.g., linear constraints, posterior probabilities and logical formulas (e.g. (Chang et al., 2007; Mann and McCallum, 2007; Mann and McCallum, 2008; Ganchev et al., 2010)). It has proven useful for many NLP tasks including unsupervised and semi-supervised POS tagging, parsing (Druck et al., 2008; Ganchev et al., 2010; Rush et al., 2012) and information extraction (Chang et al., 2007; Mann and McCallum, 2008; Reichart and Korhonen, 2012; Reichart and Barzilay, 2012). However, declarative knowledge is still created in a costly manual process. We propose inducing such knowledge directly from text with minimal human involvement. This idea could be applied to almost any NLP task. We apply it here to information structure analysis of scientifi"
Q15-1010,P12-1084,0,0.0243885,"manually annotated training corpus for constraint learning. Information Structure Analysis Various schemes have been proposed for analysing the information structure of scientific documents, in particular the patterns of topics, functions and relations at sentence level. Existing schemes include argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel et al., 2009), discourse structure (Burstein et al., 2003; Webber et al., 2011), qualitative dimensions (Shatkay et al., 2008), scientific claims (Blake, 2009), scientific concepts (Liakata et al., 2010), and information status (Markert et al., 2012), among others. Most previous work on automatic analysis of information structure relies on supervised learning (Teufel and Moens, 2002; Burstein et al., 2003; Mizuta et al., 2006; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012; Markert et al., 2012). Given the prohibitive cost 133 of manual annotation, unsupervised and minimally supervised techniques such as clustering (Kiela et al., 2014) and topic modeling (Varga et al., 2012; ´ S´eaghdha and Teufel, 2014) are highly important. O However, the performance of such approaches shows a large room for improvement. Our work is specif"
Q15-1010,D12-1080,0,0.146718,"g a detailed word list for each information structure category. For example, words such as “assay” were carefully selected and used as a strong indicator of the “Method” category: p(Method|assay) was constrained to be high (above 0.9). Such a constraint (developed for the biomedical domain) may not be applicable to a new domain (e.g. computer science) with a different vocabulary and writing style. In fact, most existing works on learning with declarative knowledge rely on manually constructed constraints. Little work exists on automatic declarative knowledge induction. A notable exception is (McClosky and Manning, 2012) that proposed a constraint learning model for timeline extraction. This approach, however, requires human supervision in several forms including task specific constraint templates (see Section 2). We present a novel framework for learning declarative knowledge which requires very limited human involvement. We apply it to information structure analysis, based on two key observations: 1) Each information structure category defines a distribution over a section-specific and an article-level set of linguistic features. 2) Each sentence in a scientific document, while having a dominant category, m"
Q15-1010,C14-1002,0,0.197314,"Missing"
Q15-1010,N12-1008,1,0.851522,"ive means of reducing human supervision and improving performance. This framework augments featurebased models with domain and expert knowledge in the form of, e.g., linear constraints, posterior probabilities and logical formulas (e.g. (Chang et al., 2007; Mann and McCallum, 2007; Mann and McCallum, 2008; Ganchev et al., 2010)). It has proven useful for many NLP tasks including unsupervised and semi-supervised POS tagging, parsing (Druck et al., 2008; Ganchev et al., 2010; Rush et al., 2012) and information extraction (Chang et al., 2007; Mann and McCallum, 2008; Reichart and Korhonen, 2012; Reichart and Barzilay, 2012). However, declarative knowledge is still created in a costly manual process. We propose inducing such knowledge directly from text with minimal human involvement. This idea could be applied to almost any NLP task. We apply it here to information structure analysis of scientific documents. Little prior work exists on automatic constraint learning. Recently, (McClosky and Manning, 2012) investigated the approach for timeline extraction. They used a set of gold relations and their temporal spans and applied distant learning to find approximate instances for classifier training. A set of constrai"
Q15-1010,C12-2097,1,0.854093,"2). We present a novel framework for learning declarative knowledge which requires very limited human involvement. We apply it to information structure analysis, based on two key observations: 1) Each information structure category defines a distribution over a section-specific and an article-level set of linguistic features. 2) Each sentence in a scientific document, while having a dominant category, may consist of features mostly related to other categories. This flexible view enables us to make use of topic models which have not proved useful in previous related works (Varga et al., 2012; Reichart and Korhonen, 2012). We construct topic models at both the individual section and article level and apply these models to data, identifying latent topics and their key linguistic features. This information is used to constrain or bias unsupervised models for the task in a straightforward way: we automatically generate constraints for a GE model and a bias term for a graph clustering objective, such that the resulting models assign each of the input sentences to one information 132 Zone Background (BKG) Problem (PROB) Method (METH) Result (RES) Conclusion (CON) Connection (CN) Difference (DIFF) Future work (FUT)"
Q15-1010,W09-1121,1,0.778261,"Missing"
Q15-1010,D12-1131,1,0.831464,"supervision. 2 Previous Work Automatic Declarative Knowledge Induction Learning with declarative knowledge offers effective means of reducing human supervision and improving performance. This framework augments featurebased models with domain and expert knowledge in the form of, e.g., linear constraints, posterior probabilities and logical formulas (e.g. (Chang et al., 2007; Mann and McCallum, 2007; Mann and McCallum, 2008; Ganchev et al., 2010)). It has proven useful for many NLP tasks including unsupervised and semi-supervised POS tagging, parsing (Druck et al., 2008; Ganchev et al., 2010; Rush et al., 2012) and information extraction (Chang et al., 2007; Mann and McCallum, 2008; Reichart and Korhonen, 2012; Reichart and Barzilay, 2012). However, declarative knowledge is still created in a costly manual process. We propose inducing such knowledge directly from text with minimal human involvement. This idea could be applied to almost any NLP task. We apply it here to information structure analysis of scientific documents. Little prior work exists on automatic constraint learning. Recently, (McClosky and Manning, 2012) investigated the approach for timeline extraction. They used a set of gold relat"
Q15-1010,J02-4002,0,0.50499,"ance, in the scientific literature, the functional role of a sentence could be the background or motivation of the research, the methods used, the experiments carried out, the observations on the results, or the author’s conclusions. Readers of scientific literature find information in IS-annotated articles much faster than in unannotated articles (Guo et al., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather limited: Liakata et al. (2012) reported perclass F-scores ranging from .53 to .76 in the biochemistry and che"
Q15-1010,D09-1155,0,0.0886792,"e structure that focuses on the functional role of a unit in the discourse (Webber et al., 2011). For instance, in the scientific literature, the functional role of a sentence could be the background or motivation of the research, the methods used, the experiments carried out, the observations on the results, or the author’s conclusions. Readers of scientific literature find information in IS-annotated articles much faster than in unannotated articles (Guo et al., 2011b). Argumentative Zoning (AZ) – an information structure scheme that has been applied successfully to many scientific domains (Teufel et al., 2009) – has improved tasks such as summarization and information extraction and retrieval (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012). Existing approaches to information structure analysis require substantial human effort. Most use feature-based machine learning, such as SVMs and CRFs (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012)) which rely on thousands of manually annotated training sentences. Also the performance of such methods is rather l"
Q15-1010,varga-etal-2012-unsupervised,0,0.0791648,"mplates (see Section 2). We present a novel framework for learning declarative knowledge which requires very limited human involvement. We apply it to information structure analysis, based on two key observations: 1) Each information structure category defines a distribution over a section-specific and an article-level set of linguistic features. 2) Each sentence in a scientific document, while having a dominant category, may consist of features mostly related to other categories. This flexible view enables us to make use of topic models which have not proved useful in previous related works (Varga et al., 2012; Reichart and Korhonen, 2012). We construct topic models at both the individual section and article level and apply these models to data, identifying latent topics and their key linguistic features. This information is used to constrain or bias unsupervised models for the task in a straightforward way: we automatically generate constraints for a GE model and a bias term for a graph clustering objective, such that the resulting models assign each of the input sentences to one information 132 Zone Background (BKG) Problem (PROB) Method (METH) Result (RES) Conclusion (CON) Connection (CN) Differ"
Q16-1002,P14-1133,0,0.0161394,"wledge questionanswering (open QA), generally require large teams of researchers, modular design and powerful infrastructure, exemplified by IBM’s Watson (Ferrucci et al., 2010). For this reason, much academic research focuses on settings in which the scope of the 25 task is reduced. This has been achieved by restricting questions to a specific topic or domain (Moll´a and Vicedo, 2007), allowing systems access to prespecified passages of text from which the answer can be inferred (Iyyer et al., 2014; Weston et al., 2015), or centering both questions and answers on a particular knowledge base (Berant and Liang, 2014; Bordes et al., 2014). In what follows, we show that the dictionary embedding models introduced in the previous sections may form a useful component of an open QA system. Given the absence of a knowledge base or web-scale information in our architecture, we narrow the scope of the task by focusing on general knowledge crossword questions. General knowledge (non-cryptic, or quick) crosswords appear in national newspapers in many countries. Crossword question answering is more tractable than general open QA for two reasons. First, models know the length of the correct answer (in letters), reduc"
Q16-1002,D14-1067,0,0.0958821,"(open QA), generally require large teams of researchers, modular design and powerful infrastructure, exemplified by IBM’s Watson (Ferrucci et al., 2010). For this reason, much academic research focuses on settings in which the scope of the 25 task is reduced. This has been achieved by restricting questions to a specific topic or domain (Moll´a and Vicedo, 2007), allowing systems access to prespecified passages of text from which the answer can be inferred (Iyyer et al., 2014; Weston et al., 2015), or centering both questions and answers on a particular knowledge base (Berant and Liang, 2014; Bordes et al., 2014). In what follows, we show that the dictionary embedding models introduced in the previous sections may form a useful component of an open QA system. Given the absence of a knowledge base or web-scale information in our architecture, we narrow the scope of the task by focusing on general knowledge crossword questions. General knowledge (non-cryptic, or quick) crosswords appear in national newspapers in many countries. Crossword question answering is more tractable than general open QA for two reasons. First, models know the length of the correct answer (in letters), reducing the search space."
Q16-1002,P12-1092,0,0.0381888,"omposition engine’. 2.4 Training Objective We train all neural language models M to map the input definition phrase sc defining word c to a location close to the the pre-trained embedding vc of c. We experiment with two different cost functions for the word-phrase pair (c, sc ) from the training data. The first is simply the cosine distance between M (sc ) and vc . The second is the rank loss max(0, m − cos(M (sc ), vc ) − cos(M (sc ), vr )) where vr is the embedding of a randomly-selected word from the vocabulary other than c. This loss function was used for language models, for example, in (Huang et al., 2012). In all experiments we apply a margin m = 0.1, which has been shown to work well on word-retrieval tasks (Bordes et al., 2015). 2.5 Implementation Details 3 Since training on the dictionary data took 6-10 hours, we did not conduct a hyper-parameter search on any validation sets over the space of possible model configurations such as embedding dimension, or size of hidden layers. Instead, we chose these parameters to be as standard as possible based on previous research. For fair comparison, any aspects of model design that are not specific to a particular class of model were kept constant acr"
Q16-1002,D14-1070,0,0.0316547,"Missing"
Q16-1002,P15-1162,0,0.0141736,"Missing"
Q16-1002,C12-1089,0,0.0178615,"under a CC-BY 4.0 license. For instance, a travel-writer might look to enhance her prose by searching for examples of a country that people associate with warm weather or an activity that is mentally or physically demanding. We show that an NLM-based reverse dictionary trained on only a handful of dictionaries identifies novel definitions and concept descriptions comparably or better than commercial systems, which rely on significant task-specific engineering and access to much more dictionary data. Moreover, by exploiting models that learn bilingual word representations (Vulic et al., 2011; Klementiev et al., 2012; Hermann and Blunsom, 2013; Gouws et al., 2014), we show that the NLM approach can be easily extended to produce a potentially useful cross-lingual reverse dictionary. The second application of our models is as a general-knowledge crossword question answerer. When trained on both dictionary definitions and the opening sentences of Wikipedia articles, NLMs produce plausible answers to (non-cryptic) crossword clues, even those that apparently require detailed world knowledge. Both BOW and RNN models can outperform bespoke commercial crossword solvers, particularly when clues contain a greater n"
Q16-1002,C94-1103,0,0.0104201,"uation settings. *Low variance in mult models is due to consistently poor scores, so not highlighted. unseen evaluation, we randomly selected 500 words from WordNet and excluded all definitions of these words from the training data of all models. Finally, for a fair comparison with OneLook, which has both the seen and unseen pairs in its internal database, we built a new dataset of concept descriptions that do not appear in the training data for any model. To do so, we randomly selected 200 adjectives, nouns or verbs from among the top 3000 most frequent tokens in the British National Corpus (Leech et al., 1994) (but outside the top 100). We then asked ten native English speakers to write a single-sentence ‘description’ of these words. To ensure the resulting descriptions were good quality, for each description we asked two participants who did not produce that description to list any words that fitted the description (up to a maximum of three). If the target word was not produced by one of the two checkers, the original participant was asked to re-write the description until the validation was passed.9 These concept descriptions, together with other evaluation sets, can be downloaded from our websit"
Q16-1002,J07-1004,0,0.0194807,"Missing"
Q16-1002,P11-2084,0,0.0169042,"Missing"
Q16-1002,W04-2105,0,0.35876,"purpose we experiment with two broad classes of neural language models (NLMs): Recurrent Neural Networks (RNNs), which naturally encode the order of input words, and simpler (feedforward) bag-of-words (BOW) embedding models. Prior to training these NLMs, we learn target lexical representations by training the Word2Vec software (Mikolov et al., 2013) on billions of words of raw text. We demonstrate the usefulness of our approach by building and releasing two applications. The first is a reverse dictionary or concept finder: a system that returns words based on user descriptions or definitions (Zock and Bilac, 2004). Reverse dictionaries are used by copywriters, novelists, translators and other professional writers to find words for notions or ideas that might be on the tip of their tongue. 17 Transactions of the Association for Computational Linguistics, vol. 4, pp. 17–30, 2016. Action Editor: Chris Callison-Burch. Submission batch: 9/2015; revised 12/2015; revised 1/2016; Published 2/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. For instance, a travel-writer might look to enhance her prose by searching for examples of a country that people associate with"
Q16-1002,N15-1184,0,\N,Missing
Q16-1002,D14-1179,1,\N,Missing
Q16-1002,D14-1079,0,\N,Missing
Q17-1022,S15-1003,0,0.0166028,"scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into pretrained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and"
Q17-1022,Q16-1031,0,0.0184747,"gual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations ‘from scratch’ by co"
Q17-1022,P98-1013,0,0.0636087,"gue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into pretrained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating"
Q17-1022,P14-2131,0,0.0313084,"ract-repel. These include: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories:"
Q17-1022,D14-1082,0,0.0239881,"lude: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train dis"
Q17-1022,P06-1038,0,0.011388,"enson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vecto"
Q17-1022,P14-1129,0,0.0264494,"Missing"
Q17-1022,D16-1136,0,0.0135803,"differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), and transfer learning for reso"
Q17-1022,ehrmann-etal-2014-representing,0,0.0525517,"ural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; Ó Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity. We then deploy the ATTRACT-R EPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations. This allows us to embed vector spaces of multiple languages into a single vector space, exploiting information from high-resource languages to improve the word representations of lower-resource ones. Table 1 illustrates the effects of cross-lingual ATTRACT-R EPEL specialization by showing the nearest neighbors for three English words across three cross-lingual spaces. 309 Transactions of the Association for Computational Linguistics, vol. 5, pp. 309–324, 2017. Action Ed"
Q17-1022,E14-1049,0,0.359731,"al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vu"
Q17-1022,P15-2076,0,0.0360459,"borne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vectors. However, their performance has not surpassed that of fine-tuning methods.3 Fine-Tuning Pre-trained Vectors Rothe and Schütze (2015) fine-tune word vector spaces to improve the representations of synsets/lexemes found in WordNet. F"
Q17-1022,N15-1184,0,0.532158,"s using Monolingual and Cross-Lingual Constraints Nikola Mrkši´c1,2 , Ivan Vuli´c1 , Diarmuid Ó Séaghdha2 , Ira Leviant3 Roi Reichart3 , Milica Gaši´c1 , Anna Korhonen1 , Steve Young1,2 1 University of Cambridge 2 Apple Inc. 3 Technion, IIT Abstract These models typically build on distributional ones by using human- or automatically-constructed knowledge bases to enrich the semantic content of existing word vector collections. Often this is done as a postprocessing step, where the distributional word vectors are refined to satisfy constraints extracted from a lexical resource such as WordNet (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2016). We term this approach semantic specialization. We present ATTRACT-R EPEL, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. ATTRACT-R EPEL facilitates the use of constraints from mono- and crosslingual resources, yielding semantically specialized cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct highquality vector spaces for a plethora of different languages, facilitating semantic transfer from high-"
Q17-1022,ganitkevitch-callison-burch-2014-multilingual,0,0.0509444,"Missing"
Q17-1022,N13-1092,0,0.0261119,"Missing"
Q17-1022,D16-1235,1,0.905072,"Missing"
Q17-1022,D14-1012,0,0.0360428,"tion datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into"
Q17-1022,P15-1119,0,0.0296068,"Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), and transfer learning for resource-lean languages (Søgaard et al., 2015; Guo et al., 2015). However, prior work on cross-lingual word embedding has tended not to exploit pre-existing linguistic resources such as BabelNet. In this work, we make use of cross-lingual constraints derived from such repositories to induce high-quality cross-lingual vector spaces by facilitating semantic transfer from highto lower-resource languages. In our experiments, we show that cross-lingual vector spaces produced by ATTRACT-R EPEL consistently outperform a representative selection of five strong cross-lingual word embedding models in both intrinsic and extrinsic evaluation across several languages."
Q17-1022,W14-4337,0,0.0396757,"re expressed by slot-value pairs such as [price: cheap] or [food: Thai]. For modular task-based systems, the Dialogue State Tracking (DST) component is in charge of maintaining the belief state, which is the system’s internal distribution over the possible states of the dialogue. Figure 1 shows the correct dialogue state for each turn of an example dialogue. Unseen Data/Labels As dialogue ontologies can be very large, many of the possible class labels (i.e., the various food types or street names) will not occur in the training set. To overcome this problem, delexicalization-based DST models (Henderson et al., 2014c; Henderson et al., 2014b; Mrkši´c et al., 2015; Wen et al., 2017) replace occurrences of ontology values with generic tags which facilitate transfer learning across different ontology values. This is done through exact matching supplemented with semantic lexicons which encode rephrasings, morphology and other linguistic variation. For instance, such lexicons would be required to deal with the underlined non-exact matches in Figure 1. Exact Matching as a Bottleneck Semantic lexicons can be hand-crafted for small dialogue domains. Mrkši´c et al. (2016) showed that semantically specialized vect"
Q17-1022,W14-4340,1,0.938481,"re expressed by slot-value pairs such as [price: cheap] or [food: Thai]. For modular task-based systems, the Dialogue State Tracking (DST) component is in charge of maintaining the belief state, which is the system’s internal distribution over the possible states of the dialogue. Figure 1 shows the correct dialogue state for each turn of an example dialogue. Unseen Data/Labels As dialogue ontologies can be very large, many of the possible class labels (i.e., the various food types or street names) will not occur in the training set. To overcome this problem, delexicalization-based DST models (Henderson et al., 2014c; Henderson et al., 2014b; Mrkši´c et al., 2015; Wen et al., 2017) replace occurrences of ontology values with generic tags which facilitate transfer learning across different ontology values. This is done through exact matching supplemented with semantic lexicons which encode rephrasings, morphology and other linguistic variation. For instance, such lexicons would be required to deal with the underlined non-exact matches in Figure 1. Exact Matching as a Bottleneck Semantic lexicons can be hand-crafted for small dialogue domains. Mrkši´c et al. (2016) showed that semantically specialized vect"
Q17-1022,P14-1006,0,0.0667397,"ts models with state-of-the-art performance, none of which learn representations jointly. 2.2 Cross-Lingual Word Representations Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual"
Q17-1022,J15-4004,1,0.925417,"multilingual DST models, which brings further performance improvements. 1 In this paper we advance the semantic specialization paradigm in a number of ways. We introduce a new algorithm, ATTRACT-R EPEL, that uses synonymy and antonymy constraints drawn from lexical resources to tune word vector spaces using linguistic information that is difficult to capture with conventional distributional training. Our evaluation shows that ATTRACT-R EPEL outperforms previous methods which make use of similar lexical resources, achieving state-of-the-art results on two word similarity datasets: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016). Introduction Word representation learning has become a research area of central importance in modern natural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; Ó Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity. We the"
Q17-1022,D15-1127,0,0.0127607,"our method to use existing cross-lingual resources to tie distributional vector spaces of different languages into a unified vector space which benefits from positive semantic transfer between its constituent languages. 3 The SimLex-999 web page (www.cl.cam.ac.uk/ ~fh295/simlex.html) lists models with state-of-the-art performance, none of which learn representations jointly. 2.2 Cross-Lingual Word Representations Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; V"
Q17-1022,D14-1070,0,0.0192932,"chart, 2015). To show that our approach yields semantically informative vectors for lower-resource languages, we collect intrinsic evaluation datasets for Hebrew and Croatian and show that cross-lingual specialization significantly improves word vector quality in these two (comparatively) low-resource languages. In the second part of the paper, we explore the use of ATTRACT-R EPEL-specialized vectors in a downstream application. One important motivation for training word vectors is to improve the lexical coverage of supervised models for language understanding tasks, e.g., question answering (Iyyer et al., 2014) or textual entailment (Rocktäschel et al., 2016). In 1 Some (negative) effects of the distributional hypothesis do persist. For example, nl_krieken (Dutch for cherries), is identified as a synonym for en_morning, presumably because the idiom ‘het krieken van de dag’ translates to ‘the crack of dawn’. 2 Our approach is not suited for languages for which no lexical resources exist. However, many languages have some coverage in cross-lingual lexicons. For instance, BabelNet 3.7 automatically aligns WordNet to Wikipedia, providing accurate cross-lingual mappings between 271 languages. In our eval"
Q17-1022,N15-1070,0,0.0340476,"ons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vectors. However, their performance has not surpassed that of fine-tuning methods.3 Fine-Tuning Pre-trained Vectors Rothe and Schütze (2015) fine-tune word vector spaces to improve the representations of synsets/lexemes found in WordNet. Faruqui et al. (2015) and Jauhar et al. (2015) use synonymy constraints in a procedure termed retrofitting to bring the vectors of semantically similar words close together, while Wieting et al. (2015) modify the skip-gram objective function to fine-tune word vectors by injecting paraphrasing constraints from PPDB. Mrkši´c et al. (2016) build on the retrofitting approach by jointly injecting synonymy and antonymy constraints; the same idea is reassessed by Nguyen et al. (2016). Kim et al. (2016a) further expand this line of work by incorporating semantic intensity information for the constraints, while Recski et al. (2016) use ensembles o"
Q17-1022,D15-1245,0,0.0248451,"EL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations"
Q17-1022,D15-1242,0,0.0418903,"epresentations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into pretrained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use sy"
Q17-1022,W16-1607,0,0.169112,"Missing"
Q17-1022,C12-1089,0,0.0230184,"eover, we show that starting from distributional vectors allows our method to use existing cross-lingual resources to tie distributional vector spaces of different languages into a unified vector space which benefits from positive semantic transfer between its constituent languages. 3 The SimLex-999 web page (www.cl.cam.ac.uk/ ~fh295/simlex.html) lists models with state-of-the-art performance, none of which learn representations jointly. 2.2 Cross-Lingual Word Representations Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al."
Q17-1022,2005.mtsummit-papers.11,0,0.0253673,"into a shared cross-lingual space. Ideally, sharing information across languages should lead to improved semantic content for each language, especially for those with limited monolingual resources. Antonymy BabelNet is also used to extract both monolingual and cross-lingual antonymy constraints. Following Faruqui et al. (2015), who found PPDB constraints more beneficial than the WordNet ones, we do not use BabelNet for monolingual synonymy. Availability of Resources Both PPDB and BabelNet are created automatically. However, PPDB relies on large, high-quality parallel corpora such as Europarl (Koehn, 2005). In total, Multilingual PPDB provides collections of paraphrases for 22 languages. On the other hand, BabelNet uses Wikipedia’s interlanguage links and statistical machine translation (Google Translate) to provide cross-lingual mappings for 271 languages. In our evaluation, we show that PPDB and BabelNet can be used jointly to improve word representations for lower-resource languages by tying them into bilingual spaces with high-resource ones. We validate this claim on Hebrew and Croatian, which act as ‘lower-resource’ languages because of their lack of any PPDB resource and their relatively"
Q17-1022,P15-1027,0,0.0203234,"2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mi"
Q17-1022,P14-2050,0,0.10357,"use of similar lexical resources, achieving state-of-the-art results on two word similarity datasets: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016). Introduction Word representation learning has become a research area of central importance in modern natural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; Ó Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity. We then deploy the ATTRACT-R EPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations. This allows us to embed vector spaces of multiple languages into a single vector space, exploiting information from high-resource languages to improve the word representations of lower-resource ones. T"
Q17-1022,P15-1145,0,0.0286255,"Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and d"
Q17-1022,W15-1521,0,0.0244293,". 3 The SimLex-999 web page (www.cl.cam.ac.uk/ ~fh295/simlex.html) lists models with state-of-the-art performance, none of which learn representations jointly. 2.2 Cross-Lingual Word Representations Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shar"
Q17-1022,P15-2130,1,0.34097,"Missing"
Q17-1022,N16-1018,1,0.276948,"Missing"
Q17-1022,P17-1163,1,0.07397,"Missing"
Q17-1022,P16-2074,0,0.108128,"-trained Vectors Rothe and Schütze (2015) fine-tune word vector spaces to improve the representations of synsets/lexemes found in WordNet. Faruqui et al. (2015) and Jauhar et al. (2015) use synonymy constraints in a procedure termed retrofitting to bring the vectors of semantically similar words close together, while Wieting et al. (2015) modify the skip-gram objective function to fine-tune word vectors by injecting paraphrasing constraints from PPDB. Mrkši´c et al. (2016) build on the retrofitting approach by jointly injecting synonymy and antonymy constraints; the same idea is reassessed by Nguyen et al. (2016). Kim et al. (2016a) further expand this line of work by incorporating semantic intensity information for the constraints, while Recski et al. (2016) use ensembles of rich concept dictionaries to further improve a combined collection of semantically specialized word vectors. ATTRACT-R EPEL is an instance of the second family of models, providing a portable, light-weight approach for incorporating external knowledge into arbitrary vector spaces. In our experiments, we show that ATTRACT-R EPEL outperforms previously proposed post-processors, setting the new state-of-art performance on the widely"
Q17-1022,N15-1100,0,0.0338437,"ducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vectors. However, their performance has not surpassed that of fine-tuning methods.3 Fine-Tunin"
Q17-1022,Q16-1030,0,0.48294,"de WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2"
Q17-1022,D14-1162,0,0.11428,"t ATTRACT-R EPEL outperforms previous methods which make use of similar lexical resources, achieving state-of-the-art results on two word similarity datasets: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016). Introduction Word representation learning has become a research area of central importance in modern natural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; Ó Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity. We then deploy the ATTRACT-R EPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations. This allows us to embed vector spaces of multiple languages into a single vector space, exploiting information from high-resource languages to i"
Q17-1022,N15-1058,0,0.0309108,"Missing"
Q17-1022,W16-1622,0,0.0658587,"al. (2015) and Jauhar et al. (2015) use synonymy constraints in a procedure termed retrofitting to bring the vectors of semantically similar words close together, while Wieting et al. (2015) modify the skip-gram objective function to fine-tune word vectors by injecting paraphrasing constraints from PPDB. Mrkši´c et al. (2016) build on the retrofitting approach by jointly injecting synonymy and antonymy constraints; the same idea is reassessed by Nguyen et al. (2016). Kim et al. (2016a) further expand this line of work by incorporating semantic intensity information for the constraints, while Recski et al. (2016) use ensembles of rich concept dictionaries to further improve a combined collection of semantically specialized word vectors. ATTRACT-R EPEL is an instance of the second family of models, providing a portable, light-weight approach for incorporating external knowledge into arbitrary vector spaces. In our experiments, we show that ATTRACT-R EPEL outperforms previously proposed post-processors, setting the new state-of-art performance on the widely used SimLex-999 word similarity dataset. Moreover, we show that starting from distributional vectors allows our method to use existing cross-lingual"
Q17-1022,P15-1173,0,0.0714818,"Missing"
Q17-1022,K15-1026,1,0.312629,"al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than represe"
Q17-1022,P13-1045,0,0.0159713,"ithub.com/nmrksic/ attract-repel. These include: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly)"
Q17-1022,D13-1170,0,0.0047346,"ithub.com/nmrksic/ attract-repel. These include: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly)"
Q17-1022,P15-1165,0,0.0197866,"Missing"
Q17-1022,P10-1040,0,0.0185912,"tian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexica"
Q17-1022,P16-1157,0,0.028014,"ual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), and transfer learning for resource-lean languages (Søgaard"
Q17-1022,P16-2084,1,0.84249,"Missing"
Q17-1022,P16-1024,1,0.758571,"Missing"
Q17-1022,E17-1042,1,0.780077,"Missing"
Q17-1022,Q15-1025,0,0.116195,"d Cross-Lingual Constraints Nikola Mrkši´c1,2 , Ivan Vuli´c1 , Diarmuid Ó Séaghdha2 , Ira Leviant3 Roi Reichart3 , Milica Gaši´c1 , Anna Korhonen1 , Steve Young1,2 1 University of Cambridge 2 Apple Inc. 3 Technion, IIT Abstract These models typically build on distributional ones by using human- or automatically-constructed knowledge bases to enrich the semantic content of existing word vector collections. Often this is done as a postprocessing step, where the distributional word vectors are refined to satisfy constraints extracted from a lexical resource such as WordNet (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2016). We term this approach semantic specialization. We present ATTRACT-R EPEL, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. ATTRACT-R EPEL facilitates the use of constraints from mono- and crosslingual resources, yielding semantically specialized cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct highquality vector spaces for a plethora of different languages, facilitating semantic transfer from high- to lower-resource one"
Q17-1022,D16-1157,0,0.0166515,"nsfer (+0.19 / +0.11 over monolingual specialization), with Italian vectors’ performance coming close to the top-performing English ones. 316 Comparison to Baselines Table 3 gives an exhaustive comparison of ATTRACT-R EPEL to counterfitting: ATTRACT-R EPEL achieved substantially stronger performance in all experiments. We believe these results conclusively show that the contextsensitive updates and L2 regularization employed by ATTRACT-R EPEL present a better alternative to the context-insensitive attract/repel terms and pair-wise regularization employed by counter-fitting.11 State-of-the-Art Wieting et al. (2016) note that the hyperparameters of the widely used Paragram-SL999 vectors (Wieting et al., 2015) are tuned on SimLex999, and as such are not comparable to methods which hold out the dataset. This implies that further work which uses these vectors (e.g., (Mrkši´c et al., 11 To understand the relative importance of the contextsensitive updates and the change in regularization, we can compare the two methods to the retrofitting procedure (Faruqui et al., 2015). Retrofitting uses L2 regularization (like ATTRACTR EPEL) and a ‘global’ attract term (like counter-fitting). The performance of retrofitti"
Q17-1022,D12-1111,0,0.0614025,"Missing"
Q17-1022,P14-2089,0,0.0246193,") into two categories: a) those which train distributed representations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into pretrained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches s"
Q17-1022,D13-1141,0,0.0291883,"rce-intensive. All resources related to this paper are available at www.github.com/nmrksic/ attract-repel. These include: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical enta"
Q17-1022,J14-3005,1,\N,Missing
Q17-1022,C98-1013,0,\N,Missing
Q18-1032,E17-1088,0,0.221535,"guarantee a reliable word estimate. Language Modeling (LM) is a key NLP task, serving Since gradual parameter estimation based on conas an important component for applications that retextual information is not feasible for rare phenomena quire some form of text generation, such as machine 451 Transactions of the Association for Computational Linguistics, vol. 6, pp. 451–465, 2018. Action Editor: Brian Roark. Submission batch: 12/2017; Revision batch: 5/2018; Published 7/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. in the full vocabulary setup (Adams et al., 2017), it is of crucial importance to construct and enable techniques that can obtain these parameters in alternative ways. One solution is to draw information from additional sources, such as characters and character sequences. As a consequence, such character-aware models should facilitate LM word-level prediction in a real-life LM setup which deals with a large amount of low-frequency or unseen words. Efforts into this direction have yielded exciting results, primarily on the input side of neural LMs. A standard RNN LM architecture relies on two word representation matrices learned during traini"
Q18-1032,W13-3520,0,0.0421794,"ubsequent fine-tuning. The preserve cost acts as a regularisation pulling the “fine-tuned” vector back to its initial value: X pres(Pw , Nw ) = λreg ||ˆ vw − vw ||2 . (6) xw ∈V w λreg = 10−9 is the L2 -regularisation constant (Mrkši´c et al., 2017); vˆw is the original word vector before the procedure. This term tries to preserve the semantic content present in the original vector space, as long as this information does not contradict the knowledge injected by the constraints. The final cost function adds the two costs: cost = attr + pres. 6 Experiments Datasets We use the Polyglot Wikipedia (Al-Rfou et al., 2013) for all available languages except for Japanese, Chinese, and Thai, and add these and further languages using Wikipedia dumps. The Wiki dumps were cleaned and preprocessed by the Polyglot tokeniser. We construct similarly-sized datasets by extracting 46K sentences for each language from the beginning of each dump, filtered to contain only full sentences, and split into train (40K), validation (3K), and test (3K). The final list of languages along with standard language codes (ISO 639-1 standard, used throughout the paper) and statistics on vocabulary and token counts are provided in Table 4."
Q18-1032,P16-1156,0,0.0377669,"Missing"
Q18-1032,D09-1003,0,0.0188023,"pped to ±2.4 A full summary of all hyper-parameters and their values is provided in Table 3. (Baseline) Language Models The availability of LM evaluation sets in a large number of diverse languages, described in Section 2, now provides an opportunity to perform a full-fledged multilingual analysis of representative LM architectures. At the same time, these different architectures serve as the baselines for our novel model which fine-tunes the output matrix M w . As mentioned, the traditional LM setup is to use words both on the input and on the output side (Goodman, 2001; Bengio et al., 2003; Deschacht and Moens, 2009) relying on n-gram word sequences. We evaluate a strong model from the n-gram family of models from the KenLM package (https://github.com/kpu/kenlm): it is based on 5grams with extended Kneser-Ney smoothing (KN5) (Kneser and Ney, 1995; Heafield et al., 2013) 5 . The rationale behind including this non-neural model is to also probe the limitations of such n-gram-based LM architectures on a diverse set of languages. Recurrent neural networks (RNNs), especially Long-Short-Term Memory networks (LSTMs), have taken over the LM universe recently (Mikolov et al., 2010; Sundermeyer et al., 2015; Chen e"
Q18-1032,N15-1184,0,0.116334,"Missing"
Q18-1032,D15-1042,0,0.057779,"rphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction Daniela Gerz1 , Ivan Vuli´c1 , Edoardo Ponti1 Jason Naradowsky3 , Roi Reichart2 Anna Korhonen1 1 2 Language Technology Lab, DTAL, University of Cambridge Faculty of Industrial Engineering and Management, Technion, IIT 3 Johns Hopkins University 1 {dsg40,iv250,ep490,alk23}@cam.ac.uk 2 roiri@ie.technion.ac.il 3 narad@jhu.edu Abstract translation (Vaswani et al., 2013), speech recognition (Mikolov et al., 2010), dialogue generation (Serban et Neural architectures are prominent in the conal., 2016), or summarisation (Filippova et al., 2015). struction of language models (LMs). HowA traditional recurrent neural network (RNN) LM ever, word-level prediction is typically agnossetup operates on a limited closed vocabulary of tic of subword-level information (characters words (Bengio et al., 2003; Mikolov et al., 2010). and character sequences) and operates over The limitation arises due to the model learning paa closed vocabulary, consisting of a limited rameters exclusive to single words. A standard trainword set. Indeed, while subword-aware models boost performance across a variety of NLP ing procedure for neural LMs gradually modi"
Q18-1032,N13-1092,0,0.0746533,"Missing"
Q18-1032,P13-2121,0,0.0327097,"a full-fledged multilingual analysis of representative LM architectures. At the same time, these different architectures serve as the baselines for our novel model which fine-tunes the output matrix M w . As mentioned, the traditional LM setup is to use words both on the input and on the output side (Goodman, 2001; Bengio et al., 2003; Deschacht and Moens, 2009) relying on n-gram word sequences. We evaluate a strong model from the n-gram family of models from the KenLM package (https://github.com/kpu/kenlm): it is based on 5grams with extended Kneser-Ney smoothing (KN5) (Kneser and Ney, 1995; Heafield et al., 2013) 5 . The rationale behind including this non-neural model is to also probe the limitations of such n-gram-based LM architectures on a diverse set of languages. Recurrent neural networks (RNNs), especially Long-Short-Term Memory networks (LSTMs), have taken over the LM universe recently (Mikolov et al., 2010; Sundermeyer et al., 2015; Chen et al., 2016, i.a.). These LMs map a sequence of input words to embedding vectors using a look-up matrix. The embeddings are passed to the LSTM as input, and 3 This choice has been motivated by the observation that rare words tend to have other rare words as"
Q18-1032,P17-1137,0,0.083385,"Missing"
Q18-1032,2005.mtsummit-papers.11,0,0.0462052,"ised in Table 6. As 461 one important finding, we observe that the gains in perplexity using our fine-tuning AP method extend also to these larger evaluation datasets. In particular, we find improvements of the same magnitude as in the PTB-sized data sets over the strongest baseline model (Char-CNN-LSTM) for all MWC languages. For instance, perplexity is reduced from 1781 to 1578 for Russian, and from 365 to 352 for English. We also observe a gain for French and Spanish with perplexity reduced from 282 to 272 and 255 to 243 respectively. In addition, we test on samples of the Europarl corpus (Koehn, 2005; Tiedemann, 2012) which contains approximately 10 times more tokens than our PTB-sized evaluation data: we use 400K sentences from Europarl for training and testing. However, this data comes from a much narrower domain of parliamentary proceedings: this property yields a very low type/token ratio as visible from Table 6. In fact, we find the type/token ratio in this corpus to be on the same level or even smaller than isolating languages (compare with the scores in Table 4): 0.02 for Dutch and 0.03 for Czech. This leads to similar perplexities with and without +AP for these two selected test l"
Q18-1032,P16-1100,0,0.111324,"Missing"
Q18-1032,P11-1015,0,0.0605287,"sed as follows: P (t1 , ...tn ) = Y i P (ti |t1 , ...ti−1 ). (1) ti is a token with the index i in the sequence. For word-level prediction a token corresponds to one word, whereas for character-level (also termed charlevel) prediction it is one character. LMs are most commonly tested on Western European languages. Standard LM benchmarks in English include the Penn Treebank (PTB) (Marcus et al., 1993), the 1 Billion Word Benchmark (BWB) (Chelba et al., 2014), and the Hutter Prize data (Hutter, 2012). English datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent work of Kim et al. (2016) reuses these datasets and adds Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Verwimp et al. (2017) use a subset of"
Q18-1032,J93-2004,0,0.064122,"nd Typological Diversity A language model defines a probability distribution over sequences of tokens, and is typically trained to maximise the likelihood of token input sequences. Formally, the LM objective is expressed as follows: P (t1 , ...tn ) = Y i P (ti |t1 , ...ti−1 ). (1) ti is a token with the index i in the sequence. For word-level prediction a token corresponds to one word, whereas for character-level (also termed charlevel) prediction it is one character. LMs are most commonly tested on Western European languages. Standard LM benchmarks in English include the Penn Treebank (PTB) (Marcus et al., 1993), the 1 Billion Word Benchmark (BWB) (Chelba et al., 2014), and the Hutter Prize data (Hutter, 2012). English datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent wor"
Q18-1032,D16-1209,0,0.261842,"fforts into this direction have yielded exciting results, primarily on the input side of neural LMs. A standard RNN LM architecture relies on two word representation matrices learned during training for its input and next-word prediction. This effectively means that there are two sets of per-word specific parameters that need to be trained. Recent work shows that it is possible to generate a word representation on-the-fly based on its constituent characters, thereby effectively solving the problem for the parameter set on the input side of the model (Kim et al., 2016; Luong and Manning, 2016; Miyamoto and Cho, 2016; Ling et al., 2015). However, it is not straightforward how to advance these ideas to the output side of the model, as this second set of word-specific parameters is directly responsible for the next-word prediction: it has to encode a much wider range of information, such as topical and semantic knowledge about words, which cannot be easily obtained from its characters alone (Jozefowicz et al., 2016). While one solution is to directly output characters instead of words (Graves, 2013; Miyamoto and Cho, 2016), a recent work from Jozefowicz et al. (2016) suggests that such purely character-base"
Q18-1032,Q17-1022,1,0.901431,"Missing"
Q18-1032,oostdijk-2000-spoken,0,0.0770072,"tion (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent work of Kim et al. (2016) reuses these datasets and adds Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Verwimp et al. (2017) use a subset of the Corpus of Spoken Dutch (Oostdijk, 2000) for Dutch LM. Kawakami et al. (2017) evaluate on 7 European languages using Wikipedia data, including Finnish. Perhaps the largest and most diverse set of languages used for multilingual LM evaluation so far is the one of Vania and Lopez (2017). Their study includes 10 languages in total representing several morphological types (fusional, e.g., Russian, and agglutinative, e.g., Finnish), as well as languages with particular morphological phenomena (root-and-pattern in Hebrew and reduplication in Malay). In this work, we provide LM evaluation datasets for 50 typologically diverse languages, wi"
Q18-1032,E17-2025,0,0.0199535,"in the sequence. For word-level prediction a token corresponds to one word, whereas for character-level (also termed charlevel) prediction it is one character. LMs are most commonly tested on Western European languages. Standard LM benchmarks in English include the Penn Treebank (PTB) (Marcus et al., 1993), the 1 Billion Word Benchmark (BWB) (Chelba et al., 2014), and the Hutter Prize data (Hutter, 2012). English datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent work of Kim et al. (2016) reuses these datasets and adds Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Verwimp et al. (2017) use a subset of the Corpus of Spoken Dutch (Oostdijk, 2000) for Dutch LM. Kawakami et al. (2017) evaluate on 7 Euro"
Q18-1032,tiedemann-2012-parallel,0,0.0172916,"6. As 461 one important finding, we observe that the gains in perplexity using our fine-tuning AP method extend also to these larger evaluation datasets. In particular, we find improvements of the same magnitude as in the PTB-sized data sets over the strongest baseline model (Char-CNN-LSTM) for all MWC languages. For instance, perplexity is reduced from 1781 to 1578 for Russian, and from 365 to 352 for English. We also observe a gain for French and Spanish with perplexity reduced from 282 to 272 and 255 to 243 respectively. In addition, we test on samples of the Europarl corpus (Koehn, 2005; Tiedemann, 2012) which contains approximately 10 times more tokens than our PTB-sized evaluation data: we use 400K sentences from Europarl for training and testing. However, this data comes from a much narrower domain of parliamentary proceedings: this property yields a very low type/token ratio as visible from Table 6. In fact, we find the type/token ratio in this corpus to be on the same level or even smaller than isolating languages (compare with the scores in Table 4): 0.02 for Dutch and 0.03 for Czech. This leads to similar perplexities with and without +AP for these two selected test languages. The thir"
Q18-1032,P17-1184,0,0.284804,"gical systems. We discuss the implications of typological diversity on the LM task, both theoretically in Section 2, and empirically in Section 7; we find a clear correspondence between performance of state-of-the art LMs and structural linguistic properties. Further, the consistent perplexity gains across the large sample of languages suggest wide applicability of our novel method. Finally, this article can also be read as a comprehensive multilingual analysis of current LM architectures on a set of languages which is much larger than the ones used in recent LM work (Botha and Blunsom, 2014; Vania and Lopez, 2017; Kawakami et al., 2017). We hope that this article with its new datasets, methodology and models, all available online at http://people.ds.cam. ac.uk/dsg40/lmmrl.html, will pave the way for true multilingual research in language modeling. 2 LM Data and Typological Diversity A language model defines a probability distribution over sequences of tokens, and is typically trained to maximise the likelihood of token input sequences. Formally, the LM objective is expressed as follows: P (t1 , ...tn ) = Y i P (ti |t1 , ...ti−1 ). (1) ti is a token with the index i in the sequence. For word-level pred"
Q18-1032,D13-1140,0,0.067895,"Missing"
Q18-1032,E17-1040,0,0.0805667,"Missing"
Q18-1032,P17-1006,1,0.915822,"Missing"
Q18-1032,P16-1125,0,0.0319225,".ti−1 ). (1) ti is a token with the index i in the sequence. For word-level prediction a token corresponds to one word, whereas for character-level (also termed charlevel) prediction it is one character. LMs are most commonly tested on Western European languages. Standard LM benchmarks in English include the Penn Treebank (PTB) (Marcus et al., 1993), the 1 Billion Word Benchmark (BWB) (Chelba et al., 2014), and the Hutter Prize data (Hutter, 2012). English datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent work of Kim et al. (2016) reuses these datasets and adds Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Verwimp et al. (2017) use a subset of the Corpus of Spoken Dutch (Oostdijk, 2000) for Dutc"
Q18-1032,Q15-1025,0,0.161463,"ction cannot fully capture word-level semantics and even hurts LM performance (Jozefowicz et al., 2016). However, shared subword units still provide useful evidence of shared semantics (Cotterell et al., 2016; Vuli´c et al., 2017): injecting this into the space M w to additionally reflect shared subword-level information should lead to improved word vector estimates, especially for MRLs. 5.1 Fine-Tuning and Constraints We inject this information into M w by adapting recent fine-tuning (often termed retrofitting or specialisation) methods for vector space post-processing (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017; Vuli´c et al., 2017, i.a.). These models enrich initial vector spaces by encoding external knowledge provided in the form of simple linguistic constraints (i.e., word pairs) into the initial vector space. There are two fundamental differences between our work and previous work on specialisation. First, previous models typically use rich hand-crafted lexical resources such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013), or manually defined rules (Vuli´c et al., 2017) to extract the constraints, while we generate them directly using the"
S12-1025,W99-0901,0,0.504301,") Pcw (s|p, r) = P (2) (s|r) P (p|cs,p,r , r) PP (p|r) 0 s0 ∈S |r) P (p|cs0 ,p,r , r) PP(s (p|r) (3) where cs,p,r is the root node of the subhierarchy containing s that was selected for p. An alternative approach to modelling with WordNet uses its hierarchical structure to define a Markov model with transitions from senses to senses and from senses to words. The intuition here is that each observation is generated by a “walk” from the root of the hierarchy to a leaf node and emitting the word 1 In this paper we use WordNet version 3.0, except where stated otherwise. corresponding to the leaf. Abney and Light (1999) proposed such a model for selectional preferences, trained via EM, but failed to achieve competitive performance on a pseudodisambiguation task. The models described above have subsequently been used in many different studies. For example: McCarthy and Carroll (2003) use Li and Abe’s method in a word sense disambiguation setting; Schulte im Walde et al. (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in w"
S12-1025,D08-1007,0,0.281626,"nd semantic subcategorisation frame learning; Shutova (2010) deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in which the methods of Resnik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone. These methods can be vectorbased (Erk et al., 2010; Thater et al., 2010), discriminative (Bergsma et al., 2008) or probabilistic ´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger (O and Mooney, 2011). In the probabilistic category, Bayesian models based on the “topic modelling” framework (Blei et al., 2003b) have been shown to achieve state-of-the-art performance in a number of evaluation settings; the models considered in this paper are also related to this framework. In machine learning, researchers have proposed a variety of topic modelling methods where the latent variables are arranged in a hierarchical structure (Blei et al., 2003a; Mimno et al., 2007). In contrast to the present work, these mode"
S12-1025,D07-1109,0,0.571507,"2003b) have been shown to achieve state-of-the-art performance in a number of evaluation settings; the models considered in this paper are also related to this framework. In machine learning, researchers have proposed a variety of topic modelling methods where the latent variables are arranged in a hierarchical structure (Blei et al., 2003a; Mimno et al., 2007). In contrast to the present work, these models use a relatively shallow hierarchy (e.g., 3 levels) and any hierarchy node can in principle emit any vocabulary item; they thus provide a poor match for our goal of modelling over WordNet. Boyd-Graber et al. (2007) describe a topic model that is directly influenced by Abney and Light’s Markov model approach; this model (LDAWN) is described further in Section 3.3 below. Reisinger and Pas¸ca (2009) investigate Bayesian methods for attaching attributes harvested from the Web at an appropriate level in the WordNet hierarchy; this task is related in spirit to the preference learning task. 172 3 3.1 Probabilistic modelling over WordNet Notation We assume that we have a lexical hierarchy in the form of a directed acyclic graph G = (S, E) where each node (or synset) s ∈ S is associated with a set of words Wn be"
S12-1025,P06-4020,0,0.0663844,"ear separately). Plausibility judgements were elicited from a large group of human subjects, then normalised and logtransformed. Table 1 gives a representative illus´ tration of the data. Following the evaluation in O S´eaghdha (2010), with which we wish to compare, we use Pearson r and Spearman ρ correlation coefficients as performance measures. All models were trained on the 90-million word 2 For a related argument in the context of topic model evaluation, see Chang et al. (2009). 175 written component of the British National Corpus,3 lemmatised, POS-tagged and parsed with the RASP toolkit (Briscoe et al., 2006). We removed predicates occurring with just one argument type and all tokens containing non-alphabetic characters. The resulting datasets consist of 3,587,172 verbobject observations (7,954 predicate types, 80,107 argument types), 3,732,470 noun-noun observations (68,303 predicate types, 105,425 argument types) and 3,843,346 adjective-noun observations (29,975 predicate types, 62,595 argument types). All the Bayesian models were trained by Gibbs sampling, as outlined above. For each model we run three sampling chains for 1,000 iterations and average the plausibility predictions for each to pro"
S12-1025,E03-1034,0,0.720475,"ted otherwise. corresponding to the leaf. Abney and Light (1999) proposed such a model for selectional preferences, trained via EM, but failed to achieve competitive performance on a pseudodisambiguation task. The models described above have subsequently been used in many different studies. For example: McCarthy and Carroll (2003) use Li and Abe’s method in a word sense disambiguation setting; Schulte im Walde et al. (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in which the methods of Resnik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone. These methods can be vectorbased (Erk et al., 2010; Thater et al., 2010), discriminative (Bergsma et al., 2008) or probabilistic ´ S´eaghdha, 2010; Ritter et al., 2010; Reisinger (O and Mooney, 2011). In the probabilistic category,"
S12-1025,J02-2003,0,0.865697,"nd Abe (1998) propose a model in which the appropriate cut c is selected according to the Minimum Description Length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising a sum of model description length and data description length. The probability of a predicate p taking as its argument an synset s is modelled as: 2 where cs,p,r is the portion of the cut learned for p that dominates s. The distribution P (s|cs,p,r ) is held to be uniform over all synsets dominated by cs,p,r , while P (c|p) is given by a maximum likelihood estimate. Clark and Weir (2002) present a model that, while not explicitly described as cut-based, likewise seeks to find the right level of generalisation for an observation. In this case, the hypernym at which to “cut” is chosen by a chi-squared test: if the aggregate preference of p for classes in the subhierarchy rooted at c differs significantly from the individual preferences of p for the immediate children of c, the hierarchy is cut below c. The probability of p taking a synset s as its argument is given by: Background and Related Work The WordNet lexical hierarchy (Fellbaum, 1998) is one of the most-used resources i"
S12-1025,J10-4007,0,0.447457,"Missing"
S12-1025,J03-3005,0,0.694024,"rb laugh is more plausibly filled by a person than by a vegetable. Human language users’ knowledge about selectional preferences has been implicated in analyses of metaphor processing (Wilks, 1978) and in psycholinguistic studies of comprehension (Rayner et al., 2004). In Natural Language Processing, automatically acquired preference models have been shown to aid a number of tasks, including semantic It is tempting to assume that with a large enough corpus, preference learning reduces to a simple language modelling task that can be solved by counting predicate-argument co-occurrences. Indeed, Keller and Lapata (2003) show that relatively good performance at plausibility estimation can be attained by submitting queries to a Web search engine. However, there are many scenarios where this approach is insufficient: for languages and language domains where Web-scale data is unavailable, for predicate types (e.g., inference rules or semantic roles) that cannot be retrieved by keyword search and for applications where accurate models of rarer words are ´ S´eaghdha (2010) shows that the Webrequired. O based approach is reliably outperformed by more complex models trained on smaller corpora for less frequent predi"
S12-1025,J98-2002,0,0.8129,". Focussing here on (a), we demonstrate that our models attain better performance than previously-proposed WordNet-based methods on a plausibility estimation task and are particularly wellsuited to estimating plausibility for arguments that were not seen in training and for which LDA cannot make useful predictions. rooted at the synset food#n#1, as all hyponyms of that synset are assumed to be edible and the immediate hypernym of the synset, substance#n#1, is too general given that many substances are rarely eaten.1 This leads to the notion of “cutting” the hierarchy at one or more positions (Li and Abe, 1998). The modelling task then becomes that of finding the cuts that are maximally general without overgeneralising. Li and Abe (1998) propose a model in which the appropriate cut c is selected according to the Minimum Description Length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising a sum of model description length and data description length. The probability of a predicate p taking as its argument an synset s is modelled as: 2 where cs,p,r is the portion of the cut learned for p that dominates s. The distribution P (s|cs,p,r ) is"
S12-1025,J03-4004,0,0.549964,"ructure to define a Markov model with transitions from senses to senses and from senses to words. The intuition here is that each observation is generated by a “walk” from the root of the hierarchy to a leaf node and emitting the word 1 In this paper we use WordNet version 3.0, except where stated otherwise. corresponding to the leaf. Abney and Light (1999) proposed such a model for selectional preferences, trained via EM, but failed to achieve competitive performance on a pseudodisambiguation task. The models described above have subsequently been used in many different studies. For example: McCarthy and Carroll (2003) use Li and Abe’s method in a word sense disambiguation setting; Schulte im Walde et al. (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in which the methods of Resnik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations"
S12-1025,D11-1097,1,0.91303,"Missing"
S12-1025,P10-1045,1,0.913983,"Missing"
S12-1025,P06-1100,0,0.0631502,"Missing"
S12-1025,D11-1130,0,0.0716457,"els trained on smaller corpora for less frequent predicate-argument combinations. Models that induce a level of semantic representation, such as probabilistic latent variable models, have a further advantage in that they can provide rich structured information for downstream tasks such as lexical dis´ S´eaghdha and Korhonen, 2011) and ambiguation (O semantic relation mining (Yao et al., 2011). Recent research has investigated the potential of Bayesian probabilistic models such as Latent Dirichlet Allocation (LDA) for modelling selec´ S´eaghdha, 2010; Ritter et al., tional preferences (O 2010; Reisinger and Mooney, 2011). These models are flexible and robust, yielding superior performance compared to previous approaches. In this paper we present a preliminary study of analogous 170 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 170–179, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics models that make use of a lexical hierarchy (in our case the WordNet hierarchy). We describe two broad classes of probabilistic models over WordNet and how they can be implemented in a Bayesian framework. The two main potential advantages of incorporating WordNet in"
S12-1025,P09-1070,0,0.0645557,"Missing"
S12-1025,P10-1044,0,0.321535,"Missing"
S12-1025,P08-1057,0,0.180668,"ntuition here is that each observation is generated by a “walk” from the root of the hierarchy to a leaf node and emitting the word 1 In this paper we use WordNet version 3.0, except where stated otherwise. corresponding to the leaf. Abney and Light (1999) proposed such a model for selectional preferences, trained via EM, but failed to achieve competitive performance on a pseudodisambiguation task. The models described above have subsequently been used in many different studies. For example: McCarthy and Carroll (2003) use Li and Abe’s method in a word sense disambiguation setting; Schulte im Walde et al. (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in which the methods of Resnik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone. These methods can be vectorbase"
S12-1025,N10-1147,0,0.0432699,"he word 1 In this paper we use WordNet version 3.0, except where stated otherwise. corresponding to the leaf. Abney and Light (1999) proposed such a model for selectional preferences, trained via EM, but failed to achieve competitive performance on a pseudodisambiguation task. The models described above have subsequently been used in many different studies. For example: McCarthy and Carroll (2003) use Li and Abe’s method in a word sense disambiguation setting; Schulte im Walde et al. (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik’s method for metaphor interpretation. Brockmann and Lapata (2003) report a comparative evaluation in which the methods of Resnik and Clark and Weir outpeform Li and Abe’s method on a plausibility estimation task. Much recent work on preference learning has focused on purely distributional methods that do not use a predefined hierarchy but learn to make generalisations about predicates and arguments from corpus observations alone. These methods can be vectorbased (Erk et al., 2010; Thater et al., 2010), discriminative (Bergsma et al., 2008) or probabilistic ´ S´eaghdha, 2010; Ri"
S12-1025,P10-1097,0,0.0823741,"Missing"
S12-1025,E09-1092,0,0.132512,"Missing"
S12-1025,D11-1135,0,0.0334435,"c roles) that cannot be retrieved by keyword search and for applications where accurate models of rarer words are ´ S´eaghdha (2010) shows that the Webrequired. O based approach is reliably outperformed by more complex models trained on smaller corpora for less frequent predicate-argument combinations. Models that induce a level of semantic representation, such as probabilistic latent variable models, have a further advantage in that they can provide rich structured information for downstream tasks such as lexical dis´ S´eaghdha and Korhonen, 2011) and ambiguation (O semantic relation mining (Yao et al., 2011). Recent research has investigated the potential of Bayesian probabilistic models such as Latent Dirichlet Allocation (LDA) for modelling selec´ S´eaghdha, 2010; Ritter et al., tional preferences (O 2010; Reisinger and Mooney, 2011). These models are flexible and robust, yielding superior performance compared to previous approaches. In this paper we present a preliminary study of analogous 170 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 170–179, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics models that make use of a lexical"
S12-1025,P09-2019,0,0.123059,"Missing"
S12-1025,P11-1156,0,0.0678019,"Missing"
S17-1003,P15-1040,0,0.109629,"sequent improvements of this line of research include the Structural Attention Neural Networks (Kokkinos and Potamianos, 2017), which adds structural information around each node of a syntactic tree. When supervised monolingual models are not feasible, language transfer can bridge between multiple languages, for instance through supervised latent Dirichlet allocation (Boyd-Graber and Resnik, 2010). Direct transfer relies on word-aligned parallel texts where the source language text is either manually or automatically annotated. The sentiment information is then projected onto the target text (Almeida et al., 2015), also leveraging non-parallel data (Zhou et al., 2015). Chen et al. (2016) devised a multi-task network where an adversarial branch spurs the shared layers to learn language-independent features. Finally, Lu et al. (2011) learned from annotated examples in both the source and target language. Alternatively, sentences from other languages are translated into English and assigned a sentiment based on lexical resources (Denecke, 2008) or supervised methods Multilingual Sentiment Analysis The task of sentiment classification is mostly addressed through supervised approaches. However, these achiev"
S17-1003,N16-1162,1,0.891573,"Missing"
S17-1003,P15-1162,0,0.0604088,"Missing"
S17-1003,E17-2093,0,0.0424732,"Missing"
S17-1003,D10-1005,0,0.0306589,"ched with context (Mousa and Schuller, 2017). On the other hand, Socher et al. (2013) put forth a Recursive Neural Tensor Network, which composes representations recursively through a single tensor-based composition function. Subsequent improvements of this line of research include the Structural Attention Neural Networks (Kokkinos and Potamianos, 2017), which adds structural information around each node of a syntactic tree. When supervised monolingual models are not feasible, language transfer can bridge between multiple languages, for instance through supervised latent Dirichlet allocation (Boyd-Graber and Resnik, 2010). Direct transfer relies on word-aligned parallel texts where the source language text is either manually or automatically annotated. The sentiment information is then projected onto the target text (Almeida et al., 2015), also leveraging non-parallel data (Zhou et al., 2015). Chen et al. (2016) devised a multi-task network where an adversarial branch spurs the shared layers to learn language-independent features. Finally, Lu et al. (2011) learned from annotated examples in both the source and target language. Alternatively, sentences from other languages are translated into English and assign"
S17-1003,P11-1033,0,0.0248891,"models are not feasible, language transfer can bridge between multiple languages, for instance through supervised latent Dirichlet allocation (Boyd-Graber and Resnik, 2010). Direct transfer relies on word-aligned parallel texts where the source language text is either manually or automatically annotated. The sentiment information is then projected onto the target text (Almeida et al., 2015), also leveraging non-parallel data (Zhou et al., 2015). Chen et al. (2016) devised a multi-task network where an adversarial branch spurs the shared layers to learn language-independent features. Finally, Lu et al. (2011) learned from annotated examples in both the source and target language. Alternatively, sentences from other languages are translated into English and assigned a sentiment based on lexical resources (Denecke, 2008) or supervised methods Multilingual Sentiment Analysis The task of sentiment classification is mostly addressed through supervised approaches. However, these achieve unsatisfactory results in resource-lean 23 (Balahur and Turchi, 2014). Finally, cross-lingual sentiment classification can leverage on shared distributed representations. Zhou et al. (2016) captured shared high-level fea"
S17-1003,D17-1070,0,0.0556519,"Missing"
S17-1003,S14-2001,0,0.0345391,"Missing"
S17-1003,D14-1079,0,0.0159063,"dition) over word representations stemming from a Skip-Gram model (§ 3.1.1). Others are direct methods, including FastSent (§ 3.1.2), a Sequential Denoising AutoEncoder (SDAE, § 3.1.3) and Paragraph Vector (§ 3.1.4). Note that FastSent relies on sentence order, SDAE on word order, and Paragraph Vector on neither. All these algorithms were trained on cleaned-up Wikipedia dumps. The choice of the algorithms was based on following criteria: i) their performance reported in recent surveys (n.b., the surveys were limited to English and evaluated on other tasks), most notably Hill et al. (2016) and Milajevs et al. (2014); ii) the variety of their modelling assumptions and features encoded. The referenced surveys already hinted that the usefulness of a representation is largely dependent on the actual application. Shallower but more interpretable representations can be decoded with spatial distance metrics. Others, more deep and convoluted architectures, outperform the others in supervised tasks. We inquire whether the generalisation is tenable also in the task of Sentiment Analysis targeting sentence polarity. 3.1.1 Additive Skip-Gram As a bottom-up method, we train word embeddings using skip-gram with negati"
S17-1003,D16-1157,0,0.0992835,"t (Mitchell and Lapata, 2010). 3.1.2 FastSent The FastSent model was proposed by Hill et al. (2016). It hinges on a sentence-level distributional hypothesis (Polajnar et al., 2015; Kiros et al., 2015). In other terms, it assumes that the meaning of a sentence can be inferred by the neighbour sentences in a text. It is a simple additive log-linear model conceived to mitigate the computational expensiveness of algorithms based on a similar assumption. 1 This excludes methods concerned with phrases, like the ECO embeddings (Poliak et al., 2017), or requiring structured knowledge, like CHARAGRAM (Wieting et al., 2016a). 24 Hence, it was preferred over SkipThought (Kiros et al., 2015) because of i) these efficiency issues and ii) its competitive performances reported by Hill et al. (2016). In FastSent, sentences are represented as bags of words: a context of sentences is used to predict the adjacent sentence. Each word w corresponds to a source vector uw and a target vector vw . A sentence Si is represented P as the sum of the source vectors of its words w∈Si uw . Hence, the cost C of a representation is given by the softmax σ(x) of a sentence representation and the target vectors of the words in its conte"
S17-1003,E17-1096,0,0.0259081,"ource scarcity: they are portable to other tasks and languages. In this section we survey deep learning techniques, adaptive models, and unsupervised distributed representations for sentiment classification in a multilingual scenario. The last approach is the focus of this work. Deep learning algorithms for sentiment classification are designed to deal with compositionality. Hence, they often rely on recurrent networks tracing the sequential history of a sentence, or special compositional devices. Recurrent models include bi-directional LSTMs (Li et al., 2015), possibly enriched with context (Mousa and Schuller, 2017). On the other hand, Socher et al. (2013) put forth a Recursive Neural Tensor Network, which composes representations recursively through a single tensor-based composition function. Subsequent improvements of this line of research include the Structural Attention Neural Networks (Kokkinos and Potamianos, 2017), which adds structural information around each node of a syntactic tree. When supervised monolingual models are not feasible, language transfer can bridge between multiple languages, for instance through supervised latent Dirichlet allocation (Boyd-Graber and Resnik, 2010). Direct transf"
S17-1003,W15-2701,0,0.0198192,"t the word level. In this paper, we focus on sentence representations that are generated in an unsupervised fashion. Furthermore, they are ‘fixed’, that is, they are not fine-tuned for any particular downstream task, since we are interested in their intrinsic content.1 Y (w,c)∈S p(S = 1|w, c, θ) Y p(S 0 = 0|w, c, θ) (w,c)∈S 0 The representation of a sentence was obtained via element-wise addition of the vectors of the words belonging to it (Mitchell and Lapata, 2010). 3.1.2 FastSent The FastSent model was proposed by Hill et al. (2016). It hinges on a sentence-level distributional hypothesis (Polajnar et al., 2015; Kiros et al., 2015). In other terms, it assumes that the meaning of a sentence can be inferred by the neighbour sentences in a text. It is a simple additive log-linear model conceived to mitigate the computational expensiveness of algorithms based on a similar assumption. 1 This excludes methods concerned with phrases, like the ECO embeddings (Poliak et al., 2017), or requiring structured knowledge, like CHARAGRAM (Wieting et al., 2016a). 24 Hence, it was preferred over SkipThought (Kiros et al., 2015) because of i) these efficiency issues and ii) its competitive performances reported by Hil"
S17-1003,D11-1016,0,0.0355862,"inates correctly whether it belongs to a set of sentences S or a set of randomly generated incorrect sentences S 0 : Distributed Sentence Representations Word vectors can be combined through various compositional operations to obtain representations of phrases and sentences. Mitchell and Lapata (2010) explored two operations: addition and multiplication. Notwithstanding their simplicity, they are hardly outperformed by more sophisticated operations (Rimell et al., 2016). Some of these compositional representations based on matrix multiplication were also evaluated on sentiment classification (Yessenalina and Cardie, 2011). Alternatively, sentence representations can be induced directly with no intermediate step at the word level. In this paper, we focus on sentence representations that are generated in an unsupervised fashion. Furthermore, they are ‘fixed’, that is, they are not fine-tuned for any particular downstream task, since we are interested in their intrinsic content.1 Y (w,c)∈S p(S = 1|w, c, θ) Y p(S 0 = 0|w, c, θ) (w,c)∈S 0 The representation of a sentence was obtained via element-wise addition of the vectors of the words belonging to it (Mitchell and Lapata, 2010). 3.1.2 FastSent The FastSent model"
S17-1003,E17-2081,0,0.0602349,"Missing"
S17-1003,P16-1133,0,0.0363391,"e-independent features. Finally, Lu et al. (2011) learned from annotated examples in both the source and target language. Alternatively, sentences from other languages are translated into English and assigned a sentiment based on lexical resources (Denecke, 2008) or supervised methods Multilingual Sentiment Analysis The task of sentiment classification is mostly addressed through supervised approaches. However, these achieve unsatisfactory results in resource-lean 23 (Balahur and Turchi, 2014). Finally, cross-lingual sentiment classification can leverage on shared distributed representations. Zhou et al. (2016) captured shared high-level features across aligned sentences through autoencoders. In this latent space, distances were optimised to reflect differences in sentiment. On the other hand, Fern´andez et al. (2015) exploited bilingual word representations, where vector dimensions mirror the distributional overlap with respect to a pivot. Le and Mikolov (2014) concatenated sentence representations obtained through variants of Paragraph Vector and trained a Logistic Regression model on top of them. Previous studies thus demonstrated that sentence representations retain information about polarity, a"
S17-1003,D13-1170,0,0.0639577,"herein. This attitude is measured quantitatively on a scale spanning from negative to positive with arbitrary granularity. As such, polarity consists in a crucial part of the meaning of a sentence, which should not be lost. The polarity of a sentence depends heavily on a complex interaction between lexical items endowed with an intrinsic polarity, and morphosyntactic constructions altering polarity, most notably negation and concession. The interaction is deemed to be recursive, hence some approaches take into account word order and phrase boundaries in order to apply the correct composition (Socher et al., 2013). However, some languages lack continuous constituents: contiguous spans of words do not correspond to syntactic subtrees, making composition unreliable (Ponti, 2016). Moreover, the expression of negation varies across languages, as demonstrated by works in Linguistic Typology (Dahl, 1979, inter alia). In particular, negation can appear as a bounded morpheme or a free morpheme; it can precede or follow the verb; it can ‘agree’ or not in polarity with indefinite pronouns; it can alter the expression of verbal Distributed representations of sentences have been developed recently to represent the"
S17-1003,S15-2027,0,0.0447492,"Missing"
S17-1003,J16-4004,0,\N,Missing
S19-1007,S13-2050,0,0.0121286,"a similar setup to CBOW (Mikolov et al., 2013) except that the context is represented with a Bidirectional LSTM rather than as a bag of words. In this way, context2vec captures sequence information in the context, and is able to produce high-quality substitutes for a sentence-completion task, while overcoming the sparseness issues in the previous substitutebased approaches. Kobayashi et al. (2017) finetune this context2vec representation to compute entity representations in a discourse for the language modelling task. A related application of second-order substitutes is word sense induction. Baskaya et al. (2013) represent contexts as second-order substitutes and apply co-occurrence modelling on top of the instance id - substitute pairs. Alagi´c et al. (2018) propose a similar method to our paper and showed that second-order lexical substitutes and first-order contexts complement each other in word sense induction. Our paper provides alternative evidence for the use of lexical substitutes in the setting of rare word modelling with analysis on the effect from different contexts. . 3 cosine(ContextVec, S0i ) f (S0i ) = P20 (1) 0 j=1 cosine(ContextVec, Sj ) SC = 20 X f (S0i ) ∗ Si (2) i=1 To directly com"
S19-1007,Q17-1010,0,0.070915,"ategy for new words but would require the contexts that come at the beginning of the training to be maximally informative. Recent Introduction As language vocabulary follows the zipfian distribution, we expect to encounter a large number of rare and unseen words no matter how large the training corpus is. The effective handling of such words is thus crucial for Natural Language Processing (NLP). Attempts to learn rare and unseen word representations can be categorized into the following three approaches: (1) constructing target word embeddings from the subword components (Pinter et al., 2017; Bojanowski et al., 2017), (2). leveraging definitions or relational structures from external resources such as Wordnet (Bahdanau et al., 2017; Pilehvar and Collier, 2017), and (3) modelling the target word from few available contexts. Our paper falls into the last approach. We demonstrate improvements in performance by employing an alternative context representation, second-order lexical substitutes, as opposed 1 The experiments can be reproduced at https:// github.com/qianchu/rare_we.git. 61 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 61–67 c Minneapolis, June 6–7,"
S19-1007,E17-2062,0,0.0170927,"As language vocabulary follows the zipfian distribution, we expect to encounter a large number of rare and unseen words no matter how large the training corpus is. The effective handling of such words is thus crucial for Natural Language Processing (NLP). Attempts to learn rare and unseen word representations can be categorized into the following three approaches: (1) constructing target word embeddings from the subword components (Pinter et al., 2017; Bojanowski et al., 2017), (2). leveraging definitions or relational structures from external resources such as Wordnet (Bahdanau et al., 2017; Pilehvar and Collier, 2017), and (3) modelling the target word from few available contexts. Our paper falls into the last approach. We demonstrate improvements in performance by employing an alternative context representation, second-order lexical substitutes, as opposed 1 The experiments can be reproduced at https:// github.com/qianchu/rare_we.git. 61 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 61–67 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics and their fitness scores are generated from context2vec. Compared with the context2vec repre"
S19-1007,P18-1002,0,0.397604,"nd-order contexts from lexical substitutes for few-shot learning of word representations Qianchu Liu, Diana McCarthy, Anna Korhonen Language Technology Lab, University of Cambridge English Faculty Building, 9 West Road, Cambridge CB3 9DA, United Kingdom ql261@cam.ac.uk, diana@dianamccarthy.co.uk, alk23@cam.ac.uk Abstract to the traditional bag of word context representations. In line with previous research in this area, we evaluate our methodology on three tasks that measure the quality of the induced unseen word representation from contexts (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Khodak et al., 2018). Our results reveal that the three tasks involve different types of contexts which put different emphasis on first or second order contexts. Our second-order substitute-based method achieves the best performance for modelling rare words in natural contexts from corpora. In the tasks in which both first order and second order contexts are important, the ensemble of these two types of contexts yields superior performance. 1 There is a growing awareness of the need to handle rare and unseen words in word representation modelling. In this paper, we focus on few-shot learning of emerging concepts"
S19-1007,D17-1010,0,0.112609,"te and processing strategy for new words but would require the contexts that come at the beginning of the training to be maximally informative. Recent Introduction As language vocabulary follows the zipfian distribution, we expect to encounter a large number of rare and unseen words no matter how large the training corpus is. The effective handling of such words is thus crucial for Natural Language Processing (NLP). Attempts to learn rare and unseen word representations can be categorized into the following three approaches: (1) constructing target word embeddings from the subword components (Pinter et al., 2017; Bojanowski et al., 2017), (2). leveraging definitions or relational structures from external resources such as Wordnet (Bahdanau et al., 2017; Pilehvar and Collier, 2017), and (3) modelling the target word from few available contexts. Our paper falls into the last approach. We demonstrate improvements in performance by employing an alternative context representation, second-order lexical substitutes, as opposed 1 The experiments can be reproduced at https:// github.com/qianchu/rare_we.git. 61 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 61–6"
S19-1007,S18-2004,0,0.0385523,"achieved superior performance on naturallyoccurring contexts from corpora. 1 2 2.1 Related work First-order context The most naive way of inducing new word representation from contexts is to simply take the average of context word embeddings that co-occur with the target word in a sentence. With stop words removed, this simple method has proven to be a strong baseline as shown in Lazaridou et al. (2017) and Herbelot and Baroni (2017). A potential improvement from the simple additive baseline model is that we weigh words with ISF (inverse sentence frequency). We follow the definition of ISF in Samardzhiev et al. (2018) and implement it as a baseline model in our study. More recently, Khodak et al. (2018) learn a transformation matrix to reconstruct pre-trained word embeddings, which essentially learns to highlight informative dimensions. Along a different line, Herbelot and Baroni (2017) take a high-risk learning rate and processing strategy for new words but would require the contexts that come at the beginning of the training to be maximally informative. Recent Introduction As language vocabulary follows the zipfian distribution, we expect to encounter a large number of rare and unseen words no matter how"
S19-1007,I17-1048,0,0.0251347,"ng fitness weights of each substitute in the context (Melamud et al., 2015; Yatbaz et al., 2012; Melamud et al., 2015). Melamud et al. (2016) later on introduced context2vec which trains both context and word embeddings in a similar setup to CBOW (Mikolov et al., 2013) except that the context is represented with a Bidirectional LSTM rather than as a bag of words. In this way, context2vec captures sequence information in the context, and is able to produce high-quality substitutes for a sentence-completion task, while overcoming the sparseness issues in the previous substitutebased approaches. Kobayashi et al. (2017) finetune this context2vec representation to compute entity representations in a discourse for the language modelling task. A related application of second-order substitutes is word sense induction. Baskaya et al. (2013) represent contexts as second-order substitutes and apply co-occurrence modelling on top of the instance id - substitute pairs. Alagi´c et al. (2018) propose a similar method to our paper and showed that second-order lexical substitutes and first-order contexts complement each other in word sense induction. Our paper provides alternative evidence for the use of lexical substitu"
S19-1007,D18-1173,0,0.0780631,"ontextVec 3 be the context representation produced by context2vec, S 0 be the set of the top 20 substitute target word vectors produced by context2vec, S be the same 20 substitutes that we look up in our base word embedding space, and f (S0i ) be the normalized fitness score of S0i as defined in equation 1. The substitute-based context (SC), and thus the unseen word representation for this context, is defined in equation 2. If the unseen word occurs multiple times, we average the unseen word representations across the multiple contexts. work implements a memory-augmented word embedding model (Sun et al., 2018) however our system shows comparable or superior performance on the two intrinsic tasks that they use (Table 1 below and Table 1 of their paper). 2.2 Second-order substitute-based context An alternative to a bag-of-words representation is a second-order substitute vector generated by a language model for the target word’s slot. For example, we can represent the context ‘It is a move.’ as a substitute vector [big 0.35, good 0.28, bold 0.05, ...] with the numbers indicating fitness weights of each substitute in the context (Melamud et al., 2015; Yatbaz et al., 2012; Melamud et al., 2015). Melamu"
S19-1007,W13-3512,0,0.0858804,"um information about the target word. We show such an example with the nearest neighbours of the representations induced by our substitutes model and additive ISF in Table 2. We can see that while the additive ISF representation is easily affected by unrelated words in the sentence, the substitutes approach clearly has at least identified that the target word is likely to be a kind of animal. The Contextual Rare Words dataset (CRW) The Contextual Rare Words dataset (CRW) was introduced by Khodak et al. (2018). It consists of a subset of 562 word pairs from the original Rare Word (RW) Dataset (Luong et al., 2013). For each pair, the second word is the rare word and is accompanied by 255 contexts. We follow the experiment setup in Khodak et al. (2018) and use their pre-trained vectors on the subcorpus that does not contain any of the rare words from the dataset. This subcorpus is also used to train the context2vec model that generates substitutes. As in Khodak et al. (2018), we randomly choose 2, 4, 6..128 number of contexts as separate conditions for 100 trials, and use these contexts to predict the rare word representations. Cosine similarity is computed between the rare word representation from the"
S19-1007,D12-1086,0,0.067739,"Missing"
S19-1007,N15-1050,0,0.0155688,"rk implements a memory-augmented word embedding model (Sun et al., 2018) however our system shows comparable or superior performance on the two intrinsic tasks that they use (Table 1 below and Table 1 of their paper). 2.2 Second-order substitute-based context An alternative to a bag-of-words representation is a second-order substitute vector generated by a language model for the target word’s slot. For example, we can represent the context ‘It is a move.’ as a substitute vector [big 0.35, good 0.28, bold 0.05, ...] with the numbers indicating fitness weights of each substitute in the context (Melamud et al., 2015; Yatbaz et al., 2012; Melamud et al., 2015). Melamud et al. (2016) later on introduced context2vec which trains both context and word embeddings in a similar setup to CBOW (Mikolov et al., 2013) except that the context is represented with a Bidirectional LSTM rather than as a bag of words. In this way, context2vec captures sequence information in the context, and is able to produce high-quality substitutes for a sentence-completion task, while overcoming the sparseness issues in the previous substitutebased approaches. Kobayashi et al. (2017) finetune this context2vec representation to comput"
S19-1007,K16-1006,0,0.0541498,"2018) however our system shows comparable or superior performance on the two intrinsic tasks that they use (Table 1 below and Table 1 of their paper). 2.2 Second-order substitute-based context An alternative to a bag-of-words representation is a second-order substitute vector generated by a language model for the target word’s slot. For example, we can represent the context ‘It is a move.’ as a substitute vector [big 0.35, good 0.28, bold 0.05, ...] with the numbers indicating fitness weights of each substitute in the context (Melamud et al., 2015; Yatbaz et al., 2012; Melamud et al., 2015). Melamud et al. (2016) later on introduced context2vec which trains both context and word embeddings in a similar setup to CBOW (Mikolov et al., 2013) except that the context is represented with a Bidirectional LSTM rather than as a bag of words. In this way, context2vec captures sequence information in the context, and is able to produce high-quality substitutes for a sentence-completion task, while overcoming the sparseness issues in the previous substitutebased approaches. Kobayashi et al. (2017) finetune this context2vec representation to compute entity representations in a discourse for the language modelling"
S19-1007,N18-1202,0,0.176787,"Missing"
W00-1325,P87-1027,0,0.307011,"scribed in section 2.2, along with the details of the threshold applied to the relative frequencies o u t p u t from the SCF acquisition system. The details of the experimental evaluation are supplied in section 3. We discuss our findings in section 3.3 and conclude with directions for future work (section 4). 2 Method 2.1 F r a m e w o r k for S C F Acquisition Briscoe and Carroll's (1997) verbal acquisition system distinguishes 163 SCFs and returns relative frequencies for each SCF found for a given predicate. The SCFs are a superset of classes found in the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987) and the COML~X Syntax dictionary, Grishman et al. (1994). They incorporate information about control of predicative arguments, as well as alternations such as extraposition and particle movement. The system employs a shallow parser to obtain the subcategorization information. Potential SCF entries are filtered before the final SCF lexicon is produced. The filter is the only component of this system which we experiment with here. The three filtering methods which we compare are described below. 2.2 2.2.1 Filtering M e t h o d s B i n o m i a l H y p o t h e s i s Test Briscoe and Carroll (1997"
W00-1325,P91-1027,0,0.108727,"has been done to tional lexicons which include valuable freremove the noise that arises when dealing with quency information. However, the accuracy naturally occurring data, and from mistakes of the resulting lexicons shows room for immade by the SCF acquisition system, for exprovement. One significant source of error ample, parser errors. lies in the statistical filtering used by some reFiltering is usually done with a hypothesearchers to remove noise from automatically sis test, and frequently with a variation of acquired subcategorization frames. In this pathe binomial filter introduced by Brent (1991, per, we compare three different approaches to 1993). Hypothesis testing is performed by forfiltering out spurious hypotheses. Two hymulating a null hypothesis, (H0), which is aspothesis tests perform poorly, compared to sumed true unless there is evidence to the confiltering frames on the basis of relative fretrary. If there is evidence to the contrary, quency. We discuss reasons for this and conH0 is rejected and the alternative hypothesider directions for future research. sis (H1) is accepted. In SCF acquisition, H0 is 1 Introduction that there is no association between a particular verb ("
W00-1325,J93-2002,0,0.866559,"Missing"
W00-1325,A97-1052,0,0.938899,"y, Boguraev et al. (1987) and the COML~X Syntax dictionary, Grishman et al. (1994). They incorporate information about control of predicative arguments, as well as alternations such as extraposition and particle movement. The system employs a shallow parser to obtain the subcategorization information. Potential SCF entries are filtered before the final SCF lexicon is produced. The filter is the only component of this system which we experiment with here. The three filtering methods which we compare are described below. 2.2 2.2.1 Filtering M e t h o d s B i n o m i a l H y p o t h e s i s Test Briscoe and Carroll (1997) used a binomial hypothesis test (BHT) to filter the acquired SCFs. They applied BHT as follows. The system recorded the total number of sets of SCF cues (n) found for a given predicate, and the number of these sets for a given SCF (ra). The system estimated the error probability (pe) that a cue for a SCF (scfi) occurred with a verb which did not take scfi. pe was estimated in two stages, as shown in equation 1. Firstly, the number of verbs which are members of the target SCF in the ANLT dictionary were extracted. This number was converted to a probability of class membership by dividing by th"
W00-1325,W98-1505,0,0.139722,"Missing"
W00-1325,J93-1003,0,0.0405344,"ve frequencies produced slightly betrecall 1 ter results than those achieved with a BrentzWhere token recall is the percentage .of SCF tokens in a sample of manually analysed text that were correctly acquired by the system. 199 style binomial filter when establishing SCFs for diathesis alternation detection. Lapata determined thresholds for each SCF using the frequency of the SCF in COMLEX Syntax dictionary (Grishman et al., 1994). Adopting the SCF acquisition system of Briscoe and Carroll, we have experimented with an alternative hypothesis test, the binomial log-likelihood ratio (LLR) test (Dunning, 1993). Sarkar and Zeman (2000) have also used this test when filtering SCFs automatically acquired for Czech. This test has been recommended for use in NLP since it does not assume a normal distribution, which invalidates many other parametric tests for use with natural language phenomena. LLR can be used in a form (-2logA) which is X2 distributed. Moreover, this asymptote is appropriate at quite low frequencies, which makes the hypothesis test particularly useful when dealing with natural language phenomena, where low frequency events are commonplace. A problem with using hypothesis testing for fi"
W00-1325,P98-1071,0,0.264875,"Missing"
W00-1325,C94-1042,0,0.0922367,"eshold applied to the relative frequencies o u t p u t from the SCF acquisition system. The details of the experimental evaluation are supplied in section 3. We discuss our findings in section 3.3 and conclude with directions for future work (section 4). 2 Method 2.1 F r a m e w o r k for S C F Acquisition Briscoe and Carroll's (1997) verbal acquisition system distinguishes 163 SCFs and returns relative frequencies for each SCF found for a given predicate. The SCFs are a superset of classes found in the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987) and the COML~X Syntax dictionary, Grishman et al. (1994). They incorporate information about control of predicative arguments, as well as alternations such as extraposition and particle movement. The system employs a shallow parser to obtain the subcategorization information. Potential SCF entries are filtered before the final SCF lexicon is produced. The filter is the only component of this system which we experiment with here. The three filtering methods which we compare are described below. 2.2 2.2.1 Filtering M e t h o d s B i n o m i a l H y p o t h e s i s Test Briscoe and Carroll (1997) used a binomial hypothesis test (BHT) to filter the acq"
W00-1325,P99-1051,0,0.121006,"p u t e r Laboratory, University of Cambridge Pembroke Street, Cambridge CB2 3QG, U K alk23@cl, cam. ac. uk, genevieve, gorrell@netdecisions, co. uk Diana McCarthy School of Cognitive and Computing Sciences University of Sussex, Brighton, BN1 9QH, UK dianam@cogs, s u s x . a c . uk Abstract The approaches to extracting SCF information from corpora have frequently employed Research ""into the automatic acquisition of statistical methods for filtering (e.g. Brent, subcategorization frames (SCFS) from corpora 1993; Manning 1993; Briscoe and Carroll, is starting to produce large-scale computa1997; Lapata, 1999). This has been done to tional lexicons which include valuable freremove the noise that arises when dealing with quency information. However, the accuracy naturally occurring data, and from mistakes of the resulting lexicons shows room for immade by the SCF acquisition system, for exprovement. One significant source of error ample, parser errors. lies in the statistical filtering used by some reFiltering is usually done with a hypothesearchers to remove noise from automatically sis test, and frequently with a variation of acquired subcategorization frames. In this pathe binomial filter introdu"
W00-1325,C00-2100,0,0.837786,"Missing"
W00-1325,W93-0109,0,0.905976,"Missing"
W00-1325,H91-1067,0,\N,Missing
W00-1325,E95-1016,0,\N,Missing
W00-1325,P93-1032,0,\N,Missing
W00-1325,C98-1068,0,\N,Missing
W00-1327,P87-1027,0,0.337112,"f the British National Corpus (BN¢) (Leech, 1992) data using Briscoe and Carroll's (1997) system (introduced in section 3.2). This gives us the observed distribution of SCFs for individual verbs and that for all verbs in the BNC data. The second method was to manually analyse around 300 occurrences of each test verb in the BNC data. This gives us an estimate of the correct SCF distributions for the individual verbs. The estimate for the correct distribution of SCFs over all English verbs was obtained by extracting the number of verbs which are members of each SCF class in the ANLT dictionary (Boguraev et al., 1987). The degree of correlation was examined by calculating the Kullback-Leibler distance (KL) (Cover and Thomas, 1991) and the Spearman rank correlation coefficient (Re) (Spearman, 1904) between the different distributions 2. The results given in tables 1 and 2 were obtained by correlating the observed SCF distributions from the BNC data. Table 1 shows an example of correlating the SCF distribution of the motion verb .fly against that of (i) its hypernym move, (ii) synonym sail, (iii) all verbs in general, and (iv) agree, which is not related semantically. The results show that the SCF distributi"
W00-1327,P91-1027,0,0.117303,"Missing"
W00-1327,J93-2002,0,0.466592,"Missing"
W00-1327,A97-1052,0,0.794982,"r, we propose a method for obtaining more accurate back-off estimates for SCF acquisition. Taking Levin's verb classification (Levin, 1993) as a starting point, we show that in terms of SCF distributions, individual verbs correlate better with other semantically similar verbs than with all verbs in general. On the basis of this observation, we propose classifying verbs according to their semantic class and using the conditional SCF distributions of a few other members in the 216 same class as back-off estimates of the class (p( sc filsernantic class j)). Adopting the SCF acquisition system of Briscoe and Carroll (1997) we report an experiment which demonstrates how these estimates can be used in filtering. This is done by acquiring the conditional SCF distributions for selected test verbs, smoothing these distributions with the unconditional distribution of the respective verb class, and applying a simple method for filtering the resulting set of SCFs. Our results show that the proposed method improves the acquisition of SCFs significantly. We discuss how this method can be used to benefit large-scale SCF acquisition. We begin by reporting our findings that the SCF distributions of semantically similar verb"
W00-1327,W98-1505,0,0.0997134,"Missing"
W00-1327,P98-1071,0,0.194104,"Missing"
W00-1327,C94-1042,0,0.0777632,", for example, were obtained by merging the SCF distributions of the verbs march, move, fly, slide and sail. Each verb used in obtaining the estimates was excluded when testing the verb itself. For example, when acquiring subcategorization for the verb fly, estimates were obtained only using verbs march, move, slide and sail. 3.2 F r a m e w o r k for S C F A c q u i s i t i o n Briscoe and Carroll's (1997) verbal acquisition system distinguishes 163 SCFs and returns relative frequencies for each SCF found for a given predicate. The SCFs are a superset of classes found in the ANLT and COMLEX (Grishman et al., 1994) dictionaries. They incorporate information about control of predicative arguments, as well as alternations such as extraposition and particle movement. The system employs a shallow parser to obtain the subcategorization information. Potential SCF entries are filtered before the final SCF lexicon is produced. While Briscoe and Carroll (1997) used a statistical filter based on bi219 nomial hypothesis test, we adopted another method, where the conditional SCF distribution from the system is smoothed before filtering the SCFS, using the different techniques introduced in section 3.3. After smooth"
W00-1327,W00-1325,1,0.365832,"Missing"
W00-1327,C00-2100,0,0.393793,"Missing"
W00-1327,W93-0109,0,0.108907,"Missing"
W00-1327,H91-1067,0,\N,Missing
W00-1327,P93-1032,0,\N,Missing
W00-1327,J93-1002,0,\N,Missing
W00-1327,C98-1068,0,\N,Missing
W02-0815,A00-2031,0,\N,Missing
W02-0815,J87-3002,0,\N,Missing
W02-0815,S01-1009,0,\N,Missing
W02-0815,preiss-etal-2002-subcategorization,1,\N,Missing
W02-0815,C00-2100,0,\N,Missing
W02-0815,C94-1042,0,\N,Missing
W02-0815,A97-1052,0,\N,Missing
W02-0815,W00-0905,0,\N,Missing
W02-0907,preiss-etal-2002-subcategorization,1,\N,Missing
W02-0907,C00-2100,0,\N,Missing
W02-0907,C94-1042,0,\N,Missing
W02-0907,W00-1325,1,\N,Missing
W02-0907,H90-1053,0,\N,Missing
W02-0907,W00-1327,1,\N,Missing
W02-0907,A97-1052,0,\N,Missing
W02-0907,J93-2002,0,\N,Missing
W02-0907,P98-1046,0,\N,Missing
W02-0907,C98-1046,0,\N,Missing
W02-0907,P93-1032,0,\N,Missing
W02-0907,P87-1027,0,\N,Missing
W02-1108,C96-1055,0,\N,Missing
W02-1108,C94-1043,0,\N,Missing
W02-1108,E99-1007,0,\N,Missing
W02-1108,W02-0907,1,\N,Missing
W02-1108,A97-1052,0,\N,Missing
W02-1108,P98-1046,0,\N,Missing
W02-1108,C98-1046,0,\N,Missing
W02-1108,P98-1112,0,\N,Missing
W02-1108,C98-1108,0,\N,Missing
W02-2014,A00-2034,0,\N,Missing
W02-2014,W02-0907,1,\N,Missing
W02-2014,A97-1052,0,\N,Missing
W02-2014,P99-1004,0,\N,Missing
W02-2014,J02-3004,0,\N,Missing
W02-2014,P98-2127,0,\N,Missing
W02-2014,C98-2122,0,\N,Missing
W04-2606,P87-1027,1,0.766622,"lated to each other via alternations and thus warrant creation of a new class. properties have not been systematically studied in terms of diathesis alternations, and therefore re-examination is warranted. In what follows, we will describe these steps in detail. 3.2.2 Rudanko’s Classification 3.1 Novel Diathesis Alternations When constructing novel diathesis alternations, we took as a starting point the subcategorization classification of Briscoe (2000). This fairly comprehensive classification incorporates 163 different subcategorization frames (SCFs), a superset of those listed in the ANLT (Boguraev et al., 1987) and COMLEX Syntax dictionaries (Grishman et al., 1994). The SCFs define mappings from surface arguments to predicate-argument structure for bounded dependency constructions, but abstract over specific particles and prepositions, as these can be trivially instantiated when the a frame is associated with a specific verb. As most diathesis alternations are only semi-predictable on a verb-by-verb basis, a distinct SCF is defined for every such construction, and thus all alternations can be represented as mappings between such SCFs. We considered possible alternations between pairs of SCF s in thi"
W04-2606,W02-1016,0,0.0866251,"Missing"
W04-2606,A97-1052,1,0.81912,"lieve accuse, condemn retreat, retire handle, deal hear, learn pair, mix coincide, alternate imagine, remember notice, feel like, hate focus, concentrate mind, worry debate, argue fight, communicate agree, contract demonstrate, quote allow, permit write, read comment, remark propose, recommend happen, occur count, weight miss, boycott loiter, hesitate continue, resume terminate, finish overlook, neglect commit, charge arrive, hit assume, adopt Table 2: New Verb Classes 1000 citations, on average, for each verb. Our method for SCF acquisition (Korhonen, 2002) involves first using the system of Briscoe and Carroll (1997) to acquire a putative SCF distribution for each test verb from corpus data. This system employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses from the PoS tagged data. The parse contexts around verbs are passed to a comprehensive SCF classifier, which selects one of the 163 SCFs. The SCF distribution is then smoothed with the back-off distribution corresponding to the semantic class of the predominant sense of a verb. Although many of the test verbs are polysemic, we relied on the knowledge that the majority of English verbs have a single"
W04-2606,briscoe-carroll-2002-robust,1,0.384232,"e mind, worry debate, argue fight, communicate agree, contract demonstrate, quote allow, permit write, read comment, remark propose, recommend happen, occur count, weight miss, boycott loiter, hesitate continue, resume terminate, finish overlook, neglect commit, charge arrive, hit assume, adopt Table 2: New Verb Classes 1000 citations, on average, for each verb. Our method for SCF acquisition (Korhonen, 2002) involves first using the system of Briscoe and Carroll (1997) to acquire a putative SCF distribution for each test verb from corpus data. This system employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses from the PoS tagged data. The parse contexts around verbs are passed to a comprehensive SCF classifier, which selects one of the 163 SCFs. The SCF distribution is then smoothed with the back-off distribution corresponding to the semantic class of the predominant sense of a verb. Although many of the test verbs are polysemic, we relied on the knowledge that the majority of English verbs have a single predominating sense in balanced corpus data (Korhonen and Preiss, 2003). The back-off estimates were obtained by the following method: (i) A few individ"
W04-2606,P98-1046,0,0.668211,"introduce 106 novel diathesis alternations, created as a side product of constructing the new classes. We demonstrate the utility of our novel classes by using them to support automatic subcategorization acquisition and show that the resulting extended classification has extensive coverage over the English verb lexicon. 1 Introduction Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)). Such classes can capture generalizations over a range of (cross-)linguistic properties, and can therefore be used as a valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. Verb classes have proved useful in various (multilingual) natural language processing (NLP) tasks and applications, such as computational lexicography (Kipper et al., 2000), language generation (Stede, 1998), machine translation (Dorr, 1997), word sense disambiguation (Prescher et al., 2000), document classification (Klavans and Kan, 1998), and subcate"
W04-2606,W02-2014,1,0.8406,"Missing"
W04-2606,P03-1007,1,0.818762,"ach test verb from corpus data. This system employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses from the PoS tagged data. The parse contexts around verbs are passed to a comprehensive SCF classifier, which selects one of the 163 SCFs. The SCF distribution is then smoothed with the back-off distribution corresponding to the semantic class of the predominant sense of a verb. Although many of the test verbs are polysemic, we relied on the knowledge that the majority of English verbs have a single predominating sense in balanced corpus data (Korhonen and Preiss, 2003). The back-off estimates were obtained by the following method: (i) A few individual verbs were chosen from a new verb class whose predominant sense according to the WordNet frequency data belongs to this class, (ii) SCF distributions were built for these verbs by manually analysing c. 300 occurrences of each verb in the BNC, (iii) the resulting SCF distributions were merged. An empirically-determined threshold was finally set on the probability estimates from smoothing to reject noisy SCF s caused by errors during the statistical parsing phase. This method for SCF acquisition is highly sensit"
W04-2606,P03-1009,1,0.833242,"most widely deployed classification in English, Levin’s (1993) taxonomy, mainly deals with verbs taking noun and prepositional phrase complements, and does not provide large numbers of exemplars of the classes. The fact that no comprehensive classification is available limits the usefulness of the classes for practical NLP. Some experiments have been reported recently which indicate that it should be possible, in the future, to automatically supplement extant classifications with novel verb classes and member verbs from corpus data (Brew and Schulte im Walde, 2002; Merlo and Stevenson, 2001; Korhonen et al., 2003). While the automatic approach will avoid the expensive overhead of manual classification, the very development of the technology capable of large-scale automatic classification will require access to a target classification and gold standard exemplification of it more extensive than that available currently. In this paper, we address these problems by introducing a substantial extension to Levin’s classification which incorporates 57 novel classes for verbs not covered (comprehensively) by Levin. These classes, many of them drawn initially from linguistic resources, were created semi-automati"
W04-2606,J01-3003,0,0.488659,"l diathesis alternations, created as a side product of constructing the new classes. We demonstrate the utility of our novel classes by using them to support automatic subcategorization acquisition and show that the resulting extended classification has extensive coverage over the English verb lexicon. 1 Introduction Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)). Such classes can capture generalizations over a range of (cross-)linguistic properties, and can therefore be used as a valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. Verb classes have proved useful in various (multilingual) natural language processing (NLP) tasks and applications, such as computational lexicography (Kipper et al., 2000), language generation (Stede, 1998), machine translation (Dorr, 1997), word sense disambiguation (Prescher et al., 2000), document classification (Klavans and Kan, 1998), and subcategorization acquisition (Korh"
W04-2606,1997.mtsummit-workshop.4,0,0.0293955,"me extensions have recently been proposed to this resource. Dang et al. (1998) have supplemented the taxonomy with intersective classes: special classes for verbs which share membership of more than one Levin class because of regular polysemy. Bonnie Dorr (University of Maryland) has provided a reformulated and extended version of Levin’s classification in her LCS database (http://www.umiacs.umd.edu/  bonnie/verbsEnglish.lcs). This resource groups 4,432 verbs (11,000 senses) into 466 Levin-based and 26 novel classes. The latter are Levin classes refined according to verbal telicity patterns (Olsen et al., 1997), while the former are additional classes for non-Levin verbs which do not fall into any of the Levin classes due to their distinctive syntactic behaviour (Dorr, 1997). As a result of this work, the taxonomy has gained considerably in depth, but not to the same extent in breadth. Verbs taking ADJP, ADVP, ADL, particle, predicative, control and sentential complements are still largely excluded, except where they show interesting behaviour with respect to NP and PP complementation. As many of these verbs are highly frequent in language, NLP applications utilizing lexical-semantic classes would b"
W04-2606,C00-2094,0,0.0306267,"cs (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)). Such classes can capture generalizations over a range of (cross-)linguistic properties, and can therefore be used as a valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. Verb classes have proved useful in various (multilingual) natural language processing (NLP) tasks and applications, such as computational lexicography (Kipper et al., 2000), language generation (Stede, 1998), machine translation (Dorr, 1997), word sense disambiguation (Prescher et al., 2000), document classification (Klavans and Kan, 1998), and subcategorization acquisition (Korhonen, 2002). Fundamentally, such classes define the mapping from surface realization of arguments to predicate-argument structure and are therefore a critical component of any NLP system which needs to recover predicate-argument structure. In many operational contexts, lexical information must be acquired from small application- and/or domain-specific corpora. The predictive power of classes can help compensate for lack of sufficient data fully exemplifying the behaviour of relevant words, through use of"
W04-2606,C94-1042,0,0.0295931,"reation of a new class. properties have not been systematically studied in terms of diathesis alternations, and therefore re-examination is warranted. In what follows, we will describe these steps in detail. 3.2.2 Rudanko’s Classification 3.1 Novel Diathesis Alternations When constructing novel diathesis alternations, we took as a starting point the subcategorization classification of Briscoe (2000). This fairly comprehensive classification incorporates 163 different subcategorization frames (SCFs), a superset of those listed in the ANLT (Boguraev et al., 1987) and COMLEX Syntax dictionaries (Grishman et al., 1994). The SCFs define mappings from surface arguments to predicate-argument structure for bounded dependency constructions, but abstract over specific particles and prepositions, as these can be trivially instantiated when the a frame is associated with a specific verb. As most diathesis alternations are only semi-predictable on a verb-by-verb basis, a distinct SCF is defined for every such construction, and thus all alternations can be represented as mappings between such SCFs. We considered possible alternations between pairs of SCF s in this classification, focusing in particular on those SCF s"
W04-2606,J98-3003,0,0.0250277,"acted considerable interest in both linguistics and computational linguistics (e.g. (Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)). Such classes can capture generalizations over a range of (cross-)linguistic properties, and can therefore be used as a valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. Verb classes have proved useful in various (multilingual) natural language processing (NLP) tasks and applications, such as computational lexicography (Kipper et al., 2000), language generation (Stede, 1998), machine translation (Dorr, 1997), word sense disambiguation (Prescher et al., 2000), document classification (Klavans and Kan, 1998), and subcategorization acquisition (Korhonen, 2002). Fundamentally, such classes define the mapping from surface realization of arguments to predicate-argument structure and are therefore a critical component of any NLP system which needs to recover predicate-argument structure. In many operational contexts, lexical information must be acquired from small application- and/or domain-specific corpora. The predictive power of classes can help compensate for lack o"
W04-2606,P98-1112,0,0.0336065,"1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)). Such classes can capture generalizations over a range of (cross-)linguistic properties, and can therefore be used as a valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. Verb classes have proved useful in various (multilingual) natural language processing (NLP) tasks and applications, such as computational lexicography (Kipper et al., 2000), language generation (Stede, 1998), machine translation (Dorr, 1997), word sense disambiguation (Prescher et al., 2000), document classification (Klavans and Kan, 1998), and subcategorization acquisition (Korhonen, 2002). Fundamentally, such classes define the mapping from surface realization of arguments to predicate-argument structure and are therefore a critical component of any NLP system which needs to recover predicate-argument structure. In many operational contexts, lexical information must be acquired from small application- and/or domain-specific corpora. The predictive power of classes can help compensate for lack of sufficient data fully exemplifying the behaviour of relevant words, through use of back-off smoothing or similar techniques. Althoug"
W04-2606,J81-4005,0,0.55639,"yntactic behaviour. These classes were originally created by an automatic verb classification algorithm described in (Dorr, 1997). Although they appear semantically meaningful, their syntactic-semantic Rudanko (1996, 2000) provides a semantically motivated classification for verbs taking various types of sentential complements (including predicative and control constructions). His relatively fine-grained classes, organized into sets of independent taxonomies, have been created in a manner similar to Levin’s. We took 43 of Rundanko’s verb classes for consideration. 3.2.3 Sager’s Classification Sager (1981) presents a small classification consisting of 13 classes, which groups verbs (mostly) on the basis of their syntactic alternations. While semantic properties are largely ignored, many of the classes appear distinctive also in terms of semantics. 3.2.4 Levin’s Classification At least 20 (broad) Levin classes involve verb senses which take sentential complements. Because full treatment of these senses requires considering sentential complementation, we re-evaluated these classes using our method. 3.3 Method for Creating Classes Each candidate class was evaluated as follows: 1. We extracted from"
W04-2606,C98-1046,0,\N,Missing
W04-2606,C98-1108,0,\N,Missing
W07-0605,P87-1027,0,0.105814,"for subcategorization acquisition the new sys- ents/caretakers. The mean utterance length (measured tem of Preiss, Briscoe and Korhonen (2007) which in words) in CHILD and CDS were 3.48 and 4.61, reis essentially a much improved and extended version spectively. The mean age of the child speaker in CHILD 4 of Briscoe and Carroll’s (1997) system. It incorpo- is around 3 years 6 months. rates 168 SCF distinctions, a superset of those found 2 See Section 4 for details of F-measure. in the COMLEX Syntax (Grishman et al., 1994) and 3 CHAT is the transcription and coding format used by all the ANLT (Boguraev et al., 1987) dictionaries. Currently, transcriptions within CHILDES. 4 SCFs abstract over specific lexically governed partiThe complete age range is from 1 year and 1 month up to 7 cles and prepositions and specific predicate selectional years. 34 3.1 We selected a set of 161 verbs for experimentation. The words were selected at random, subject to the constraint that a sufficient number of SCFs would be extracted (&gt; 100) from both corpora to facilitate maximally useful comparisons. All sentences containing an occurrence of one of the test verbs were extracted from the two corpora and fed into the SCF acqu"
W07-0605,A97-1052,0,0.113975,"Missing"
W07-0605,P06-4020,0,0.472271,"n the CHILDES database without ciently accurate to yield annotations useful for linguisany domain-specific tuning. We demonstrate tic purposes. They also gather important qualitative that the acquired information is sufficiently acand quantitative information, which is difficult for hucurate to confirm and extend previously remans to obtain, as a side-effect of the acquisition proported research findings. We also report qualicess. tative results which can be used to further imFor instance, state-of-the-art statistical parsers, prove parsing and lexical acquisition technole.g. (Charniak, 2000; Briscoe et al., 2006), have wide ogy for child language data in the future. coverage and yield grammatical representations capable of supporting various applications (e.g. summa1 Introduction rization, information extraction). In addition, lexiLarge empirical data containing children’s speech are cal information (e.g. subcategorization, lexical classes) the key to developing and evaluating different theo- can now be acquired automatically from parsed ries of child language acquisition (CLA). Particularly data (McCarthy and Carroll, 2003; Schulte im Walde, important are data related to syntactic complexity of 2006;"
W07-0605,A00-2018,0,0.0675651,"uffispeech within the CHILDES database without ciently accurate to yield annotations useful for linguisany domain-specific tuning. We demonstrate tic purposes. They also gather important qualitative that the acquired information is sufficiently acand quantitative information, which is difficult for hucurate to confirm and extend previously remans to obtain, as a side-effect of the acquisition proported research findings. We also report qualicess. tative results which can be used to further imFor instance, state-of-the-art statistical parsers, prove parsing and lexical acquisition technole.g. (Charniak, 2000; Briscoe et al., 2006), have wide ogy for child language data in the future. coverage and yield grammatical representations capable of supporting various applications (e.g. summa1 Introduction rization, information extraction). In addition, lexiLarge empirical data containing children’s speech are cal information (e.g. subcategorization, lexical classes) the key to developing and evaluating different theo- can now be acquired automatically from parsed ries of child language acquisition (CLA). Particularly data (McCarthy and Carroll, 2003; Schulte im Walde, important are data related to syntac"
W07-0605,C94-1042,0,0.0757159,"ubcategorization Acquisition System CDS contained input from speakers identified as parWe used for subcategorization acquisition the new sys- ents/caretakers. The mean utterance length (measured tem of Preiss, Briscoe and Korhonen (2007) which in words) in CHILD and CDS were 3.48 and 4.61, reis essentially a much improved and extended version spectively. The mean age of the child speaker in CHILD 4 of Briscoe and Carroll’s (1997) system. It incorpo- is around 3 years 6 months. rates 168 SCF distinctions, a superset of those found 2 See Section 4 for details of F-measure. in the COMLEX Syntax (Grishman et al., 1994) and 3 CHAT is the transcription and coding format used by all the ANLT (Boguraev et al., 1987) dictionaries. Currently, transcriptions within CHILDES. 4 SCFs abstract over specific lexically governed partiThe complete age range is from 1 year and 1 month up to 7 cles and prepositions and specific predicate selectional years. 34 3.1 We selected a set of 161 verbs for experimentation. The words were selected at random, subject to the constraint that a sufficient number of SCFs would be extracted (&gt; 100) from both corpora to facilitate maximally useful comparisons. All sentences containing an oc"
W07-0605,W02-2014,1,0.852037,"verbs. We employed the most accurate version of the lexicon here (87.3 Fmeasure)—this lexicon was obtained by selecting high frequency SCFs and supplementing them with lower frequency SCFs from manually built lexicons. 4 Analysis 4.1 Verb go want get know put see come like make say take eat play need look fall sit think break give Test Verbs and SCF Lexicons Methods for Analysis The similarity between verb and SCF distributions in the lexicons was examined. To maintain a robust analysis in the presence of noise, multiple similarity measures were used to compare the verb and SCF distributions (Korhonen and Krymolowski, 2002). In the following p = (pi ) and q = (qi ) where pi and qi are the probabilities associated with SCFi in distributions (lexicons) P and Q: • Intersection (IS) - the intersection of non-zero probability SCF s in p and q; • Spearman rank correlation (RC) - lies in the range [1; 1], with values near 0 denoting a low degree of association and values near -1 and 1 denoting strong association; • Kullback-Leibler (KL) distance - a measure of the additional information needed to describe p using q, KL is always ≥ 0 and = 0 only when p ≡ q; The SCFs distributions acquired from the corpora for the chose"
W07-0605,korhonen-etal-2006-large,1,0.867171,"e is from 1 year and 1 month up to 7 cles and prepositions and specific predicate selectional years. 34 3.1 We selected a set of 161 verbs for experimentation. The words were selected at random, subject to the constraint that a sufficient number of SCFs would be extracted (&gt; 100) from both corpora to facilitate maximally useful comparisons. All sentences containing an occurrence of one of the test verbs were extracted from the two corpora and fed into the SCF acquisition system described earlier in section 2. In some of our experiments the two lexicons were compared against the VALEX lexicon (Korhonen et al., 2006)—a large subcategorization lexicon for English which was acquired automatically from several crossdomain corpora (containing both written and spoken language). VALEX includes SCF and frequency information for 6,397 English verbs. We employed the most accurate version of the lexicon here (87.3 Fmeasure)—this lexicon was obtained by selecting high frequency SCFs and supplementing them with lower frequency SCFs from manually built lexicons. 4 Analysis 4.1 Verb go want get know put see come like make say take eat play need look fall sit think break give Test Verbs and SCF Lexicons Methods for Anal"
W07-0605,J03-4004,0,0.0195557,"istical parsers, prove parsing and lexical acquisition technole.g. (Charniak, 2000; Briscoe et al., 2006), have wide ogy for child language data in the future. coverage and yield grammatical representations capable of supporting various applications (e.g. summa1 Introduction rization, information extraction). In addition, lexiLarge empirical data containing children’s speech are cal information (e.g. subcategorization, lexical classes) the key to developing and evaluating different theo- can now be acquired automatically from parsed ries of child language acquisition (CLA). Particularly data (McCarthy and Carroll, 2003; Schulte im Walde, important are data related to syntactic complexity of 2006; Preiss et al., 2007). This information complechild language since considerable evidence suggests ments the basic grammatical analysis and provides acthat syntactic information plays a central role during cess to the underlying predicate-argument structure. language acquisition, e.g. (Lenneberg, 1967; Naigles, Containing considerable ellipsis and error, spoken 1990; Fisher et al., 1994). child language can be challenging for current NLP The standard corpus in the study of CLA is the techniques which are typically op"
W07-0605,P07-1115,1,0.804417,"ged and lemmatized formats. However, adequate plexity of children’s speech is important for investigation of syntactic complexity requires deeper theories of language acquisition. Currently annotations related to e.g. syntactic parses, subcategomuch of this data is absent in the annotated rization frames (SCFs), lexical classes and predicateversions of the CHILDES database. In this argument structures. perliminary study, we show that a state-ofAlthough manual syntactic annotation is possible, the-art subcategorization acquisition system of it is extremely costly. The alternative is to use natuPreiss et al. (2007) can be used to extract largeral language processing (NLP) techniques for annotascale subcategorization (frequency) information. Automatic techniques are now viable, cost effection from the (i) child and (ii) child-directed tive and, although not completely error-free, are suffispeech within the CHILDES database without ciently accurate to yield annotations useful for linguisany domain-specific tuning. We demonstrate tic purposes. They also gather important qualitative that the acquired information is sufficiently acand quantitative information, which is difficult for hucurate to confirm and e"
W07-0605,P05-1025,0,0.0190347,"et al., 2007). This information complechild language since considerable evidence suggests ments the basic grammatical analysis and provides acthat syntactic information plays a central role during cess to the underlying predicate-argument structure. language acquisition, e.g. (Lenneberg, 1967; Naigles, Containing considerable ellipsis and error, spoken 1990; Fisher et al., 1994). child language can be challenging for current NLP The standard corpus in the study of CLA is the techniques which are typically optimized for written CHILDES database (MacWhinney, 2000)1 which proadult language. Yet Sagae et al. (2005) have recently vides 300MB of transcript data of interactions bedemonstrated that existing statistical parsing tech1 See http://childes.psy.cmu.edu for details. niques can be usefully modified to analyse CHILDES 33 Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 33–40, c Prague, Czech Republic, June 2007 2007 Association for Computational Linguistics with promising accuracy. Although further improvements are still required for optimal accuracy, this research has opened up the exciting possibility of automatic grammatical annotation of the entire CH"
W07-0605,J06-2001,0,0.0523206,"Missing"
W09-0210,W02-1016,0,0.352058,"Missing"
W09-0210,briscoe-carroll-2002-robust,0,0.143976,"Missing"
W09-0210,D07-1043,0,0.692738,"umber of target clusters in advance, renders them promising for the many NLP tasks where clustering is used for learning purposes. While the results of Vlachos et al. (2008) are promising, the use of a clustering approach which discovers the number of clusters in data presents a new challenge to existing evaluation measures. In this work, we investigate optimal evaluation for such approaches, using the dataset and the basic method of Vlachos et al. as a starting point. We review the applicability of existing evaluation measures and propose a modified version of the newly introduced V-measure (Rosenberg and Hirschberg, 2007). We complement the quantitative evaluation with thorough qualitative assessment, for which we introduce a method to summarize samples obtained from a clustering algorithm. In preliminary work by Vlachos et al. (2008), a constrained version of DPMMs which takes advantage of must-link and cannot-link pairwise constraints was introduced. It was demonstrated how such constraines can guide the clustering solution towards some prior intuition or considerations relevant to the specific NLP application in mind. We explain the inference algorithm for the constrained DPMM in greater detail and evaluate"
W09-0210,P07-1107,0,0.031621,"Department of Engineering University of Cambridge Cambridge CB2 1PZ, UK zoubin@eng.cam.ac.uk Introduction Bayesian non-parametric models have received a lot of attention in the machine learning community. These models have the attractive property that the number of components used to model the data is not fixed in advance but is actually determined by the model and the data. This property is particularly interesting for NLP where many tasks are aimed at discovering novel, previously unknown information in corpora. Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. Recently, Vlachos et al. (2008) applied the basic models of this class, Dirichlet Process Mixture Models (DPMMs) (Neal, 2000), to a typical learning task in NLP: lexical-semantic verb clustering. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other NLP tasks, such as word sense disambiguation, parsing and semantic role labeling (Dang, 2004; Swier and St"
W09-0210,P06-1124,0,0.0600125,"ac.uk Introduction Bayesian non-parametric models have received a lot of attention in the machine learning community. These models have the attractive property that the number of components used to model the data is not fixed in advance but is actually determined by the model and the data. This property is particularly interesting for NLP where many tasks are aimed at discovering novel, previously unknown information in corpora. Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. Recently, Vlachos et al. (2008) applied the basic models of this class, Dirichlet Process Mixture Models (DPMMs) (Neal, 2000), to a typical learning task in NLP: lexical-semantic verb clustering. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other NLP tasks, such as word sense disambiguation, parsing and semantic role labeling (Dang, 2004; Swier and Stevenson, 2004). Proceedings of the EACL 2009 Workshop on GEMS: GEometical"
W09-0210,korhonen-etal-2006-large,1,0.879281,"ot discovered like by the DPMM. Secondly, PC uses the similarities between the instances to perform the clustering, while the DPMM attempts to find the parameters of the process that generated the data, which is a different and typically a harder task. In addition, the DPMM has two clear advantages which we illustrate in the following sections: it can be used to discover novel information and it can be modified to incorporate intuitive human supervision. Table 1: Clustering performances. syntactic context in which the verb occurs. SCFs were extracted from the publicly available VALEX lexicon (Korhonen et al., 2006a). VALEX was acquired automatically using a domain-independent statistical parsing toolkit, RASP (Briscoe and Carroll, 2002), and a classifier which identifies verbal SCFs. As a consequence, it includes some noise due to standard text processing and parsing errors and due to the subtlety of argument-adjunct distinction. In our experiments, we used the SCFs obtained from VALEX1, parameterized for the prepositional frame, which had the best performance in the experiments of Sun et al. (2008). The feature sets based on verbal SCFs are very sparse and the counts vary over a large range of values."
W09-0210,P06-1044,1,0.889629,"ot discovered like by the DPMM. Secondly, PC uses the similarities between the instances to perform the clustering, while the DPMM attempts to find the parameters of the process that generated the data, which is a different and typically a harder task. In addition, the DPMM has two clear advantages which we illustrate in the following sections: it can be used to discover novel information and it can be modified to incorporate intuitive human supervision. Table 1: Clustering performances. syntactic context in which the verb occurs. SCFs were extracted from the publicly available VALEX lexicon (Korhonen et al., 2006a). VALEX was acquired automatically using a domain-independent statistical parsing toolkit, RASP (Briscoe and Carroll, 2002), and a classifier which identifies verbal SCFs. As a consequence, it includes some noise due to standard text processing and parsing errors and due to the subtlety of argument-adjunct distinction. In our experiments, we used the SCFs obtained from VALEX1, parameterized for the prepositional frame, which had the best performance in the experiments of Sun et al. (2008). The feature sets based on verbal SCFs are very sparse and the counts vary over a large range of values."
W09-1314,W07-1008,0,0.0499996,"Missing"
W10-0608,P06-4020,0,0.0188779,"n Africa. 3 This was done in order to avoid articles on very speciﬁc topics which are unlikely to contain basic information about the target concept. 63 relation-feature triples consists of two main stages. In the ﬁrst stage, we extract large sets of candidate concept-relation-feature triples for each target concept from parsed corpus data. In the second stage, we re-rank and ﬁlter these triples with the intention of retaining only those triples which are likely to be true semantic features. In the ﬁrst stage, the corpora are parsed using the Robust Accurate Statistical Parsing (RASP) system (Briscoe et al., 2006). For each sentence in the corpora, this yields the most probable analysis returned by the parser in the form of a set of grammatical relations (GRs). The GR sets for each sentence containing the target concept noun are then retrieved from the corpus. These GRs form an undirected acyclic graph, whose nodes are labelled with words in the sentence and their POS, and whose edges are labelled with the GR types linking the nodes together. Using this graph we generate all possible paths which are rooted at our target concept node using a breadth-ﬁrst search. We then examine whether any of these path"
W10-0608,P08-1027,0,0.0133249,"feature. 61 Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 61–69, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics of norms lists features for over 500 concepts, the relatively small size of property norm sets still gives cause for concern. Larger sets of norms would be useful to psycholinguists; however, large-scale property norming studies are time-consuming and costly. In NLP, researchers have developed methods for extracting and classifying generic relationships from data, e.g. Pantel and Pennacchiotti (2008), Davidov and Rappoport (2008a, 2008b). In recent years, researchers have also begun to develop methods which can automatically extract feature norm-like representations from corpora, e.g. Almuhareb and Poesio (2005), Barbu (2008), Baroni et al. (2009). The automatic approach is capable of gathering large-scale distributional data, and furthermore it is cost-effective. Corpora contain natural-language instances of words denoting concepts and their features, and therefore serve as ideal material for feature generation tasks. However, current methods are restricted to speciﬁc relations between concepts and their features, o"
W10-0608,P08-1079,0,0.019995,"feature. 61 Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 61–69, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics of norms lists features for over 500 concepts, the relatively small size of property norm sets still gives cause for concern. Larger sets of norms would be useful to psycholinguists; however, large-scale property norming studies are time-consuming and costly. In NLP, researchers have developed methods for extracting and classifying generic relationships from data, e.g. Pantel and Pennacchiotti (2008), Davidov and Rappoport (2008a, 2008b). In recent years, researchers have also begun to develop methods which can automatically extract feature norm-like representations from corpora, e.g. Almuhareb and Poesio (2005), Barbu (2008), Baroni et al. (2009). The automatic approach is capable of gathering large-scale distributional data, and furthermore it is cost-effective. Corpora contain natural-language instances of words denoting concepts and their features, and therefore serve as ideal material for feature generation tasks. However, current methods are restricted to speciﬁc relations between concepts and their features, o"
W10-0608,W10-0609,1,0.875694,"Missing"
W10-0608,C94-1103,0,0.218437,"Missing"
W10-0608,W09-0210,1,0.726334,"ts us to extract relation verbs. This underscores the usefulness of parsing for semantically meaningful feature extraction. This is consistent with recent work in the ﬁeld of computational lexical semantics, although GR data has not previously been successfully applied to feature extraction. We showed that semantic information about cooccurring concept and feature clusters can be used to enhance feature acquisition. We employed the McRae norms for our analysis, however we could also employ other knowledge resources and cluster relation verbs using recent methods, e.g. Sun and Korhonen (2009), Vlachos et al. (2009). Our paper has also investigated methods of evaluation, which is a critical but difﬁcult issue for feature extraction. Most recent approaches have been evaluated against the ESSLLI sub-set of the McRae norms which expands the set of features in the norms with their synonyms. Yet even expansion sets like the ESSLLI norms do not facilitate adequate evaluation because they are not complete in the sense that there are true features which are not included in the norms. Our qualitative analysis shows that many of the errors against the recoded norms are in fact correct or plausible features. Future"
W10-0608,D09-1067,1,\N,Missing
W10-0609,P06-4020,0,0.0179611,"action method, which aims to extract property-norm-like, psychologically meaningful features from corpus data (Kelly et al., 2010). The method aims to extract semantically unconstrained feature triples of the form concept-relation-feature , where feature is a feature (either noun or adjective) of the target concept and relation is a verb representing the semantic relationship between them. Examples of extracted triples include: swan be white, swan have neck and screwdriver be tool. The model uses a corpus parsed for grammatical relations (GRs) using Robust Accurate Statistical Parsing (RASP) (Briscoe et al., 2006). For each sentence containing a target concept, the set of GRs for that sentence are examined to test whether they match manually-created rules. These rules include prototypical feature-relation GR structures connecting elements of the sentence and represent dependency patterns which encode potential semantic relationships between the concept and candidate feature terms occurring in the sentence. A large set of candidate triples are extracted by applying these rules to each sentence in the corpus containing a target concept, and the triples for each concept are ranked by their frequency of ex"
W10-0609,W10-0608,1,0.898121,"sional representation of each target term. Sim3 http://www.cs.cmu.edu/˜tom/science2008/ semanticFeatureVectors.html 4 In Baroni et al.’s implementation a context window of 5 (Baroni and Lenci, 2008) or 20 (Baroni et al., 2009) words either side of the target word was used instead; we chose a sentence-based context window as it is analogous to the context used in our experimental method (described in the following section). 72 Novel extraction method Finally we implemented a novel extraction method, which aims to extract property-norm-like, psychologically meaningful features from corpus data (Kelly et al., 2010). The method aims to extract semantically unconstrained feature triples of the form concept-relation-feature , where feature is a feature (either noun or adjective) of the target concept and relation is a verb representing the semantic relationship between them. Examples of extracted triples include: swan be white, swan have neck and screwdriver be tool. The model uses a corpus parsed for grammatical relations (GRs) using Robust Accurate Statistical Parsing (RASP) (Briscoe et al., 2006). For each sentence containing a target concept, the set of GRs for that sentence are examined to test whethe"
W10-0609,C94-1103,0,0.012689,"nce with 25 sensory-motor verbs (eat, manipulate, push, etc) in a very large corpus. Our reimplementation of this method used the co-occurrence statistics provided by Mitchell et al.3 which were extracted from the Google n-gram corpus consisting of 1 trillion words of web text. ilarity between pairs of target words was calculated as the cosine between their vectors, and for each of the 60 concept words in the experimental stimuli we chose the 200 most similar target words to act as the feature terms extracted by the model. The corpus used with this model was the British National Corpus (BNC) (Leech et al., 1994). 2.2 2.3 SVD model Secondly, we implemented a co-occurrence-based Singular Value Decomposition (SVD) model based on the one described by Baroni and colleagues (Baroni and Lenci, 2008; Baroni et al., 2009). This model combines aspects of both the HAL (Landauer et al., 1998) and LSA (Lund and Burgess, 1996) models in constructing representations for words based on their co-occurrences in texts. A wordby-word co-occurrence matrix was constructed for our corpus, storing how often each target word cooccurred with each context word. The set of context words consisted of the 5,000 most frequent cont"
W10-0609,D09-1065,0,0.0219905,"results suggest that general feature-based representations of concepts, which place no a priori distinction on sensory-motor properties, may be equally capable of predicting activation to conceptual stimuli. This highlights the potential for the Mitchell et al. method to be used to inform both distributed and sensory-motor accounts of conceptual representation (e.g. McRae et al. (1997), Cree et al. (2006), Tyler et al. (2000), Tyler & Moss (2001), Moss et al. (2007), Martin & Chao (2001)), as well as providing a benchmark with which to assess semantic 77 model development. In a similar vein, Murphy et al. (2009) used a dependency-parsed corpus yielding verb co-occurrence statistics to predict EEG9 activation patterns with significant accuracy. The training and evaluation framework presented by Mitchell et al. (2008) represents just one point in a large space of possibilities for using computational modelling to predict human brain activity associated with conceptual stimuli. In these initial experiments, we have chosen to follow the Mitchell et al. approach as closely as possible, in order to maximize comparability with their results. In future work, we aim to investigate other methods for training a"
W10-1913,P07-2009,0,0.0101004,"l verb, we used all the verbs instead. Verb Class. Because individual verbs can result in sparse data problems, we also experimented with a novel feature: verb class (e.g. the class of EXPERI MENT verbs for verbs such as measure and inject). We obtained 60 classes by clustering verbs appearing in full cancer risk assessment articles using the approach of Sun and Korhonen (2009). POS. Tense tends to vary from one category to another, e.g. past is common in RES and past partici103 7 Experimental evaluation ple in CON. We used the part-of-speech (POS) tag of each verb assigned by the C&C tagger (Curran et al., 2007) as a feature. GR. Structural information about heads and dependents has proved useful in text classification. We used grammatical relations (GRs) returned by the C&C parser as features. They consist of a named relation, a head and a dependent, and possibly extra parameters depending on the relation involved, e.g. (dobj investigate mouse). We created features for each subject (ncsubj), direct object (dobj), indirect object (iobj) and second object (obj2) relation in the corpus. Subj and Obj. As some GR features may suffer from data sparsity, we collected all the subjects and objects (appearing"
W10-1913,I08-1050,0,0.649211,"ambridge, UK yg244@cam.ac.uk alk23@cam.ac.uk Maria Liakata Aberystwyth University, UK mal@aber.ac.uk Ilona Silins Lin Sun Ulla Stenius Karolinska Institutet, SWEDEN University of Cambridge, UK Karolinska Institutet, SWEDEN Ilona.Silins@ki.se ls418@cam.ac.uk Ulla.Stenius@ki.se Abstract Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are richer in information"
W10-1913,D09-1067,1,0.195817,"of sentences, and can vary from one category to another. For example, experiment is frequent in METH and conclude in CON. Previous works have used the matrix verb of each sentence as a feature. Because the matrix verb is not the only meaningful verb, we used all the verbs instead. Verb Class. Because individual verbs can result in sparse data problems, we also experimented with a novel feature: verb class (e.g. the class of EXPERI MENT verbs for verbs such as measure and inject). We obtained 60 classes by clustering verbs appearing in full cancer risk assessment articles using the approach of Sun and Korhonen (2009). POS. Tense tends to vary from one category to another, e.g. past is common in RES and past partici103 7 Experimental evaluation ple in CON. We used the part-of-speech (POS) tag of each verb assigned by the C&C tagger (Curran et al., 2007) as a feature. GR. Structural information about heads and dependents has proved useful in text classification. We used grammatical relations (GRs) returned by the C&C parser as features. They consist of a named relation, a head and a dependent, and possibly extra parameters depending on the relation involved, e.g. (dobj investigate mouse). We created feature"
W10-1913,J02-4002,0,0.815757,"n Guo Anna Korhonen University of Cambridge, UK University of Cambridge, UK yg244@cam.ac.uk alk23@cam.ac.uk Maria Liakata Aberystwyth University, UK mal@aber.ac.uk Ilona Silins Lin Sun Ulla Stenius Karolinska Institutet, SWEDEN University of Cambridge, UK Karolinska Institutet, SWEDEN Ilona.Silins@ki.se ls418@cam.ac.uk Ulla.Stenius@ki.se Abstract Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for ful"
W10-1913,W09-1325,1,0.898833,"Missing"
W10-1913,D09-1155,0,0.337523,"ac.uk alk23@cam.ac.uk Maria Liakata Aberystwyth University, UK mal@aber.ac.uk Ilona Silins Lin Sun Ulla Stenius Karolinska Institutet, SWEDEN University of Cambridge, UK Karolinska Institutet, SWEDEN Ilona.Silins@ki.se ls418@cam.ac.uk Ulla.Stenius@ki.se Abstract Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are richer in information and also considered t"
W10-1913,liakata-etal-2010-corpora,1,0.825214,"wyth University, UK mal@aber.ac.uk Ilona Silins Lin Sun Ulla Stenius Karolinska Institutet, SWEDEN University of Cambridge, UK Karolinska Institutet, SWEDEN Ilona.Silins@ki.se ls418@cam.ac.uk Ulla.Stenius@ki.se Abstract Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are richer in information and also considered to be more in need of the definition of inform"
W10-1913,W06-3309,0,0.326688,"UK University of Cambridge, UK yg244@cam.ac.uk alk23@cam.ac.uk Maria Liakata Aberystwyth University, UK mal@aber.ac.uk Ilona Silins Lin Sun Ulla Stenius Karolinska Institutet, SWEDEN University of Cambridge, UK Karolinska Institutet, SWEDEN Ilona.Silins@ki.se ls418@cam.ac.uk Ulla.Stenius@ki.se Abstract Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are"
W10-1913,W09-3603,0,0.332738,"Missing"
W10-1913,W04-1205,0,\N,Missing
W12-1702,P06-4018,0,0.0188896,"bull has nose). 10. All remaining stop-words were removed; properties ending in stop-words (e.g., bull has in and bull has its) were removed completely. This yielded 7,518 property-triples with 254 distinct relations and an average of 14.7 triples per concept. 2.3 Parsing We parsed both corpora using the C&C parser (Clark and Curran, 2007) as we employ both GR and POS information in our learning method. To accelerate this stage, we process only sentences containing a form (e.g., singular/plural) of one of our training/testing concepts. We lemmatise each word using the WordNet NLTK lemmatiser (Bird, 2006). Parsing our corpora yields around 10Gb and 12Gb of data for UKWAC and Wikipedia respectively. The C&C dependency parse output contains, for a given sentence, a set of GRs forming an acyclic graph whose nodes correspond to words from the sentence, with each node also labelled with the POS of that word. Thus the GR-POS graph interrelates all 14 lexical, POS and GR information for the entire sentence. It is therefore possible to construct a GR-POS graph rooted at our target term (the concept in question), with POS-labelled words as nodes, and edges labelled with GRs linking the nodes to one ano"
W12-1702,J90-1003,0,0.452752,"taining an NLTK (Bird, 2006) corpus stop-word. Despite these exclusions, some general (and therefore less informative) relation/feature combinations (e.g., is good, is new ) were still ranking highly. To mitigate this, we extract both log-likelihood (LL) and pointwise mutual information (PMI) scores for each concept/feature pair to assess the relative saliency of each extracted feature, with a view to downweighting common but less interesting features. To speed up this and later stages, we calculate both statistics for the top 1,000 triples extracted for each concept only. PMI was proposed by Church and Hanks (1990) to estimate word association. We will use it to measure the strength of association between a concept and its feature. We hope that emphasising conceptfeature pairs with high mutual information will render our triples more relevant/informative. 16 We also employ the LL measure across our set of concept-feature pairs. Proposed by Dunning (1993), LL is a measure of the distribution of linguistic phenomena in texts and has been used to contrast the relative corpus frequencies of words. Our aim is to highlight features which are particularly distinctive for a given concept, and hence likely to be"
W12-1702,J07-4004,0,0.0174114,"sed for sending letters → envelope usedfor-sending letters and envelope used-for sending). 9. Any remaining multi-word properties were split with the first term after the concept acting as the relation (e.g., bull has ring in its nose → bull has ring, bull has in, bull has its and bull has nose). 10. All remaining stop-words were removed; properties ending in stop-words (e.g., bull has in and bull has its) were removed completely. This yielded 7,518 property-triples with 254 distinct relations and an average of 14.7 triples per concept. 2.3 Parsing We parsed both corpora using the C&C parser (Clark and Curran, 2007) as we employ both GR and POS information in our learning method. To accelerate this stage, we process only sentences containing a form (e.g., singular/plural) of one of our training/testing concepts. We lemmatise each word using the WordNet NLTK lemmatiser (Bird, 2006). Parsing our corpora yields around 10Gb and 12Gb of data for UKWAC and Wikipedia respectively. The C&C dependency parse output contains, for a given sentence, a set of GRs forming an acyclic graph whose nodes correspond to words from the sentence, with each node also labelled with the POS of that word. Thus the GR-POS graph int"
W12-1702,P07-1030,0,0.0250514,"previous and related work. Having framed our task in this way, there is an obvious parallel with relation extraction: both necessitate the selection/classification of relationships between individual entities (in our case, between concept and feature). Hearst (1992) was the first to propose a pattern-based approach to this task using lexico-syntactic patterns to automatically extract hyponyms and this technique has frequently been used for ontology learning. For example, Pantel and Pennacchiotti (2008) linked instantiations of a set of semantic relations into existing semantic ontologies and Davidov et al. (2007) employed seed concepts from a given semantic class to discover relations shared by concepts in that class. Our task is more complex than classic relation extraction for two main reasons: 1) the relations which we aim to extract are not limited to a small set of just a few well-defined relations (e.g., is-a and partof) nor to the relations of a specific semantic class (e.g., capital-is for countries). Indeed the relations can be as many and diverse as the concepts themselves (e.g., each concept could possess a unique and distinguishing relation and feature). 2) We are attempting to simultaneou"
W12-1702,J93-1003,0,0.101718,"aliency of each extracted feature, with a view to downweighting common but less interesting features. To speed up this and later stages, we calculate both statistics for the top 1,000 triples extracted for each concept only. PMI was proposed by Church and Hanks (1990) to estimate word association. We will use it to measure the strength of association between a concept and its feature. We hope that emphasising conceptfeature pairs with high mutual information will render our triples more relevant/informative. 16 We also employ the LL measure across our set of concept-feature pairs. Proposed by Dunning (1993), LL is a measure of the distribution of linguistic phenomena in texts and has been used to contrast the relative corpus frequencies of words. Our aim is to highlight features which are particularly distinctive for a given concept, and hence likely to be features of that concept alone. We calculate an overall score for a triple, t, by a weighted combination of the triple’s SVM, PMI and LL scores using the following formula: score(t) = βPMI ·PMI(t)+βLL ·LL(t)+βSVM ·SVM(t) where the PMI, SVM and LL scores are normalised so they are in the range [0, 1]. The relative β weights thus give an estimat"
W12-1702,gimenez-marquez-2004-svmtool,0,0.0498136,"Missing"
W12-1702,C92-2082,0,0.0525765,"ation feature pairs given a particular concept. This recoding renders our task more well-defined and makes evaluation of our method 12 ncmod marine0NNP ncsubj include2VBP dobj species0IN ncmod five1DT ncmod of3IN dobj turtle5NN Figure 1: C&C-derived GR-POS graph for the sentence Marine reptiles include five species of turtle. more comparable to previous and related work. Having framed our task in this way, there is an obvious parallel with relation extraction: both necessitate the selection/classification of relationships between individual entities (in our case, between concept and feature). Hearst (1992) was the first to propose a pattern-based approach to this task using lexico-syntactic patterns to automatically extract hyponyms and this technique has frequently been used for ontology learning. For example, Pantel and Pennacchiotti (2008) linked instantiations of a set of semantic relations into existing semantic ontologies and Davidov et al. (2007) employed seed concepts from a given semantic class to discover relations shared by concepts in that class. Our task is more complex than classic relation extraction for two main reasons: 1) the relations which we aim to extract are not limited t"
W12-1702,W10-0608,1,0.886771,"ReVerb (Etzioni et al., 2011) and WOE (Wu and Weld, 2010) systems. These systems are designed to extract legitimate relations from a given sentence. In contrast, our aim is to capture more general relationships which are ‘commonsense’; just because an extracted relation is correct in a given context does not automatically make it true in general. Previous reasoned approaches to our task have taken their lead from Hearst and her successors, employing manually-created rulesets to extract such properties from corpora (e.g., Baroni et al. (2009), Devereux et al. (2010), and our comparison system (Kelly et al., 2010)). Baroni et al. extract relational information in the form of ‘type-sketches’, which give an approximate, implicit description of the relationship whereas we are aiming to extract explicit relations between the target concept and its corresponding features. Devereux et al. and Kelly et al. have attempted this, but both employ WordNet (Fellbaum, 1998) to extract semantic relatedness information. We use semi-supervised learning as it offers a flexible technique of harnessing small amounts of labelled data to derive information from unlabelled datasets/corpora and allows us to guide the extracti"
W12-1702,P10-1013,0,0.0120589,"., each concept could possess a unique and distinguishing relation and feature). 2) We are attempting to simultaneously extract two pieces of information: features of the concept and those features’ defining relationship with the concept, but only those relations and features which would be classified as ‘common-sense’, something which is easy for humans to recognise but difficult (if not impossible) to describe rigorously or formally. There has recently been work on the automatic extraction of binary relations that scale to a web corpus, for example the ReVerb (Etzioni et al., 2011) and WOE (Wu and Weld, 2010) systems. These systems are designed to extract legitimate relations from a given sentence. In contrast, our aim is to capture more general relationships which are ‘commonsense’; just because an extracted relation is correct in a given context does not automatically make it true in general. Previous reasoned approaches to our task have taken their lead from Hearst and her successors, employing manually-created rulesets to extract such properties from corpora (e.g., Baroni et al. (2009), Devereux et al. (2010), and our comparison system (Kelly et al., 2010)). Baroni et al. extract relational in"
W13-2609,N09-1003,0,0.131669,"relations between word concepts and have achieved impressive performance in related NLP tasks (Sahlgren, 2006; Turney & Pantel, 2010). In these studies, however, it is not always clear exactly which semantic relation is best reflected by the implemented models. Indeed, research has shown that by changing certain parameter settings in the standard VSM architecture, models can be adapted to better reflect one relation type or another. Specifically, models with smaller context windows are reportedly better at reflecting similarity, whereas models with larger windows better reflect association. (Agirre et al., 2009; Peirsman et al., 2008) Our experiments in this section aim first to corroborate these findings by testing how models of varying context window sizes perform on empirical data of both association and similarity. We then test if this effect differentially affects performance on concrete and abstract words. 3.2 3.3 Results In line with previous studies, we observed that VSMs with smaller window sizes were better able to predict similarity. The model with window size 3 achieves a higher correlation with similarity (Spearman rank rs = -0.29) than the model with window size 9 (rs = -0.25). However"
W13-2609,P94-1019,0,0.102124,"igan (1968) or Toglia and Battig (1978). In both cases contributors were asked to rate words based on a scale of 1 (very abstract) to 7 (very concrete). 1 We extracted the all 2,230 nouns from the USF data for which concreteness scores were known, yielding a total of 15,195 noun-noun pairs together with concreteness and association values. Although some empirical word-similarity datasets are publically available, they contain few if any abstract words (Finkelstein et al., 2002; Rubenstein & Goodenough, 1965). Therefore to evaluate similarity modeling, we use Wu-Palmer Similarity (similarity) (Wu & Palmer, 1994), a word similarity metric based on the position of the senses of two words in the WordNet taxonomy (Felbaum, 1998). similarity can be applied to both abstract and concrete nouns and achieves a high correlation, with human similarity judgments (Wu & Palmer, 1994).2 Motivation VSMs are well established as a method of quantifying relations between word concepts and have achieved impressive performance in related NLP tasks (Sahlgren, 2006; Turney & Pantel, 2010). In these studies, however, it is not always clear exactly which semantic relation is best reflected by the implemented models. Indeed,"
W13-2609,D11-1098,0,\N,Missing
W13-2609,C94-1103,0,\N,Missing
W13-2609,D11-1063,0,\N,Missing
W16-2501,N09-1003,0,0.644427,"Missing"
W16-2501,J15-4004,1,0.698951,"erform and they are often used to estimate the quality of representations before using them in downstream applications. The underlying assumption is that intrinsic evaluations can, to some degree, predict extrinsic performance. In this study, we demonstrate that this assumption fails to hold for many standard datasets. We generate a set of word representations with varying context window sizes and compare their performance in intrinsic and extrinsic evaluations, showing that these evaluations yield mutually inconsistent results. Among all the benchmarks explored in our study, only SimLex-999 (Hill et al., 2015) is a good predictor of downstream performance. This may be related to the fact that it stands out among other benchmark datasets in distinguishing highly similar concepts (male, man) from highly related but dissimilar ones (computer, keyboard). The quality of word representations is frequently assessed using correlation with human judgements of word similarity. Here, we question whether such intrinsic evaluation can predict the merits of the representations for downstream tasks. We study the correlation between results on ten word similarity benchmarks and tagger performance on three standard"
W16-2501,P06-4018,0,0.00545276,"2012) Radinsky et al. (2011) Halawi et al. (2012) Luong et al. (2013) Hill et al. (2015) Table 2: Intrinsic evaluation datasets 2.2 Corpora and Pre-processing To create word vectors, we gather a large corpus of unannotated English text, drawing on publicly available resources identified in word2vec distribution materials. Table 1 lists the text sources and their sizes. We extract raw text from the Wikipedia dump using the Wikipedia Extractor2 ; the other sources are textual. We pre-process all text with the Sentence Splitter and the Treebank Word Tokenizer provided by the NLTK python library (Bird, 2006). In total, there are 3.8 billion tokens (19 million distinct types) in the processed text. 2.3 #Tokens (Train/Test) 337,195 / 129,892 211,727 / 47,377 203,621 / 46,435 2.5 Extrinsic evaluation To evaluate the word representations in downstream tasks, we use them in three standard sequence labeling tasks selected by Collobert et al. (2011): POS tagging of Wall Street Journal sections of Penn Treebank (PTB) (Marcus et al., 1993), chunking of CoNLL’00 shared task data (Tjong Kim Sang and Buchholz, 2000), and NER of CoNLL’03 shared task data (Tjong Kim Sang and De Meulder, 2003). We use the stand"
W16-2501,P14-2050,0,0.0258938,"ext windows, representations tend to capture the topic or domain or a word, while smaller windows tend to emphasize the learning of word function. This is because the role/function of a word is categorized by its proximate syntactic context, while a large window captures words that are less informative for this categorization (Turney, 2012). For example, in the sentence Australian scientist discovers star with telescope, the context of the word discovers in a window of size 1 includes scientist and star, while a larger context window will include more words related by topic such as telescope (Levy and Goldberg, 2014). The association of large window sizes with greater topicality is discussed also by Hill et al. (2015) and Levy et al. (2015). 5 Conclusion One of the primary goals of intrinsic evaluation is to provide insight into the quality of a representation before it is used in downstream applications. However, we found that the majority of word similarity datasets fail to predict which representations will be successful in sequence labelling tasks, with only one intrinsic measure, SimLex-999, showing high correlation with extrinsic measures. In concurrent work, we have also observed a similar effect f"
W16-2501,Q15-1016,0,0.154162,"word function. This is because the role/function of a word is categorized by its proximate syntactic context, while a large window captures words that are less informative for this categorization (Turney, 2012). For example, in the sentence Australian scientist discovers star with telescope, the context of the word discovers in a window of size 1 includes scientist and star, while a larger context window will include more words related by topic such as telescope (Levy and Goldberg, 2014). The association of large window sizes with greater topicality is discussed also by Hill et al. (2015) and Levy et al. (2015). 5 Conclusion One of the primary goals of intrinsic evaluation is to provide insight into the quality of a representation before it is used in downstream applications. However, we found that the majority of word similarity datasets fail to predict which representations will be successful in sequence labelling tasks, with only one intrinsic measure, SimLex-999, showing high correlation with extrinsic measures. In concurrent work, we have also observed a similar effect for biomedical domain tasks and word vectors (Chiu et al., 2016). We further considered the differentiation between relatedness"
W16-2501,P12-1015,0,0.205967,"Missing"
W16-2501,W13-3512,0,0.205281,"Missing"
W16-2501,J93-2004,0,0.0645654,"the Wikipedia Extractor2 ; the other sources are textual. We pre-process all text with the Sentence Splitter and the Treebank Word Tokenizer provided by the NLTK python library (Bird, 2006). In total, there are 3.8 billion tokens (19 million distinct types) in the processed text. 2.3 #Tokens (Train/Test) 337,195 / 129,892 211,727 / 47,377 203,621 / 46,435 2.5 Extrinsic evaluation To evaluate the word representations in downstream tasks, we use them in three standard sequence labeling tasks selected by Collobert et al. (2011): POS tagging of Wall Street Journal sections of Penn Treebank (PTB) (Marcus et al., 1993), chunking of CoNLL’00 shared task data (Tjong Kim Sang and Buchholz, 2000), and NER of CoNLL’03 shared task data (Tjong Kim Sang and De Meulder, 2003). We use the standard train/test splits and evaluation criteria for each dataset, evaluating PTB POS tagging using token-level accuracy and CoNLL’00/03 chunking and NER using chunk/entity-level F -scores as implemented in the conlleval evaluation script. Table 3 shows basic statistics for each dataset. Intrinsic evaluation We perform intrinsic evaluations on the ten benchmark datasets presented in Table 2. We follow the standard experimental pro"
W16-2501,W16-2922,1,0.192482,"greater topicality is discussed also by Hill et al. (2015) and Levy et al. (2015). 5 Conclusion One of the primary goals of intrinsic evaluation is to provide insight into the quality of a representation before it is used in downstream applications. However, we found that the majority of word similarity datasets fail to predict which representations will be successful in sequence labelling tasks, with only one intrinsic measure, SimLex-999, showing high correlation with extrinsic measures. In concurrent work, we have also observed a similar effect for biomedical domain tasks and word vectors (Chiu et al., 2016). We further considered the differentiation between relatedness (association) and similarity (synonymy) as an explanatory factor, noting that the majority of intrinsic evaluation datasets do not systematically make this distinction. Our results underline once more the importance of including also extrinsic evaluation when assessing NLP methods and resources. To encourage extrinsic evaluation of vector space representations, we make all of our newly introduced methods available to the community under open licenses from https://github.com/ cambridgeltl/RepEval-2016. This phenomenon provides a po"
W16-2501,P14-5004,0,0.0380063,"otated corpus of 3.8 billion words, and demonstrate that most intrinsic evaluations are poor predictors of downstream performance. We argue that this issue can be traced in part to a failure to distinguish specific similarity from relatedness in intrinsic evaluation datasets. We make our evaluation tools openly available to facilitate further study. 1 Introduction 2 The use of vector representations of words is now pervasive in natural language processing, and the importance of their evaluation is increasingly recognized (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chen et al., 2013; Schnabel et al., 2015). Such evaluations can be broadly divided into intrinsic and extrinsic. The most common form of intrinsic evaluation uses word pairs annotated by humans to determine their degree of similarity (for varying definitions of similarity). These are then used to directly assess word representations based on how they rank the word pairs. In contrast, in extrinsic evaluation, word representations are used as input to a downstream task such as part-of-speech (POS) tagging or named entity recognition (NER). Here, good models are simply those that provide 2.1 Ma"
W16-2501,D15-1036,0,0.0859089,"onstrate that most intrinsic evaluations are poor predictors of downstream performance. We argue that this issue can be traced in part to a failure to distinguish specific similarity from relatedness in intrinsic evaluation datasets. We make our evaluation tools openly available to facilitate further study. 1 Introduction 2 The use of vector representations of words is now pervasive in natural language processing, and the importance of their evaluation is increasingly recognized (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chen et al., 2013; Schnabel et al., 2015). Such evaluations can be broadly divided into intrinsic and extrinsic. The most common form of intrinsic evaluation uses word pairs annotated by humans to determine their degree of similarity (for varying definitions of similarity). These are then used to directly assess word representations based on how they rank the word pairs. In contrast, in extrinsic evaluation, word representations are used as input to a downstream task such as part-of-speech (POS) tagging or named entity recognition (NER). Here, good models are simply those that provide 2.1 Materials and Methods Word Vectors We generat"
W16-2501,S14-2048,0,0.0521007,"Missing"
W16-2501,W00-0726,0,0.365583,"Missing"
W16-2501,W03-0419,0,0.289727,"Missing"
W16-2501,P10-1040,0,0.0720468,"ariety of word vectors induced from an unannotated corpus of 3.8 billion words, and demonstrate that most intrinsic evaluations are poor predictors of downstream performance. We argue that this issue can be traced in part to a failure to distinguish specific similarity from relatedness in intrinsic evaluation datasets. We make our evaluation tools openly available to facilitate further study. 1 Introduction 2 The use of vector representations of words is now pervasive in natural language processing, and the importance of their evaluation is increasingly recognized (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chen et al., 2013; Schnabel et al., 2015). Such evaluations can be broadly divided into intrinsic and extrinsic. The most common form of intrinsic evaluation uses word pairs annotated by humans to determine their degree of similarity (for varying definitions of similarity). These are then used to directly assess word representations based on how they rank the word pairs. In contrast, in extrinsic evaluation, word representations are used as input to a downstream task such as part-of-speech (POS) tagging or named entity recognition (NER). Here, g"
W16-2501,W14-3302,0,\N,Missing
W16-2922,P06-4018,0,0.0126278,"Open Access articles. The PubMed database has more than 25 million citations that cover the titles and abstracts of biomedical scientific publications. A version of PMC articles is distributed in text format1 whereas PubMed is distributed in XML. Thus, we use a PubMed text extractor2 to extract title and abstract texts from the PubMed source XML. Both PubMed and PMC were pre-processed with the Genia Sentence Splitter (GeniaSS) (Sætre et al., 2007), which is optimized for bio-medical text. We further tokenize the sentences with the Tree bank Word Tokenizer provided by the NLTK python library (Bird, 2006). The corpus statistics are shown in Table 1. 2.2 0.0125 / 0.025 / 0.05 / 0.1 25 / 50 / 100 / 200 / 400 / 500 / 800 1 / 2 / 4 / 5 / 8 / 16 / 20 / 25 / 30 Table 2: Hyper-parameters and tested values. Default values shown in bold. Materials and Methods 2.1 Values 1 / 2 / 3 / 5 / 8 /10 / 15 0 / 1e-1 / 1e-2 / 1e-3 / 1e-4 1e-5 / 1e-6 / 1e-7 / 1e-8 / 1e-9 0 / 5 / 10 / 20 / 50 / 100 / 200 400 / 800 / 1000 / 1200 / 2400 2.3 Hyper-parameters We test the following key hyper-parameters: Negative sample size (neg): the representation of a word is learned by maximizing its predicted probability to co-occur"
W16-2922,D14-1162,0,0.118238,"more, we find that bigger corpora do not necessarily produce better biomedical domain word embeddings. We make our evaluation tools and resources as well as the created state-of-the-art word embeddings available under open licenses from https://github.com/ cambridgeltl/BioNLP-2016. 1 Introduction As one of the main inputs of many NLP methods, word representations have long been a major focus of research. Recently, the embedding of words into a low-dimensional space using neural networks was suggested (Bengio et al., 2003; Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013b; Pennington et al., 2014). These approaches represent each word as a dense vector of real numbers, where words that are semantically related to one another map to similar vectors. Among neural embedding approaches, the skip-gram model of Mikolov et al. (2013a) has achieved cutting-edge results in many NLP tasks, including sentence completion, analogy and sentiment analysis (Mikolov et al., 2013a; Mikolov et al., 2013b; Fern´andez et al., 2014). 166 Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 166–174, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistic"
W16-2922,W16-2501,1,0.223336,"vs. CBOW) and hyper-parameter settings (negative sampling, sub sample rate, min-count, learning rate, vector dimension, context window size). For corpora, sentence-shuffled PubMed texts appear to produce the best performance, exceeding that of the notably larger combination with PMC texts. For hyper-parameter settings, it is evident that performance can be notably improved over the default parameters, but the effects of the different hyper-parameters on performance are mixed and sometimes counterintuitive. We have previously found a similar result in general domain work (with Wikipedia text) (Chiu et al., 2016). Several directions remain open for future work. First, in addition to tuning individual parameters in isolation, we can study the effect of tuning two or more parameters simultaneously. In addition, the number of training iterations was not considered in the experiments here, and careful tuning of this parameter both separately and jointly with associated parameters such as alpha may offer further opportunities for improvement. Discussion In this study, we have created vectors with PubMed, PMC and the combination of the two with a large variety of different model, preprocessing and parameter"
W16-2922,S14-2048,0,0.0133937,"Missing"
W16-2922,J15-4004,1,0.534673,"d extrinsic evaluation results for window size (Unit: ρ: dashed line, F-score: solid line) 3.2.3 Setting PubMed skip-gram 10 200 0.05 1e-4 2, 30 5 tion to primarily capture word function (Turney, 2012). It is possible that for intrinsic evaluation datasets such as UMNSRS it is more important to model topical rather than functional similarity. Conversely, it is intuitively clear that for tasks such as named entity recognition the modeling of functional similarity such as co-hyponymymy is centrally important. For further discussion on the effect of the context window size parameter, we refer to Hill et al. (2015) and Levy et al. (2015). Context Window Size (win) We find contradictory results from changing the size of the context window parameter (Figure 6). All three sets of vectors show a notable increase in the intrinsic measures when the context window size grows (Table 16). However, the extrinsic evaluation shows the opposite pattern (Table 17): all results in extrinsic tasks have an early perofmance peak with a narrow window (e.g. win = 1), followed by a gradual decrease when window size increases. One possible explanation may be that a larger window emphasizes the learning of domain/topic simila"
W16-2922,W04-1213,0,0.602102,"-of-the-art performance that can be further improved with parameter tuning, we focus on its performance on biomedical data with different inputs and hyper-parameters. We use all available biomedical scientific literature for learning word embeddings using models implemented in word2vec. For intrinsic evaluation, we use the standard UMNSRS-Rel and UMNSRS-Sim datasets (Pakhomov et al., 2010), which enable us to measure similarity and relatedness separately. For extrinsic evaluation, we apply a neural network-based named entity recognition (NER) model to two standard benchmark NER tasks, JNLPBA (Kim et al., 2004) and the BioCreative II Gene Mention task (Smith et al., 2008). Apart from showing that the optimization of hyper-parameters boosts the performance of vectors, we also find that one such parameter leads to contradictory results between intrinsic and extrinsic evaluations. We further observe that a larger corpus does not necessarily guarantee better reThe quality of word embeddings depends on the input corpora, model architectures, and hyper-parameter settings. Using the state-of-the-art neural embedding tool word2vec and both intrinsic and extrinsic evaluations, we present a comprehensive stud"
W16-2922,P10-1040,0,0.0142778,"intrinsic and extrinsic evaluations. Furthermore, we find that bigger corpora do not necessarily produce better biomedical domain word embeddings. We make our evaluation tools and resources as well as the created state-of-the-art word embeddings available under open licenses from https://github.com/ cambridgeltl/BioNLP-2016. 1 Introduction As one of the main inputs of many NLP methods, word representations have long been a major focus of research. Recently, the embedding of words into a low-dimensional space using neural networks was suggested (Bengio et al., 2003; Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013b; Pennington et al., 2014). These approaches represent each word as a dense vector of real numbers, where words that are semantically related to one another map to similar vectors. Among neural embedding approaches, the skip-gram model of Mikolov et al. (2013a) has achieved cutting-edge results in many NLP tasks, including sentence completion, analogy and sentiment analysis (Mikolov et al., 2013a; Mikolov et al., 2013b; Fern´andez et al., 2014). 166 Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 166–174, c Berlin, Germany, August 12, 20"
W16-2922,Q14-1041,0,0.013299,"Missing"
W16-2922,Q15-1016,0,0.464773,"on. The magnitude of these updates is controlled by the learning rate. Vector dimension (dim): The vector dimension is the size of the learned word vector. While a higher dimension tends to capture better word representations, their training is more computationally costly and produces a larger word embedding matrix. Context window size (win): The size of the context window defines the range of words to be included as the context of a target word. For instance, a window size of 5 takes five words before and after a target word as its context for training. We refer to Mikolov et al. (2013a) and Levy et al. (2015) for further details regarding these parameters. 2.4 2.6 Given that the ultimate evaluation for word vectors is their performance in downstream applications, we also assess the quality of the vectors by performing NER using two well-established biomedical reference standards: the BioCreative II Gene Mention task corpus (BC2) (Smith et al., 2008) and the JNLPBA corpus (PBA) (Kim et al., 2004). Both of these corpora consist of approximately 20,000 sentences from PubMed abstracts manually annotated for mentions of biomedical entity names. Following the window approach architecture with word-level"
W16-2922,W15-3820,0,0.198816,"od Word Embeddings for Biomedical NLP Billy Chiu Gamal Crichton Anna Korhonen Sampo Pyysalo Language Technology Lab DTAL, University of Cambridge {hwc25|gkoc2|alk23}@cam.ac.uk, sampo@pyysalo.net Abstract Although word embeddings have been studied extensively in recent work (e.g. Lapesa and Evert (2014)), most such studies only involve general domain texts and evaluation datasets, and their results do not necessarily apply to biomedical NLP tasks. In the biomedical domain, Stenetorp et al. (2012) studied the effect of corpus size and domain on various word clustering and embedding methods, and Muneeb et al. (2015) compared two state-of-the-art word embedding tools: word2vec and Global Vectors (GloVe) on a word-similarity task. They showed that skip-gram significantly out-performs other models and that its performance can be further improved by using higher dimensional vectors. The word2vec tool was also used to create biomedical domain word representations by Pyysalo et al. (2013) and Kosmopoulos et al. (2015). Given that word2vec has been shown to achieve state-of-the-art performance that can be further improved with parameter tuning, we focus on its performance on biomedical data with different input"
W16-5101,W16-2922,1,0.793637,"ion 3.4, the word vectors used to initialize the embedding layer of the network can have a significant effect on performance. We trained the models using each of the word vectors shown in Table 3 with oversampling (see above) and evaluated development set performance using the maximum F-score and AUC metrics. The results are summarized in Figure 2. Surprisingly, we find that the general domain Google News vectors give very competitive performance despite their high out-of-vocabulary rate (see Table 3), outperforming all in-domain vectors with the exception of the window size 2 word vectors of Chiu et al. (2016). Even these biomedical word vectors only show very modest advantage over the Google News vectors for AUC. In the last development set experiments below and the final test set experiments, we apply the PubMed-based vectors induced with window size 2 from Chiu et al. that were shown to give the best results here. 97.8 97.75 97.7 97.65 97.6 97.55 97.5 97.45 97.4 97.35 97.3 97.25 1 1 filter size 2 3 2 filter sizes 4 5 3 filter sizes 6 7 4 filter sizes 8 9 10 5 filter sizes Figure 3: Macro-average AUC with respect to a varying number of filter sizes. Each point on the graph represents the maximum"
W16-5101,J81-4005,0,0.763341,"Missing"
W16-5101,D14-1181,0,0.11742,"eatures are extracted from the training data and are then filtered by frequency to remove features that are too common or too rare, leaving behind only the most discriminating features. We use a linear kernel and fine-tune the regularization parameter c on the development dataset using the same process applied for the BoW model. As there are significantly more negatively labelled documents than positives, we use inverse class weighting in order to correct for the class imbalance when training the classifiers. 3.3 Convolutional Neural Network We base our CNN architecture on the simple model of Kim (2014). In brief, this model consists of an initial embedding layer that maps input texts into matrices, followed by convolutions of different filter sizes and 1-max pooling, and finally a fully connected layer. The architecture is illustrated in Figure 1. We implemented the neural network using Keras (Chollet, 2015). Model hyperparameters and the training setup were initially based on those applied by Kim (2014), summarized in the following: Parameter Word vector size Filter sizes Number of filters Dropout probability Minibatch size Value 300 (Google News vectors) 3, 4, and 5 300 (100 of each size)"
W16-5101,W16-2918,0,0.016505,"CNN). CNNs were first proposed for image processing (LeCun and Bengio, 1995) and have been recently shown to achieve state-of-the-art performance in a range of NLP tasks, in particular in text classification (Zhang et al., 2015; Severyn and Moschitti, 2015; Zhang and Wallace, 2015). While neural network-based methods in general and “deep” networks in particular are increasingly popular for general domain NLP, there has been comparatively little work applying this class of methods to biomedical text. One recent study applying a CNN model to biomedical text classification task was presented by (Limsopatham and Collier, 2016), who applied CNNs to the task of adverse drug reaction detection in social media messages (Ginn et al., 2014). In addition to the specific subdomain of the source texts and the novel categories represented by the hallmarks of cancer, one factor that sets apart the task here from this previous work is the length of the texts: instead of sentences or brief social media messages, our task involves the classification of publication abstracts typically consisting of hundreds of words. 2 Data For training and evaluating our methods, we use the corpus of 1852 biomedical publication abstracts annotat"
W16-5101,W13-2008,1,0.860593,"Missing"
W16-5101,S15-2079,0,0.0172903,"ext Mining (BioTxtM 2016), pages 1–9, Osaka, Japan, December 12th 2016. In this work, our focus is on studying biomedical text classification using machine learning methods that emphasize feature learning rather than manual feature engineering. We adopt the task setting and dataset of Baker et al. (2016), but instead of SVMs, we focus on convolutional neural networks (CNN). CNNs were first proposed for image processing (LeCun and Bengio, 1995) and have been recently shown to achieve state-of-the-art performance in a range of NLP tasks, in particular in text classification (Zhang et al., 2015; Severyn and Moschitti, 2015; Zhang and Wallace, 2015). While neural network-based methods in general and “deep” networks in particular are increasingly popular for general domain NLP, there has been comparatively little work applying this class of methods to biomedical text. One recent study applying a CNN model to biomedical text classification task was presented by (Limsopatham and Collier, 2016), who applied CNNs to the task of adverse drug reaction detection in social media messages (Ginn et al., 2014). In addition to the specific subdomain of the source texts and the novel categories represented by the hallmarks of"
W16-5101,D09-1067,1,0.777256,"12) and included as features using a BoW-style representation. Noun bigrams Compound nouns (without lemmatization) are combined to generate bigram features. Nouns pairs often represent specific, discriminative concepts such as “gene mutation”. Grammatical relations triples The C&C Parser with a biomedical domain model (Rimell and Clark, 2009) is used to parse the documents, and the dobj (direct object), ncsubj (non-clausal subject) and iobj (indirect object) relations, and their head and dependent words then represented as features. Verb classes The hierarchical classification of 399 verbs of Sun and Korhonen (2009) is used to generate features for verbs, utilizing all three levels of abstraction by allocating three bits in the feature representation for each concrete class, i.e. one bit for each level of the verb class hierarchy. Named entities (NE) The ABNER NER tool (Settles, 2005) is used to identify five named entity types that are particularly relevant to cancer research: proteins, DNA, RNA, cell lines and cell types. Features are then created pairing each entity type and its associated words. Medical subject headings (MeSH) The MeSH headings assigned to the documents in the biomedical publication"
W17-0903,D11-1027,0,0.0745905,"d et al., 2007). The latter counts the largest amount of ex25 Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 25–30, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics 2 3 Events in Linguistic Theory Previous Work Many previous works identified causal relations using metrics or traditional machine learning algorithms. Metrics of the ‘causal potential’ of event pairs were estimated using distributional information (Beamer and Girju, 2009), verb pairs (Riaz and Girju, 2013) or discourse relation markers (Do et al., 2011). Other techniques employed manually defined rules, consisting in high-level patterns (Grivaz, 2012) or a set of axioms (Ovchinnikova et al., 2010). The machine learning approaches formulated causal relation identification as a binary classification problem. This problem sometimes involved an intermediate step of discourse marker prediction (Zhou et al., 2010). Features based on finegrained syntactic representations proved particularly helpful (Wang et al., 2010), and were sometimes supplemented with information about word polarity, verb classes, and discourse context (Pitler et al., 2009; Lin"
W17-0903,W15-1622,0,0.284856,"a et al., 2010). The machine learning approaches formulated causal relation identification as a binary classification problem. This problem sometimes involved an intermediate step of discourse marker prediction (Zhou et al., 2010). Features based on finegrained syntactic representations proved particularly helpful (Wang et al., 2010), and were sometimes supplemented with information about word polarity, verb classes, and discourse context (Pitler et al., 2009; Lin et al., 2014). Few approaches based on deep learning have been proposed for discourse relation classification so far. Zhang et al. (2015) focused on implicit relations. They introduced a Shallow Convolutional Neural Network that learns exclusively from lexical features. It adopts some strategies to amend the sparseness and imbalance of the dataset, such as a shallow architecture, naive convolutional operations, random under-sampling, and normalization. This approach outperforms baselines based on a Support Vector Machine, a Transductive Support Vector Machine, and a Recursive AutoEncoder. Moreover, related work on nominal relation classification (Zeng et al., 2014; Zhang and Wang, 2015) showed improvements due to using addition"
W17-0903,W13-4004,0,0.0989194,"tions are partly subjective and have low inter-annotator agreement (Grivaz, 2012; Dunietz et al., 2015). Finally, they have to be detected through linguistic context and world knowledge: unfortunately, this information cannot be approximated by explicit relations deprived of their signal (Sporleder and Lascarides, 2008). Notwithstanding the partial redundancy between signal and context, implicit examples and explicit examples belonging to the same class appear to be too dissimilar linguistically. Although various techniques have been proposed for the task, ranging from distributional metrics (Riaz and Girju, 2013, inter alia) to traditional machine learning algorithms (Lin et al., 2014, inter alia), few have been based on deep learning. Those that have used deep learning have mostly relied on lexical features (Zhang et al., 2015; Zhang and Wang, 2015). The aim of our work is to enrich Artificial Neural Networks with features that capture insights from linguistic theory (§ 2) as well as related works (§ 3). In particular, they capture information about the content and position of the events involved in the relation. After presenting the datasets (§ 4), the method (§ 6) and the experimental results (§ 7"
W17-0903,P10-1073,0,0.0152715,"estimated using distributional information (Beamer and Girju, 2009), verb pairs (Riaz and Girju, 2013) or discourse relation markers (Do et al., 2011). Other techniques employed manually defined rules, consisting in high-level patterns (Grivaz, 2012) or a set of axioms (Ovchinnikova et al., 2010). The machine learning approaches formulated causal relation identification as a binary classification problem. This problem sometimes involved an intermediate step of discourse marker prediction (Zhou et al., 2010). Features based on finegrained syntactic representations proved particularly helpful (Wang et al., 2010), and were sometimes supplemented with information about word polarity, verb classes, and discourse context (Pitler et al., 2009; Lin et al., 2014). Few approaches based on deep learning have been proposed for discourse relation classification so far. Zhang et al. (2015) focused on implicit relations. They introduced a Shallow Convolutional Neural Network that learns exclusively from lexical features. It adopts some strategies to amend the sparseness and imbalance of the dataset, such as a shallow architecture, naive convolutional operations, random under-sampling, and normalization. This appr"
W17-0903,W14-0702,0,0.159853,"orks Contribute to Identifying Causal Relations in Discourse Edoardo Maria Ponti LTL, University of Cambridge ep490@cam.ac.uk Anna Korhonen LTL, University of Cambridge alk23@cam.ac.uk Abstract amples and is the only resource distinguishing between explicit and implicit relations. The discourse signal marking causal relations is often ambiguous (i.e. shared with other kinds of relation), or lacking altogether. Identifying implicit causal relations is challenging for several reasons. They often entail a temporal relation of precedence, but this condition is not mandatory (Bethard et al., 2008; Mirza et al., 2014). Moreover, implicit causal relations are partly subjective and have low inter-annotator agreement (Grivaz, 2012; Dunietz et al., 2015). Finally, they have to be detected through linguistic context and world knowledge: unfortunately, this information cannot be approximated by explicit relations deprived of their signal (Sporleder and Lascarides, 2008). Notwithstanding the partial redundancy between signal and context, implicit examples and explicit examples belonging to the same class appear to be too dissimilar linguistically. Although various techniques have been proposed for the task, rangi"
W17-0903,C14-1220,0,0.274845,"ve been proposed for discourse relation classification so far. Zhang et al. (2015) focused on implicit relations. They introduced a Shallow Convolutional Neural Network that learns exclusively from lexical features. It adopts some strategies to amend the sparseness and imbalance of the dataset, such as a shallow architecture, naive convolutional operations, random under-sampling, and normalization. This approach outperforms baselines based on a Support Vector Machine, a Transductive Support Vector Machine, and a Recursive AutoEncoder. Moreover, related work on nominal relation classification (Zeng et al., 2014; Zhang and Wang, 2015) showed improvements due to using additional features (neighbours and hypernyms of nouns), as well as measuring the relative distance of each token in a sentence from the target nouns. Although these features are possibly relevant for the identification of causal relations, they have not been investigated for this task before. Events are complex entities bridging between semantic meaning and the syntactic form (Croft, 2002). The token expressing an event in a text is called a mention and usually consists in a verbal predicate. An event denotes a situation and consists of"
W17-0903,L16-1262,0,0.0307542,"Missing"
W17-0903,D15-1266,0,0.475838,"not be approximated by explicit relations deprived of their signal (Sporleder and Lascarides, 2008). Notwithstanding the partial redundancy between signal and context, implicit examples and explicit examples belonging to the same class appear to be too dissimilar linguistically. Although various techniques have been proposed for the task, ranging from distributional metrics (Riaz and Girju, 2013, inter alia) to traditional machine learning algorithms (Lin et al., 2014, inter alia), few have been based on deep learning. Those that have used deep learning have mostly relied on lexical features (Zhang et al., 2015; Zhang and Wang, 2015). The aim of our work is to enrich Artificial Neural Networks with features that capture insights from linguistic theory (§ 2) as well as related works (§ 3). In particular, they capture information about the content and position of the events involved in the relation. After presenting the datasets (§ 4), the method (§ 6) and the experimental results (§ 7) we conclude (§ 8) by highlighting that the observed improvements stem from the link between event semantics and discourse relations. Although our work focuses on implicit causal relations, the proposed features are sho"
W17-0903,ovchinnikova-etal-2010-data,0,0.0159257,"assifier outperforms strong baselines on two datasets (the Penn Discourse Treebank and the CSTNews corpus) annotated with different schemes and containing examples in two languages, English and Portuguese. This result demonstrates the importance of events for identifying discourse relations. 1 Introduction The identification of causal and temporal relations is potentially useful to many NLP tasks (Mirza et al., 2014), such as information extraction from narrative texts (e.g., question answering, text summarization, decision support) and reasoning through inference based on a knowledge source (Ovchinnikova et al., 2010). A number of resources provide examples of causal relations annotated between event mentions (Mirza et al., 2014) or text spans (Bethard et al., 2008). Among the second group, there are corpora compliant with the assumptions of the Rhetorical Structure Theory (RST) in various languages (Carlson et al., 2002; Aleixo and Pardo, 2008), and the Penn Discourse Treebank (Prasad et al., 2007). The latter counts the largest amount of ex25 Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 25–30, c Valencia, Spain, April 3, 2017. 2017 Associat"
W17-0903,C10-2172,0,0.0187933,"metrics or traditional machine learning algorithms. Metrics of the ‘causal potential’ of event pairs were estimated using distributional information (Beamer and Girju, 2009), verb pairs (Riaz and Girju, 2013) or discourse relation markers (Do et al., 2011). Other techniques employed manually defined rules, consisting in high-level patterns (Grivaz, 2012) or a set of axioms (Ovchinnikova et al., 2010). The machine learning approaches formulated causal relation identification as a binary classification problem. This problem sometimes involved an intermediate step of discourse marker prediction (Zhou et al., 2010). Features based on finegrained syntactic representations proved particularly helpful (Wang et al., 2010), and were sometimes supplemented with information about word polarity, verb classes, and discourse context (Pitler et al., 2009; Lin et al., 2014). Few approaches based on deep learning have been proposed for discourse relation classification so far. Zhang et al. (2015) focused on implicit relations. They introduced a Shallow Convolutional Neural Network that learns exclusively from lexical features. It adopts some strategies to amend the sparseness and imbalance of the dataset, such as a"
W17-0903,P09-1077,0,0.271249,"arkers (Do et al., 2011). Other techniques employed manually defined rules, consisting in high-level patterns (Grivaz, 2012) or a set of axioms (Ovchinnikova et al., 2010). The machine learning approaches formulated causal relation identification as a binary classification problem. This problem sometimes involved an intermediate step of discourse marker prediction (Zhou et al., 2010). Features based on finegrained syntactic representations proved particularly helpful (Wang et al., 2010), and were sometimes supplemented with information about word polarity, verb classes, and discourse context (Pitler et al., 2009; Lin et al., 2014). Few approaches based on deep learning have been proposed for discourse relation classification so far. Zhang et al. (2015) focused on implicit relations. They introduced a Shallow Convolutional Neural Network that learns exclusively from lexical features. It adopts some strategies to amend the sparseness and imbalance of the dataset, such as a shallow architecture, naive convolutional operations, random under-sampling, and normalization. This approach outperforms baselines based on a Support Vector Machine, a Transductive Support Vector Machine, and a Recursive AutoEncoder"
W17-2339,W16-2922,1,0.8381,"Missing"
W17-2339,D14-1181,0,0.0146006,"Missing"
W17-2339,N16-1063,0,0.447495,"ogy and evaluation procedure, and then we present and discuss our results. 2 Related work There have been numerous works that focus on solving hierarchical text classification. Sun and Lim (2001) proposed top-down level-based SVM classification. More recently, Sokolov and BenHur (2010); Sokolov et al. (2013) predict ontology terms by explicitly modeling the structure hierarchy using kernel methods for structured output space. Clark and Radivojac (2013) use a Bayesian network, structured according to the underlying ontology to model the prior probability. Within the context of neural networks, Kurata et al. (2016) propose a scheme for initializing neural networks hidden output layers by taking into account multi-label co-occurrence. Their method treats some of the neurons in the final hidden layer as dedicated neurons for each pattern of label cooccurrence. These dedicated neurons are initialized to connect to the corresponding co-occurring labels with stronger weights than to others. They evaluated their approach on the RCV1-v2 dataset (Lewis et al., 2004) from the general domain, containing only flat labels. Their evaluation shows promising results. However, their applicability to the biomedical doma"
W17-2339,W13-2008,0,0.0876399,"meters. 4 Data We investigate our approach using two multi-label classification tasks. In this section, we describe the nature of these tasks and the annotated gold standard data. Task 1: The Hallmarks of Cancer The Hallmarks of Cancer describe a set of interrelated biological properties and behaviors that enable cancer to thrive in the body. Introduced in the seminal paper by Hanahan and Weinberg (2000)— the most cited paper in the journal Cell—the hallmarks of cancer have seen widespread use in BioNLP for many systems and works, including the BioNLP Shared Task 2013, ‘Cancer Genetics task’ (Pyysalo et al., 2013), which involved the extraction of events (i.e. biological processes) from cancer-domain texts. Baker et al. (2016) have released an expert-annotated dataset for cancer hallmark classification for both sentences and documents from PubMed. The data consists of multilabelled documents and sentences using a taxonomy of 37 classes. Task 2: The exposure taxonomy Larsson et al. (2017) introduce a new task and an associated annotated dataset for the classification of text (documents or sentences) for chemical risk assessment: more specifically, the assessment of exposure routes (such as ingestion, in"
W19-5014,W17-2339,1,0.89318,"tors and behaviours that enable cancer to thrive in the body. Introduced by Weinberg and Hanahan (2000), it has been widely used in biomedical NLP, including as part of HOC Document Train Dev Test Total 1,303 183 366 1,852 EXP Sentence Document Sentence 12,279 1,775 3,410 17,464 2,555 384 722 3,661 25,307 3,770 7,100 36,177 Table 2: Summary statistics of the Hallmarks of Cancer (HOC) and the Chemical Exposure Assessment (EXP) datasets. The model follows the convolutional neural network (CNN) model proposed by Kim (2014). An implementation of this algorithm on HOC and EXP has been published by Baker and Korhonen (2017); we use this implementation in our experiment. The input to the model is an initial word embedding layer that maps input texts into matrices, which is then followed by convolutions of different filter sizes, 1-max pooling, and finally a fully-connected layer leading to an output Softmax layer predicting labels for text. Model hyperparameters and the training setup are summarized in Table 3. Parameters Values Vector dimension Filter sizes Number of filters Dropout probability Minibatch size Input size (in tokens) 200 3,4 and 5 300 0.5 50 500 (documents), 100 (sentences) Table 3: Hyper-paramete"
W19-5014,W18-2311,0,0.037339,"Missing"
W19-5014,P05-1022,0,0.0396845,"ROT) (Krallinger et al., 2017). The corpus provides mention and relation annotations for complex events related to chemical– protein interaction in molecular biology. The goal of this task is to predict whether a given chemical– protein pair is related or not, and to then verify its corresponding relation type. There are five types of relations: Up-regulator, Down-regulator, Agonist, Antagonist, and Substrate. The corpus is provided in the Turku Event Extraction System (TEES) XML format and are installed with the Turku Extraction System (Bj¨orne, 2014). It is parsed with the the BLLIP parser (Charniak and Johnson, 2005) with the McClosky bio-model (Mcclosky, 2010), followed by conversion of the constituency parses into dependency parses using the Stanford Tools (MacCartney et al., 2006). Table 4 summarizes key statistics for the dataset. Train Dev Test Total initial word embedding layer that maps input texts into matrices, followed by convolutions of different filter sizes and 1-max pooling, and finally a fully connected layer, leading to an output Softmax layer for predicting labels. Performance is evaluated using the standard precision, recall, and F1 -score metrics of the labels in the model. Classificati"
W19-5014,W16-2922,1,0.932118,"can be applied to any pretrained word embedding vectors. The intuition behind retrofitting is to encourage the retrofitted vectors to be similar to the vectors of related word 125 Proceedings of the BioNLP 2019 workshop, pages 125–134 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics types and similar to their original distributional representations. Using a standard “off-the-shelf” retrofitting algorithm, we apply the idea of retrofitting to verb clusters to two sets of widely-used pretrained embedding vectors in BioNLP (those by Pyysalo et al. (2013a) and by Chiu et al. (2016)) to obtain improved embeddings. We show that by doing nothing more than using this simple approach, we achieve state-of-the-art results on two text classification tasks (both tasks evaluated on document and sentence level classification), and a relation extraction task. We make our retrofitted embeddings freely available to the BioNLP community along with our code.1 The main contribution of this work is to be the first of its kind to apply verb-based retrofitting in the biomedical domain. Retrofitting has thus far only been applied for aligning vectors to Medical Subject Headings (MeSH) (Yu e"
W19-5014,N15-1184,0,0.496344,"esis is therefore that by retrofitting embedded word representations to semantic verb classes, semanticallysimilar verbs (i.e. member verbs within the same lexical class) like “suppress” and “inhibit” will be pulled together in vector space, whereas verbs like “collect” and “examine” will not. Consequently, this allows NLP systems to generalize away from individual verbs, alleviating the data sparseness problem of representing each verb in the corpus individually. Retrofitting is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors (Faruqui et al., 2015). It is applied as a post-processing step by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors. It can be applied to any pretrained word embedding vectors. The intuition behind retrofitting is to encourage the retrofitted vectors to be similar to the vectors of related word 125 Proceedings of the BioNLP 2019 workshop, pages 125–134 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics types and similar to their original distributional representations. Using a standard “off-the-shelf” retrofitting alg"
W19-5014,N13-1092,0,0.102769,"Missing"
W19-5014,D14-1181,0,0.00425196,"classifications. The Hallmarks of Cancer depicts a set of interrelated biological factors and behaviours that enable cancer to thrive in the body. Introduced by Weinberg and Hanahan (2000), it has been widely used in biomedical NLP, including as part of HOC Document Train Dev Test Total 1,303 183 366 1,852 EXP Sentence Document Sentence 12,279 1,775 3,410 17,464 2,555 384 722 3,661 25,307 3,770 7,100 36,177 Table 2: Summary statistics of the Hallmarks of Cancer (HOC) and the Chemical Exposure Assessment (EXP) datasets. The model follows the convolutional neural network (CNN) model proposed by Kim (2014). An implementation of this algorithm on HOC and EXP has been published by Baker and Korhonen (2017); we use this implementation in our experiment. The input to the model is an initial word embedding layer that maps input texts into matrices, which is then followed by convolutions of different filter sizes, 1-max pooling, and finally a fully-connected layer leading to an output Softmax layer predicting labels for text. Model hyperparameters and the training setup are summarized in Table 3. Parameters Values Vector dimension Filter sizes Number of filters Dropout probability Minibatch size Inpu"
W19-5014,P06-1044,1,0.754334,"Missing"
W19-5014,C18-1205,0,0.0306855,"ing large corpora for (re-)training as the joint-learning models do. Among these methods, retrofitting (Faruqui et al., 2015) is widely used. Given any (pretrained) vector-space representations, the goal of retrofitting is to bring closer words which are connected via a relation (e.g. synonyms) in a given semantic network or lexical resource (i.e. linguistic constraints). For example, Yu et al. (2016, 2017) retrofit word vector spaces of MeSH terms by using additional linkage information from the UMNSRS hierarchy to improve the representations of biomedical concepts. Building on retrofitting, Lengerich et al. (2018) generalize retrofitting methods by explicitly modelling individual linguistic constraints that are commonly found in health and clinical-related lexicons (e.g. causal-relations between diseases and drugs). In theory, the joint-learning models could be as effective (or better) as those produced by finetuning distributional vectors. However, the performance of joint-learning models has not surpassed that of fine-tuning methods.2 Furthermore, the joint-learning objectives are usually model-specific and are tailored to a particular model, making it difficult to use them with other methods. In thi"
W19-5014,de-marneffe-etal-2006-generating,0,0.0710737,"Missing"
W19-5014,N10-1004,0,0.0201926,"on and relation annotations for complex events related to chemical– protein interaction in molecular biology. The goal of this task is to predict whether a given chemical– protein pair is related or not, and to then verify its corresponding relation type. There are five types of relations: Up-regulator, Down-regulator, Agonist, Antagonist, and Substrate. The corpus is provided in the Turku Event Extraction System (TEES) XML format and are installed with the Turku Extraction System (Bj¨orne, 2014). It is parsed with the the BLLIP parser (Charniak and Johnson, 2005) with the McClosky bio-model (Mcclosky, 2010), followed by conversion of the constituency parses into dependency parses using the Stanford Tools (MacCartney et al., 2006). Table 4 summarizes key statistics for the dataset. Train Dev Test Total initial word embedding layer that maps input texts into matrices, followed by convolutions of different filter sizes and 1-max pooling, and finally a fully connected layer, leading to an output Softmax layer for predicting labels. Performance is evaluated using the standard precision, recall, and F1 -score metrics of the labels in the model. Classification is performed as multilabel classification"
W19-5014,W13-2008,0,0.272906,"ion to update word vectors. It can be applied to any pretrained word embedding vectors. The intuition behind retrofitting is to encourage the retrofitted vectors to be similar to the vectors of related word 125 Proceedings of the BioNLP 2019 workshop, pages 125–134 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics types and similar to their original distributional representations. Using a standard “off-the-shelf” retrofitting algorithm, we apply the idea of retrofitting to verb clusters to two sets of widely-used pretrained embedding vectors in BioNLP (those by Pyysalo et al. (2013a) and by Chiu et al. (2016)) to obtain improved embeddings. We show that by doing nothing more than using this simple approach, we achieve state-of-the-art results on two text classification tasks (both tasks evaluated on document and sentence level classification), and a relation extraction task. We make our retrofitted embeddings freely available to the BioNLP community along with our code.1 The main contribution of this work is to be the first of its kind to apply verb-based retrofitting in the biomedical domain. Retrofitting has thus far only been applied for aligning vectors to Medical S"
W19-5014,P14-2089,0,0.0292684,"ed evaluation. We end with a discussion of the evaluation results. 2 Related work Lexical resources can be used to enrich representation models by providing them other sources of linguistics information beyond the distributional statistics obtained from corpora. In recent literature, various methods to leverage knowledge available in human- and automatically-constructed lexical resources have been proposed. One such method involves modifying the objectives in the original representation learning procedures so that they can jointly learn both distributional and lexical information—for example, Yu and Dredze (2014) modify the CBOW objective function by introducing semantic constraints as obtained from the paraphrase database (Ganitkevitch et al., 2013) to train word representations which focus on word similarity over word relatedness. 1 Our retrofitted embeddings and code are released under an open license and can be found here: https://github.com/cambridgeltl/ retrofitted-bio-embeddings Another class of methods incorporates lexical information into the vector representations as a post-processing procedure. The method fine-tunes the pretrained word vectors to satisfy linguistic constraints from the exte"
W19-5014,W16-6106,0,0.120032,"016)) to obtain improved embeddings. We show that by doing nothing more than using this simple approach, we achieve state-of-the-art results on two text classification tasks (both tasks evaluated on document and sentence level classification), and a relation extraction task. We make our retrofitted embeddings freely available to the BioNLP community along with our code.1 The main contribution of this work is to be the first of its kind to apply verb-based retrofitting in the biomedical domain. Retrofitting has thus far only been applied for aligning vectors to Medical Subject Headings (MeSH) (Yu et al., 2016), and been validated only in an extrinsic setting. We show that with very little effort, we can achieve state-of-the art results on various downstream tasks in a range of biomedical subdomains. This paper will first describe relevant work on retrofitting to lexical resources in BioNLP; we then briefly give an overview of two verb cluster and lexicons that we use in our methodology, and then our task-based evaluation. We end with a discussion of the evaluation results. 2 Related work Lexical resources can be used to enrich representation models by providing them other sources of linguistics inf"
Y09-1003,D07-1018,0,0.018126,"s being assigned to one class only. Yet the investigation showed that polysemy has a considerable impact on verb classification: optimal results were obtained with when clustering was evaluated against the polysemous gold standard, not the monosemous version of it which assumed the predominant sense according to WordNet. Clearly polysemy is an issue that needs to be dealt with, and this amounts to both extending gold standards to capture non-predominant senses as well as finding a suitable ML method. Recently a multi-label classification method was used for supervised adjective classification Boleda et al. (2007) which might yield useful results also with verbs. Also methods for modelling the overlap between lexical categories might be of use. 4.4 Other languages and domains Most work on verb classification has been conducted on English. Considerable research has also been done on German (Schulte im Walde, 2006), but only small scale experiments exist on other languages, e.g. Chinese, Italian (Merlo et al., 2002), Spanish (Ferrer, 2004) and Japanese (Oishi and Matsumoto, 1997). Evaluating the applicability of classification techniques to several languages is critical for both theoretical and practical"
Y09-1003,W02-1016,0,0.0318477,"Missing"
Y09-1003,P07-1028,0,0.0212407,"they used a fully unsupervised approach to both verb clustering and SP acquisition. This suggests that NLP and ML techniques have now developed to the point where the use of deeper, theoretically-motivated features is becoming feasible. Yet high accuracy SP acquisition from undisambiguated corpus data is still an unmet challenge and is especially complex in the context of verb classification where SP models are needed for specific syntactic slots for which the data may be sparse. Recently a number of techniques have been proposed which may offer ideas for further improvement of the approach (Erk, 2007; Bergsma et al., 2008; Schulte im Walde et al., 2008). The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation. However, the main semantic features in manual classification are actually diathesis alternations. Some studies have attempted automatic alternation detection using WordNet classes as SP models (Lapata, 1999; McCarthy, 2001), but no recent large-scale work has been conducted, and no attepts have been made to detect alternations in a fully unsupervised fashion. The time may now be ripe"
Y09-1003,P04-2007,0,0.0142706,"re non-predominant senses as well as finding a suitable ML method. Recently a multi-label classification method was used for supervised adjective classification Boleda et al. (2007) which might yield useful results also with verbs. Also methods for modelling the overlap between lexical categories might be of use. 4.4 Other languages and domains Most work on verb classification has been conducted on English. Considerable research has also been done on German (Schulte im Walde, 2006), but only small scale experiments exist on other languages, e.g. Chinese, Italian (Merlo et al., 2002), Spanish (Ferrer, 2004) and Japanese (Oishi and Matsumoto, 1997). Evaluating the applicability of classification techniques to several languages is critical for both theoretical and practical reasons; for 1) improving the accuracy, scalability and robustness of the techniques mainly developed for English or German, 2) advancing work in other languages, 3) gaining a better understanding of the language-specific / cross-linguistic components of lexical information (e.g. the extent to which the features used for English or German are also valid for other languages), and 4) in a long term, improving the performance of s"
Y09-1003,C08-1057,1,0.353633,"proved very time-consuming. Class-based differences are typically manifested in differences in the statistics over usages of syntactic-semantic features. This statistical information is difficult collect by hand as it is highly domain-sensitive, i.e. it varies with predominant word senses, which change across corpora and domains. In recent years, automatic induction of verb classes from corpus data has become increasingly popular (Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; ´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). Li and Brew, 2008; Korhonen et al., 2008; O This work is important as it opens up the opportunity of learning and tuning classifications for the application and domain in question. Automatic classification is not only cost-effective but it also gathers the important statistical information as side effect of the acquisition process and can easily be applied to new domains and usage patterns provided relevant corpus data is available. To date, a variety of approaches have been proposed for verb classification and applied to general English and other languages. Both supervised and unsupervised machine learning (ML) methods have been us"
Y09-1003,P03-1009,1,0.808615,"Polysemy Polysemy is frequent in language. In particular, many high frequency verbs have several senses and can therefore be members of several classes. Most work on automatic classification has bypassed this issue by assuming a single class for each verb – usually the one corresponding to its predominating (the most frequent sense) in language according to e.g. WordNet. This is not only unrealistic thinking of real-world application of verb classes but also the predominating sense is not static but varies across domains and sub-languages. Few attempts have been made to address this problem. Korhonen et al. (2003) performed a clustering experiment with highly polysemous verbs. They constructed a polysemous gold standard for c. 200 English verbs and examined whether a soft clustering method (Information Bottleneck) could be used to assign these verbs to several classes. The clustering turned out hard, with the majority of verbs being assigned to one class only. Yet the investigation showed that polysemy has a considerable impact on verb classification: optimal results were obtained with when clustering was evaluated against the polysemous gold standard, not the monosemous version of it which assumed the"
Y09-1003,P99-1051,0,0.0197764,"eeded for specific syntactic slots for which the data may be sparse. Recently a number of techniques have been proposed which may offer ideas for further improvement of the approach (Erk, 2007; Bergsma et al., 2008; Schulte im Walde et al., 2008). The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation. However, the main semantic features in manual classification are actually diathesis alternations. Some studies have attempted automatic alternation detection using WordNet classes as SP models (Lapata, 1999; McCarthy, 2001), but no recent large-scale work has been conducted, and no attepts have been made to detect alternations in a fully unsupervised fashion. The time may now be ripe for this research and its integration in verb classification. The development of an optimal approach will require a good understanding of the linguistic basis of verb classification as well as adequate NLP and ML expertise. The approach will need to be general enough to cover most types of alternations, efficient enough for a large scale use and resistant to the problems of sparse data. 4.2 Classification In section"
Y09-1003,P08-1050,0,0.200957,"mbers of words has proved very time-consuming. Class-based differences are typically manifested in differences in the statistics over usages of syntactic-semantic features. This statistical information is difficult collect by hand as it is highly domain-sensitive, i.e. it varies with predominant word senses, which change across corpora and domains. In recent years, automatic induction of verb classes from corpus data has become increasingly popular (Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; ´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). Li and Brew, 2008; Korhonen et al., 2008; O This work is important as it opens up the opportunity of learning and tuning classifications for the application and domain in question. Automatic classification is not only cost-effective but it also gathers the important statistical information as side effect of the acquisition process and can easily be applied to new domains and usage patterns provided relevant corpus data is available. To date, a variety of approaches have been proposed for verb classification and applied to general English and other languages. Both supervised and unsupervised machine learning (M"
Y09-1003,J01-3003,0,0.0741789,"ecause no fully accurate or comprehensive lexical classification is available. There is no such resource because manual classification of large numbers of words has proved very time-consuming. Class-based differences are typically manifested in differences in the statistics over usages of syntactic-semantic features. This statistical information is difficult collect by hand as it is highly domain-sensitive, i.e. it varies with predominant word senses, which change across corpora and domains. In recent years, automatic induction of verb classes from corpus data has become increasingly popular (Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; ´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). Li and Brew, 2008; Korhonen et al., 2008; O This work is important as it opens up the opportunity of learning and tuning classifications for the application and domain in question. Automatic classification is not only cost-effective but it also gathers the important statistical information as side effect of the acquisition process and can easily be applied to new domains and usage patterns provided relevant corpus data is available. To date, a variety of approaches have been"
Y09-1003,P02-1027,0,0.0167473,"ending gold standards to capture non-predominant senses as well as finding a suitable ML method. Recently a multi-label classification method was used for supervised adjective classification Boleda et al. (2007) which might yield useful results also with verbs. Also methods for modelling the overlap between lexical categories might be of use. 4.4 Other languages and domains Most work on verb classification has been conducted on English. Considerable research has also been done on German (Schulte im Walde, 2006), but only small scale experiments exist on other languages, e.g. Chinese, Italian (Merlo et al., 2002), Spanish (Ferrer, 2004) and Japanese (Oishi and Matsumoto, 1997). Evaluating the applicability of classification techniques to several languages is critical for both theoretical and practical reasons; for 1) improving the accuracy, scalability and robustness of the techniques mainly developed for English or German, 2) advancing work in other languages, 3) gaining a better understanding of the language-specific / cross-linguistic components of lexical information (e.g. the extent to which the features used for English or German are also valid for other languages), and 4) in a long term, improv"
Y09-1003,C08-1082,0,0.0683322,"Missing"
Y09-1003,J06-2001,0,0.234842,"Missing"
Y09-1003,P08-1057,0,0.0189266,"both verb clustering and SP acquisition. This suggests that NLP and ML techniques have now developed to the point where the use of deeper, theoretically-motivated features is becoming feasible. Yet high accuracy SP acquisition from undisambiguated corpus data is still an unmet challenge and is especially complex in the context of verb classification where SP models are needed for specific syntactic slots for which the data may be sparse. Recently a number of techniques have been proposed which may offer ideas for further improvement of the approach (Erk, 2007; Bergsma et al., 2008; Schulte im Walde et al., 2008). The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation. However, the main semantic features in manual classification are actually diathesis alternations. Some studies have attempted automatic alternation detection using WordNet classes as SP models (Lapata, 1999; McCarthy, 2001), but no recent large-scale work has been conducted, and no attepts have been made to detect alternations in a fully unsupervised fashion. The time may now be ripe for this research and its integration in verb classif"
Y09-1003,W03-0410,0,0.208634,"Missing"
Y09-1003,D09-1067,1,0.119987,"syntactic features, exploiting the fact that similar alternations tend to result in similar syntactic behaviour. The syntactic features have been shallow syntactic slots (e.g. NPs preceding or following the verb) extracted using a lemmatizer or a chunker, or verb SCFs extracted using a chunker or a parser. These both feature types have been refined with information about prepositional preferences (PPs) of verbs. Joanis et al. (2008) have reported better results using syntactic slots, while several others have obtained good results using SCF s, e.g. (Schulte im Walde, 2006; Li and Brew, 2008; Sun and Korhonen, 2009). While SCF s correspond better (than syntactic slots) with the features used in manual work, optimal results have required including in SCFs also additional information about adjuncts (not only arguments) of verbs (Sun et al., 2008) which are typically not used in manual classification. Recent research has also experimented with replacing or supplementing SCFs with information about basic lexical context (co-occurrences (COs)) of verbs, or lexical preferences (LPs) in specific grammatical relations (GRs) associated with verbs in parsed data (for example, the type and frequency of prepositions"
Y09-1003,W09-0210,1,0.794999,"ssification of large numbers of words has proved very time-consuming. Class-based differences are typically manifested in differences in the statistics over usages of syntactic-semantic features. This statistical information is difficult collect by hand as it is highly domain-sensitive, i.e. it varies with predominant word senses, which change across corpora and domains. In recent years, automatic induction of verb classes from corpus data has become increasingly popular (Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; ´ S´eaghdha and Copestake, 2008; Vlachos et al., 2009). Li and Brew, 2008; Korhonen et al., 2008; O This work is important as it opens up the opportunity of learning and tuning classifications for the application and domain in question. Automatic classification is not only cost-effective but it also gathers the important statistical information as side effect of the acquisition process and can easily be applied to new domains and usage patterns provided relevant corpus data is available. To date, a variety of approaches have been proposed for verb classification and applied to general English and other languages. Both supervised and unsupervised"
Y09-1003,P08-1063,0,0.0338416,"Missing"
Y09-1003,D08-1007,0,\N,Missing
