1993.eamt-1.2,J87-3005,0,\N,Missing
1993.eamt-1.2,C92-2083,0,\N,Missing
1993.eamt-1.2,A88-1012,0,\N,Missing
1993.eamt-1.2,C88-2098,0,\N,Missing
1993.eamt-1.2,C92-4216,0,\N,Missing
1993.eamt-1.2,T87-1005,0,\N,Missing
1993.eamt-1.2,C90-2067,1,\N,Missing
1993.eamt-1.2,P85-1037,0,\N,Missing
1993.eamt-1.2,T87-1004,0,\N,Missing
1993.eamt-1.2,P90-1033,0,\N,Missing
1993.eamt-1.2,J92-1001,0,\N,Missing
1993.eamt-1.2,P89-1012,0,\N,Missing
1993.eamt-1.2,P86-1018,0,\N,Missing
1993.eamt-1.2,P84-1036,0,\N,Missing
1993.eamt-1.2,J87-3003,0,\N,Missing
2020.iwltp-1.10,P14-5010,0,0.0093506,"ily in their conventions for specifying input and output. Table 1 gives an overview of the synchronous protocol APIs for several NLPaaS services, including the CLARIN-D (Hinrichs et al., 2010) and CLARIN-PL (Piasecki, 2014) services from the European CLARIN project; ETRI NLP API Korean NLP5 , developed and maintained by the Electronics and Telecommunications Research Institute (ETRI); Google Natural Language API, a commercial service provided by Google; and PubDictionaries (Kim et al., 2019), a service provided by the Database Center for Life Science (DBCLS). We also include Stanford CoreNLP (Manning et al., 2014), which is one of the most widely used NLP toolsets that is also implemented as a NLPaaS web service. Figure 3: General asynchronous protocol with status checking POST request) and completed by a response from the server to the client. Synchronous protocols block activity on the client as well as the server while the request is being processed; therefore, to avoid resource starvation in unexpected situations (network problems, errors, etc.), the request is typically subjected to a conservative timeout.4 Therefore, when requests are expected to take an extended amount of time, e.g. in order to"
2020.lrec-1.855,L18-1726,1,0.819929,"Missing"
2020.lrec-1.855,L18-1327,1,0.92361,"ntities. In addition, several infrastructures supporting biomedical text mining have been developed, including U-Compare (Kano et al., 2008) and Argo (Rak et al., 2012). The General Architecture for Text Engineering (GATE) (Cunningham et al., 2011), a broader-based framework for text mining, also includes some tools for handling biomedical texts. These tools and infrastructures are typically self-contained and focused on lexical, syntactic and shallow semantic (namedentity) approaches. More recently, the LAPPS Grid (Ide et al., 2014) has been augmented to support mining biomedical literature (Ide et al., 2018), as well as sophisticated interactive annotation and machine learning tools for domain adaptation to support mining literature in the life sciences. 3. In order to fully support the complete cycle of literaturebased discovery and symbiotic improvement in language resources in the genomics domain with an existing vast body of work, we need large scale infrastructures. The following subsections discuss our Gene Ontology Semantic Tagger (GOST) (section 3.1.) and its integration with both our new Buster NLP pipeline (section 3.3.) and the LAPPS Grid (section 3.2.) which was previously developed b"
2020.lrec-1.855,I08-2122,0,0.0508389,"portance of the biomedical text mining, a variety of NLP tools have been developed and modified to support it. Among the main tools and corpora developed for such purposes include the Genia tagger and corpus (Tsuruoka et al., 2005; Thompson et al., 2017), GOST tagger (El-Haj et al., 2018), and Termine7 . A related biomedical annotation tool is the Penn BioTagger8 (Jin et al., 2006), which is capable of tagging gene entities, genomic variations entities and malignancy type entities. In addition, several infrastructures supporting biomedical text mining have been developed, including U-Compare (Kano et al., 2008) and Argo (Rak et al., 2012). The General Architecture for Text Engineering (GATE) (Cunningham et al., 2011), a broader-based framework for text mining, also includes some tools for handling biomedical texts. These tools and infrastructures are typically self-contained and focused on lexical, syntactic and shallow semantic (namedentity) approaches. More recently, the LAPPS Grid (Ide et al., 2014) has been augmented to support mining biomedical literature (Ide et al., 2018), as well as sophisticated interactive annotation and machine learning tools for domain adaptation to support mining litera"
2020.lrec-1.855,W12-2425,0,0.0356477,"atural Language Processing (NLP) field. GOST output can then be processed by tools that may generate additional annotation layers (“views&quot; in LAPPS Grid terminology), with or without using GOST’s semantic annotations. The LAPPS Grid also provides a Solr-based query engine for PubMed data that is augmented with ranking rules whose weights can be tweaked as desired by the user, results from which can be used as input to GOST. A major advantage of incorporating GOST into the LAPPS Grid is the access to PubMed data provided by the newlyestablished incorporation of the facilities of PubAnnotation (Kim and Wang, 2012) into the Grid. PubAnnotation not only provides access to all PubMed texts, but also, crucially, serves as a repository of annotations that are linked together by common reference (via standoff annotation) to the canonical texts. A common annotation repository enables combining GOST’s semantic annotations with annotations generated by other software and/or by human annotators, which in turn can yield insight into linguistic and semantic properties of biomedical terminology and improve our ability to extract meaningful information from biomedical publications. PubAnnotation also provides an ann"
2020.lrec-1.855,W04-1804,0,0.103141,"or biomedical text mining, 3) a large annotated corpus consisting of open access PubMed Central papers, 4) open platforms to support research reproducibility, and 5) supporting literature based discovery with a novel combination of NLP and CL methods. 2. Related Work Analysing biomedical data using Natural Language Processing (NLP) and text mining requires a significant amount of domain knowledge (Tan and Lambrix, 2009). Such knowledge is usually found in domain specific ontologies such as the Gene ontology resource1 which contains important information related to gene products and functions (Kumar et al., 2004). Over many years, NLP techniques have been widely applied to biomedical text mining to facilitate large-scale information extraction and knowledge discovery from the rapidly increasing body of biomedical literature. Since the begin1 6921 http://www.geneontology.org ning of biomedical language processing in the late 1990s, the field continued to receive great attention with specialised events and workshops focusing on biomedical NLP, such as the BioNLP Workshop series. Current biomedical libraries such as MEDLINE2 by the US National Library of Medicine (NLM)3 provide searchable databases that"
2020.lrec-1.855,W09-1307,0,0.0349332,"uction observed in the 1980s and 1990s. Our specific contributions in this paper are as follows: 1) entity identification and semantic linking in the genomics domain, 2) a novel open infrastructure for biomedical text mining, 3) a large annotated corpus consisting of open access PubMed Central papers, 4) open platforms to support research reproducibility, and 5) supporting literature based discovery with a novel combination of NLP and CL methods. 2. Related Work Analysing biomedical data using Natural Language Processing (NLP) and text mining requires a significant amount of domain knowledge (Tan and Lambrix, 2009). Such knowledge is usually found in domain specific ontologies such as the Gene ontology resource1 which contains important information related to gene products and functions (Kumar et al., 2004). Over many years, NLP techniques have been widely applied to biomedical text mining to facilitate large-scale information extraction and knowledge discovery from the rapidly increasing body of biomedical literature. Since the begin1 6921 http://www.geneontology.org ning of biomedical language processing in the late 1990s, the field continued to receive great attention with specialised events and work"
2020.lrec-1.893,P10-4005,0,0.0417135,"chitecture for Text Engineering (GATE) (Cunningham et al., 2013) have been served as well-established and popular tool-chaining platforms for researchers and NLP developers. Although GATE focuses primarily on textual data, UIMA provides an extremely general model of type systems and annotations that can be applied to multimedia source data. However, there is a steep learning curve supporting UIMA’s generality, due in large part to its tight binding to XML syntax and the Java programming language. More recently, web-based workflow engines such as the LAPPS Grid (Ide et al., 2014) and WebLicht (Hinrichs et al., 2010) have been developed that provide user-friendly web interfaces for chaining NLP tools. These platforms not only offer tool repositories containing state-of-the-art NLP tools for annotating textual data at a variety of linguistic levels (e.g., CoreNLP (Manning et al., 2014), OpenNLP (OpenNLP, 2017), UDPipe (Straka and Strakov´a, 2017)), but also provide open source software development kits (SDKs) for tool developers in order to promote adoption. The LAPPS Grid and WebLicht both provide for chaining tools from different developers, which use a variety of I/O formats, by virtue of underlying dat"
2020.lrec-1.893,ide-etal-2014-language,1,0.813,"et al., 2009) and the General Architecture for Text Engineering (GATE) (Cunningham et al., 2013) have been served as well-established and popular tool-chaining platforms for researchers and NLP developers. Although GATE focuses primarily on textual data, UIMA provides an extremely general model of type systems and annotations that can be applied to multimedia source data. However, there is a steep learning curve supporting UIMA’s generality, due in large part to its tight binding to XML syntax and the Java programming language. More recently, web-based workflow engines such as the LAPPS Grid (Ide et al., 2014) and WebLicht (Hinrichs et al., 2010) have been developed that provide user-friendly web interfaces for chaining NLP tools. These platforms not only offer tool repositories containing state-of-the-art NLP tools for annotating textual data at a variety of linguistic levels (e.g., CoreNLP (Manning et al., 2014), OpenNLP (OpenNLP, 2017), UDPipe (Straka and Strakov´a, 2017)), but also provide open source software development kits (SDKs) for tool developers in order to promote adoption. The LAPPS Grid and WebLicht both provide for chaining tools from different developers, which use a variety of I/O"
2020.lrec-1.893,W12-2425,0,0.0198493,"Brat. among those tools. The LAPPS Grid uses LIF (Verhagen et al., 2015), a JSON-LD serialization, as its interchange format; while WebLicht uses its XML-based Text Corpus Format (TCF) (Heid et al., 2010). Additionally, the LAPPS Grid defines a linked data vocabulary that ensures semantic interoperability (Ide et al., 2015). Beyond in-platform interoperability, the LAPPS Grid has established multi-platform interoperability between LAPPS Grid and two CLARIN platforms (Hinrichs et al., 2018) as well as several other platforms (e.g., DKPro (Eckart de Castilho and Gurevych, 2014), PubAnnotation (Kim and Wang, 2012), and INCEpTION (Klie et al., 2018)). Figure 1 shows a visualization of named entity annotations in the LAPPS Grid, using Brat (Stenetorp et al., 2012). All annotations are represented in the LIF format and linked either to offsets within read-only primary data or to other annotation layers. Within the LIF document containing the annotations, each annotation references a name (e.g., PERSON), possibly coupled with additional attributes, that links to a full definition in the LAPPS Grid Web Service Exchange Vocabulary (WSEV)5 . Alternative names used within specific tools are mapped to the WSEV"
2020.lrec-1.893,C18-2002,0,0.0631695,"Missing"
2020.lrec-1.893,P14-5010,0,0.00570971,"stems and annotations that can be applied to multimedia source data. However, there is a steep learning curve supporting UIMA’s generality, due in large part to its tight binding to XML syntax and the Java programming language. More recently, web-based workflow engines such as the LAPPS Grid (Ide et al., 2014) and WebLicht (Hinrichs et al., 2010) have been developed that provide user-friendly web interfaces for chaining NLP tools. These platforms not only offer tool repositories containing state-of-the-art NLP tools for annotating textual data at a variety of linguistic levels (e.g., CoreNLP (Manning et al., 2014), OpenNLP (OpenNLP, 2017), UDPipe (Straka and Strakov´a, 2017)), but also provide open source software development kits (SDKs) for tool developers in order to promote adoption. The LAPPS Grid and WebLicht both provide for chaining tools from different developers, which use a variety of I/O formats, by virtue of underlying data interchange formats that impose a common I/O format 7230 Figure 1: Named entity annotations in the LAPPS Grid, generated by LINDAT/CLARIN’s NameTag 4 tool and visualized with Brat. among those tools. The LAPPS Grid uses LIF (Verhagen et al., 2015), a JSON-LD serializatio"
2020.lrec-1.893,W19-2512,1,0.777297,"resent the collections in their client software (e.g., in what order, in which orientation, on what zoom level, etc). Unfortunately, however, the IIIF does not provide detailed specifications for the semantics of the content of the textual annotations. In principle, one could design an independent, adequate data model for textual annotation that can be carried out on IIIF, since the IIF specification is built upon the Open Annotation model and linked-data conformity, but this would involve significant additional effort. MMIF is an interoperable representation format that is used in the CLAMS (Rim et al., 2019) project. CLAMS is a platform of computational analysis tools designed for digital libraries and archives who have to deal with not just textual data, but also audiovisual time-based data. To handle the complexity of multimodal content and semantics of the audiovisual data sources, MMIF is specifically designed to enable alignment of annotations on different modes of the primary data sources. 6 7 https://iiif.io/ https://iiif.io/api/presentation/3.0/ Specifically, multimodal annotations in MMIF are first categorized by the anchor type on which the annotation is placed. That is, an annotation c"
2020.lrec-1.893,schmidt-etal-2008-exchange,0,0.0890206,"not only NLP, but also computer vision (CV), and speech technologies processing audio and video data. The machine learning approach to solving problems is data-driven, and most state-of-the-art applications are based on supervised algorithms which rely on large sets of training data. To ensure the high quality of datasets containing rich multimodal annotations, the CL community has had to move beyond text-only annotation practices, and has tried to establish a common format for multimodal annotations, particularly with regard to annotating speech in audio and gestures in video. For example, (Schmidt et al., 2008) outline a diversity of annotation applications and formats, as well as the community effort to develop an interoperable format that carries complicated, layered multimodal annotation. As a result, 7231 Figure 2: A primary data text collection can be created from an image and can then be input to downstream NLP components. UIMA and Component MetaData Infrastructure (CMDI) (Broeder et al., 2012) have been widely adopted frameworks that provide interoperable multimodal information exchange between computational analysis tools, and for metadata repositories for discoverability, respectively. More"
2020.lrec-1.893,E12-2021,0,0.107914,"Missing"
2020.lrec-1.893,K17-3009,0,0.0570048,"Missing"
2020.nlpcovid19-2.28,W19-4021,1,0.860469,"Missing"
2020.nlpcovid19-2.28,P10-4005,0,0.035952,"re rendered mutually interoperable via transduction to the LAPPS Grid Interchange Format (LIF) (Verhagen et al., 2015) and the Web Service Exchange Vocabulary (WSEV) (Ide et al., 2014b), both designed to capture fundamental properties of existing annotation models in order to serve as a common pivot among them. The LAPPS Grid also provides interoperable, two-way access to the PubAnnotation annotation repository (Kim and Wang, 2012), the INCEpTION machine learning-assisted annotation platform (Klie et al., 2018), along with multi-lingual NLP tools and data in two EU-CLARIN platforms: WebLicht (Hinrichs et al., 2010) and LINDAT/CLARIN (Straka et al., 2016). All LAPPS Grid components are released under the Apache 2.0 open source license. 4 AskMe Overview The AskMe application is an open-source, Solrbased microservice architecture running in a Docker Swarm on Jetstream (Stewart et al., 2015; Towns et al., 2014), an NSF-funded compute cluster. Originally developed to allow for search and retrieval from the PubMed database, it now allows for search of the full set of CORD-19 documents, which are updated nightly. Figure 2: AskMe Query Results In response to a natural language query, AskMe applies several scori"
2020.nlpcovid19-2.28,ide-etal-2014-language,1,0.876929,"Missing"
2020.nlpcovid19-2.28,W14-5204,1,0.868638,"ed to other query engines available to search covid-related publications. 1 Introduction The onset of the coronavirus pandemic has prompted concerted efforts within the field of natural language processing (NLP) to enable researchers and practitioners to easily access and mine the growing body of literature concerned with the virus. Between February and May 2020, the number of scientific papers published on COVID19 research increased from 29,000 to more than 138,000; this number is expected to exceed one million by the end of 2020. In a recent project, the Language Applications (LAPPS) Grid1 (Ide et al., 2014a) was augmented to support the mining of scientific publications2 (Ide et al., 2018). The results of that effort have now been repurposed to focus on Covid-19 literature, including modification of the LAPPS Grid “AskMe” query and retrieval engine to access nightly updates of the CORD-19 dataset3 (Wang et al., 2020) available from the Allen Institute for AI. In this paper, we describe the AskMe system and discuss its functionality as compared to other query engines available to search covid-related publications. Because the AskMe engine is deployed 1 https://galaxy.lappsgrid.org Funded by U.S."
2020.nlpcovid19-2.28,L18-1327,1,0.850229,"tion The onset of the coronavirus pandemic has prompted concerted efforts within the field of natural language processing (NLP) to enable researchers and practitioners to easily access and mine the growing body of literature concerned with the virus. Between February and May 2020, the number of scientific papers published on COVID19 research increased from 29,000 to more than 138,000; this number is expected to exceed one million by the end of 2020. In a recent project, the Language Applications (LAPPS) Grid1 (Ide et al., 2014a) was augmented to support the mining of scientific publications2 (Ide et al., 2018). The results of that effort have now been repurposed to focus on Covid-19 literature, including modification of the LAPPS Grid “AskMe” query and retrieval engine to access nightly updates of the CORD-19 dataset3 (Wang et al., 2020) available from the Allen Institute for AI. In this paper, we describe the AskMe system and discuss its functionality as compared to other query engines available to search covid-related publications. Because the AskMe engine is deployed 1 https://galaxy.lappsgrid.org Funded by U.S. National Science Foundation grant NSFEAGER 181123. 3 https://www.kaggle.com/allen-in"
2020.nlpcovid19-2.28,W12-2425,0,0.0245035,"opment engine (Afgan et al., 2018), and may also be accessed directly via SOAP calls or programmatically through Java and Python interfaces. All tools and resources in the LAPPS Grid are rendered mutually interoperable via transduction to the LAPPS Grid Interchange Format (LIF) (Verhagen et al., 2015) and the Web Service Exchange Vocabulary (WSEV) (Ide et al., 2014b), both designed to capture fundamental properties of existing annotation models in order to serve as a common pivot among them. The LAPPS Grid also provides interoperable, two-way access to the PubAnnotation annotation repository (Kim and Wang, 2012), the INCEpTION machine learning-assisted annotation platform (Klie et al., 2018), along with multi-lingual NLP tools and data in two EU-CLARIN platforms: WebLicht (Hinrichs et al., 2010) and LINDAT/CLARIN (Straka et al., 2016). All LAPPS Grid components are released under the Apache 2.0 open source license. 4 AskMe Overview The AskMe application is an open-source, Solrbased microservice architecture running in a Docker Swarm on Jetstream (Stewart et al., 2015; Towns et al., 2014), an NSF-funded compute cluster. Originally developed to allow for search and retrieval from the PubMed database, i"
2020.nlpcovid19-2.28,C18-2002,0,0.0405808,"Missing"
2020.nlpcovid19-2.28,L16-1680,0,0.0227748,"Missing"
2020.nlpcovid19-2.28,2020.nlpcovid19-acl.1,0,0.0282017,"ned with the virus. Between February and May 2020, the number of scientific papers published on COVID19 research increased from 29,000 to more than 138,000; this number is expected to exceed one million by the end of 2020. In a recent project, the Language Applications (LAPPS) Grid1 (Ide et al., 2014a) was augmented to support the mining of scientific publications2 (Ide et al., 2018). The results of that effort have now been repurposed to focus on Covid-19 literature, including modification of the LAPPS Grid “AskMe” query and retrieval engine to access nightly updates of the CORD-19 dataset3 (Wang et al., 2020) available from the Allen Institute for AI. In this paper, we describe the AskMe system and discuss its functionality as compared to other query engines available to search covid-related publications. Because the AskMe engine is deployed 1 https://galaxy.lappsgrid.org Funded by U.S. National Science Foundation grant NSFEAGER 181123. 3 https://www.kaggle.com/allen-institute-for-ai/CORD19-research-challenge/data 2 primarily as a front end to the LAPPS Grid, one of its most salient features is the ability to further process query results with the large array of NLP tools available in the Grid. Th"
C00-1031,P98-1011,0,0.122549,"Missing"
C00-1031,P98-1044,1,0.949232,"oach that assumes that all anaphors can be resolved intra-unit; Linear-1 models an approach that corresponds roughly to centering (Grosz et al., 1995). Linear-k is consistent with the assumptions that underlie most current anaphora resolution systems, which look back k units in order to resolve an anaphor. Discourse-VT-k models. In this class of models, LPAs include all the referential expressions found in the discourse unit under scrutiny and the k discourse units that hierarchically precede it. The units that hierarchically precede a given unit are determined according to Veins Theory (VT) (Cristea et al., 1998), which is described briefly below. 2.2 Veins Theory VT extends and formalizes the relation between discourse structure and reference proposed by Fox (1987). It identifies ”veins”, i.e., chains of elementary discourse units, over discourse structure trees that are built according to the requirements put forth in Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). One of the conjectures of VT is that the vein expression of an elementary discourse unit provides a coherent ”abstract” of the discourse fragment that contains that unit. As an internally coherent discourse fragment, most of"
C00-1031,W98-1119,0,0.0489352,"Missing"
C00-1031,J86-3001,0,0.152384,"Missing"
C00-1031,J95-2003,0,0.699716,"tly (Hobbs, 1978; Lappin and Leass, 1994; Mitkov,  On leave from the Faculty of Computer Science, University “Al. I. Cuza” of Iasi. 1997; Kameyama, 1997). In other cases, these modules are integrated by means of statistical (Ge et al., 1998) or uncertainty reasoning techniques (Mitkov, 1997). The fact that current anaphora resolution systems rely exclusively on the linear nature of texts in order to determine the LPA of an anaphor seems odd, given that several studies have claimed that there is a strong relation between discourse structure and reference (Sidner, 1981; Grosz and Sidner, 1986; Grosz et al., 1995; Fox, 1987; Vonk et al., 1992; Azzam et al., 1998; Hitzeman and Poesio, 1998). These studies claim, on the one hand, that the use of referents in naturally occurring texts imposes constraints on the interpretation of discourse; and, on the other, that the structure of discourse constrains the LPAs to which anaphors can be resolved. The oddness of the situation can be explained by the fact that both groups seem prima facie to be right. Empirical experiments studies that employ linear techniques for determining the LPAs of anaphors report recall and precision anaphora resolution results in the"
C00-1031,P98-1090,0,0.0384449,"Missing"
C00-1031,W97-1307,0,0.0321218,"ce University of Southern California Los Angeles, CA, USA marcu@isi.edu Valentin Tablan Department of Computer Science University of Sheffield United Kingdom v.tablan@sheffield.ac.uk Abstract We compare the potential of two classes of linear and hierarchical models of discourse to determine co-reference links and resolve anaphors. The comparison uses a corpus of thirty texts, which were manually annotated for co-reference and discourse structure. 1 Introduction Most current anaphora resolution systems implement a pipeline architecture with three modules (Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997). 1. A C OLLECT module determines a list of potential antecedents (LPA) for each anaphor (pronoun, definite noun, proper name, etc.) that have the potential to resolve it. 2. A F ILTER module eliminates referees incompatible with the anaphor from the LPA. 3. A P REFERENCE module determines the most likely antecedent on the basis of an ordering policy. In most cases, the C OLLECT module determines an LPA by enumerating all antecedents in a window of text that precedes the anaphor under scrutiny (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997; Ge et al., 1998). This window can"
C00-1031,J94-4002,0,0.0316058,"itute and Department of Computer Science University of Southern California Los Angeles, CA, USA marcu@isi.edu Valentin Tablan Department of Computer Science University of Sheffield United Kingdom v.tablan@sheffield.ac.uk Abstract We compare the potential of two classes of linear and hierarchical models of discourse to determine co-reference links and resolve anaphors. The comparison uses a corpus of thirty texts, which were manually annotated for co-reference and discourse structure. 1 Introduction Most current anaphora resolution systems implement a pipeline architecture with three modules (Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997). 1. A C OLLECT module determines a list of potential antecedents (LPA) for each anaphor (pronoun, definite noun, proper name, etc.) that have the potential to resolve it. 2. A F ILTER module eliminates referees incompatible with the anaphor from the LPA. 3. A P REFERENCE module determines the most likely antecedent on the basis of an ordering policy. In most cases, the C OLLECT module determines an LPA by enumerating all antecedents in a window of text that precedes the anaphor under scrutiny (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997; Ge"
C00-1031,W99-0307,1,0.81955,"inks of the REs in the other units, the full equivalence class can be determined. This is consistent with the distinction between ”direct” and ”indirect” references discussed by Cristea, et al.(1998). 3 The Experiment 3.1 Materials We used thirty newspaper texts whose lengths varied widely; the mean  is 408 words and the standard deviation  is 376. The texts were annotated manually for co-reference relations of identity (Hirschman and Chinchor, 1997). The co-reference relations define equivalence classes on the set of all marked referents in a text. The texts were also manually annotated by Marcu et al. (1999) with discourse structures built in the style of Mann and Thompson (1988). Each discourse analysis yielded an average of 52 elementary discourse units. See (Hirschman and Chinchor, 1997) and (Marcu et al., 1999) for details of the annotation processes. Figure 2: The RST analysis of the text in figure 1. The tree is represented using the conventions proposed by Mann and Thompson (1988). 3.2 Comparing potential to establish co-referential links 3.2.1 Method The annotations for co-reference relations and rhetorical structure trees for the thirty texts were fused, yielding representations that ref"
C00-1031,W97-1303,0,0.242773,"Computer Science University of Southern California Los Angeles, CA, USA marcu@isi.edu Valentin Tablan Department of Computer Science University of Sheffield United Kingdom v.tablan@sheffield.ac.uk Abstract We compare the potential of two classes of linear and hierarchical models of discourse to determine co-reference links and resolve anaphors. The comparison uses a corpus of thirty texts, which were manually annotated for co-reference and discourse structure. 1 Introduction Most current anaphora resolution systems implement a pipeline architecture with three modules (Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997). 1. A C OLLECT module determines a list of potential antecedents (LPA) for each anaphor (pronoun, definite noun, proper name, etc.) that have the potential to resolve it. 2. A F ILTER module eliminates referees incompatible with the anaphor from the LPA. 3. A P REFERENCE module determines the most likely antecedent on the basis of an ordering policy. In most cases, the C OLLECT module determines an LPA by enumerating all antecedents in a window of text that precedes the anaphor under scrutiny (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997; Ge et al., 1998)"
C00-1031,J81-4001,0,0.520573,"Missing"
C00-1031,C98-1044,1,\N,Missing
C00-1031,C98-1087,0,\N,Missing
C00-1031,C98-1011,0,\N,Missing
C04-1192,P92-1032,0,0.0380023,"g supported by available aligned wordnets for the languages in the corpus. The wordnets are aligned to the Princeton Wordnet, according to the principles established by EuroWordNet. The evaluation of the WSD system, implementing the method described herein showed very encouraging results. The same system used in a validation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as BalkaNet and EuroWordNet. 1 Introduction Word Sense Disambiguation (WSD) is wellknown as one of the more difficult problems in the field of natural language processing, as noted in (Gale et al, 1992; Kilgarriff, 1997; Ide and Véronis, 1998), and others. The difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given sense, lack of a standard (and possibly exhaustive) sense inventory, and the subjectivity of the human evaluation of such algorithms. To address the last problem, (Gale et al, 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. The lower bound should not drop below the baseline usage of"
C04-1192,W02-0808,1,0.791538,"Missing"
C04-1192,W03-0308,1,0.883846,"Missing"
C04-1192,tufis-etal-2004-word,1,0.87677,"Missing"
C04-1192,W99-0508,1,\N,Missing
C04-1192,J98-1001,1,\N,Missing
C04-1192,C02-1002,1,\N,Missing
C14-1054,N03-2003,0,0.0539806,"of genre-dependent models for a variety of natural language processing (NLP) tasks such as parsing (Ravi et al., 2008; McClosky et al., 2010; Roux et al., 2012), speech recognition (Iyer and Ostendorf, 1999), word sense disambiguation (Martinez and Agirre, 2000), and machine translation (Wang et al., 2012) has been found to significantly improve performance. The ability to match documents by genre has also become important for collecting data to train language models for spoken language understanding, given the difficulty of creating large repositories of transcribed spoken language corpora (Bulyko and Ostendorf, 2003; Sarikaya et al., 2005). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 565 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 565–576, Dublin, Ireland, August 23-29 2014. While the utility of document characterization by genre for empirical language analysis is widely acknowledged, there is relatively little agreement on methodology. In part, this stems from the dif"
C14-1054,A00-2018,0,0.162856,"makes it possible to locate new documents in the defined space, it would be possible to identify which MASC documents a new set of documents is most similar to. PCA scores could be computed on the four dimensions for corresponding features in the new documents. This approach could be used in any application where it is desirable to find similar documents, such as retrieval, language modeling, or domain adaptation. For example, in recent work on domain adaptation of parsers, McClosky et al. (2010) present a confusion matrix with six corpora to demonstrate how performance of a Charniak parser (Charniak, 2000) varies depending on which corpus it is trained on. They assume that a new target domain will be a mixture of their six source domains and build a simple regression (three features) to predict which of the six parsers will perform best on a new corpus. They subsequently state that an alternative approach could use a high-dimensional vector space to compare corpora. Inspired by this suggestion, we are currently developing a web service that will allow researchers to locate their corpora in the 4-dimensional space identified in this study, and to compute the values of their PCA scores. This woul"
C14-1054,N09-2044,0,0.0168024,"er range of traditional genres as well as new social media (email, blogs, twitter) and collectively generated fiction (ficlets). We take advantage of MASC’s rich set of validated annotations to include features that would not have been (easily) available at the time of Biber’s study, and reconsider the use of some features used in his work. Some work on genre classification contrasts with Biber’s approach, which assumes that documents fall discretely into distinct classes or clusters. Genre classification has been treated as a standalone task (Karlgren and Cutting, 1994; Kessler et al., 1997; Feldman et al., 2009; Stamatatos et al., 2000a; Santini, 2004), or combined with topic classification (Rauber and M¨uller-K¨ogler, 2001; Lee and Myaeng, 2002). All of these studies assume that documents fall discretely into distinct classes or clusters. These studies vary in their approach to determining the genre of text, either by using corpora with pre-defined classes (Karlgren and Cutting, 1994), manually refining pre-existing classes (Kessler et al., 1997), creating genre classes using annotators, or locating a priori classifications (e.g., web product reviews). The feature sets in genre studies have remaine"
C14-1054,P10-2013,1,0.832494,"aim that the dimensions of variation he identified arise from underlying constraints on usage. We find three components similar to his, and a new one he did not find, based on our use of Named Entity features. We find that genres that are separable on one component are often co-extensive on another. To quantify the distinctiveness of each of the genres relative to the others, we use a metric that has previously been used to measure separability of classes. 2 Related work and motivation Our work builds on Biber’s 1988 study, but differs in the corpus and features used. Biber’s corpus and MASC (Ide et al., 2010), the corpus used in our study, differ in source language (British English versus American English), time coverage (skewed towards a single year versus three decades), and the situations of use. Biber’s corpus was drawn from the Lancaster-Oslo-Bergen (LOB) Corpus of British English, consisting of works published in 1961, the London-Lund corpus of spoken English, consisting of 87 texts of British English from private conversation, public interviews and panel discussions, telephone conversations, radio broadcasts, spontaneous speeches and prepared speeches produced in the 1970s. To these Biber a"
C14-1054,C94-2174,0,0.814604,"resents a set of shared regularities among written or spoken documents that enables readers, writers, listeners and speakers to signal discourse function, and that conditions their expectations of linguistic form. Genre distinctions are therefore an important aspect of language use and understanding. They clearly have a role to play in statistical language processing, which relies on regularities of form as well as content. Indeed, with the advent of the Web, statistical methods for genre differentiation have been applied to information retrieval to limit search criteria and organize results (Karlgren and Cutting, 1994; Kessler et al., 1997; Mehler et al., 2010; Ward and Werner, 2013), and the study of genres on the web has become a sub-field in its own right (see for example (Mehler et al., 2010)). More recently, the development of genre-dependent models for a variety of natural language processing (NLP) tasks such as parsing (Ravi et al., 2008; McClosky et al., 2010; Roux et al., 2012), speech recognition (Iyer and Ostendorf, 1999), word sense disambiguation (Martinez and Agirre, 2000), and machine translation (Wang et al., 2012) has been found to significantly improve performance. The ability to match do"
C14-1054,P97-1005,0,0.632684,"Missing"
C14-1054,W00-1326,0,0.0773224,"thods for genre differentiation have been applied to information retrieval to limit search criteria and organize results (Karlgren and Cutting, 1994; Kessler et al., 1997; Mehler et al., 2010; Ward and Werner, 2013), and the study of genres on the web has become a sub-field in its own right (see for example (Mehler et al., 2010)). More recently, the development of genre-dependent models for a variety of natural language processing (NLP) tasks such as parsing (Ravi et al., 2008; McClosky et al., 2010; Roux et al., 2012), speech recognition (Iyer and Ostendorf, 1999), word sense disambiguation (Martinez and Agirre, 2000), and machine translation (Wang et al., 2012) has been found to significantly improve performance. The ability to match documents by genre has also become important for collecting data to train language models for spoken language understanding, given the difficulty of creating large repositories of transcribed spoken language corpora (Bulyko and Ostendorf, 2003; Sarikaya et al., 2005). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/"
C14-1054,P02-1040,0,0.0950055,"Missing"
C14-1054,D08-1093,0,0.201795,"istical language processing, which relies on regularities of form as well as content. Indeed, with the advent of the Web, statistical methods for genre differentiation have been applied to information retrieval to limit search criteria and organize results (Karlgren and Cutting, 1994; Kessler et al., 1997; Mehler et al., 2010; Ward and Werner, 2013), and the study of genres on the web has become a sub-field in its own right (see for example (Mehler et al., 2010)). More recently, the development of genre-dependent models for a variety of natural language processing (NLP) tasks such as parsing (Ravi et al., 2008; McClosky et al., 2010; Roux et al., 2012), speech recognition (Iyer and Ostendorf, 1999), word sense disambiguation (Martinez and Agirre, 2000), and machine translation (Wang et al., 2012) has been found to significantly improve performance. The ability to match documents by genre has also become important for collecting data to train language models for spoken language understanding, given the difficulty of creating large repositories of transcribed spoken language corpora (Bulyko and Ostendorf, 2003; Sarikaya et al., 2005). This work is licensed under a Creative Commons Attribution 4.0 Int"
C14-1054,sharoff-etal-2010-web,0,0.155756,"Missing"
C14-1054,C00-2117,0,0.134367,"l genres as well as new social media (email, blogs, twitter) and collectively generated fiction (ficlets). We take advantage of MASC’s rich set of validated annotations to include features that would not have been (easily) available at the time of Biber’s study, and reconsider the use of some features used in his work. Some work on genre classification contrasts with Biber’s approach, which assumes that documents fall discretely into distinct classes or clusters. Genre classification has been treated as a standalone task (Karlgren and Cutting, 1994; Kessler et al., 1997; Feldman et al., 2009; Stamatatos et al., 2000a; Santini, 2004), or combined with topic classification (Rauber and M¨uller-K¨ogler, 2001; Lee and Myaeng, 2002). All of these studies assume that documents fall discretely into distinct classes or clusters. These studies vary in their approach to determining the genre of text, either by using corpora with pre-defined classes (Karlgren and Cutting, 1994), manually refining pre-existing classes (Kessler et al., 1997), creating genre classes using annotators, or locating a priori classifications (e.g., web product reviews). The feature sets in genre studies have remained rather stable over the"
C14-1054,J00-4001,0,0.0976731,"l genres as well as new social media (email, blogs, twitter) and collectively generated fiction (ficlets). We take advantage of MASC’s rich set of validated annotations to include features that would not have been (easily) available at the time of Biber’s study, and reconsider the use of some features used in his work. Some work on genre classification contrasts with Biber’s approach, which assumes that documents fall discretely into distinct classes or clusters. Genre classification has been treated as a standalone task (Karlgren and Cutting, 1994; Kessler et al., 1997; Feldman et al., 2009; Stamatatos et al., 2000a; Santini, 2004), or combined with topic classification (Rauber and M¨uller-K¨ogler, 2001; Lee and Myaeng, 2002). All of these studies assume that documents fall discretely into distinct classes or clusters. These studies vary in their approach to determining the genre of text, either by using corpora with pre-defined classes (Karlgren and Cutting, 1994), manually refining pre-existing classes (Kessler et al., 1997), creating genre classes using annotators, or locating a priori classifications (e.g., web product reviews). The feature sets in genre studies have remained rather stable over the"
C14-1054,2012.amta-papers.18,0,0.0198145,"information retrieval to limit search criteria and organize results (Karlgren and Cutting, 1994; Kessler et al., 1997; Mehler et al., 2010; Ward and Werner, 2013), and the study of genres on the web has become a sub-field in its own right (see for example (Mehler et al., 2010)). More recently, the development of genre-dependent models for a variety of natural language processing (NLP) tasks such as parsing (Ravi et al., 2008; McClosky et al., 2010; Roux et al., 2012), speech recognition (Iyer and Ostendorf, 1999), word sense disambiguation (Martinez and Agirre, 2000), and machine translation (Wang et al., 2012) has been found to significantly improve performance. The ability to match documents by genre has also become important for collecting data to train language models for spoken language understanding, given the difficulty of creating large repositories of transcribed spoken language corpora (Bulyko and Ostendorf, 2003; Sarikaya et al., 2005). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 565 Proceedings of COLING 2014, the 25th Int"
C14-1054,N10-1004,0,\N,Missing
C90-2067,J87-3003,0,0.0609279,"Missing"
C90-2067,P84-1036,0,0.0481294,"Missing"
C90-2067,P85-1037,0,0.213116,"Missing"
C90-2067,P86-1018,0,0.0602406,"Missing"
C90-2067,J87-3005,0,\N,Missing
C92-2089,P84-1008,0,0.0759894,"Missing"
C92-2089,C88-2098,0,0.0200051,"Missing"
C92-2089,C90-2067,1,0.807863,"Missing"
C92-2089,J87-3003,0,0.0417163,"Missing"
C92-2089,A88-1012,0,\N,Missing
C92-2089,P86-1018,0,\N,Missing
C92-2089,P84-1036,0,\N,Missing
C94-1097,C94-1094,1,0.772036,"Missing"
C94-1097,A92-1018,0,0.026827,"Missing"
C94-1097,P91-1023,0,0.0454091,"Missing"
C94-1097,A88-1019,0,\N,Missing
C98-1044,P87-1022,0,0.607207,"Missing"
C98-1044,J86-3001,0,0.521933,"Missing"
C98-1044,J96-3006,0,\N,Missing
C98-1044,P97-1012,1,\N,Missing
C98-1044,J95-2003,0,\N,Missing
C98-1044,J92-4007,0,\N,Missing
C98-1044,P83-1007,0,\N,Missing
C98-1044,P96-1036,0,\N,Missing
C98-1044,J96-2005,0,\N,Missing
C98-1049,C94-1097,1,\N,Missing
calzolari-etal-2002-towards,J87-3006,0,\N,Missing
calzolari-etal-2002-towards,bel-etal-2000-simple,1,\N,Missing
caselli-etal-2008-bilingual,W01-1313,0,\N,Missing
caselli-etal-2008-bilingual,W01-1309,0,\N,Missing
de-melo-etal-2012-empirical,passonneau-etal-2010-word,1,\N,Missing
de-melo-etal-2012-empirical,W09-1127,0,\N,Missing
de-melo-etal-2012-empirical,W09-3021,1,\N,Missing
E91-1040,J87-3003,0,0.639931,"Missing"
E91-1040,P84-1036,0,0.293143,"Missing"
E91-1040,P85-1037,0,0.717858,"Missing"
E91-1040,P86-1018,0,0.638233,"Missing"
E91-1040,C88-2098,0,0.202831,"Missing"
E91-1040,C90-2067,1,\N,Missing
erjavec-etal-2000-concede,P98-1050,1,\N,Missing
erjavec-etal-2000-concede,C98-1049,1,\N,Missing
ide-etal-2000-xces,W98-1102,1,\N,Missing
ide-etal-2000-xces,erjavec-etal-2000-concede,1,\N,Missing
ide-etal-2002-american,ide-etal-2000-xces,1,\N,Missing
ide-etal-2008-masc,W04-0803,0,\N,Missing
ide-etal-2008-masc,W04-0811,0,\N,Missing
ide-etal-2008-masc,passonneau-etal-2006-inter,1,\N,Missing
ide-etal-2008-masc,S07-1018,1,\N,Missing
ide-etal-2008-masc,W07-1501,1,\N,Missing
ide-etal-2008-masc,S07-1048,0,\N,Missing
ide-etal-2008-masc,fillmore-etal-2004-framenet,1,\N,Missing
ide-etal-2010-anc2go,ide-etal-2008-masc,1,\N,Missing
ide-etal-2010-anc2go,W07-1501,1,\N,Missing
ide-etal-2010-anc2go,W09-3021,0,\N,Missing
ide-etal-2010-anc2go,W09-3004,1,\N,Missing
ide-etal-2014-language,windhouwer-2012-relcat,0,\N,Missing
ide-etal-2014-language,cieri-etal-2014-new,1,\N,Missing
ide-etal-2014-language,piperidis-2012-meta,0,\N,Missing
ide-etal-2014-language,cassidy-etal-2014-alveo,0,\N,Missing
ide-etal-2014-language,J08-3010,0,\N,Missing
ide-etal-2014-language,W14-5211,1,\N,Missing
ide-etal-2014-language,P13-1166,0,\N,Missing
ide-etal-2014-language,W14-5204,1,\N,Missing
ide-romary-2004-registry,W03-1901,1,\N,Missing
ide-romary-2004-registry,W03-0804,1,\N,Missing
ide-romary-2006-representing,W03-1901,1,\N,Missing
ide-romary-2006-representing,ide-romary-2004-registry,1,\N,Missing
ide-romary-2006-representing,W03-0804,1,\N,Missing
ide-suderman-2004-american,ide-etal-2000-xces,1,\N,Missing
ide-suderman-2004-american,tufis-2000-using,0,\N,Missing
ide-suderman-2004-american,P98-1029,0,\N,Missing
ide-suderman-2004-american,C98-1029,0,\N,Missing
ide-suderman-2004-american,ide-etal-2002-american,1,\N,Missing
ide-suderman-2006-integrating,P98-1029,0,\N,Missing
ide-suderman-2006-integrating,C98-1029,0,\N,Missing
ide-suderman-2006-integrating,ide-suderman-2004-american,1,\N,Missing
ide-woolner-2004-exploiting,ide-etal-2000-xces,1,\N,Missing
J98-1001,W89-0232,0,0.0300972,"Missing"
J98-1001,W97-0211,0,0.0269739,"disambiguate other occurrences. If another occurrence can be disambiguated with certitude, the system automatically acquires additional statistical information from these newly disambiguated occurrences, thus improving its knowledge incrementally. Hearst indicates that an initial set of at least 10 occurrences is necessary for the procedure, and that 20 or 3{) occurrences are necessary for high precision. This overall strategy is more or less that of most subsequent work on bootstrapping. Recently, a class-based bootstrapping method for semantic tagging in specific domains has been proposed (Basili et al. 1997). Sch~tze (1992, 1993) proposes a method that avoids tagging each occurrence in the training corpus. Using letter fourgrams within a 1,001-character window, his method, building on the vector-space model from information retrieval (see Salton, Wong, and Yang [1975]), automatically clusters the words in the text (each target word is represented by a vector); a sense is then assigned manually to each cluster, rather than to each occurrence. Assigning a sense demands examining 10 to 20 members of each cluster, and each sense may be represented by several clusters. This method reduces the amount o"
J98-1001,1997.tc-1.13,0,0.0479055,"Missing"
J98-1001,P91-1034,0,0.367071,"y to each cluster, rather than to each occurrence. Assigning a sense demands examining 10 to 20 members of each cluster, and each sense may be represented by several clusters. This method reduces the amount of manual intervention but still requires the examination of a hundred or so occurrences for each ambiguous word. A more serious issue for this method is that it is not clear what the senses derived from the clusters correspond to (see, for example Pereira, Tishby, and Lee [1993]); moreover, the senses are not directly usable by other systems, since they are derived from the corpus itself. Brown et al. (1991) and Gale, Church, and Yarowsky, (1992a, 1993) propose the use of bilingual corpora to avoid hand-tagging of training data. Their premise is that different senses of a given word often translate differently in another language (for example, pen in English is stylo in French for its 'writing implement' sense, and enclos for its 'enclosure' sense). By using a parallel aligned corpus, the translation of each occurrence of a word such as pen can be used to automatically determine its sense. This method has some limitations, since many ambiguities are preserved in the target language (e.g., French"
J98-1001,J92-4003,0,0.0805197,"d Gale (1991) have proposed a means to improve methods for the estimation of bigrams, which could be extended to co-occurrences: they take into account the frequency of the individual words that compose the bigram and make the hypothesis that each word appears independently of the others. However, this hypothesis contradicts hypotheses of disambiguation based on co-occurrence, which rightly assume that some associations are more probable than others. Class-based models attempt to obtain the best estimates by combining observations of classes of words considered to belong to a common category. Brown et al. (1992), Pereira and Tishby (1992), and Pereira, Tishby, and Lee (1993) propose methods that derive classes from the distributional properties of the corpus itself, while other authors use external information sources to define classes: Resnik (1992) uses the taxonomy of WordNet; Yarowsky (1992) uses the categories of Roget's Thesaurus, Slator (1992) and Liddy and Paik (1993) use the subject codes in the LDOCE; Luk (1995) uses conceptual sets built from the LDOCE definitions. Class-based methods answer in part the problem of data sparseness and eliminate the need for pretagged 15 For example, in a wi"
J98-1001,P94-1020,0,0.0453151,"Missing"
J98-1001,W97-0205,0,0.0240351,"Missing"
J98-1001,J87-3003,0,0.0722476,"Missing"
J98-1001,P84-1036,0,0.0605907,"Missing"
J98-1001,P96-1041,0,0.0248688,"Missing"
J98-1001,P85-1037,0,0.0736313,"Missing"
J98-1001,J93-1001,0,0.0357,"Missing"
J98-1001,H92-1046,0,0.0315042,"Missing"
J98-1001,J94-4003,0,0.486722,"glish is stylo in French for its 'writing implement' sense, and enclos for its 'enclosure' sense). By using a parallel aligned corpus, the translation of each occurrence of a word such as pen can be used to automatically determine its sense. This method has some limitations, since many ambiguities are preserved in the target language (e.g., French souris--English mouse); furthermore, the few available large-scale parallel corpora are very specialized (for example, the Hansard corpus of Canadian Parliamentary Debates), which skews the sense representation. 14Dagan, Itai, and Schwall (1991) and Dagan and Itai (1994) propose a similar method, but instead of a parallel corpus use two monolingual corpora and a bilingual dictionary. This solves, in part, the problems of availability and specificity of domain that plague the parallel corpus approach, since monolingual corpora, including corpora from diverse domains and genres, are much easier to obtain than parallel corpora. 13 This study involvesonly nouns. 14 For example,Gale, Church, and Yarowsky(1993) remark that it is difficultto find any sense other than the financialsense for the word bank in the Hansard corpus. 16 Ide and V~ronis Introduction Other me"
J98-1001,P91-1017,0,0.0852678,"Missing"
J98-1001,P93-1022,0,0.00691215,"Missing"
J98-1001,P94-1038,0,0.0064553,"Missing"
J98-1001,1992.tmi-1.9,0,0.0360454,"Missing"
J98-1001,P92-1032,0,0.033913,"Missing"
J98-1001,H92-1045,0,0.17031,"Missing"
J98-1001,C94-1042,0,0.030279,"Missing"
J98-1001,H93-1050,0,0.0238328,"Missing"
J98-1001,P91-1019,0,0.0121216,"Missing"
J98-1001,J93-1005,0,0.0977817,"hkeepsie, New York 12604-0520. E-mail: ide@cs.vassar.edu ~ LaboratoireParole et Langage, ESA 6057 CNRS, Universit~ de Provence, 29 Avenue Robert Schuman, 13621 Aix-en-ProvenceCedex 1, France. E-mail:Jean.Veronis@lpl.univ-aix.fr (~) 1998 Associationfor Computational Linguistics Computational Linguistics Volume 24, Number 1 and is masculine in the former sense, feminine in the latter) to properly tag it as a masculine noun. Sense disambiguation is also necessary for certain syntactic analyses, such as prepositional phrase attachment (Jensen and Binot 1987; Whittemore, Ferrara, and Brunner 1990; Hindle and Rooth 1993), and, in general, restricts the space of competing parses (Alshawi and Carter 1994). speech processing: sense disambiguation is required for correct phonetization of words in speech synthesis, for example, the word conjure in He conjured up an image or in I conjure you to help me (Sproat, Hirschberg, and Yarowsky 1992; Yarowsky 1997), and also for word segmentation and homophone discrimination in speech recognition (Connine 1990; Seneff 1992). text processing: sense disambiguation is necessary for spelling correction (for example, to determine when diacritics should be inserted, such as in Fr"
J98-1001,T87-1006,0,0.0638975,"Missing"
J98-1001,H93-1051,0,0.0226464,"Missing"
J98-1001,W97-0203,0,0.0394341,"Missing"
J98-1001,J92-1001,0,0.0154936,"Missing"
J98-1001,H94-1046,0,0.0153163,"Missing"
J98-1001,H93-1061,0,0.0133405,"corpus is extremely costly, and, at present, very few sense-tagged corpora are available. Several efforts to create sense-tagged corpora have been or are being made: the Linguistic Data Consortium 15 Computational Linguistics Volume 24, Number 1 distributes a corpus of approximately 200,000 sentences from the Brown Corpus and the Wall Street Journal in which all occurrences of 191 words are hand-tagged with their WordNet senses (see Ng and Lee [1996]). Also, the Cognitive Science Laboratory at Princeton has undertaken the hand-tagging of 1,000 words from the Brown Corpus with WordNet senses (Miller et al. 1993) (so far, 200,000 words are available via ftp), and hand-tagging of 25 verbs in a small segment of the Wall Street Journal (12,925 sentences), is also underway (Wiebe et al. 1997). However, these corpora are far smaller than those typically used with statistical methods. Several efforts have been made to automatically sense-tag a training corpus via bootstrapping methods. Hearst (1991) proposed an algorithm (CatchWord) that includes a training phase during which each occurrence of a set of nouns to be disambiguated is manually sense-tagged in several occurrences. 13 Statistical information ext"
J98-1001,J91-1002,0,0.232196,"Missing"
J98-1001,C88-2098,0,0.0505598,"Missing"
J98-1001,P96-1006,0,0.301085,"Missing"
J98-1001,C94-1049,0,0.0119168,"Missing"
J98-1001,1983.tc-1.13,0,0.187522,"Missing"
J98-1001,2004.jeptalnrecital-long.14,0,0.0610898,"e role of the domain in sense disambiguation, making a point that was reiterated several decades later by Gale, Church, and Yarowsky (1992c): In mathematics, to take what is probably the easiest example, one can very nearly say that each word, within the general context of a mathematical article, has one and only one meaning. (1955, 20) Following directly from this observation, much effort in the early days of machine translation was devoted to the development of specialized dictionaries or ""microglossaries"" (Oswald 1952, 1957; Oswald and Lawson 1953; Oettinger 1955; Dostert 1955; Gould 1957; Panov 1960). Such microglossaries contain only the meaning of a given word relevant for texts in a particular domain of discourse; e.g., a microglossary for the domain of mathematics would contain only the relevant definition of triangle, and not the definition of triangle as a musical instrument. The need for knowledge representation for WSD was also acknowledged from the outset: Weaver concludes by noting the ""tremendous amount of work [needed] in the logical structure of languages"" (1995, 23). Several researchers attempted to devise Ide and V~ronis Introduction an ""interlingua"" based on logical and ma"
J98-1001,H93-1054,0,0.017239,"to compute semantic distance. The hypothesis is that for a given set of terms occurring near each other in a text, choosing the senses that minimize the distance among them selects the correct senses. SuSsna's disambiguation results are demonstrated to be significantly better than chance. His work is particularly interesting because it is one of the few to date that utilizes not only WordNet's IS-A hierarchy, but other relational links as well. Resnik (1995a) draws on his body of earlier work on WordNet, in which he explores a measure of semantic similarity for words in the WordNet hierarchy (Resnik 1993a, 1993b, 1995a). He computes the shared information content of words, which is a measure of the specificity of the concept that subsumes the words in the WordNet IS-A hierarchy--the more specific the concept that subsumes two or more words, the more semantically related they are assumed to be. Resnik contrasts his method of computing similarity to those which compute path length (e.g., Sussna 1993), arguing that the links in the WordNet taxonomy do not represent uniform distances (cf. Resnik 1995b). Resnik's method, applied using WordNet's fine-grained sense distinctions and measured against"
J98-1001,W95-0105,0,0.0553377,"pe leaving a node and the depth of a given edge in the overall ""tree."" This metric is applied to arcs in the shortest path between nodes (word senses) to compute semantic distance. The hypothesis is that for a given set of terms occurring near each other in a text, choosing the senses that minimize the distance among them selects the correct senses. SuSsna's disambiguation results are demonstrated to be significantly better than chance. His work is particularly interesting because it is one of the few to date that utilizes not only WordNet's IS-A hierarchy, but other relational links as well. Resnik (1995a) draws on his body of earlier work on WordNet, in which he explores a measure of semantic similarity for words in the WordNet hierarchy (Resnik 1993a, 1993b, 1995a). He computes the shared information content of words, which is a measure of the specificity of the concept that subsumes the words in the WordNet IS-A hierarchy--the more specific the concept that subsumes two or more words, the more semantically related they are assumed to be. Resnik contrasts his method of computing similarity to those which compute path length (e.g., Sussna 1993), arguing that the links in the WordNet taxonomy"
J98-1001,W97-0217,0,0.0251113,"s and types of polysemy, must first be resolved. SENSEVAL will likely take us a step closer to this understanding; at the least, it will force consideration of what can be meaningfully regarded as an isolatable sense distinction and provide some measure of the distance between the performance of current systems and a predefined standard. The in vitro evaluation envisaged for SENSEVAL demands the creation of a manually sense-tagged reference corpus containing an agreed-upon set of sense distinctions. The difficulties of attaining sense agreement, even among experts, have already been outlined. Resnik and Yarowsky (1997b) have proposed that for WSD evaluation, 25 Computational Linguistics Volume 24, Number 1 it may be practical to retain only those sense distinctions that are lexicalized crosslinguistically. This proposal has the merit of being immediately usable, but in view of the types of problems cited in the previous section, systematic study of interlanguage relations will be required to determine its viability and generality. At present, the apparent best source of sense distinctions is assumed to be on-line resources such as LDOCE or WordNet, although the problems of utilizing such resources are well"
J98-1001,W97-0213,0,0.0296926,"s and types of polysemy, must first be resolved. SENSEVAL will likely take us a step closer to this understanding; at the least, it will force consideration of what can be meaningfully regarded as an isolatable sense distinction and provide some measure of the distance between the performance of current systems and a predefined standard. The in vitro evaluation envisaged for SENSEVAL demands the creation of a manually sense-tagged reference corpus containing an agreed-upon set of sense distinctions. The difficulties of attaining sense agreement, even among experts, have already been outlined. Resnik and Yarowsky (1997b) have proposed that for WSD evaluation, 25 Computational Linguistics Volume 24, Number 1 it may be practical to retain only those sense distinctions that are lexicalized crosslinguistically. This proposal has the merit of being immediately usable, but in view of the types of problems cited in the previous section, systematic study of interlanguage relations will be required to determine its viability and generality. At present, the apparent best source of sense distinctions is assumed to be on-line resources such as LDOCE or WordNet, although the problems of utilizing such resources are well"
J98-1001,J82-2005,0,0.226498,"ng in the area of machine translation, used a semantic network to derive the representation of sentences in an interlingua comprised of fundamental language concepts; sense distinctions are implicitly made by choosing representations that reflect groups of closely related nodes in the network. She developed a set of 100 primitive concept types (THING, DO, etc.), in terms of which her group built a 15,000-entry concept dictionary, where concept types are organized in a lattice with inheritance of properties from superconcepts to subconcepts. Building on this and on work on semantic networks by Richens (1958), Quillian (1961, 1962a, 1962b, 1967, 1968, 1969) built a network that includes links among words (tokens) and concepts (types), in which links are labeled with various semantic relations or simply indicate associations between words. The network is created starting from dictionary definitions, but is enhanced by human knowledge that is hand-encoded. When two words are presented to the network, Quillian's program simulates the gradual activation of concept nodes along a path of links originating from each input word by means of marker passing; disambiguation is accomplished because only one co"
J98-1001,W97-0305,0,0.0237063,"Missing"
J98-1001,C92-4196,0,0.0299647,"Missing"
J98-1001,J92-1004,0,0.104839,"cessary for certain syntactic analyses, such as prepositional phrase attachment (Jensen and Binot 1987; Whittemore, Ferrara, and Brunner 1990; Hindle and Rooth 1993), and, in general, restricts the space of competing parses (Alshawi and Carter 1994). speech processing: sense disambiguation is required for correct phonetization of words in speech synthesis, for example, the word conjure in He conjured up an image or in I conjure you to help me (Sproat, Hirschberg, and Yarowsky 1992; Yarowsky 1997), and also for word segmentation and homophone discrimination in speech recognition (Connine 1990; Seneff 1992). text processing: sense disambiguation is necessary for spelling correction (for example, to determine when diacritics should be inserted, such as in French, changing comte to comte [Yarowsky 1994a, 1994b]); for case changes (HE READ THE TIMES ~ He read the Times); for lexical access of Semitic languages (in which vowels are not written), etc. The problem of word sense disambiguation (WSD) has been described as ""AI-complete,"" that is, a problem which can be solved only by first resolving all the difficult problems in artificial intelligence (AI), such as the representation of common sense and"
J98-1001,C90-2067,1,0.498116,"Missing"
J98-1001,E91-1040,1,0.214643,"Missing"
J98-1001,P90-1004,0,0.0367304,"Missing"
J98-1001,T75-2009,0,0.4317,"Missing"
J98-1001,J96-3009,0,0.00944183,"Missing"
J98-1001,C92-2070,0,0.387984,"s masculine in the former sense, feminine in the latter) to properly tag it as a masculine noun. Sense disambiguation is also necessary for certain syntactic analyses, such as prepositional phrase attachment (Jensen and Binot 1987; Whittemore, Ferrara, and Brunner 1990; Hindle and Rooth 1993), and, in general, restricts the space of competing parses (Alshawi and Carter 1994). speech processing: sense disambiguation is required for correct phonetization of words in speech synthesis, for example, the word conjure in He conjured up an image or in I conjure you to help me (Sproat, Hirschberg, and Yarowsky 1992; Yarowsky 1997), and also for word segmentation and homophone discrimination in speech recognition (Connine 1990; Seneff 1992). text processing: sense disambiguation is necessary for spelling correction (for example, to determine when diacritics should be inserted, such as in French, changing comte to comte [Yarowsky 1994a, 1994b]); for case changes (HE READ THE TIMES ~ He read the Times); for lexical access of Semitic languages (in which vowels are not written), etc. The problem of word sense disambiguation (WSD) has been described as ""AI-complete,"" that is, a problem which can be solved onl"
J98-1001,H93-1052,0,0.877514,"l models of McCulloch and Pitts (1943) and Hebb's (1949) work on neurological development, which saw its first full development in Rosenblatt's (1958) ""perceptrons."" 8 Ide and V~ronis Introduction 2.3 Knowledge-based Methods The AI-based work of the 1970s and 1980s was theoretically interesting but not at all practical for language understanding in any but extremely limited domains. A significant roadblock to generalizing WSD work was the difficulty and cost of hand-crafting the enormous amounts of knowledge required for WSD: the so-called ""knowledge acquisition bottleneck"" (Gale, Church, and Yarowsky 1993). Work on WSD reached a turning point in the 1980s when large-scale lexical resources, such as dictionaries, thesauri, and corpora, became widely available. Attempts were made to automatically extract knowledge from these sources (Sections 2.3.1 and 2.3.2) and, more recently, to construct large-scale knowledge bases by hand (Section 2.3.3). A corresponding shift away from methods based in linguistic theories and towards empirical methods also occurred at this time, as well as a decrease in emphasis on do-all systems in favor of ""intermediate"" tasks such as WSD. 2.3.1 Machine-Readable Dictionar"
J98-1001,P94-1013,0,0.172028,"e space of competing parses (Alshawi and Carter 1994). speech processing: sense disambiguation is required for correct phonetization of words in speech synthesis, for example, the word conjure in He conjured up an image or in I conjure you to help me (Sproat, Hirschberg, and Yarowsky 1992; Yarowsky 1997), and also for word segmentation and homophone discrimination in speech recognition (Connine 1990; Seneff 1992). text processing: sense disambiguation is necessary for spelling correction (for example, to determine when diacritics should be inserted, such as in French, changing comte to comte [Yarowsky 1994a, 1994b]); for case changes (HE READ THE TIMES ~ He read the Times); for lexical access of Semitic languages (in which vowels are not written), etc. The problem of word sense disambiguation (WSD) has been described as ""AI-complete,"" that is, a problem which can be solved only by first resolving all the difficult problems in artificial intelligence (AI), such as the representation of common sense and encyclopedic knowledge. The inherent difficulty of sense disambiguation was a central point in Bar-Hillel's well-known treatise on machine translation (Bar-Hillel 1960), where he asserted that he"
J98-1001,P95-1026,0,0.138905,"Missing"
J98-1001,J98-1005,0,\N,Missing
J98-1001,J94-4005,0,\N,Missing
J98-1001,1993.eamt-1.2,1,\N,Missing
J98-1001,J98-1006,0,\N,Missing
J98-1001,J87-3005,0,\N,Missing
J98-1001,J98-1003,0,\N,Missing
J98-1001,C90-2028,0,\N,Missing
J98-1001,C94-2113,0,\N,Missing
J98-1001,C80-1057,0,\N,Missing
J98-1001,P94-1002,0,\N,Missing
J98-1001,J98-1002,0,\N,Missing
J98-1001,J94-2001,0,\N,Missing
J98-1001,J98-1004,0,\N,Missing
J98-1001,P96-1042,0,\N,Missing
J98-1001,P86-1018,0,\N,Missing
J98-1001,P95-1025,0,\N,Missing
J98-1001,W97-0202,0,\N,Missing
J98-1001,J90-3007,0,\N,Missing
J98-1001,P93-1024,0,\N,Missing
L16-1073,ide-etal-2014-language,1,0.876291,"erability, Workflow management 1. 2 Overview 1 The NSF-SI -funded LAPPS Grid project is a collaborative effort among Brandeis University, Vassar College, Carnegie-Mellon University (CMU), and the Linguistic Data Consortium (LDC) at the University of Pennsylvania, which has developed an open, web-based infrastructure through which massive and distributed resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, evaluated, disseminated and consumed by researchers, developers, and students across a wide variety of disciplines (Ide et al., 2014). The LAPPS Grid is part of a larger multiway international collaboration including key individuals and projects from the U.S., Europe, Australia, and Asia involved with language resource development and distribution and standards-making, who are creating the “The Federated Grid of Language Services” (FGLS) federation (Ishida et al., 2014), a multi-lingual, international network of web service grids and providers. We have also recently entered into a formal partnership with WebLicht/T¨ubingen and LINDAT/CLARIN (Prague) to create a “trust network” among our sites in order to provide mutual acce"
L16-1073,P13-1166,0,0.0680149,"Missing"
L16-1073,W09-3034,1,0.6995,"ebLicht/T¨ubingen and LINDAT/CLARIN (Prague) to create a “trust network” among our sites in order to provide mutual access to all from any one of the three portals. The key to the success of these partnerships is the interoperability among tools and services that is accomplished via the service-oriented architecture and the development of common vocabularies and multi-way mappings that has involved key researchers from around the world for over a decade2 . 1 http://www.lappsgrid.org E.g., the NSF-funded Sustainable Interoperability for Language Technology (SILT) project (NSF-INTEROP 0753069) (Ide et al., 2009), the EU-funded Fostering Language Resources Network (FLaReNet) project (Calzolari et al., 2009), the International Standards Organization (ISO) committee for Language Resource Management (ISO TC37 SC4), and parallel efforts in Asia and Australia, together with the LAPPS project and international col2 The LAPPS Grid currently includes a wide range of NLP component web services and provides facilities for service discovery, service composition (including automatic format conversion between tools where necessary), performance evaluation (via provision of component-level measures for standard eva"
L18-1025,D15-1301,0,0.0261746,"to reproducibility is a problem because without them, we cannot compare studies of reproducibility. A number of such studies have appeared very recently, and in general, the results have been depressing. Multiple studies over the course of the past two years have reported widespread failures of reproducibility (Collaboration and others, 2015; Collberg et al., 2015). They range from unusually large-scale studies in psychology (Collaboration and others, 2015), to surprisingly large ones in computer science (Collberg et al., 2015), to case studies in natural language processing (Schwartz, 2010; Borgholt et al., 2015; Cohen et al., 2016; Gomes et al., 2016; N´ev´eol et al., 2016; Cassidy and Estival, 2017; Kilicoglu, 2017; Mieskes, 2017). Yet, it is still quite difficult to get even a rough sense of the actual scale of the problem in natural language processing, because the lack of agreement about what exactly is being assessed makes it difficult to compare findings across papers on reproducibility issues. 156 To address this problem of a lack of consensus definitions, this paper proposes a set of dimensions of reproducibility. Perhaps counter-intuitively, we first give the definition of replicability or"
L18-1025,J92-1002,0,0.025332,"anguage-related value that stimulated an enormous amount of academic work, some of which has been evaluated with respect to the extent to which it does or does not reproduce the values reported in (Shannon, 1951). For example, (Cover and King, 1978) used a very different method from Shannon’s original one and found a value of 1.3 bits for the entropy of written English. The paper explicitly states that this value “agrees well with Shannon’s estimate,” suggesting that the authors considered their value to have reproduced Shannon’s original value in (Shannon, 1951)3 . In a very different tone, (Brown et al., 1992) reported an upper bound of exactly 1.75 bits, but did not explicitly compare that to previous findings, although it is clear from the paper that they considered it different from—and better than—previously reported values. As the authors put it: We see this paper as a gauntlet thrown down before the computational linguistics community. A relevant value from our papers that was not reproduced is the mean value for the frequency of negation. We reported this in our papers (Cohen et al., 2010) and (Cohen et al., 2017a). They were different by roughly a factor of 2, even though we used the same c"
L18-1025,daelemans-hoste-2002-evaluation,0,0.0820242,"as the bug that we report in this paper. 4.3. Definitions of dimensions of reproducibility in the larger context of natural language processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computational linguistics and natural language processing that integrates these strengths of our field and also explores the relationships between natural language processing; computational and corpus linguistics; artificia"
L18-1025,P13-1166,0,0.316125,"Missing"
L18-1025,H90-1013,0,0.482748,", it can be quite difficult to achieve (Fokkens et al., 2013; N´ev´eol et al., 2016), and the causes of reproducibility problems can be well-hidden—see (Johnson et al., 2007; Cohen et al., 2017b), as well as the bug that we report in this paper. 4.3. Definitions of dimensions of reproducibility in the larger context of natural language processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computati"
L18-1025,H94-1017,0,0.0985049,"difficult to achieve (Fokkens et al., 2013; N´ev´eol et al., 2016), and the causes of reproducibility problems can be well-hidden—see (Johnson et al., 2007; Cohen et al., 2017b), as well as the bug that we report in this paper. 4.3. Definitions of dimensions of reproducibility in the larger context of natural language processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computational linguistics"
L18-1025,W16-6110,1,0.905358,"Missing"
L18-1025,J05-1004,0,0.0161754,"e processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computational linguistics and natural language processing that integrates these strengths of our field and also explores the relationships between natural language processing; computational and corpus linguistics; artificial intelligence, theoretical linguistics, and cognitive science (Cori et al., 2002). (See also (Cori and L´eon, 2002) for a dis"
L18-1025,W10-1726,0,0.114498,"initions related to reproducibility is a problem because without them, we cannot compare studies of reproducibility. A number of such studies have appeared very recently, and in general, the results have been depressing. Multiple studies over the course of the past two years have reported widespread failures of reproducibility (Collaboration and others, 2015; Collberg et al., 2015). They range from unusually large-scale studies in psychology (Collaboration and others, 2015), to surprisingly large ones in computer science (Collberg et al., 2015), to case studies in natural language processing (Schwartz, 2010; Borgholt et al., 2015; Cohen et al., 2016; Gomes et al., 2016; N´ev´eol et al., 2016; Cassidy and Estival, 2017; Kilicoglu, 2017; Mieskes, 2017). Yet, it is still quite difficult to get even a rough sense of the actual scale of the problem in natural language processing, because the lack of agreement about what exactly is being assessed makes it difficult to compare findings across papers on reproducibility issues. 156 To address this problem of a lack of consensus definitions, this paper proposes a set of dimensions of reproducibility. Perhaps counter-intuitively, we first give the definiti"
L18-1206,W06-2920,0,0.0607984,"Missing"
L18-1206,W14-5211,0,0.072335,"Missing"
L18-1206,hinrichs-krauwer-2014-clarin,1,0.875661,"Missing"
L18-1206,L16-1262,1,0.886086,"Missing"
L18-1206,L16-1680,1,0.905458,"Missing"
L18-1327,ide-etal-2014-language,1,0.911249,"h papers for associations and connections (such as between drugs and side effects, or genes and disease pathways) that humans reading each paper individually might not notice. Up to now, the use of NLP technologies has required considerable skill in the field. However, recent development of environments for constructing customizable NLP applications has opened the door for scientists to exploit NLP technologies for discovering and mining information from massive bodies of scientific publications such as those found in PubMed, PLoS, Web of Science, etc. The Language Applications (LAPPS) Grid1 (Ide et al., 2014) provides an infrastructure for rapid development of natural language processing applications (NLP) by providing access to a wide range of tools and making them both syntactically and semantically interoperable. The LAPPS Grid uses the Galaxy platform2 (Giardine et al., 2005), originally developed for use by genomics researchers with little computational expertise, as its workflow engine. The Galaxy interface and the interoperability among tools together provide an intuitive and easy-to-use platform that enables users to experiment with and exploit NLP tools and resources without the need to d"
L18-1327,I08-2122,0,0.537432,"Environment (XSEDE)6 and the associated Jetstream7 cloud environment. These resources allow users to create virtual machines configured as specialized versions of the LAPPS Grid on the remote resource. If necessary, access can be given to specified domain servers holding secure data. 2.1. http://www.lappsgrid.org http://incommon.org 6 https://www.xsede.org 7 https://jetstream-cloud.org 8 http://galaxyproject.org 5 2.2. Comparison to existing platforms for biomedical text analysis The two most well-known platforms that currently support scientific literature mining are the UIMA-based UCompare (Kano et al., 2008) and a more recently developed platform named Argo (Rak et al., 2012). Both of these systems allow the user to assemble modular pipelines and perform evaluations against a gold standard. U-Compare is plagued by instabilities of platform interoperability, permissions, and the like, and typically requires the intercession of a specialized software engineer. Argo attempts to ameliorate some of these problems by providing a webbased interface to the underlying UIMA-based system, but suffers from many of the same problems as U-Compare and is seemingly unsupported at this time. Frameworks that suppo"
macleod-etal-2000-american,ide-etal-2000-xces,1,\N,Missing
macleod-etal-2000-american,J93-2004,0,\N,Missing
macleod-etal-2000-american,A97-1015,0,\N,Missing
macleod-etal-2000-american,J95-4004,0,\N,Missing
macleod-etal-2000-american,P93-1032,0,\N,Missing
macleod-etal-2000-american,C94-1103,0,\N,Missing
macleod-etal-2000-american,W98-1102,1,\N,Missing
P00-1053,P98-1011,0,0.019315,"g the hierarchical discourse structure of texts, one can increase the potential of natural language systems to correctly determine co-referential links, which is a requirement for correctly resolving anaphors. In general, the potential to correctly determine coreferential links was greater for VT than for linear models when one looks back 4 elementary discourse units. When looking back more than four units, the linear model was equally effective. Here, we compare VT to stack-based models of discourse structure based on Grosz and Sidner's (1986) (G&S) focus spaces (e.g., Hahn and Strübe, 1997; Azzam, et al., 1998). In these approaches, discourse segments are pushed on the stack as they are encountered in a linear traversal of the text. Before a dominating segment is pushed, subordinate segments that precede it are popped from the stack. Antecedents for REs appearing in the segment on the top of the stack are sought in discourse segments in the stack below it. Therefore, in cases where a subordinate segment a precedes a dominating segment b, a reference to an entity in a by an RE in b is not resolvable. Special provision could be made in order to handle such cases—e.g., subsequently pushing a on top of"
P00-1053,P98-1044,1,0.868092,"d 2, the vein expression of unit 3, which contains units 1 In general, co-referential relations (such as the identity relation) induce equivalence classes over the set of referential expressions in a text. When hierarchical adjacency is considered, an anaphor may be resolved to a referent that is not the closest in a linear interpretation of a text. However, because referential expressions are organized in equivalence classes, it is sufficient that an anaphor is resolved to some member of the set. This is consistent with the distinction between &quot;direct&quot; and &quot;indirect&quot; references discussed in (Cristea, et al., 1998). and 3, suggests that anaphors from unit 3 should be resolved only to referential expressions in units 1 and 3. Because unit 2 is a satellite to unit 1, it is considered to be “blocked” to referential links from unit 3. In contrast, the DRA of unit 9, consisting of units 1, 8, and 9, reflects the intuition that anaphors from unit 9 can be resolved only to referential expressions from unit 1, which is the most important unit in span [1,7] and to unit 8, a satellite that immediately precedes unit 9. Figure 2 shows the heads and veins of all internal nodes in the rhetorical representation. H = 1"
P00-1053,J86-3001,0,0.24904,"Missing"
P00-1053,P97-1014,0,0.0223913,"howed that by exploiting the hierarchical discourse structure of texts, one can increase the potential of natural language systems to correctly determine co-referential links, which is a requirement for correctly resolving anaphors. In general, the potential to correctly determine coreferential links was greater for VT than for linear models when one looks back 4 elementary discourse units. When looking back more than four units, the linear model was equally effective. Here, we compare VT to stack-based models of discourse structure based on Grosz and Sidner's (1986) (G&S) focus spaces (e.g., Hahn and Strübe, 1997; Azzam, et al., 1998). In these approaches, discourse segments are pushed on the stack as they are encountered in a linear traversal of the text. Before a dominating segment is pushed, subordinate segments that precede it are popped from the stack. Antecedents for REs appearing in the segment on the top of the stack are sought in discourse segments in the stack below it. Therefore, in cases where a subordinate segment a precedes a dominating segment b, a reference to an entity in a by an RE in b is not resolvable. Special provision could be made in order to handle such cases—e.g., subsequentl"
P00-1053,W99-0307,0,0.195222,"r example, one RST analysis of (1) proposed by Moser and Moore (1996) is given in Figure 3. Moser and Moore note that the relation of an RST nucleus to its satellite is analogous to the dominates relation proposed by G&S (see also Marcu, 2000). As a subordinate segment preceding the segment that dominates it, the satellite is popped from the stack before the dominant segment (the nucleus) is pushed in the stack-based model, and therefore it is not included among the discourse segments that are searched to resolve co-references.3 Similarly, the text in (2), taken from the MUC annotated corpus (Marcu, et al., 1999), was assigned the RST structure in Figure 4, which presents the same problem for the stack-based approach: the referent for this in C2 is to the Clinton program in A2, but because it is a subordinate segment, it is no longer on the stack when C2 is processed. (1) A1. George Bush supports big business. B1. He's sure to veto House Bill 1711. evidence A1 B1 Figure 3: RST analysis of (1) 3 Note that Moser and Moore (1996) also propose an informational RST structure for the same text, in which a « volitional-cause » relation holds between the nucleus a and the satellite b, thus providing for a to"
P00-1053,J96-3006,0,0.20976,"REs appearing in the segment on the top of the stack are sought in discourse segments in the stack below it. Therefore, in cases where a subordinate segment a precedes a dominating segment b, a reference to an entity in a by an RE in b is not resolvable. Special provision could be made in order to handle such cases—e.g., subsequently pushing a on top of b—but this would violate the overall strategy of resolving REs appearing in segments currently on the top of the stack. The special status given to left satellites in VT addresses this problem. For example, one RST analysis of (1) proposed by Moser and Moore (1996) is given in Figure 3. Moser and Moore note that the relation of an RST nucleus to its satellite is analogous to the dominates relation proposed by G&S (see also Marcu, 2000). As a subordinate segment preceding the segment that dominates it, the satellite is popped from the stack before the dominant segment (the nucleus) is pushed in the stack-based model, and therefore it is not included among the discourse segments that are searched to resolve co-references.3 Similarly, the text in (2), taken from the MUC annotated corpus (Marcu, et al., 1999), was assigned the RST structure in Figure 4, whi"
P00-1053,J81-4001,0,0.404319,"Missing"
P00-1053,C00-1076,0,\N,Missing
P00-1053,C00-1031,1,\N,Missing
P00-1053,C98-1044,1,\N,Missing
P00-1053,C98-1011,0,\N,Missing
P01-1040,ide-etal-2000-xces,1,0.822453,"actic tagging, syntactic annotation, co-reference annotation, etc.), which can be instantiated in different ways depending on the annotators approach and goals. We have implemented both the abstract model and various instantiations using XML schemas (Thompson, et al., 2000), the Resource Definition Framework (RDF) (Lassila and Swick, 2000) and RDF schemas (Brickley and Guha, 2000), which enable description and definition of abstract data models together with means to interpret, via the model, information encoded according to different conventions. The results have been incorporated into XCES (Ide, et al., 2000a), part of the EAGLES Guidelines developed by the Expert Advisory Group on Language Engineering Standards (EAGLES)1. The XCES provides a ready-made, standard encoding format together with a data architecture designed specifically for linguistically annotated corpora. In this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation. The framework has been applied to the representation of terminology (Terminological Markup Framework2, ISO project n.16642) and computational lexicons (Ide, et al., 2000b), thus demonstrating its general"
P01-1040,A97-1011,0,0.0573664,"Missing"
P10-2013,W09-3021,1,0.0814968,"is an XML serialization of the LAF abstract model of annotations, which consists of a directed graph decorated with feature structures providing the annotation content. GrAF’s primary role is to serve as a “pivot” format for transducing among annotations represented in different formats. However, because the underlying data structure is a graph, the GrAF representation itself can serve as the basis for analysis via application of 3.1 WordNet Sense Annotations A focus of the MASC project is to provide corpus evidence to support an effort to harmonize sense distinctions in WordNet and FrameNet (Baker and Fellbaum, 2009), (Fellbaum and Baker, to appear). The WordNet and FrameNet teams have selected for this purpose 100 common polysemous words whose senses they will study in detail, and the MASC team is annotating occurrences of these words in the MASC. As a first step, fifty occurrences of each word are annotated using the WordNet 3.0 inventory and analyzed for problems in sense assignment, after which the WordNet team may make modifications to the inventory if needed. The revised inventory (which will be released as part of WordNet 3.1) is then used to annotate 1000 occurrences. Because of its small size, MA"
P10-2013,passonneau-etal-2010-word,1,0.87312,"Missing"
P10-2013,W07-1501,1,0.76262,"maximal benefit from the semantic information provided by these resources, the entire corpus is also annotated and manually validated for shallow parses (noun and verb chunks) and named entities (person, location, organization, date and time). Several additional types of annotation have either been contracted by the MASC project or contributed from other sources. The 220K words of MASC I and II include seventeen different types of linguistic annotation4 , shown in Table 2. All MASC annotations, whether contributed or produced in-house, are transduced to the Graph Annotation Framework (GrAF) (Ide and Suderman, 2007) defined by ISO TC37 SC4’s Linguistic Annotation Framework (LAF) (Ide and Romary, 2004). GrAF is an XML serialization of the LAF abstract model of annotations, which consists of a directed graph decorated with feature structures providing the annotation content. GrAF’s primary role is to serve as a “pivot” format for transducing among annotations represented in different formats. However, because the underlying data structure is a graph, the GrAF representation itself can serve as the basis for analysis via application of 3.1 WordNet Sense Annotations A focus of the MASC project is to provide"
P10-2013,ide-etal-2008-masc,1,0.866054,"Missing"
P10-2013,ide-etal-2010-anc2go,1,0.809614,"Missing"
P10-2013,J93-2004,0,0.0393414,"t of Computer Science Vassar College Poughkeepsie, NY, USA ide@cs.vassar.edu Collin Baker International Computer Science Institute Berkeley, California USA collinb@icsi.berkeley.edu Christiane Fellbaum Princeton University Princeton, New Jersey USA fellbaum@princeton.edu Rebecca Passonneau Columbia University New York, New York USA becky@cs.columbia.edu Abstract teen million word Open American National Corpus annotations are largely unvalidated. The most well-known multiply-annotated and validated corpus of English is the one million word Wall Street Journal corpus known as the Penn Treebank (Marcus et al., 1993), which over the years has been fully or partially annotated for several phenomena over and above the original part-of-speech tagging and phrase structure annotation. The usability of these annotations is limited, however, by the fact that many of them were produced by independent projects using their own tools and formats, making it difficult to combine them in order to study their inter-relations. More recently, the OntoNotes project (Pradhan et al., 2007) released a one million word English corpus of newswire, broadcast news, and broadcast conversation that is annotated for Penn Treebank sy"
P10-2013,W09-2402,1,0.836095,"Missing"
P10-2013,W03-0804,1,\N,Missing
P98-1044,P87-1022,0,0.607371,"Missing"
P98-1044,J95-2003,0,0.646943,"Missing"
P98-1044,J86-3001,0,0.499813,"Missing"
P98-1044,J96-3006,0,\N,Missing
P98-1044,P97-1012,1,\N,Missing
P98-1044,J92-4007,0,\N,Missing
P98-1044,P83-1007,0,\N,Missing
P98-1044,P96-1036,0,\N,Missing
P98-1044,J96-2005,0,\N,Missing
P98-1050,C94-1097,1,0.803308,"Missing"
passonneau-etal-2010-word,passonneau-etal-2006-inter,1,\N,Missing
passonneau-etal-2010-word,W99-0502,0,\N,Missing
passonneau-etal-2010-word,passonneau-2004-computing,1,\N,Missing
passonneau-etal-2010-word,W02-0805,0,\N,Missing
passonneau-etal-2010-word,W09-2402,1,\N,Missing
passonneau-etal-2010-word,W02-0808,1,\N,Missing
passonneau-etal-2010-word,P04-1039,0,\N,Missing
passonneau-etal-2010-word,J08-4004,0,\N,Missing
passonneau-etal-2010-word,D07-1107,0,\N,Missing
passonneau-etal-2012-masc,passonneau-etal-2010-word,1,\N,Missing
passonneau-etal-2012-masc,W10-1806,1,\N,Missing
passonneau-etal-2012-masc,passonneau-etal-2006-inter,1,\N,Missing
passonneau-etal-2012-masc,passonneau-2006-measuring,1,\N,Missing
passonneau-etal-2012-masc,W09-2402,1,\N,Missing
passonneau-etal-2012-masc,N06-2015,0,\N,Missing
passonneau-etal-2012-masc,J08-4004,0,\N,Missing
passonneau-etal-2012-masc,P03-2030,1,\N,Missing
tufis-etal-2004-word,barbu-2004-word,0,\N,Missing
tufis-etal-2004-word,C02-1002,1,\N,Missing
tufis-etal-2004-word,W02-0808,1,\N,Missing
W00-1506,bird-etal-2000-atlas,0,0.0620023,"Missing"
W00-1506,C96-2187,0,0.110875,"Missing"
W00-1506,C94-1097,1,0.880367,"Missing"
W00-1506,ide-etal-2000-xces,1,0.866593,"Missing"
W00-1506,A97-2017,0,\N,Missing
W02-0808,J96-2004,0,0.0176412,"nouns that occur a minimum of 10 times in the corpus, have no undetermined translations and at least five different translations in the six nonEnglish languages, and have the log likelihood score of at least 18; that is: LL(TT, TS) = 2 * 2 2 Â Â n ij * log j =1 i =1 n ij * n ** n i* * n * j ≥ 18 where nij stands for the number of times TT and TS have been seen together in aligned sentences, ni* and n*j stand for the number occurrences of TT and TS, respectively, and n** represents the total 4 We computed raw percentages only; common measures of annotator agreement such as the Kappa statistic (Carletta, 1996) proved to be inappropriate for our two-category (“yesno”) classification scheme. number of potential translation equivalents in the parallel corpus. The LL score is set at a maximum value to ensure high precision for the extracted translation equivalents, which minimizes sense clustering errors due to incorrect word alignment. Table 2 summarizes the data. No. of words 76 No. of example sentences 2399 Average examples/word 32 No. of senses (annotator 1) 241 No. of senses (annotator 2) 280 No. of senses (annotator 3) 213 No. of senses (annotator 4) 232 No. of senses (all annotators) 345 Average"
W02-0808,J94-4003,0,0.0379677,"Bucharest 74311, ROMANIA tufis@racai.ro to those that are realized lexically in some minimum subset of those languages. This idea would seem to provide an answer, at least in part, to the problem of determining different senses of a word: intuitively, one assumes that if another language lexicalizes a word in two or more ways, there must be a conceptual motivation. If we look at enough languages, we would be likely to find the significant lexical differences that delimit different senses of a word. Several studies have used parallel texts for WSD (e.g., Gale et al., 1993; Dagan et al., 1991; Dagan and Itai, 1994) as well as to define semantic properties of and relations among lexemes (Dyvik, 1998). More recently, two studies have examined the use of cross-lingual lexicalization as a criterion for validating sense distinctions: Ide (1999) used translation equivalents derived from aligned versions of Orwell’s Nineteen Eighty-Four among five languages from four different languages families, while Resnik and Yarowsky (2000) used translations generated by native speakers presented with isolated sentences in English. In both of these studies, translation information was used to validate sense distinctions p"
W02-0808,P91-1017,0,0.133913,"ea 13 Septembrie 13, Bucharest 74311, ROMANIA tufis@racai.ro to those that are realized lexically in some minimum subset of those languages. This idea would seem to provide an answer, at least in part, to the problem of determining different senses of a word: intuitively, one assumes that if another language lexicalizes a word in two or more ways, there must be a conceptual motivation. If we look at enough languages, we would be likely to find the significant lexical differences that delimit different senses of a word. Several studies have used parallel texts for WSD (e.g., Gale et al., 1993; Dagan et al., 1991; Dagan and Itai, 1994) as well as to define semantic properties of and relations among lexemes (Dyvik, 1998). More recently, two studies have examined the use of cross-lingual lexicalization as a criterion for validating sense distinctions: Ide (1999) used translation equivalents derived from aligned versions of Orwell’s Nineteen Eighty-Four among five languages from four different languages families, while Resnik and Yarowsky (2000) used translations generated by native speakers presented with isolated sentences in English. In both of these studies, translation information was used to valida"
W02-0808,C94-1097,1,0.739994,"building resources in the parallel languages (e.g., WordNets for the Eastern European languages in our study). In addition, if different senses of target words are identified in parallel texts, contextual information for different senses of a word can be gathered for use in disambiguating other, unrelated texts. The greatest obstacle to application of this approach is, obviously, the lack of parallel corpora: existing freely available parallel corpora including several languages are typically small (e.g., the Orwell), domain dependent (e.g. the MULTEXT Journal of the Commission (JOC) corpus; Ide and Véronis, 1994), and/or represent highly stylized language (e.g. the Bible; Resnik et al., 1999). Appropriate parallel data including Asian languages is virtually non-existent. Given that our method applies only to words for which different senses are lexicalized differently in at least one other language, its broad application depends on the future availability of large-scale parallel corpora including a variety of language types. Many studies have pointed out that coarser-grained sense distinctions can be assigned more reliably by human annotators than finer distinctions such as those in WordNet. In our st"
W02-0808,W99-0502,0,0.0612155,"Missing"
W02-0808,W97-0213,0,0.143334,"s to obtain large samples of “sense-tagged” data without the high cost of human annotation. 1 Introduction It is well known that the most nagging issue for word sense disambiguation (WSD) is the definition of just what a word sense is. At its base, the problem is a philosophical and linguistic one that is far from being resolved. However, work in automated language processing has led to efforts to find practical means to distinguish word senses, at least to the degree that they are useful for natural language processing tasks such as summarization, document retrieval, and machine translation. Resnik and Yarowsky (1997) suggest that for the purposes of WSD, the different senses of a word could be determined by considering only sense distinctions that are lexicalized cross-linguistically. In particular, they propose that some set of target languages be identified, and that the sense distinctions to be considered for language processing applications and evaluation be restricted Dan Tufis RACAI Romanian Academy Casa Academiei, Calea 13 Septembrie 13, Bucharest 74311, ROMANIA tufis@racai.ro to those that are realized lexically in some minimum subset of those languages. This idea would seem to provide an answer,"
W02-0808,W99-0627,0,0.0161471,"Missing"
W03-0804,P01-1040,1,0.753817,"e semantics of data categories included in annotations (whether they exist in the Registry or not) are well-defined and understood. The data model that will define the pivot format must be capable of representing all of the information contained in diverse annotation types. The model we assume is a feature structure graph for annotation information, capable of referencing n-dimensional regions of primary data as well as other annotations. The choice of this model is indicated by its almost universal use in defining general-purpose annotation formats, including the Generic Modeling Tool (GMT) (Ide & Romary, 2001, 2002) and Annotation Graphs (Bird & Liberman, 2001). The XML-based GMT could serve as a starting point for defining the pivot format; its applicability to diverse annotation types, including terminology, dictionaries and other lexical data (Ide, et al., 2000), morphological annotation (Ide & Romary, 2001a; 2003) and syntactic annotation (Ide & Romary, 2001b) demonstrates its generality. As specified by the LAF architecture, the GMT implements a feature structure graph, and exploits the hierarchical structure of XML elements and XML’s powerful interand intra-document pointing and linkage mech"
W03-0804,ide-romary-2002-standards,1,\N,Missing
W03-1901,P01-1040,1,0.907049,"n the primary data) 3 o super- and sub-segments, where groups of segments will comprise the parts of a larger segment (e.g., a contiguous word segments typically comprise a sentence segment) o discontinuous segments (linked continuous segments) LAF development has proceeded by first identifying an abstract data model that can formally describe linguistic annotations, distinct from any particular representation (as defined in the previous section). Development of this model has been discussed extensively within the language engineering community and tested on a variety of annotation types (see Ide and Romary, 2001a, 2001b, 2002). The data model forms the core of the framework by serving as the reference point for all annotation representation schemes. landmarks (e.g. time stamps) that note a point in the primary data In current practice, segmental information may or may not appear in the document containing the primary data itself. Documents considered to be read-only, for example, might be segmented by specifying byte offsets into the o LAF overview The overall design of LAF is illustrated in Figure 1. The fundamental principle is that the user controls the representation format for linguistic annotat"
W03-1901,ide-romary-2002-standards,1,0.31302,". Alternatively, users may define their own data categories or establish variants of categories in the registry; in such cases, the newly defined data categories will be formalized using the same format as definitions available in the registry. 5 5.1 Implementation Dump format The dump format is instantiated in XML. Structural nodes are represented as XML elements. The XML-based GMT will serve as a starting point for defining the dump format. Its applicability to diverse annotation types, including terminology, dictionaries and other lexical data (Ide, et al., 2000), morphological annotation (Ide and Romary, 2002) and syntactic annotation (Ide and Romary, 2001b, 2003) demonstrates its generality. As specified by the LAF architecture, the GMT implements a feature structure graph. Structural nodes in the graph are represented with the XML element <struct>. <brack> and <alt> elements are provided as grouping tags to handle aggregation (grouping) and alternatives (disjunction), as described above. A <feat> element is used to express category/value pairs. All of these elements are recursively nestable. Therefore, hierarchical relations among annotations and annotation components can be expressed via XML syn"
W03-1901,W03-0804,1,0.435409,"terminologies (which have already been treated in ISO/TC 37). The worldwide use of ISO/TC 37/SC 4 standards should improve information management within industrial, technical and scientific environments, and increase efficiency in computersupported language communication. Within ISO/TC 37/SC 4, a working group (WG11) has been established to develop a Linguistic Annotation Framework (LAF) that can serve as a basis for harmonizing existing language resources as well as developing new ones. The overall design of the architecture and the data model that it will instantiate have been described in Ide et al., 2003. In this paper we provide a description of the data model and its instantiations in LAF, in order to enable annotators to begin to explore how their schemes will map into the framework. 2 Terms and definitions The following terms and definitions are used in the discussion that follows: Annotation: The process of adding linguistic information to language data (“annotation of a corpus”) or the linguistic information itself (“an annotation”), independent of its representation. For example, one may annotate a document for syntax using a LISP-like representation, an XML representation, etc. Repres"
W03-1901,W03-1905,1,\N,Missing
W03-1905,atkins-etal-2002-resources,1,0.888583,"Missing"
W06-2716,W03-0804,1,\N,Missing
W07-1501,C02-1114,0,0.0322418,"xchange. Overview Graph theory provides a well-understood model for representing objects that can be viewed as a connected set of more elementary sub-objects, together with a wealth of graph-analytic algorithms for information extraction and analysis. As a result, graphs and graph-analytic algorithms are playing an increasingly important role in language data analysis, including finding related web pages (Kleinberg, 1999; Dean and Henzinger, 1999; Brin, 1998; Grangier and Bengio, 2005), patterns of web access (McEneaney, 2001; Zaki, 2002), and the extraction of semantic information from text (Widdows and Dorow, 2002; Krizhanovsky, 2005; Nastase and Szpakowicz, 2006). Recently, there has been work that treats linguistic annotations as graphs (Cui et al., 2005; Bunescu and Mooney, 2006; Nguyen et al., 2007; Gabrilovich and Markovitch, 2007) in order to identify, for example, measures of semantic similarity based on common subgraphs. As the need to merge and study linguistic annotations for multiple phenomena becomes increasingly important for language analysis, it is essential to identify a general model that can capture the relevant information and enable efficient and effective analysis. Graphs have long"
W07-1501,ide-romary-2004-registry,1,0.866946,"but also sets of merged annotations as a single graph. To demonstrate this, we have automatically transduced several different annotations of the Wall Street Journal corpus into GrAF and show how the annotations can then be merged, analyzed, and visualized using standard graph algorithms and tools. We also discuss how, as a standard graph representation, it allows for the application of well-established graph traversal and analysis algorithms to produce information about interactions and commonalities among merged annotations. GrAF is an extension of the Linguistic Annotation Framework (LAF) (Ide and Romary, 2004, 2006) developed within ISO TC37 SC4 and as such, implements state-of-the-art best practice guidelines for representing linguistic annotations. 1 Introduction Although linguistic annotation of corpora has a long history, over the past several years the need for corpora annotated for a wide variety of phenomena has come to be recognized as critical for the future development of language processing applications. Considerable attention has been devoted to the development of means to represent annotations so that phenomena at different levels can be merged and/or analyzed in combination. A partic"
W07-1501,ide-romary-2006-representing,1,0.687552,"Missing"
W07-1501,W06-3813,0,0.014293,"l-understood model for representing objects that can be viewed as a connected set of more elementary sub-objects, together with a wealth of graph-analytic algorithms for information extraction and analysis. As a result, graphs and graph-analytic algorithms are playing an increasingly important role in language data analysis, including finding related web pages (Kleinberg, 1999; Dean and Henzinger, 1999; Brin, 1998; Grangier and Bengio, 2005), patterns of web access (McEneaney, 2001; Zaki, 2002), and the extraction of semantic information from text (Widdows and Dorow, 2002; Krizhanovsky, 2005; Nastase and Szpakowicz, 2006). Recently, there has been work that treats linguistic annotations as graphs (Cui et al., 2005; Bunescu and Mooney, 2006; Nguyen et al., 2007; Gabrilovich and Markovitch, 2007) in order to identify, for example, measures of semantic similarity based on common subgraphs. As the need to merge and study linguistic annotations for multiple phenomena becomes increasingly important for language analysis, it is essential to identify a general model that can capture the relevant information and enable efficient and effective analysis. Graphs have long been used to describe linguistic annotations, most"
W07-1529,W06-2810,0,0.0533584,"Missing"
W07-1529,ide-suderman-2004-american,1,0.897407,"ion adequately handles relative pronouns, a new project that is annotating coreference is less likely to include relative pronouns in their annotation; and (b) The use of particular test corpora to determine whether a particular annotation task can feasibly achieve good agreement scores. (c) The use of underlying models for representing annotation content that facilitate merging, comparison, and analysis. This study will focus on the problem of identifying such corpora as well as the suitability of two candidate corpora: the Open portion of the American National Corpus (Ide and Macleod, 2001; Ide and Suderman, 2004) and the “Controversial” portions of the WikipediaXML corpus (Denoyer and 184 Proceedings of the Linguistic Annotation Workshop, pages 184–190, c Prague, June 2007. 2007 Association for Computational Linguistics (d) To the extent possible, the use of common annotation categories or a mapping among categories for the same phenomenon used by different annotation groups. In selecting shared corpora, we believe that the following issues must be taken into consideration: 1. The diversity of genres, lexical items and linguistic phenomena – this will ensure that the corpora will be useful to many dif"
W07-1529,ide-suderman-2006-integrating,1,0.846263,". ANC annotations are distributed as stand-off documents representing a set of graphs over the primary data, thus allowing for layering of annotations and inclusion of multiple annotations of the same type. Because most existing tools for corpus access and manipulation do not handle stand-off annotations, we have developed an easy-to-use tool and user interface to merge the user’s choice of stand-off annotations with the primary data to form a single document in any of several XML and non-XML formats, which is distributed with the corpus. The ANC architecture and format is described fully in (Ide and Suderman, 2006). 2.1 The ULA Subcorpus The Unified Linguistic Annotation (ULA) project has selected a 40,000 word subcorpus of the Open ANC for annotation with several different annotation schemes including: the Penn Treebank, PropBank, NomBank, the Penn Discourse Treebank, TimeML and Opinion Annotation.2 This initial subcorpus can be broken down as follows: • Spoken Language – charlotte: 5K words – switchboard: 5K words • Slate (Journal): 5K words • Travel guides: 5K words • 911report: 5K words • OUP books (Kaufman): 5K words As the ULA project progresses, the participants intend to expand the corpora annot"
W07-1529,N07-2032,0,0.0226548,"n 3.4) 5. and the corpus has various other properties that many researchers feel would be interesting to exploit. To date research in Computational Linguistics using Wikipedia includes: Automatic derivation of taxonomy information (Strube and Ponzetto, 2006; Suchanek et al., 2007; Zesch and Gurevych, 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.science.uva.nl/WiQA/); and information retrieval (Fuhr et al., 2006). Some other interesting properties of Wikipedia that have yet to be explored to our knowledge include: (1) Most main articles have talk pages which discuss them – perhaps this relation can be exploited by systems which try to detect discussions about topics, e.g., searches for discussions about curr"
W07-1529,N07-3003,0,0.014895,"atistical systems); • letters: 10K words 2. the textual information is well structured 2 Other corpora being annotated by the ULA project include sections of the Brown corpus and LDC parallel corpora. 186 3. Wikipedia is a large and growing corpus 4. the articles are multilingual (cf. section 3.4) 5. and the corpus has various other properties that many researchers feel would be interesting to exploit. To date research in Computational Linguistics using Wikipedia includes: Automatic derivation of taxonomy information (Strube and Ponzetto, 2006; Suchanek et al., 2007; Zesch and Gurevych, 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.science.uva.nl/WiQA/); and information retrieval (Fuhr et al., 2006). So"
W07-1529,W07-0201,0,0.0149065,"lexical information for statistical systems); • letters: 10K words 2. the textual information is well structured 2 Other corpora being annotated by the ULA project include sections of the Brown corpus and LDC parallel corpora. 186 3. Wikipedia is a large and growing corpus 4. the articles are multilingual (cf. section 3.4) 5. and the corpus has various other properties that many researchers feel would be interesting to exploit. To date research in Computational Linguistics using Wikipedia includes: Automatic derivation of taxonomy information (Strube and Ponzetto, 2006; Suchanek et al., 2007; Zesch and Gurevych, 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.science.uva.nl/WiQA/); and information retrieval (Fuhr"
W09-2402,J08-4004,0,0.258989,"eport Cohen’s κ; note the similarity in values3 . As with the various agreement coefficients that factor out the agreement that would occur by chance, values range from 1 for perfect agreement and -1 for perfect opposition, to 0 for chance agreement. While there are no hard and fast criteria for what constitutes good IA, Landis and Koch (Landis and Koch, 1977) consider values between 0.40 and 0.60 to represent moderately good agreement, and values above 0.60 as quite good; Krippendorff (Krippendorff, 1980) considers values above 0.67 moderately good, and values above 0.80 as quite good. (cf. (Artstein and Poesio, 2008) for discussion of agreement measurement for computational linguistic tasks.) Table 2 shows IA for a pair of adjectives, nouns and verbs from our sample for which the IA scores are at the extremes (high and low) in each pair: the average delta is 0.24. Note that the agreement decreases as part-of-speech varies from adjectives to nouns to verbs, but for all three parts-of-speech, there is a wide spread of values. It is striking, given that the same annotators did all words, that one in each pair has relatively better agreement. 3 α handles multiple annotators; Arstein and Poesio (Artstein and P"
W09-2402,W02-0805,0,0.237598,"Missing"
W09-2402,P04-1039,0,0.110351,"Missing"
W09-2402,W02-0808,1,0.90498,"Missing"
W09-2402,W99-0502,0,0.672072,"he variations in IA. We conclude with a summary of our findings goals. 2 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 2–9, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Word fair long quiet land time work know say show tell POS Adj Adj Adj Noun Noun Noun Verb Verb Verb Verb No. senses 10 9 6 11 10 7 11 11 12 8 No. occurrences 463 2706 244 1288 21790 5780 10334 20372 11877 4799 Table 1: Ten Words 2005). Pedersen (Pedersen, 2002a) examines variation across individual words in evaluating WSD systems, but does not attempt to explain it. Factors that have"
W09-2402,passonneau-etal-2006-inter,1,0.905734,"Missing"
W09-2402,passonneau-2004-computing,1,0.862694,"ed 8 5 7 8 8 11 Table 2: Varying interannotator agreement across words We expected to find varying levels of interannotator agreement (IA) among all six annotators, depending on obvious grouping factors such as the part of speech, or the number of senses per word. We do find widely varying levels of agreement, but as described here, most of the variation does not depend on these a priori factors. Inherent usage properties of the words themselves, and systematic patterns of variation across annotators, seem to be the primary factors, with a secondary effect of part of speech. In previous work (Passonneau, 2004), we have discussed why we use Krippendorff’s α (Krippendorff, 1980), and for purposes of comparison we also report Cohen’s κ; note the similarity in values3 . As with the various agreement coefficients that factor out the agreement that would occur by chance, values range from 1 for perfect agreement and -1 for perfect opposition, to 0 for chance agreement. While there are no hard and fast criteria for what constitutes good IA, Landis and Koch (Landis and Koch, 1977) consider values between 0.40 and 0.60 to represent moderately good agreement, and values above 0.60 as quite good; Krippendorff"
W09-2402,J91-4003,0,0.397183,"es in word usage. It has been widely observed that usage features such as vocabulary and syntax vary across corpora of different genres and registers (Biber, 1995), and that serve different functions (Kittredge et al., 1991). Still, we are far from able to predict specific morphosyntactic and lexical variations across corpora (Kilgarriff, 2001), much less quantify them in a way that makes it possible to apply the same analysis tools (taggers, parsers) without retraining. In comparison to morphosyntactic properties of language, word and phrasal meaning is fluid, and to some degree, generative (Pustejovsky, 1991; 2 Nancy Ide Department of Computer Science Vassar College Poughkeepsie, NY, USA ide@cs.vassar.edu Nunberg, 1979). Based on our initial observations from a word sense annotation task for relatively polysemous words, carried out by multiple annotators on a heterogeneous corpus, we hypothesize that different words lead to greater or lesser interannotator agreement (IA) for reasons that in the long run should be explicitly modelled in order for Natural Language Processing (NLP) applications to handle usage differences more robustly. This pilot study is a step in that direction. We present relate"
W09-2402,D07-1107,0,0.0851426,"Missing"
W09-2402,W02-0812,0,\N,Missing
W09-2402,W02-0806,0,\N,Missing
W09-3004,W07-1501,1,0.796578,"and Suderman, 2007) and the data formats utilized by two general-purpose annotation systems: the General Architecture for Text Engineering (GATE) (Cunningham, 2002) and the Unstructured Information Management Architecture (UIMA)1 . UIMA and GATE are similar in design and purpose: both represent documents as text plus annotations and allow users to define pipelines of processes that manipulate the document. However, there are some differences in implementation and representation format that prohibit direct exchange of data and annotations between the two. The Graph Annotation Framework (GrAF) (Ide and Suderman, 2007) is intended to serve as a “pivot” to enable interoperability among different formats for data and linguistics annotations and the systems that create and exploit them. In this paper, we describe the steps required to perform a round-trip rendering from GrAF to GATE and GrAF to UIMA CAS and back again, and outline the commonalities as well as the differences and gaps that came to light in the process. In doing so, we hope to shed some light on the design and implementation choices that either contribute to or impede progress toward interoperability, which can feed future development. This pape"
W09-3004,ide-etal-2000-xces,1,0.836929,"re a kind of part of speech annotation. The hierarchy does not reflect other kinds of relations such as the relation between a ”lemma” annotation and Without a source of external knowledge, GrAF does not attempt to make any assumptions about the annotations and features in the graph. However, all of these problems are avoided by providing an XML Schema or other source of information about the GrAF annotations that can be 29 used when generating the type system. The XML schema can specify the type hierarchy, data types and restricted ranges for feature values, etc. (see, for example, the XCES (Ide et al., 2000) schema is used for the data and annotations in the American National Corpus (ANC)4 .) precedes[B,A] == true then it is assumed that the annotation types have the same priority. Once the list of annotation types has been collected and the precedence matrix constructed, the matrix can be used to to sort the annotation types: 3.2 int compare(Annotation A, Annotation B, PrecedenceMatrix m) { boolean AB = m.precedes(A,B); boolean BA = m.precedes(B,A); if (AB && BA) { return 0; // equal } else if (AB) { return -1; // A first. } else if (BA) { return 1; // B first. } // Neither AB or BA means A and"
W10-1806,W04-2323,0,0.0356907,"Missing"
W10-1806,W02-0805,0,0.0241892,"5a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part of speech, with the number of WordNet senses, the number of senses used by the trained annotators (TAs), the number of annotators, and Alpha. We call this the Trained annotator (TA) data. We find that interannotator agreement (IA) among half a dozen annotators varies depending on the word. For ten words nearly balanced with 2 3 49 http://w"
W10-1806,P04-1039,0,0.0209073,"o investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part of speech, with the number of WordNet senses, the number of senses used by the trained annotators (TAs), the number of annotators, and Alpha. We call this the Trained annotator (TA) data. We find that interannotator agreement (IA) among half a dozen annotators varies depending on the word. For ten words nearly balanced with 2 3 49 http://www.anc.org http://www"
W10-1806,W09-3953,1,0.886481,"Missing"
W10-1806,W02-0808,1,0.797594,"(WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part of speech, with the number of WordNet senses, the number of senses used by the trained annotators (TAs), the number of annotators, and Alpha. We call this the Trained annota"
W10-1806,P10-2013,1,0.866373,"Missing"
W10-1806,H05-1073,0,0.0809682,"Missing"
W10-1806,J08-4004,0,0.123441,"artificial, annotators might disagree on word senses because they disagree on the boundaries between one sense and another, just as professional lexicographers do. Beyond Interannotator Agreement (IA) Assessing the reliability of an annotation typically addresses the question of whether different annotators (effectively) assign the same annotation labels. Various measures can be used to compare different annotators, including agreement coefficients such as Krippendorff’s alpha (Krippendorff, 1980). Extensive reviews of the properties of such coefficients have been presented elsewhere, e.g., (Artstein and Poesio, 2008). Briefly, an agreement produce values in the interval [-1,1] indicating how much of the observed agreement is above (or below) agreement that would be predicted by chance (value of 0). To measure reliability in this way is to assume that for most of the instances in the data, there is a single correct response. Here we present the use of reliability metrics and other measures for word sense annotation, and we assume that in some cases there may not be a single correct response. When annotators have less than excellent agreement, we aim to examine possible causes. Apart from the artificiality"
W10-1806,D07-1107,0,0.055535,"Missing"
W10-1806,D08-1027,0,0.583911,"Missing"
W10-1806,W99-0502,0,0.0209562,"age cohort, account for the disagreement. 4 Word Sense Annotation Data Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part"
W10-1806,J05-1004,0,0.0104773,"hen annotators disagree, having multiple annotators is necessary in order to determine whether the disagreement is due to noise based on insufficiently clear sense definitions versus a systematic difference between individuals, e.g., those who see a glass as half empty where others see it as half full. To insure the opportunity to observe how varied the labeling of a single word can be, we collect word sense annotations from multiple annotators. One potential benefit of such investigation might be a better understanding of how to model word meaning. In sum, we hypothesize the following cases: Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, depending on whether t"
W10-1806,passonneau-etal-2006-inter,1,0.845777,"ss languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part of speech, with the number of WordNet senses, the number of senses used by the trained annotators (TAs), the number of annotators, and Alpha. We call this the Trained annotator (TA) data. We find tha"
W10-1806,passonneau-etal-2008-relation,1,0.852796,"s in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, depending on whether the disagreement is due to noise or systematic behavior. Noise has relatively little impact compared with systematic disagreements. In (Passonneau et al., 2008), a similar lack of correlation between interannotator agreement and machine learning performance is found in an empirical investigation. • Outliers: A small proportion of annotators may assign senses in a manner that differs markedly from the remaining annotators. • Confusability of senses: If multiple annotators assign multiple senses in an apparently random fashion, it may be that the senses are not sufficiently distinct. 5 5.1 Trained Annotator data The Manually Annotated Sub-Corpus (MASC) project (Ide et al., 2010) is creating a small, representative corpus of American English written and"
W10-1806,J08-3001,0,0.0240304,"w varied the labeling of a single word can be, we collect word sense annotations from multiple annotators. One potential benefit of such investigation might be a better understanding of how to model word meaning. In sum, we hypothesize the following cases: Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, depending on whether the disagreement is due to noise or systematic behavior. Noise has relatively little impact compared with systematic disagreements. In (Passonneau et al., 2008), a similar lack of correlation between interannotator agreement and machine learning performance is found in an empirical investigation. • Outliers: A small proportion of annotators may assig"
W10-1806,W02-0812,0,\N,Missing
W10-1806,W02-0806,0,\N,Missing
W10-1840,P98-1013,0,0.0615463,"ication are called its concrete and abstract syntax, respectively. A distinguishing feature of the proposed methodology is that the semantics is defined for the structures of the abstract syntax, rather than for the expressions that represent these structures. In this paper, we generalize the design methodology defined in (Bunt, 2010) and demonstrate its use for generating a mapping from an existing annotation scheme to a representation in GrAF format. By way of illustration, we apply the mapping strategy to annotations from ISOTimeML (ISO, 2009), PropBank (Palmer et al., 2005), and FrameNet (Baker et al., 1998). In this paper, we apply the annotation scheme design methodology defined in (Bunt, 2010) and demonstrate its use for generating a mapping from an existing annotation scheme to a representation in GrAF format. The most important features of this methodology are (1) the distinction of the abstract and concrete syntax of an annotation language; (2) the specification of a formal semantics for the abstract syntax; and (3) the formalization of the relation between abstract and concrete syntax, which guarantees that any concrete syntax inherits the semantics of the abstract syntax, and thus guarant"
W10-1840,W07-1501,1,0.847952,"bstract syntax; and (3) the formalization of the relation between abstract and concrete syntax, which guarantees that any concrete syntax inherits the semantics of the abstract syntax, and thus guarantees meaning-preserving mappings between representation formats. By way of illustration, we apply this mapping strategy to annotations from ISOTimeML, PropBank, and FrameNet. 1 Introduction The Linguistic Annotation Framework (LAF, (Ide and Romary, 2004); ISO 24612, 2009) defines an abstract model for annotations together with an XML serialization of the model, the Graph Annotation Format (GrAF, (Ide and Suderman, 2007)). GrAF is intended to be a pivot format capable of representing diverse annotation types of varying complexity, guaranteeing syntactic consistency among the different annotations. GrAF does not address the issue of semantic consistency among annotation labels and categories; this is assumed to be handled by other standardization efforts such as ISOCat (Kemps-Snijders et al., 2009). ISOCat provides a set of data categories at various levels of granularity, each accompanied by a precise definition of its linguistic meaning. Labels applied in a user-defined annotation scheme 247 Proceedings of t"
W10-1840,J05-1004,0,0.0323261,"wo components of the language specification are called its concrete and abstract syntax, respectively. A distinguishing feature of the proposed methodology is that the semantics is defined for the structures of the abstract syntax, rather than for the expressions that represent these structures. In this paper, we generalize the design methodology defined in (Bunt, 2010) and demonstrate its use for generating a mapping from an existing annotation scheme to a representation in GrAF format. By way of illustration, we apply the mapping strategy to annotations from ISOTimeML (ISO, 2009), PropBank (Palmer et al., 2005), and FrameNet (Baker et al., 1998). In this paper, we apply the annotation scheme design methodology defined in (Bunt, 2010) and demonstrate its use for generating a mapping from an existing annotation scheme to a representation in GrAF format. The most important features of this methodology are (1) the distinction of the abstract and concrete syntax of an annotation language; (2) the specification of a formal semantics for the abstract syntax; and (3) the formalization of the relation between abstract and concrete syntax, which guarantees that any concrete syntax inherits the semantics of th"
W10-1840,pustejovsky-etal-2010-iso,1,0.791774,"f the primary data. ISO-TimeML link structures define a relation between two entity structures, and are rendered in GrAF as a labeled edge between the nodes annotated with the entity structure information. In the ISO-TimeML example, an annotation with label MLINK (‘measure link’) is created with a single feature relType. The from and to attributes on the &lt;edge> element link the node with the EVENT entity structure annotation (node tml-n1 in the example) to the node with the TIME-AMOUNT annotation (tml-n2). This edge is then associated with the MLINK annotation (cf. Bunt and Pustejovsky, 2009; Pustejovsky et al., 2010). Figure 1 shows the rendering of the ISOTimeML abstract syntax in the ICS-1 concrete syntax. Following Section 3.2, these two realizations of the abstract syntax for ISO-TimeML are isomorphic. &lt;node xml:id=""tml-n1""/> &lt;a label=""EVENT"" ref=""tml-n1"" as=""TimeML""> &lt;fs> &lt;f name=""event"" value=""fast""/> &lt;f name=""tense"" value=""Past""/> &lt;f name=""signature"" value=""individual""/> &lt;/fs> &lt;/a> &lt;edge xml:id=""tml-e1"" from=""tml-n1"" to=""t2""/> ISO-TimeML example &lt;node xml:id=""tml-n2""/> &lt;a label=""TIME-AMOUNT"" ref=""tml-n2"" as=""TimeML""> &lt;fs> &lt;f name=""numeral"" value=""2""/> &lt;f name=""unit"" value=""day""/> &lt;/fs> &lt;/a> The GrA"
W10-1840,C98-1013,0,\N,Missing
W10-1840,W03-0804,1,\N,Missing
W12-3608,auer-etal-2010-elan,0,0.0174284,"are connected to other nodes in the same or other annotation documents via outgoing edges; • Header documents associated with each primary data document and annotation document, and a resource header that provides information about the resource as whole. GrAF Overview GrAF has been developed with ISO TC37 SC4 to provide a general framework for representing linguistically annotated resources. Its design has been informed by previous and current approaches and tools, including but not limited to UIMA CAS(Ferrucci and Lally, 2004), GATE (Cunningham et al., 2002), ANVIL (Kipp, Forthcoming), ELAN (Auer et al., 2010), and the NLP Interchange Format (NIF)5 under development within the Linked Open Data (LOD) effort6 . The approach has been to develop a lingua franca or “pivot” format into and out of which other models may be translated in order to enable exchange among systems.7 In order to serve this purpose, the GrAF data model was designed to capture the relevant structural generalization underlying best practices for linguistic annotation, which is the directed (acyclic) graph. The overall architecture of a linguisticallyannotated resource rendered in GrAF consists of the following: We describe the GrAF"
W12-3608,chiarcos-2012-ontologies,0,0.104935,"aph Annotation Format (GrAF) (Ide and Suderman, 2007; Ide and Suderman, Submitted). It provides mechanisms for describing the organization of the resource, documenting the conventions used in the resource, associating data and annotation documents, and defining and selecting defined portions of the resource and its annotations. It has been designed to accommodate the use of XML technologies for processing, including XPath, XSLT, and, by virtue of the system’s linkage strategy, RDF/OWL, and to accommodate linkage to web-based ontologies and data category registries such as the OLiA ontologies (Chiarcos, 2012) and ISOCat (Marc KempsSnijders and Wright, 2008). 1 Introduction While substantial effort has gone into defining standardized representation formats for linguistically annotated language resources, very little attention has been paid to standardizing the metadata and documentation practices associated with these resources (see, for example, (Ide and Pustejovsky, 2010)). Multiple techniques have been proposed to represent resource provenance, and a W3C Working Group1 has recently been convened to devise means 1 http://www.w3.org/2011/01/prov-wg-charter.html to enable provenance information to"
W12-3608,W07-1501,1,0.779314,"apabilities. This paper describes a comprehensive standard for resource description developed within ISO TC37 SC44 . The standard is instantiated in a system of XML headers that accompany data and annotation documents represented using the the Linguis2 http://www.ldc.upenn.edu http://www.elra.info 4 http://www.tc37sc4.org 3 57 Proceedings of the 6th Linguistic Annotation Workshop, pages 57–66, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics • One or more primary data documents, in any medium; tic Annotation Framework’s Graph Annotation Format (GrAF) (Ide and Suderman, 2007; Ide and Suderman, Submitted). It provides mechanisms for describing the organization of the resource, documenting the conventions used in the resource, associating data and annotation documents, and defining and selecting defined portions of the resource and its annotations. It has been designed to accommodate the use of XML technologies for processing, including XPath, XSLT, and, by virtue of the system’s linkage strategy, RDF/OWL, and to accommodate linkage to web-based ontologies and data category registries such as the OLiA ontologies (Chiarcos, 2012) and ISOCat (Marc Kemps-Snijders and"
W12-3608,ide-etal-2008-masc,1,0.906371,"Missing"
W12-3608,P10-2013,1,0.892805,"Missing"
W12-3608,kemps-snijders-etal-2008-isocat,0,\N,Missing
W13-2312,basile-etal-2012-developing,0,0.0178675,"ovides access to multiple annotation layers in the corpus. This access provides information about inter-layer relations and dependencies that have been previously difficult to explore, and which are highly valuable for continued development of language processing applications. 1 Introduction Over the past decade, corpora with multiple layers of linguistic annotation have been developed in order to extend the range of empirically-based linguistic research and enable study of inter-layer interactions. Recently created corpora include OntoNotes (Pradhan et al., 2007), the Groningen Meaning Bank (Basile et al., 2012), and the Manually Annotated Sub-Corpus (MASC)1 (Ide et al., 2010). Typically, such corpora are represented in idiosyncratic in-house formats, and developers provide special software to access and query the annotations (for example, the OntoNotes “db tool” and Groningen’s GMB Explorer). Access without the use of developer-supplied software often requires significant programming expertise, and as a result, it is not easy–or even possible–for others to add to or modify data and annotations in the resource. This paper describes the importation of MASC data and annotations into the linguistic data"
W13-2312,W07-1501,1,0.750868,"Importing MASC into the ANNIS linguistic database: A case study of mapping GrAF Arne Neumann Nancy Ide Manfred Stede EB Cognitive Science and SFB 632 Department of Computer Science EB Cognitive Science and SFB 632 University of Potsdam Vassar College neumana@uni-potsdam.de ide@cs.vassar.edu ANNIS2 (Chiarcos et al., 2008; Zeldes et al., 2009), which was designed to visualize and query linguistically-annotated corpora. Unlike most other corpora with multi-layer annotations, no special software has been developed for access to MASC. Instead, all MASC data and annotations are represented in GrAF (Ide and Suderman, 2007), the XML serialization of the abstract model for annotations defined by ISO TC37 SC4’s Linguistic Annotation Framework (ISO/LAF) (Ide and Suderman, In press). GrAF is intended to serve as a generic “pivot” format that is isomorphic to annotation schemes conforming to the abstract model and therefore readily mappable to schemes used in available systems. We outline the process of mapping GrAF to ANNIS’s internal format relANNIS and demonstrate how the system provides access to multiple annotation layers in MASC. Abstract This paper describes the importation of Manually Annotated Sub-Corpus (MA"
W13-2312,P10-2013,1,0.889118,"s provides information about inter-layer relations and dependencies that have been previously difficult to explore, and which are highly valuable for continued development of language processing applications. 1 Introduction Over the past decade, corpora with multiple layers of linguistic annotation have been developed in order to extend the range of empirically-based linguistic research and enable study of inter-layer interactions. Recently created corpora include OntoNotes (Pradhan et al., 2007), the Groningen Meaning Bank (Basile et al., 2012), and the Manually Annotated Sub-Corpus (MASC)1 (Ide et al., 2010). Typically, such corpora are represented in idiosyncratic in-house formats, and developers provide special software to access and query the annotations (for example, the OntoNotes “db tool” and Groningen’s GMB Explorer). Access without the use of developer-supplied software often requires significant programming expertise, and as a result, it is not easy–or even possible–for others to add to or modify data and annotations in the resource. This paper describes the importation of MASC data and annotations into the linguistic database 1 University of Potsdam stede@uni-potsdam.de 2 The ANNIS Infr"
W14-3005,W09-3021,0,0.0602874,"Missing"
W14-3005,de-melo-etal-2012-empirical,1,0.906679,"Missing"
W14-3005,ovchinnikova-etal-2010-data,0,0.0220778,"recently, FrameNet was ported to RDF/OWL for inclusion in the Linked Open Data (LOD) cloud2 (Nuzzolese et al., 2011). The possibility of linking WordNet and FrameNet in the Semantic Web has also spawned efforts such as (Bryl et al., 2012) that build on numerous efforts over the past several years to align and/or extend these two resources (Burchardt et al., 2005; Ide, 2006; De Cao et al., 2008; de Melo et al., 2012; Bryl et al., 2012). Others have analyzed FrameNet in order to formalize its semantics so as to be appropriate for use with Description Logic (DL) reasoners compatible with OWL-DL (Ovchinnikova et al., 2010). Given all of the activity surrounding FrameNet as a resource for the Semantic Web, one would expect to see multiple examples of the use of Semantic Web implementations of FrameNet for NLP development and research. However, these examples do not exist, for two reasons. The first is a reality of the Semantic Web: simply put, the Semantic Web has not yet come to fruition, despite its having been around as a concept for well over a decade, and despite the development of a suite of W3C standard technologies to support it. FrameNet is the ideal resource for representation as linked data, and sever"
W14-3005,fillmore-etal-2002-framenet,0,0.0579394,"ment of a suite of W3C standard technologies to support it. FrameNet is the ideal resource for representation as linked data, and several renderings of the resource in RDF/OWL have been created. FrameNet has also been and continues to be linked to other major resources, including WordNet, BabelNet, and MASC, in the Linguistic Linked Open Data cloud. Although so far the supporting technologies have not enabled easy and widespread access to the envisioned massive network of language resources, a conflation of recent efforts suggests this may be a reality in the not-too-distant future. FrameNet (Fillmore et al., 2002; Ruppenhofer et al., 2006) is the ideal resource for representation in the Semantic Web (SW) as what is now widely known as “linked data”. The Semantic Web consists of objects whose properties are represented by named links to other objects that constitute their values and supports representing and reasoning over ontologies defined the the SW framework. FrameNet is also a complex semantic network linking lexical units to semantic frames, and semantic frames to one another in a shallow hierarchy, over which inheritance and sub-frame relations are defined. In sentences annotated for FrameNet fr"
W14-3005,passonneau-etal-2012-masc,1,0.893668,"Missing"
W14-3005,P10-2013,1,0.900423,"Missing"
W14-3005,kemps-snijders-etal-2008-isocat,0,0.0678563,"Missing"
W14-3005,moro-etal-2014-annotating,0,0.0412236,"Missing"
W14-3005,W03-1904,0,0.0907948,"Missing"
W14-3005,P10-1023,0,0.100802,"Missing"
W14-3005,W08-2208,0,\N,Missing
W14-3005,L10-1000,0,\N,Missing
W14-5204,cassidy-etal-2014-alveo,0,0.05905,"se grids by users of any one of them and, perhaps most importantly, facilitate adding additional grids and service platforms to the federation. Currently, the European META-NET initiative is committed to joining the federation in the near future. In addition to the projects listed above, we are also collaborating with several groups on technical solutions to achieve interoperability and in particular, on development of the WS-EV, the JSON-LD format, and a corollary development of an ontology of web service types. These collaborators include the Alveo Project (Macquarie University, Australia) (Cassidy et al., 2014), the Language Grid project, and the Lider project29 . We actively seek collaboration with others in order to move closer to achieving a “global laboratory” for language applications. 6 Conclusion In this paper, we have given a brief overview of the LAPPS Web Service Exchange Vocabulary (WSEV), which provides a terminology for a core of linguistic objects and features exchanged among NLP tools that consume and produce linguistically annotated data. The goal is to bring the field closer to achieving semantic interoperability among NLP data, tools, and services. We are actively working to both e"
W14-5204,P13-1166,0,0.106613,"Missing"
W14-5204,W09-3034,1,0.689856,"Grid provides a critical missing layer of functionality for NLP: although existing frameworks such as UIMA and GATE provide the capability to wrap, integrate, and deploy language services, they do not provide general support for service discovery, composition, and reuse. The LAPPS Grid is a collaborative effort among US partners Brandeis University, Vassar College, Carnegie-Mellon University, and the Linguistic Data Consortium at the University of Pennsylvania, and is funded by the US National Science Foundation (NSF). The project builds on the foundation laid in the NSF-funded project SILT (Ide et al., 2009), which established a set of needs for interoperability and developed standards and best practice guidelines to implement them. LAPPS is similar in its scope and goals to ongoing projects such as The Language Grid15 , PANACEA/MetaNET16 , LinguaGrid17 , and CLARIN18 , which also provide web service access to basic NLP processing tools and resources and enable pipelining these tools to create custom NLP applications and composite services such as question answering and machine translation, as well as access to language resources such as mono- and multilingual corpora and lexicons that support NL"
W14-5204,ide-etal-2014-language,1,0.862088,"Missing"
W14-5204,windhouwer-2012-relcat,0,\N,Missing
W14-5204,W14-5211,0,\N,Missing
W16-5202,P13-1166,0,0.0336972,"Missing"
W16-5202,gilmanov-etal-2014-swift,0,0.0168661,"allowing for their immediate inclusion in workflows supporting sophisticated applications as well as evaluation of their performance side-by-side with comparable components. Although many contributors host their own contributed services (which are called from within the LAPPS Grid), where necessary the LAPPS Grid provides hosting to ensure that software remains available to the community. Recently contributed tools include all core tools from University of Darmstadt’s DKPro3 , the AIFdb services for Argumentation analysis4 (Lawrence et al., 2012), the SWIFT Aligner for cross-lingual transfer (Gilmanov et al., 2014), the EDISON feature extraction framework5 (Sammons et al., 2016) and other tools available from the University of Illinois (e.g., semantic role labelers, entity extractors), among others. In addition, several of the basic components produced by the ARIEL team working within DARPA’s Low Resource Languages for Emergent Incidents (LORELEI) program have been integrated into the LAPPS Grid, which include tools and data to support a wide array of under-resourced languages. The LAPPS Grid has been adopted by a Mellon-funded project at the University of Illinois, which is utilizing the platform to ap"
W16-5202,W14-5204,1,0.848346,"s in views, where each view contains metadata that spells out the information contained in that view, including information necessary to determine compatibility with other tools and data. Semantic interoperability is achieved via references to definitions in the Web Services Exchange Vocabulary (WSEV). The WSEV has been built bottom up, driven by the needs of components in the LAPPS Grid and closely following standard practice in the field as well as adopting, where possible, existing terminology and type systems. Both LIF and the WSEV are described in detail elsewhere (Verhagen et al., 2016; Ide et al., 2014; Ide et al., 2016). Another distinctive feature of the LAPPS Grid is its Open Advancement (OA) Evaluation system, a sophisticated environment that was used to develop IBM’s Jeopardy-winning Watson. OA can be simultaneously applied to multiple variant workflows involving alternative tools for a given task, and the results are evaluated and displayed so that the best possible configuration is readily apparent. Similarly, the weak links in a chain are easily detected, which can lead to module-specific improvements that affect the entire process. The inputs, tools, parameters and settings used fo"
W16-5202,N16-3019,0,0.0252091,"considerable programming effort to add or modify components. Similarly, the Natural Language Toolkit (NLTK) requires Python programming and effectively limits the user to the tools that are built-in to the system. In contrast, modules can be easily added to the LAPPS Grid by wrapping them as a service, using provided templates; and, more importantly, no programming experience or technical expertise is required, since workflows are constructed using the Galaxy project’s workflow management framework. This makes the LAPPS Grid ideal for instructional use. The recently introduced Kathaa system (Mohanty et al., 2016) provides functionality similar to the LAPPS Grid, but allows modules to be interfaced only if compatible with one another–i.e., there is no attempt to standardize inputs and outputs among modules, so that mixing and matching of different tools that perform the same function is limited. The LAPPS Grid’s Open Advancement evaluation modules, which exploit the ability to construct alternative pipelines in order produce statistics identifying the most effective tool sequence and/or components accounting for the largest proportion of error, are also unique; Kathaa in contrast has only basic evaluat"
W16-5202,L16-1645,0,0.0188181,"histicated applications as well as evaluation of their performance side-by-side with comparable components. Although many contributors host their own contributed services (which are called from within the LAPPS Grid), where necessary the LAPPS Grid provides hosting to ensure that software remains available to the community. Recently contributed tools include all core tools from University of Darmstadt’s DKPro3 , the AIFdb services for Argumentation analysis4 (Lawrence et al., 2012), the SWIFT Aligner for cross-lingual transfer (Gilmanov et al., 2014), the EDISON feature extraction framework5 (Sammons et al., 2016) and other tools available from the University of Illinois (e.g., semantic role labelers, entity extractors), among others. In addition, several of the basic components produced by the ARIEL team working within DARPA’s Low Resource Languages for Emergent Incidents (LORELEI) program have been integrated into the LAPPS Grid, which include tools and data to support a wide array of under-resourced languages. The LAPPS Grid has been adopted by a Mellon-funded project at the University of Illinois, which is utilizing the platform to apply sophisticated HLT text mining methods to the HathiTrust Resea"
W17-0808,W14-5204,1,0.928633,"er Science ♠ Vassar College, Department of Computer Science ♥ University of Oslo, Department of Informatics ♦ Brandeis University, Linguistics and Computational Linguistics Abstract representation—a uniform framework-internal convention—with mappings from tool-specific input and output formats. Specifically, we will take an in-depth look at how the results of morphosyntactic analysis are represented in (a) the DKPro Core component collection1 (Eckart de Castilho and Gurevych, 2014), (b) the Language Analysis Portal2 (LAP; Lapponi et al. (2014)), and (c) the Language Application (LAPPS) Grid3 (Ide et al., 2014a). These three systems all share the common goal of facilitating the creation of complex NLP workflows, allowing users to combine tools that would otherwise need input and output format conversion in order to be made compatible. While the programmatic interface of DKPro Core targets more technically inclined users, LAP and LAPPS are realized as web applications with a point-andclick graphical interface. All three have been under active development for the past several years and have—in contemporaneous, parallel work— designed and implemented framework-specific representations. These designs a"
W17-0808,lapponi-etal-2014-road,1,0.860158,"Marc Verhagen♦ ♣ Technische Universität Darmstadt, Department of Computer Science ♠ Vassar College, Department of Computer Science ♥ University of Oslo, Department of Informatics ♦ Brandeis University, Linguistics and Computational Linguistics Abstract representation—a uniform framework-internal convention—with mappings from tool-specific input and output formats. Specifically, we will take an in-depth look at how the results of morphosyntactic analysis are represented in (a) the DKPro Core component collection1 (Eckart de Castilho and Gurevych, 2014), (b) the Language Analysis Portal2 (LAP; Lapponi et al. (2014)), and (c) the Language Application (LAPPS) Grid3 (Ide et al., 2014a). These three systems all share the common goal of facilitating the creation of complex NLP workflows, allowing users to combine tools that would otherwise need input and output format conversion in order to be made compatible. While the programmatic interface of DKPro Core targets more technically inclined users, LAP and LAPPS are realized as web applications with a point-andclick graphical interface. All three have been under active development for the past several years and have—in contemporaneous, parallel work— designed"
W17-0808,P12-2074,1,0.837847,"pendencyStructure which can bind multiple dependency relations together and thus supports multiple parallel dependency structures even within a single LIF view. Media–Tokenization Mismatches Tokenizers may apply transformations to the original input text that introduce character offset mismatches with the normalized output. For example, some Penn Treebank–compliant tokenizers normalize different conventions for quotation marks (which may be rendered as straight ‘typewriter’ quotes or in multicharacter LATEX-style encodings, e.g. &quot; or ``) into opening (left) and closing (right) Unicode glyphs (Dridan and Oepen, 2012). To make such normalization accessible to downstream processing, it is insufficient to represent tokens as only a region (sub-string) of the underlying linguistic signal. In LXF, the string output of tokenizers is recorded in the annotations encapsulated with each token node, which is in turn linked to a region recording its character offsets in the original media. LIF (which is largely inspired by ISO LAF, much like LXF) also records the token string and its character offsets in the original medium. LIF supports this via the word property on tokens. DKPro Core has also recently started intro"
W17-0808,W14-5201,1,0.897235,"Missing"
W17-0808,heid-etal-2010-corpus,0,0.0249296,"Lacking interface standardization, thus, severely limits interoperability. The frameworks surveyed in this work address interoperability by means of a common 2 Terminological Definitions A number of closely interrelated concepts apply to the discussion of design choices in the repre1 https://dkpro.github.io/dkpro-core https://lap.clarino.uio.no 3 https://www.lappsgrid.org 4 There are, of course, additional designs and workflow frameworks that we would ultimately hope to include in this comparison, as for example the representations used by CONCRETE, WebLicht, and FoLiA (Ferraro et al., 2014; Heid et al., 2010; van Gompel and Reynaert, 2013), to name just a few. However, some of these frameworks are at least abstractly very similar to representatives in our current sample, and also for reasons of space we need to restrict this in-depth comparison to a relatively small selection. 2 67 Proceedings of the 11th Linguistic Annotation Workshop, pages 67–75, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics root nsubj det aux nn DT The the NNP Olympic Olympic neg NNP Committee Committee VBZ does do RB n’t not xcomp VB regret regret VBG choosing choose ORGANIZATION NNP China"
W17-0808,ide-etal-2014-language,1,0.90873,"Missing"
W19-4021,L18-1726,0,0.0588349,"Missing"
W19-4021,W12-2425,1,0.81516,"A CAS (Götz and Suhre, 2004) data model. This section briefly introduces the three platforms comprising our ecosystem (Figure 1) . Each represents a particular class of systems: a repository for annotated corpora, an NLP services platform, and an interactive annotation platform. These are introduced as platforms and not as tools as they are designed as open and extensible software systems. All are open source software and users can set up their own installations, e.g. for their own project, lab, or community. Some also run a canonical instance accessible to any registered user. PubAnnotation (Kim and Wang, 2012) takes on the role of the annotation repository in our ecosystem. It links all contributed annotations through references to canonical texts. It also supports annotation development coupled with PubDictionaries, a similarly open repository of dictionaries (term lexicons, etc.) to which users can add by registering their own dictionaries or modifying those already in the repository; as well as TextAE, a browser-based visualizer/editor for text annotation. The service-oriented architecture makes it easy for end-users to customize annotation tools by engaging in the annotation process from start"
W19-4021,C18-2002,1,0.789242,"Missing"
W19-4021,P13-4020,0,0.0296041,"ation to better automate knowledge extraction and identify relevant information in the literature has become an increasingly major activity over the past decade. Numerous platforms and frameworks that support text annotation have been developed, including the General Architecture for Text Engineering (GATE (Cunningham et al., 2013)), CLARIN WebLicht (Hinrichs et al., 2010), the Language Applications (LAPPS) Grid (Ide et al., 2014), OpenMinTeD (Labropoulou et al., 2018), and several systems based on the Unstructured Information Management Architecture (UIMA (Ferrucci et al., 2009)), e.g. ARGO (Rak et al., 2013), Apache cTAKES (Savova et al., 2010), DKPro Core (Eckart de Castilho and Gurevych, 2014). However, due to factors such as the often highly domain-specific vocabularies in specialized areas of science, these frameworks are rarely usable outof-the-box. As a result, scholars interested in mining publications may spend considerable effort to adapt existing annotation tools and resources to their particular domains of research (e.g., tune 189 Proceedings of the 13th Linguistic Annotation Workshop, pages 189–194 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Pla"
W99-0106,P98-1011,0,0.0305787,"Missing"
W99-0106,P98-1044,1,0.870719,"Missing"
W99-0106,J95-2003,0,0.183646,"Missing"
W99-0106,P98-1090,0,0.0343354,"Missing"
W99-0106,J94-4002,0,0.270617,"Computer Science Vassar College Poughkeepsie, NY, USA dcristea @ info iasi. r@ ideOcs.vassar.edu Daniel Marcu Information Sciences Institute and Department of Computer Science University of Southern California Los Angeles, CA, USA Valentin Tablan Department of Computer Science University &quot;A.I. Cuza&apos;&quot; la~i, Rom~Lnia valyt@ infoiasi.ro marcu @ isi. edu Abstract PREFERENCE module imposes preferences on potential antecedents on the basis of their grammatical roles, parallelism, frequency, proximity, etc. In some cases, anaphora resolution systems implement these modules explicitly (I-Iobbs, 1978; Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997). In other cases, these modules are integrated by means of statistical (Ge et al., 1998) or uncertainty reasoning techniques (Mitkov, 1997). The fact that current anaphora resolution systems rely exclusively on the linear nature of texts in Order to determine the LPA of an anaphor seems odd, given that several studies have claimed that there is a strong relation between discourse structure and reference (Sidner, 1981; Gmsz and Sidner, 1986; Grosz et aL, 1995; Fox, 1987; Vonk et al., 1992; Azzam et al., 1998; Hitzeman and P.oesio, 1998). These studies claim, on th"
W99-0106,W99-0307,1,0.785428,"998). 3 The Experiment 3.1 MaterhJs We used thirty newspaper texts whose lengths varied widely; the mean o is 408 words and the standard deviation/~ is 376. The texts were annotated manually for co-reference relations of identity (ITh&apos;schman and Chinchor, 1997). The coreference relations define equivalence classes on the set of all marked referents in a text. The texts were also manually annotated with discourse structures built in the style of Mann and Thompson (1988). Each analysis yielded an average of 52 elementary discourse units. Details of the discourse annotation process are given in (Marcu et al., 1999). 3-~ Comparing potential to establish co-referential links 3~,.1 Method The annotations for co-reference relations and rhetorical structure trees for the thirty texts were fused, yielding representations that ~flect not only the discourse structure, but also the c~reference 49 equivalence classes specific to each text. Based on this information, we evaluated the potential of each of the two classes of models discussed in section 2 (Linear-k and Discourse-VT-k) to correctly estab• lish co-referential links as follows: For each model, each k, and each marked referential expression a, we determi"
W99-0106,W97-1303,0,0.0336328,"College Poughkeepsie, NY, USA dcristea @ info iasi. r@ ideOcs.vassar.edu Daniel Marcu Information Sciences Institute and Department of Computer Science University of Southern California Los Angeles, CA, USA Valentin Tablan Department of Computer Science University &quot;A.I. Cuza&apos;&quot; la~i, Rom~Lnia valyt@ infoiasi.ro marcu @ isi. edu Abstract PREFERENCE module imposes preferences on potential antecedents on the basis of their grammatical roles, parallelism, frequency, proximity, etc. In some cases, anaphora resolution systems implement these modules explicitly (I-Iobbs, 1978; Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997). In other cases, these modules are integrated by means of statistical (Ge et al., 1998) or uncertainty reasoning techniques (Mitkov, 1997). The fact that current anaphora resolution systems rely exclusively on the linear nature of texts in Order to determine the LPA of an anaphor seems odd, given that several studies have claimed that there is a strong relation between discourse structure and reference (Sidner, 1981; Gmsz and Sidner, 1986; Grosz et aL, 1995; Fox, 1987; Vonk et al., 1992; Azzam et al., 1998; Hitzeman and P.oesio, 1998). These studies claim, on the one hand, th"
W99-0106,J81-4001,0,0.05935,"Missing"
W99-0106,W98-1427,0,\N,Missing
W99-0106,W98-1119,0,\N,Missing
W99-0106,P89-1031,0,\N,Missing
W99-0106,P99-1079,0,\N,Missing
W99-0106,C98-1044,1,\N,Missing
W99-0106,J94-2006,0,\N,Missing
W99-0106,P87-1022,0,\N,Missing
W99-0106,J96-2004,0,\N,Missing
W99-0106,W99-0309,0,\N,Missing
W99-0106,J86-3001,0,\N,Missing
W99-0106,C98-1011,0,\N,Missing
W99-0106,W97-1307,0,\N,Missing
W99-0508,H93-1051,0,\N,Missing
W99-0508,C92-2070,0,\N,Missing
W99-0508,H93-1052,0,\N,Missing
W99-0508,J94-4003,0,\N,Missing
W99-0508,P98-2228,0,\N,Missing
W99-0508,C98-2223,0,\N,Missing
W99-0508,P91-1017,0,\N,Missing
W99-0508,J96-2004,0,\N,Missing
Y09-2026,P06-1068,0,0.070853,"Missing"
Y09-2026,H92-1041,0,0.270356,", a filter function is applied to generate the top-2000 frequent words calculated against the whole BNC corpus. 2.2 Features of Latin Etymologies in BNC Texts Challenging tasks in text categorization: (1) which features should be selected and (2) what type of values to be assigned to the selected features. Well discussed features include single tokens/words (Nigam et al., 1999; Olsson and Douglas, 2006), keywords (Wang et al., 2004; Anette, 2006), bi-grams/n-grams (Mansur et al., 2006; Kanaris and Stamatatos, 2007), noun phrases (Liao et al., 2003; Zhang et al., 2006), and syntactic patterns (Lewis, 1992; Johannes et al., 1998). Most of the above feature selection requires more engineering effort such as the parsing of the texts so that the target syntactic patterns can be identified successfully. Furthermore, the classification performance relies on the qualities of the identified features. Selected features are assigned a numerical value to show their significance to the classification task. Summary of term weighting schemes include binary feature (BI), term frequency (TF), inversed document frequency (IDF), TF.IDF, TF.χ2, TF.RF (relevance frequency) etc borrowed from information retrieval."
