2021.latechclfl-1.12,Translationese in {R}ussian Literary Texts,2021,-1,-1,3,0,5502,maria kunilovskaya,"Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"The paper reports the results of a translationese study of literary texts based on translated and non-translated Russian. We aim to find out if translations deviate from non-translated literary texts, and if the established differences can be attributed to typological relations between source and target languages. We expect that literary translations from typologically distant languages should exhibit more translationese, and the fingerprints of individual source languages (and their families) are traceable in translations. We explore linguistic properties that distinguish non-translated Russian literature from translations into Russian. Our results show that non-translated fiction is different from translations to the degree that these two language varieties can be automatically classified. As expected, language typology is reflected in translations of literary texts. We identified features that point to linguistic specificity of Russian non-translated literature and to shining-through effects. Some of translationese features cut across all language pairs, while others are characteristic of literary translations from languages belonging to specific language families."
2021.acl-short.55,An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers,2021,-1,-1,3,0.759851,650,tharindu ranasinghe,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Most studies on word-level Quality Estimation (QE) of machine translation focus on language-specific models. The obvious disadvantages of these approaches are the need for labelled data for each language pair and the high cost required to maintain several language-specific models. To overcome these problems, we explore different approaches to multilingual, word-level QE. We show that multilingual QE models perform on par with the current language-specific models. In the cases of zero-shot and few-shot QE, we demonstrate that it is possible to accurately predict word-level quality for any given new language pair from models trained on other language pairs. Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios."
2020.wmt-1.122,{T}rans{Q}uest at {WMT}2020: Sentence-Level Direct Assessment,2020,-1,-1,3,1,650,tharindu ranasinghe,Proceedings of the Fifth Conference on Machine Translation,0,"This paper presents the team TransQuest{'}s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. The proposed methods achieve state-of-the-art results surpassing the results obtained by OpenKiwi, the baseline used in the shared task. We further fine tune the QE framework by performing ensemble and data augmentation. Our approach is the winning solution in all of the language pairs according to the WMT 2020 official results."
2020.semeval-1.94,{RGCL} at {S}em{E}val-2020 Task 6: Neural Approaches to {D}efinition{E}xtraction,2020,-1,-1,4,1,650,tharindu ranasinghe,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper presents the RGCL team submission to SemEval 2020 Task 6: DeftEval, subtasks 1 and 2. The system classifies definitions at the sentence and token levels. It utilises state-of-the-art neural network architectures, which have some task-specific adaptations, including an automatically extended training set. Overall, the approach achieves acceptable evaluation scores, while maintaining flexibility in architecture selection."
2020.eamt-1.19,Intelligent Translation Memory Matching and Retrieval with Sentence Encoders,2020,18,0,3,1,650,tharindu ranasinghe,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"Matching and retrieving previously translated segments from the Translation Memory is a key functionality in Translation Memories systems. However this matching and retrieving process is still limited to algorithms based on edit distance which we have identified as a major drawback in Translation Memories systems. In this paper, we introduce sentence encoders to improve matching and retrieving process in Translation Memories systems - an effective and efficient solution to replace edit distance-based algorithms."
2020.coling-main.445,{T}rans{Q}uest: Translation Quality Estimation with Cross-lingual Transformers,2020,-1,-1,3,1,650,tharindu ranasinghe,Proceedings of the 28th International Conference on Computational Linguistics,0,"Recent years have seen big advances in the field of sentence-level quality estimation (QE), largely as a result of using neural-based architectures. However, the majority of these methods work only on the language pair they are trained on and need retraining for new language pairs. This process can prove difficult from a technical point of view and is usually computationally expensive. In this paper we propose a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. Our evaluation shows that the proposed methods achieve state-of-the-art results outperforming current open-source quality estimation frameworks when trained on datasets from WMT. In addition, the framework proves very useful in transfer learning settings, especially when dealing with low-resourced languages, allowing us to obtain very competitive results."
W19-8703,What Influences the Features of Post-editese? A Preliminary Study,2019,-1,-1,3,0,5000,sheila castilho,Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019),0,"While a number of studies have shown evidence of translationese phenomena, that is, statistical differences between original texts and translated texts (Gellerstam, 1986), results of studies searching for translationese features in postedited texts (what has been called {''}posteditese{''} (Daems et al., 2017)) have presented mixed results. This paper reports a preliminary study aimed at identifying the presence of post-editese features in machine-translated post-edited texts and at understanding how they differ from translationese features. We test the influence of factors such as post-editing (PE) levels (full vs. light), translation proficiency (professionals vs. students) and text domain (news vs. literary). Results show evidence of post-editese features, especially in light PE texts and in certain domains."
S19-2228,{RGCL}-{WLV} at {S}em{E}val-2019 Task 12: Toponym Detection,2019,0,0,5,1,15125,alistair plum,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"This article describes the system submitted by the RGCL-WLV team to the SemEval 2019 Task 12: Toponym resolution in scientific papers. The system detects toponyms using a bootstrapped machine learning (ML) approach which classifies names identified using gazetteers extracted from the GeoNames geographical database. The paper evaluates the performance of several ML classifiers, as well as how the gazetteers influence the accuracy of the system. Several runs were submitted. The highest precision achieved for one of the submissions was 89{\%}, albeit it at a relatively low recall of 49{\%}."
R19-1115,Enhancing Unsupervised Sentence Similarity Methods with Deep Contextualised Word Representations,2019,0,1,3,1,650,tharindu ranasinghe,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Calculating Semantic Textual Similarity (STS) plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. All modern state of the art STS methods rely on word embeddings one way or another. The recently introduced contextualised word embeddings have proved more effective than standard word embeddings in many natural language processing tasks. This paper evaluates the impact of several contextualised word embeddings on unsupervised STS methods and compares it with the existing supervised/unsupervised STS methods for different datasets in different languages and different domains"
R19-1116,Semantic Textual Similarity with {S}iamese Neural Networks,2019,0,2,3,1,650,tharindu ranasinghe,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Calculating the Semantic Textual Similarity (STS) is an important research area in natural language processing which plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. This paper evaluates Siamese recurrent architectures, a special type of neural networks, which are used here to measure STS. Several variants of the architecture are compared with existing methods"
N19-1275,{B}ridging the Gap: {A}ttending to Discontinuity in Identification of Multiword Expressions,2019,0,1,5,1,22749,omid rohanian,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"We introduce a new method to tag Multiword Expressions (MWEs) using a linguistically interpretable language-independent deep learning architecture. We specifically target discontinuity, an under-explored aspect that poses a significant challenge to computational treatment of MWEs. Two neural architectures are explored: Graph Convolutional Network (GCN) and multi-head self-attention. GCN leverages dependency parse information, and self-attention attends to long-range relations. We finally propose a combined model that integrates complementary information from both, through a gating mechanism. The experiments on a standard multilingual dataset for verbal MWEs show that our model outperforms the baselines not only in the case of discontinuous MWEs but also in overall F-score."
S18-1090,{WLV} at {S}em{E}val-2018 Task 3: Dissecting Tweets in Search of Irony,2018,0,4,4,1,22749,omid rohanian,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes the systems submitted to SemEval 2018 Task 3 {``}Irony detection in English tweets{''} for both subtasks A and B. The first system leveraging a combination of sentiment, distributional semantic, and text surface features is ranked third among 44 teams according to the official leaderboard of the subtask A. The second system with slightly different representation of the features ranked ninth in subtask B. We present a method that entails decomposing tweets into separate parts. Searching for contrast within the constituents of a tweet is an integral part of our system. We embrace an extensive definition of contrast which leads to a vast coverage in detecting ironic content."
S18-1160,Wolves at {S}em{E}val-2018 Task 10: Semantic Discrimination based on Knowledge and Association,2018,0,1,5,1,10018,shiva taslimipoor,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes the system submitted to SemEval 2018 shared task 10 {`}Capturing Dicriminative Attributes{'}. We use a combination of knowledge-based and co-occurrence features to capture the semantic difference between two words in relation to an attribute. We define scores based on association measures, ngram counts, word similarity, and ConceptNet relations. The system is ranked 4th (joint) on the official leaderboard of the task."
D18-1528,Classifying Referential and Non-referential It Using Gaze,2018,0,0,4,1,12256,victoria yaneva,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"When processing a text, humans and machines must disambiguate between different uses of the pronoun it, including non-referential, nominal anaphoric or clause anaphoric ones. In this paper we use eye-tracking data to learn how humans perform this disambiguation and use this knowledge to improve the automatic classification of it. We show that by using gaze data and a POS-tagger we are able to significantly outperform a common baseline and classify between three categories of it with an accuracy comparable to that of linguistic-based approaches. In addition, the discriminatory power of specific gaze features informs the way humans process the pronoun, which, to the best of our knowledge, has not been explored using data from a natural reading task."
silvestre-baquero-mitkov-2017-translation,Translation Memory Systems Have a Long Way to Go,2017,1,0,2,0,31148,andrea baquero,Proceedings of the Workshop Human-Informed Translation and Interpreting Technology,0,"The TM memory systems changed the work of translators and now the translators not benefiting from these tools are a tiny minority. These tools operate on fuzzy (surface) matching mostly and cannot benefit from already translated texts which are synonymous to (or paraphrased versions of) the text to be translated. The match score is mostly based on character-string similarity, calculated through Levenshtein distance. The TM tools have difficulties with detecting similarities even in sentences which represent a minor revision of sentences already available in the translation memory. This shortcoming of the current TM systems was the subject of the present study and was empirically proven in the experiments we conducted. To this end, we compiled a small translation memory (English-Spanish) and applied several lexical and syntactic transformation rules to the source sentences with both English and Spanish being the source language. The results of this study show that current TM systems have a long way to go and highlight the need for TM systems equipped with NLP capabilities which will offer the translator the advantage of he/she not having to translate a sentence again if an almost identical sentence has already been already translated."
W17-5030,Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers,2017,-1,-1,3,0.350426,24998,sanja vstajner,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze-based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively-based measures (word concreteness, familiarity, age of acquisition and imagability)."
W17-1718,Investigating the Opacity of Verb-Noun Multiword Expression Usages in Context,2017,16,2,3,1,10018,shiva taslimipoor,Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017),0,This study investigates the supervised token-based identification of Multiword Expressions (MWEs). This is an ongoing research to exploit the information contained in the contexts in which different instances of an expression could occur. This information is used to investigate the question of whether an expression is literal or MWE. Lexical and syntactic context features derived from vector representations are shown to be more effective over traditional statistical measures to identify tokens of MWEs.
S16-1096,{WOLVESAAR} at {S}em{E}val-2016 Task 1: Replicating the Success of Monolingual Word Alignment and Neural Embeddings for Semantic Textual Similarity,2016,28,0,5,0,34275,hannah bechara,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,This paper describes the WOLVESAAR systems that participated in the English Semantic Textual Similarity (STS) task in SemEval2016. We replicated the top systems from the last two editions of the STS task and extended the model using GloVe word embeddings and dense vector space LSTM based sentence representations. We compared the difference in performance of the replicated system and the extended variants. Our variants to the replicated system show improved correlation scores and all of our submissions outperform the median scores from all participating systems.
L16-1045,Evaluating the Readability of Text Simplification Output for Readers with Cognitive Disabilities,2016,0,1,3,1,12256,victoria yaneva,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents an approach for automatic evaluation of the readability of text simplification output for readers with cognitive disabilities. First, we present our work towards the development of the EasyRead corpus, which contains easy-to-read documents created especially for people with cognitive disabilities. We then compare the EasyRead corpus to the simplified output contained in the LocalNews corpus (Feng, 2009), the accessibility of which has been evaluated through reading comprehension experiments including 20 adults with mild intellectual disability. This comparison is made on the basis of 13 disability-specific linguistic features. The comparison reveals that there are no major differences between the two corpora, which shows that the EasyRead corpus is to a similar reading level as the user-evaluated texts. We also discuss the role of Simple Wikipedia (Zhu et al., 2010) as a widely-used accessibility benchmark, in light of our finding that it is significantly more complex than both the EasyRead and the LocalNews corpora."
L16-1077,A Corpus of Text Data and Gaze Fixations from Autistic and Non-Autistic Adults,2016,26,1,3,1,12256,victoria yaneva,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The paper presents a corpus of text data and its corresponding gaze fixations obtained from autistic and non-autistic readers. The data was elicited through reading comprehension testing combined with eye-tracking recording. The corpus consists of 1034 content words tagged with their POS, syntactic role and three gaze-based measures corresponding to the autistic and control participants. The reading skills of the participants were measured through multiple-choice questions and, based on the answers given, they were divided into groups of skillful and less-skillful readers. This division of the groups informs researchers on whether particular fixations were elicited from skillful or less-skillful readers and allows a fair between-group comparison for two levels of reading ability. In addition to describing the process of data collection and corpus development, we present a study on the effect that word length has on reading in autism. The corpus is intended as a resource for investigating the particular linguistic constructions which pose reading difficulties for people with autism and hopefully, as a way to inform future text simplification research intended for this population."
W15-5203,Improving Translation Memory Matching through Clause Splitting,2015,-1,-1,2,0,36493,katerina timonera,Proceedings of the Workshop Natural Language Processing for Translation Memories,0,None
S15-2017,{M}ini{E}xperts: An {SVM} Approach for Measuring Semantic Textual Similarity,2015,21,5,7,0,33757,hanna bechara,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the system submitted by the University of Wolverhampton and the University of Malaga for SemEval-2015 Task 2: Semantic Textual Similarity. The system uses a Supported Vector Machine approach based on a number of linguistically motivated features. Our system performed satisfactorily for English and obtained a mean 0.7216 Pearson correlation. However, it performed less adequately for Spanish, obtaining only a mean 0.5158."
W14-5604,"The Fewer, the Better? A Contrastive Study about Ways to Simplify",2014,7,5,1,1,5503,ruslan mitkov,Proceedings of the Workshop on Automatic Text Simplification - Methods and Applications in the Multilingual Society ({ATS}-{MA} 2014),0,"Simplified texts play an important role in providing accessible and easy-to-understand information for a whole range of users who, due to linguistic, developmental or social barriers, would have difficulty in understanding materials which are not adapted and/or simplified. However, the production of simplified texts can be a time-consuming and labour-intensive task. In this paper we show that the employment of a short list of simple simplification rules could result in texts of comparable readability to those written as a result of applying a long list of more fine-grained rules. We also prove that the simplification process based on the short list of simple rules is more time efficient and consistent. 1 Rationale Simplified texts play an important role in providing accessible and easy-to-understand information for a whole range of users who, due to linguistic, developmental or social barriers, would have difficulty in understanding materials which are not adapted and/or simplified. Such users include but are not limited to people with insufficient knowledge of the language in which the document is written, people with specific language disorders and people with low literacy levels. However, while the production of simplified texts is certainly an indispensable activity, it often proves to be a time-consuming and labour-intensive task. Various methodologies and simplification strategies have been developed which are often employed by authors to simplify original texts. Most methods involve a high number of rules which could result not only in the simplification task being time-consuming but also in the authors getting confused as to which rules to apply. We hypothesise that it is possible to achieve a comparable simplification effect by using a small set of simple rules similar to the ones used in Controlled Languages which, in addition, enhances the productivity and reliability of the simplification process. In order to test our hypothesis we conduct the following experiments. First, we propose six Controlled Language-inspired rules which we believe are simple and easy enough for writers of simplified texts to understand and apply. We then ask two writers to apply these rules to a selection of newswire texts and also to produce simplified versions of these texts using the 28 rules used in the Simplext project (Saggion et al., 2011). Both sets of texts are compared in terms of readability. In both simplification tasks the time efficiency is assessed and the inter-annotator agreement is evaluated. In an additional experiment, we seek to investigate the possible effect of familiarisation in simplification. In this experiment a third writer simplifies a sample of the texts used in the previous experiments by applying each set of rules in a mixed sequence pattern which does not offer any familiarisation nor the advantage of one set of rules over the other. Using these samples, three-way inter-annotator agreement is reported. The rest of the paper is structured as follows. Section 2 outlines related work on simplification rules. Section 3 introduces our proposal for a small set of easy-to-understand and easy-to-apply rules and contrasts them with the longer and more elaborate rules employed in the Simplext proposal. Section 4 details the experiments conducted in order to validate or refute our hypothesis, and outlines the data used for the experiments. Section 5 presents and discusses the results, while the last section of the paper summarises the main conclusions of this study."
W14-1201,One Step Closer to Automatic Evaluation of Text Simplification Systems,2014,36,10,2,0.714286,24998,sanja vstajner,Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations ({PITR}),0,"This study explores the possibility of replacing the costly and time-consuming human evaluation of the grammaticality and meaning preservation of the output of text simplification (TS) systems with some automatic measures. The focus is on six widely used machine translation (MT) evaluation metrics and their correlation with human judgements of grammaticality and meaning preservation in text snippets. As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising."
2013.mtsummit-wmwumttt.4,A flexible framework for collocation retrieval and translation from parallel and comparable corpora,2013,-1,-1,2,0,41877,oscar rivera,Proceedings of the Workshop on Multi-word Units in Machine Translation and Translation Technologies,0,None
2013.mtsummit-european.7,Pangeanic in the {EXPERT} Project: Exploiting Empirical app{R}oaches to Translation,2013,-1,-1,4,0,4981,manuel herranz,Proceedings of Machine Translation Summit XIV: European projects,0,None
stajner-mitkov-2012-diachronic,Diachronic Changes in Text Complexity in 20th Century {E}nglish Language: An {NLP} Approach,2012,14,3,2,1,24998,sanja vstajner,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"A syntactically complex text may represent a problem for both comprehension by humans and various NLP tasks. A large number of studies in text simplification are concerned with this problem and their aim is to transform the given text into a simplified form in order to make it accessible to the wider audience. In this study, we were investigating what the natural tendency of texts is in 20th century English language. Are they becoming syntactically more complex over the years, requiring a higher literacy level and greater effort from the readers, or are they becoming simpler and easier to read? We examined several factors of text complexity (average sentence length, Automated Readability Index, sentence complexity and passive voice) in the 20th century for two main English language varieties - British and American, using the `Brown family' of corpora. In British English, we compared the complexity of texts published in 1931, 1961 and 1991, while in American English we compared the complexity of texts published in 1961 and 1992. Furthermore, we demonstrated how the state-of-the-art NLP tools can be used for automatic extraction of some complex features from the raw text version of the corpora."
konstantinova-etal-2012-review,"A review corpus annotated for negation, speculation and their scope",2012,9,29,6,0,43070,natalia konstantinova,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents a freely available resource for research on handling negation and speculation in review texts. The SFU Review Corpus, consisting of 400 documents of movie, book, and consumer product reviews, was annotated at the token level with negative and speculative keywords and at the sentence level with their linguistic scope. We report statistics on corpus size and the consistency of annotations. The annotated corpus will be useful in many applications, such as document mining and sentiment analysis."
temnikova-etal-2012-clcm,{CLCM} - A Linguistic Resource for Effective Simplification of Instructions in the Crisis Management Domain and its Evaluations,2012,25,3,3,0.238095,23303,irina temnikova,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Due to the increasing number of emergency situations which can have substantial consequences, both financially and fatally, the Crisis Management (CM) domain is developing at an exponential speed. The efficient management of emergency situations relies on clear communication between all of the participants in a crisis situation. For these reasons the Text Complexity (TC) of the CM domain needed to be investigated and showed that CM domain texts exhibit high TC levels. This article presents a new linguistic resource in the form of Controlled Language (CL) guidelines for manual text simplification in the CM domain which aims to address high TC in the CM domain and produce clear messages to be used in crisis situations. The effectiveness of the resource has been tested via evaluation from several different perspectives important for the domain. The overall results show that the CLCM simplification has a positive impact on TC, reading comprehension, manual translation and machine translation. Additionally, an investigation of the cognitive difficulty in applying manual simplification operations led to interesting discoveries. This article provides details of the evaluation methods, the conducted experiments, their results and indications about future work."
E12-1072,{E}lliphant: Improved Automatic Detection of Zero Subjects and Impersonal Constructions in {S}panish,2012,28,7,3,0,34744,luz rello,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In pro-drop languages, the detection of explicit subjects, zero subjects and non-referential impersonal constructions is crucial for anaphora and co-reference resolution. While the identification of explicit and zero subjects has attracted the attention of researchers in the past, the automatic identification of impersonal constructions in Spanish has not been addressed yet and this work is the first such study. In this paper we present a corpus to underpin research on the automatic detection of these linguistic phenomena in Spanish and a novel machine learning-based methodology for their computational treatment. This study also provides an analysis of the features, discusses performance across two different genres and offers error analysis. The evaluation results show that our system performs better in detecting explicit subjects than alternative systems."
C12-2112,Automatic Question Generation in Multimedia-Based Learning,2012,13,4,4,0,43719,yvonne skalban,Proceedings of {COLING} 2012: Posters,0,"We investigate whether questions generated automatically by two Natural Language Processing (NLP) based systems (one developed by the authors, the other a state-of-the-art system) can successfully be used to assist multimedia-based learning. We examine the feasibility of using a Question Generation (QG) systemxe2x80x99s output as pre-questions; with different types of pre-questions used: text-based and with images. We also compare the psychometric parameters of the automatically generated questions by the two systems and of those generated manually. Specifically, we analyse the effect such pre-questions have on test-takersxe2x80x99 performance on a comprehension test about a scientific video documentary. We also compare the discrimination power of the questions generated automatically against that of questions generated manually. The results indicate that the presence of pre-questions (preferably with images) improves the performance of test-takers. They indicate that the psychometric parameters of the questions generated by our system are comparable if not better than those of the state-of-the-art system."
W11-4112,Diachronic Stylistic Changes in {B}ritish and {A}merican Varieties of 20th Century Written {E}nglish Language,2011,28,5,2,1,24998,sanja vstajner,Proceedings of the Workshop on Language Technologies for Digital Humanities and Cultural Heritage,0,"In this paper we present the results of a study investigating the diachronic changes of four stylistic features: average sentence length, Automated Readability Index, lexical density and lexical richness in 20th century written English language. All experiments were conducted on the largest existing diachronic corpora of British and American English xe2x80x90 the Brown xe2x80x98familyxe2x80x99 corpora, employing NLP techniques for automatic extraction of the features. Additionally, we compare the trends of changes between the two English varieties and make suggestions for future studies of diachronic language change."
cardey-etal-2010-resources,Resources for Controlled Languages for Alert Messages and Protocols in the {E}uropean Perspective,2010,6,2,4,0,45893,sylviane cardey,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper is concerned with resources for controlled languages for alert messages and protocols in the European perspective. These resources have been produced as the outcome of a project (Alert Messages and Protocols: MESSAGE) which has been funded with the support of the European Commission - Directorate-General Justice, Freedom and Security, and with the specific objective of 'promoting and supporting the development of security standards, and an exchange of know-how and experience on protection of people'. The MESSAGE project involved the development and transfer of a methodology for writing safe and safely translatable alert messages and protocols created by Centre Tesni{\`e}re in collaboration with the aircraft industry, the health profession, and emergency services by means of a consortium of four partners to their four European member states in their languages (ES, FR (Coordinator), GB, PL). The paper describes alert messages and protocols, controlled languages for safety and security, the target groups involved, controlled language evaluation, dissemination, the resources that are available, both ÂFreely availableÂ and ÂFrom OwnerÂ, together with illustrations of the resources, and the potential transferability to other sectors and users."
W09-0207,Semantic Similarity of Distractors in Multiple-Choice Tests: Extrinsic Evaluation,2009,13,36,1,1,5503,ruslan mitkov,Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,0,"Mitkov and Ha (2003) and Mitkov et al. (2006) offered an alternative to the lengthy and demanding activity of developing multiple-choice test items by proposing an NLP-based methodology for construction of test items from instructive texts such as textbook chapters and encyclopaedia entries. One of the interesting research questions which emerged during these projects was how better quality distractors could automatically be chosen. This paper reports the results of a study seeking to establish which similarity measures generate better quality distractors of multiple-choice tests. Similarity measures employed in the procedure of selection of distractors are collocation patterns, four different methods of WordNet-based semantic similarity (extended gloss overlap measure, Leacock and Chodorow's, Jiang and Conrath's as well as Lin's measures), distributional similarity, phonetic similarity as well as a mixed strategy combining the aforementioned measures. The evaluation results show that the methods based on Lin's measure and on the mixed strategy outperform the rest, albeit not in a statistically significant fashion."
ha-etal-2008-mutual,Mutual Bilingual Terminology Extraction,2008,13,20,3,1,12258,le ha,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper describes a novel methodology to perform bilingual terminology extraction, in which automatic alignment is used to improve the performance of terminology extraction for each language. The strengths of monolingual terminology extraction for each language are exploited to improve the performance of terminology extraction in the other language, thanks to the availability of a sentence-level aligned bilingual corpus, and an automatic noun phrase alignment mechanism. The experiment indicates that weaknesses in monolingual terminology extraction due to the limitation of resources in certain languages can be overcome by using another language which has no such limitation."
orasan-etal-2008-anaphora,Anaphora Resolution Exercise: an Overview,2008,-1,-1,3,0,25217,constantin oruasan,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Evaluation campaigns have become an established way to evaluate automatic systems which tackle the same task. This paper presents the first edition of the Anaphora Resolution Exercise (ARE) and the lessons learnt from it. This first edition focused only on English pronominal anaphora and NP coreference, and was organised as an exploratory exercise where various issues were investigated. ARE proposed four different tasks: pronominal anaphora resolution and NP coreference resolution on a predefined set of entities, pronominal anaphora resolution and NP coreference resolution on raw texts. For each of these tasks different inputs and evaluation metrics were prepared. This paper presents the four tasks, their input data and evaluation metrics used. Even though a large number of researchers in the field expressed their interest to participate, only three institutions took part in the formal evaluation. The paper briefly presents their results, but does not try to interpret them because in this edition of ARE our aim was not about finding why certain methods are better, but to prepare the ground for a fully-fledged edition."
arnaudov-mitkov-2008-smarty,Smarty - Extendable Framework for Bilingual and Multilingual Comprehension Assistants,2008,4,2,2,0,48482,todor arnaudov,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper discusses a framework for development of bilingual and multilingual comprehension assistants and presents a prototype implementation of an English-Bulgarian comprehension assistant. The framework is based on the application of advanced graphical user interface techniques, WordNet and compatible lexical databases as well as a series of NLP preprocessing tasks, including POS-tagging, lemmatisation, multiword expressions recognition and word sense disambiguation. The aim of this framework is to speed up the process of dictionary look-up, to offer enhanced look-up functionalities and to perform a context-sensitive narrowing-down of the set of translation alternatives proposed to the user."
2008.amta-papers.5,Translation universals: do they exist? A corpus-based {NLP} study of convergence and simplification,2008,16,17,2,0,28901,gloria pastor,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"Convergence and simplification are two of the so-called universals in translation studies. The first one postulates that translated texts tend to be more similar than non-translated texts. The second one postulates that translated texts are simpler, easier-to-understand than non-translated ones. This paper discusses the results of a project which applies NLP techniques over comparable corpora of translated and non-translated texts in Spanish seeking to establish whether these two universals hold Corpas Pastor (2008)."
W06-1416,Generating Multiple-Choice Test Items from Medical Text: A Pilot Study,2006,4,33,3,0,46630,nikiforos karamanis,Proceedings of the Fourth International Natural Language Generation Conference,0,We report the results of a pilot study on generating Multiple-Choice Test Items from medical text and discuss the main tasks involved in this process and how our system was evaluated by domain experts.
puscasu-mitkov-2006-establishing,"If {``}it{''} were {``}then{''}, then when was {``}it{''}? Establishing the anaphoric role of {``}then{''}",2006,-1,-1,2,0,48464,georgiana pucscacsu,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The adverb ``then'' is among the most frequent Englishtemporal adverbs, being also capable of filling a variety of semantic roles. The identification of anaphoric usages of ``then``is important for temporal expression resolution, while thetemporal relationship usage is important for event ordering. Given that previous work has not tackled the identification and temporal resolution of anaphoric ``then'', this paper presents a machine learning approach for setting apart anaphoric usages and a rule-based normaliser that resolves it with respect to an antecedent. The performance of the two modules is evaluated. The present paper also describes the construction of an annotated corpus and the subsequent derivation of training data required by the machine learning module."
2005.eamt-1.28,Building a {WSD} module within an {MT} system to enable interactive resolution in the user{'}s source language,2005,1,0,5,1,12551,constantin orasan,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"Ambiguous words pose very serious problems to existing machine translation systems. The Translation Checker, a system part of Translution Central addresses this problem by allowing users to disambiguate words in their own language, with little or no knowledge of the target language. In order to achieve this, a multilingual dictionary is being developed using EuroWordNet. Languages are too ambiguous to feasibly present users with all the senses available for a word. To this end, a suite of language processing modules has been developed to reduce the ambiguity of words. The implemented modules and an evaluation of their influence on English, French, German, Italian and Spanish corpora are presented. The results of the evaluation show that the proposed approach dramatically reduces the ambiguity of the language."
tutin-etal-2004-annotation,Annotation of Anaphoric Expressions in an Aligned Bilingual Corpus,2004,7,1,3,0,32037,agnes tutin,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper discusses a French-English corpus annotated and aligned at anaphoric level. It also presents an annotation scheme based on the study of a detailed corpus featuring different types of correspondences and mismatches. The scheme which is adapted from EAGLES recommendations, supports the alignment at anaphoric level and caters for the different kinds of mismatches."
pekar-etal-2004-categorizing,Categorizing Web Pages as a Preprocessing Step for Information Extraction,2004,7,3,3,0,31547,viktor pekar,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"At present, information systems combining crawling and information extraction (IE) technologies acquire a lot of research and industrial interest. In this paper, we present an algorithm that exploits techniques for unsupervised IE pattern acquisition in order to facilitate identification of web pages containing information relevant to the IE task."
W03-0203,Computer-Aided Generation of Multiple-Choice Tests,2003,4,146,1,1,5503,ruslan mitkov,Proceedings of the {HLT}-{NAACL} 03 Workshop on Building Educational Applications Using Natural Language Processing,0,"This paper describes a novel computer-aided procedure for generating multiple-choice tests from electronic instructional documents. In addition to employing various NLP techniques including term extraction and shallow parsing, the program makes use of language resources such as a corpus and WordNet. The system generates test questions and distractors, offering the user the option to post-edit the test items."
E03-1066,{CAST}: A computer-aided summarisation tool,2003,7,18,2,1,12551,constantin orasan,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper we propose computer-aided summarisation (CAS) as an alternative approach to automatic summarisation, and present an ongoing project which aims to develop a CAS system. The need for such an alternative approach is justified by the relatively poor performance of fully automatic methods used in summarisation. Our system combines several summarisation methods, allowing the user of the system to interact with their parameters and output in order to improve the quality of the produced summary."
barbu-etal-2002-corpus,A corpus based investigation of morphological disagreement in anaphoric relations,2002,14,4,3,0,53447,cuatualina barbu,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
C02-1027,Shallow Language Processing Architecture for {B}ulgarian,2002,7,17,2,0.952381,11964,hristo tanev,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper describes LINGUA - an architecture for text processing in Bulgarian. First, the pre-processing modules for tokenisation, sentence splitting, paragraph segmentation, part-of-speech tagging, clause chunking and noun phrase extraction are outlined. Next, the paper proceeds to describe in more detail the anaphora resolution module. Evaluation results are reported for each processing task."
P01-1006,Evaluation Tool for Rule-based Anaphora Resolution Methods,2001,16,29,2,0,53861,catalina barbu,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,In this paper we argue that comparative evaluation in anaphora resolution has to be performed using the same pre-processing tools and on the same set of data. The paper proposes an evaluation environment for comparing anaphora resolution algorithms which is illustrated by presenting the results of the comparative evaluation of three methods on the basis of several evaluation measures.
J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,2001,36,23,1,1,5503,ruslan mitkov,Computational Linguistics,0,"Anaphora accounts for cohesion in texts and is a phenomenon under active study in formal and computational linguistics alike. The correct interpretation of anaphora is vital for natural language processing (NLP). For example, anaphora resolution is a key task in natural language interfaces, machine translation, text summarization, information extraction, question answering, and a number of other NLP applications. After considerable initial research, followed by years of relative silence in the early 1980s, anaphora resolution has attracted the attention of many researchers in the last 10 years and a great deal of successful work on the topic has been carried out. Discourseoriented theories and formalisms such as Discourse Representation Theory and Centering Theory inspired new research on the computational treatment of anaphora. The drive toward corpus-based robust NLP solutions further stimulated interest in alternative and/or data-enriched approaches. Last, but not least, application-driven research in areas such as automatic abstracting and information extraction independently highlighted the importance of anaphora and coreference resolution, boosting research in this area. Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input. However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (Dagan and Itai 1990, 1991; Lappin and Leass 1994; Nasukawa 1994; Kennedy and Boguraev 1996; Williams, Harvey, and Preston 1996; Baldwin 1997; Mitkov 1996, 1998b). The drive toward knowledge-poor and robust approaches was further motivated by the emergence of cheaper and more reliable corpus-based NLP tools such as partof-speech taggers and shallow parsers, alongside the increasing availability of corpora and other NLP resources (e.g., ontologies). In fact, the availability of corpora, both raw and annotated with coreferential links, provided a strong impetus to anaphora resolu"
mitkov-2000-towards,Towards More Comprehensive Evaluation in Anaphora Resolution,2000,0,12,1,1,5503,ruslan mitkov,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,None
2000.bcs-1.18,Evaluation environment for anaphora resolution,2000,-1,-1,2,0,53861,catalina barbu,Proceedings of the International Conference on Machine Translation and Multilingual Applications in the new Millennium: MT 2000,0,None
2000.bcs-1.20,{LINGUA}: a robust architecture for text processing and anaphora resolution in {B}ulgarian,2000,-1,-1,2,0.952381,11964,hristo tanev,Proceedings of the International Conference on Machine Translation and Multilingual Applications in the new Millennium: MT 2000,0,None
J99-4007,Book Reviews: Centering Theory in Discourse,1999,-1,-1,1,1,5503,ruslan mitkov,Computational Linguistics,0,None
W98-1502,Multilingual Robust Anaphora Resolution,1998,29,23,1,1,5503,ruslan mitkov,Proceedings of the Third Conference on Empirical Methods for Natural Language Processing,0,"Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and timehconsuming task. This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals. This approach is a modification of the practical approach (Mitkov 1998a) and operates on texts pre-processed by a part-of-speech taggcr. Input is checked against agreement and a nlonhber of antecedent indicators. Candidates are assigried scores by each indicator and the candidate with the highest ag~ gregate score is returned as the antecedent. We pro~ pose this approach as a platform for multilingual pronoun resolution. The robust approach was initially de~ veloped and tested for English, but we have also adapted and tested it for Polish and Arabic. For both languages, we found that adaptation required minimum modification and that further, even if used un~ modified, the approach delivers acceptable success rates. Preliminary evaluation reports high success rates in the range of and over 90%"
P98-2143,Robust Pronoun Resolution with Limited Knowledge,1998,15,273,1,1,5503,ruslan mitkov,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and time-consuming task. This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the success rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be successfully adapted for other languages with minimum modifications."
C98-2138,Robust pronoun resolution with limited knowledge,1998,15,273,1,1,5503,ruslan mitkov,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and time-consuming task. This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the success rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be successfully adapted for other languages with minimum modifications."
W97-1303,Factors in anaphora resolution: they are not the only things that matter. A case study based on two different approaches,1997,-1,-1,1,1,5503,ruslan mitkov,"Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts",0,None
W97-1312,How far are we from (semi-)automatic of anaphoric links in corpora?,1997,0,0,1,1,5503,ruslan mitkov,"Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts",0,None
1996.tc-1.6,Towards a more efficient use of {PC}-based {MT} in education,1996,-1,-1,1,1,5503,ruslan mitkov,Proceedings of Translating and the Computer 18,0,None
1995.tmi-1.6,Anaphora Resolution in Machine Translation,1995,-1,-1,1,1,5503,ruslan mitkov,Proceedings of the Sixth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
J94-1008,Book Reviews: Expressibility and the Problem of Efficient Text Planning,1994,-1,-1,1,1,5503,ruslan mitkov,Computational Linguistics,0,None
C94-2191,An Integrated Model for Anaphora Resolution,1994,14,28,1,1,5503,ruslan mitkov,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The paper discusses a new knowledge-based and sublanguage-oriented model for anaphora resolution, which integrates syntactic, semantic, discourse, domain and heuristical knowledge for the sublanguage of computer science. Special attention is paid to a new approach for tracking the center throughout a discourse segment, which plays an important role in proposing the most likely antecedent to the anaphor in case of ambiguity."
1994.bcs-1.8,"Machine translation, ten years on: Discourse has yet to make a breakthrough",1994,-1,-1,1,1,5503,ruslan mitkov,Proceedings of the Second International Conference on Machine Translation: Ten years on,0,"Progress in Machine Translation (MT) during the last ten years has been observed at different levels, but discourse has yet to make a breakthrough. MT research and development has concentrated so far mostly on sentence translation (discourse analysis being a very complicated task) and the successful operation of most of the working MT systems does not usually go beyond the sentence level. To start with, the paper will refer to the MT research and development in the last ten years at the IAI in SaarbruÌcken. Next, the MT discourse issues will be discussed both from the point of view of source language analysis and target text generation, and on the basis of the preliminary results of an ongoing ``discourse-oriented MT'' project . Probably the most important aspect in successfully analysing multisentential source texts is the capacity to establish the anaphoric references to preceding discourse entities. The paper will discuss the problem of anaphora resolution from the perspective of MT. A new integrated model for anaphora resolution, developed for the needs of MT, will be also outlined. As already mentioned, most machine translation systems perform translation sentence by sentence. But even in the case of paragraph translation, the discourse structure of the target text tends to be identical to that of the source text. However, the sublanguage discourse structures may differ across the different languages, and thus a translated text which assumes the same discourse structure as the source text may sound unnatural and perhaps disguise the true intent of the writer. Finally, the paper will outline a new approach for generating discourse structures, appropriate to the target sublanguage and will discuss some of the complicated problems encountered."
W93-0223,How Could Rhetorical Relations Be Used in Machine Translation?,1993,-1,-1,1,1,5503,ruslan mitkov,Intentionality and Structure in Discourse Relations,0,None
