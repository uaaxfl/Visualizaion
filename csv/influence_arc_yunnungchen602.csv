2020.acl-main.347,J84-3009,0,0.6734,"Missing"
2020.acl-main.347,Q18-1036,0,0.0145914,"e 1. Ladhak et al. (2016) introduced LatticeRNN, a variant of recurrent neural networks (RNNs) that generalize RNNs to latticestructured inputs in order to improve SLU. Zhang and Yang (2018) proposed a similar idea for Chinese name entity recognition. Sperber et al. (2019); Xiao et al. (2019); Zhang et al. (2019) proposed extensions to enable the transformer model (Vaswani et al., 2017) to consume lattice inputs for machine translation. Huang and Chen (2019) proposed to adapt the transformer model originally pre-trained on written texts to consume lattices in order to improve SLU performance. Buckman and Neubig (2018) also found that utilizing lattices that represent multiple granularities of sentences can improve language modeling. With recent introduction of large pre-trained language models (LMs) such as ELMo (Peters et al., 2018), GPT (Radford, 2018) and BERT (Devlin et al., 2019), we have observed huge improvements on natural language understanding tasks. These models are pre-trained on large amount of written texts so that they provide the downstream tasks with high-quality representations. However, applying these models to the spoken scenarios poses 3764 Proceedings of the 58th Annual Meeting of the"
2020.acl-main.347,N19-1213,0,0.0282703,"Missing"
2020.acl-main.347,H94-1010,0,0.281315,"5.55 9.19 SNIPS 13,084 700 700 7 45.61 18.79 SWDA 103,326 8,989 15,927 43 28.41 17.15 MRDA 73,588 15,037 14,800 5 32.04 21.53 with overall classification accuracy. 3.2 Table 1: Data statistics. 3 Experiments In order to evaluate the quality of the pre-trained lattice LM, we conduct the experiments for two common tasks in spoken language understanding. 3.1 Tasks and Datasets Intent detection and dialogue act recognition are two common tasks about spoken language understanding. The benchmark datasets used for intent detection are ATIS (Airline Travel Information Systems) (Hemphill et al., 1990; Dahl et al., 1994; Tur et al., 2010) and SNIPS (Coucke et al., 2018). We use the NXT-format of the Switchboard (Stolcke et al., 2000) Dialogue Act Corpus (SWDA) (Calhoun et al., 2010) and the ICSI Meeting Recorder Dialogue Act Corpus (MRDA) (Shriberg et al., 2004) for benchmarking dialogue act recognition. The SNIPS corpus only contains written text, so we synthesize a spoken version of the dataset using a commercial text-to-speech service. We use an ASR system trained on WSJ (Paul and Baker, 1992) with Kaldi (Povey et al., 2011) to transcribe ATIS, and an ASR system released by Kaldi to transcribe other datas"
2020.acl-main.347,N19-1423,0,0.142254,"ao et al. (2019); Zhang et al. (2019) proposed extensions to enable the transformer model (Vaswani et al., 2017) to consume lattice inputs for machine translation. Huang and Chen (2019) proposed to adapt the transformer model originally pre-trained on written texts to consume lattices in order to improve SLU performance. Buckman and Neubig (2018) also found that utilizing lattices that represent multiple granularities of sentences can improve language modeling. With recent introduction of large pre-trained language models (LMs) such as ELMo (Peters et al., 2018), GPT (Radford, 2018) and BERT (Devlin et al., 2019), we have observed huge improvements on natural language understanding tasks. These models are pre-trained on large amount of written texts so that they provide the downstream tasks with high-quality representations. However, applying these models to the spoken scenarios poses 3764 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3764–3769 c July 5 - 10, 2020. 2020 Association for Computational Linguistics several discrepancies between the pre-training task and the target task, such as the domain mismatch between written texts and spoken utterances"
2020.acl-main.347,N18-2118,1,0.82978,"e 1: Illustration of a lattice. Introduction The task of spoken language understanding (SLU) aims at extracting useful information from spoken utterances. Typically, SLU can be decomposed with a two-stage method: 1) an accurate automatic speech recognition (ASR) system transcribes the input speech into texts, and then 2) language understanding techniques are applied to the transcribed texts. These two modules can be developed separately, so most prior work developed the backend language understanding systems based on manual transcripts (Yao et al., 2014; Guo et al., 2014; Mesnil et al., 2014; Goo et al., 2018). Despite the simplicity of the two-stage method, prior work showed that a tighter integration between two components can lead to better performance. Researchers have extended the ASR 1-best results to n-best lists or word confusion networks in order to preserve the ambiguity of the transcripts. 1 The scource code is available at: https://github. com/MiuLab/Lattice-ELMo. (Tur et al., 2002; Hakkani-T¨ur et al., 2006; Henderson et al., 2012; T¨ur et al., 2013; Masumura et al., 2018). Another line of research focused on using lattices produced by ASR systems. Lattices are directed acyclic graphs"
2020.acl-main.347,H90-1021,0,0.84707,"Missing"
2020.acl-main.347,P18-1031,0,0.0998865,"the downstream tasks with high-quality representations. However, applying these models to the spoken scenarios poses 3764 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3764–3769 c July 5 - 10, 2020. 2020 Association for Computational Linguistics several discrepancies between the pre-training task and the target task, such as the domain mismatch between written texts and spoken utterances with ASR errors. It has been shown that fine-tuning the pre-trained language models on the data from the target tasks can mitigate the domain mismatch problem (Howard and Ruder, 2018; Chronopoulou et al., 2019). Siddhant et al. (2018) focused on pre-training a language model specifically for spoken content with huge amount of automatic transcripts, which requires a large collection of indomain speech. In this paper, we propose a novel spoken language representation learning framework, which focuses on learning contextualized representations of lattices based on our proposed lattice language modeling objective. The proposed framework consists of two stages of LM pre-training to reduce the demands for lattice data. We conduct experiments on benchmark datasets for spoken lan"
2020.acl-main.347,H92-1073,0,0.787537,"he benchmark datasets used for intent detection are ATIS (Airline Travel Information Systems) (Hemphill et al., 1990; Dahl et al., 1994; Tur et al., 2010) and SNIPS (Coucke et al., 2018). We use the NXT-format of the Switchboard (Stolcke et al., 2000) Dialogue Act Corpus (SWDA) (Calhoun et al., 2010) and the ICSI Meeting Recorder Dialogue Act Corpus (MRDA) (Shriberg et al., 2004) for benchmarking dialogue act recognition. The SNIPS corpus only contains written text, so we synthesize a spoken version of the dataset using a commercial text-to-speech service. We use an ASR system trained on WSJ (Paul and Baker, 1992) with Kaldi (Povey et al., 2011) to transcribe ATIS, and an ASR system released by Kaldi to transcribe other datasets. The statistics of datasets are summarized in Table 1. All tasks are evaluated Model and Training Details In order to conduct fair comparison with ELMo (Peters et al., 2018), we directly adopt their pre-trained model as our pre-trained sequential LM. The hidden size of the LatticeLSTM classifier is set to 300. We use adam as the optimizer with learning rate 0.0001 for LM pre-training and 0.001 for training the classifier. The checkpoint with the best validation accuracy is used"
2020.acl-main.347,N18-1202,0,0.490857,"e name entity recognition. Sperber et al. (2019); Xiao et al. (2019); Zhang et al. (2019) proposed extensions to enable the transformer model (Vaswani et al., 2017) to consume lattice inputs for machine translation. Huang and Chen (2019) proposed to adapt the transformer model originally pre-trained on written texts to consume lattices in order to improve SLU performance. Buckman and Neubig (2018) also found that utilizing lattices that represent multiple granularities of sentences can improve language modeling. With recent introduction of large pre-trained language models (LMs) such as ELMo (Peters et al., 2018), GPT (Radford, 2018) and BERT (Devlin et al., 2019), we have observed huge improvements on natural language understanding tasks. These models are pre-trained on large amount of written texts so that they provide the downstream tasks with high-quality representations. However, applying these models to the spoken scenarios poses 3764 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3764–3769 c July 5 - 10, 2020. 2020 Association for Computational Linguistics several discrepancies between the pre-training task and the target task, such as the domain"
2020.acl-main.347,W04-2319,0,0.0385844,"f the pre-trained lattice LM, we conduct the experiments for two common tasks in spoken language understanding. 3.1 Tasks and Datasets Intent detection and dialogue act recognition are two common tasks about spoken language understanding. The benchmark datasets used for intent detection are ATIS (Airline Travel Information Systems) (Hemphill et al., 1990; Dahl et al., 1994; Tur et al., 2010) and SNIPS (Coucke et al., 2018). We use the NXT-format of the Switchboard (Stolcke et al., 2000) Dialogue Act Corpus (SWDA) (Calhoun et al., 2010) and the ICSI Meeting Recorder Dialogue Act Corpus (MRDA) (Shriberg et al., 2004) for benchmarking dialogue act recognition. The SNIPS corpus only contains written text, so we synthesize a spoken version of the dataset using a commercial text-to-speech service. We use an ASR system trained on WSJ (Paul and Baker, 1992) with Kaldi (Povey et al., 2011) to transcribe ATIS, and an ASR system released by Kaldi to transcribe other datasets. The statistics of datasets are summarized in Table 1. All tasks are evaluated Model and Training Details In order to conduct fair comparison with ELMo (Peters et al., 2018), we directly adopt their pre-trained model as our pre-trained sequent"
2020.acl-main.347,P19-1115,0,0.0181124,"om/MiuLab/Lattice-ELMo. (Tur et al., 2002; Hakkani-T¨ur et al., 2006; Henderson et al., 2012; T¨ur et al., 2013; Masumura et al., 2018). Another line of research focused on using lattices produced by ASR systems. Lattices are directed acyclic graphs (DAGs) that represent multiple recognition hypotheses. An example of ASR lattice is shown in Figure 1. Ladhak et al. (2016) introduced LatticeRNN, a variant of recurrent neural networks (RNNs) that generalize RNNs to latticestructured inputs in order to improve SLU. Zhang and Yang (2018) proposed a similar idea for Chinese name entity recognition. Sperber et al. (2019); Xiao et al. (2019); Zhang et al. (2019) proposed extensions to enable the transformer model (Vaswani et al., 2017) to consume lattice inputs for machine translation. Huang and Chen (2019) proposed to adapt the transformer model originally pre-trained on written texts to consume lattices in order to improve SLU performance. Buckman and Neubig (2018) also found that utilizing lattices that represent multiple granularities of sentences can improve language modeling. With recent introduction of large pre-trained language models (LMs) such as ELMo (Peters et al., 2018), GPT (Radford, 2018) and BE"
2020.acl-main.347,J00-3003,0,0.117367,"Missing"
2020.acl-main.347,P19-1298,0,0.0216936,"(Tur et al., 2002; Hakkani-T¨ur et al., 2006; Henderson et al., 2012; T¨ur et al., 2013; Masumura et al., 2018). Another line of research focused on using lattices produced by ASR systems. Lattices are directed acyclic graphs (DAGs) that represent multiple recognition hypotheses. An example of ASR lattice is shown in Figure 1. Ladhak et al. (2016) introduced LatticeRNN, a variant of recurrent neural networks (RNNs) that generalize RNNs to latticestructured inputs in order to improve SLU. Zhang and Yang (2018) proposed a similar idea for Chinese name entity recognition. Sperber et al. (2019); Xiao et al. (2019); Zhang et al. (2019) proposed extensions to enable the transformer model (Vaswani et al., 2017) to consume lattice inputs for machine translation. Huang and Chen (2019) proposed to adapt the transformer model originally pre-trained on written texts to consume lattices in order to improve SLU performance. Buckman and Neubig (2018) also found that utilizing lattices that represent multiple granularities of sentences can improve language modeling. With recent introduction of large pre-trained language models (LMs) such as ELMo (Peters et al., 2018), GPT (Radford, 2018) and BERT (Devlin et al., 2"
2020.acl-main.347,P19-1649,0,0.0188797,"Hakkani-T¨ur et al., 2006; Henderson et al., 2012; T¨ur et al., 2013; Masumura et al., 2018). Another line of research focused on using lattices produced by ASR systems. Lattices are directed acyclic graphs (DAGs) that represent multiple recognition hypotheses. An example of ASR lattice is shown in Figure 1. Ladhak et al. (2016) introduced LatticeRNN, a variant of recurrent neural networks (RNNs) that generalize RNNs to latticestructured inputs in order to improve SLU. Zhang and Yang (2018) proposed a similar idea for Chinese name entity recognition. Sperber et al. (2019); Xiao et al. (2019); Zhang et al. (2019) proposed extensions to enable the transformer model (Vaswani et al., 2017) to consume lattice inputs for machine translation. Huang and Chen (2019) proposed to adapt the transformer model originally pre-trained on written texts to consume lattices in order to improve SLU performance. Buckman and Neubig (2018) also found that utilizing lattices that represent multiple granularities of sentences can improve language modeling. With recent introduction of large pre-trained language models (LMs) such as ELMo (Peters et al., 2018), GPT (Radford, 2018) and BERT (Devlin et al., 2019), we have observe"
2020.acl-main.347,P18-1144,0,0.0283533,"ambiguity of the transcripts. 1 The scource code is available at: https://github. com/MiuLab/Lattice-ELMo. (Tur et al., 2002; Hakkani-T¨ur et al., 2006; Henderson et al., 2012; T¨ur et al., 2013; Masumura et al., 2018). Another line of research focused on using lattices produced by ASR systems. Lattices are directed acyclic graphs (DAGs) that represent multiple recognition hypotheses. An example of ASR lattice is shown in Figure 1. Ladhak et al. (2016) introduced LatticeRNN, a variant of recurrent neural networks (RNNs) that generalize RNNs to latticestructured inputs in order to improve SLU. Zhang and Yang (2018) proposed a similar idea for Chinese name entity recognition. Sperber et al. (2019); Xiao et al. (2019); Zhang et al. (2019) proposed extensions to enable the transformer model (Vaswani et al., 2017) to consume lattice inputs for machine translation. Huang and Chen (2019) proposed to adapt the transformer model originally pre-trained on written texts to consume lattices in order to improve SLU performance. Buckman and Neubig (2018) also found that utilizing lattices that represent multiple granularities of sentences can improve language modeling. With recent introduction of large pre-trained l"
2020.acl-main.63,J84-3009,0,0.196468,"Missing"
2020.acl-main.63,W17-5525,0,0.153266,"Missing"
2020.acl-main.63,P18-1203,0,0.0704188,"Missing"
2020.acl-main.63,N19-1410,0,0.0243395,"inese to English; the goal of automatic speech recognition (ASR) is opposite to the one of text-to-speech (TTS) (Tjandra et al., 2017), and so on. Previous work first exploited the duality of the task pairs and proposed supervised (Xia et al., 2017) and unsupervised (reinforcement learning) (He et al., 2016) learning frameworks. These recent studies magnified the importance of the duality by revealing exploitation of it could boost the learning of both tasks. Su et al. (2019) employed the dual supervised learning framework to train NLU and NLG and improve both models simultaneously. Recently, Shen et al. (2019) improved models for conditional text generation using techniques from computational pragmatics. The techniques formulated language production as a game between speakers and listeners, where a speaker should generate text which a listener can use to correctly identify the original input the text describes. However, although the duality has been considered into the learning objective, two models in previous work are still trained separately. In contrast, this work proposes a general learning framework that trains the models jointly, so that unsupervised learning methods in this research field c"
2020.acl-main.63,P19-1545,1,0.251332,"Yu Su Chao-Wei Huang Yun-Nung Chen Department of Computer Science and Information Engineering National Taiwan University, Taipei, Taiwan {f05921117,r07922069}@ntu.edu.tw y.v.chen@ieee.org Abstract In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations. However, the dual property between understanding and generation has been rarely explored. The prior work (Su et al., 2019) is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework. However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion. The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NL"
2020.acl-main.63,D18-1416,1,0.929032,"e source code is available at: https://github. com/MiuLab/DuaLUG. components: a speech recognizer that transcribes a user’s speech input into texts, a natural language understanding module (NLU) to classify the domain along with domain-specific intents and fill in a set of slots to form a semantic frame (Tur and De Mori, 2011; Hakkani-T¨ur et al., 2016). A dialogue state tracking (DST) module predicts the current dialogue state according to the multi-turn conversations, then the dialogue policy determines the system action for the next turn given the current dialogue state (Peng et al., 2018; Su et al., 2018a). Finally, the semantic frame indicating the policy is fed into a natural language generationt (NLG) module to construct a response utterance to the user (Wen et al., 2015b; Su et al., 2018b). Generally, NLU is to extract core semantic concepts from the given utterances, while NLG is to construct corresponding sentences based on the given semantic representations. However, the dual property between understanding and generation has been rarely investigated, Su et al. (2019) first introduced the duality into the typical supervised learning schemes to train these two models. Different from the"
2020.acl-main.63,N18-2010,1,0.939229,"e source code is available at: https://github. com/MiuLab/DuaLUG. components: a speech recognizer that transcribes a user’s speech input into texts, a natural language understanding module (NLU) to classify the domain along with domain-specific intents and fill in a set of slots to form a semantic frame (Tur and De Mori, 2011; Hakkani-T¨ur et al., 2016). A dialogue state tracking (DST) module predicts the current dialogue state according to the multi-turn conversations, then the dialogue policy determines the system action for the next turn given the current dialogue state (Peng et al., 2018; Su et al., 2018a). Finally, the semantic frame indicating the policy is fed into a natural language generationt (NLG) module to construct a response utterance to the user (Wen et al., 2015b; Su et al., 2018b). Generally, NLU is to extract core semantic concepts from the given utterances, while NLG is to construct corresponding sentences based on the given semantic representations. However, the dual property between understanding and generation has been rarely investigated, Su et al. (2019) first introduced the duality into the typical supervised learning schemes to train these two models. Different from the"
2020.acl-main.63,N18-1194,1,0.928722,"e source code is available at: https://github. com/MiuLab/DuaLUG. components: a speech recognizer that transcribes a user’s speech input into texts, a natural language understanding module (NLU) to classify the domain along with domain-specific intents and fill in a set of slots to form a semantic frame (Tur and De Mori, 2011; Hakkani-T¨ur et al., 2016). A dialogue state tracking (DST) module predicts the current dialogue state according to the multi-turn conversations, then the dialogue policy determines the system action for the next turn given the current dialogue state (Peng et al., 2018; Su et al., 2018a). Finally, the semantic frame indicating the policy is fed into a natural language generationt (NLG) module to construct a response utterance to the user (Wen et al., 2015b; Su et al., 2018b). Generally, NLU is to extract core semantic concepts from the given utterances, while NLG is to construct corresponding sentences based on the given semantic representations. However, the dual property between understanding and generation has been rarely investigated, Su et al. (2019) first introduced the duality into the typical supervised learning schemes to train these two models. Different from the"
2020.acl-main.63,W15-4639,0,0.193322,"rstanding module (NLU) to classify the domain along with domain-specific intents and fill in a set of slots to form a semantic frame (Tur and De Mori, 2011; Hakkani-T¨ur et al., 2016). A dialogue state tracking (DST) module predicts the current dialogue state according to the multi-turn conversations, then the dialogue policy determines the system action for the next turn given the current dialogue state (Peng et al., 2018; Su et al., 2018a). Finally, the semantic frame indicating the policy is fed into a natural language generationt (NLG) module to construct a response utterance to the user (Wen et al., 2015b; Su et al., 2018b). Generally, NLU is to extract core semantic concepts from the given utterances, while NLG is to construct corresponding sentences based on the given semantic representations. However, the dual property between understanding and generation has been rarely investigated, Su et al. (2019) first introduced the duality into the typical supervised learning schemes to train these two models. Different from the prior work, this paper proposes a general learning framework leveraging the duality between understanding and generation, providing flexibility of incorporating not only sup"
2020.acl-main.63,E17-1042,0,0.0256506,"tion Spoken dialogue systems that assist users to solve complex tasks such as booking a movie ticket have become an emerging research topic in artificial intelligence and natural language processing areas. With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. Nowadays, there are several virtual intelligent assistants, such as Apple’s Siri, Google Assistant, Microsoft’s Cortana, and Amazon’s Alexa. The recent advance of deep learning has inspired many applications of neural dialogue systems (Wen et al., 2017; Bordes et al., 2017). A typical dialogue system pipeline can be divided into several 1 The source code is available at: https://github. com/MiuLab/DuaLUG. components: a speech recognizer that transcribes a user’s speech input into texts, a natural language understanding module (NLU) to classify the domain along with domain-specific intents and fill in a set of slots to form a semantic frame (Tur and De Mori, 2011; Hakkani-T¨ur et al., 2016). A dialogue state tracking (DST) module predicts the current dialogue state according to the multi-turn conversations, then the dialogue policy determine"
2020.acl-main.63,D15-1199,0,0.0880086,"Missing"
2020.emnlp-main.233,D18-1547,0,0.0209833,"h 115,000 DBPedia 115,000 Yahoo 115,000 Experimental Setup To evaluate the proposed method, we conduct a set of experiments detailed below. 3.1 Datasets To evaluate the capability of L2KD on diverse sequence generation tasks, we pick the following three tasks from DecaNLP (McCann et al., 2018): • WikiSQL (Zhong et al., 2017): a dataset for developing natural language interfaces for relational databases, in which the model needs to generate structured queries from natural language. • CNN/DailyMail (See et al., 2017): a text summarization dataset collected from online news articles. • MultiWOZ (Budzianowski et al., 2018): a multi-domain wizard-of-oz dataset for taskoriented dialogue modeling, in which the model needs to generate the semantic state sequences based on the given partial dialogues. Note that we skip machine translation dataset (IWSLT) in DecaNLP here, because GPT-2 does 2 7,600 7,600 7,600 7,600 7,600 Model and Training Details We build our proposed approach based on the implementation of LAMOL2 to make the results comparable. We use the same pre-trained small GPT2 (Radford et al., 2019) for all single-task teacher, multitask and LLL models, and train the GPT-2 nine epochs for each dataset. We us"
2020.emnlp-main.233,P15-2123,0,0.0302832,"tting problem (McCloskey and Cohen, 1989): after learning a new task, the model forgets how to handle the samples from previous tasks. Lifelong learning manages to accumulate the knowledge and retain the performance of previously learned tasks. It is important especially for real-world natural language processing (NLP) applications, because these applications need to interact with many users from different domains everyday, and the language usage also evolves from time to time. Hence, various NLP tasks have been studied for lifelong learning in the previous work, including sentiment analysis (Chen et al., 2015; Xia et al., 1 The source code and data are available at https:// github.com/voidism/L2KD. 2017), conversational agents (Lee, 2017), word and sentence representation learning (Xu et al., 2018; Liu et al., 2019), text classification, and question answering (d’Autume et al., 2019). In recent, LAMOL (Sun et al., 2020) improved the performance of LLL by a general framework: 1) it followed the idea about considering many NLP tasks as question answering (QA) (McCann et al., 2018) and adapted all tasks into the language modeling (LM) form. In the unified framework, it can perform LLL on many NLP tas"
2020.emnlp-main.233,P19-1595,0,0.0190315,", 2016), in which a student (smaller) model is trained to imitate the behavior of a teacher (larger) model in order to reach the performance closer to the teacher model, the LLL model in L2KD can be seen as a weak learner that needs to compress knowledge from different tasks into a compact single model. Thus LLL can benefit from the similar procedure of knowledge distillation, although the model size is equal to its teacher model. The similar idea about distilling knowledge from equal-size models has also been studied in born-again neural network (Furlanello et al., 2018), multitask learning (Clark et al., 2019) and lifelong computer vision learning (Hou et al., 2018), but never been explored in lifelong language learning research. In L2KD, we train a new teacher model when facing a new task, and the LLL model imitates the behavior of its teacher at each training stage, as illustrated in Figure 1. This method only needs a little extra time to train a disposable teacher model for each new task, and the teacher model can be discarded when learning the next task; therefore, there is no extra memory or model capacity required for the target LLL model, making the proposed model more memory-efficient for r"
2020.emnlp-main.233,D16-1139,0,0.183166,"for sequence generation learning. 2914 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2914–2924, c November 16–20, 2020. 2020 Association for Computational Linguistics The motivation of our work mainly comes from how to efficiently compress the knowledge under a lifelong language learning framework. If the model can learn a new task in an efficient way, the previously learned knowledge may not be affected and thus the problem of catastrophic forgetting can be mitigated. Inspired by knowledge distillation (Bucila et al., 2006; Hinton et al., 2015; Kim and Rush, 2016), in which a student (smaller) model is trained to imitate the behavior of a teacher (larger) model in order to reach the performance closer to the teacher model, the LLL model in L2KD can be seen as a weak learner that needs to compress knowledge from different tasks into a compact single model. Thus LLL can benefit from the similar procedure of knowledge distillation, although the model size is equal to its teacher model. The similar idea about distilling knowledge from equal-size models has also been studied in born-again neural network (Furlanello et al., 2018), multitask learning (Clark e"
2020.emnlp-main.233,N19-1331,0,0.0609635,"Missing"
2020.emnlp-main.233,W17-5525,0,0.0359748,"Missing"
2020.emnlp-main.233,P17-1099,0,0.0183562,"orithm 1. 3 Text Classification for Different Tasks AGNews 115,000 Yelp 115,000 Amazon Exact Match 115,000 DBPedia 115,000 Yahoo 115,000 Experimental Setup To evaluate the proposed method, we conduct a set of experiments detailed below. 3.1 Datasets To evaluate the capability of L2KD on diverse sequence generation tasks, we pick the following three tasks from DecaNLP (McCann et al., 2018): • WikiSQL (Zhong et al., 2017): a dataset for developing natural language interfaces for relational databases, in which the model needs to generate structured queries from natural language. • CNN/DailyMail (See et al., 2017): a text summarization dataset collected from online news articles. • MultiWOZ (Budzianowski et al., 2018): a multi-domain wizard-of-oz dataset for taskoriented dialogue modeling, in which the model needs to generate the semantic state sequences based on the given partial dialogues. Note that we skip machine translation dataset (IWSLT) in DecaNLP here, because GPT-2 does 2 7,600 7,600 7,600 7,600 7,600 Model and Training Details We build our proposed approach based on the implementation of LAMOL2 to make the results comparable. We use the same pre-trained small GPT2 (Radford et al., 2019) for"
2020.emnlp-main.233,D15-1199,0,0.0423268,"Missing"
2020.emnlp-main.555,J84-3009,0,0.334252,"Missing"
2020.emnlp-main.555,D17-1151,0,0.0324181,"ntion module, the input representations should also contain the position information. In Transformers (Vaswani et al., 2017), a word embedding is directly added with the positional encoding as the final representation: zi = W E(xi ) + P E(i), where xi is the token at the i-th position, W E is the word embedding, and P E is the positional encoding, which can be either a learnable embedding or a pre-defined function. Multi-Head Self-Attention The attention mechanism is often used in an encoder-decoder architecture, and there are many variants of attention implementations (Bahdanau et al., 2014; Britz et al., 2017). In Transformers, the scaled dot-product attention is applied: attention(Q, K, V ) = softmax( QW K T W √ )V W, dk where W is a linear projection and Q, K, V represent query, key and value matrices respectively. Transformer blocks are composed of multi-head self-attention. Literally, the inputs Q, K, V are the same and the attention is performed multiple times, and then the output heads are concatenated as the final output hidden state h. This process can be formulated as headi = attention(Q, K, V ) h = concat([head1 , ..., headn ])W. Transformer Encoder A Transformer encoder layer is composed"
2020.emnlp-main.555,P19-1285,0,0.0149746,"became a trend among many NLP tasks after (Peters et al., 2018) introduced ELMo. Affected by ELMo, OpenAI GPT (Radford et al., 2018) is the first pretrained language model using a Transformer architecture, then many different variant of pre-trained Transformer including BERT (Devlin et al., 2018), RoBERTa (Roberts, 2005) and GPT-2 (Radford et al., 2019) started evolving the researches of NLP tremendously. In Transformers, the attention values are the same in each input position. Thus, Shaw et al. (2018) proposed a relative position representation in the attention level to address this issue. Dai et al. (2019) used a segment-level recurrence mechanism on Transformers and also utilized an adaptive version of relative position embeddings inspired by Shaw et al. (2018). Furthermore, Wang et al. (2019) extended the embedding space from real numbers to complex values , and also proposed a new learnable positional encoding function instead of a simple position embedding mapping. 3 Transformer Transformer is an encoder-decoder sequence-tosequence model proposed by Vaswani et al. (2017). In the architecture, Transformer is composed of self-attention blocks that are position-insensitive modules. Therefore,"
2020.emnlp-main.555,2020.acl-main.703,0,0.0586018,"Missing"
2020.emnlp-main.555,2021.ccl-1.108,0,0.0613842,"Missing"
2020.emnlp-main.555,N19-4009,0,0.0152776,"However, in the larger dataset: Wikitext-103, skip position attack leads BERT position embeddings to extremely awful performance. The observation here is consistent with the inferences mentioned in section 4, and we can conclude that position embeddings of Transformer encoders focus on capturing the information nearby, especially BERT, which involves even less position information than RoBERTa. 5.3 PE Experimental Setup We experiment on the Multi30k English-German dataset from WMT2016 shared tasks. The properties of the dataset are shown in Table 5. We use the scripts implemented by Fairseq (Ott et al., 2019) for a faster training process. The encoder and decoder have both 6 layers where each layer has 4 heads, 512, and 1024 hidden size for attention head and feed-forward respectively. Also, byte-pair encoding (BPE) (Sennrich et al., 2015; Gage, 1994) is applied to the corpus and the vocabulary size is reduced to 10, 000. Sentence Pairs Average Length Train Valid Test 29, 000 12 1, 015 12 1, 000 12 Table 5: Statistics of Multi30k dataset. To respectively investigate the effectiveness on the encoder and the decoder, there are total four different initialization settings of pre-trained position embe"
2020.emnlp-main.555,N18-1202,0,0.0386337,"osing the suitable positional encoding method in the target task. 2 Related Work The concept of using position embedding on position-insensitive models was first proposed by convolutional seq2seq (Gehring et al., 2017), which built an encoder-decoder architecture on convolutional neural networks. Vaswani et al. (2017) proposed Transformers that used the self-attention mechanism in the basic blocks. Because the attention mechanism is position-insensitive, it proposed a pre-defined sinusoidal function as positional encoding. Pre-trained language models became a trend among many NLP tasks after (Peters et al., 2018) introduced ELMo. Affected by ELMo, OpenAI GPT (Radford et al., 2018) is the first pretrained language model using a Transformer architecture, then many different variant of pre-trained Transformer including BERT (Devlin et al., 2018), RoBERTa (Roberts, 2005) and GPT-2 (Radford et al., 2019) started evolving the researches of NLP tremendously. In Transformers, the attention values are the same in each input position. Thus, Shaw et al. (2018) proposed a relative position representation in the attention level to address this issue. Dai et al. (2019) used a segment-level recurrence mechanism on T"
2020.emnlp-main.555,P05-2009,0,0.051746,"n convolutional neural networks. Vaswani et al. (2017) proposed Transformers that used the self-attention mechanism in the basic blocks. Because the attention mechanism is position-insensitive, it proposed a pre-defined sinusoidal function as positional encoding. Pre-trained language models became a trend among many NLP tasks after (Peters et al., 2018) introduced ELMo. Affected by ELMo, OpenAI GPT (Radford et al., 2018) is the first pretrained language model using a Transformer architecture, then many different variant of pre-trained Transformer including BERT (Devlin et al., 2018), RoBERTa (Roberts, 2005) and GPT-2 (Radford et al., 2019) started evolving the researches of NLP tremendously. In Transformers, the attention values are the same in each input position. Thus, Shaw et al. (2018) proposed a relative position representation in the attention level to address this issue. Dai et al. (2019) used a segment-level recurrence mechanism on Transformers and also utilized an adaptive version of relative position embeddings inspired by Shaw et al. (2018). Furthermore, Wang et al. (2019) extended the embedding space from real numbers to complex values , and also proposed a new learnable positional e"
2020.emnlp-main.555,N18-2074,0,0.0307538,"nsensitive, it proposed a pre-defined sinusoidal function as positional encoding. Pre-trained language models became a trend among many NLP tasks after (Peters et al., 2018) introduced ELMo. Affected by ELMo, OpenAI GPT (Radford et al., 2018) is the first pretrained language model using a Transformer architecture, then many different variant of pre-trained Transformer including BERT (Devlin et al., 2018), RoBERTa (Roberts, 2005) and GPT-2 (Radford et al., 2019) started evolving the researches of NLP tremendously. In Transformers, the attention values are the same in each input position. Thus, Shaw et al. (2018) proposed a relative position representation in the attention level to address this issue. Dai et al. (2019) used a segment-level recurrence mechanism on Transformers and also utilized an adaptive version of relative position embeddings inspired by Shaw et al. (2018). Furthermore, Wang et al. (2019) extended the embedding space from real numbers to complex values , and also proposed a new learnable positional encoding function instead of a simple position embedding mapping. 3 Transformer Transformer is an encoder-decoder sequence-tosequence model proposed by Vaswani et al. (2017). In the archi"
2020.findings-emnlp.198,J84-3009,0,0.577275,"Missing"
2020.findings-emnlp.198,D17-1042,0,0.0118501,"t rationalization for better interpretability.1 1 Introduction Resolving NLP tasks by deep neural networks has been proven to be effective, and it is also important to investigate how the models make such a decision. For example, only providing the prediction to medical tasks may not be enough, and providing the associated reasons is more crucial for the practical applications. Therefore, there has been increasing attempts that focus on interpretability or explainability of the machine-learned models. There are different ways of explaining how machines make the decision (Ribeiro et al., 2016; Alvarez-Melis and Jaakkola, 2017; Lee et al., 2019; Liu et al., 2019a), and one of these methods is to extract rationales (Lei et al., 2016; DeYoung et al., 2019). However, most prior work focused on extracting rationales in a supervised manner (DeYoung et al., 2019), but not all datasets contain such annotated rationales for model learning, making the rationalization task difficult and impractical. 1 The source code and the processed data is available at: https://github.com/MiuLab/ ZeroShotRationale. y.v.chen@ieee.org Rationalization is defined as a task that focuses on extracting the rationales from the input texts for bet"
2020.findings-emnlp.198,N19-1371,0,0.132465,"Missing"
2020.findings-emnlp.198,P19-1441,0,0.0213917,"troduction Resolving NLP tasks by deep neural networks has been proven to be effective, and it is also important to investigate how the models make such a decision. For example, only providing the prediction to medical tasks may not be enough, and providing the associated reasons is more crucial for the practical applications. Therefore, there has been increasing attempts that focus on interpretability or explainability of the machine-learned models. There are different ways of explaining how machines make the decision (Ribeiro et al., 2016; Alvarez-Melis and Jaakkola, 2017; Lee et al., 2019; Liu et al., 2019a), and one of these methods is to extract rationales (Lei et al., 2016; DeYoung et al., 2019). However, most prior work focused on extracting rationales in a supervised manner (DeYoung et al., 2019), but not all datasets contain such annotated rationales for model learning, making the rationalization task difficult and impractical. 1 The source code and the processed data is available at: https://github.com/MiuLab/ ZeroShotRationale. y.v.chen@ieee.org Rationalization is defined as a task that focuses on extracting the rationales from the input texts for better justification and interpretation"
2020.findings-emnlp.198,P11-1015,0,0.530533,"model for three tasks at the same time as illustrated in Figure 2. During testing, given a context from any domain with a corresponding question, the question-answering module (the right branch) is capable of finding the associated rationale we expect without training on the rationales from the target domain (highlighted in red in Figure 2), achieving zero-shot rationalization. 3.1 Shared Layer MovieReview This dataset contains the reviews obtained from IMDB, where each review was labeled as positive or negative without any rationales, because it is originally proposed for sentiment analysis (Maas et al., 2011). Another similar dataset consists of 2,000 movie reviews from IMDB with their rationales that explain why the review is positive/negative (Zaidan et al., 2007). Note that each review may contain multiple rationales. Hence, we similarly utilize the annotated rationales as the testing data for validating the performance of zero-shot rationalization. Input Document Task-Specific Predictor sentence covers. These annotations can be seen as the rationale of the aspect-specific rating, which can be used for evaluating the extracted rationales. (5) Hence, we can obtain the answer span yqa based on th"
2020.findings-emnlp.198,P19-1487,0,0.0377253,"of the extracted rationales. Moreover, DeYoung et al. (2019) proposed to learn rationale extraction in a supervised manner and prepared the benchmark experiments in diverse rationalization tasks. From the experimental results, it can be found that supervised learning for rationalization may not be always better than the unsupervised method due to the complex reasoning process. Considering that in the practical application, the target domain may not contain the annotated rationales for supervised training, transferring the knowledge about rationalization to the target domain may be applicable. Rajani et al. (2019) proposed to utilize the pre-trained language model for explaining the common sense towards zero-shot knowledge transfer. However, it requires that the target domain should be covered by the pre-trained language model so that the common sense questions can be well-answered. Such requirements limit the potential of being applied to a lot of realworld applications, because the target domain we aim at extracting rationales for may not be general (e.g. medical texts and financial texts may not be covered by the pre-trained model). Instead of directly transferring the knowledge to the target domain"
2020.findings-emnlp.198,D16-1264,0,0.333501,"ark rationalization datasets without any annotated rationales. 2 Datasets This paper focuses on zero-shot rationalization by transferring knowledge from question answering. Thus, three datasets are used in the experiments, where a QA dataset, SQuAD 2.0, is utilized for the transfer purpose and two benchmark rationalization datasets, BeerReview and MovieReview, are used for evaluating the performance of zero-shot rationalization for the proposed method. The datasets are briefly introduced below, and their statistics is detailed in Table 1. SQuAD 2.0 Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is a benchmark dataset for reading comprehension, consisting of questions posed by crowdworkers on over 500 Wikipedia articles. The answer to each question is a text span from the corresponding paragraph. There are 100,000 answerable questions and over 50,000 unanswerable questions similar to answerable ones written by crowdworkers adversarially. This data is for enhancing the ability of text understanding in our model so that we can transfer the knowledge to zero-shot rationalization. BeerReview This is a beer review dataset processed by Lei et al. (2016)2 , which contains 1.5 million review"
2020.findings-emnlp.198,N16-3020,0,0.0440524,"potential of zero-shot rationalization for better interpretability.1 1 Introduction Resolving NLP tasks by deep neural networks has been proven to be effective, and it is also important to investigate how the models make such a decision. For example, only providing the prediction to medical tasks may not be enough, and providing the associated reasons is more crucial for the practical applications. Therefore, there has been increasing attempts that focus on interpretability or explainability of the machine-learned models. There are different ways of explaining how machines make the decision (Ribeiro et al., 2016; Alvarez-Melis and Jaakkola, 2017; Lee et al., 2019; Liu et al., 2019a), and one of these methods is to extract rationales (Lei et al., 2016; DeYoung et al., 2019). However, most prior work focused on extracting rationales in a supervised manner (DeYoung et al., 2019), but not all datasets contain such annotated rationales for model learning, making the rationalization task difficult and impractical. 1 The source code and the processed data is available at: https://github.com/MiuLab/ ZeroShotRationale. y.v.chen@ieee.org Rationalization is defined as a task that focuses on extracting the ratio"
2020.findings-emnlp.198,D19-1420,0,0.020107,"difficult and impractical. 1 The source code and the processed data is available at: https://github.com/MiuLab/ ZeroShotRationale. y.v.chen@ieee.org Rationalization is defined as a task that focuses on extracting the rationales from the input texts for better justification and interpretation. Lei et al. (2016) is the first work that attempted to extract rationales in order to justify the model’s answers, where a rationale generator extracts the context and a predictor generates the answer based on the extracted rationales. This method shows great precision in extracting rationales. Recently, Yu et al. (2019) proposed an introspective model, an extension of the prior work that further improved the comprehensiveness of the extracted rationales. Moreover, DeYoung et al. (2019) proposed to learn rationale extraction in a supervised manner and prepared the benchmark experiments in diverse rationalization tasks. From the experimental results, it can be found that supervised learning for rationalization may not be always better than the unsupervised method due to the complex reasoning process. Considering that in the practical application, the target domain may not contain the annotated rationales for s"
2020.findings-emnlp.198,N07-1033,0,0.0258845,"-answering module (the right branch) is capable of finding the associated rationale we expect without training on the rationales from the target domain (highlighted in red in Figure 2), achieving zero-shot rationalization. 3.1 Shared Layer MovieReview This dataset contains the reviews obtained from IMDB, where each review was labeled as positive or negative without any rationales, because it is originally proposed for sentiment analysis (Maas et al., 2011). Another similar dataset consists of 2,000 movie reviews from IMDB with their rationales that explain why the review is positive/negative (Zaidan et al., 2007). Note that each review may contain multiple rationales. Hence, we similarly utilize the annotated rationales as the testing data for validating the performance of zero-shot rationalization. Input Document Task-Specific Predictor sentence covers. These annotations can be seen as the rationale of the aspect-specific rating, which can be used for evaluating the extracted rationales. (5) Hence, we can obtain the answer span yqa based on the predicted answer start as and answer end ae . 2189 Multi-Task Training Zero-Shot Rationalization Movie Review Beer Review Encoder Encoder sigmoid Rating Score"
2020.findings-emnlp.443,J84-3009,0,0.195436,"Missing"
2020.findings-emnlp.443,H90-1021,0,0.343379,"Missing"
2020.findings-emnlp.443,W17-5525,0,0.0396756,"Missing"
2020.findings-emnlp.443,P16-1162,0,0.0109874,"the evaluation metrics include BLEU and ROUGE-(1, 2, L) scores with multiple references. The hyperparameters and other training settings are reported in Appendix A. • SNIPS (Coucke et al., 2018): an NLU dataset focusing on evaluating voice assistants for multiple domains, which has sentence-level intents and word-level slot tags. • E2E NLG (Novikova et al., 2017): an NLG dataset in the restaurant domain, where each meaning representation has up to 5 references in natural language and no intent labels. We use the open-sourced Tokenizers2 package for preprocessing with byte-pair-encoding (BPE) (Sennrich et al., 2016). The details of datasets are shown in Table 1, where the vocabulary size is based on BPE subwords. We augment NLU data for NLG usage and NLG data for NLU usage, and the augmentation strategy are detailed in Appendix C. 3.2 Results and Analysis Three baselines are performed for each dataset: (1) Iterative Baseline: simply training NLU and NLG iteratively, (2) Dual Supervised Learning (Su et al., 2019), and (3) Joint Baseline: the output from one model is sent to another as in Su et al. (2020)3 . In joint baselines, the outputs of NLU are intent and IOB-slot tags, whose modalities are different"
2020.findings-emnlp.443,P19-1545,1,0.89046,"(reinforcement learning) (He et al., 2016) learning frameworks in machine translation. The recent studies magnified the importance of the duality by revealing exploitation of it could boost the learning for both tasks. Natural language understanding (NLU) (Tur and De Mori, 2011; Hakkani-T¨ur et al., 2016) and natural language generation (NLG) (Wen et al., 2015; Su et al., 2018) are two major components in modular conversational systems, where NLU extracts core semantic concepts from the given utterances, and NLG constructs the associated sentences based on the given semantic representations. Su et al. (2019) was the first attempt that leveraged the duality in dialogue modeling and employed the dual supervised learning framework for training NLU and NLG. Furthermore, Su et al. (2020) proposed a joint learning framework that can train two modules seamlessly towards the potential of unsupervised NLU and NLG. Recently, Zhu et al. (2020) proposed a semi-supervised framework to learn NLU with an auxiliary generation model for pseudo-labeling to make use of unlabeled data. Despite the effectiveness showed by the prior work, they all focused on leveraging the duality in the training process to obtain pow"
2020.findings-emnlp.443,2020.acl-main.63,1,0.833751,"could boost the learning for both tasks. Natural language understanding (NLU) (Tur and De Mori, 2011; Hakkani-T¨ur et al., 2016) and natural language generation (NLG) (Wen et al., 2015; Su et al., 2018) are two major components in modular conversational systems, where NLU extracts core semantic concepts from the given utterances, and NLG constructs the associated sentences based on the given semantic representations. Su et al. (2019) was the first attempt that leveraged the duality in dialogue modeling and employed the dual supervised learning framework for training NLU and NLG. Furthermore, Su et al. (2020) proposed a joint learning framework that can train two modules seamlessly towards the potential of unsupervised NLU and NLG. Recently, Zhu et al. (2020) proposed a semi-supervised framework to learn NLU with an auxiliary generation model for pseudo-labeling to make use of unlabeled data. Despite the effectiveness showed by the prior work, they all focused on leveraging the duality in the training process to obtain powerful NLU and NLG models. However, there has been little investigation on how to leverage the dual relationship into the inference stage. Considering the fast-growing scale of mo"
2020.findings-emnlp.443,N18-2010,1,0.850121,"ncluding machine translation (Wu et al., 2016), speech recognition and synthesis (Tjandra et al., 2017), and so on. Previous work first exploited the duality of the task pairs and proposed supervised (Xia et al., 2017) and unsupervised (reinforcement learning) (He et al., 2016) learning frameworks in machine translation. The recent studies magnified the importance of the duality by revealing exploitation of it could boost the learning for both tasks. Natural language understanding (NLU) (Tur and De Mori, 2011; Hakkani-T¨ur et al., 2016) and natural language generation (NLG) (Wen et al., 2015; Su et al., 2018) are two major components in modular conversational systems, where NLU extracts core semantic concepts from the given utterances, and NLG constructs the associated sentences based on the given semantic representations. Su et al. (2019) was the first attempt that leveraged the duality in dialogue modeling and employed the dual supervised learning framework for training NLU and NLG. Furthermore, Su et al. (2020) proposed a joint learning framework that can train two modules seamlessly towards the potential of unsupervised NLU and NLG. Recently, Zhu et al. (2020) proposed a semi-supervised framew"
2020.findings-emnlp.443,D15-1199,0,0.0347227,"Missing"
2021.naacl-main.318,2020.acl-main.282,0,0.125257,"ments, our proposed framework is able to improve upon best-performing predictors on the benchmark MIMIC datasets. 1 a set of ICD prediction …250.61, 357.2, 564.0, 564.5, 401.9… 564.0 : Constipation 564.5 : Functional diarrhea Figure 1: An example of conflicting predictions. While these two codes share the same root in the hierarchical ICD structure and are semantically similar, they are unlikely to appear together. ICD coding has been improved significantly (Choi et al., 2016; Shi et al., 2017; Mullenbach et al., 2018; Baumel et al., 2018; Xie and Xing, 2018; Li and Yu, 2020; Vu et al., 2020; Cao et al., 2020). Prior work on neural models mostly treated the task of automatic ICD coding as a multi-label classification problem. These models mostly employ a shared text encoder, and build one binary clas1 Introduction sifier for each label on top of the encoder. This architecture along side with binary cross-entropy Clinical notes from electronic health records loss make the prediction of each label independent (EHRs) are free-from text generated by clinicians of each other, which might lead to incomplete or during patient visits. The associated diagnostic conflicting predictions. An example of such er"
2021.naacl-main.318,J05-1003,0,0.121447,"s only. Proposed Framework Candidate Generation In the first stage of the framework, we employ a base predictor to perform probabilistic prediction for all labels, and we use the predicted probabilities to generate top-k most probable label sets. More formally, given a clinical note x, we perform a base predictor and obtain the prediction for all labels: The concept of retrieve-and-rerank has been Pbase (yi = 1 |x, θbase ), i = 1, 2, · · · , |Y|, widely used in automatic speech recognition (Ostendorf et al., 1991), natural language processwhere θbase denotes the parameters of the base preing (Collins and Koo, 2005) and machine transdictor. The predicted results are used to generate lation (Shen et al., 2004). Li et al. (2019) proˆ ⊆ Y with top-k highest top-k probable sets, i.e., y posed to rerank the possible predictions generated probability prediction: by a base predictor with a calibrator. This method is conceptually similar to our framework, where |Y| Y we both follow the retrieve-and-rerank procedure. Pbase (ˆ y |x, θbase ) = Pbase (yi = yˆi |x, θbase ). The main difference between is that they leveri=1 aged an extra dataset for training the calibrator, Although there are 2|Y |possible subsets, th"
2021.naacl-main.318,P19-1285,0,0.0431697,"Missing"
2021.naacl-main.318,N18-1100,0,0.0609917,"Missing"
2021.naacl-main.318,H91-1013,0,0.458289,"terms to leverage the ICD structure (Tsai ∗ Equal contribution. 1 et al., 2019). However, they borrowed the depenThe source code of this project is available at https://github.com/MiuLab/ICD-Correlation. dency from domain experts and did not consider 4043 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4043–4052 June 6–11, 2021. ©2021 Association for Computational Linguistics the label correlation in the data. 2 Inspired by the success of reranking techniques on automatic speech recognition (Ostendorf et al., 1991) and dependency parsing (Zhu et al., 2015; Sangati et al., 2009), we propose a two-stage reranking framework for ICD code prediction, which captures the label correlation without any expert knowledge. In the first stage, we use a base predictor to generate possible label set candidates. In the second stage, a label set reranker is employed to rerank the candidates. We design two rerankers to help to capture the correlation between labels. The experimental results show that our proposed framework consistently improves the results of different base predictors on the benchmark MIMIC datasets (Sae"
2021.naacl-main.318,W09-3839,0,0.0565599,"et al., 2019). However, they borrowed the depenThe source code of this project is available at https://github.com/MiuLab/ICD-Correlation. dency from domain experts and did not consider 4043 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4043–4052 June 6–11, 2021. ©2021 Association for Computational Linguistics the label correlation in the data. 2 Inspired by the success of reranking techniques on automatic speech recognition (Ostendorf et al., 1991) and dependency parsing (Zhu et al., 2015; Sangati et al., 2009), we propose a two-stage reranking framework for ICD code prediction, which captures the label correlation without any expert knowledge. In the first stage, we use a base predictor to generate possible label set candidates. In the second stage, a label set reranker is employed to rerank the candidates. We design two rerankers to help to capture the correlation between labels. The experimental results show that our proposed framework consistently improves the results of different base predictors on the benchmark MIMIC datasets (Saeed et al., 2011; Johnson et al., 2016). The results also show th"
2021.naacl-main.318,P15-1112,0,0.13431,"al contribution. 1 et al., 2019). However, they borrowed the depenThe source code of this project is available at https://github.com/MiuLab/ICD-Correlation. dency from domain experts and did not consider 4043 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4043–4052 June 6–11, 2021. ©2021 Association for Computational Linguistics the label correlation in the data. 2 Inspired by the success of reranking techniques on automatic speech recognition (Ostendorf et al., 1991) and dependency parsing (Zhu et al., 2015; Sangati et al., 2009), we propose a two-stage reranking framework for ICD code prediction, which captures the label correlation without any expert knowledge. In the first stage, we use a base predictor to generate possible label set candidates. In the second stage, a label set reranker is employed to rerank the candidates. We design two rerankers to help to capture the correlation between labels. The experimental results show that our proposed framework consistently improves the results of different base predictors on the benchmark MIMIC datasets (Saeed et al., 2011; Johnson et al., 2016). T"
2021.naacl-main.318,N04-1023,0,0.122321,"se predictor to perform probabilistic prediction for all labels, and we use the predicted probabilities to generate top-k most probable label sets. More formally, given a clinical note x, we perform a base predictor and obtain the prediction for all labels: The concept of retrieve-and-rerank has been Pbase (yi = 1 |x, θbase ), i = 1, 2, · · · , |Y|, widely used in automatic speech recognition (Ostendorf et al., 1991), natural language processwhere θbase denotes the parameters of the base preing (Collins and Koo, 2005) and machine transdictor. The predicted results are used to generate lation (Shen et al., 2004). Li et al. (2019) proˆ ⊆ Y with top-k highest top-k probable sets, i.e., y posed to rerank the possible predictions generated probability prediction: by a base predictor with a calibrator. This method is conceptually similar to our framework, where |Y| Y we both follow the retrieve-and-rerank procedure. Pbase (ˆ y |x, θbase ) = Pbase (yi = yˆi |x, θbase ). The main difference between is that they leveri=1 aged an extra dataset for training the calibrator, Although there are 2|Y |possible subsets, the topwhile we train a distribution estimator on the same k sets can be efficiently generated wi"
2021.naacl-main.318,D19-6206,1,0.898391,"of ICD prediction. On the other hand, several work tried to integrate external medical knowledge into this task. In order to leverage the information of definition of each ICD code, RNN and CNN were adopted to encode the diagnostic descriptions of ICD codes for better prediction via attention mechanism (Shi et al., 2017; Mullenbach et al., 2018). Moreover, the prior work tried to consider the hierarchical structure of ICD codes (Xie and Xing, 2018), which proposed a tree-of-sequences LSTM to simultaneously capture the hierarchical relationship among codes and the semantics of each code. Also, Tsai et al. (2019) introduced various ways of leveraging the hierarchical knowledge of ICD by adding refined loss functions. Recently, Cao et al. (2020) proposed to train ICD code embeddings in hyperbolic space to model the hierarchical structure. Additionally, they used graph neural network to capture the code co-occurrences. 2.2 • This paper is the first attempt to improve multi-label classification with a reranking method for automatic ICD coding. • The experiments show that the proposed approaches are capable of improving bestperforming base predictors on the benchmark datasets MIMIC-2 and MIMIC-3, demonstr"
2021.naacl-main.318,1983.tc-1.13,0,0.124342,"Missing"
2021.naacl-main.318,P18-1098,0,0.158331,"king module for medical code prediction. In the experiments, our proposed framework is able to improve upon best-performing predictors on the benchmark MIMIC datasets. 1 a set of ICD prediction …250.61, 357.2, 564.0, 564.5, 401.9… 564.0 : Constipation 564.5 : Functional diarrhea Figure 1: An example of conflicting predictions. While these two codes share the same root in the hierarchical ICD structure and are semantically similar, they are unlikely to appear together. ICD coding has been improved significantly (Choi et al., 2016; Shi et al., 2017; Mullenbach et al., 2018; Baumel et al., 2018; Xie and Xing, 2018; Li and Yu, 2020; Vu et al., 2020; Cao et al., 2020). Prior work on neural models mostly treated the task of automatic ICD coding as a multi-label classification problem. These models mostly employ a shared text encoder, and build one binary clas1 Introduction sifier for each label on top of the encoder. This architecture along side with binary cross-entropy Clinical notes from electronic health records loss make the prediction of each label independent (EHRs) are free-from text generated by clinicians of each other, which might lead to incomplete or during patient visits. The associated diag"
2021.woah-1.12,D16-1120,0,0.0718591,"Missing"
2021.woah-1.12,S19-2082,0,0.0489843,"Missing"
2021.woah-1.12,D19-1418,0,0.128911,"TLD models; when the models are trained on the biased datasets, these biases are inherited by the models and further exacerbated during the learning process (Zhou et al., 2021). The biases in TLD systems can make the opinions from the members of minority groups more likely to be removed by the online platform, which may significantly hinder their experience as well as exacerbate the discrimination against them in real life. So far, many debiasing methods have been developed to mitigate biases in learned models, such as data re-balancing (Dixon et al., 2018), residual fitting (He et al., 2019; Clark et al., 2019), adversarial training (Xia et al., 2020) and data filtering approach (Bras et al., 2020; Zhou et al., 2021). While most of these works are successful on other natural language processing (NLP) tasks, their performance on debasing the TLD tasks are unsatisfactory (Zhou et al., 2021). A possible reason is that the toxicity of language is more subjective and nuanced than general NLP tasks that often have unequivocally correct labels (Zhou et al., 2021). As current debiasing techniques reduce the biased behaviors of models by correcting the training data or measuring the difficulty of modeling th"
2021.woah-1.12,D19-6115,0,0.0239427,"datasets for the TLD models; when the models are trained on the biased datasets, these biases are inherited by the models and further exacerbated during the learning process (Zhou et al., 2021). The biases in TLD systems can make the opinions from the members of minority groups more likely to be removed by the online platform, which may significantly hinder their experience as well as exacerbate the discrimination against them in real life. So far, many debiasing methods have been developed to mitigate biases in learned models, such as data re-balancing (Dixon et al., 2018), residual fitting (He et al., 2019; Clark et al., 2019), adversarial training (Xia et al., 2020) and data filtering approach (Bras et al., 2020; Zhou et al., 2021). While most of these works are successful on other natural language processing (NLP) tasks, their performance on debasing the TLD tasks are unsatisfactory (Zhou et al., 2021). A possible reason is that the toxicity of language is more subjective and nuanced than general NLP tasks that often have unequivocally correct labels (Zhou et al., 2021). As current debiasing techniques reduce the biased behaviors of models by correcting the training data or measuring the diff"
2021.woah-1.12,2021.ccl-1.108,0,0.0774712,"Missing"
2021.woah-1.12,D18-1302,0,0.0248916,"erficial correlation at the level of syntax and semantics, and makes the toxicity detector learn to use generalizable features for prediction, thus effectively reducing the impact of dataset biases and yielding a fair TLD model. 2 Previous works Debiasing the TLD Task Researchers have proposed a range of debiasing methods for the TLD task. Some of them try to mitigate the biases by processing the training dataset. For example, Dixon et al. (2018) add additional non-toxic examples containing the identity terms highly correlated to toxicity to balance their distribution in the training dataset. Park et al. (2018) use the combination of debiased word2vec and gender swap data augmentation to reduce the gender bias in TLD task. Badjatiya et al. (2019) apply the strategy of replacing the bias sensitive words (BSW) in training data based on multiple knowledge generalization. Some researchers pay more attention to modifying the models and learning less biased features. Xia et al. (2020) use adversarial training to reduce the tendency of the TLD system to misclassify the AAE texts as toxic speech. Mozafari et al. (2020) propose a novel re-weighting mechanism to alleviate the racial bias in English tweets. Va"
2021.woah-1.12,P19-1163,0,0.0535228,"Missing"
2021.woah-1.12,2020.acl-main.770,0,0.0148829,"t the model from picking up the spurious correlation between the certain trigger-words and toxicity labels. Debiasing Other NLP Task There are many methods proposed to mitigate the biases in NLP tasks other than TLD. Clark et al. (2019) train a robust classifier in an ensemble with a bias-only model to learn the more generalizable patterns in training dataset, which are difficult to be learned by the naive bias-only model. Bras et al. (2020) develop AFLITE, an iterative greedy algorithm that can adversarially filter the biases from the training dataset, as well as the framework to support it. Utama et al. (2020) introduce a novel approach of regularizing the confidence of models on the biased examples, which successfully makes the models perform well on both in-distribution and out-of-distribution data. 3 Invariant Rationalization 3.1 Basic Formulation for Rationalization We propose TLD debiasing based on I NV R AT in this paper. The goal of rationalization is to find a subset of inputs that 1) suffices to yield the same outcome 2) is human interpretable. Normally, we would prefer to find rationale in unsupervised ways because the lack of such annotations in the data. A typical formulation to find ra"
2021.woah-1.12,2020.socialnlp-1.2,0,0.0807152,"TM) (Bojkovsk`y and Pikuliak, 2019), logistic regression (Davidson et al., 2017) and fine-tuning BERT (d’Sa et al., 2020). However, the existing TLD systems exhibit some problematic and discriminatory behaviors (Zhou ∗ * Work is not related to employment at Amazon. The source code is available at https://github. com/voidism/invrat_debias. 1 et al., 2021). Experiments show that the tweets containing certain surface markers, such as identity terms and expressions in African American English (AAE), are more likely to be classified as hate speech by the current TLD systems (Davidson et al., 2017; Xia et al., 2020), although some of them are not actually hateful. Such an issue is predominantly attributed to the biases in training datasets for the TLD models; when the models are trained on the biased datasets, these biases are inherited by the models and further exacerbated during the learning process (Zhou et al., 2021). The biases in TLD systems can make the opinions from the members of minority groups more likely to be removed by the online platform, which may significantly hinder their experience as well as exacerbate the discrimination against them in real life. So far, many debiasing methods have b"
C18-3006,P17-1045,1,0.833186,"al., 2016) demonstrated that using neural dialogue models can overcome current obstacles of deploying dialogue systems in larger dialogue domains. Rastogi et al. (2017) also proposed a multi-domain dialogue state tracker to achieve effective and efficient domain adaptation. Dialogue Management – Dialogue Policy Optimization The dialogue policy can be learned in either a supervised or a reinforcement learning manner (Su et al., 2016). The reinforcement learning based dialogue agent has been recently developed in different tasks and shown applicable for interactive scenarios (Li et al., 2017b; Dhingra et al., 2017; Shah et al., 2016). In order to enable reinforcement learning, a simulated environment is required. Several approaches are proposed for building user simulators as the interactive environment (Li et al., 2016; El Asri et al., 2016; Crook and Marin, 2017), so that the dialogue policy can be trained in a reinforcement framework. Natural Language Generation The RNN-based models have been applied to language generation for both chit-chat and task-orientated dialogue systems (Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned data by jointly optimizing sentence p"
C18-3006,W16-3622,0,0.0392894,"Missing"
C18-3006,W13-4073,0,0.0289462,"a good mechanism for integrating longer term knowledge context and shorter term dialogue context into these models (Chen et al., 2016b; Chen et al., 2016c). In addition, the importance of the LU module is investigated in Li et al. (2017a), where different types of errors from LU may degrade the whole system performance in an reinforcement learning setting. Dialogue Management – Dialogue State Tracking The state-of-the-art dialogue managers focus on monitoring the dialogue progress by neural dialogue state trackers. Among the initial models are the RNN based dialogue state tracking approaches (Henderson et al., 2013) that has shown to outperform Bayesian networks (Thomson and Young, 2010). More recent work that provided conjoint representations between the utterances, slot-value pairs as well as knowledge graph representations (Wen et al., 2016; Mrkˇsi´c et al., 2016) demonstrated that using neural dialogue models can overcome current obstacles of deploying dialogue systems in larger dialogue domains. Rastogi et al. (2017) also proposed a multi-domain dialogue state tracker to achieve effective and efficient domain adaptation. Dialogue Management – Dialogue Policy Optimization The dialogue policy can be l"
C18-3006,C16-1038,0,0.0214715,"asks, which supported flexible question types, allowed user-initiated requests during conversation, and finally achieved better robustness. Human feedback is also effectively leveraged into the learning framework for on-line training in an end-to-end manner (Liu et al., 2018). Dialogue Breadth In order to extend the coverage of the systems, transfer learning has been applied to different extended systems in order to proceed to a multi-domain scenario. Chen et al. (2016a) transfered the dialogue acts across different domains so that the performance of the newly-developed domain can be boosted. Kim et al. (2016) proposed to learn a domain-specific and domain-independent information in order to transfer the shared knowledge more efficiently and effectively. In addition, Gaˇsi´c et al. (2015) presented the policy committee in order to boost the performance for policy training in a new domain. All above work extended the dialogue coverage using different directions. Dialogue Depth Most current systems focus on knowledge-based understanding, but there are hierarchical understanding according to the dialogue complexity. For example, an intent about party scheduling may include restaurant reserving and inv"
C18-3006,I17-1074,1,0.889711,"Missing"
C18-3006,N18-1187,1,0.878801,"Missing"
C18-3006,P16-1230,0,0.0236728,", 2010). More recent work that provided conjoint representations between the utterances, slot-value pairs as well as knowledge graph representations (Wen et al., 2016; Mrkˇsi´c et al., 2016) demonstrated that using neural dialogue models can overcome current obstacles of deploying dialogue systems in larger dialogue domains. Rastogi et al. (2017) also proposed a multi-domain dialogue state tracker to achieve effective and efficient domain adaptation. Dialogue Management – Dialogue Policy Optimization The dialogue policy can be learned in either a supervised or a reinforcement learning manner (Su et al., 2016). The reinforcement learning based dialogue agent has been recently developed in different tasks and shown applicable for interactive scenarios (Li et al., 2017b; Dhingra et al., 2017; Shah et al., 2016). In order to enable reinforcement learning, a simulated environment is required. Several approaches are proposed for building user simulators as the interactive environment (Li et al., 2016; El Asri et al., 2016; Crook and Marin, 2017), so that the dialogue policy can be trained in a reinforcement framework. Natural Language Generation The RNN-based models have been applied to language generat"
C18-3006,N18-2010,1,0.822287,"(Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned data by jointly optimizing sentence planning and surface realization, and language variation can be easily achieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism for controlling the dialogue act during generation in order to avoid semantics repetition, showing promising results. Several aspects of improvement have been achieved using contextual and structured information (Duˇsek and Jurcicek, 2016; Nayak et al., 2017; Su et al., 2018) 4 Recent Trends and Challenges on Learning Dialogues This part will focus on discussing the recent trends and current challenges on dialogue system technology. End-to-End Learning for Dialogue Systems With the power of neural networks, there are more and more attempts for learning dialogue systems in an end-to-end fashion. Different learning frameworks are applied, including supervised learning and reinforcement learning. This part will discuss the work about end-to-end learning for dialogues (Dhingra et al., 2016; Wen et al., 2016; Williams and Zweig, 2016; Zhao and Eskenazi, 2016; Li et al."
C18-3006,W15-4639,0,0.0722328,"Missing"
C18-3006,D15-1199,0,0.0234091,"fferent tasks and shown applicable for interactive scenarios (Li et al., 2017b; Dhingra et al., 2017; Shah et al., 2016). In order to enable reinforcement learning, a simulated environment is required. Several approaches are proposed for building user simulators as the interactive environment (Li et al., 2016; El Asri et al., 2016; Crook and Marin, 2017), so that the dialogue policy can be trained in a reinforcement framework. Natural Language Generation The RNN-based models have been applied to language generation for both chit-chat and task-orientated dialogue systems (Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned data by jointly optimizing sentence planning and surface realization, and language variation can be easily achieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism for controlling the dialogue act during generation in order to avoid semantics repetition, showing promising results. Several aspects of improvement have been achieved using contextual and structured information (Duˇsek and Jurcicek, 2016; Nayak et al., 2017; Su et al., 2018) 4 Recent Trends and Ch"
C18-3006,W16-3601,0,0.02409,"yak et al., 2017; Su et al., 2018) 4 Recent Trends and Challenges on Learning Dialogues This part will focus on discussing the recent trends and current challenges on dialogue system technology. End-to-End Learning for Dialogue Systems With the power of neural networks, there are more and more attempts for learning dialogue systems in an end-to-end fashion. Different learning frameworks are applied, including supervised learning and reinforcement learning. This part will discuss the work about end-to-end learning for dialogues (Dhingra et al., 2016; Wen et al., 2016; Williams and Zweig, 2016; Zhao and Eskenazi, 2016; Li et al., 2017b). Recent advance of deep learning has inspired many applications of neural models to dialogue systems. Wen et al. (2016) and Bordes and Weston (2016) introduced a network-based end-to-end trainable taskoriented dialogue system, which treated dialogue system learning as the problem of learning a mapping from dialogue histories to system responses, and applied an encoder-decoder model to train the whole system. However, the system is trained in a supervised fashion, thus requires a lot of training data, and may not be able to explore the unknown space that does not exist in th"
D17-1034,P15-2003,0,0.0388354,"ustering methods and probabilistic modeling methods. Reisinger and Mooney (2010) first proposed multi-sense word representations on the vector space based on clustering techniques. With the power of deep learning, some work exploited neural networks to learn embeddings with sense selection based on clustering (Huang et al., 2012; Neelakantan et al., 2014). Chen et al. (2014) replaced the clustering procedure with a word sense Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch¨utze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al., 2014; ˇ Ettinger et al., 2016; Suster et al., 2016), which may be unavailable in some language, our model can be trained using only an unlabeled corpus. Also, some prior work proposed to learn topical embeddings and word embeddings jointly in order to consider the contexts (Liu et al., 2015a,b), whereas this paper focuses on learning multi-sense 328 1 sample collocation Corpus: { Smartphone companies including apple blackberry, and sony will be invited.} Sense selection for collocated word ?? ′ Sense selection for target word ?? 2 ?(??3"
D17-1034,D14-1110,0,0.213543,"Missing"
D17-1034,P12-1092,0,0.437957,"usetts Institute of Technology, Cambridge, MA † National Taiwan University, Taipei, Taiwan guanghe@mit.edu y.v.chen@ieee.org Abstract the distances between the word and its synonym in each sense would upper-bound the distance between the respective synonyms, which may be mutually irrelevant, in embedding space1 . Due to the theoretical inability to account for polysemy using a single embedding representation per word, multi-sense word representations are proposed to address the ambiguity issue using multiple embedding representations for different senses in a word (Reisinger and Mooney, 2010; Huang et al., 2012). This paper focuses on unsupervised learning from the unannotated corpus. There are two key mechanisms for a multi-sense word representation system in such scenario: 1) a sense selection (decoding) mechanism infers the most probable sense for a word given its context and 2) a sense representation mechanism learns to embed word senses in a continuous space. Under this framework, prior work focused on designing a single model to deliver both mechanisms (Neelakantan et al., 2014; Li and Jurafsky, 2015; Qiu et al., 2016). However, the previously proposed models introduce side-effects: 1) mixing w"
D17-1034,P15-1010,0,0.0744379,"d probabilistic modeling methods. Reisinger and Mooney (2010) first proposed multi-sense word representations on the vector space based on clustering techniques. With the power of deep learning, some work exploited neural networks to learn embeddings with sense selection based on clustering (Huang et al., 2012; Neelakantan et al., 2014). Chen et al. (2014) replaced the clustering procedure with a word sense Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch¨utze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al., 2014; ˇ Ettinger et al., 2016; Suster et al., 2016), which may be unavailable in some language, our model can be trained using only an unlabeled corpus. Also, some prior work proposed to learn topical embeddings and word embeddings jointly in order to consider the contexts (Liu et al., 2015a,b), whereas this paper focuses on learning multi-sense 328 1 sample collocation Corpus: { Smartphone companies including apple blackberry, and sony will be invited.} Sense selection for collocated word ?? ′ Sense selection for target word ?? 2 ?(??3 |?ഥ? ) matrix ?? matrix ?"
D17-1034,N15-1070,0,0.207743,"cess in sense representations system. • We are among the first to propose a single objective for modularized unsupervised sense embedding learning. • We introduce a sense exploration mechanism for the sense selection module to achieve better flexibility and robustness. • Our experimental results show the state-ofthe-art performance for synonym selection and contextual word similarities in terms of MaxSimC. 2 Instead of clustering, probabilistic modeling methods have been applied for learning multisense embeddings in order to make the sense selection more flexible, where Tian et al. (2014) and Jauhar et al. (2015) conducted probabilistic modeling with EM training. Li and Jurafsky (2015) exploited Chinese Restaurant Process to infer the sense identity. Furthermore, Bartunov et al. (2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al., 2013b). Despite reasonable modeling on sense selection, all above methods mixed wordlevel and sense-level tokens during representation learning—unable to conduct representation learning in the pure sense level due to the complicated computation in their EM algorithms. Recently, Qiu et al. (2016) proposed an EM algorithm to learn purely"
D17-1034,D14-1113,0,0.078768,"ss the ambiguity issue using multiple embedding representations for different senses in a word (Reisinger and Mooney, 2010; Huang et al., 2012). This paper focuses on unsupervised learning from the unannotated corpus. There are two key mechanisms for a multi-sense word representation system in such scenario: 1) a sense selection (decoding) mechanism infers the most probable sense for a word given its context and 2) a sense representation mechanism learns to embed word senses in a continuous space. Under this framework, prior work focused on designing a single model to deliver both mechanisms (Neelakantan et al., 2014; Li and Jurafsky, 2015; Qiu et al., 2016). However, the previously proposed models introduce side-effects: 1) mixing word-level and sense-level tokens achieves efficient sense selection but introduces ambiguous word-level tokens during the representation learning process (Neelakantan et al., 2014; Li and Jurafsky, 2015), and 2) pure sense-level tokens prevent ambiguity from word-level tokens but require exponential time complexity when decoding a sense sequence (Qiu et al., 2016). Unlike the prior work, this paper proposes MUSE2 —a novel modularization framework incorporating sense selection"
D17-1034,W15-1504,0,0.0441565,"Missing"
D17-1034,D14-1162,0,0.0802073,"election mechanism, so efficient sense selection by leveraging wordlevel tokens can be achieved only at the cost of mixing word-level and sense-level tokens in their representation learning process. 3.1.2 Sense Representation Module The semantic representation learning is typically formulated as a maximum likelihood estimation (MLE) problem for collocation likelihood. In this paper, we use the skip-gram formulation (Mikolov et al., 2013b) considering that it requires less training time, where only two sense identities are required for stochastic training. Other popular candidates, like GloVe (Pennington et al., 2014) and CBOW (Mikolov et al., 2013a), require more sense identities to be selected as input and thus not suitable for our scenario. For example, GloVe (Pennington et al., 2014) takes computationally expensive collocation counting statistics 3.2 Learning Without the supervised signal for the proposed modules, it is desirable to connect two modules in a way where they can improve each other by their own estimations. First, a trivial way is to forward the prediction of the sense selection module 330 to the representation module. Then we cast the estimated collocation likelihood as a reward signal fo"
D17-1034,D16-1174,0,0.2696,"Missing"
D17-1034,D15-1180,0,0.0322069,"Missing"
D17-1034,D16-1018,0,0.11964,"epresentations for different senses in a word (Reisinger and Mooney, 2010; Huang et al., 2012). This paper focuses on unsupervised learning from the unannotated corpus. There are two key mechanisms for a multi-sense word representation system in such scenario: 1) a sense selection (decoding) mechanism infers the most probable sense for a word given its context and 2) a sense representation mechanism learns to embed word senses in a continuous space. Under this framework, prior work focused on designing a single model to deliver both mechanisms (Neelakantan et al., 2014; Li and Jurafsky, 2015; Qiu et al., 2016). However, the previously proposed models introduce side-effects: 1) mixing word-level and sense-level tokens achieves efficient sense selection but introduces ambiguous word-level tokens during the representation learning process (Neelakantan et al., 2014; Li and Jurafsky, 2015), and 2) pure sense-level tokens prevent ambiguity from word-level tokens but require exponential time complexity when decoding a sense sequence (Qiu et al., 2016). Unlike the prior work, this paper proposes MUSE2 —a novel modularization framework incorporating sense selection and representation learning models, which"
D17-1034,D16-1011,0,0.0413894,"Missing"
D17-1034,N10-1013,0,0.117329,"window. Our modular design addresses such drawback, where the sense selection module decodes a sense sequence with linear-time complexity, while the sense representation module remains representation learning in the pure sense level. Related Work There are three dominant types of approaches for learning multi-sense word representations in the literature: 1) clustering methods, 2) probabilistic modeling methods, and 3) lexical ontology based methods. Our reinforcement learning based approach can be loosely connected to clustering methods and probabilistic modeling methods. Reisinger and Mooney (2010) first proposed multi-sense word representations on the vector space based on clustering techniques. With the power of deep learning, some work exploited neural networks to learn embeddings with sense selection based on clustering (Huang et al., 2012; Neelakantan et al., 2014). Chen et al. (2014) replaced the clustering procedure with a word sense Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch¨utze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al., 2014;"
D17-1034,D15-1200,0,0.172726,"ng multiple embedding representations for different senses in a word (Reisinger and Mooney, 2010; Huang et al., 2012). This paper focuses on unsupervised learning from the unannotated corpus. There are two key mechanisms for a multi-sense word representation system in such scenario: 1) a sense selection (decoding) mechanism infers the most probable sense for a word given its context and 2) a sense representation mechanism learns to embed word senses in a continuous space. Under this framework, prior work focused on designing a single model to deliver both mechanisms (Neelakantan et al., 2014; Li and Jurafsky, 2015; Qiu et al., 2016). However, the previously proposed models introduce side-effects: 1) mixing word-level and sense-level tokens achieves efficient sense selection but introduces ambiguous word-level tokens during the representation learning process (Neelakantan et al., 2014; Li and Jurafsky, 2015), and 2) pure sense-level tokens prevent ambiguity from word-level tokens but require exponential time complexity when decoding a sense sequence (Qiu et al., 2016). Unlike the prior work, this paper proposes MUSE2 —a novel modularization framework incorporating sense selection and representation lear"
D17-1034,P15-1173,0,0.106843,"Missing"
D17-1034,N16-1160,0,0.28131,"es. Neelakantan et al. (2014) pointed out that, due to triangle inequality in vector space, if one word has two different senses but is restricted to one embedding, the sum of 1 d(rock, stone) + d(rock, shake) ≥ d(stone, shake) The trained models and code are available at https: //github.com/MiuLab/MUSE. 2 327 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 327–337 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics disambiguation model using WordNet (Miller, 1995). K˚ageb¨ack et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure. Moreover, Guo et al. (2014) leveraged bilingual resources for clustering. However, most of the above approaches separated the clustering procedure and the representation learning procedure without a joint objective, which may suffer from the error propagation issue. Instead, the proposed approach, MUSE, enables joint training on sense selection and representation learning. MUSE enables linear time sense identity decoding with a sense selection module and purely senselevel representation learning with"
D17-1034,P14-5010,0,0.00884239,"Missing"
D17-1034,C14-1016,0,0.425636,"the sense selection process in sense representations system. • We are among the first to propose a single objective for modularized unsupervised sense embedding learning. • We introduce a sense exploration mechanism for the sense selection module to achieve better flexibility and robustness. • Our experimental results show the state-ofthe-art performance for synonym selection and contextual word similarities in terms of MaxSimC. 2 Instead of clustering, probabilistic modeling methods have been applied for learning multisense embeddings in order to make the sense selection more flexible, where Tian et al. (2014) and Jauhar et al. (2015) conducted probabilistic modeling with EM training. Li and Jurafsky (2015) exploited Chinese Restaurant Process to infer the sense identity. Furthermore, Bartunov et al. (2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al., 2013b). Despite reasonable modeling on sense selection, all above methods mixed wordlevel and sense-level tokens during representation learning—unable to conduct representation learning in the pure sense level due to the complicated computation in their EM algorithms. Recently, Qiu et al. (2016) proposed an EM"
D17-1034,N16-1151,0,0.0468436,"polysemy issues. Neelakantan et al. (2014) pointed out that, due to triangle inequality in vector space, if one word has two different senses but is restricted to one embedding, the sum of 1 d(rock, stone) + d(rock, shake) ≥ d(stone, shake) The trained models and code are available at https: //github.com/MiuLab/MUSE. 2 327 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 327–337 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics disambiguation model using WordNet (Miller, 1995). K˚ageb¨ack et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure. Moreover, Guo et al. (2014) leveraged bilingual resources for clustering. However, most of the above approaches separated the clustering procedure and the representation learning procedure without a joint objective, which may suffer from the error propagation issue. Instead, the proposed approach, MUSE, enables joint training on sense selection and representation learning. MUSE enables linear time sense identity decoding with a sense selection module and purely senselevel representation learning with"
D17-1034,P10-4014,0,0.0394421,"(Q) enormously, (A) appropriately, (B) uniquely, (C) tremendously, (D) decidedly}, and the answer is (C). For multi-sense representations system, it selects the synonym of the question word wQ using the maximum senselevel cosine similarity as a proxy of the semantic similarity (Jauhar et al., 2015). Our model is compared with the following baselines: 1) conventional word embeddings: global context vectors (Huang et al., 2012) and skipgram (Mikolov et al., 2013b); 2) applying supervised word sense disambiguation using the IMS system and then applying skip-gram on disambiguated corpus (IMS+SG) (Zhong and Ng, 2010); 3) unsupervised sense embeddings: EM algorithm (Jauhar et al., 2015), multi-sense skipgram (MSSG) (Neelakantan et al., 2014), Chinese restaurant process (CRP) (Li and Jurafsky, 2015), and the MUSE models; 4) supervised sense embeddings with WordNet (Miller, 1995): retrofitting global context vectors (Retro-GC) and retrofitting skip-gram (Retro-SG) (Jauhar et al., 2015). Among unsupervised sense embedding approaches, CRP and MSSG refer to the baselines with highest MaxSimC and AvgSimC in Table 1 respectively. Here we report the setting for baselines based on the best average performance in th"
D18-1025,P14-2037,0,0.0841412,"Missing"
D18-1025,2005.mtsummit-papers.11,0,0.0122529,"uman judgment. For each Chinese word, we randomly sample one example sentence in Chinese WordNet that matches the PoS tag we selected in section 4. For each English word, we traverse the whole English Wikipedia dump to find the sentences that contain the target English word. We then sample one sentence where the target word is tagged as the matched PoS tag4 . 5 Experiments 5.1 Experimental Setup Two sets of parallel data are used in the experiments, one for English-Chinese (EN-ZH) and another for English-German (EN-DE). UMcorpus (Tian et al.) is used for EN-ZH training, while Europarl corpus (Koehn, 2005) is used for EN-DE training. UM-corpus contains 15,764,200 parallel sentences with 381,921,583 English words and 572,277,658 unsegmented Chinese words. Europarl contains 1,920,209 parallel sentences with 44,548,491 German words and 47,818,827 English words. We evaluate our proposed model on the benchmark monolingual dataset, SCWS, and on the bilingual dataset, our proposed BCWS, where the evaluation metrics are actually introduced in section 5.4. 5.2 Baseline 5.4 Evaluation Metric Reisinger and Mooney (2010) introduced two contextual similarity estimations, AvgSimC and MaxSimC. AvgSimC is a so"
D18-1025,N12-1095,0,0.054856,"Missing"
D18-1025,D17-1034,1,0.647157,"imilarity between word vectors can indicate similar meanings of words. Therefore, embeddings that encode semantics have been shown to serve as the good initialization and benefit several NLP tasks. However, word embeddings do not allow a word to have different meanings in different contexts, which is a phenomenon known as polysemy. For example, “apple” may have different meanings in fruit and technology contexts. Several attempts have been proposed to tackle this problem by inferring multi-sense word representations (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Li and Jurafsky, 2015; Lee and Chen, 2017). 1 The code and dataset are available at http:// github.com/MiuLab/CLUSE. 271 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 271–281 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics bilingual corpora where contexts of bilingual word pairs were jointly predicted. Wei and Deng (2017) proposed a variational autoencoding approach that explicitly models the underlying semantics of the parallel sentence pairs and guided the generation of the sentence pairs. Although the above approaches aimed to learn c"
D18-1025,D15-1200,0,0.0900581,"ed manner. The higher similarity between word vectors can indicate similar meanings of words. Therefore, embeddings that encode semantics have been shown to serve as the good initialization and benefit several NLP tasks. However, word embeddings do not allow a word to have different meanings in different contexts, which is a phenomenon known as polysemy. For example, “apple” may have different meanings in fruit and technology contexts. Several attempts have been proposed to tackle this problem by inferring multi-sense word representations (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Li and Jurafsky, 2015; Lee and Chen, 2017). 1 The code and dataset are available at http:// github.com/MiuLab/CLUSE. 271 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 271–281 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics bilingual corpora where contexts of bilingual word pairs were jointly predicted. Wei and Deng (2017) proposed a variational autoencoding approach that explicitly models the underlying semantics of the parallel sentence pairs and guided the generation of the sentence pairs. Although the above approa"
D18-1025,W15-1521,0,0.404175,"listic model that simultaneously learns alignments and distributed representations for bilingual data by marginalizing over word alignments. Hermann and Blunsom (2014) learned word embeddings by minimizing the distances between compositional representations beˇ tween parallel sentence pairs. Suster et al. (2016) reconstructed the bag-of-words representation of semantic equivalent sentence pairs to learn word embeddings. Shi et al. (2015) proposed a training algorithm in the form of matrix decomposition, and induced cross-lingual constraints for simultaneously factorizing monolingual matrices. Luong et al. (2015) extended the skip-gram model to 3 CLUSE: Cross-Lingual Unsupervised Sense Embeddings Our proposed model borrows the idea about modularization from Lee and Chen (2017), which treats the sense induction and representation mod272 ?? Apple company designs the best cellphone in the world ??′ 蘋果 公司 設計 世界 一流的 手機 Bilingual Sense Induction (EN-ZH) Bilingual Sense Induction (EN-ZH) ? ?? ?(??1 |?? , ??′ ) ?? ? ?(??2 |?? , ??′ ) ? ?ℎ ?(??3 |?? , ??′ ) Monolingual Sense Representation Learning (EN) company_1 ? ?? ?(?? |?? ) ?? ? ?(?? |?? ) Bilingual Sense Representation Learning (EN-ZH) Monolingual Sense"
D18-1025,S17-2002,0,0.139317,"Missing"
D18-1025,C14-1048,0,0.0603933,"nse Embeddings Ta-Chung Chi and Yun-Nung Chen National Taiwan University, Taipei, Taiwan r06922028@ntu.edu.tw y.v.chen@ieee.org Abstract These approaches relied on the “one-sense per collocation” heuristic (Yarowsky, 1993), which assumes that presence of nearby words correlates with the sense of the word of interest. However, this heuristic provides only a weak signal for discriminating sense identities, and it requires a large amount of training data to achieve competitive performance. Considering that different senses of a word may be translated into different words in a foreign lanˇ guage, Guo et al. (2014) and Suster et al. (2016) proposed to learn multi-sense embeddings using this additional signal. For example, “bank” in English can be translated into banc or banque in French, depending on whether the sense is financial or geographical. Such information allows the model to identify which sense a word belongs to. However, the drawback of these models is that the trained foreign language embeddings are not aligned well with the original embeddings in the vector space. This paper addresses these limitations by proposing a bilingual modularized sense induction and representation learning system."
D18-1025,P10-1023,0,0.0945678,"ranging from 1.0 (different) to 10.0 (same) for each question to indicate the semantic similarity of bilingual word pairs based on sentential clues. The annotated dataset shows very high intra-rater consistency; we leave one rater out and calculate Spearman correlation between the rater and the average of the rest, and the average number is about 0.83, indicating the human-level performance (the average number in SCWS is 0.52). We describe the construction of BCWS below. English Candidate Word Extraction We have to find an English counterpart for each Chinese word in lc . We utilize BabelNet (Navigli and Ponzetto, 2010), a free and open-sourced knowledge resource, to serve as our bilingual dictionary. To be more concrete, we first query the selected Chinese word using the free API call provided by Babelnet to retrieve all WordNet senses3 . For example, the Chinese word “制服” has two major meanings: • a type of clothing worn by members of an organization • force to submit or subdue. Hence, we can obtain two candidate English words “uniform” and “subjugate”. Each word in lc retrieves its associated English candidate words and obtain the dictionary D. Enriching Semantic Relationship Note that D is merely a simpl"
D18-1025,P14-1006,0,0.111096,", which may be helpful for disambiguating senses. Cross-Lingual Word Embeddings Klementiev et al. (2012) first pointed out the importance of learning cross-lingual word embeddings in the same space and proposed the cross-lingual document classification (CLDC) dataset for extrinsic evaluation. Gouws et al. (2015) trained directly on monolingual data and extracted a bilingual signal from a smaller set of parallel data. Koˇcisk`y et al. (2014) used a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data by marginalizing over word alignments. Hermann and Blunsom (2014) learned word embeddings by minimizing the distances between compositional representations beˇ tween parallel sentence pairs. Suster et al. (2016) reconstructed the bag-of-words representation of semantic equivalent sentence pairs to learn word embeddings. Shi et al. (2015) proposed a training algorithm in the form of matrix decomposition, and induced cross-lingual constraints for simultaneously factorizing monolingual matrices. Luong et al. (2015) extended the skip-gram model to 3 CLUSE: Cross-Lingual Unsupervised Sense Embeddings Our proposed model borrows the idea about modularization from"
D18-1025,D14-1113,0,0.494704,"s learned in an unsupervised manner. The higher similarity between word vectors can indicate similar meanings of words. Therefore, embeddings that encode semantics have been shown to serve as the good initialization and benefit several NLP tasks. However, word embeddings do not allow a word to have different meanings in different contexts, which is a phenomenon known as polysemy. For example, “apple” may have different meanings in fruit and technology contexts. Several attempts have been proposed to tackle this problem by inferring multi-sense word representations (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Li and Jurafsky, 2015; Lee and Chen, 2017). 1 The code and dataset are available at http:// github.com/MiuLab/CLUSE. 271 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 271–281 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics bilingual corpora where contexts of bilingual word pairs were jointly predicted. Wei and Deng (2017) proposed a variational autoencoding approach that explicitly models the underlying semantics of the parallel sentence pairs and guided the generation of the sentence pairs. Al"
D18-1025,N10-1013,0,0.121274,"entential contexts for measuring the quality of sense embeddings. However, it is a monolingual dataset constructed in English, so it cannot evaluate cross-lingual semantic word similarity. On the other hand, while CamachoCollados et al. (2017) proposed a cross-lingual semantic similarity dataset, it ignored the contextual words but kept only word pairs, making it impossible to judge sense-level similarity. In this paper, we present an English-Chinese contextual word similarity dataset in order to benchmark the experiments about bilingual sense embeddings. Sense Embeddings Reisinger and Mooney (2010) first proposed multi-prototype embeddings to address the lexical ambiguity when using a single embedding to represent multiple meanings of a word. Huang et al. (2012); Neelakantan et al. (2014); Li and Jurafsky (2015); Bartunov et al. (2016) utilized neural networks as well as the Bayesian non-parametric method to learn sense embeddings. Lee and Chen (2017) first utilized a reinforcement learning approach and proposed a modularized framework that separates learning of senses from that of words. However, none of them leverages the bilingual signal, which may be helpful for disambiguating sense"
D18-1025,P12-1092,0,0.20981,"based on the chosen sense. Bansal et al. (2012) proposed an unsupervised method for clustering the translations of a word, such that the translations in each cluster share a common semantic sense. Upadhyay et al. (2017) leveraged cross-lingual signals in more than two languages. However, they either used pretrained embeddings or learned only for the English side, which is undesirable since cross-lingual embeddings shall be jointly learned such that they aligned well in the embedding space. Evaluation Datasets Several datasets can be used to justify the performance of learned sense embeddings. Huang et al. (2012) presented SCWS, the first and only dataset that contains word pairs and their sentential contexts for measuring the quality of sense embeddings. However, it is a monolingual dataset constructed in English, so it cannot evaluate cross-lingual semantic word similarity. On the other hand, while CamachoCollados et al. (2017) proposed a cross-lingual semantic similarity dataset, it ignored the contextual words but kept only word pairs, making it impossible to judge sense-level similarity. In this paper, we present an English-Chinese contextual word similarity dataset in order to benchmark the expe"
D18-1025,P15-2093,0,0.0723154,"valuation. Gouws et al. (2015) trained directly on monolingual data and extracted a bilingual signal from a smaller set of parallel data. Koˇcisk`y et al. (2014) used a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data by marginalizing over word alignments. Hermann and Blunsom (2014) learned word embeddings by minimizing the distances between compositional representations beˇ tween parallel sentence pairs. Suster et al. (2016) reconstructed the bag-of-words representation of semantic equivalent sentence pairs to learn word embeddings. Shi et al. (2015) proposed a training algorithm in the form of matrix decomposition, and induced cross-lingual constraints for simultaneously factorizing monolingual matrices. Luong et al. (2015) extended the skip-gram model to 3 CLUSE: Cross-Lingual Unsupervised Sense Embeddings Our proposed model borrows the idea about modularization from Lee and Chen (2017), which treats the sense induction and representation mod272 ?? Apple company designs the best cellphone in the world ??′ 蘋果 公司 設計 世界 一流的 手機 Bilingual Sense Induction (EN-ZH) Bilingual Sense Induction (EN-ZH) ? ?? ?(??1 |?? , ??′ ) ?? ? ?(??2 |?? , ??′ )"
D18-1025,N16-1160,0,0.0355605,"Missing"
D18-1025,W17-2613,0,0.127587,"valuation. Cross-Lingual Sense Embeddings Guo et al. (2014) adopted the heuristics where different meanings of a polysemous word usually can be represented by different words in another language and clustered bilingual word embeddings to inˇ duce senses. Suster et al. (2016) proposed an encoder, which uses parallel corpora to choose a sense for a given word, and a decoder that predicts context words based on the chosen sense. Bansal et al. (2012) proposed an unsupervised method for clustering the translations of a word, such that the translations in each cluster share a common semantic sense. Upadhyay et al. (2017) leveraged cross-lingual signals in more than two languages. However, they either used pretrained embeddings or learned only for the English side, which is undesirable since cross-lingual embeddings shall be jointly learned such that they aligned well in the embedding space. Evaluation Datasets Several datasets can be used to justify the performance of learned sense embeddings. Huang et al. (2012) presented SCWS, the first and only dataset that contains word pairs and their sentential contexts for measuring the quality of sense embeddings. However, it is a monolingual dataset constructed in En"
D18-1025,H93-1052,0,0.808388,"Missing"
D18-1025,C12-1089,0,\N,Missing
D19-1098,P17-2021,0,0.0272034,"ng, and further learning more explainable attention scores1 . 1 Introduction Human languages exhibit a rich hierarchical structure which is currently not exploited nor mirrored by the self-attention mechanism that is the core of the now popular Transformer architecture. Prior work that integrated hierarchical structure into neural networks either used recursive neural networks (Tree-RNNs) (C.Goller and A.Kuchler, 1996; Socher et al., 2011; Tai et al., 2015) or simultaneously generated a syntax tree and language in RNN (Dyer et al., 2016), which have shown beneficial for many downstream tasks (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Strubell et al., 2018; Zaremoodi and Haffari, 2018). Considering the requirement of the annotated parse trees and the 1 The source code is publicly available at https:// github.com/yaushian/Tree-Transformer. costly annotation effort, most prior work relied on the supervised syntactic parser. However, a supervised parser may be unavailable when the language is low-resourced or the target data has different distribution from the source domain. Therefore, the task of learning latent tree structures without human-annotated data, called grammar induction (Carroll and Charni"
D19-1098,P06-1109,0,0.0684132,"hreshold in Algorithm 1, the leaf nodes are not strictly binary. increasing the layer number results in better performance, because it allows the Tree Transformer to model deeper trees. However, the performance stops growing when the depth is above 10. The words in the layers above the certain layer are all grouped into the same constituent, and therefore increasing the layer number will no longer help model discover useful tree structures. In Table 2, we report the results on WSJ-10. Some of the baselines including CCM (Klein and Manning, 2002), DMV+CCM (Klein and Manning, 2005) and UML-DOP (Bod, 2006) are not directly comparable to our model, because they are trained using POS tags our model does not consider. In addition, we further investigate what kinds of trees are induced by our model. Following URNNG, we evaluate the performance of constituents by its label in Table 3. The trees induced by different methods are quite different. Our model is inclined to discover noun phrases (NP) and adverb phrases (ADVP), but not easy to discover verb phrases (VP) or adjective phrases (ADJP). We show an induced parse tree in Figure 3 and more induced parse trees can be found in Appendix. 7.3 Analysis"
D19-1098,N16-1024,0,0.3162,"ee Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores1 . 1 Introduction Human languages exhibit a rich hierarchical structure which is currently not exploited nor mirrored by the self-attention mechanism that is the core of the now popular Transformer architecture. Prior work that integrated hierarchical structure into neural networks either used recursive neural networks (Tree-RNNs) (C.Goller and A.Kuchler, 1996; Socher et al., 2011; Tai et al., 2015) or simultaneously generated a syntax tree and language in RNN (Dyer et al., 2016), which have shown beneficial for many downstream tasks (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Strubell et al., 2018; Zaremoodi and Haffari, 2018). Considering the requirement of the annotated parse trees and the 1 The source code is publicly available at https:// github.com/yaushian/Tree-Transformer. costly annotation effort, most prior work relied on the supervised syntactic parser. However, a supervised parser may be unavailable when the language is low-resourced or the target data has different distribution from the source domain. Therefore, the task of learning latent tree st"
D19-1098,P17-2012,0,0.0445465,"e explainable attention scores1 . 1 Introduction Human languages exhibit a rich hierarchical structure which is currently not exploited nor mirrored by the self-attention mechanism that is the core of the now popular Transformer architecture. Prior work that integrated hierarchical structure into neural networks either used recursive neural networks (Tree-RNNs) (C.Goller and A.Kuchler, 1996; Socher et al., 2011; Tai et al., 2015) or simultaneously generated a syntax tree and language in RNN (Dyer et al., 2016), which have shown beneficial for many downstream tasks (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Strubell et al., 2018; Zaremoodi and Haffari, 2018). Considering the requirement of the annotated parse trees and the 1 The source code is publicly available at https:// github.com/yaushian/Tree-Transformer. costly annotation effort, most prior work relied on the supervised syntactic parser. However, a supervised parser may be unavailable when the language is low-resourced or the target data has different distribution from the source domain. Therefore, the task of learning latent tree structures without human-annotated data, called grammar induction (Carroll and Charniak, 1992; Klein and Man"
D19-1098,W18-5452,0,0.029189,"number of layers, denoted as L, in the following experiments. 7.2 Grammar Induction In this section, we evaluate the performance of our model on unsupervised constituency parsing. Our model is trained on WSJ training set and WSJ-all (i.e. including testing and validation sets) by using BERT Masked LM (Devlin et al., 2018) as unsupervised training task. We use WordPiece (Wu et al., 2016) tokenizer from BERT to tokenize words with a 16k token vocabulary. Our best result is optimized by adam with a learning rate of 0.0001, β1 = 0.9 and β2 = 0.98. Following the evaluation settings of prior work (Htut et al., 2018; Shen et al., 2018b)2 , we evaluate F1 scores of our model on WSJ-test and WSJ-10 of Penn Treebank (PTB) (Marcus et al., 1993). The WSJ-10 has 7422 sentences from whole PTB with sentence length restricted to 10 after punctuation removal, while WSJ-test has 2416 sentences from the PTB testing set with unrestricted sentence length. The results on WSJ-test are in Table 1. We mainly compare our model to PRPN (Shen et al., 2018a), On-lstm (Shen et al., 2018b) and Compound PCFG(C-PCFG) (Kim et al., 2019a), in which the evaluation settings and the training data are identical to our model. DIORA (Dro"
D19-1098,P19-1228,0,0.193978,"Missing"
D19-1098,N19-1114,0,0.254114,"d the parsing operations are regarded as its actions. The actor manages to maximize total rewards, which are the performance of downstream tasks. PRPN (Shen et al., 2018a) and On-LSTM (Shen et al., 2018b) induce tree structures by introducing a bias to recurrent neural networks. PRPN proposes a parsing network to compute the syntactic distance of all word pairs, and a reading network utilizes the syntactic structure to attend relevant memories. On-LSTM allows hidden neurons to learn long-term or short-term information by the proposed new gating mechanism and new activation function. In URNNG (Kim et al., 2019b), they applied amortized variational inference between a recurrent neural network grammar (RNNG) (Dyer et al., 2016) decoder and a tree structures inference network, which encourages the decoder to generate reasonable tree structures. DIORA (Drozdov et al., 2019) proposed using inside-outside dynamic programming to compose latent representations from all possible binary trees. The representations of inside and outside passes from same sentences are optimized to be close to each other. Compound PCFG (Kim et al., 2019a) achieves grammar induction by maximizing the marginal likelihood of the se"
D19-1098,P02-1017,0,0.697009,"et al., 2017; Strubell et al., 2018; Zaremoodi and Haffari, 2018). Considering the requirement of the annotated parse trees and the 1 The source code is publicly available at https:// github.com/yaushian/Tree-Transformer. costly annotation effort, most prior work relied on the supervised syntactic parser. However, a supervised parser may be unavailable when the language is low-resourced or the target data has different distribution from the source domain. Therefore, the task of learning latent tree structures without human-annotated data, called grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2002; Smith and Eisner, 2005), has become an important problem and attractd more attention from researchers recently. Prior work mainly focused on inducing tree structures from recurrent neural networks (Shen et al., 2018a,b) or recursive neural networks (Yogatama et al., 2017; Drozdov et al., 2019), while integrating tree structures into Transformer remains an unexplored direction. Pre-training Transformer from large-scale raw texts successfully learns high-quality language representations. By further fine-tuning pre-trained Transformer on desired tasks, wide range of NLP tasks obtain the state-o"
D19-1098,N19-1112,0,0.134269,"entioned in Section 5.2, the values of a are strictly increasing, which indicates that a directly learns the hierarchical structures from layer to layer. Algorithm 1 details how we utilize hierarchical information of a for unsupervised parsing. The unsupervised parsing starts from the top layer, and recursively moves down to the last layer after finding a breakpoint until reaching the bottom layer m. The bottom layer m is a hyperparameter needed to be tuned, and is usually set to 2 or 3. We discard a from layers below m, because we find the lowest few layers do not learn good representations (Liu et al., 2019) and thus the parsing results are poor (Shen et al., 2018b). All values of a on top few layers are very close to 1, suggesting that those are not good breakpoints. Therefore, we set a threshold for deciding a breakpoint, where a minimum a will be viewed as a valid breakpoint only if its value is below the threshold. As we find that our model is not very sensitive to the threshold value, we set it to be 0.8 for all experiments. 7 Experiments In order to evaluate the performance of our proposed model, we conduct the experiments detailed below. 7.1 Model Architecture Our model is built upon a bid"
D19-1098,J93-2004,0,0.0645104,"mance of our model on unsupervised constituency parsing. Our model is trained on WSJ training set and WSJ-all (i.e. including testing and validation sets) by using BERT Masked LM (Devlin et al., 2018) as unsupervised training task. We use WordPiece (Wu et al., 2016) tokenizer from BERT to tokenize words with a 16k token vocabulary. Our best result is optimized by adam with a learning rate of 0.0001, β1 = 0.9 and β2 = 0.98. Following the evaluation settings of prior work (Htut et al., 2018; Shen et al., 2018b)2 , we evaluate F1 scores of our model on WSJ-test and WSJ-10 of Penn Treebank (PTB) (Marcus et al., 1993). The WSJ-10 has 7422 sentences from whole PTB with sentence length restricted to 10 after punctuation removal, while WSJ-test has 2416 sentences from the PTB testing set with unrestricted sentence length. The results on WSJ-test are in Table 1. We mainly compare our model to PRPN (Shen et al., 2018a), On-lstm (Shen et al., 2018b) and Compound PCFG(C-PCFG) (Kim et al., 2019a), in which the evaluation settings and the training data are identical to our model. DIORA (Drozdov et al., 2019) and URNNG (Kim et al., 2019b) use a relative larger training data and the evaluation settings are slightly d"
D19-1098,D18-1548,0,0.0617573,"scores1 . 1 Introduction Human languages exhibit a rich hierarchical structure which is currently not exploited nor mirrored by the self-attention mechanism that is the core of the now popular Transformer architecture. Prior work that integrated hierarchical structure into neural networks either used recursive neural networks (Tree-RNNs) (C.Goller and A.Kuchler, 1996; Socher et al., 2011; Tai et al., 2015) or simultaneously generated a syntax tree and language in RNN (Dyer et al., 2016), which have shown beneficial for many downstream tasks (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Strubell et al., 2018; Zaremoodi and Haffari, 2018). Considering the requirement of the annotated parse trees and the 1 The source code is publicly available at https:// github.com/yaushian/Tree-Transformer. costly annotation effort, most prior work relied on the supervised syntactic parser. However, a supervised parser may be unavailable when the language is low-resourced or the target data has different distribution from the source domain. Therefore, the task of learning latent tree structures without human-annotated data, called grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2002; Smith and E"
D19-1098,P15-1150,0,0.338219,"Missing"
D19-1098,N18-1101,0,0.0356773,"Missing"
D19-1098,D18-1408,0,0.0458843,"o each other, only some distinct patterns such as attending previous words or named entities can be found informative (Vig, 2019). The attention matrices do not match our intuitions about hierarchical structures. In order to make the attention learned by Transformer more interpretable and allow Transformer to comprehend language hierarchically, we propose Tree Transformer, which integrates tree structures into bidirectional Transformer encoder. At each layer, words are constrained to attend to other words in the same constituents. This constraint has been proven to be effective in prior work (Wu et al., 2018). Different from the prior 1061 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1061–1070, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics work that required a supervised parser, in Tree Transformer, the constituency tree structures is automatically induced from raw texts by our proposed “Constituent Attention” module, which is simply implemented by self-attention. Motivated by Tree-RNNs, which compose each phrase and the sentence repres"
D19-1098,C18-1120,0,0.0236269,"on Human languages exhibit a rich hierarchical structure which is currently not exploited nor mirrored by the self-attention mechanism that is the core of the now popular Transformer architecture. Prior work that integrated hierarchical structure into neural networks either used recursive neural networks (Tree-RNNs) (C.Goller and A.Kuchler, 1996; Socher et al., 2011; Tai et al., 2015) or simultaneously generated a syntax tree and language in RNN (Dyer et al., 2016), which have shown beneficial for many downstream tasks (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Strubell et al., 2018; Zaremoodi and Haffari, 2018). Considering the requirement of the annotated parse trees and the 1 The source code is publicly available at https:// github.com/yaushian/Tree-Transformer. costly annotation effort, most prior work relied on the supervised syntactic parser. However, a supervised parser may be unavailable when the language is low-resourced or the target data has different distribution from the source domain. Therefore, the task of learning latent tree structures without human-annotated data, called grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2002; Smith and Eisner, 2005), has become an im"
D19-1194,N18-1165,0,0.031452,"46.02 53.98 47.31 100.00 60.54 69.46 50.78 22.50 57.84 62.83 42.98 25.74 71.11 77.35 30.76 28.42 15.14 18.74 41.33 25.31 69.30 77.30 42.81 25.01 74.63 77.09 47.05 25.91 74.52 78.56 Table 5: The results of knowledge graph entities prediction. 5.2 Multi-Hop Reasoning We leverage multi-hop reasoning (Lao et al., 2011) to allow our model to quickly adapt to dynamic knowledge graphs. Recently, prior work used convolutional neural network (Toutanova et al., 2015), recurrent neural network (Neelakantan et al., 2015; Das et al., 2017), and reinforcement learning (Xiong et al., 2017; Das et al., 2018; Chen et al., 2018; Shen et al., 2018) to model multi-hop reasoning on knowledge graphs, and has proved this concept useful in link prediction. These reasoning models, however, have not yet explored on dialogue generation. The proposed model is the first attempt at adapting conversations via a reasoning procedure. 6 Experiments For all models, we use gated recurrent unit (GRU) based Seq2Seq models (Cho et al., 2014; Chung et al., 2014; Vinyals and Le, 2015). Both encoder and decoder for HGZHZ are 256 dimension with 1 layer; ones for Friends are 128 dimension with 1 layer. We benchmark the task, dynamic knowledg"
D19-1194,W14-4012,0,0.0195466,"Missing"
D19-1194,E17-1013,0,0.0362428,"ric Generated-KW Recall Precision Recall Precision 23.22 5.57 6.88 2.02 37.18 100.00 46.02 53.98 47.31 100.00 60.54 69.46 50.78 22.50 57.84 62.83 42.98 25.74 71.11 77.35 30.76 28.42 15.14 18.74 41.33 25.31 69.30 77.30 42.81 25.01 74.63 77.09 47.05 25.91 74.52 78.56 Table 5: The results of knowledge graph entities prediction. 5.2 Multi-Hop Reasoning We leverage multi-hop reasoning (Lao et al., 2011) to allow our model to quickly adapt to dynamic knowledge graphs. Recently, prior work used convolutional neural network (Toutanova et al., 2015), recurrent neural network (Neelakantan et al., 2015; Das et al., 2017), and reinforcement learning (Xiong et al., 2017; Das et al., 2018; Chen et al., 2018; Shen et al., 2018) to model multi-hop reasoning on knowledge graphs, and has proved this concept useful in link prediction. These reasoning models, however, have not yet explored on dialogue generation. The proposed model is the first attempt at adapting conversations via a reasoning procedure. 6 Experiments For all models, we use gated recurrent unit (GRU) based Seq2Seq models (Cho et al., 2014; Chung et al., 2014; Vinyals and Le, 2015). Both encoder and decoder for HGZHZ are 256 dimension with 1 layer; one"
D19-1194,P16-1154,0,0.61548,"l., 2018) retrieve only one-hop relation paths. As fictitious life in drama, realistic responses often use knowledge entities existing in multi-hop relational paths, e.g., the residence of a friend of mine. Therefore, we propose a model that incorporates multi-hop reasoning (Lao et al., 2011; Neelakantan et al., 2015; Xiong et al., 2017) on the graph structure into a neural conversation generation model. Our proposed model, called quick adaptive dynamic knowledge-grounded neural conversation model (Qadpt), is based on a Seq2Seq model (Sutskever et al., 2014) with a widely-used copy mechanism (Gu et al., 2016; Merity et al., 2017; He et al., 2017; Xing et al., 2017; Zhu et al., 2017; Eric et al., 2017; Ke et al., 2018). To enable multi-hop reasoning, the model factorizes a transition matrix for a Markov chain. In order to focus on the capability of producing reasonable knowledge entities and adapting with dynamic knowledge graphs, we propose two types of automatic metrics. First, given the provided knowledge graphs, we examine if the models can generate responses with proper usage of multi-hop reasoning over knowledge graphs. Second, after randomly replacing some crucial entities in knowledge grap"
D19-1194,P17-1021,0,0.0151219,"have emerged for its capability to be fully data-driven and endto-end trained. While the generated responses are often reasonable but general (without useful information), recent work proposed knowledgegrounded models (Eric et al., 2017; Ghazvininejad et al., 2018; Zhou et al., 2018b; Qian et al., 2018) to incorporate external facts in an end-toend fashion without hand-crafted slot filling. Effectively combining text and external knowledge 1 The data and code are available in https://github. com/Pascalson/DyKGChat. graphs have also been a crucial topic in question answering (Yin et al., 2016; Hao et al., 2017; Levy et al., 2017; Sun et al., 2018; Das et al., 2019). Nonetheless, prior work rarely analyzed the model capability of zero-shot adaptation to dynamic knowledge graphs, where the states/entities and their relations are temporal and evolve as a single time scale process. For example, as shown in Figure 1, the entity Jin-Xi was originally related to the entity Feng, Ruozhao with the type EnemyOf, but then evolved to be related to the entity Nian, Shilan. The goal of this paper is to facilitate knowledgegrounded neural conversation models to learn and zero-shot adapt with dynamic knowledge gra"
D19-1194,P17-1019,0,0.22778,"on paths. As fictitious life in drama, realistic responses often use knowledge entities existing in multi-hop relational paths, e.g., the residence of a friend of mine. Therefore, we propose a model that incorporates multi-hop reasoning (Lao et al., 2011; Neelakantan et al., 2015; Xiong et al., 2017) on the graph structure into a neural conversation generation model. Our proposed model, called quick adaptive dynamic knowledge-grounded neural conversation model (Qadpt), is based on a Seq2Seq model (Sutskever et al., 2014) with a widely-used copy mechanism (Gu et al., 2016; Merity et al., 2017; He et al., 2017; Xing et al., 2017; Zhu et al., 2017; Eric et al., 2017; Ke et al., 2018). To enable multi-hop reasoning, the model factorizes a transition matrix for a Markov chain. In order to focus on the capability of producing reasonable knowledge entities and adapting with dynamic knowledge graphs, we propose two types of automatic metrics. First, given the provided knowledge graphs, we examine if the models can generate responses with proper usage of multi-hop reasoning over knowledge graphs. Second, after randomly replacing some crucial entities in knowledge graphs, we test if the models can accordin"
D19-1194,P18-1139,0,0.104284,"ledge entities existing in multi-hop relational paths, e.g., the residence of a friend of mine. Therefore, we propose a model that incorporates multi-hop reasoning (Lao et al., 2011; Neelakantan et al., 2015; Xiong et al., 2017) on the graph structure into a neural conversation generation model. Our proposed model, called quick adaptive dynamic knowledge-grounded neural conversation model (Qadpt), is based on a Seq2Seq model (Sutskever et al., 2014) with a widely-used copy mechanism (Gu et al., 2016; Merity et al., 2017; He et al., 2017; Xing et al., 2017; Zhu et al., 2017; Eric et al., 2017; Ke et al., 2018). To enable multi-hop reasoning, the model factorizes a transition matrix for a Markov chain. In order to focus on the capability of producing reasonable knowledge entities and adapting with dynamic knowledge graphs, we propose two types of automatic metrics. First, given the provided knowledge graphs, we examine if the models can generate responses with proper usage of multi-hop reasoning over knowledge graphs. Second, after randomly replacing some crucial entities in knowledge graphs, we test if the models can accordingly generate correspondent responses. The empirical results show that our"
D19-1194,D11-1049,0,0.699833,"d conversation models (Sutskever et al., 2014; Ghazvininejad et al., 2018; Zhu et al., 2017) did not directly use the graph structure, so it is unknown how a changed graph will influence the generated responses. In addition, key-value retrieved-based models (Yin et al., 2016; Eric et al., 2017; Levy et al., 2017; Qian et al., 2018) retrieve only one-hop relation paths. As fictitious life in drama, realistic responses often use knowledge entities existing in multi-hop relational paths, e.g., the residence of a friend of mine. Therefore, we propose a model that incorporates multi-hop reasoning (Lao et al., 2011; Neelakantan et al., 2015; Xiong et al., 2017) on the graph structure into a neural conversation generation model. Our proposed model, called quick adaptive dynamic knowledge-grounded neural conversation model (Qadpt), is based on a Seq2Seq model (Sutskever et al., 2014) with a widely-used copy mechanism (Gu et al., 2016; Merity et al., 2017; He et al., 2017; Xing et al., 2017; Zhu et al., 2017; Eric et al., 2017; Ke et al., 2018). To enable multi-hop reasoning, the model factorizes a transition matrix for a Markov chain. In order to focus on the capability of producing reasonable knowledge e"
D19-1194,K17-1034,0,0.31025,"ts capability to be fully data-driven and endto-end trained. While the generated responses are often reasonable but general (without useful information), recent work proposed knowledgegrounded models (Eric et al., 2017; Ghazvininejad et al., 2018; Zhou et al., 2018b; Qian et al., 2018) to incorporate external facts in an end-toend fashion without hand-crafted slot filling. Effectively combining text and external knowledge 1 The data and code are available in https://github. com/Pascalson/DyKGChat. graphs have also been a crucial topic in question answering (Yin et al., 2016; Hao et al., 2017; Levy et al., 2017; Sun et al., 2018; Das et al., 2019). Nonetheless, prior work rarely analyzed the model capability of zero-shot adaptation to dynamic knowledge graphs, where the states/entities and their relations are temporal and evolve as a single time scale process. For example, as shown in Figure 1, the entity Jin-Xi was originally related to the entity Feng, Ruozhao with the type EnemyOf, but then evolved to be related to the entity Nian, Shilan. The goal of this paper is to facilitate knowledgegrounded neural conversation models to learn and zero-shot adapt with dynamic knowledge graphs. To our observa"
D19-1194,N16-1014,0,0.0776717,"15). Both encoder and decoder for HGZHZ are 256 dimension with 1 layer; ones for Friends are 128 dimension with 1 layer. We benchmark the task, dynamic knowledgegrounded dialogue generation, and corpus DyKgChat by providing a detailed comparison between the prior conversational models and our proposed model as the preliminary experiments. We evaluate their capability of quick adaptation by randomized whole, last 1, last 2 reasoning paths as described in Section 2.1.2. We evaluate the produced responses by sentence-level BLEU-2 (Papineni et al., 2002; Liu et al., 2016), perplexity, distinct-n (Li et al., 2016), and our proposed metrics for predicting knowledge entities descrin section 2.1.1. Because of the significant data imbalance of Friends, we first train on whole training data, and then fine-tune the models using the subset containing knowledge entities. Early stopping is adopted in all experiments. 6.1 Baselines We compare our model with prior knowledgegrounded conversation models: the memory network (Ghazvininejad et al., 2018) and knowledgeaware model (KAware) (Zhu et al., 2017; Zhou et al., 2018b). We also leverage the topic-aware model (TAware) (Xing et al., 2017; Wu et al., 2018; Zhou et"
D19-1194,W17-5506,0,0.240376,"zhao Zhen, Huan: Thanks for Consort Jing&apos;s concern. Enemy of Jin-Xi is Nian, Shilan (an unseen relation) Zhen, Huan: Thanks for Consort Hua&apos;s concern. Figure 1: An example of an ideal conversation model with dynamic knowledge graphs. Introduction In the chit-chat dialogue generation, neural conversation models (Sutskever et al., 2014; Sordoni et al., 2015; Vinyals and Le, 2015) have emerged for its capability to be fully data-driven and endto-end trained. While the generated responses are often reasonable but general (without useful information), recent work proposed knowledgegrounded models (Eric et al., 2017; Ghazvininejad et al., 2018; Zhou et al., 2018b; Qian et al., 2018) to incorporate external facts in an end-toend fashion without hand-crafted slot filling. Effectively combining text and external knowledge 1 The data and code are available in https://github. com/Pascalson/DyKGChat. graphs have also been a crucial topic in question answering (Yin et al., 2016; Hao et al., 2017; Levy et al., 2017; Sun et al., 2018; Das et al., 2019). Nonetheless, prior work rarely analyzed the model capability of zero-shot adaptation to dynamic knowledge graphs, where the states/entities and their relations ar"
D19-1194,D16-1230,0,0.0846841,"Missing"
D19-1194,P19-1081,0,0.092952,"mative responses. For knowledge from unstructured texts, Ghazvininejad et al. (2018) used bag-of-word representations and Long et al. (2017) applied a convolutional neural network to encode the whole texts. With structured knowledge graphs, Zhu et al. (2017) and Zhou et al. (2018b) utilized graph embedding methods (e.g., TransE (Bordes et al., 2013)) to encode each triplet. The above methods generated responses without explicit relationship to each external knowledge triplet. Therefore, when a triplet is added or deleted, it is unknown whether their generated responses can change accordingly. Moon et al. (2019) recently presented a similar concept, walking on the knowledge graph, for response generation. Nonetheless, their purpose is to find explainable path on a large-scaled knowledge graph instead of adaptation with the changed knowledge graphs. Hence, the proposed attention-based graph walker may suffer from the same issue as previous embedding-based methods. 1860 Model MemNet + multi TAware + multi KAware Qadpt + multi + TAware HGZHZ Change Rate Accurate Change Rate All Last1 Last2 All Last1 Last2 92.98 31.78 37.46 62.19 1.17 2.92 98.69 77.87 81.96 83.82 3.40 10.74 94.38 68.33 71.86 78.88 1.95 9"
D19-1194,P15-1016,0,0.173599,"els (Sutskever et al., 2014; Ghazvininejad et al., 2018; Zhu et al., 2017) did not directly use the graph structure, so it is unknown how a changed graph will influence the generated responses. In addition, key-value retrieved-based models (Yin et al., 2016; Eric et al., 2017; Levy et al., 2017; Qian et al., 2018) retrieve only one-hop relation paths. As fictitious life in drama, realistic responses often use knowledge entities existing in multi-hop relational paths, e.g., the residence of a friend of mine. Therefore, we propose a model that incorporates multi-hop reasoning (Lao et al., 2011; Neelakantan et al., 2015; Xiong et al., 2017) on the graph structure into a neural conversation generation model. Our proposed model, called quick adaptive dynamic knowledge-grounded neural conversation model (Qadpt), is based on a Seq2Seq model (Sutskever et al., 2014) with a widely-used copy mechanism (Gu et al., 2016; Merity et al., 2017; He et al., 2017; Xing et al., 2017; Zhu et al., 2017; Eric et al., 2017; Ke et al., 2018). To enable multi-hop reasoning, the model factorizes a transition matrix for a Markov chain. In order to focus on the capability of producing reasonable knowledge entities and adapting with"
D19-1194,P02-1040,0,0.104102,"q models (Cho et al., 2014; Chung et al., 2014; Vinyals and Le, 2015). Both encoder and decoder for HGZHZ are 256 dimension with 1 layer; ones for Friends are 128 dimension with 1 layer. We benchmark the task, dynamic knowledgegrounded dialogue generation, and corpus DyKgChat by providing a detailed comparison between the prior conversational models and our proposed model as the preliminary experiments. We evaluate their capability of quick adaptation by randomized whole, last 1, last 2 reasoning paths as described in Section 2.1.2. We evaluate the produced responses by sentence-level BLEU-2 (Papineni et al., 2002; Liu et al., 2016), perplexity, distinct-n (Li et al., 2016), and our proposed metrics for predicting knowledge entities descrin section 2.1.1. Because of the significant data imbalance of Friends, we first train on whole training data, and then fine-tune the models using the subset containing knowledge entities. Early stopping is adopted in all experiments. 6.1 Baselines We compare our model with prior knowledgegrounded conversation models: the memory network (Ghazvininejad et al., 2018) and knowledgeaware model (KAware) (Zhu et al., 2017; Zhou et al., 2018b). We also leverage the topic-awar"
D19-1194,N15-1020,0,0.127593,"Missing"
D19-1194,D18-1455,0,0.0154338,"fully data-driven and endto-end trained. While the generated responses are often reasonable but general (without useful information), recent work proposed knowledgegrounded models (Eric et al., 2017; Ghazvininejad et al., 2018; Zhou et al., 2018b; Qian et al., 2018) to incorporate external facts in an end-toend fashion without hand-crafted slot filling. Effectively combining text and external knowledge 1 The data and code are available in https://github. com/Pascalson/DyKGChat. graphs have also been a crucial topic in question answering (Yin et al., 2016; Hao et al., 2017; Levy et al., 2017; Sun et al., 2018; Das et al., 2019). Nonetheless, prior work rarely analyzed the model capability of zero-shot adaptation to dynamic knowledge graphs, where the states/entities and their relations are temporal and evolve as a single time scale process. For example, as shown in Figure 1, the entity Jin-Xi was originally related to the entity Feng, Ruozhao with the type EnemyOf, but then evolved to be related to the entity Nian, Shilan. The goal of this paper is to facilitate knowledgegrounded neural conversation models to learn and zero-shot adapt with dynamic knowledge graphs. To our observation, however, the"
D19-1194,D15-1174,0,0.0122709,"9 KW Acc 3.81 22.79 34.92 62.74 72.96 13.52 74.00 74.44 73.57 Friends KW/Generic Generated-KW Recall Precision Recall Precision 23.22 5.57 6.88 2.02 37.18 100.00 46.02 53.98 47.31 100.00 60.54 69.46 50.78 22.50 57.84 62.83 42.98 25.74 71.11 77.35 30.76 28.42 15.14 18.74 41.33 25.31 69.30 77.30 42.81 25.01 74.63 77.09 47.05 25.91 74.52 78.56 Table 5: The results of knowledge graph entities prediction. 5.2 Multi-Hop Reasoning We leverage multi-hop reasoning (Lao et al., 2011) to allow our model to quickly adapt to dynamic knowledge graphs. Recently, prior work used convolutional neural network (Toutanova et al., 2015), recurrent neural network (Neelakantan et al., 2015; Das et al., 2017), and reinforcement learning (Xiong et al., 2017; Das et al., 2018; Chen et al., 2018; Shen et al., 2018) to model multi-hop reasoning on knowledge graphs, and has proved this concept useful in link prediction. These reasoning models, however, have not yet explored on dialogue generation. The proposed model is the first attempt at adapting conversations via a reasoning procedure. 6 Experiments For all models, we use gated recurrent unit (GRU) based Seq2Seq models (Cho et al., 2014; Chung et al., 2014; Vinyals and Le, 2015)."
D19-1194,D17-1060,0,0.136168,"4; Ghazvininejad et al., 2018; Zhu et al., 2017) did not directly use the graph structure, so it is unknown how a changed graph will influence the generated responses. In addition, key-value retrieved-based models (Yin et al., 2016; Eric et al., 2017; Levy et al., 2017; Qian et al., 2018) retrieve only one-hop relation paths. As fictitious life in drama, realistic responses often use knowledge entities existing in multi-hop relational paths, e.g., the residence of a friend of mine. Therefore, we propose a model that incorporates multi-hop reasoning (Lao et al., 2011; Neelakantan et al., 2015; Xiong et al., 2017) on the graph structure into a neural conversation generation model. Our proposed model, called quick adaptive dynamic knowledge-grounded neural conversation model (Qadpt), is based on a Seq2Seq model (Sutskever et al., 2014) with a widely-used copy mechanism (Gu et al., 2016; Merity et al., 2017; He et al., 2017; Xing et al., 2017; Zhu et al., 2017; Eric et al., 2017; Ke et al., 2018). To enable multi-hop reasoning, the model factorizes a transition matrix for a Markov chain. In order to focus on the capability of producing reasonable knowledge entities and adapting with dynamic knowledge gra"
D19-1194,W16-0106,0,0.143859,"als and Le, 2015) have emerged for its capability to be fully data-driven and endto-end trained. While the generated responses are often reasonable but general (without useful information), recent work proposed knowledgegrounded models (Eric et al., 2017; Ghazvininejad et al., 2018; Zhou et al., 2018b; Qian et al., 2018) to incorporate external facts in an end-toend fashion without hand-crafted slot filling. Effectively combining text and external knowledge 1 The data and code are available in https://github. com/Pascalson/DyKGChat. graphs have also been a crucial topic in question answering (Yin et al., 2016; Hao et al., 2017; Levy et al., 2017; Sun et al., 2018; Das et al., 2019). Nonetheless, prior work rarely analyzed the model capability of zero-shot adaptation to dynamic knowledge graphs, where the states/entities and their relations are temporal and evolve as a single time scale process. For example, as shown in Figure 1, the entity Jin-Xi was originally related to the entity Feng, Ruozhao with the type EnemyOf, but then evolved to be related to the entity Nian, Shilan. The goal of this paper is to facilitate knowledgegrounded neural conversation models to learn and zero-shot adapt with dyn"
D19-1333,N18-2091,0,0.0789317,"Missing"
D19-1333,D17-1215,0,0.0294489,"ize the model to not simply learn the superficial correlation for answering questions. The experiments show that our proposed QAInfomax achieves the state-of-the-art performance on the benchmark Adversarial-SQuAD dataset1 . 1 Introduction Question answering tasks are widely used for training and testing machine comprehension and reasoning (Rajpurkar et al., 2016; Joshi et al., 2017). However, high performance in standard automatic metrics has been achieved with only superficial understanding, as models exploit simple correlations in the data that happen to be predictive on most test examples. Jia and Liang (2017) addressed this problem and proposed an adversarial version of the SQuAD dataset, which was created by adding a distractor sentence to each paragraph. The distractor sentences challenge the model robustness, and the created Adversarial-SQuAD data shows the inability of a model about distinguishing a sentence that actually answers the question from one that merely has words in common with it, where almost all state-of-the-art machine comprehension systems are significantly degraded on 1 The source code is publicly available at https:// github.com/MiuLab/QAInfomax. adversarial examples. Lewis an"
D19-1333,P17-1147,0,0.0362705,"do not actually answer the question. To address this problem, we propose QAInfomax as a regularizer in reading comprehension systems by maximizing mutual information among passages, a question, and its answer. QAInfomax helps regularize the model to not simply learn the superficial correlation for answering questions. The experiments show that our proposed QAInfomax achieves the state-of-the-art performance on the benchmark Adversarial-SQuAD dataset1 . 1 Introduction Question answering tasks are widely used for training and testing machine comprehension and reasoning (Rajpurkar et al., 2016; Joshi et al., 2017). However, high performance in standard automatic metrics has been achieved with only superficial understanding, as models exploit simple correlations in the data that happen to be predictive on most test examples. Jia and Liang (2017) addressed this problem and proposed an adversarial version of the SQuAD dataset, which was created by adding a distractor sentence to each paragraph. The distractor sentences challenge the model robustness, and the created Adversarial-SQuAD data shows the inability of a model about distinguishing a sentence that actually answers the question from one that merely"
D19-1333,D16-1264,0,0.0692539,", which look related but do not actually answer the question. To address this problem, we propose QAInfomax as a regularizer in reading comprehension systems by maximizing mutual information among passages, a question, and its answer. QAInfomax helps regularize the model to not simply learn the superficial correlation for answering questions. The experiments show that our proposed QAInfomax achieves the state-of-the-art performance on the benchmark Adversarial-SQuAD dataset1 . 1 Introduction Question answering tasks are widely used for training and testing machine comprehension and reasoning (Rajpurkar et al., 2016; Joshi et al., 2017). However, high performance in standard automatic metrics has been achieved with only superficial understanding, as models exploit simple correlations in the data that happen to be predictive on most test examples. Jia and Liang (2017) addressed this problem and proposed an adversarial version of the SQuAD dataset, which was created by adding a distractor sentence to each paragraph. The distractor sentences challenge the model robustness, and the created Adversarial-SQuAD data shows the inability of a model about distinguishing a sentence that actually answers the question"
D19-1627,D18-1181,0,0.0241161,"gs and the space of definition embeddings. Specifically, a better mapping indicates richer sense-specific cues in the contextualized word embedding. Different from the definition modeling in the prior work (Noraset et al., 2017; Gadetsky et al., 2018; Chang et al., 2018), we reformulate the task from natural language generation (NLG) to classification, i.e., selecting the most reasonable definition according to the target word and its contexts. As recent work has shown the great success in encoding lexical resources into consistent representations (Tissier et al., 2017; Bahdanau et al., 2017; Bosc and Vincent, 2018), in this paper, we leverage pretrained sentence encoder (Cer et al., 2018) to project all definitions in the Oxford dictionary to a consistent embedding space, supporting our reformulation which requires to learn a mapping transforming from the contextualized word embedding space to the definition embedding space. Therefore, we can avoid some predicaments in NLG, such as troubles in generating fluent sequences, the exposure bias problem (Ranzato et al., 2015) and the difficulties in evaluation (Stent et al., 2005). 2 Methodology The goal of this paper is to analyze whether we can distill sens"
D19-1627,D18-2029,0,0.0707493,"es richer sense-specific cues in the contextualized word embedding. Different from the definition modeling in the prior work (Noraset et al., 2017; Gadetsky et al., 2018; Chang et al., 2018), we reformulate the task from natural language generation (NLG) to classification, i.e., selecting the most reasonable definition according to the target word and its contexts. As recent work has shown the great success in encoding lexical resources into consistent representations (Tissier et al., 2017; Bahdanau et al., 2017; Bosc and Vincent, 2018), in this paper, we leverage pretrained sentence encoder (Cer et al., 2018) to project all definitions in the Oxford dictionary to a consistent embedding space, supporting our reformulation which requires to learn a mapping transforming from the contextualized word embedding space to the definition embedding space. Therefore, we can avoid some predicaments in NLG, such as troubles in generating fluent sequences, the exposure bias problem (Ranzato et al., 2015) and the difficulties in evaluation (Stent et al., 2005). 2 Methodology The goal of this paper is to analyze whether we can distill sense-specific information from the pretrained contextualized word embeddings s"
D19-1627,P18-2043,0,0.0305718,", context) pair. We train and evaluate our model on 1 The source code and the trained models are publicly available at https://github.com/MiuLab/GenDef. the online Oxford dictionary dataset released by Chang et al. (2018). To analyze if the embeddings are senseinformative, our work focuses on learning a mapping between the semantic space of contextualized word embeddings and the space of definition embeddings. Specifically, a better mapping indicates richer sense-specific cues in the contextualized word embedding. Different from the definition modeling in the prior work (Noraset et al., 2017; Gadetsky et al., 2018; Chang et al., 2018), we reformulate the task from natural language generation (NLG) to classification, i.e., selecting the most reasonable definition according to the target word and its contexts. As recent work has shown the great success in encoding lexical resources into consistent representations (Tissier et al., 2017; Bahdanau et al., 2017; Bosc and Vincent, 2018), in this paper, we leverage pretrained sentence encoder (Cer et al., 2018) to project all definitions in the Oxford dictionary to a consistent embedding space, supporting our reformulation which requires to learn a mapping tra"
D19-1627,D14-1181,0,0.00761504,"der are concatenated as the input to the transformation net, which is simply implemented as a fullyconnected layer with the ReLU activation. ble of automatically capturing potential similarities. Examples of the generated synonyms can be found in the supplementary material. 3 • ELMo: we apply a weighted sum over 3 extracted contextualized word embeddings, i.e., the output of character CNN and two LSTMs, getting a single context-dependent vector for concatenating with the target word vector. • BERT: the target word is tokenized into word pieces, and we use one-dimensional convolution (conv1d) (Kim, 2014) and maxpooling to tackle the variable-lengthed features. We extract the last 4 layers from BERT and jointly learn softmax-normalized weights corresponding to each layer similar to ELMo.2 Figure 1 illustrates the mapping model leveraging features from BERT, which expresses the best capability of carrying sense-specific explanation among all variants. 2.3 arg max cos(f¯(uw , v), y). (2) w∈V In our experiments, the word set corresponding to the top-k highest cosine scores often contains the actual target word, also overlapping with a few synonyms provided by the Oxford dictionary. When applying"
D19-1627,D17-1034,1,0.887162,"Missing"
D19-1627,D14-1113,0,0.100449,"Missing"
D19-1627,P02-1040,0,0.104809,"Missing"
D19-1627,N18-1202,0,0.0714681,"further investigate what contextualized word embeddings capture, this paper analyzes whether they can indicate the corresponding sense definitions and proposes a general framework that is capable of explaining word meanings given contextualized word embeddings for better interpretation. The experiments show that both ELMo and BERT embeddings can be well interpreted via a readable textual form, and the findings may benefit the research community for a better understanding of what the embeddings capture1 . 1 Introduction Contextualized word embeddings, such as ELMo, BERT, and OpenAI GPT, GPT-2 (Peters et al., 2018; Devlin et al., 2018; Radford et al., a,b) have been shown to yield richer representations of meaning and boosted many NLP tasks. To understand what contextualized word embeddings capture, Schuster et al. (2019) recently visualized the representations of ELMo and showed that 1) embeddings of the same word in different contexts can form a cluster, and 2) when a word has multiple senses, the embeddings can be separated into multiple distinct groups, one for each meaning. To further investigate the meanings contextualized word embeddings indicate, this paper focuses on analyzing whether a contex"
D19-1627,J82-2005,0,0.56429,"Missing"
D19-1627,N19-1162,0,0.0292834,"word meanings given contextualized word embeddings for better interpretation. The experiments show that both ELMo and BERT embeddings can be well interpreted via a readable textual form, and the findings may benefit the research community for a better understanding of what the embeddings capture1 . 1 Introduction Contextualized word embeddings, such as ELMo, BERT, and OpenAI GPT, GPT-2 (Peters et al., 2018; Devlin et al., 2018; Radford et al., a,b) have been shown to yield richer representations of meaning and boosted many NLP tasks. To understand what contextualized word embeddings capture, Schuster et al. (2019) recently visualized the representations of ELMo and showed that 1) embeddings of the same word in different contexts can form a cluster, and 2) when a word has multiple senses, the embeddings can be separated into multiple distinct groups, one for each meaning. To further investigate the meanings contextualized word embeddings indicate, this paper focuses on analyzing whether a contextualized embedding is sense-informative enough to indicate the corresponding sense definition given a (target word, context) pair. We train and evaluate our model on 1 The source code and the trained models are p"
D19-1627,D17-1024,0,0.0313171,"emantic space of contextualized word embeddings and the space of definition embeddings. Specifically, a better mapping indicates richer sense-specific cues in the contextualized word embedding. Different from the definition modeling in the prior work (Noraset et al., 2017; Gadetsky et al., 2018; Chang et al., 2018), we reformulate the task from natural language generation (NLG) to classification, i.e., selecting the most reasonable definition according to the target word and its contexts. As recent work has shown the great success in encoding lexical resources into consistent representations (Tissier et al., 2017; Bahdanau et al., 2017; Bosc and Vincent, 2018), in this paper, we leverage pretrained sentence encoder (Cer et al., 2018) to project all definitions in the Oxford dictionary to a consistent embedding space, supporting our reformulation which requires to learn a mapping transforming from the contextualized word embedding space to the definition embedding space. Therefore, we can avoid some predicaments in NLG, such as troubles in generating fluent sequences, the exposure bias problem (Ranzato et al., 2015) and the difficulties in evaluation (Stent et al., 2005). 2 Methodology The goal of this"
D19-5812,N19-1241,0,0.0384811,"Missing"
D19-5812,N18-1177,0,0.0249729,"Missing"
D19-5812,P17-1097,0,0.0241803,"Missing"
D19-5812,P16-1138,0,0.0543797,"Missing"
D19-5812,P18-2124,0,0.108177,"Missing"
D19-5812,D16-1264,0,0.466365,"MC models. • FlowDelta consistently improves the performance on various conversational MC datasets, including CoQA and QuAC. • The proposed method achieves the state-ofthe-art results on QuAC and sequential instruction understanding task (SCONE). Introduction Machine reading comprehension has been increasingly studied in the NLP area, which aims to read a given passage and then answer questions correctly. However, human usually seeks answers in a conversational manner by asking follow-up questions given the previous answers. Traditional machine reading comprehension (MC) tasks such as SQuAD (Rajpurkar et al., 2016) focus on a single-turn setting, and there is no connection between different questions and answers to the same passage. To address the multi-turn issue, several datasets about conversational question answering (QA) were introduced, such as CoQA (Reddy et al., 2018) and QuAC (Choi et al., 2018). Most existing machine comprehension models apply single-turn methods and augment the input with question and answer history, ignoring previous reasoning processes in the models. Recently proposed FlowQA (Huang et al., 2018) attempted at modeling such multi-turn reasoning in dialogues in order to improv"
D19-5812,P18-1193,0,0.0382763,"Missing"
D19-6206,D14-1181,0,0.00369178,"s. Therefore, we propose four types of mechanisms that incorporate hierarchy category knowledge to improve the ICD prediction below. Convolutional Models There are various models for sequence-level classification, and this paper focuses on two types of convolutional models for investigation. The models are described as follows. Note that the proposed mechanism is flexible for diverse models. TextCNN Let xi ∈ IRk be the k-dimensional word embedding corresponding to the i-th word in the document, represented by the matrix X = [x1 , x2 , ..., xN ], where N is the length of the document. TextCNN (Kim, 2014) applies both convolution and max-pooling operations in one dimension along the document length. For instance, a feature ci is generated from a window of words xi , xi+1 , ..., xi+h , where h is the kernel size of the filters. The pooling operation is then applied over c = [c1 , c2 , ..., cn−h+1 ] to pick the maximum value cˆ = max(c) as the feature corresponding to this filter. We implement the model with kernel Cluster Penalty Motivated by Nie et al. (2018), we compute two constraints to share the parameters of the ICD codes under the same categories. The between-cluster constraint, Ωbetween"
D19-6206,N18-1100,0,0.482787,"ment of Computer Science and Information Engineering National Taiwan University, Taipei, Taiwan {r06946004,r06922168}@ntu.edu.tw y.v.chen@ieee.org Abstract 10 taxonomies (Organization et al., 2007), and 2) noisy text, including irrelevant information, misspellings and non-standard abbreviations, and a large medical vocabulary. Several recent work attempted at solving this task by neural models (Shi et al., 2017; Mullenbach et al., 2018). However, most prior work considered the output labels independently, so that the codes with few samples are difficult to learn (Shi et al., 2017). Therefore, Mullenbach et al. (2018) proposed an attentional model to effectively utilize the textural forms of codes to facilitate learning. In addition to textual definitions of codes, the category domain knowledge may provide additional cues to allow the codes under same category to share parameters, so the codes with few samples can benefit from it. To effectively utilize the category knowledge from the ICD codes, this paper proposes several refined category losses and incorporate them into convolutional models and then evaluate the performance on both MIMIC-3 (Johnson et al., 2016) and our internal dataset. The experiments"
D19-6206,D18-1308,0,0.0138317,"parameters, so the codes with few samples can benefit from it. To effectively utilize the category knowledge from the ICD codes, this paper proposes several refined category losses and incorporate them into convolutional models and then evaluate the performance on both MIMIC-3 (Johnson et al., 2016) and our internal dataset. The experiments on MIMIC shows that the proposed knowledge integration model significantly improves the previous methods and achieves the state-of-the-art performance, and the improvement can also be observed in our internal dataset. The idea is similar to the prior work (Singh et al., 2018), which considered the keyword hierarchy for information extraction from medical documents, but our work focuses on leveraging domain knowledge for clinical code prediction. Our contributions are three-fold: Clinical notes are essential medical documents to record each patient’s symptoms. Each record is typically annotated with medical diagnostic codes, which means diagnosis and treatment. This paper focuses on predicting diagnostic codes given the descriptive present illness in electronic health records by leveraging domain knowledge. We investigate various losses in a convolutional model to"
D19-6214,W04-1013,0,\N,Missing
D19-6214,P17-1099,0,\N,Missing
D19-6214,P18-1240,0,\N,Missing
D19-6214,W18-5623,0,\N,Missing
D19-6214,N19-1423,0,\N,Missing
I13-1074,N12-1041,1,0.885506,"ASR transcripts due to recognition errors, lack of proper segmentation, etc. However, it also offers some advantages by making it possible to leverage extra-textual information such as emotion and other speaker states through an incorporation of prosodic knowledge into the summarization model. A study by Maskey and Hirschberg (2005) on the relevance of various levels of linguistic knowledge (including lexical, prosodic and discourse structure) showed that enhancing a summarizer with prosodic information leads to more accurate and informed results. In this work we extend the model proposed by Chen and Metze (2012c), where a random walk is performed on a lexico-topical graph structure to yield summaries. They exploited intra- and interspeaker relationships through partial topic sharing for judging the importance of utterances in the context of multi-party meetings. This paper, on the other hand, enriches the underlying graph structure with prosodic information, rather than lexicotopical knowledge, to model speaker states and emotions. Also different from Maskey and Hirschberg (2005), we model the multimedia document structure as a graph, which allows for flexibility as well as expressive power in repre"
I13-1074,N06-2023,0,0.0172524,"pical knowledge, to model speaker states and emotions. Also different from Maskey and Hirschberg (2005), we model the multimedia document structure as a graph, which allows for flexibility as well as expressive power in representation. This graph structure provides the easy incorporation of targeted features into the model as well as indepth analyses of individual feature contributions towards representing speaker information. To the best of our knowledge this paper presents the first attempt at performing speech summarization using no lexical information in a completely unsupervised setting. Maskey and Hirschberg (2006) use an HMM to perform summarization by relying solely on prosodic features. However, their model – unlike ours – is supervised. The only requirement of the model in this paper is a preprocessing step that segments the audio into “utThis paper presents a graph-based model that integrates prosodic features into an unsupervised speech summarization framework without any lexical information. In particular it builds on previous work using mutually reinforced random walks, in which a two-layer graph structure is used to select the most salient utterances of a conversation. The model consists of one"
I13-1074,2007.sigdial-1.26,0,0.0322773,"u et al., 2007; Brin and Page, 1998). 650 4 4.1 Experiments 1998)). The latter was empirically found to produce the most fine-grained and precise alignments, and was consequently used in all our experiments. Pre-processing – Time Alignment We have previously stressed that while our model is independent from the lexical representation of an audio document, it does rely on a preprocessing step that chunks the document into individual utterances. It is noted that this may not be a trivial task. Speaker diarization (Tranter and Reynolds, 2006) and utterance segmentation (Christensen et al., 2005; Geertzen et al., 2007) are open areas of research in the NLP community. Systems developed for these purposes may be used to produce the initial chunking required by our model. In this paper, however, we do not explore these methods and instead rely on segmentation obtained from manually produced textual transcripts. This is to study the efficacy of our model in isolation. A second reason for using textual transcripts is the presentation of experimental evaluation. This form of data allows for tangible results that are obtained through evaluation metrics such as ROUGE, which rely on measuring n-gram overlap between"
I13-1074,huang-etal-2006-open,0,0.032729,"ten acts as a latent channel for communication of information, where excitement or emphasis is implicitly conveyed by a speaker. Prosodic Feature Extraction As previously stated, the only pre-requisite of the model proposed in this paper is a segmentation of the input document into chunks that are dictated by some meaningful notion of utterances. Once the audio has been segmented utterance-wise, the rest of the pipeline is effectively agnostic to all but its acoustic properties. Given a set of pre-segmented audio files, we extract the following prosodic features from them using PRAAT scripts (Huang et al., 2006). 3 Two-Layer Mutually Reinforced Random Walk In this section we describe our method for modelling speech data as a two-layered interconnected graph structure and run the mutually reinforced random-walk algorithm for summarization. • Number of syllables and number of pauses. • Duration time, – which is the speaking time including pauses – and the phonation time, – 649 rule is given by: ( (t+1) (0) (t) FU = (1 − α)FU + α · LU P FP (t+1) (0) (t) FP = (1 − α)FP + α · LP U FU Given an input speech document that is suitably segmented into utterance chunks, we construct a linked two-layer graph G co"
I13-1074,W04-1013,0,0.00604759,"Missing"
I17-2028,E17-1042,0,0.0982729,"Missing"
I17-2028,D14-1181,0,0.00711088,"Missing"
I17-2028,I17-1074,1,0.836921,"Missing"
I17-2028,D14-1162,0,0.0860183,"an represent the given natural 3 In the experiments, CNN achieved slightly better performance with fewer parameters compared with BLSTM. 165 annotated. Human-human dialogues contain rich and complex human behaviors and bring much difficulty to all dialogue-related tasks. Given the fact that different speaker roles behave differently, DSTC4 is a suitable benchmark dataset for evaluation. We choose a mini-batch adam as the optimizer with the batch size of 128 examples (Kingma and Ba, 2014). The size of each hidden recurrent layer is 128. We use pre-trained 200dimensional word embeddings GloV e (Pennington et al., 2014). We only apply 30 training epochs without any early stop approach. The sentence encoder is implemented using a CNN with the filters of size [2, 3, 4], 128 filters each size, and max pooling over time. The idea is to capture the most important feature (the highest value) for each feature map. This pooling scheme naturally deals with variable sentence lengths. Please refer to Kim (2014) for more details. For both tasks, we focus on predicting multiple labels including speech acts and attributes, so the evaluation metric is average F1 score for balancing recall and precision in each utterance. N"
I17-5003,N16-1014,1,0.743479,"ieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism to control the dialogue act during generation in order to avoid repetition. Social Chat Bots Social bots are of growing importance in facilitating smooth interaction between humans and their electronic devices. Recently, researcher have begun to explore data-driven generation of conversational responses within the framework of nerual machine translation (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a), as illustrated in Figure 2. However, the generated responses are often too general to carry meaningful information, such as “I don’t know.”, which can serve as a response to any user questions. A mutual information based model was proposed to address the issue, a mutual information model is proposed by Li et al. (2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c). Furthermore, Li et al. (2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots"
I17-5003,P16-1094,1,0.83828,"ieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism to control the dialogue act during generation in order to avoid repetition. Social Chat Bots Social bots are of growing importance in facilitating smooth interaction between humans and their electronic devices. Recently, researcher have begun to explore data-driven generation of conversational responses within the framework of nerual machine translation (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a), as illustrated in Figure 2. However, the generated responses are often too general to carry meaningful information, such as “I don’t know.”, which can serve as a response to any user questions. A mutual information based model was proposed to address the issue, a mutual information model is proposed by Li et al. (2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c). Furthermore, Li et al. (2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots"
I17-5003,D16-1127,1,0.772136,"ieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism to control the dialogue act during generation in order to avoid repetition. Social Chat Bots Social bots are of growing importance in facilitating smooth interaction between humans and their electronic devices. Recently, researcher have begun to explore data-driven generation of conversational responses within the framework of nerual machine translation (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a), as illustrated in Figure 2. However, the generated responses are often too general to carry meaningful information, such as “I don’t know.”, which can serve as a response to any user questions. A mutual information based model was proposed to address the issue, a mutual information model is proposed by Li et al. (2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c). Furthermore, Li et al. (2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots"
I17-5003,I17-1074,1,0.653231,"Missing"
I17-5003,P17-1163,0,0.0437381,"Missing"
I17-5003,W16-3601,0,0.0238467,"works, including supervised learning and reinforcement learning (Yang et al., 2017). Wen et al. (2016) and Bordes and Weston (2016) introduced a network-based end-to-end trainable task-oriented dialogue system. The authors treated training a dialogue system as learning a mapping from dialogue histories to system responses, and applied an encoder-decoder model.However, the system is trained in a supervised fashion that requires a lot of training data. Thus, the agent cannot learn a robust dialogue policy since it never explore the unknown space that is not covered by the limited training data. Zhao and Eskenazi (2016) presented an end-toend reinforcement learning (RL) approach to dialogue state tracking and policy learning. They show some promising results when applying the agent to the task of guessing the famous person a user is thinking of. Dhingra et al. (2017) proposed an end-to-end differentiable KB-Infobot for efficient information access. Li et al. (2017b) presented an end-to-end neural dialogue system for task completion. The agent can handle a wide varity of question types, including user-initated request. 5 Instructors Yun-Nung (Vivian) Chen is currently an assistant professor at the Department"
I17-5003,N15-1020,1,0.821506,"these models (Chen et al., 2016a,b). In addition, the importance of the NLU module is investigated in Li et al. (2017a), showing that different types of errors from NLU can degrade the whole system’s performance in a reinforcement learning setting. Natural Language Generation NLG approaches can be grouped into two categories, one focuses on generating text using templates or rules (linguistic) methods, the other uses corpus-based statistical methods (Oh and Rudnicky, 2002). The RNN-based models have been applied to language generation for both social bots and task-orientated dialogue systems (Sordoni et al., 2015; Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned 7 Source: conversation history … because encoder of your game? EOS Yeah I’m on my Yeah I’m on my way decoder Target: response Figure 2: Illustration of a sequence-to-sequence model for chit-chat dialogues. 4 data by jointly optimizing sentence planning and surface realization, and language variation can be achieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism to control the dialogue act during generation in order to"
I17-5003,W17-5505,0,0.0173344,"inforcement learning (Li et al., 2016c). Furthermore, Li et al. (2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots are originally developed for different purposes, there is a trend of combining both as a step towards building an open-domain dialogue agent. For example, on the one hand, Ghazvininejad et al. (2017) presented a fully data-driven and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. On the other hand, Zhao et al. (2017) proposed a task-oriented dialogue agented based on the encoder-decoder model with chatting capability. End-to-End Task-Oriented Dialogue System Awaring the representation power of deep neural networks, there are more and more attempts to learning dialogue systems in an end-to-end fashion using different learning frameworks, including supervised learning and reinforcement learning (Yang et al., 2017). Wen et al. (2016) and Bordes and Weston (2016) introduced a network-based end-to-end trainable task-oriented dialogue system. The authors treated training a dialogue system as learning a mapping"
I17-5003,W15-4639,0,0.0679508,"Missing"
I17-5003,D15-1199,0,0.023949,"tion, the importance of the NLU module is investigated in Li et al. (2017a), showing that different types of errors from NLU can degrade the whole system’s performance in a reinforcement learning setting. Natural Language Generation NLG approaches can be grouped into two categories, one focuses on generating text using templates or rules (linguistic) methods, the other uses corpus-based statistical methods (Oh and Rudnicky, 2002). The RNN-based models have been applied to language generation for both social bots and task-orientated dialogue systems (Sordoni et al., 2015; Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned 7 Source: conversation history … because encoder of your game? EOS Yeah I’m on my Yeah I’m on my way decoder Target: response Figure 2: Illustration of a sequence-to-sequence model for chit-chat dialogues. 4 data by jointly optimizing sentence planning and surface realization, and language variation can be achieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism to control the dialogue act during generation in order to avoid repetition. Social Chat Bots Socia"
I17-5003,W13-4073,0,\N,Missing
I17-5003,P17-1045,1,\N,Missing
L16-1117,S15-2048,0,0.0430218,"Missing"
L16-1117,P10-2057,0,0.0173616,"is that what thFigure 1: Actionable item examples in meeting corpus. meeting (for example, when a meeting participant commits to an action item during a meeting, s/he may be sent a note or reminder about that action item, later on). Our annotations on a subset of the ICSI meetings corpus (Janin et al., 2003) enable research on actionable item detection and associated argument extraction tasks, and the study of appropriate user interface designs for implementation of these actionable items is left as a future work. Previous work on meeting understanding investigated de739 tection of decisions (Bui and Peters, 2010; Fern´andez et al., 2008), action items (Yang et al., 2008), agreement and disagreements (Galley et al., 2004; Hillard et al., 2003), and summarization (Riedhammer et al., 2010; Xie et al., 2009; Chen and Metze, 2013). Our task is closest to detection of action items, actually, action items are considered as a subgroup of actionable items that can be actionable in the form of reminders or to do lists. 2. Human-Machine Genre create_calendar_entry schedule a meeting with &lt;contact_name&gt;John&lt;/contact_name&gt; &lt;start_time&gt;this afternoon&lt;/start_time&gt; Human-Human Genre create_calendar_entry how about t"
L16-1117,W08-0125,0,0.0496516,"Missing"
L16-1117,P04-1085,0,0.0496143,"cipant commits to an action item during a meeting, s/he may be sent a note or reminder about that action item, later on). Our annotations on a subset of the ICSI meetings corpus (Janin et al., 2003) enable research on actionable item detection and associated argument extraction tasks, and the study of appropriate user interface designs for implementation of these actionable items is left as a future work. Previous work on meeting understanding investigated de739 tection of decisions (Bui and Peters, 2010; Fern´andez et al., 2008), action items (Yang et al., 2008), agreement and disagreements (Galley et al., 2004; Hillard et al., 2003), and summarization (Riedhammer et al., 2010; Xie et al., 2009; Chen and Metze, 2013). Our task is closest to detection of action items, actually, action items are considered as a subgroup of actionable items that can be actionable in the form of reminders or to do lists. 2. Human-Machine Genre create_calendar_entry schedule a meeting with &lt;contact_name&gt;John&lt;/contact_name&gt; &lt;start_time&gt;this afternoon&lt;/start_time&gt; Human-Human Genre create_calendar_entry how about the &lt;contact_name&gt;three of us&lt;/contact_name&gt; discuss this later &lt;start_time&gt;this afternoon&lt;/start_time&gt;? AIMU D"
L16-1117,D14-1002,0,0.0770124,"Missing"
L16-1117,N03-2012,0,0.0772157,"action item during a meeting, s/he may be sent a note or reminder about that action item, later on). Our annotations on a subset of the ICSI meetings corpus (Janin et al., 2003) enable research on actionable item detection and associated argument extraction tasks, and the study of appropriate user interface designs for implementation of these actionable items is left as a future work. Previous work on meeting understanding investigated de739 tection of decisions (Bui and Peters, 2010; Fern´andez et al., 2008), action items (Yang et al., 2008), agreement and disagreements (Galley et al., 2004; Hillard et al., 2003), and summarization (Riedhammer et al., 2010; Xie et al., 2009; Chen and Metze, 2013). Our task is closest to detection of action items, actually, action items are considered as a subgroup of actionable items that can be actionable in the form of reminders or to do lists. 2. Human-Machine Genre create_calendar_entry schedule a meeting with &lt;contact_name&gt;John&lt;/contact_name&gt; &lt;start_time&gt;this afternoon&lt;/start_time&gt; Human-Human Genre create_calendar_entry how about the &lt;contact_name&gt;three of us&lt;/contact_name&gt; discuss this later &lt;start_time&gt;this afternoon&lt;/start_time&gt;? AIMU Data Set The actionable"
L16-1499,E12-2021,0,0.0418682,"Missing"
N12-1041,W04-1013,0,0.0103064,"Missing"
N15-1064,N10-1138,0,0.0528064,"be expressed on the basis of semantic frames, which encompass three major components: frame (F), frame elements (FE), and lexical units (LU). For example, the frame “food” contains words referring to items of food. A descriptor frame element within the food frame indicates the characteristic of the food. For example, the phrase “low fat milk” should be analyzed with “milk” evoking the food frame and “low fat” filling the descriptor FE of that frame. In our approach, we parse all ASR-decoded utterances in our corpus using SEMAFOR5 , a stateof-the-art semantic parser for frame-semantic parsing (Das et al., 2010; Das et al., 2013), and extract all frames from semantic parsing results as slot candidates, where the LUs that correspond to the frames are extracted for slot filling. For example, Figure 2 shows an example of an ASR-decoded text output parsed by SEMAFOR. SEMAFOR generates three frames (capability, expensiveness, and locale by use) for the utterance, which we consider as slot candidates for training the SLU model. Note that for each slot candidate, SEMAFOR also includes the corresponding lexical unit (can i, cheap, 5 http://www.ark.cs.cmu.edu/SEMAFOR/ 621 Independent Semantic Decoder With ou"
N15-1064,W08-1301,0,0.00761516,"Missing"
N15-1064,J06-2003,0,0.0227167,"walk, are still less-studied for direct inference on knowledge graphs of the spoken contents. In the natural language processing literature, Lao et al. (2011) used a random walk algorithm to construct inference rules on large entity-based knowledge bases, and leveraged syntactic information for reading the Web (Lao et al., 2012). Even though this work has important contributions, the proposed algorithm cannot learn mutually-recursive relations, and does not to consider lexical items— in fact, more and more studies show that, in addition to semantic knowledge graphs, lexical knowledge graphs (Inkpen and Hirst, 2006; Song et al., 2011; Li et al., 2013b) that model surface-level natural language realization, multiword expressions, and context (Li et al., 2013a), are also critical for short text understanding (Song et al., 2011; Wang et al., 2014). From the engineering perspective, quick and easy development turnaround time for domain-specific dialogue applications is also critical (Chen and Rudnicky, 2014). Prior work shows that it is possible to use the frame-semantics theory to automatically in1 http://www.windowsphone.com/en-us/how-to/ wp8/cortana 2 http://www.google.com/landing/now 3 http://www.apple."
N15-1064,D11-1049,0,0.0360983,"ment perspective, empowering the system with a large knowledge base is of crucial significance to modern spoken dialogue systems. On this end, our work clearly aligns with recent studies on leveraging semantic knowledge graphs for SLU modeling (Heck et al., 2013; Hakkani-T¨ur et al., 2013; Hakkani-T¨ur et al., 2014; El-Kahky et al., 2014; Chen et al., 2014a). While leveraging external knowledge is the trend, efficient inference algorithms, such as random walk, are still less-studied for direct inference on knowledge graphs of the spoken contents. In the natural language processing literature, Lao et al. (2011) used a random walk algorithm to construct inference rules on large entity-based knowledge bases, and leveraged syntactic information for reading the Web (Lao et al., 2012). Even though this work has important contributions, the proposed algorithm cannot learn mutually-recursive relations, and does not to consider lexical items— in fact, more and more studies show that, in addition to semantic knowledge graphs, lexical knowledge graphs (Inkpen and Hirst, 2006; Song et al., 2011; Li et al., 2013b) that model surface-level natural language realization, multiword expressions, and context (Li et a"
N15-1064,D12-1093,0,0.00457124,"recent studies on leveraging semantic knowledge graphs for SLU modeling (Heck et al., 2013; Hakkani-T¨ur et al., 2013; Hakkani-T¨ur et al., 2014; El-Kahky et al., 2014; Chen et al., 2014a). While leveraging external knowledge is the trend, efficient inference algorithms, such as random walk, are still less-studied for direct inference on knowledge graphs of the spoken contents. In the natural language processing literature, Lao et al. (2011) used a random walk algorithm to construct inference rules on large entity-based knowledge bases, and leveraged syntactic information for reading the Web (Lao et al., 2012). Even though this work has important contributions, the proposed algorithm cannot learn mutually-recursive relations, and does not to consider lexical items— in fact, more and more studies show that, in addition to semantic knowledge graphs, lexical knowledge graphs (Inkpen and Hirst, 2006; Song et al., 2011; Li et al., 2013b) that model surface-level natural language realization, multiword expressions, and context (Li et al., 2013a), are also critical for short text understanding (Song et al., 2011; Wang et al., 2014). From the engineering perspective, quick and easy development turnaround t"
N15-1064,P14-2050,0,0.0784672,"ings of the raw audio conversation files, and then adapt the FrameNet-style frames to the semantic slots in the target semantic space, so that they can be used practically in the SDSs. Chen et al. formulated the semantic mapping and adaptation problem as a ranking problem to differentiate generic semantic concepts from target semantic space for task-oriented dialogue systems. This paper improves the adaptation process by leveraging distributed word embeddings associated with typed syntactic dependencies between words to infer inter-slot relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014). The proposed framework is shown in Figure 1. In the remainder of the section, we first introduce framesemantic parsing to obtain slot candidates. With slot candidates, then we train the independent semantic decoders. The adaptation process, which is the main focus of this paper, is performed to decide outputted slots. Finally we can build an SLU model based on the learned semantic decoders and induced slots. 3.1 Probabilistic Semantic Parsing FrameNet is a linguistically-principled semantic resource that offers annotations of predicate-argument semantics, and associated lexical units for Eng"
N15-1064,N13-1090,0,0.154004,"from automatic speech recognition (ASR) decodings of the raw audio conversation files, and then adapt the FrameNet-style frames to the semantic slots in the target semantic space, so that they can be used practically in the SDSs. Chen et al. formulated the semantic mapping and adaptation problem as a ranking problem to differentiate generic semantic concepts from target semantic space for task-oriented dialogue systems. This paper improves the adaptation process by leveraging distributed word embeddings associated with typed syntactic dependencies between words to infer inter-slot relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014). The proposed framework is shown in Figure 1. In the remainder of the section, we first introduce framesemantic parsing to obtain slot candidates. With slot candidates, then we train the independent semantic decoders. The adaptation process, which is the main focus of this paper, is performed to decide outputted slots. Finally we can build an SLU model based on the learned semantic decoders and induced slots. 3.1 Probabilistic Semantic Parsing FrameNet is a linguistically-principled semantic resource that offers annotations of predicate-argume"
N15-1064,P13-1045,0,0.00828976,"Missing"
N15-1064,P14-2105,0,0.060405,"ns the word participates in for training embeddings, where the embeddings are less topical but offer more functional similarity compared to original embeddings (Levy and Goldberg, 2014). Table 1 shows the extracted dependency-based contexts for each target word from the example in Figure 4, where headwords and their dependents can form the contexts by following the arc on a word in the dependency tree, and −1 denotes the directionality of the dependency. After replacing original bagof-words contexts with dependency-based contexts, we can train dependency-based embeddings for all target words (Yih et al., 2014; Bordes et al., 2011; Bordes et al., 2013). For training dependency-based word embeddings, each word w is associated with a word vector vw ∈ Rd and each context c is represented as a context vector vc ∈ Rd , where d is the embedding dimensionality. We learn vector representations for both words and contexts such that the dot product vw · vc associated with “good” word-context pairs belonging to the training data D is maximized, leading to the objective function: X 1 arg max log , (5) vw ,vc 1 + exp(−vc · vw ) (w,c)∈D which can be trained using stochastic-gradient updates (Levy and Goldberg, 2"
N15-1064,P98-1013,0,\N,Missing
N15-1064,C98-1013,0,\N,Missing
N15-1064,J14-1002,0,\N,Missing
N18-1194,I17-1074,1,0.833775,"earch topic in artificial intelligence and natural language processing areas. With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. Today, there are several virtual intelligent assistants, such as Apple’s Siri, Google’s Home, Microsoft’s Cortana, and Amazon’s Echo. Recent advance of deep learning has 1 The source code is at: https://github.com/ MiuLab/Time-Decay-SLU. inspired many applications of neural models to dialogue systems (Wen et al., 2017; Bordes et al., 2017; Dhingra et al., 2017; Li et al., 2017). A key component of a dialogue system is a spoken language understanding (SLU) module— it parses user utterances into semantic frames that capture the core meaning (Tur and De Mori, 2011). A typical pipeline of SLU is to first decide the domain given the input utterance, and based on the domain, to predict the intent and to fill associated slots corresponding to a domainspecific semantic template, where each utterance is treated independently (Hakkani-T¨ur et al., 2016; Chen et al., 2016b,a; Wang et al., 2016). To overcome the error propagation and further improve understanding performance, t"
N18-1194,D14-1162,0,0.0902557,"Unlike previous DSTC series collected human-computer dialogues, human-human dialogues contain rich and complex human behaviors and bring much difficulty to all the tasks. Given the complex dialogue patterns and longer contexts, DSTC4 is a suitable benchmark dataset for evaluation. We randomly selected 28 dialogues as the training set, 5 dialogues as the testing set, and 2 dialogues as the validation set. We choose the mini-batch Adam as the optimizer with the batch size of 256 examples. The size of each hidden recurrent layer is 128. We use pre-trained 200-dimensional word embeddings GloV e (Pennington et al., 2014). We only apply 30 training epochs without any early stop approach. We focus on predicting multiple labels including intents and attributes, so the evaluation metric is an average F1 score for balancing recall and precision in each utterance. The experiments are shown in Table 1, where we report the average results over five runs. We include the best understanding performance (row (a)) from the participants of DSTC4 in IWSDS 2016 for reference (Kim et al., 2016). The one-tailed t-test is performed to validate the significance of improvement, and the numbers with markers indicate the significan"
N18-1194,I17-2028,1,0.850311,"antic template, where each utterance is treated independently (Hakkani-T¨ur et al., 2016; Chen et al., 2016b,a; Wang et al., 2016). To overcome the error propagation and further improve understanding performance, the contextual information has been shown useful (Bhargava et al., 2013; Xu and Sarikaya, 2014; Chen et al., 2015; Sun et al., 2016). Prior work incorporated the dialogue history into the recurrent neural networks (RNN) for improving domain classification, intent prediction, and slot filling (Xu and Sarikaya, 2014; Shi et al., 2015; Weston et al., 2015; Chen et al., 2016c). Recently, Chi et al. (2017) and Zhang et al. (2018) demonstrated that modeling speaker role information can learn the notable variance in speaking habits during conversations in order to benefit understanding. In addition, neural models incorporating attention mechanisms have had great successes in machine translation (Bahdanau et al., 2014), image captioning (Xu et al., 2015), and various tasks. Attentional models have been successful because they separate two different concerns: 1) deciding which input contexts are most relevant to the output and 2) actually predicting an output given the most relevant inputs. For exa"
N18-1194,E17-1042,0,0.0417404,"ks such as booking a movie ticket have become an emerging research topic in artificial intelligence and natural language processing areas. With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. Today, there are several virtual intelligent assistants, such as Apple’s Siri, Google’s Home, Microsoft’s Cortana, and Amazon’s Echo. Recent advance of deep learning has 1 The source code is at: https://github.com/ MiuLab/Time-Decay-SLU. inspired many applications of neural models to dialogue systems (Wen et al., 2017; Bordes et al., 2017; Dhingra et al., 2017; Li et al., 2017). A key component of a dialogue system is a spoken language understanding (SLU) module— it parses user utterances into semantic frames that capture the core meaning (Tur and De Mori, 2011). A typical pipeline of SLU is to first decide the domain given the input utterance, and based on the domain, to predict the intent and to fill associated slots corresponding to a domainspecific semantic template, where each utterance is treated independently (Hakkani-T¨ur et al., 2016; Chen et al., 2016b,a; Wang et al., 2016). To overcome the error"
N18-1194,P17-1045,1,0.831512,"become an emerging research topic in artificial intelligence and natural language processing areas. With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. Today, there are several virtual intelligent assistants, such as Apple’s Siri, Google’s Home, Microsoft’s Cortana, and Amazon’s Echo. Recent advance of deep learning has 1 The source code is at: https://github.com/ MiuLab/Time-Decay-SLU. inspired many applications of neural models to dialogue systems (Wen et al., 2017; Bordes et al., 2017; Dhingra et al., 2017; Li et al., 2017). A key component of a dialogue system is a spoken language understanding (SLU) module— it parses user utterances into semantic frames that capture the core meaning (Tur and De Mori, 2011). A typical pipeline of SLU is to first decide the domain given the input utterance, and based on the domain, to predict the intent and to fill associated slots corresponding to a domainspecific semantic template, where each utterance is treated independently (Hakkani-T¨ur et al., 2016; Chen et al., 2016b,a; Wang et al., 2016). To overcome the error propagation and further improve understand"
N18-2010,W17-5525,0,0.185162,"Missing"
N18-2010,W15-4639,0,0.392999,"es is crucial for user experience. The common and mostly adopted method is the rule-based (or template-based) method (Mirkovic and Cavedon, 2011), which can ensure the natural language quality and fluency. Considering that designing templates is time-consuming and the scalability issue, data-driven approaches have been investigated for open-domain NLG tasks. Recent advances in recurrent neural networkbased language model (RNNLM) (Mikolov et al., 2010, 2011) have demonstrated the capability of modeling long-term dependency by leveraging RNN structure. Previous work proposed an RNNLM-based NLG (Wen et al., 2015) that can be trained on any corpus of dialogue actutterance pairs without any semantic alignment and hand-crafted features. Sequence-to-sequence (seq2seq) generators (Cho et al., 2014; Sutskever et al., 2014) further offer better results by leveraging encoder-decoder structure: previous model encoded syntax trees and dialogue acts into sequences (Duˇsek and Jurˇc´ıcˇ ek, 2016) as inputs of attentional seq2seq model (Bahdanau et al., 2015). However, it is challenging to generate long and complex sentences by the simple encoder-decoder structure due to grammar complexity and lack of diction know"
N18-2010,E17-1042,0,0.0362578,"ng complex and long sentences, because the decoder has to learn all grammar and diction knowledge. This paper introduces a hierarchical decoding NLG model based on linguistic patterns in different levels, and shows that the proposed method outperforms the traditional one with a smaller model size. Furthermore, the design of the hierarchical decoding is flexible and easily-extensible in various NLG systems 1 . 1 Introduction Spoken dialogue systems that can help users to solve complex tasks have become an emerging research topic in artificial intelligence and natural language processing areas (Wen et al., 2017; Bordes et al., 2017; Dhingra et al., 2017; Li et al., 2017). A typical dialogue system pipeline contains a speech recognizer, a natural language understanding component, a dialogue manager, and a natural language generator (NLG). The first two authors have equal contributions. The source code is available at https://github. com/MiuLab/HNLG. 1 61 Proceedings of NAACL-HLT 2018, pages 61–66 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics GRU Decoder … a is 1. Repeat-input 2. Inner-Layer Teacher Forcing 3. Inter-Layer Teacher Forcing 4. Curriculum Learn"
N18-2010,P17-1045,1,0.827573,"the decoder has to learn all grammar and diction knowledge. This paper introduces a hierarchical decoding NLG model based on linguistic patterns in different levels, and shows that the proposed method outperforms the traditional one with a smaller model size. Furthermore, the design of the hierarchical decoding is flexible and easily-extensible in various NLG systems 1 . 1 Introduction Spoken dialogue systems that can help users to solve complex tasks have become an emerging research topic in artificial intelligence and natural language processing areas (Wen et al., 2017; Bordes et al., 2017; Dhingra et al., 2017; Li et al., 2017). A typical dialogue system pipeline contains a speech recognizer, a natural language understanding component, a dialogue manager, and a natural language generator (NLG). The first two authors have equal contributions. The source code is available at https://github. com/MiuLab/HNLG. 1 61 Proceedings of NAACL-HLT 2018, pages 61–66 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics GRU Decoder … a is 1. Repeat-input 2. Inner-Layer Teacher Forcing 3. Inter-Layer Teacher Forcing 4. Curriculum Learning Near All Bar One is a moderately priced"
N18-2010,P16-2008,0,0.118502,"Missing"
N19-1272,P82-1020,0,0.69164,"Missing"
N19-1272,D14-1058,0,0.544314,"s $10. How many notebook can he buy after buying 5 pens?” and the associated equation is x = (10 − 1 × 5) ÷ 0.5. The associated equation is x = (10 − 1 × 5) ÷ 0.5. • This paper achieves the state-of-the-art performance on the large benchmark dataset Math23K. • This paper is capable of providing interpretation and reasoning for the math word problem solving procedure. 2 Related Work There is a lot of prior work that utilized handcrafted features, such as POS tags, paths in the dependency trees, keywords, etc., to allow the model to focus on the quantities in the problems (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015; Roy and Roth, 2015; Koncel-Kedziorski et al., 2015; Roy et al., 2016; Upadhyay et al., 2016; Upadhyay and Chang, 2017; Roy and Roth, 2018; Wang et al., 2018). Recently, Mehta et al.; Wang et al.; Ling et al. attempted at learning models without predefined features. Following the recent trend, the proposed end-to-end model in this paper does not need any hand-crafted features. Kushman et al. first extracted templates about math expressions from the training answers, and then trained models to select templates and map quantities in the problem to the slots in the template. Su"
N19-1272,D15-1202,0,0.803588,"buying 5 pens?” and the associated equation is x = (10 − 1 × 5) ÷ 0.5. The associated equation is x = (10 − 1 × 5) ÷ 0.5. • This paper achieves the state-of-the-art performance on the large benchmark dataset Math23K. • This paper is capable of providing interpretation and reasoning for the math word problem solving procedure. 2 Related Work There is a lot of prior work that utilized handcrafted features, such as POS tags, paths in the dependency trees, keywords, etc., to allow the model to focus on the quantities in the problems (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015; Roy and Roth, 2015; Koncel-Kedziorski et al., 2015; Roy et al., 2016; Upadhyay et al., 2016; Upadhyay and Chang, 2017; Roy and Roth, 2018; Wang et al., 2018). Recently, Mehta et al.; Wang et al.; Ling et al. attempted at learning models without predefined features. Following the recent trend, the proposed end-to-end model in this paper does not need any hand-crafted features. Kushman et al. first extracted templates about math expressions from the training answers, and then trained models to select templates and map quantities in the problem to the slots in the template. Such two-stage approach has been tried a"
N19-1272,Q18-1012,0,0.371802,".5. • This paper achieves the state-of-the-art performance on the large benchmark dataset Math23K. • This paper is capable of providing interpretation and reasoning for the math word problem solving procedure. 2 Related Work There is a lot of prior work that utilized handcrafted features, such as POS tags, paths in the dependency trees, keywords, etc., to allow the model to focus on the quantities in the problems (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015; Roy and Roth, 2015; Koncel-Kedziorski et al., 2015; Roy et al., 2016; Upadhyay et al., 2016; Upadhyay and Chang, 2017; Roy and Roth, 2018; Wang et al., 2018). Recently, Mehta et al.; Wang et al.; Ling et al. attempted at learning models without predefined features. Following the recent trend, the proposed end-to-end model in this paper does not need any hand-crafted features. Kushman et al. first extracted templates about math expressions from the training answers, and then trained models to select templates and map quantities in the problem to the slots in the template. Such two-stage approach has been tried and achieved good results (Upadhyay and Chang, 2017). The prior work highly relied on human knowledge, where they parsed"
N19-1272,D16-1117,0,0.0261722,"Missing"
N19-1272,P14-1026,0,0.344897,"h pen takes $1. Tom has $10. How many notebook can he buy after buying 5 pens?” and the associated equation is x = (10 − 1 × 5) ÷ 0.5. The associated equation is x = (10 − 1 × 5) ÷ 0.5. • This paper achieves the state-of-the-art performance on the large benchmark dataset Math23K. • This paper is capable of providing interpretation and reasoning for the math word problem solving procedure. 2 Related Work There is a lot of prior work that utilized handcrafted features, such as POS tags, paths in the dependency trees, keywords, etc., to allow the model to focus on the quantities in the problems (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015; Roy and Roth, 2015; Koncel-Kedziorski et al., 2015; Roy et al., 2016; Upadhyay et al., 2016; Upadhyay and Chang, 2017; Roy and Roth, 2018; Wang et al., 2018). Recently, Mehta et al.; Wang et al.; Ling et al. attempted at learning models without predefined features. Following the recent trend, the proposed end-to-end model in this paper does not need any hand-crafted features. Kushman et al. first extracted templates about math expressions from the training answers, and then trained models to select templates and map quantities in the problem to the sl"
N19-1272,Q15-1001,0,0.184643,"k can he buy after buying 5 pens?” and the associated equation is x = (10 − 1 × 5) ÷ 0.5. The associated equation is x = (10 − 1 × 5) ÷ 0.5. • This paper achieves the state-of-the-art performance on the large benchmark dataset Math23K. • This paper is capable of providing interpretation and reasoning for the math word problem solving procedure. 2 Related Work There is a lot of prior work that utilized handcrafted features, such as POS tags, paths in the dependency trees, keywords, etc., to allow the model to focus on the quantities in the problems (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015; Roy and Roth, 2015; Koncel-Kedziorski et al., 2015; Roy et al., 2016; Upadhyay et al., 2016; Upadhyay and Chang, 2017; Roy and Roth, 2018; Wang et al., 2018). Recently, Mehta et al.; Wang et al.; Ling et al. attempted at learning models without predefined features. Following the recent trend, the proposed end-to-end model in this paper does not need any hand-crafted features. Kushman et al. first extracted templates about math expressions from the training answers, and then trained models to select templates and map quantities in the problem to the slots in the template. Such two-stage appro"
N19-1272,P17-1015,0,0.202697,"n be found in Algorithm 1. 4 Experiments To evaluate the performance of the proposed model, we conduct the experiments on the benchmark dataset and analyze the learned semantics. 4.1 Settings The experiments are benchmarked on the dataset Math23k (Wang et al., 2017), which contains 23,162 math problems with annotated equations. Each problem can be solved by a singleunknown-variable equation and only uses operators +, −, ×, ÷. Also, except π and 1, quantities in the equation can be found in the problem text. There are also other large scale datasets like Dolphin18K (Shi et al., 2015) and AQuA (Ling et al., 2017), containing 18,460 and 100,000 math word problems respectively. The reasons about not evaluating on these two datasets are 1) Dolphin18k contains some unlabeled math word problems and some incorrect labels, and 2) AQuA contains rational for solving the problems, but the equations in the rational are not formal (e.g. mixed with texts, using x to represent ×, etc.) and inconsistent. Therefore, the following experiments are performed and analyzed using Math23K, the only large scaled, good-quality dataset. 4.2 Results The results are shown in Table 1. The retrievalbased methods compare problems i"
N19-1272,D15-1166,0,0.0167041,"some operators are only applicable to certain combinations of operand semantics, which is similar to the type system in programming languages. For example, operating multiplication is applicable to the combination of “quantity of an item” and “price of an item”, while operating addition is not. Considering that all math operators supported here (+, −, ×, ÷) are binary operators, the semantic representations of the stack’s top 2 elements at the time t − 1 are considered: st = [eSlt ; eSlt ]. (5) • qt incorporates problem information in the decision. It is believed that the attention mechanism (Luong et al., 2015) can effectively capture dependency for longer distance. Thus, the attention mechanism over exp(si ) αi = Pm , l=1 exp(si ) (8) si = wT tanh(W T [u; vi ] + b). (9) In order to model the dynamic features for different decoding steps, features in rtsa is gated as follows: sa sa sa rtsa = [gt,1 · hD t ; gt,2 · st ; gt,3 · qt ], (10) gtsa (11) = σ(W sa · [hD t ; st ; qt ]), where σ is a sigmoid function and W sa is a learned gating parameter. rtopd is defined similarly, but with a different learned gating parameter W opd . 3.3.1 Stack Action Selector The stack action selector is to select an stack"
N19-1272,I17-3017,0,0.0281672,"Missing"
N19-1272,E17-1047,0,0.324078,"on is x = (10 − 1 × 5) ÷ 0.5. • This paper achieves the state-of-the-art performance on the large benchmark dataset Math23K. • This paper is capable of providing interpretation and reasoning for the math word problem solving procedure. 2 Related Work There is a lot of prior work that utilized handcrafted features, such as POS tags, paths in the dependency trees, keywords, etc., to allow the model to focus on the quantities in the problems (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015; Roy and Roth, 2015; Koncel-Kedziorski et al., 2015; Roy et al., 2016; Upadhyay et al., 2016; Upadhyay and Chang, 2017; Roy and Roth, 2018; Wang et al., 2018). Recently, Mehta et al.; Wang et al.; Ling et al. attempted at learning models without predefined features. Following the recent trend, the proposed end-to-end model in this paper does not need any hand-crafted features. Kushman et al. first extracted templates about math expressions from the training answers, and then trained models to select templates and map quantities in the problem to the slots in the template. Such two-stage approach has been tried and achieved good results (Upadhyay and Chang, 2017). The prior work highly relied on human knowledg"
N19-1272,D16-1029,0,0.0533189,". The associated equation is x = (10 − 1 × 5) ÷ 0.5. • This paper achieves the state-of-the-art performance on the large benchmark dataset Math23K. • This paper is capable of providing interpretation and reasoning for the math word problem solving procedure. 2 Related Work There is a lot of prior work that utilized handcrafted features, such as POS tags, paths in the dependency trees, keywords, etc., to allow the model to focus on the quantities in the problems (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015; Roy and Roth, 2015; Koncel-Kedziorski et al., 2015; Roy et al., 2016; Upadhyay et al., 2016; Upadhyay and Chang, 2017; Roy and Roth, 2018; Wang et al., 2018). Recently, Mehta et al.; Wang et al.; Ling et al. attempted at learning models without predefined features. Following the recent trend, the proposed end-to-end model in this paper does not need any hand-crafted features. Kushman et al. first extracted templates about math expressions from the training answers, and then trained models to select templates and map quantities in the problem to the slots in the template. Such two-stage approach has been tried and achieved good results (Upadhyay and Chang, 2017). The prior work highl"
N19-1272,D17-1088,0,0.360278,"Missing"
P15-1047,J80-3005,0,0.713369,"Missing"
P15-1047,P98-1013,0,0.0995053,"that each semantic slot occurs in the testing utterance, and how likely each slot is domain-specific simultaneously. In other words, the SLU model is able to transform the testing utterances into domain-specific semantic representations without human involvement. 4 The Matrix Factorization Approach Considering the benefits brought by MF techniques, including 1) modeling the noisy data, 2) modeling hidden semantics, and 3) modeling the 485 (capability, expensiveness, and locale by use) are generated for the utterance, which we consider as slot candidates for a domain-specific dialogue system (Baker et al., 1998). Then we build a slot matrix Fs with binary values based on the induced slots, which also denotes the slot features for the utterances (right part of the matrix in Figure 1(b)). To build the feature model MF , we concatenate two matrices: can i have a cheap restaurant Frame: expensiveness FT LU: cheap Frame: capability Frame: locale by use FT LU: can FE Filler: i FT/FE LU: restaurant Figure 2: An example of probabilistic framesemantic parsing on ASR output. FT: frame target. FE: frame element. LU: lexical unit. long-range dependencies between observations, in this work we apply an MF approach"
P15-1047,N15-1064,1,0.818583,"l., 2013b). Section 4.1 explains the detail of the feature model. In order to consider the additional inter-word and inter-slot relations, we propose a knowledge graph propagation model based on two knowledge graphs, which includes a word relation model (blue block) and a slot relation model (pink block), described in Section 4.2. The method of automatic knowledge graph construction is introduced in Section 5, where we leverage distributed word embeddings associated with typed syntactic dependencies to model the relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014; Chen et al., 2015). Finally, we train the SLU model by learning latent feature vectors for utterances and slot candidates through MF techniques. Combining with a knowledge graph propagation model based on word/slot relations, the trained SLU model estimates the probability that each semantic slot occurs in the testing utterance, and how likely each slot is domain-specific simultaneously. In other words, the SLU model is able to transform the testing utterances into domain-specific semantic representations without human involvement. 4 The Matrix Factorization Approach Considering the benefits brought by MF techn"
P15-1047,N10-1138,0,0.0189836,"F by integrating a feature model and a knowledge graph propagation model below. p(Mu,x = 1 |θu,x ) = σ(θu,x ) = 4.1 Feature Model First, we build a word pattern matrix Fw with binary values based on observations, where each row represents an utterance and each column refers to an observed unigram. In other words, Fw carries the basic word vectors for the utterances, which is illustrated as the left part of the matrix in Figure 1(b). To induce the semantic elements, we parse all ASR-decoded utterances in our corpus using SEMAFOR2 , a state-of-the-art semantic parser for frame-semantic parsing (Das et al., 2010; Das et al., 2013), and extract all frames from semantic parsing results as slot candidates (Chen et al., 2013b; Dinarelli et al., 2009). Figure 2 shows an example of an ASR-decoded output parsed by SEMAFOR. Three FrameNet-defined frames 2 (2) Knowledge Graph Propagation Model Since SEMAFOR was trained on FrameNet annotation, which has a more generic frame-semantic context, not all the frames from the parsing results can be used as the actual slots in the domainspecific dialogue systems. For instance, in Figure 2, we see that the frames “expensiveness” and “locale by use” are essentially the"
P15-1047,W09-0505,0,0.0287041,"First, we build a word pattern matrix Fw with binary values based on observations, where each row represents an utterance and each column refers to an observed unigram. In other words, Fw carries the basic word vectors for the utterances, which is illustrated as the left part of the matrix in Figure 1(b). To induce the semantic elements, we parse all ASR-decoded utterances in our corpus using SEMAFOR2 , a state-of-the-art semantic parser for frame-semantic parsing (Das et al., 2010; Das et al., 2013), and extract all frames from semantic parsing results as slot candidates (Chen et al., 2013b; Dinarelli et al., 2009). Figure 2 shows an example of an ASR-decoded output parsed by SEMAFOR. Three FrameNet-defined frames 2 (2) Knowledge Graph Propagation Model Since SEMAFOR was trained on FrameNet annotation, which has a more generic frame-semantic context, not all the frames from the parsing results can be used as the actual slots in the domainspecific dialogue systems. For instance, in Figure 2, we see that the frames “expensiveness” and “locale by use” are essentially the key slots for the purpose of understanding in the restaurant query domain, whereas the “capability” frame does not convey particularly va"
P15-1047,P93-1008,0,0.0867577,"Missing"
P15-1047,N13-1008,0,0.0514169,"tic slots per utterance. 4.4.2 det Figure 4: The dependency parsing result. ln p(Mu |θ) − λθ , where Mu is the vector corresponding to the utterance u from Mu,x in (1), because we assume that each utterance is independent of others. To avoid treating unobserved facts as designed negative facts, we consider our positive-only data as implicit feedback. Bayesian Personalized Ranking (BPR) is an optimization criterion that learns from implicit feedback for MF, which uses a variant of the ranking: giving observed true facts higher scores than unobserved (true or false) facts (Rendle et al., 2009). Riedel et al. (2013) also showed that BPR learns the implicit relations for improving the relation extraction task. dobj can i have a cheap restaurant u∈U = arg max u∈U nsubj u∈U = arg max 4.4.1 ccomp Relation Weight Estimation For the edges in the knowledge graphs, we model the relations between two connected nodes xi and xj as rˆ(xi , xj ), where x is either a slot s or a word pattern w. Since the weights are measured based on the relations between nodes regardless of the directions, we combine the scores of two directional dependencies: Optimization To maximize the objective in (6), we employ a stochastic grad"
P15-1047,J92-1004,0,0.103938,"ed MF approaches produce better SLU models that are able to predict semantic slots and word patterns taking into account their relations and domain-specificity in a joint manner. 1 Introduction A key component of a spoken dialogue system (SDS) is the spoken language understanding (SLU) module—it parses the users’ utterances into semantic representations; for example, the utterance “find a cheap restaurant” can be parsed into (price=cheap, target=restaurant) (Pieraccini et al., 1992). To design the SLU module of a SDS, most previous studies relied on predefined slots1 for training the decoder (Seneff, 1992; Dowding 1 A slot is defined as a basic semantic unit in SLU, such as “price” and “target” in the example. 483 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 483–494, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a knowledge graph propagation based model, fusing both a word-based lexical knowledge graph and a slot-based semantic graph. In fact, as it is shown in the Netflix challenge, MF is credited as the most useful technique for reco"
P15-1047,P13-1045,0,0.0113493,"Missing"
P15-1047,P14-2050,0,0.239037,"n Figure 1(a)) (Chen et al., 2013b). Section 4.1 explains the detail of the feature model. In order to consider the additional inter-word and inter-slot relations, we propose a knowledge graph propagation model based on two knowledge graphs, which includes a word relation model (blue block) and a slot relation model (pink block), described in Section 4.2. The method of automatic knowledge graph construction is introduced in Section 5, where we leverage distributed word embeddings associated with typed syntactic dependencies to model the relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014; Chen et al., 2015). Finally, we train the SLU model by learning latent feature vectors for utterances and slot candidates through MF techniques. Combining with a knowledge graph propagation model based on word/slot relations, the trained SLU model estimates the probability that each semantic slot occurs in the testing utterance, and how likely each slot is domain-specific simultaneously. In other words, the SLU model is able to transform the testing utterances into domain-specific semantic representations without human involvement. 4 The Matrix Factorization Approach Considering the benefits"
P15-1047,N13-1090,0,0.0868124,"ugh frame-semantic parsing (the yellow block in Figure 1(a)) (Chen et al., 2013b). Section 4.1 explains the detail of the feature model. In order to consider the additional inter-word and inter-slot relations, we propose a knowledge graph propagation model based on two knowledge graphs, which includes a word relation model (blue block) and a slot relation model (pink block), described in Section 4.2. The method of automatic knowledge graph construction is introduced in Section 5, where we leverage distributed word embeddings associated with typed syntactic dependencies to model the relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014; Chen et al., 2015). Finally, we train the SLU model by learning latent feature vectors for utterances and slot candidates through MF techniques. Combining with a knowledge graph propagation model based on word/slot relations, the trained SLU model estimates the probability that each semantic slot occurs in the testing utterance, and how likely each slot is domain-specific simultaneously. In other words, the SLU model is able to transform the testing utterances into domain-specific semantic representations without human involvement. 4 The Matri"
P15-1047,P14-2105,0,0.021105,"ns the word participates in for training embeddings, where the embeddings are less topical but offer more functional similarity compared to original embeddings (Levy and Goldberg, 2014). Table 1 shows the extracted dependency-based contexts for each target word from the example in Figure 4, where headwords and their dependents can form the contexts by following the arc on a word in the dependency tree, and −1 denotes the directionality of the dependency. After replacing original bag-of-words contexts with dependencybased contexts, we can train dependency-based embeddings for all target words (Yih et al., 2014; Bordes et al., 2011; Bordes et al., 2013). For training dependency-based word embeddings, each target x is associated with a vector vx ∈ Rd and each context c is represented as a context vector vc ∈ Rd , where d is the embedding dimensionality. We learn vector representations for both targets and contexts such that the dot product vx · vc associated with “good” targetcontext pairs belonging to the training data D is maximized, leading to the objective function: arg max Target Word restaurant cheap locale by use expansiveness Experiments Experimental Setup In this experiment, we used the Camb"
P15-1047,C98-1013,0,\N,Missing
P15-1047,J14-1002,0,\N,Missing
P15-1047,H93-1008,0,\N,Missing
P15-1047,P14-1004,0,\N,Missing
P15-3001,P15-1047,1,0.825973,") used a semi-supervised LDA model to show improvement on the slot filling task. Also, Zhai and Williams (2014) proposed an unsupervised model for connecting words with latent states in HMMs using topic models, obtaining interesting qualitative and quantitative results. However, for unsupervised SLU, it is not obvious how to incorporate additional information in the HMMs. With increasing works about learn2 ing the feature matrices for language representations (Mikolov et al., 2013), matrix factorization (MF) has become very popular for both implicit and explicit feedback (Rendle et al., 2009; Chen et al., 2015a). This thesis proposal is the first to propose a framework about unsupervised SLU modeling, which is able to simultaneously consider various local and global knowledge automatically learned from unlabelled data using a matrix factorization (MF) technique. 3 can i have a cheap restaurant Frame: expensiveness FT LU: cheap Frame: capability Frame: locale_by_use FT LU: can FE Filler: i FT/FE LU: restaurant Figure 2: An example of probabilistic framesemantic parsing on ASR output. FT: frame target. FE: frame element. LU: lexical unit. terance implies the meaning facet food. The MF approach is abl"
P15-3001,N15-1064,1,0.923885,") used a semi-supervised LDA model to show improvement on the slot filling task. Also, Zhai and Williams (2014) proposed an unsupervised model for connecting words with latent states in HMMs using topic models, obtaining interesting qualitative and quantitative results. However, for unsupervised SLU, it is not obvious how to incorporate additional information in the HMMs. With increasing works about learn2 ing the feature matrices for language representations (Mikolov et al., 2013), matrix factorization (MF) has become very popular for both implicit and explicit feedback (Rendle et al., 2009; Chen et al., 2015a). This thesis proposal is the first to propose a framework about unsupervised SLU modeling, which is able to simultaneously consider various local and global knowledge automatically learned from unlabelled data using a matrix factorization (MF) technique. 3 can i have a cheap restaurant Frame: expensiveness FT LU: cheap Frame: capability Frame: locale_by_use FT LU: can FE Filler: i FT/FE LU: restaurant Figure 2: An example of probabilistic framesemantic parsing on ASR output. FT: frame target. FE: frame element. LU: lexical unit. terance implies the meaning facet food. The MF approach is abl"
P15-3001,N13-1008,0,0.0113032,"ive-only data 2 The values in the diagonal of MR are 0 to model the propagation from other entries. 5 • SLU modeling by matrix factorization as implicit feedback. Bayesian Personalized Ranking (BPR) is an optimization criterion that learns In this thesis proposal, ongoing work and future from implicit feedback for MF, which uses a variplans have been presented towards an automatiant of the ranking: giving observed true facts cally built domain-specific SDS. With increasing higher scores than unobserved (true or false) semantic resources, such as Google’s Knowledge facts (Rendle et al., 2009). Riedel et al. (2013) Graph and Microsoft Satori, the dissertation shows also showed that BPR learns the implicit relations the feasibility that utilizing available knowledge and improves a relation extraction task. improves the generalization and the scalability of To estimate the parameters in (4), we create a dialogue system development for practical usage. dataset of ranked pairs from M in (3): for each utterance u and each observed fact f + = hu, x+ i, Acknowledgements where Mu,x ≥ δ, we choose each semantic conI thank my committee members, Prof. Alexander cept x− such that f − = hu, x− i, where Mu,x &lt; I. Rud"
P15-3001,N10-1138,0,0.0677016,"Missing"
P15-3001,W09-0505,0,0.0304178,"knowledge graph corresponding to the restaurant domain. The experiments showed that considering inter-slot relations is crucial for generating a more coherent and compete slot set, resulting in a better SLU model, while enhancing the interpretability of semantic slots. PREP_FOR PREP_FOR food AMOD expensiveness AMOD relational_quantity Figure 3: A simplified example of the automatically derived knowledge graph. ances are parsed using SEMAFOR1 , a state-ofthe-art frame-semantic parser (Das et al., 2010; Das et al., 2013), and then all frames from parsed results are extracted as slot candidates (Dinarelli et al., 2009). For example, Figure 2 shows an example of an ASR-decoded text output parsed by SEMAFOR. There are three frames (capability, expensiveness, and locale by use) in the utterance, which we consider as slot candidates. Since SEMAFOR was trained on FrameNet annotation, which has a more generic framesemantic context, not all the frames from the parsing results can be used as the actual slots in the domain-specific dialogue systems. For instance, in Figure 2, “expensiveness” and “locale by use” frames are essentially the key slots for the purpose of understanding in the restaurant query domain, wher"
P15-3001,P14-1004,0,0.0302999,"statistical speech recognition (Jelinek, 1997). Recently, Celikyilmaz et al. (2011) were the first to study the intent detection problem using query logs and a discrete Bayesian latent variable model. In the field of dialogue modeling, the partially observable Markov decision process (POMDP) (Young et al., 2013) model is a popular technique for dialogue management, reducing the cost of handcrafted dialogue managers while producing robustness against speech recognition errors. More recently, Tur et al. (2013) used a semi-supervised LDA model to show improvement on the slot filling task. Also, Zhai and Williams (2014) proposed an unsupervised model for connecting words with latent states in HMMs using topic models, obtaining interesting qualitative and quantitative results. However, for unsupervised SLU, it is not obvious how to incorporate additional information in the HMMs. With increasing works about learn2 ing the feature matrices for language representations (Mikolov et al., 2013), matrix factorization (MF) has become very popular for both implicit and explicit feedback (Rendle et al., 2009; Chen et al., 2015a). This thesis proposal is the first to propose a framework about unsupervised SLU modeling,"
P15-3001,J14-1002,0,\N,Missing
P17-1045,W14-4340,0,0.0295509,"ut as input, and selects an action at as output. The action space, denoted by A, consists of M + 1 actions — request(slot=i) for 1 ≤ i ≤ M will ask the user for the value of slot i, and inform(I) will inform the user with an ordered list of results I from the KB. The dialogue ends once the agent chooses inform. We adopt a modular approach, typical to goaloriented dialogue systems (Wen et al., 2016b), consisting of: a belief tracker module for identifying user intents, extracting associated slots, and tracking the dialogue state (Yao et al., 2014; Hakkani-T¨ur et al., 2016; Chen et al., 2016b; Henderson et al., 2014; Henderson, 2015); an interface with the database to query for relevant results (Soft-KB lookup); a summary module to summarize the state into a vector; a dialogue policy which selects the next system action based on current state (Young et al., 2013). We assume the agent only responds with dialogue acts. A templatebased Natural Language Generator (NLG) can be easily constructed for converting dialogue acts into natural language. 4.2 |{w ∈ ut } ∩ {w ∈ v}| |{w ∈ v}| stj [v] = Beliefs Summary  [v] + C stj [v] + btj + 1(req t [j] = 1) (5) ptj [v] ∝ pt−1 j Here C is a tuning parameter, and the n"
P17-1045,D16-1127,1,0.0992659,"l,jfgao}@microsoft.com Abstract y.v.chen@ieee.org factual questions, and sometimes also aimlessly chit-chat with the user, but they still lag far behind a human assistant in terms of both the variety and complexity of tasks they can perform. In particular, they lack the ability to learn from interactions with a user in order to improve and adapt with time. Recently, Reinforcement Learning (RL) has been explored to leverage user interactions to adapt various dialogue agents designed, respectively, for task completion (Gaˇsi´c et al., 2013), information access (Wen et al., 2016b), and chitchat (Li et al., 2016a). We focus on KB-InfoBots, a particular type of dialogue agent that helps users navigate a Knowledge Base (KB) in search of an entity, as illustrated by the example in Figure 1. Such agents must necessarily query databases in order to retrieve the requested information. This is usually done by performing semantic parsing on the input to construct a symbolic query representing the beliefs of the agent about the user goal, such as Wen et al. (2016b), Williams and Zweig (2016), and Li et al. (2017)’s work. We call such an operation a Hard-KB lookup. While natural, this approach has two drawback"
P17-1045,D15-1199,0,0.0476488,"Missing"
P17-1045,I17-1074,1,0.601884,"for task completion (Gaˇsi´c et al., 2013), information access (Wen et al., 2016b), and chitchat (Li et al., 2016a). We focus on KB-InfoBots, a particular type of dialogue agent that helps users navigate a Knowledge Base (KB) in search of an entity, as illustrated by the example in Figure 1. Such agents must necessarily query databases in order to retrieve the requested information. This is usually done by performing semantic parsing on the input to construct a symbolic query representing the beliefs of the agent about the user goal, such as Wen et al. (2016b), Williams and Zweig (2016), and Li et al. (2017)’s work. We call such an operation a Hard-KB lookup. While natural, this approach has two drawbacks: (1) the retrieved results do not carry any information about uncertainty in semantic parsing, and (2) the retrieval operation is non differentiable, and hence the parser and dialog policy are trained separately. This makes online endto-end learning from user feedback difficult once the system is deployed. In this work, we propose a probabilistic framework for computing the posterior distribution of the user target over a knowledge base, which we term a Soft-KB lookup. This distribution is const"
P17-1045,N07-2038,0,0.536228,"ing values denoted by X). periments that this framework allows the agent to achieve a higher task success rate in fewer dialogue turns. Further, the retrieval process is differentiable, allowing us to construct an end-to-end trainable KB-InfoBot, all of whose components are updated online using RL. Reinforcement learners typically require an environment to interact with, and hence static dialogue corpora cannot be used for their training. Running experiments on human subjects, on the other hand, is unfortunately too expensive. A common workaround in the dialogue community (Young et al., 2013; Schatzmann et al., 2007b; Scheffler and Young, 2002) is to instead use user simulators which mimic the behavior of real users in a consistent manner. For training KB-InfoBot, we adapt the publicly available2 simulator described in Li et al. (2016b). Evaluation of dialogue agents has been the subject of much research (Walker et al., 1997; M¨oller et al., 2006). While the metrics for evaluating an InfoBot are relatively clear — the agent should return the correct entity in a minimum number of turns — the environment for testing it not so much. Unlike previous KB-based QA systems, our focus is on multi-turn interaction"
P17-1045,2007.sigdial-1.48,0,0.0309207,"ing values denoted by X). periments that this framework allows the agent to achieve a higher task success rate in fewer dialogue turns. Further, the retrieval process is differentiable, allowing us to construct an end-to-end trainable KB-InfoBot, all of whose components are updated online using RL. Reinforcement learners typically require an environment to interact with, and hence static dialogue corpora cannot be used for their training. Running experiments on human subjects, on the other hand, is unfortunately too expensive. A common workaround in the dialogue community (Young et al., 2013; Schatzmann et al., 2007b; Scheffler and Young, 2002) is to instead use user simulators which mimic the behavior of real users in a consistent manner. For training KB-InfoBot, we adapt the publicly available2 simulator described in Li et al. (2016b). Evaluation of dialogue agents has been the subject of much research (Walker et al., 1997; M¨oller et al., 2006). While the metrics for evaluating an InfoBot are relatively clear — the agent should return the correct entity in a minimum number of turns — the environment for testing it not so much. Unlike previous KB-based QA systems, our focus is on multi-turn interaction"
P17-1045,W16-0105,0,0.0321301,"). These agents can perform simple tasks, answer 1 The source code is available at: https://github. com/MiuLab/KB-InfoBot 484 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 484–495 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1045 2 Related Work Entity-Centric Knowledge Base Movie=? Actor=Bill Murray Release Year=1993 Actor Release Year Groundhog Day Bill Murray 1993 Australia Nicole Kidman X Mad Max: Fury Road X 2015 Movie Our work is motivated by the neural GenQA (Yin et al., 2016a) and neural enquirer (Yin et al., 2016b) models for querying KBs via natural language in a fully “neuralized” way. However, the key difference is that these systems assume that users can compose a complicated, compositional natural language query that can uniquely identify the element/answer in the KB. The research task is to parse the query, i.e., turning the natural language query into a sequence of SQL-like operations. Instead we focus on how to query a KB interactively without composing such complicated queries in the first place. Our work is motivated by the observations that (1) users"
P17-1045,P97-1035,0,0.811706,"ers typically require an environment to interact with, and hence static dialogue corpora cannot be used for their training. Running experiments on human subjects, on the other hand, is unfortunately too expensive. A common workaround in the dialogue community (Young et al., 2013; Schatzmann et al., 2007b; Scheffler and Young, 2002) is to instead use user simulators which mimic the behavior of real users in a consistent manner. For training KB-InfoBot, we adapt the publicly available2 simulator described in Li et al. (2016b). Evaluation of dialogue agents has been the subject of much research (Walker et al., 1997; M¨oller et al., 2006). While the metrics for evaluating an InfoBot are relatively clear — the agent should return the correct entity in a minimum number of turns — the environment for testing it not so much. Unlike previous KB-based QA systems, our focus is on multi-turn interactions, and as such there are no publicly available benchmarks for this problem. We evaluate several versions of KB-InfoBot with the simulator and on real users, and show that the proposed Soft-KB lookup helps the reinforcement learner discover better dialogue policies. Initial experiments on the end-to-end agent also"
P17-1045,W16-3601,0,0.583178,"ity of the whole system. As a result, training of various components of the dialogue system is performed separately. The intent network and belief trackers are trained using supervised labels specifically collected for them; while the policy network and generation network are trained separately on the system utterances. We retain modularity of the network by keeping the belief trackers separate, but replace the hard lookup with a differentiable one. Dialogue agents can also interface with the database by augmenting their output action space with predefined API calls (Williams and Zweig, 2016; Zhao and Eskenazi, 2016; Bordes and Weston, 2016; Li et al., 2017). The API calls modify a query hypothesis maintained outside the end-toend system which is used to retrieve results from this KB. This framework does not deal with uncertainty in language understanding since the query hypothesis can only hold one slot-value at a time. Our approach, on the other hand, directly models the uncertainty to construct the posterior over the KB. Wu et al. (2015) presented an entropy minimization dialogue management strategy for InFind me the Bill Murray’s movie. When was it released? I think it came out in 1993. User Groundho"
P17-1045,D16-1233,0,0.025563,"Missing"
P17-1045,W16-0106,0,\N,Missing
P17-5004,W13-4073,0,0.0263856,"ponse Natural Language Generation (NLG) Where are you located? Dialogue Management (DM) • Dialogue State Tracking (DST) • Dialogue Policy Optimization System Action / Policy request_location Backend Knowledge Providers Figure 1: Pipeline framework of spoken dialog system. W S I find action ↓ ↓ O B-genre find movie movies ↓ O this ↓ B-date weekend ↓ I-date forcement learning setting. Dialogue Management The state-of-the-art dialog managers focus on monitoring the dialog progress by neural dialog state tracking models. Among the initial models are the RNN based dialog state tracking approaches (Henderson et al., 2013) that has shown to outperform Bayesian networks (Thomson and Young, 2010). More recent work on Neural Dialog Managers that provide conjoint representations between the utterances, slot-value pairs as well as knowledge graph representations (Wen et al., 2016; Mrkˇsi´c et al., 2016) demonstrate that using neural dialog models can overcome current obstacles of deploying dialogue systems in larger dialog domains. Figure 2: An example utterance with annotations of semantic slots in IOB format (S) and intent (I), B-date and I-date denote the date slot. 4 Deep Learning Based Dialogue System With the"
P17-5004,I17-1074,1,0.888554,"chitecture can be merged with CRFs (Xu and Sarikaya, 2013). Yao et al. (2013) and Mesnil et al. (2015) later employed RNNs for sequence labeling in order to perform slot filling. Such architectures have later been extended to jointly model intent detection and slot filling in multiple domains (Hakkani-T¨ur et al., 2016; Jaech et al., 2016). End-to-end memory networks have been shown to provide a good mechanism for integrating longer term knowledge context and shorter term dialogue context into these models (Chen et al., 2016b,c). In addition, the importance of the LU module is investigated in Li et al. (2017a), where different types of errors from LU may degrade the whole system performance in an reinNatural Language Generation The RNNbased models have been applied to language generation for both chit-chat and task-orientated dialogue systems (Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned data by jointly optimizing sentence planning and surface realization, and language variation can be easily achieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism for controlling the"
P17-5004,P17-1163,0,0.078703,"Missing"
P17-5004,W15-4639,0,0.0489578,"Missing"
P17-5004,J80-3005,0,0.746031,"Missing"
P17-5004,D15-1199,0,0.0125654,"ultiple domains (Hakkani-T¨ur et al., 2016; Jaech et al., 2016). End-to-end memory networks have been shown to provide a good mechanism for integrating longer term knowledge context and shorter term dialogue context into these models (Chen et al., 2016b,c). In addition, the importance of the LU module is investigated in Li et al. (2017a), where different types of errors from LU may degrade the whole system performance in an reinNatural Language Generation The RNNbased models have been applied to language generation for both chit-chat and task-orientated dialogue systems (Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned data by jointly optimizing sentence planning and surface realization, and language variation can be easily achieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism for controlling the dialogue act during generation in order to avoid semantics repetition, showing promising results. 5 Recent Trends and Challenges on Learning Dialogues This part will focus on discussing the recent trends and current challenges on dialogue system technology. 10 End-to-End Learnin"
P17-5004,W16-3601,0,0.0243641,"tion, showing promising results. 5 Recent Trends and Challenges on Learning Dialogues This part will focus on discussing the recent trends and current challenges on dialogue system technology. 10 End-to-End Learning for Dialogue System With the power of neural networks, there are more and more attempts for learning dialogue systems in an end-to-end fashion. Different learning frameworks are applied, including supervised learning and reinforcement learning. This part will discuss the work about end-to-end learning for dialogues (Dhingra et al., 2016; Wen et al., 2016; Williams and Zweig, 2016; Zhao and Eskenazi, 2016; Li et al., 2017b). Recent advance of deep learning has inspired many applications of neural models to dialogue systems. Wen et al. (2016) and Bordes and Weston (2016) introduced a network-based end-to-end trainable task-oriented dialogue system, which treated dialogue system learning as the problem of learning a mapping from dialogue histories to system responses, and applied an encoder-decoder model to train the whole system. However, the system is trained in a supervised fashion, thus requires a lot of training data, and may not be able to explore the unknown space that does not exist in t"
P19-1545,I17-2028,1,0.842279,"ned dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. The recent advance of deep learning has inspired many applications of neural dialogue systems (Wen et al., 2017; Bordes et al., 2017; Dhingra et al., 2017; Li et al., 2017). A typical dialogue system pipeline can be divided into several parts: 1) a speech recognizer that transcribes a user’s speech input into texts, 2) a natural language understanding module (NLU) that classifies the domain and associated intents and fills slots to form a semantic frame (Chi et al., 2017; Chen et al., 2017; Zhang et al., 2018; Su et al., 2018c, https://github.com/MiuLab/DualSL RESTAURANT=“McDonald’s” PRICE=“cheap” LOCATION= “nearby the station” Natural Language Generation Introduction 1 Semantic Frame Natural Language 2019), 3) a dialogue state tracker (DST) that predicts the current dialogue state in the multi-turn conversations, 4) a dialogue policy that determines the system action for the next step given the current state (Peng et al., 2018; Su et al., 2018a), and 5) a natural language generator (NLG) that outputs a response given the action semantic frame (Wen et al., 20"
P19-1545,P17-1045,1,0.839575,"nship.1 1 McDonald’s is a cheap restaurant nearby the station. Figure 1: NLU and NLG emerge as a dual form. Spoken dialogue systems that can help users solve complex tasks such as booking a movie ticket have become an emerging research topic in artificial intelligence and natural language processing areas. With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. The recent advance of deep learning has inspired many applications of neural dialogue systems (Wen et al., 2017; Bordes et al., 2017; Dhingra et al., 2017; Li et al., 2017). A typical dialogue system pipeline can be divided into several parts: 1) a speech recognizer that transcribes a user’s speech input into texts, 2) a natural language understanding module (NLU) that classifies the domain and associated intents and fills slots to form a semantic frame (Chi et al., 2017; Chen et al., 2017; Zhang et al., 2018; Su et al., 2018c, https://github.com/MiuLab/DualSL RESTAURANT=“McDonald’s” PRICE=“cheap” LOCATION= “nearby the station” Natural Language Generation Introduction 1 Semantic Frame Natural Language 2019), 3) a dialogue state tracker (DST) th"
P19-1545,I17-1074,1,0.837825,"s a cheap restaurant nearby the station. Figure 1: NLU and NLG emerge as a dual form. Spoken dialogue systems that can help users solve complex tasks such as booking a movie ticket have become an emerging research topic in artificial intelligence and natural language processing areas. With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. The recent advance of deep learning has inspired many applications of neural dialogue systems (Wen et al., 2017; Bordes et al., 2017; Dhingra et al., 2017; Li et al., 2017). A typical dialogue system pipeline can be divided into several parts: 1) a speech recognizer that transcribes a user’s speech input into texts, 2) a natural language understanding module (NLU) that classifies the domain and associated intents and fills slots to form a semantic frame (Chi et al., 2017; Chen et al., 2017; Zhang et al., 2018; Su et al., 2018c, https://github.com/MiuLab/DualSL RESTAURANT=“McDonald’s” PRICE=“cheap” LOCATION= “nearby the station” Natural Language Generation Introduction 1 Semantic Frame Natural Language 2019), 3) a dialogue state tracker (DST) that predicts the cu"
P19-1545,W17-5525,0,0.167693,"Missing"
P19-1545,P18-1203,0,0.101574,"Missing"
P19-1545,D18-1416,1,0.835858,"people can accomplish certain tasks more easily via natural language interactions. The recent advance of deep learning has inspired many applications of neural dialogue systems (Wen et al., 2017; Bordes et al., 2017; Dhingra et al., 2017; Li et al., 2017). A typical dialogue system pipeline can be divided into several parts: 1) a speech recognizer that transcribes a user’s speech input into texts, 2) a natural language understanding module (NLU) that classifies the domain and associated intents and fills slots to form a semantic frame (Chi et al., 2017; Chen et al., 2017; Zhang et al., 2018; Su et al., 2018c, https://github.com/MiuLab/DualSL RESTAURANT=“McDonald’s” PRICE=“cheap” LOCATION= “nearby the station” Natural Language Generation Introduction 1 Semantic Frame Natural Language 2019), 3) a dialogue state tracker (DST) that predicts the current dialogue state in the multi-turn conversations, 4) a dialogue policy that determines the system action for the next step given the current state (Peng et al., 2018; Su et al., 2018a), and 5) a natural language generator (NLG) that outputs a response given the action semantic frame (Wen et al., 2015; Su et al., 2018b; Su and Chen, 2018). Many artificia"
P19-1545,N18-2010,1,0.858018,"people can accomplish certain tasks more easily via natural language interactions. The recent advance of deep learning has inspired many applications of neural dialogue systems (Wen et al., 2017; Bordes et al., 2017; Dhingra et al., 2017; Li et al., 2017). A typical dialogue system pipeline can be divided into several parts: 1) a speech recognizer that transcribes a user’s speech input into texts, 2) a natural language understanding module (NLU) that classifies the domain and associated intents and fills slots to form a semantic frame (Chi et al., 2017; Chen et al., 2017; Zhang et al., 2018; Su et al., 2018c, https://github.com/MiuLab/DualSL RESTAURANT=“McDonald’s” PRICE=“cheap” LOCATION= “nearby the station” Natural Language Generation Introduction 1 Semantic Frame Natural Language 2019), 3) a dialogue state tracker (DST) that predicts the current dialogue state in the multi-turn conversations, 4) a dialogue policy that determines the system action for the next step given the current state (Peng et al., 2018; Su et al., 2018a), and 5) a natural language generator (NLG) that outputs a response given the action semantic frame (Wen et al., 2015; Su et al., 2018b; Su and Chen, 2018). Many artificia"
P19-1545,N18-1194,1,0.846765,"people can accomplish certain tasks more easily via natural language interactions. The recent advance of deep learning has inspired many applications of neural dialogue systems (Wen et al., 2017; Bordes et al., 2017; Dhingra et al., 2017; Li et al., 2017). A typical dialogue system pipeline can be divided into several parts: 1) a speech recognizer that transcribes a user’s speech input into texts, 2) a natural language understanding module (NLU) that classifies the domain and associated intents and fills slots to form a semantic frame (Chi et al., 2017; Chen et al., 2017; Zhang et al., 2018; Su et al., 2018c, https://github.com/MiuLab/DualSL RESTAURANT=“McDonald’s” PRICE=“cheap” LOCATION= “nearby the station” Natural Language Generation Introduction 1 Semantic Frame Natural Language 2019), 3) a dialogue state tracker (DST) that predicts the current dialogue state in the multi-turn conversations, 4) a dialogue policy that determines the system action for the next step given the current state (Peng et al., 2018; Su et al., 2018a), and 5) a natural language generator (NLG) that outputs a response given the action semantic frame (Wen et al., 2015; Su et al., 2018b; Su and Chen, 2018). Many artificia"
P19-1545,E17-1042,0,0.0243416,"g the effectiveness of the dual relationship.1 1 McDonald’s is a cheap restaurant nearby the station. Figure 1: NLU and NLG emerge as a dual form. Spoken dialogue systems that can help users solve complex tasks such as booking a movie ticket have become an emerging research topic in artificial intelligence and natural language processing areas. With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. The recent advance of deep learning has inspired many applications of neural dialogue systems (Wen et al., 2017; Bordes et al., 2017; Dhingra et al., 2017; Li et al., 2017). A typical dialogue system pipeline can be divided into several parts: 1) a speech recognizer that transcribes a user’s speech input into texts, 2) a natural language understanding module (NLU) that classifies the domain and associated intents and fills slots to form a semantic frame (Chi et al., 2017; Chen et al., 2017; Zhang et al., 2018; Su et al., 2018c, https://github.com/MiuLab/DualSL RESTAURANT=“McDonald’s” PRICE=“cheap” LOCATION= “nearby the station” Natural Language Generation Introduction 1 Semantic Frame Natural Language"
P19-1545,D15-1199,0,0.0975226,"Missing"
P19-1545,1983.tc-1.13,0,0.313183,"Missing"
W14-4414,P05-1045,0,0.00457601,"tage generation for surface realization. In preprocessing, we perform sentence segmentation for each email, and then manually annotate each sentence with a structure element, which is used to create a structural label sequence for each email and then to model sender style and topic structure for email organization (1st stage in the figure). The defined structural labels include greeting, inform, request, suggestion, question, answer, regard, acknowledgement, sorry, and signature. We also annotate content slots, including general classes automatically created by named entity recognition (NER) (Finkel et al., 2005) and hand-crafted topic classes, to model text content for surface realization (2nd stage in the figure). The content slots include person, organization, location, time, money, percent, and date (general classes), and meeting, issue, and discussion (topic classes). Introduction This paper focuses on synthesizing emails that reflect sender style and the intent of the communication. Such a process might be used for the generation of common messages (for example a request for a meeting without direct intervention from the sender). It can also be used in situations where naturalistic emails are ne"
W14-4414,W14-4425,1,\N,Missing
W14-4425,N04-1015,0,0.0202694,"le and are more spontaneous. Lampert et al. (2009) segmented email messages into zones, including sender zones, quoted conversation zones, and boilerplate zones. This paper only models the text in the sender zone, new content from the current sender. In the present work, we investigate the use of stochastic techniques for generation of a different class of communications and whether global structures can be convincingly created in the email domain. A lot of NLG systems are applied in dialogue systems, some of which focus on topic modeling (Sauper and Barzilay, 2009; Barzilay and Lapata, 2008; Barzilay and Lee, 2004), proposing algorithms to balance local fit of information and global coherence. However, they seldom consider to model the speaker’s characteristics. Gill et al. (2012) considered sentiment such as openness and neuroticism to specify characters for dialogue generation. In stead of modeling authors’ attitudes, this paper proposes the first approach of synthesizing emails by modeling their writing patterns. Specifically we investigate whether stochastic techniques can be used to acceptably model longer texts and individual speaker characteristics in the emails, both of which may require higher"
W14-4425,P05-1045,0,0.00994748,"d: to have or show respect or concern for, usually at the end of an email. 8. acknowledgement: to show or express appreciation or gratitude. 9. sorry: express regret, compunction, sympathy, pity, etc. 10. signature: a sender’s name usually at the end of the email. We perform sentence segmentation using punctuation and line-breaks and then manually tag each sentence with a structure label. We exclude the header of emails for labeling. Figure 2 shows an example email with structural labels. 3.2.1 General Class We use existing named entity recognition (NER) tools for identifying general classes. Finkel et al. (2005) used CRF to label sequences of words in text that are names of things, such as person, organization, etc. There are three models trained on different data, which are a 4-class model trained for CoNLL1 , a 7-class model trained for MUC, and a 3-class model trained on both data sets for the intersection of those class sets below. • 4-class: location, person, organization, misc • 7-class: location, person, organization, time, money, percent, date Considering that 3-class model performs higher accuracy and 7-class model provides better coverage, we take the union of outputs produced by 3class and"
W14-4425,W12-1508,0,0.0172628,"nly models the text in the sender zone, new content from the current sender. In the present work, we investigate the use of stochastic techniques for generation of a different class of communications and whether global structures can be convincingly created in the email domain. A lot of NLG systems are applied in dialogue systems, some of which focus on topic modeling (Sauper and Barzilay, 2009; Barzilay and Lapata, 2008; Barzilay and Lee, 2004), proposing algorithms to balance local fit of information and global coherence. However, they seldom consider to model the speaker’s characteristics. Gill et al. (2012) considered sentiment such as openness and neuroticism to specify characters for dialogue generation. In stead of modeling authors’ attitudes, this paper proposes the first approach of synthesizing emails by modeling their writing patterns. Specifically we investigate whether stochastic techniques can be used to acceptably model longer texts and individual speaker characteristics in the emails, both of which may require higher cohesion to be acceptable. This paper describes a two-stage process for stochastic generation of email, in which the first stage structures the emails according to sende"
W14-4425,D09-1096,0,0.0682054,"Missing"
W14-4425,P09-1024,0,0.0279537,"messages differ from them, which reflect senders’ style and are more spontaneous. Lampert et al. (2009) segmented email messages into zones, including sender zones, quoted conversation zones, and boilerplate zones. This paper only models the text in the sender zone, new content from the current sender. In the present work, we investigate the use of stochastic techniques for generation of a different class of communications and whether global structures can be convincingly created in the email domain. A lot of NLG systems are applied in dialogue systems, some of which focus on topic modeling (Sauper and Barzilay, 2009; Barzilay and Lapata, 2008; Barzilay and Lee, 2004), proposing algorithms to balance local fit of information and global coherence. However, they seldom consider to model the speaker’s characteristics. Gill et al. (2012) considered sentiment such as openness and neuroticism to specify characters for dialogue generation. In stead of modeling authors’ attitudes, this paper proposes the first approach of synthesizing emails by modeling their writing patterns. Specifically we investigate whether stochastic techniques can be used to acceptably model longer texts and individual speaker characterist"
W14-4425,J08-1001,0,\N,Missing
W15-3105,D09-1131,1,0.917531,"Missing"
W15-3105,W09-2307,0,0.0805233,"Missing"
W15-3105,P03-1056,0,0.0952037,"Missing"
W15-3105,D12-1132,0,0.059989,"Missing"
W15-3105,J05-4005,0,0.121342,"Missing"
W15-3105,P11-1141,0,0.0626573,"Missing"
W15-3105,I08-4008,0,0.0516585,"Missing"
W15-3105,huang-etal-2010-predicting,1,0.541592,"(Kenneth) Huang, Yun-Nung Chen, and Lingpeng Kong Language Technologies Institute, Carnegie Mellon University 5000 Forbes Ave, Pittsburgh, PA 15213, USA {tinghaoh, yvchen, lingpenk}@cs.cmu.edu Abstract reflects that the core task of Chinese morphological analysis should be aimed at bi-character words. Previous work tended to focus on longer unknown words (Tseng and Chen, 2002; Tseng et al., 2005; Lu et al., 2008; Qiu et al., 2008) or the functionality of morphemic characters (Galmar and Chen, 2010), and none of them effectively covered Chinese bi-character words. To the best of our knowledge, Huang et al. (2010) is the only work focused on Chinese bi-character words, where they analyzed Chinese morphological types and developed a suite of classifiers to predict the types. However, their work covers only a subset of Chinese content words and has limited scalability. Therefore, this paper addresses the issues, which expands their work by developing a more detailed scheme and collecting more words to produce a generalized analyzer. Our contributions are three-fold: While morphological information has been demonstrated to be useful for various Chinese NLP tasks, there is still a lack of complete theories"
W15-3105,C08-1089,0,0.0638818,"Missing"
W15-3105,W02-1811,0,0.106383,"Missing"
W15-3105,I05-3005,0,0.0875391,"Missing"
W15-3105,P13-1013,0,0.047095,"Missing"
