2020.coling-main.460,Identifying Motion Entities in Natural Language and A Case Study for Named Entity Recognition,2020,-1,-1,4,1,19497,ngoc vo,Proceedings of the 28th International Conference on Computational Linguistics,0,"Motion recognition is one of the basic cognitive capabilities of many life forms, however, detecting and understanding motion in text is not a trivial task. In addition, identifying motion entities in natural language is not only challenging but also beneficial for a better natural language understanding. In this paper, we present a Motion Entity Tagging (MET) model to identify entities in motion in a text using the Literal-Motion-in-Text (LiMiT) dataset for training and evaluating the model. Then we propose a new method to split clauses and phrases from complex and long motion sentences to improve the performance of our MET model. We also present results showing that motion features, in particular, entity in motion benefits the Named-Entity Recognition (NER) task. Finally, we present an analysis for the special co-occurrence relation between the person category in NER and animate entities in motion, which significantly improves the classification performance for the person category in NER."
L18-1035,A Large Resource of Patterns for Verbal Paraphrases,2018,0,0,1,1,21558,octavian popescu,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1469,{QUEST}: A Natural Language Interface to Relational Databases,2018,0,1,6,0,19498,vadim sheinin,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L16-1539,Corpora for Learning the Mutual Relationship between Semantic Relatedness and Textual Entailment,2016,0,2,2,1,19497,ngoc vo,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper we present the creation of a corpora annotated with both semantic relatedness (SR) scores and textual entailment (TE) judgments. In building this corpus we aimed at discovering, if any, the relationship between these two tasks for the mutual benefit of resolving one of them by relying on the insights gained from the other. We considered a corpora already annotated with TE judgments and we proceed to the manual annotation with SR scores. The RTE 1-4 corpora used in the PASCAL competition fit our need. The annotators worked independently of one each other and they did not have access to the TE judgment during annotation. The intuition that the two annotations are correlated received major support from this experiment and this finding led to a system that uses this information to revise the initial estimates of SR scores. As semantic relatedness is one of the most general and difficult task in natural language processing we expect that future systems will combine different sources of information in order to solve it. Our work suggests that textual entailment plays a quantifiable role in addressing it."
W15-1702,Paraphrase Identification and Semantic Similarity in {T}witter with Simple Features,2015,24,7,3,1,19497,ngoc vo,Proceedings of the third International Workshop on Natural Language Processing for Social Media,0,"Paraphrase Identification and Semantic Similarity are two different yet well related tasks in NLP. There are many studies on these two tasks extensively on structured texts in the past. However, with the strong rise of social media data, studying these tasks on unstructured texts, particularly, social texts in Twitter is very interesting as it could be more complicated problems to deal with. We investigate and find a set of simple features which enables us to achieve very competitive performance on both tasks in Twitter data. Interestingly, we also confirm the significance of using word alignment techniques from evaluation metrics in machine translation in the overall performance of these tasks."
S15-2005,{FBK}-{HLT}: An Effective System for Paraphrase Identification and Semantic Similarity in {T}witter,2015,10,2,3,1,19497,ngoc vo,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper reports the description and performance of our system, FBK-HLT, participating in the SemEval 2015, Task #1 Paraphrase and Semantic Similarity in Twitter, for both subtasks. We submitted two runs with different classifiers in combining typical features (lexical similarity, string similarity, word n-grams, etc) with machine translation metrics and edit distance features. We outperform the baseline system and achieve a very competitive result to the best system on the first subtask. Eventually, we are ranked 4 th out of 18 teams participating in subtask Paraphrase Identification."
S15-2018,{FBK}-{HLT}: A New Framework for Semantic Textual Similarity,2015,18,5,3,1,19497,ngoc vo,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper reports the description and performance of our system, FBK-HLT, participating in the SemEval 2015, Task #2 xe2x80x9cSemantic Textual Similarityxe2x80x9d, English subtask. We submitted three runs with different hypothesis in combining typical features (lexical similarity, string similarity, word n-grams, etc) with syntactic structure features, resulting in different sets of features. The results evaluated on both STS 2014 and 2015 datasets prove our hypothesis of building a STS system taking into consideration of syntactic information. We outperform the best system on STS 2014 datasets and achieve a very competitive result to the best system on STS 2015 datasets."
S15-2041,{FBK}-{HLT}: An Application of Semantic Textual Similarity for Answer Selection in Community Question Answering,2015,9,6,3,1,19497,ngoc vo,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper reports the description and performance of our system, FBK-HLT, participating in the SemEval 2015, Task #3 Answer Selection in Community Question Answering for English, for both subtasks. We submit two runs with different classifiers in combining typical features (lexical similarity, string similarity, word n-grams, etc.) with machine translation evaluation metrics and with some ad hoc features (e.g user overlapping, spam filtering). We outperform the baseline system and achieve interesting results on both subtasks."
S15-2053,{S}em{E}val-2015 Task 15: A {CPA} dictionary-entry-building task,2015,11,3,6,0,25266,vit baisa,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the first SemEval task to explore the usen of Natural Language Processing systems for building dictionaryn entries, in the framework of Corpus Pattern Analysis. CPA is an corpus-driven technique which provides tools and resources ton identify and represent unambiguously the main semantic patternsn in which words are used. Task 15 draws on the Patternn Dictionary of English Verbs (www.pdev.org.uk), for the targetedn lexical entries, and on the British National Corpus for then input text. Dictionary entry building is split into threen subtasks which all start from the same concordance sample: 1)n CPA parsing, where arguments and their syntactic and semanticn categories have to be identified, 2) CPA clustering, in whichn sentences with similar patterns have to be clustered and 3) CPAn automatic lexicography where the structure of patterns have ton be constructed automatically. Subtask 1 attracted 3 teams,n though none could beat the baseline (rule-based system).n Subtask 2 attracted 2 teams, one of which beat the baselinen (majority-class classifier). Subtask 3 did not attract anyn participant. The task has produced a major semanticn multidataset resource which includes data for 121 verbs andn about 17,000 annotated sentences, and which is freelyn accessible."
S15-2147,"{S}em{E}val 2015, Task 7: Diachronic Text Evaluation",2015,5,17,1,1,21558,octavian popescu,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"In this paper we describe a novel task, namely the Diachronic Text Evaluation task. A corpus of snippets which contain relevant information for the time when the text was created is extracted from a large collection of newspapers published between 1700 and 2010. The task, subdivided in three subtasks, requires the automatic system to identify the time interval when the piece of news was written. The subtasks concern specific type of information that might be available in news. The intervals come in three grades: fine, medium and coarse according to their length. The systems participating in the tasks have proved that this a doable task with very interesting possible continuations."
R15-1052,Learning the Impact of Machine Translation Evaluation Metrics for Semantic Textual Similarity,2015,0,0,3,0,17170,simone magnolini,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,None
R15-1088,Learning the Impact and Behavior of Syntactic Structure: A Case Study in Semantic Textual Similarity,2015,19,0,2,1,19497,ngoc vo,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"We present a case study on the role of syntactic structures towards resolving the Semantic Textual Similarity (STS) task. Although various approaches have been proposed, the research of using syntactic information to determine the semantic similarity is a relatively under-researched area. At the level of syntactic structure, it is not clear how significant the syntactic structure contributes to the overall accuracy of the task. In this paper, we analyze the impact of syntactic structure towards the overall performance and its behavior in different score ranges of the STS seman-"
P15-5004,Corpus Patterns for Semantic Processing,2015,12,1,1,1,21558,octavian popescu,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing: Tutorial Abstracts,0,"This tutorial presents a corpus-driven, pattern-based empirical approach to meaning representation and computation. Patterns in text are everywhere, but techniques for identifying and processing them are still rudimentary. Patterns are not merely syntactic but syntagmatic: each pattern identifies a lexico-semantic clause structure consisting of a predicator (verb or predicative adjective) together with open-ended lexical sets of collocates in different clause roles (subject, object, prepositional argument, etc.). If NLP is to make progress in identifying and processing text meaning, pattern recognition and collocational analysis will play an essential role, because:"
N15-2009,A Preliminary Evaluation of the Impact of Syntactic Structure in Semantic Textual Similarity and Semantic Relatedness Tasks,2015,15,1,2,1,19497,ngoc vo,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"The well related tasks of evaluating the Semantic Textual Similarity and Semantic Relatedness have been under a special attention in NLP community. Many different approaches have been proposed, implemented and evaluated at different levels, such as lexical similarity, word/string/POS tags overlapping, semantic modeling (LSA, LDA), etc. However, at the level of syntactic structure, it is not clear how significant it contributes to the overall accuracy. In this paper, we make a preliminary evaluation of the impact of the syntactic structure in the tasks by running and analyzing the results from several experiments regarding to how syntactic structure contributes to solving these tasks."
S14-2046,{FBK}-{TR}: Applying {SVM} with Multiple Linguistic Features for Cross-Level Semantic Similarity,2014,15,4,3,1,19497,ngoc vo,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"Recently, the task of measuring semantic similarity between given texts has drawn much attention from the Natural Language Processing community. Especially, the task becomes more interesting when it comes to measuring the semantic similarity between different-sized texts, e.g paragraph-sentence, sentence-phrase, phrase-word, etc. In this paper, we, the FBK-TR team, describe our system participating in Task 3 Cross-Level Semantic Similarity, at SemEval 2014. We also report the results obtained by our system, compared to the baseline and other participating systems in this task."
S14-2047,{FBK}-{TR}: {SVM} for Semantic Relatedeness and Corpus Patterns for {RTE},2014,11,5,2,1,19497,ngoc vo,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper reports the description and scores of our system, FBK-TR, which participated at the SemEval 2014 task #1 Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Entailment. The system consists of two parts: one for computing semantic relatedness, based on SVM, and the other for identifying the entailment values on the basis of both semantic relatedness scores and entailment patterns based on verb-specific semantic frames. The system ranked 11 th on both tasks with competitive results."
jezek-etal-2014-pas,{T}-{PAS}; A resource of Typed Predicate Argument Structures for linguistic analysis and semantic processing,2014,15,4,5,0,18933,elisabetta jezek,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The goal of this paper is to introduce T-PAS, a resource of typed predicate argument structures for Italian, acquired from corpora by manual clustering of distributional information about Italian verbs, to be used for linguistic analysis and semantic processing tasks. T-PAS is the first resource for Italian in which semantic selection properties and sense-in-context distinctions of verbs are characterized fully on empirical ground. In the paper, we first describe the process of pattern acquisition and corpus annotation (section 2) and its ongoing evaluation (section 3). We then demonstrate the benefits of pattern tagging for NLP purposes (section 4), and discuss current effort to improve the annotation of the corpus (section 5). We conclude by reporting on ongoing experiments using semiautomatic techniques for extending coverage (section 6)."
popescu-etal-2014-mapping,Mapping {CPA} Patterns onto {O}nto{N}otes Senses,2014,31,4,1,1,21558,octavian popescu,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper we present an alignment experiment between patterns of verb use discovered by Corpus Pattern Analysis (CPA; Hanks 2004, 2008, 2012) and verb senses in OntoNotes (ON; Hovy et al. 2006, Weischedel et al. 2011). We present a probabilistic approach for mapping one resource into the other. Firstly we introduce a basic model, based on conditional probabilities, which determines for any given sentence the best CPA pattern match. On the basis of this model, we propose a joint source channel model (JSCM) that computes the probability of compatibility of semantic types between a verb phrase and a pattern, irrespective of whether the verb phrase is a norm or an exploitation. We evaluate the accuracy of the proposed mapping using cluster similarity metrics based on entropy."
E14-1007,Inducing Example-based Semantic Frames from a Massive Amount of Verb Uses,2014,40,16,3,0,3202,daisuke kawahara,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present an unsupervised method for inducing semantic frames from verb uses in giga-word corpora. Our semantic frames are verb-specific example-based frames that are distinguished according to their senses. We use the Chinese Restaurant Process to automatically induce these frames from a massive amount of verb instances. In our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach."
D14-1171,Fast and Accurate Misspelling Correction in Large Corpora,2014,18,1,1,1,21558,octavian popescu,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"There are several NLP systems whose accuracy depends crucially on finding misspellings fast. However, the classical approach is based on a quadratic time algorithm with 80% coverage. We present a novel algorithm for misspelling detection, which runs in constant time and improves the coverage to more than 96%. We use this algorithm together with a cross document coreference system in order to find proper name misspellings. The experiments confirmed significant improvement over the state of the art."
W13-3811,Regular Patterns - Probably Approximately Correct Language Model,2013,0,1,1,1,21558,octavian popescu,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,None
W13-3830,Determining is-a relationships for Textual Entailment,2013,23,1,2,0,20226,vlad niculae,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,None
W13-0117,Learning Corpus Patterns Using Finite State Automata,2013,17,8,1,1,21558,octavian popescu,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,None
I13-1040,Behind the Times: Detecting Epoch Changes using Large Corpora,2013,11,19,1,1,21558,octavian popescu,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Using large corpora of chronologically ordered language, it is possible to explore diachronic phenomena, identifying previously unknown correlations between language usage and time periods, or epochs. We focused on a statistical approach to epoch delimitation and introduced the task of epoch characterization. We investigated the significant changes in the distribution of terms in the Google N-gram corpus and their relationships with emotion words. The results show that the method is reliable and the task is feasible."
popescu-2012-buildind,Buildind a Resource of Patterns Using Semantic Types,2012,4,1,1,1,21558,octavian popescu,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"While a word in isolation has a high potential of expressing various senses, in certain phrases this potential is restricted up to the point that one and only one sense is possible. A phrase is called sense stable if the senses of all the words compounding it do not change their sense irrespective of the context which could be added to its left or to its right. By comparing sense stable phrases we can extract corpus patterns. These patterns have slots which are filled by semantic types that capture the relevant information for disambiguation. The relationship between slots is such that a chain like disambiguation process is possible. Annotating a corpus with these kinds of patterns is beneficial for NLP, because problems such as data sparseness, noise, learning complexity are alleviated. We evaluate the inter agreement of annotators on examples coming from BNC."
C10-2114,Dynamic Parameters for Cross Document Coreference,2010,21,2,1,1,21558,octavian popescu,Coling 2010: Posters,0,"In this paper we present a new algorithm for the Person Cross Document Coreference task. We show that accurate results require a way to adapt the parameters of the similarity function - metrics and threshold -- to the ontological constraints obeyed by individuals. The technique we propose dynamically changes the initial weights computed when the context is analyzed. The weight recomputation is necessary in order to resolve clusters borders, which are inevitably blurred by a static approach. The results show a significant gain in accuracy."
N09-2039,Name Perplexity,2009,10,1,1,1,21558,octavian popescu,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"The accuracy of a Cross Document Coreference system depends on the amount of context available, which is a parameter that varies greatly from corpora to corpora. This paper presents a statistical model for computing name perplexity classes. For each perplexity class, the prior probability of coreference is estimated. The amount of context required for coreference is controlled by the prior coreference probability. We show that the prior probability coreference is an important factor for maintaining a good balance between precision and recall for cross document coreference systems."
D09-1104,Person Cross Document Coreference with Name Perplexity Estimates,2009,16,5,1,1,21558,octavian popescu,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"The Person Cross Document Coreference systems depend on the context for making decisions on the possible coreferences between person name mentions. The amount of context required is a parameter that varies from corpora to corpora, which makes it difficult for usual disambiguation methods. In this paper we show that the amount of context required can be dynamically controlled on the basis of the prior probabilities of coreference and we present a new statistical model for the computation of these probabilities. The experiment we carried on a news corpus proves that the prior probabilities of coreference are an important factor for maintaining a good balance between precision and recall for cross document coreference systems."
S07-1040,{IRST}-{BP}: Preposition Disambiguation based on Chain Clarifying Relationships Contexts,2007,8,7,1,1,21558,octavian popescu,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"We are going to present a technique of preposition disambiguation based on sense discriminative patterns, which are acquired using a variant of Angluin's algorithm. They represent the essential information extracted from a particular type of local contexts we call Chain Clarifying Relationship contexts. The data set and the results we present are from the Semeval task, WSD of Preposition (Litkowski 2007)."
S07-1041,{IRST}-{BP}: Web People Search Using Name Entities,2007,4,24,1,1,21558,octavian popescu,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"In this paper we describe a person clustering system for web pages and report the results we have obtained on the test set of the Semeval 2007 Web Person Search task. Deciding which particular person a name refers to within a text document depends mainly on the capacity to extract the relevant information out of texts when it is present. We consider relevant here to stand primarily for two properties: (1) uniqueness and (2) appropriateness. In order to address both (1) and (2) our method gives primary importance to Name Entities (NEs), defined according to the ACE specifications. The common nouns not referring to entities are considered further as coreference clues only if they are found within already coreferred documents."
W06-0504,Ontology Population from Textual Mentions: Task Definition and Benchmark,2006,10,20,3,0,1501,bernardo magnini,Proceedings of the 2nd Workshop on Ontology Learning and Population: Bridging the Gap between Text and Knowledge,0,"In this paper we propose and investigate Ontology Population from Textual Mentions (OPTM), a sub-task of Ontology Population from text where we assume that mentions for several kinds of entities (e.g. PERSON, ORGANIZATION , LOCATION , GEOPOLITICAL_ ENTITY) are already extracted from a document collection. On the one hand, OPTM simplifies the general Ontology Population task, limiting the input textual material; on the other hand, it introduces challenging extensions to Ontology Population restricted to named entities, being open to a wider spectrum of linguistic phenomena. We describe a manually created benchmark for OPTM and discuss several factors which determine the difficulty of the task."
atserias-etal-2004-cross,Cross-Language Acquisition of Semantic Models for Verbal Predicates,2004,11,2,3,0,36866,jordi atserias,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper presents a semantic-driven methodology for the automatic acquisition of verbal models. Our approach relies strongly on the semantic generalizations allowed by already existing resources (e.g. Domain labels, Named Entity categories, concepts in the SUMO ontology, etc). Several experiments have been carried out using comparable corpora in four languages (Italian, Spanish, Basque and English) and two domains (FINANCE and SPORT) showing that the semantic patterns acquired can be general enough to be ported from one language to the other language."
