2020.acl-demos.32,D19-1565,1,0.816288,"Missing"
2020.acl-demos.32,N19-1423,0,0.00636415,"ge [0,1], which allows us to show a confidence for each prediction. Further details about the techniques, the model, the data, and the experiments can be found in (Da San Martino et al., 2019).4 3 Figure 1: The architecture of our model. Model Our model is based on multi-task learning with the following two tasks: FLC Fragment-level classification. Given a sentence, identify all spans of use of propaganda techniques in it and the type of technique. SLC Sentence-level classification. Given a sentence, predict whether it contains at least one propaganda technique. Our model adds on top of BERT (Devlin et al., 2019) a set of layers that combine information from the fragment- and the sentence-level annotations to boost the performance of the FLC task on the basis of the SLC task. The network architecture is shown in Figure 1, and we refer to it as a multigranularity network. It features 19 output units for each input token in the FLC task, standing for one of the 18 propaganda techniques or “no technique.” A complementary output focuses on the SLC task, which is used to generate, through a trainable gate, a weight w that is multiplied by the input of the FLC task. The gate consists of a projection layer t"
2020.acl-demos.32,D17-2002,0,0.0670506,", the application allows users to input and to analyze any text or URL of interest; this is also possible via an API, which allows other applications to be built on top of the system. Prta relies on a supervised multi-granularity gated BERT-based model, which we train on a corpus of news articles annotated at the fragment level with 18 propaganda techniques, a total of 350K word tokens (Da San Martino et al., 2019). Consider the game Argotario, which educates people to recognize and create fallacies such as ad hominem, red herring and irrelevant authority, which directly relate to propaganda (Habernal et al., 2017, 2018a). Unlike them, we have a richer inventory of techniques and we show them in the context of actual news. The remainder of this paper is organized as follows. Section 2 introduces the machine learning model at the core of the Prta system. Section 3 sketches the full architecture of Prta, with focus on the process of collection and processing of the input articles. Section 4 describes the system interface and its functionality, and presents some examples. Section 5 draws conclusions and discusses possible directions for future work. Our work is in contrast to previous efforts, where propa"
2020.acl-demos.32,L18-1526,0,0.0984893,"f collection and processing of the input articles. Section 4 describes the system interface and its functionality, and presents some examples. Section 5 draws conclusions and discusses possible directions for future work. Our work is in contrast to previous efforts, where propaganda has been tackled primarily at the article level (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Barr´on-Cede˜no et al., 2019). It is also different from work in the related field of computational argumentation, which deals with some specific logical fallacies related to propaganda, such as ad hominem fallacy (Habernal et al., 2018b). 288 2 Data and Model Data We train our model on a corpus of 350K tokens (Da San Martino et al., 2019; Yu et al., 2019), manually annotated by professional annotators with the instances of use of eighteen propaganda techniques. See Table 1 for a complete list and examples for each of these techniques.2 2 Detailed list with definitions and examples is available at http://propaganda.qcri.org/annotations/definitions.html For the Prta system, we applied a softmax operator to turn its output into a bounded value in the range [0,1], which allows us to show a confidence for each prediction. Furthe"
2020.acl-demos.32,N18-1036,0,0.0123929,"f collection and processing of the input articles. Section 4 describes the system interface and its functionality, and presents some examples. Section 5 draws conclusions and discusses possible directions for future work. Our work is in contrast to previous efforts, where propaganda has been tackled primarily at the article level (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Barr´on-Cede˜no et al., 2019). It is also different from work in the related field of computational argumentation, which deals with some specific logical fallacies related to propaganda, such as ad hominem fallacy (Habernal et al., 2018b). 288 2 Data and Model Data We train our model on a corpus of 350K tokens (Da San Martino et al., 2019; Yu et al., 2019), manually annotated by professional annotators with the instances of use of eighteen propaganda techniques. See Table 1 for a complete list and examples for each of these techniques.2 2 Detailed list with definitions and examples is available at http://propaganda.qcri.org/annotations/definitions.html For the Prta system, we applied a softmax operator to turn its output into a bounded value in the range [0,1], which allows us to show a confidence for each prediction. Furthe"
2020.acl-demos.32,D17-1317,0,0.130782,"chniques and we show them in the context of actual news. The remainder of this paper is organized as follows. Section 2 introduces the machine learning model at the core of the Prta system. Section 3 sketches the full architecture of Prta, with focus on the process of collection and processing of the input articles. Section 4 describes the system interface and its functionality, and presents some examples. Section 5 draws conclusions and discusses possible directions for future work. Our work is in contrast to previous efforts, where propaganda has been tackled primarily at the article level (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Barr´on-Cede˜no et al., 2019). It is also different from work in the related field of computational argumentation, which deals with some specific logical fallacies related to propaganda, such as ad hominem fallacy (Habernal et al., 2018b). 288 2 Data and Model Data We train our model on a corpus of 350K tokens (Da San Martino et al., 2019; Yu et al., 2019), manually annotated by professional annotators with the instances of use of eighteen propaganda techniques. See Table 1 for a complete list and examples for each of these techniques.2 2 Detailed list with defi"
2020.semeval-1.186,2020.semeval-1.241,0,0.183778,"ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. A large number of the participating teams built systems that rely heavily on engineered features. For instance, Team CyberWallE(SI:8) (Blaschke et al., 2020) used features modeling sentiment, rhetorical structure, and POS tags, while team UTMN(SI:23) injected the sentiment intensity from VADER and it wa"
2020.semeval-1.186,I17-1078,0,0.0125125,"pective: a fine-grained analysis of the text that complements existing approaches and can, in principle, be combined with them. Propaganda in text (and in other channels) is conveyed through the use of diverse propaganda techniques (Miller, 1939), which range from leveraging on the emotions of the audience —such as using loaded language or appeals to fear— to using logical fallacies —such as straw men (misrepresenting someone’s opinion), hidden ad-hominem fallacies, and red herring (presenting irrelevant data). Some of these techniques have been studied in tasks such as hate speech detection (Gao et al., 2017) and computational argumentation (Habernal et al., 2018). Figure 1 shows the fine-grained propaganda identification pipeline, including the two targeted subtasks. Our goal is to facilitate the development of models capable of spotting text fragments where propaganda techniques are used. The task featured the following subtasks: Subtask SI (Span Identification): Given a plain-text document, identify those specific fragments that contain at least one propaganda technique. (This is a binary sequence tagging task.) Subtask TC (Technique Classification): Given a propagandistic text snippet and its"
2020.semeval-1.186,2020.semeval-1.187,0,0.0431519,"Missing"
2020.semeval-1.186,2020.semeval-1.238,0,0.192615,"8. CyberWallE     10. Duth   11. DiSaster  Ë    13. SocCogCom  Ë Ë  Ë 14. TTUI     15. JUST   16. NLFIIT Ë  Ë Ë Ë 17. UMSIForeseer    18. BPGC  Ë Ë  Ë   19. UPB   20. syrapropa      21. WMD  Ë Ë Ë Ë   22. YNUHPCC   Ë 24. DoNotDistribute     25. NTUAAILS   26. UAIC1860 ËË Ë    27. UNTLing     1. (Jurkiewicz et al., 2020) 2. (Chernyavskiy et al., 2020) 3. (Morio et al., 2020) 4. (Raj et al., 2020) 5. (Singh et al., 2020) 6. (Dimov et al., 2020) 7. (Grigorev and Ivanov, 2020) 8. (Blaschke et al., 2020) 10. (Bairaktaris et al., 2020) 11. (Kaas et al., 2020) 13. (Krishnamurthy et al., 2020) 14. (Kim and Bethard, 2020) 15. (Altiti et al., 2020) 16. (Martinkovic et al., 2020) 17. (Jiang et al., 2020) 18. (Patil et al., 2020) 19. (Paraschiv and Cercel, 2020) 20. (Li and Xiao, 2020) 21. (Daval-Frerot and Yannick, 2020) 22. (Dao et al., 2020) 24. (Kranzlein et al., 2020) 25. (Arsenos and Siolas, 2020) 26. (Ermurachi and Gifu, 2020) 27. (Petee and Palmer, 2020) 29. (Verma et al., 2020) Table 4: Overview of the approaches to the technique classification subtask. =part of the official submission; Ë=considered in internal experiments. The references to t"
2020.semeval-1.186,2020.semeval-1.230,0,0.187088,"n distributional semantics. Finally, team WMD(SI:33) (Daval-Frerot and Yannick, 2020) applied multiple strategies to augment the data such as back translation, synonym replacement and TF.IDF replacement (replace unimportant words, based on TF.IDF score, by other unimportant words). Closing the top-three submissions, Team aschern(SI:3) (Chernyavskiy et al., 2020) fine-tuned an ensemble of two differently intialized RoBERTa models, each with an attached CRF for sequence labeling and span character boundary post-processing. There have been several other promising strategies. Team LTIatCMU(SI:4) (Khosla et al., 2020) used a multi-granular BERT BiLSTM model with additional syntactic and semantic features at the word, sentence and document level, including PoS, named entities, sentiment, and subjectivity. It was trained jointly for token and sentence propaganda classification, with class balancing. They further fine-tuned BERT on persuasive language using 10,000 articles from propaganda websites, which turned out to be important. Team PsuedoProp(SI:14) (Chauhan and Diddee, 2020) built a preliminary sentence-level classifier using an ensemble of XLNet and RoBERTa, before it fine-tuned a BERT-based CRF sequen"
2020.semeval-1.186,2020.semeval-1.240,0,0.194543,"ApplicaAI aschern LTIatCMU UPB NoPropaganda CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the"
2020.semeval-1.186,2020.semeval-1.196,0,0.206471,"CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal"
2020.semeval-1.186,2020.semeval-1.232,0,0.203255,". 27. 28. 31. 33. – Hitachi ApplicaAI aschern LTIatCMU UPB NoPropaganda CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification"
2020.semeval-1.186,J15-3003,0,0.0243002,"Missing"
2020.semeval-1.186,S19-2149,1,0.79131,"of 24.88 only (and only 10.43 in the datathon). This is why, here we decided to split the task into subtasks in order to allow researchers to focus on one subtask at a time. Moreover, we merged some of the original 18 propaganda techniques to reduce data sparseness issues. Other related shared tasks include the FEVER 2018 and 2019 tasks on Fact Extraction and VERification (Thorne et al., 2018), the SemEval 2017 and 2019 tasks on determining the veracity of rumors (Derczynski et al., 2017; Gorrell et al., 2019) and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019). Also, the CLEF 2018–2020 CheckThat! labs’ shared tasks (Nakov et al., 2018; Elsayed et al., 2019a; Elsayed et al., 2019b; Barr´on-Cede˜no et al., 2020a; Barr´on-Cede˜no et al., 2020b), which featured tasks on automatic identification (Atanasova et al., 2018; Atanasova et al., 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019; Hasanain et al., 2020; Shaar et al., 2020) of claims in political debates and in social media. 8 You can also try the Prta system (Da San Martino et al., 2020b) online at: http://www.tanbih.org/prta http://www.datasciencesociety.net/hack-news-d"
2020.semeval-1.186,2020.semeval-1.245,0,0.0349937,"Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. A large number of the participating teams built systems that rely heavily on engineered features. For instance, Team CyberWallE(SI:8) (Blaschke et al., 2020) used features modeling sentiment, rhetorical s"
2020.semeval-1.186,2020.semeval-1.228,0,0.577546,"bindex on the right of each team represents their official rank in the subtasks. Appendix A includes brief descriptions of all systems. 4.1 Span Identification Subtask Table 3 shows a quick overview of the systems that took part in the SI subtask.7 All systems in the top-10 positions relied on some kind of Transformer, in combination with an LSTM or a CRF. In most cases, the Transformer-generated representations were complemented by engineered features, such as named entities and the presence of sentiment and subjectivity clues. Team Hitachi(SI:1) achieved the top performance in this subtask (Morio et al., 2020). They used a BIO encoding, which is typical for related segmentation and labeling tasks (e.g., named entity recognition). They relied upon a complex heterogeneous multi-layer neural network, trained end-to-end. The network uses pre-trained language models, which generate a representation for each input token. They further added part-of-speech (PoS) and named entity (NE) embeddings. As a result, there are three representations for each token, which are concatenated and used as an input to bi-LSTMs. At this moment, the network branches, as it is trained with three objectives: (i) the main BIO t"
2020.semeval-1.186,2020.semeval-1.244,0,0.326496,"S UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. A large number of the participating teams built systems that"
2020.semeval-1.186,2020.semeval-1.226,0,0.607064,"syntactic and semantic features at the word, sentence and document level, including PoS, named entities, sentiment, and subjectivity. It was trained jointly for token and sentence propaganda classification, with class balancing. They further fine-tuned BERT on persuasive language using 10,000 articles from propaganda websites, which turned out to be important. Team PsuedoProp(SI:14) (Chauhan and Diddee, 2020) built a preliminary sentence-level classifier using an ensemble of XLNet and RoBERTa, before it fine-tuned a BERT-based CRF sequence tagger to identify the exact spans. Team BPGC(SI:21) (Patil et al., 2020) went beyond these multigranularity approaches. Information both at the article and at the sentence level was considered when classifying each word as propaganda or not, by computing and concatenating vectorial representations for the three inputs. 7 Tables 3 and 4 only include the systems for which a description paper was submitted. 1384 Transformers Learning Models Representations Misc BERT RoBERTa XLNet XLM XLM RoBERTa ALBERT GPT-2 SpanBERT LaserTagger LSTM CNN SVM Na¨ıve Bayes Boosting Log regressor Random forest CRF Embeddings ELMo NEs Words/n-grams Chars/n-grams PoS Trees Sentiment Subje"
2020.semeval-1.186,2020.semeval-1.243,0,0.229367,"al., 2020) 3. (Morio et al., 2020) 4. (Raj et al., 2020) 5. (Singh et al., 2020) 6. (Dimov et al., 2020) 7. (Grigorev and Ivanov, 2020) 8. (Blaschke et al., 2020) 10. (Bairaktaris et al., 2020) 11. (Kaas et al., 2020) 13. (Krishnamurthy et al., 2020) 14. (Kim and Bethard, 2020) 15. (Altiti et al., 2020) 16. (Martinkovic et al., 2020) 17. (Jiang et al., 2020) 18. (Patil et al., 2020) 19. (Paraschiv and Cercel, 2020) 20. (Li and Xiao, 2020) 21. (Daval-Frerot and Yannick, 2020) 22. (Dao et al., 2020) 24. (Kranzlein et al., 2020) 25. (Arsenos and Siolas, 2020) 26. (Ermurachi and Gifu, 2020) 27. (Petee and Palmer, 2020) 29. (Verma et al., 2020) Table 4: Overview of the approaches to the technique classification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. Team aschern(TC:2) (Chernyavskiy et al., 2020) was the second best, and it based its success on a RoBERTa ensemble with several interesting techniques. They treated the task as one of sequence classification, using an average embedding of the surrounding tokens and the length of the span as contextual features. They further incorporated knowledge from the spa"
2020.semeval-1.186,C10-2115,1,0.593446,"Missing"
2020.semeval-1.186,2020.semeval-1.236,0,0.0349132,"Team 1. ApplicaAI    2. aschern     3. Hitachi           4. Solomon  Ë    5. newsSweeper Ë  ËË Ë 6. NoPropaganda   7. Inno Ë Ë ËË Ë Ë 8. CyberWallE     10. Duth   11. DiSaster  Ë    13. SocCogCom  Ë Ë  Ë 14. TTUI     15. JUST   16. NLFIIT Ë  Ë Ë Ë 17. UMSIForeseer    18. BPGC  Ë Ë  Ë   19. UPB   20. syrapropa      21. WMD  Ë Ë Ë Ë   22. YNUHPCC   Ë 24. DoNotDistribute     25. NTUAAILS   26. UAIC1860 ËË Ë    27. UNTLing     1. (Jurkiewicz et al., 2020) 2. (Chernyavskiy et al., 2020) 3. (Morio et al., 2020) 4. (Raj et al., 2020) 5. (Singh et al., 2020) 6. (Dimov et al., 2020) 7. (Grigorev and Ivanov, 2020) 8. (Blaschke et al., 2020) 10. (Bairaktaris et al., 2020) 11. (Kaas et al., 2020) 13. (Krishnamurthy et al., 2020) 14. (Kim and Bethard, 2020) 15. (Altiti et al., 2020) 16. (Martinkovic et al., 2020) 17. (Jiang et al., 2020) 18. (Patil et al., 2020) 19. (Paraschiv and Cercel, 2020) 20. (Li and Xiao, 2020) 21. (Daval-Frerot and Yannick, 2020) 22. (Dao et al., 2020) 24. (Kranzlein et al., 2020) 25. (Arsenos and Siolas, 2020) 26. (Ermurachi and Gifu, 2020) 27. (Petee and Palmer, 2020) 29. (Verma et al., 2020) Table 4:"
2020.semeval-1.186,D17-1317,0,0.400865,"ching very large audiences (Muller, 2018; Tard´aguila et al., 2018; Glowacki et al., 2018). Propaganda is most successful when it goes unnoticed by the reader, and it often takes some training for people to be able to spot it. The task is way more difficult for inexperienced users, and the volume of text produced on a daily basis makes it difficult for experts to cope with it manually. With the recent interest in “fake news”, the detection of propaganda or highly biased texts has emerged as an active research area. However, most previous work has performed analysis at the document level only (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019a) or has analyzed the general patterns of online propaganda (Garimella et al., 2015; Chatfield et al., 2015). SemEval-2020 Task 11 offers a different perspective: a fine-grained analysis of the text that complements existing approaches and can, in principle, be combined with them. Propaganda in text (and in other channels) is conveyed through the use of diverse propaganda techniques (Miller, 1939), which range from leveraging on the emotions of the audience —such as using loaded language or appeals to fear— to using logical fallacies —such as straw men (misrepres"
2020.semeval-1.186,2020.semeval-1.231,0,0.293475,"1. 2. 3. 4. 5. 7. 8. 9. 11. 13. 14. 16. 17. 20. 21. 22. 23. 25. 26. 27. 28. 31. 33. – Hitachi ApplicaAI aschern LTIatCMU UPB NoPropaganda CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et"
2020.semeval-1.186,2020.semeval-1.247,0,0.0380213,"processing Rank. Team 1. 2. 3. 4. 5. 7. 8. 9. 11. 13. 14. 16. 17. 20. 21. 22. 23. 25. 26. 27. 28. 31. 33. – Hitachi ApplicaAI aschern LTIatCMU UPB NoPropaganda CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 202"
2020.semeval-1.186,C18-1283,0,0.020638,"classification F1 performance on the development set. The systems are ordered based on the final ranking on the test set (cf. Table 6), whereas the ranking is the one on the development set. Columns 1 to 14 show the performance on each class (cf. Section 2). The best score for each class is bold. Rnk 1 2 8 16 5 10 11 13 7 6 23 9 26 28 17 24 20 19 18 21 3 33 29 25 30 32 34 37 36 41 43 27 4 12 14 15 22 31 35 38 39 40 42 44 45 46 47 6 Related Work Propaganda is particularly visible in the context of “fake news” on social media, which have attracted a lot of research recently (Shu et al., 2017). Thorne and Vlachos (2018) surveyed fact-checking approaches to fake news and related problems, and Li et al. (2016) focused on truth discovery in general. Two recent articles in Science offered a general discussion on the science of “fake news” (Lazer et al., 2018) and the process of proliferation of true and false news online (Vosoughi et al., 2018). We are particularly interested here in how different forms of propaganda are manifested in text. So far, the computational identification of propaganda has been tackled mostly at the article level. Rashkin et al. (2017) created a corpus, where news articles are labeled a"
2020.semeval-1.186,N18-1074,0,0.0272213,"d ELMo, or context-independent representations based on lexical, sentiment, readability, and TF-IDF features. As in the task at hand, ensembles were also popular. Still, the most successful submissions achieved an F1 -score of 24.88 only (and only 10.43 in the datathon). This is why, here we decided to split the task into subtasks in order to allow researchers to focus on one subtask at a time. Moreover, we merged some of the original 18 propaganda techniques to reduce data sparseness issues. Other related shared tasks include the FEVER 2018 and 2019 tasks on Fact Extraction and VERification (Thorne et al., 2018), the SemEval 2017 and 2019 tasks on determining the veracity of rumors (Derczynski et al., 2017; Gorrell et al., 2019) and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019). Also, the CLEF 2018–2020 CheckThat! labs’ shared tasks (Nakov et al., 2018; Elsayed et al., 2019a; Elsayed et al., 2019b; Barr´on-Cede˜no et al., 2020a; Barr´on-Cede˜no et al., 2020b), which featured tasks on automatic identification (Atanasova et al., 2018; Atanasova et al., 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019; Hasanain et al., 2"
2020.semeval-1.186,2020.semeval-1.239,0,0.211588,"Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. A large number of the participating teams built systems that rely heavily on engineered features. For instance, Team CyberWall"
2020.semeval-1.186,E17-1017,1,0.83519,"tained using distant supervision, assuming that all articles from a given news source share the label of that source, which introduces noise (Horne et al., 2018). Barr´on-Cede˜no et al. (2019b) experimented with a binary version of the problem: propaganda vs. no propaganda. See (Da San Martino et al., 2020a) for a recent survey on computational propaganda detection. In general, propaganda techniques serve as a means to persuade people, often in argumentative settings. While they may increase the rhetorical effectiveness of arguments, they naturally harm other aspects of argumentation quality (Wachsmuth et al., 2017). In particular, many of the span propaganda techniques considered in this shared task relate to the notion of fallacies, i.e. arguments whose reasoning is flawed in some way, often hidden and often on purpose (Tindale, 2007). Some recent work in computational argumentation has dealt with such fallacies. Among these, Habernal et al. (2018) presented and analyzed a corpus of web forum discussions with Ad hominem fallacies, and Habernal et al. (2017) introduced Argotario, a game that educates people to recognize fallacies. Argotario also had a corpus as a by-product, with 1.3k arguments annotate"
barron-cedeno-etal-2010-corpus,clough-etal-2002-building,0,\N,Missing
barron-cedeno-etal-2010-corpus,P02-1020,0,\N,Missing
C10-1005,J93-2003,0,0.047958,"ient for the CL-ESA data requirements. 3.2 In this model an estimation of how likely is that d′ is a translation of dq is performed. It is based on the adaptation of the Bayes rule for MT: p(d′ |dq ) = p(d′ ) p(dq |d′ ) . p(dq ) (1) As p(dq ) does not depend on d′ , it is neglected. From an MT point of view, the conditional probability p(dq |d′ ) is known as translation model probability and is computed on the basis of a statistical bilingual dictionary. p(d′ ) is known as language model probability; it describes the target language L′ in order to obtain grammatically acceptable translations (Brown et al., 1993). Translating dq into L′ is not the concern of this method, rather it focuses on retrieving texts written in L′ which are potential translations of dq . Therefore, Barr´on-Cede˜no et al. (2008) proposed replacing the language model (the one used in T+MA) by that known as length model. This model depends on text’s character lengths instead of language structures. Multiple translations from d into L′ are possible, and it is uncommon to find a pair of translated texts d and d′ such that |d |= |d′ |. Nevertheless, the length of such translations is closely related to a translation length factor. I"
C10-1005,clough-etal-2002-building,0,0.11682,"1 where µ and σ are the mean and the standard deviation of the character lengths between translations of texts from L into L′ . If the length of d′ is not the expected given dq , it receives a low qualification. The translation model probability is defined as: p(d |d′ ) = YX p(x, y), In other Information Retrieval tasks a plethora of corpora is available for experimental and comparison purposes. However, plagiarism implies an ethical infringement and, to the best of our knowledge, there is no corpora of actual cases available, other than some seminal efforts on creating corpora of text reuse (Clough et al., 2002), artificial plagiarism (Potthast et al., 2009), and simulated plagiarism (Clough and Stevenson, 2010). The problem is worse for cross-language plagiarism. Therefore, in our experiments we use two parallel corpora: Software, an en-eu translation memory of software manuals generously supplied by Elhuyar Fundazioa5 ; and Consumer, a corpus extracted from a consumer oriented magazine that includes articles written in Spanish along with their Basque, Catalan, and Galician translations6 (Alc´azar, 2006). Software includes 288, 000 parallel sentences; 8.66 (6.83) words per sentence in the English (B"
C10-1005,P07-2045,0,0.00738927,"Missing"
C10-1005,N04-1034,0,0.0137599,"Missing"
C10-1005,J03-1002,0,0.00763746,"Missing"
C10-1005,steinberger-etal-2006-jrc,0,0.0300396,"m of CLPD in Basque, with source texts written in Spanish (the co-official language of the Figure 1: First sentences from the Wikipedia articles “Party of European Socialists” (en),“Partido Socialista Europeo” (es), and “Europako Alderdi Sozialista” (eu) (Wikipedia, 2010b). 100 words are contained in the en, es and eu articles, respectively). Of high relevance is that the two corpora used in this work were manually constructed by translating English and Spanish text into Basque. In the experiments carried out by Potthast et al. (2010), which inspired our work, texts from the JCRAcquis corpus (Steinberger et al., 2006) and Wikipedia were used. The first one is a multilingual corpus with no clear definition of source and target languages, whereas in Wikipedia no specific relationship exists between the different languages in which a topic may be broached. In some cases (cf. Fig. 1) they are clearly co-derived, but in others they are completely independent. CLPD has been investigated just recently, mainly by adapting models formerly proposed for cross-language information retrieval. This is the case of cross-language explicit semantic analysis (CL-ESA), proposed by Potthast et al. (2008). In this case the com"
C10-1005,barron-cedeno-etal-2010-corpus,1,\N,Missing
C10-2115,ambati-etal-2010-active,0,0.0315161,"Missing"
C10-2115,N03-1003,0,0.0248306,"Missing"
C10-2115,clough-etal-2002-building,0,0.519814,"Missing"
C16-1163,C16-1237,1,0.746099,"Missing"
C16-1163,P15-2114,0,0.0146485,". Table 1: A re-ranking example: we report the Google rank (G), the gold standard relevance (GS) and our rank (R) for each question. et al., 2016), which exploits tree kernel function itself to auto-filter the non relevant subtrees. The main difference with the approach we present in the current paper is the use of neural networks for learning attention weights and thus modeling sentence or word pruning. Neural Approaches Recent work has shown the effectiveness of neural models for answer selection (Severyn and Moschitti, 2015; Tan et al., 2015; Feng et al., 2015) and question similarity (dos Santos et al., 2015) in community question answering. For instance, dos Santos et al. (2015) used CNN and bag-of-words (BOW) representations of original and related questions in order to compute cosine similarity scores. Recently, Bahdanau et al. (2014) presented a neural attention model for machine translation and showed that the attention mechanism is helpful for addressing long sentences. We use an LSTM model (Hochreiter and Schmidhuber, 1997) with an attention mechanism for capturing long dependencies in questions for the question similarity task. The major difference with previous work is that we exploit the"
C16-1163,P08-1019,0,0.0283351,"s our learning-to-rank approach. Section 4 describes the application of LSTMs in TK-based ranking models. Section 5 describes our text selection strategies. Section 6 discusses our experiments and the obtained results. Finally, Section 7 concludes the paper. 2 Related Work Question ranking in cQA has been central in the research community practically since the begining of cQA system design. Beside “standard” similarity measures, different characterizations and models have been explored. For instance, Cao et al. (2008) proposed a question recommendation system based on the questions’ topic and Duan et al. (2008) added the question’s focus into the formula. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). Here, the authors use LDA topic modeling to learn the latent semantic topics that generate question/answer pairs and use the learned topic distribution to retrieve similar historical questions. Various methods rely on machine-translation models. For instance, Jeon et al. (2005) and Zhou et al. (2011) used monolingual phrase-based translation models to compare the questions. Jeon et al. (2005) built their translator from a col"
C16-1163,P15-1097,1,0.787031,"ranslation models to compare the questions. Jeon et al. (2005) built their translator from a collection of previously identified similar questions whereas Zhou et al. (2011) used question–answer pairs. Other approaches are based on syntactic representations. This is the case of Wang et al. (2009), who consider the number of common substructures of parse trees to estimate the similarity between two questions. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use parse trees as well. The difference is that they use them directly within a tree kernel, with the use of the KeLP platform (Filice et al., 2015a). The latter two models were applied on the SemEval 2016 Task 3 challenge on cQA (Nakov et al., 2016), which proposed a task on question ranking (together with one on answer ranking). The best-performing system in this task was the one from Franco-Salvador et al. (2016), which used SVMrank (Joachims, 2006) on a manifold of features, including distributed representations and semantic resources. To our knowledge, the only work exploring text selection for improving cQA or QA systems is (Barr´on-Cede˜no 1735 Original Question qo : What are the tourist places in Qatar? I’m likely to travel in th"
C16-1163,S16-1172,1,0.882623,"uestions. Various methods rely on machine-translation models. For instance, Jeon et al. (2005) and Zhou et al. (2011) used monolingual phrase-based translation models to compare the questions. Jeon et al. (2005) built their translator from a collection of previously identified similar questions whereas Zhou et al. (2011) used question–answer pairs. Other approaches are based on syntactic representations. This is the case of Wang et al. (2009), who consider the number of common substructures of parse trees to estimate the similarity between two questions. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use parse trees as well. The difference is that they use them directly within a tree kernel, with the use of the KeLP platform (Filice et al., 2015a). The latter two models were applied on the SemEval 2016 Task 3 challenge on cQA (Nakov et al., 2016), which proposed a task on question ranking (together with one on answer ranking). The best-performing system in this task was the one from Franco-Salvador et al. (2016), which used SVMrank (Joachims, 2006) on a manifold of features, including distributed representations and semantic resources. To our knowledge, the only work exploring text select"
C16-1163,S16-1126,0,0.0424839,"s is the case of Wang et al. (2009), who consider the number of common substructures of parse trees to estimate the similarity between two questions. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use parse trees as well. The difference is that they use them directly within a tree kernel, with the use of the KeLP platform (Filice et al., 2015a). The latter two models were applied on the SemEval 2016 Task 3 challenge on cQA (Nakov et al., 2016), which proposed a task on question ranking (together with one on answer ranking). The best-performing system in this task was the one from Franco-Salvador et al. (2016), which used SVMrank (Joachims, 2006) on a manifold of features, including distributed representations and semantic resources. To our knowledge, the only work exploring text selection for improving cQA or QA systems is (Barr´on-Cede˜no 1735 Original Question qo : What are the tourist places in Qatar? I’m likely to travel in the month of June. Just wanna know some good places to visit. G GS R Retrieved Questions 1 -1 8 The Qatar banana island will be transfered by the end of 2013 to 5 stars resort called Anantara. Has anyone seen this island? Where is it? Is it near to Corniche? 2 +1 2 Is there"
C16-1163,W01-0515,0,0.030947,"gure 1: Representation of two questions as syntactic trees. Related nodes are enriched with REL links. 3.4 Feature Vectors We combine the kernel above with an RBF kernel applied to feature vectors composed of similarity features. These are computed between the original and the related question and the Google rank. Such text similarity features (sim) are 20 similarities sim(qo , qs ) using word n-grams (n = [1, . . . , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also add a structural similarity obtained by comparing the syntactic trees of the questions of an example pair using the partial tree kernel, i.e., T K(t(qo , qs ), t(qs , qo )). Note that the operands of the kernel function are members of the same pair. The ranking-based feature (rank) is computed using the ranking generated by the baseline Google search engine system. Each candidate question is located in one position in the range [1, . . . , 10]. We exploit this information as the inverse of the position. 4 Long Short-Term Memory Networks for TK-based Reranking A"
C16-1163,S15-2047,1,0.587169,"vance (GS) and our rank (R) for each question. et al., 2016), which exploits tree kernel function itself to auto-filter the non relevant subtrees. The main difference with the approach we present in the current paper is the use of neural networks for learning attention weights and thus modeling sentence or word pruning. Neural Approaches Recent work has shown the effectiveness of neural models for answer selection (Severyn and Moschitti, 2015; Tan et al., 2015; Feng et al., 2015) and question similarity (dos Santos et al., 2015) in community question answering. For instance, dos Santos et al. (2015) used CNN and bag-of-words (BOW) representations of original and related questions in order to compute cosine similarity scores. Recently, Bahdanau et al. (2014) presented a neural attention model for machine translation and showed that the attention mechanism is helpful for addressing long sentences. We use an LSTM model (Hochreiter and Schmidhuber, 1997) with an attention mechanism for capturing long dependencies in questions for the question similarity task. The major difference with previous work is that we exploit the weights learned by the attention model for selecting important text seg"
C16-1163,N16-1152,1,0.849591,"in Section 2, several neural approaches have been successfully applied to QA tasks. Unfortunately, question retrieval in cQA is heavily affected by a large amount of noise and a rather different domain, which make it difficult to effectively use out-of-domain embeddings to pre-train neural networks. This probably prevented the participants to SemEval tasks from achieving satisfactory results with such models (Nakov et al., 2016). In this work, we also tried to exploit neural models using their top-level representations for the (qo , qs ) pair and fed them into the TK classifier as proposed by Tymoshenko et al. (2016), but this simple combination proved to be ineffective as well. In contrast, neural embeddings and weights can be useful for selecting better representations for TK models. In the reminder of this section, we present LSTM networks for question retrieval and our approach for incorporating them into TK-based rerankers. We approach question ranking as a classification task: given a pair (qo , qs ), we need to classify qs as relevant or irrelevant. In order to evaluate the neural classifiers on our ranking task, we can rank candidates, qs , according to their posterior probability. Among the diffe"
C16-1163,P06-1051,1,0.693051,"ining examples, αi are weights, yi are the example labels, φ(qoi , qsi ) is the representation of pairs of the original and candidate questions. This leads to the following scoring function: r(qo , qs ) = n X αi yi φ(qo , qs ) · φ(qoi , qsi ) = i=1 n X  αi yi K hqo , qs i, hqoi , qsi i , i=1 where the kernel, K(·, ·), intends to capture the similarity between pairs of objects constituted by the original and retrieved questions. The definition of effective Ks for QA and other relational learning tasks, e.g., textual entailment and paraphrasing, has been studied in a large body of work, e.g., (Zanzotto and Moschitti, 2006; Filice et al., 2015b). Given the high similarity between question ranking in cQA and passage ranking in QA, we opted for the state-of-the-art model proposed by Severyn and Moschitti (2012). It should be noted that we apply TK models to pairs of questions rather than questions with their passages. Figure 1 displays an example of the structure we used for representing the original question, qo and the seventh candidate question, qs , in Table 1. The graph is composed by two macro-trees, one for each question, which in turn are constituted by the syntactic trees of the sentences composing the t"
C16-1163,P11-1066,0,0.015238,"have been explored. For instance, Cao et al. (2008) proposed a question recommendation system based on the questions’ topic and Duan et al. (2008) added the question’s focus into the formula. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). Here, the authors use LDA topic modeling to learn the latent semantic topics that generate question/answer pairs and use the learned topic distribution to retrieve similar historical questions. Various methods rely on machine-translation models. For instance, Jeon et al. (2005) and Zhou et al. (2011) used monolingual phrase-based translation models to compare the questions. Jeon et al. (2005) built their translator from a collection of previously identified similar questions whereas Zhou et al. (2011) used question–answer pairs. Other approaches are based on syntactic representations. This is the case of Wang et al. (2009), who consider the number of common substructures of parse trees to estimate the similarity between two questions. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use parse trees as well. The difference is that they use them directly within a tree kernel, wit"
C16-1163,S16-1138,1,\N,Missing
C16-1237,S16-1138,1,0.881056,"Missing"
C16-1237,P15-2114,0,0.171147,"09) computed their similarity function on the syntactic-tree representations of the questions. The more substructures the trees have in common, the more similar their associated questions are. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). Here, the authors use LDA topic modeling to learn the latent semantic topics that generate question/answer pairs and use the learned topics distribution to retrieve similar historical questions. The recent boom in neural network approaches has also impacted question retrieval. dos Santos et al. (2015) applied convolutional neural networks to retrieve semantically-equivalent questions’ subjects. When dealing with whole questions —subject and (generally long) body—, they had to aggregate a bag-of-words neural network to boost the model’s performance. They suggested that the performance of state-of-the-art models for semantic paraphrasing assessment on short texts (e.g., (Filice et al., 2015b)) cannot be applied straightforwardly to whole questions in cQA. For the first time, we address such problem in this paper. The two editions of the SemEval Task 3 on cQA (Nakov et al., 2015; Nakov et al."
C16-1237,P08-1019,0,0.125266,"r model is based on textual content only. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Question ranking can be approached from different fronts. Cao et al. (2008) approached it as a recommendation task: given a query question, recommend questions that could be interesting or relevant, regardless of whether they convey the same information request. They tackle this problem by comparing representations based on topic terms graphs; i.e., by judging topic similarity. In a follow up paper, Duan et al. (2008) searched for equivalent questions by considering the question’s focus as well. Zhou et al. (2011) dodged the lexical gap between two questions by assessing their similarity on the basis of a (monolingual) phrase-based translation model (Koehn et al., 2003). They considered the (pre-filtered) contents of the question–answer pairs as their “parallel” corpus to learn the translation model from. Jeon et al. (2005b) had used monolingual translation as well. Given a large repository of question and answer threads, they looked for highly-similar threads (Jeon et al., 2005a). Similar answers are like"
C16-1237,P15-1097,1,0.531448,"hat generate question/answer pairs and use the learned topics distribution to retrieve similar historical questions. The recent boom in neural network approaches has also impacted question retrieval. dos Santos et al. (2015) applied convolutional neural networks to retrieve semantically-equivalent questions’ subjects. When dealing with whole questions —subject and (generally long) body—, they had to aggregate a bag-of-words neural network to boost the model’s performance. They suggested that the performance of state-of-the-art models for semantic paraphrasing assessment on short texts (e.g., (Filice et al., 2015b)) cannot be applied straightforwardly to whole questions in cQA. For the first time, we address such problem in this paper. The two editions of the SemEval Task 3 on cQA (Nakov et al., 2015; Nakov et al., 2016) have triggered a manifold of approaches. The datasets they released include manual crowd-sourced annotation rather than forum-inferred judgments. The 2015 edition focused on answer retrieval. The first performing system (Tran et al., 2015) applied machine translation in a similar fashion as Jeon et al. (2005b) and Zhou et al. (2011), together with topic models, embeddings, and similar"
C16-1237,S16-1172,1,0.800539,"fashion as Jeon et al. (2005b) and Zhou et al. (2011), together with topic models, embeddings, and similarities. Both the first and the second runners (Hou et al., 2015; Nicosia et al., 2015) applied supervised models with lexical, syntactic and meta-data features. The 2016 edition included a question retrieval challenge as well. We take ad2516 vantage of the evaluation framework developed for this task. The top-three participants opted for SVMs as learning models. The top-ranked (Franco-Salvador et al., 2016) used SVMrank (Joachims, 2006), the first (Barr´on-Cede˜no et al., 2016) and second (Filice et al., 2016) runners up used KeLP (Filice et al., 2015a) to combine various kernels. Another difference between these models is in the amount of knowledge they use. Franco-Salvador et al. (2016) rely heavily on distributed representations and semantic information sources, such as Babelnet and Framenet. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use lexical similarities and tree kernels on parse trees. No statistically-significant differences were observed in the performance of these three systems. To the best of our knowledge, so far the only other work exploring text selection to improve"
C16-1237,S16-1126,0,0.451216,"sed on answer retrieval. The first performing system (Tran et al., 2015) applied machine translation in a similar fashion as Jeon et al. (2005b) and Zhou et al. (2011), together with topic models, embeddings, and similarities. Both the first and the second runners (Hou et al., 2015; Nicosia et al., 2015) applied supervised models with lexical, syntactic and meta-data features. The 2016 edition included a question retrieval challenge as well. We take ad2516 vantage of the evaluation framework developed for this task. The top-three participants opted for SVMs as learning models. The top-ranked (Franco-Salvador et al., 2016) used SVMrank (Joachims, 2006), the first (Barr´on-Cede˜no et al., 2016) and second (Filice et al., 2016) runners up used KeLP (Filice et al., 2015a) to combine various kernels. Another difference between these models is in the amount of knowledge they use. Franco-Salvador et al. (2016) rely heavily on distributed representations and semantic information sources, such as Babelnet and Framenet. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use lexical similarities and tree kernels on parse trees. No statistically-significant differences were observed in the performance of these th"
C16-1237,S15-2035,0,0.0138226,"le questions in cQA. For the first time, we address such problem in this paper. The two editions of the SemEval Task 3 on cQA (Nakov et al., 2015; Nakov et al., 2016) have triggered a manifold of approaches. The datasets they released include manual crowd-sourced annotation rather than forum-inferred judgments. The 2015 edition focused on answer retrieval. The first performing system (Tran et al., 2015) applied machine translation in a similar fashion as Jeon et al. (2005b) and Zhou et al. (2011), together with topic models, embeddings, and similarities. Both the first and the second runners (Hou et al., 2015; Nicosia et al., 2015) applied supervised models with lexical, syntactic and meta-data features. The 2016 edition included a question retrieval challenge as well. We take ad2516 vantage of the evaluation framework developed for this task. The top-three participants opted for SVMs as learning models. The top-ranked (Franco-Salvador et al., 2016) used SVMrank (Joachims, 2006), the first (Barr´on-Cede˜no et al., 2016) and second (Filice et al., 2016) runners up used KeLP (Filice et al., 2015a) to combine various kernels. Another difference between these models is in the amount of knowledge they"
C16-1237,D15-1068,1,0.877252,"Missing"
C16-1237,N03-1017,0,0.0194065,"ao et al. (2008) approached it as a recommendation task: given a query question, recommend questions that could be interesting or relevant, regardless of whether they convey the same information request. They tackle this problem by comparing representations based on topic terms graphs; i.e., by judging topic similarity. In a follow up paper, Duan et al. (2008) searched for equivalent questions by considering the question’s focus as well. Zhou et al. (2011) dodged the lexical gap between two questions by assessing their similarity on the basis of a (monolingual) phrase-based translation model (Koehn et al., 2003). They considered the (pre-filtered) contents of the question–answer pairs as their “parallel” corpus to learn the translation model from. Jeon et al. (2005b) had used monolingual translation as well. Given a large repository of question and answer threads, they looked for highly-similar threads (Jeon et al., 2005a). Similar answers are likely to address similar questions! The questions in the so-generated pairs compose their “parallel” corpus. Wang et al. (2009) computed their similarity function on the syntactic-tree representations of the questions. The more substructures the trees have in"
C16-1237,W01-0515,0,0.484216,"Missing"
C16-1237,E06-1015,1,0.568992,"ns on top of all the Irrelevant ones, regardless of the order within both subsets (Cao et al., 2008; dos Santos et al., 2015; Jeon et al., 2005b). We adopt the same architecture as the recently-proposed, most successful, models on this kind of setting (Franco-Salvador et al., 2016; Barr´on-Cede˜no et al., 2016; Filice et al., 2016). We apply a kernel approach to solve a binary classification problem, f : Q × D → {Relevant, Irrelevant}, and sort the forum questions d ∈ D according to their classification score against q: f (q, d). We opt for a tree kernel applied to parse-tree representations (Moschitti, 2006b; Sun et al., 2011), as it performs well in ranking both passages (Severyn and Moschitti, 2012) and questions (Barr´on-Cede˜no et al., 2016; Da San Martino et al., 2016; Filice et al., 2016). Additionally, the nodes of the parse trees of the pairs (q, d) are marked with a REL tag when there is at least a lexical match between the phrases of the questions (c.f. (Filice et al., 2015b) for details). The approach for dealing with a pair of trees, is to compose kernels on single trees K T (x1 , x2 ): K((qi , di ), (qj , dj )) = K T (t(qi , di )), t(qj , dj ))) + K T (t(di , qi )), t(dj , qj ))), ("
C16-1237,S15-2047,1,0.911316,"Missing"
C16-1237,S15-2036,1,0.865019,"Missing"
C16-1237,C16-1163,1,0.744397,"Missing"
C16-1237,P08-1082,0,0.0407781,"2 Related Work Community question answering poses various challenges: answer and question ranking, and question de-duplication are three examples. We now review the related literature with focus on question ranking. One of the first approaches to answer ranking relied completely on the website’s metadata (Jeon et al., 2006), such as an author’s reputation and click counts. Agichtein et al. (2008) explored a graphbased model of contributors relationships together with both content- and usage-based features. These approaches depend heavily on the forum’s meta-data and social features. Still, as Surdeanu et al. (2008) stress, relying on this kind of data causes the model portability to be difficult; a drawback that disappears when focusing on the content of the questions and answers only. Therefore, our model is based on textual content only. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Question ranking can be approached from different fronts. Cao et al. (2008) approached it as a recommendation task: given a query question, recommend questions that could be interesting or relevant, regardless of wh"
C16-1237,S15-2038,0,0.0124471,"he model’s performance. They suggested that the performance of state-of-the-art models for semantic paraphrasing assessment on short texts (e.g., (Filice et al., 2015b)) cannot be applied straightforwardly to whole questions in cQA. For the first time, we address such problem in this paper. The two editions of the SemEval Task 3 on cQA (Nakov et al., 2015; Nakov et al., 2016) have triggered a manifold of approaches. The datasets they released include manual crowd-sourced annotation rather than forum-inferred judgments. The 2015 edition focused on answer retrieval. The first performing system (Tran et al., 2015) applied machine translation in a similar fashion as Jeon et al. (2005b) and Zhou et al. (2011), together with topic models, embeddings, and similarities. Both the first and the second runners (Hou et al., 2015; Nicosia et al., 2015) applied supervised models with lexical, syntactic and meta-data features. The 2016 edition included a question retrieval challenge as well. We take ad2516 vantage of the evaluation framework developed for this task. The top-three participants opted for SVMs as learning models. The top-ranked (Franco-Salvador et al., 2016) used SVMrank (Joachims, 2006), the first ("
C16-1237,P11-1066,0,0.485679,"le threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Question ranking can be approached from different fronts. Cao et al. (2008) approached it as a recommendation task: given a query question, recommend questions that could be interesting or relevant, regardless of whether they convey the same information request. They tackle this problem by comparing representations based on topic terms graphs; i.e., by judging topic similarity. In a follow up paper, Duan et al. (2008) searched for equivalent questions by considering the question’s focus as well. Zhou et al. (2011) dodged the lexical gap between two questions by assessing their similarity on the basis of a (monolingual) phrase-based translation model (Koehn et al., 2003). They considered the (pre-filtered) contents of the question–answer pairs as their “parallel” corpus to learn the translation model from. Jeon et al. (2005b) had used monolingual translation as well. Given a large repository of question and answer threads, they looked for highly-similar threads (Jeon et al., 2005a). Similar answers are likely to address similar questions! The questions in the so-generated pairs compose their “parallel”"
C16-1237,P15-1025,0,0.0200524,"utation and click counts. Agichtein et al. (2008) explored a graphbased model of contributors relationships together with both content- and usage-based features. These approaches depend heavily on the forum’s meta-data and social features. Still, as Surdeanu et al. (2008) stress, relying on this kind of data causes the model portability to be difficult; a drawback that disappears when focusing on the content of the questions and answers only. Therefore, our model is based on textual content only. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Question ranking can be approached from different fronts. Cao et al. (2008) approached it as a recommendation task: given a query question, recommend questions that could be interesting or relevant, regardless of whether they convey the same information request. They tackle this problem by comparing representations based on topic terms graphs; i.e., by judging topic similarity. In a follow up paper, Duan et al. (2008) searched for equivalent questions by considering the question’s focus as well. Zhou et al. (2011) dodged the lexical gap between two questi"
C16-2001,P15-2113,1,0.887768,"Missing"
C16-2001,S16-1138,1,0.853194,"Missing"
C16-2001,D15-1068,1,0.89046,"Missing"
C16-2001,P03-1054,0,0.0264421,"larity value using a similarity matrix. The similarity and the embeddings along with other additional similarity features are then passed through a hidden layer and next to the output layer for classification. The qe and ce are learned by backpropagating the (cross entropy) errors from the output layer. qe and ce vectors are finally concatenated and used as features in our SVM model. Tree kernels We use tree kernels to measure the syntactic similarity between the question and the comment. First, we produce shallow syntactic trees for the question and for the comment using the Stanford parser (Klein and Manning, 2003). Following Severyn and Moschitti (2012), we link the two trees by connecting nodes such as NP, PP, VP, when there is at least one lexical overlap between the corresponding phrases of the trees, and we mark those links using a specific tag. The kernel function K is defined as: K((t1 , t2 ), (c1 , c2 )) = T K(t1 , c1 )+T K(t2 , c2 ), where T K(t, c) is a tree kernel function operating over a pair of question (t) and comment (c) trees.3 Classification Performance We evaluated our comment classifier on the SemEval-2016 Task 3 test set with the official scorer, obtaining the following results: MAP"
C16-2001,S15-2047,1,0.903863,"Missing"
C16-2001,S15-2036,1,0.910076,"Missing"
D15-1068,S15-2047,1,0.877035,"Missing"
D15-1068,S12-1059,0,0.0177823,"Missing"
D15-1068,S15-2036,1,0.541482,"Missing"
D15-1068,P15-2113,1,0.19975,"Missing"
D15-1068,P04-1035,0,0.00602109,"og sij , etc. The goal of the ILP problem is to find an assignment A to all variables xiG , xiB , xijS , xijD that minimizes the cost function: Graph Partition Approach Here our goal is to find a partition P = (G, B) that minimizes the following cost: hX i X X C(P ) = λ siB + siG + (1 − λ) sij ci ∈G ci ∈B ci ∈G,cj ∈B The first part of the cost function discourages misclassification of individual comments, while the second part encourages similar comments to be in the same class. The mixing parameter λ ∈ [0, 1] determines the relative strength of the two components. Our approach is inspired by Pang and Lee (2004), where they model the proximity relation between sentences for finding subjective sentences in product reviews, whereas we are interested in global inference based on local classifiers. The optimization problem can be efficiently solved by finding a minimum cut of a weighted undirected graph G = (V, E). The set of nodes V = {v1 , v2 , · · · , vn , s, t} represent the n comments in a thread, the source and the sink. We connect each comment node vi to the source node s by adding an edge w(vi , s) with capacity siG , and to the sink node t by adding an edge w(vi , t) with capacity siB . Finally,"
D15-1068,I11-1164,0,0.0214203,"ssification in Community Question Answering ˜ Giovanni Da San Martino, Simone Filice, Shafiq Joty, Alberto Barr´on-Cedeno, Llu´ıs M`arquez, Alessandro Moschitti, and Preslav Nakov, Qatar Computing Research Institute, HBKU {sjoty,albarron,gmartino,sfilice, lmarquez,amoschitti,pnakov}@qf.org.qa Abstract As question-comment threads can get quite long, finding good answers in a thread can be timeconsuming. This has triggered research in trying to automatically determine which answers might be good and which ones are likely to be bad or irrelevant. One early work going in this direction is that of Qu and Liu (2011), who tried to determine whether a question is “solved” or not, given its associated thread of comments. As a first step in the process, they performed a comment-level classification, considering four classes: problem, solution, good feedback, and bad feedback. More recently, the shared task at SemEval 2015 on Answer Selection in CQA (Nakov et al., 2015), whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread. In that task, the top participating systems used threadlevel features, in addition to the usual local feat"
D15-1068,W04-2401,0,0.398636,"he CQA-QL dataset: after merging Bad and Potential into Bad. The Task 1 Train 2,600 16,541 8,069 8,472 3 http://www.qatarliving.com/moving-qatar/posts/can-iobtain-driving-license-my-qid-written-employee http://www.qatarliving.com/forum http://alt.qcri.org/semeval2015/task3/ 574 3.1 ci and cj have the same label; assigning 0 to xijS means that ci and cj do not have the same label. The same interpretation holds for the other possible classes (in this case only Different).4 Let ciG be the cost of classifying ci as Good, cijS be the cost of assigning the same labels to ci and cj , etc. Following (Roth and Yih, 2004), these costs are obtained from local classifiers by taking log probabilities, i.e., ciG = − log siG , cijS = − log sij , etc. The goal of the ILP problem is to find an assignment A to all variables xiG , xiB , xijS , xijD that minimizes the cost function: Graph Partition Approach Here our goal is to find a partition P = (G, B) that minimizes the following cost: hX i X X C(P ) = λ siB + siG + (1 − λ) sij ci ∈G ci ∈B ci ∈G,cj ∈B The first part of the cost function discourages misclassification of individual comments, while the second part encourages similar comments to be in the same class. The"
D15-1068,S15-2035,0,0.084801,"ment-level classification, considering four classes: problem, solution, good feedback, and bad feedback. More recently, the shared task at SemEval 2015 on Answer Selection in CQA (Nakov et al., 2015), whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread. In that task, the top participating systems used threadlevel features, in addition to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in"
D15-1068,W03-0402,0,0.128215,"Missing"
D15-1068,P15-2117,0,0.0999102,"on to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). Community question answering, a recent evolution of question answering in the Web context, allows a user to"
D15-1068,W01-0515,0,0.0287716,"he comment-pair variables are consistent: xijD = xiG ⊕ xjG , ∀i, j 1 ≤ i < j ≤ n. λ ∈ [0, 1] is a parameter used to balance the contribution of the two sources of information. 4 Local Classifiers For classification, we use Maximum Entropy, or MaxEnt, (Murphy, 2012), as it yields a probability distribution over the class labels, which we then use directly for the graph arcs and the ILP costs. 4.1 Good-vs-Bad Classifier Our most important features measure the similarity between the question (q) and the comment (c). We compare lemmata and POS [1, 2, 3, 4]-grams using Jaccard (1901), containment (Lyon et al., 2001), and cosine, as well as using some similarities from DKPro (B¨ar et al., 2012) such as longest common substring (Allison and Dix, 1986) and greedy string tiling (Wise, 1996). We also compute similarity using partial tree kernels (Moschitti, 2006) on shallow syntactic trees. Forty-three Boolean features express whether (i) c includes URLs or emails, the words “yes”, “sure”, “no”, “neither”, “okay”, etc., as well as ‘?’ and ‘@’ or starts with “yes” (12 features); (ii) c includes a word longer than fifteen characters (1); Integer Linear Programming Approach Here we follow the inference with clas"
D15-1068,S15-2037,0,0.0777175,"on to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). Community question answering, a recent evolution of question answering in the Web context, allows a user to"
D15-1068,D07-1002,0,\N,Missing
D15-1068,N10-1145,0,\N,Missing
D15-1068,P07-1098,1,\N,Missing
D15-1068,C10-1131,0,\N,Missing
D15-1068,N13-1106,0,\N,Missing
D15-1068,S15-2038,0,\N,Missing
D15-1068,P08-1082,0,\N,Missing
D15-1068,W13-3509,1,\N,Missing
D15-1068,D13-1044,1,\N,Missing
D19-1565,N19-1423,0,0.340062,"T2 TN 1 We define two tasks based on the corpus described in Section 3: (i) SLC (Sentence-level Classification), which asks to predict whether a sentence contains at least one propaganda technique, and (ii) FLC (Fragment-level classification), which asks to identify both the spans and the type of propaganda technique. Note that these two tasks are of different granularities, g1 and g2 , i.e., tokens for FLC and sentences for SLC. We split the corpus into training, development and test, each containing 293, 57, 101 articles and 14,857, 2,108, 4,265 sentences. 5.1 Baselines We depart from BERT (Devlin et al., 2019), as it has achieved state-of-the-art performance on multiple NLP benchmarks, and we design three baselines based on it. BERT. We add a linear layer on top of BERT and we fine-tune it, as suggested in (Devlin et al., 2019). For the FLC task, we feed the final hidden representation for each token to a layer Lg2 that makes a 19-way classification: does this token belong to one of the eighteen propaganda techniques or to none of them (cf. Figure 3-a). For the SLC task, we feed the final hidden representation for the special [CLS] token, which BERT uses to represent the full sentence, to a two-dim"
D19-1565,D17-2002,0,0.111551,". Barr´on-Cede˜no et al. (2019) experimented with a binarized version of the corpus from (Rashkin et al., 2017): propaganda vs. the other three categories. The corpus labels were obtained with distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with cases of ad hominem fallacy identified. Habernal et al. (2017, 2018a) introduced Argotario, a game to educate people to recognize and create fallacies. A byproduct of Argotario is a corpus with 1.3k arguments annotated with five fallacies, including ad hominem, red herring and irrelevant authority, which directly relate to propaganda techniques (cf. Section 2). Differently from (Habernal et al., 2017, 2018a,b), our corpus has 18 techniques annotated on the same set of news articles. Moreover, our annotations aim at identifying the minimal fragments related to a technique instead of flagging entire arguments. Conclusion and Future Work We have argued for"
D19-1565,L18-1526,0,0.0643642,"propaganda, trusted, hoax, or satire. They included articles from eight sources, two of which are propagandistic. Barr´on-Cede˜no et al. (2019) experimented with a binarized version of the corpus from (Rashkin et al., 2017): propaganda vs. the other three categories. The corpus labels were obtained with distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with cases of ad hominem fallacy identified. Habernal et al. (2017, 2018a) introduced Argotario, a game to educate people to recognize and create fallacies. A byproduct of Argotario is a corpus with 1.3k arguments annotated with five fallacies, including ad hominem, red herring and irrelevant authority, which directly relate to propaganda techniques (cf. Section 2). Differently from (Habernal et al., 2017, 2018a,b), our corpus has 18 techniques annotated on the same set of news articles. Moreover, our annotations aim at identifying the minimal fra"
D19-1565,N18-1036,0,0.0307582,"propaganda, trusted, hoax, or satire. They included articles from eight sources, two of which are propagandistic. Barr´on-Cede˜no et al. (2019) experimented with a binarized version of the corpus from (Rashkin et al., 2017): propaganda vs. the other three categories. The corpus labels were obtained with distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with cases of ad hominem fallacy identified. Habernal et al. (2017, 2018a) introduced Argotario, a game to educate people to recognize and create fallacies. A byproduct of Argotario is a corpus with 1.3k arguments annotated with five fallacies, including ad hominem, red herring and irrelevant authority, which directly relate to propaganda techniques (cf. Section 2). Differently from (Habernal et al., 2017, 2018a,b), our corpus has 18 techniques annotated on the same set of news articles. Moreover, our annotations aim at identifying the minimal fra"
D19-1565,J15-3003,0,0.026974,"Missing"
D19-1565,C14-2023,0,0.0434456,"Missing"
D19-1565,C10-2115,1,0.852121,"Missing"
D19-1565,D17-1317,0,0.252149,". To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at the fragment level with eighteen propaganda techniques and we propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines. 1 Introduction Research on detecting propaganda has focused primarily on articles (Barr´on-Cedeno et al., 2019; Rashkin et al., 2017). In many cases, there are no labeled data for individual articles, but there are such labels for entire news outlets. Thus, often all articles from the same news outlet get labeled the way that this outlet is labeled. Yet, it has been observed that propagandistic sources could post objective non-propagandistic articles periodically to increase their credibility (Horne et al., 2018). Similarly, media generally recognized as objective might occasionally post articles that promote a particular editorial agenda and are thus propagandistic. Thus, it is clear that transferring the label of the news"
D19-1565,W02-2024,0,0.298611,"Missing"
D19-1565,W03-0419,0,0.400775,"Missing"
D19-3038,P15-2072,0,0.0737003,"Missing"
D19-3038,D19-1565,1,0.834147,"Missing"
D19-3038,E17-3016,1,0.8158,"ehow (0.4 ≤ p &lt; 0.6), likely (0.6 ≤ p &lt; 0.8), and very likely (p ≥ 0.8). Crawlers and Translation Our crawlers collect articles from a growing list of sources10 , which currently includes 155 RSS feeds, 82 Twitter accounts and two websites. Once a link to an article has been obtained from any of these sources, we rely on the Newspaper3k Python library to extract its contents.11 After deduplication based on both URL and text content, our crawlers currently download 7k-10k articles per day. As of present, we have more than 700k articles stored in our database. We use QCRI’s Machine Translation (Dalvi et al., 2017) to translate English content into Arabic and vice versa. Since translation is performed offline, we select the most accurate system in Dalvi et al. (2017), i.e., the neural-based one. 8 http://kafka.apache.org http://kubernetes.io 10 http://www.tanbih.org/about 11 http://newspaper.readthedocs.io 9 12 http://github.com/several27/ FakeNewsCorpus 224 2.4 Framing Bias Detection We implemented our stance detection model as fine-tuning of BERT on the FNC-1 dataset from the Fake News Challenge13 . Our model outperformed the best submitted system (Hanselowski et al., 2018), obtaining an F1macro of 75"
D19-3038,C18-1158,0,0.133435,"Missing"
D19-3038,D18-1389,1,0.607642,"Missing"
D19-3038,N18-5006,1,0.812156,"Missing"
D19-3038,N19-1216,1,0.55511,"Missing"
D19-3038,P17-1092,0,0.0313814,"Missing"
D19-3038,D18-1483,0,0.0187346,"11): V (u) = tf (u,C0 ) total(C0 ) 2 tf (u,C ) tf (u,C1 ) 0 total(C0 ) + total(C1 ) −1 (1) Figure 2: The Tanbih main page. where tf (u, C0 ) is the number of times (term frequency) item u is cited by group C0 , and total(C0 ) is the sum of the term frequencies of all items cited by C0 . tf (u, C1 ) and total(C1 ) are defined in a similar fashion. We subdivided the range between -1 and 1 into 5 equal size ranges and we assigned the labels far-left, left, center, right, and far-right to those ranges. 2.9 The model achieved state-of-the-art performance on the testing partition of the corpus from Miranda et al. (2018): an F1 of 98.11 and an F1BCubed of 94.41.15 As a comparison, the best model described in (Miranda et al., 2018) achieved an F1 of 94.1. See Staykovski et al. (2019) for further details. Event Identification / Clustering 3 The clustering module aggregates news articles into stories. The pipeline is divided into two stages: (i) local topic identification and (ii) longterm topic matching for story generation. For step (i), we represent each article as a TF.IDF vector, built from the title and the body concatenated. The pre-processing consists of casefolding, lemmatization, punctuation and stopwo"
D19-5024,D19-5021,0,0.0556224,"Char. Emb.           Features Unsup. Tuning    Table 2: Overview of the approaches for the fragment-level classification task. Team NSIT CUNLP JUSTDeep Tha3aroon LIACC MIC-CIS CAUnLP YMJA jinfen ProperGander BERT LSTM      logreg USE CNN Embeddings Features          Context             Table 3: Overview of the approaches used for the sentence-level classification task. 9.2 Team Tha3aroon (Fadel and Al-Ayyoub, 2019) implemented an ensemble of three classifiers: two based on BERT and one based on a universal sentence encoder (Cer et al., 2018). Team NSIT (Aggarwal and Sadana, 2019) explored three of the most popular transfer learning models: various versions of ELMo, BERT, and RoBERTa (Liu et al., 2019). Team Mindcoders (Vlad et al., 2019) combined BERT, Bi-LSTM and Capsule networks (Sabour et al., 2017) into a single deep neural network and pre-trained the resulting network on corpora used for related tasks, e.g., emotion classification. Finally, team ltuorp (Mapes et al., 2019) used an attention transformer using BERT trained on Wikipedia and BookCorpus. Teams Participating in the Sentence-Level Classification Only Team CAUnLP (Hou and Chen, 2019) used two context-awa"
D19-5024,D19-5016,0,0.0895208,"evel Classification Only Team CAUnLP (Hou and Chen, 2019) used two context-aware representations based on BERT. In the first representation, the target sentence is followed by the title of the article. In the second representation, the previous sentence is also added. They performed subsampling in order to deal with class imbalance, and experimented with BERTBASE and BERTLARGE Team LIACC (Ferreira Cruz et al., 2019) used hand-crafted features and pre-trained ELMo embeddings. They also observed a boost in performance when balancing the dataset by dropping some negative examples. Team JUSTDeep (Al-Omari et al., 2019) used a combination of models and features, including word embeddings based on GloVe (Pennington et al., 2014) concatenated with vectors representing affection and lexical features. These were combined in an ensemble of supervised models: bi-LSTM, XGBoost, and variations of BERT. Team YMJA (Hua, 2019) also based their approach on fine-tuned BERT. Inspired by kaggle competitions on sentiment analysis, they created an ensemble of models via cross-validation. Team jinfen (Li et al., 2019) used a logistic regression model fed with a manifold of representations, including TF.IDF and BERT vectors, a"
D19-5024,Q17-1010,0,0.00764997,"on fine-tuned BERT. Inspired by kaggle competitions on sentiment analysis, they created an ensemble of models via cross-validation. Team jinfen (Li et al., 2019) used a logistic regression model fed with a manifold of representations, including TF.IDF and BERT vectors, as well as vocabularies and readability measures. 9.3 Teams Participating in Both Tasks Team MIC-CIS (Gupta et al., 2019) participated in both tasks. For the sentence-level classification, they used a voting ensemble including logistic regression, convolutional neural networks, and BERT, in all cases using FastText embeddings (Bojanowski et al., 2017) and pre-trained BERT models. Beside these representations, multiple features of readability, sentiment and emotions were considered. For the fragment-level task, they used a multi-task neural sequence tagger, based on LSTM-CRF (Huang et al., 2015), in conjunction with linguistic features. Finally, they applied sentence- and fragment-level models jointly. 166 SLC Task: Test Set (Official Results) Rank Team F1 Precision Recall 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ltuorp ProperGander YMJA MIC-CIS CUNLP Tha3aroon JUSTDeep CAUnLP LIPN LIACC aschern MindCoders jinfen"
D19-5024,D19-5012,0,0.155995,"523 0.7212 0.6860 0.6197 0.7902 0.6684 0.6075 0.7280 0.6603 0.5155 0.4749 0.4695 0.4627 0.5074 0.5074 0.5074 0.3518 0.2286 Table 5: Results for the SLC task on the development set at the end of phase 1 (see Section 6). Team CUNLP (Alhindi et al., 2019) considered two approaches for the sentence-level task. The first approach was based on fine-tuning BERT. The second approach complemented the fine-tuned BERT approach by feeding its decision into a logistic regressor, together with features from the Linguistic Inquiry and Word Count (LIWC)2 lexicon and punctuation-derived features. Similarly to Gupta et al. (2019), for the fragment-level problem they used a Bi-LSTM-CRF architecture, combining both character- and word-level embeddings. Team ProperGander (Madabushi et al., 2019) also used BERT, but they paid special attention to the imbalance of the data, as well as to the differences between training and testing. They showed that augmenting the training data by oversampling yielded improvements when testing on data that is temporally far from the training (by increasing recall). In order to deal with the imbalance, they performed cost-sensitive classification, i.e., the errors on the smaller positive cl"
D19-5024,D17-2002,0,0.139704,") annotated a large binary corpus of propagandist vs. non-propagandist articles and proposed a feature-based system for discriminating between them. In all these cases, the labels were obtained using distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with instances of ad hominem fallacy. Habernal et al. (2017, 2018a) introduced Argotario, a game to educate people to recognize and create fallacies, a by-product of which is a corpus with 1.3k arguments annotated with five fallacies such as ad hominem, red herring and irrelevant authority, which directly relate to propaganda. Introduction Propaganda aims at influencing people’s mindset with the purpose of advancing a specific agenda. In the Internet era, thanks to the mechanism of sharing in social networks, propaganda campaigns have the potential of reaching very large audiences (Glowacki et al., 2018; Muller, 2018; Tard´aguila et al., 2018). Propag"
D19-5024,D19-1565,1,0.818121,"Missing"
D19-5024,L18-1526,0,0.0144225,"a binarized version of that corpus: propaganda vs. the other three categories. Barr´on-Cedeno et al. (2019) annotated a large binary corpus of propagandist vs. non-propagandist articles and proposed a feature-based system for discriminating between them. In all these cases, the labels were obtained using distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with instances of ad hominem fallacy. Habernal et al. (2017, 2018a) introduced Argotario, a game to educate people to recognize and create fallacies, a by-product of which is a corpus with 1.3k arguments annotated with five fallacies such as ad hominem, red herring and irrelevant authority, which directly relate to propaganda. Introduction Propaganda aims at influencing people’s mindset with the purpose of advancing a specific agenda. In the Internet era, thanks to the mechanism of sharing in social networks, propaganda campaigns have the potentia"
D19-5024,N18-1036,0,0.0158899,"a binarized version of that corpus: propaganda vs. the other three categories. Barr´on-Cedeno et al. (2019) annotated a large binary corpus of propagandist vs. non-propagandist articles and proposed a feature-based system for discriminating between them. In all these cases, the labels were obtained using distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with instances of ad hominem fallacy. Habernal et al. (2017, 2018a) introduced Argotario, a game to educate people to recognize and create fallacies, a by-product of which is a corpus with 1.3k arguments annotated with five fallacies such as ad hominem, red herring and irrelevant authority, which directly relate to propaganda. Introduction Propaganda aims at influencing people’s mindset with the purpose of advancing a specific agenda. In the Internet era, thanks to the mechanism of sharing in social networks, propaganda campaigns have the potentia"
D19-5024,N19-1423,0,0.0167189,"18 techniques randomly. The inefficacy of such a simple random baseline is illustrated in Tables 6 and 7. 5 (0.59) 10 (1.18) 9 (1.07) 101 (11.95) 26 (3.08) 2 (0.24) 10 (1.18) 10 (1.18) 9 Setup The shared task had two phases: In the development phase, the participants were provided labeled training and development datasets; in the testing phase, testing input was further provided. 9.1 Teams Participating in the Fragment-Level Classification Only Team newspeak (Yoosuf and Yang, 2019) achieved the best results on the test set for the FLC task using 20-way word-level classification based on BERT (Devlin et al., 2019): a word could belong to one of the 18 propaganda techniques, to none of them, or to an auxiliary (token-derived) class. The team fed one sentence at a time in order to reduce the workload. In addition to experimenting with an out-of-the-box BERT, they also tried unsupervised fine-tuning both on the 1M news dataset and on Wikipedia. Their best model was based on the uncased base model of BERT, with 12 Transformer layers (Vaswani et al., 2017), and 110 million parameters. Moreover, oversampling of the least represented classes proved to be crucial for the final performance. Finally, careful ana"
D19-5024,D19-5023,0,0.090948,"ary (token-derived) class. The team fed one sentence at a time in order to reduce the workload. In addition to experimenting with an out-of-the-box BERT, they also tried unsupervised fine-tuning both on the 1M news dataset and on Wikipedia. Their best model was based on the uncased base model of BERT, with 12 Transformer layers (Vaswani et al., 2017), and 110 million parameters. Moreover, oversampling of the least represented classes proved to be crucial for the final performance. Finally, careful analysis has shown that the model pays special attention to adjectives and adverbs. Team Stalin (Ek and Ghanimifard, 2019) focused on data augmentation to address the relatively small size of the data for fine-tuning contextual embedding representations based on ELMo (Peters et al., 2018), BERT, and Grover (Zellers et al., 2019). The balancing of the embedding space was carried out by means of synthetic minority class over-sampling. Then, the learned representations were fed into an LSTM. Phase 1. The participants tried to achieve the best performance on the development set. A live leaderboard kept track of the submissions. Phase 2. The test set was released and the participants had few days to make final predict"
D19-5024,D19-5020,0,0.105337,"Missing"
D19-5024,D19-5010,0,0.315396,"detect the sentences that contain propaganda. A total of 12 teams submitted systems for the FLC task, 25 teams did so for the SLC task, and 14 teams eventually submitted a system description paper. For both subtasks, most systems managed to beat the baseline by a sizable margin. The leaderboard and the data from the competition are available at http://propaganda.qcri. org/nlp4if-shared-task/. 1 2 Related Work Propaganda has been tackled mostly at the article level. Rashkin et al. (2017) created a corpus of news articles labelled as propaganda, trusted, hoax, or satire. Barr´on-Cede˜no et al. (2019) experimented with a binarized version of that corpus: propaganda vs. the other three categories. Barr´on-Cedeno et al. (2019) annotated a large binary corpus of propagandist vs. non-propagandist articles and proposed a feature-based system for discriminating between them. In all these cases, the labels were obtained using distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies rela"
D19-5024,D17-1317,0,0.273697,"Missing"
D19-5024,D19-5019,0,0.092582,"ith class imbalance, and experimented with BERTBASE and BERTLARGE Team LIACC (Ferreira Cruz et al., 2019) used hand-crafted features and pre-trained ELMo embeddings. They also observed a boost in performance when balancing the dataset by dropping some negative examples. Team JUSTDeep (Al-Omari et al., 2019) used a combination of models and features, including word embeddings based on GloVe (Pennington et al., 2014) concatenated with vectors representing affection and lexical features. These were combined in an ensemble of supervised models: bi-LSTM, XGBoost, and variations of BERT. Team YMJA (Hua, 2019) also based their approach on fine-tuned BERT. Inspired by kaggle competitions on sentiment analysis, they created an ensemble of models via cross-validation. Team jinfen (Li et al., 2019) used a logistic regression model fed with a manifold of representations, including TF.IDF and BERT vectors, as well as vocabularies and readability measures. 9.3 Teams Participating in Both Tasks Team MIC-CIS (Gupta et al., 2019) participated in both tasks. For the sentence-level classification, they used a voting ensemble including logistic regression, convolutional neural networks, and BERT, in all cases u"
D19-5024,D19-5017,0,0.0948953,"ed a boost in performance when balancing the dataset by dropping some negative examples. Team JUSTDeep (Al-Omari et al., 2019) used a combination of models and features, including word embeddings based on GloVe (Pennington et al., 2014) concatenated with vectors representing affection and lexical features. These were combined in an ensemble of supervised models: bi-LSTM, XGBoost, and variations of BERT. Team YMJA (Hua, 2019) also based their approach on fine-tuned BERT. Inspired by kaggle competitions on sentiment analysis, they created an ensemble of models via cross-validation. Team jinfen (Li et al., 2019) used a logistic regression model fed with a manifold of representations, including TF.IDF and BERT vectors, as well as vocabularies and readability measures. 9.3 Teams Participating in Both Tasks Team MIC-CIS (Gupta et al., 2019) participated in both tasks. For the sentence-level classification, they used a voting ensemble including logistic regression, convolutional neural networks, and BERT, in all cases using FastText embeddings (Bojanowski et al., 2017) and pre-trained BERT models. Beside these representations, multiple features of readability, sentiment and emotions were considered. For"
D19-5024,2021.ccl-1.108,0,0.0792132,"Missing"
D19-5024,D19-5018,0,0.0333779,"e development set at the end of phase 1 (see Section 6). Team CUNLP (Alhindi et al., 2019) considered two approaches for the sentence-level task. The first approach was based on fine-tuning BERT. The second approach complemented the fine-tuned BERT approach by feeding its decision into a logistic regressor, together with features from the Linguistic Inquiry and Word Count (LIWC)2 lexicon and punctuation-derived features. Similarly to Gupta et al. (2019), for the fragment-level problem they used a Bi-LSTM-CRF architecture, combining both character- and word-level embeddings. Team ProperGander (Madabushi et al., 2019) also used BERT, but they paid special attention to the imbalance of the data, as well as to the differences between training and testing. They showed that augmenting the training data by oversampling yielded improvements when testing on data that is temporally far from the training (by increasing recall). In order to deal with the imbalance, they performed cost-sensitive classification, i.e., the errors on the smaller positive class were more costly. For the fragment-level classification, inspired by named entity recognition, they used a model based on BERT using Continuous Random Field stack"
D19-5024,D19-5014,0,0.053654,".2 Team Tha3aroon (Fadel and Al-Ayyoub, 2019) implemented an ensemble of three classifiers: two based on BERT and one based on a universal sentence encoder (Cer et al., 2018). Team NSIT (Aggarwal and Sadana, 2019) explored three of the most popular transfer learning models: various versions of ELMo, BERT, and RoBERTa (Liu et al., 2019). Team Mindcoders (Vlad et al., 2019) combined BERT, Bi-LSTM and Capsule networks (Sabour et al., 2017) into a single deep neural network and pre-trained the resulting network on corpora used for related tasks, e.g., emotion classification. Finally, team ltuorp (Mapes et al., 2019) used an attention transformer using BERT trained on Wikipedia and BookCorpus. Teams Participating in the Sentence-Level Classification Only Team CAUnLP (Hou and Chen, 2019) used two context-aware representations based on BERT. In the first representation, the target sentence is followed by the title of the article. In the second representation, the previous sentence is also added. They performed subsampling in order to deal with class imbalance, and experimented with BERTBASE and BERTLARGE Team LIACC (Ferreira Cruz et al., 2019) used hand-crafted features and pre-trained ELMo embeddings. They"
D19-5024,D19-5022,0,0.136973,"detect the sentences that contain propaganda. A total of 12 teams submitted systems for the FLC task, 25 teams did so for the SLC task, and 14 teams eventually submitted a system description paper. For both subtasks, most systems managed to beat the baseline by a sizable margin. The leaderboard and the data from the competition are available at http://propaganda.qcri. org/nlp4if-shared-task/. 1 2 Related Work Propaganda has been tackled mostly at the article level. Rashkin et al. (2017) created a corpus of news articles labelled as propaganda, trusted, hoax, or satire. Barr´on-Cede˜no et al. (2019) experimented with a binarized version of that corpus: propaganda vs. the other three categories. Barr´on-Cedeno et al. (2019) annotated a large binary corpus of propagandist vs. non-propagandist articles and proposed a feature-based system for discriminating between them. In all these cases, the labels were obtained using distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies rela"
D19-5024,D14-1162,0,0.0840148,"RT. In the first representation, the target sentence is followed by the title of the article. In the second representation, the previous sentence is also added. They performed subsampling in order to deal with class imbalance, and experimented with BERTBASE and BERTLARGE Team LIACC (Ferreira Cruz et al., 2019) used hand-crafted features and pre-trained ELMo embeddings. They also observed a boost in performance when balancing the dataset by dropping some negative examples. Team JUSTDeep (Al-Omari et al., 2019) used a combination of models and features, including word embeddings based on GloVe (Pennington et al., 2014) concatenated with vectors representing affection and lexical features. These were combined in an ensemble of supervised models: bi-LSTM, XGBoost, and variations of BERT. Team YMJA (Hua, 2019) also based their approach on fine-tuned BERT. Inspired by kaggle competitions on sentiment analysis, they created an ensemble of models via cross-validation. Team jinfen (Li et al., 2019) used a logistic regression model fed with a manifold of representations, including TF.IDF and BERT vectors, as well as vocabularies and readability measures. 9.3 Teams Participating in Both Tasks Team MIC-CIS (Gupta et"
D19-5024,N18-1202,0,0.024846,"supervised fine-tuning both on the 1M news dataset and on Wikipedia. Their best model was based on the uncased base model of BERT, with 12 Transformer layers (Vaswani et al., 2017), and 110 million parameters. Moreover, oversampling of the least represented classes proved to be crucial for the final performance. Finally, careful analysis has shown that the model pays special attention to adjectives and adverbs. Team Stalin (Ek and Ghanimifard, 2019) focused on data augmentation to address the relatively small size of the data for fine-tuning contextual embedding representations based on ELMo (Peters et al., 2018), BERT, and Grover (Zellers et al., 2019). The balancing of the embedding space was carried out by means of synthetic minority class over-sampling. Then, the learned representations were fed into an LSTM. Phase 1. The participants tried to achieve the best performance on the development set. A live leaderboard kept track of the submissions. Phase 2. The test set was released and the participants had few days to make final predictions. In phase 2, no immediate feedback on the submissions was provided. The winner was determined based on the performance on the test set. 7 Participants and Approac"
D19-5024,D19-5011,0,0.0628514,"mance of this baseline on the SLC task is shown in Tables 4 and 5. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. The inefficacy of such a simple random baseline is illustrated in Tables 6 and 7. 5 (0.59) 10 (1.18) 9 (1.07) 101 (11.95) 26 (3.08) 2 (0.24) 10 (1.18) 10 (1.18) 9 Setup The shared task had two phases: In the development phase, the participants were provided labeled training and development datasets; in the testing phase, testing input was further provided. 9.1 Teams Participating in the Fragment-Level Classification Only Team newspeak (Yoosuf and Yang, 2019) achieved the best results on the test set for the FLC task using 20-way word-level classification based on BERT (Devlin et al., 2019): a word could belong to one of the 18 propaganda techniques, to none of them, or to an auxiliary (token-derived) class. The team fed one sentence at a time in order to reduce the workload. In addition to experimenting with an out-of-the-box BERT, they also tried unsupervised fine-tuning both on the 1M news dataset and on Wikipedia. Their best model was based on the uncased base model of BERT, with 12 Transformer layers (Vaswani et al., 2017), and 110 million pa"
D19-5024,D19-5015,0,\N,Missing
gencheva-etal-2017-context,J15-3002,0,\N,Missing
gencheva-etal-2017-context,nakov-etal-2017-trust,1,\N,Missing
gencheva-etal-2017-context,W02-0109,0,\N,Missing
J13-4005,C10-1005,1,0.811149,"Missing"
J13-4005,N03-1003,0,0.271833,"d guidelines used for its annotation are available at http://clic.ub.edu/corpus/en/paraphrases-en. The subsets of the MSRP and WRPA corpora annotated with the same typology are also available at this Web site. 10 http://staffwww.dcs.shef.ac.uk/people/T.Cohn/paraphrase corpus.html. 11 http://wicopaco.limsi.fr/. 925 Computational Linguistics Volume 39, Number 4 or meaning-altering. There also exist works where the focus is not to build a paraphrase corpus, but to create a paraphrase extraction or generation system, which ends up in also building a paraphrase collection, such as Barzilay and Lee (2003). Plagiarism detection experts are starting to turn their attention to paraphrasing. Burrows, Potthast, and Stein (2012) built the Webis Crowd Paraphrase Corpus by crowd-sourcing more than 4,000 manually simulated samples of paraphrase plagiarism.12 In order to create feasible mechanisms for crowd-sourcing paraphrase acquisition, they built a classifier to reject bad instances of paraphrase plagiarism (e.g., cases of verbatim plagiarism). These crowd-sourced instances are similar to the cases of simulated plagiarism in the PAN-PC-10 corpus, and hence the P4P (see the following). P4P was built"
J13-4005,P01-1008,0,0.0241186,"Missing"
J13-4005,P99-1071,0,0.0313701,"Missing"
J13-4005,clough-etal-2002-building,0,0.778948,"Missing"
J13-4005,J08-4005,0,0.0117388,"Missing"
J13-4005,I05-5002,0,0.0158835,"Missing"
J13-4005,max-wisniewski-2010-mining,0,0.0182925,"Missing"
J13-4005,C10-2115,1,0.925888,"Missing"
karadzhov-etal-2017-fully,W01-0515,0,\N,Missing
karadzhov-etal-2017-fully,N09-2040,0,\N,Missing
karadzhov-etal-2017-fully,D14-1162,0,\N,Missing
karadzhov-etal-2017-fully,P16-2065,1,\N,Missing
karadzhov-etal-2017-fully,Y10-1062,0,\N,Missing
N18-5006,N16-3003,0,0.160717,"opriate sentence tokenizer for each language. For English, NLTK’s (Loper and Bird, 2002) sent_tokenize handles splitting the text into sentences. However, for Arabic it can only split text based on the presence of the period (.) character. This is because other sentence endings — such as question marks— are different characters (e.g., the Arabic question mark is ‘?’, and not ‘?’). Hence, we used our custom regular expressions to split the Arabic text into sentences. Next comes tokenization. For English, we used NLTK’s tokenizer (Bird et al., 2009), while for Arabic we used Farasa’s segmenter (Abdelali et al., 2016). For Arabic, tokenization is not enough; we also need word segmentation since conjunctions and clitics are commonly attached to the main word, e.g., Â ¢þ + Âþtya + Á¤ (‘and his house’, lit. “and house his”). This causes explosion in the vocabulary size and data sparseness. Features Here we do not propose new features, but rather reuse features that have been previously shown to work well for check-worthiness (Hassan et al., 2015; Gencheva et al., 2017). From (Hassan et al., 2015), we include TF.IDFweighted bag of words, part-of-speech tags, named entities as recognized by Alchemy API, sentim"
N18-5006,P17-1042,0,0.0134211,"n (e.g., speaker, system messages), in order to be able to process any free text, and also discourse parse features, as we do not have a discourse parser for Arabic. One of the most important components of the system that we had to port across languages were the word embeddings. We experimented with the following cross-language embeddings: – VecMap: we used a parallel English-Arabic corpus of TED talks1 (Cettolo et al., 2012) to generate monolingual embeddings (Arabic and English) using word2vec (Mikolov et al., 2013). Then we projected these embeddings into a joint vector space using VecMap (Artetxe et al., 2017). 2 Note that these results are not comparable to those in (Gencheva et al., 2017) as we use a different evaluation setup: train/test split vs. cross-validation, debates that involve not only Hillary Clinton and Donald Trump, and we also disable the metadata and the discourse parse features. 1 We used TED talks as they are conversational large corpora, which is somewhat close to the debates we train on. 28 Figure 3: Screenshot of ClaimRank’s output for an Arabic news article, sorted by score. System word2vec VecMap MUSE Attract-Repel Random MAP 0.323 0.298 0.319 0.342 0.161 English R-Pr P@5 P@"
N18-5006,W02-0109,0,0.043495,"eir corresponding color codes. Scores are also stored in the session object along with the sentence list as parallel arrays. In case the user wants the sentences sorted by their scores, or wants to mimic one of the annotation sources strategy in sentence selection, the server gets the text from the session, and re-scores/orders it and sends it back to the client. 3.2 Model 3.4 Adaptation to Arabic To handle Arabic along with English, we integrated some new tools. First, we had to add a language detector in order to use the appropriate sentence tokenizer for each language. For English, NLTK’s (Loper and Bird, 2002) sent_tokenize handles splitting the text into sentences. However, for Arabic it can only split text based on the presence of the period (.) character. This is because other sentence endings — such as question marks— are different characters (e.g., the Arabic question mark is ‘?’, and not ‘?’). Hence, we used our custom regular expressions to split the Arabic text into sentences. Next comes tokenization. For English, we used NLTK’s tokenizer (Bird et al., 2009), while for Arabic we used Farasa’s segmenter (Abdelali et al., 2016). For Arabic, tokenization is not enough; we also need word segmen"
N18-5006,J93-2004,0,0.0701683,"se (Vuli´c et al., 2017), thus yielding better vectors, even for English. The overall MAP results for Arabic are competitive, compared to English. The best model is MUSE, while Attract-Repel is way behind, probably because, unlike VecMap and MUSE, its word embeddings are trained on unsegmented Arabic, which causes severe data sparseness issues. We further needed a part-of-speech (POS) tagger for Arabic, for which we used Farasa (Abdelali et al., 2016), while we used NLTK’s POS tagger for English (Bird et al., 2009). This yields different tagsets: for English, this is the Penn Treebank tagset (Marcus et al., 1993), while for Arabic this the Farasa tagset. Thus, we had to further map all POS tags to the same tagset: the Universal tagset (Petrov et al., 2012). 3.5 Evaluation We train the system on five English political debates, and we test on two debates: either English or their Arabic translations. Note that, compared to our original model (Gencheva et al., 2017), here we use more debates: seven instead of four. Moreover, here we exclude some of the features, namely some debate-specific information (e.g., speaker, system messages), in order to be able to process any free text, and also discourse parse"
N18-5006,2012.eamt-1.60,0,0.018724,"compared to our original model (Gencheva et al., 2017), here we use more debates: seven instead of four. Moreover, here we exclude some of the features, namely some debate-specific information (e.g., speaker, system messages), in order to be able to process any free text, and also discourse parse features, as we do not have a discourse parser for Arabic. One of the most important components of the system that we had to port across languages were the word embeddings. We experimented with the following cross-language embeddings: – VecMap: we used a parallel English-Arabic corpus of TED talks1 (Cettolo et al., 2012) to generate monolingual embeddings (Arabic and English) using word2vec (Mikolov et al., 2013). Then we projected these embeddings into a joint vector space using VecMap (Artetxe et al., 2017). 2 Note that these results are not comparable to those in (Gencheva et al., 2017) as we use a different evaluation setup: train/test split vs. cross-validation, debates that involve not only Hillary Clinton and Donald Trump, and we also disable the metadata and the discourse parse features. 1 We used TED talks as they are conversational large corpora, which is somewhat close to the debates we train on. 2"
N18-5006,Q17-1022,0,0.0427226,"Missing"
N18-5006,gencheva-etal-2017-context,1,0.907961,"n et al., 2015). It is trained on data annotated by students, professors, and journalists, and uses features such as sentiment, TF.IDF-weighted words, part-of-speech tags, and named entities. In contrast, (i) we have much richer features, (ii) we support English and Arabic, (iii) we learn from choices made by nine reputable fact-checking organizations, and (iv) we can mimic the selection strategy of each of them. In our previous work, we focused on debates from the US 2016 Presidential Campaign and we used pre-existing annotations from online fact-checking reports by professional journalists (Gencheva et al., 2017). Here we use roughly the same features, with some differences (see below). However, (i) we train on more debates (seven instead of four for English, and also Arabic translations for two debates), (ii) we add support for Arabic, and (iii) we deploy a working system. Patwari et al. (2017) focused on the 2016 US Election campaign as well and independently obtained their data in a similar way. However, they used less features, they did not mimic any specific website, nor did they deploy a working system. Introduction The proliferation of fake news demands the attention of both investigative journ"
N18-5006,petrov-etal-2012-universal,0,0.0725666,"Missing"
N18-5006,P13-1162,0,0.0541594,"ttached to the main word, e.g., Â ¢þ + Âþtya + Á¤ (‘and his house’, lit. “and house his”). This causes explosion in the vocabulary size and data sparseness. Features Here we do not propose new features, but rather reuse features that have been previously shown to work well for check-worthiness (Hassan et al., 2015; Gencheva et al., 2017). From (Hassan et al., 2015), we include TF.IDFweighted bag of words, part-of-speech tags, named entities as recognized by Alchemy API, sentiment scores, and sentence length (in tokens). From (Gencheva et al., 2017), we adopt lexicon features, e.g., for bias (Recasens et al., 2013), for sentiment (Liu et al., 2005), for assertiveness (Hooper, 1974), and also for subjectivity. 27 Figure 2: Screenshot of ClaimRank’s output for an English presidential debate, in natural order. – MUSE embeddings: In a similar fashion, we generated cross-language embeddings from the same TED talks using Facebook’s supervised MUSE model (Lample et al., 2017) to project the Arabic and the English monolingual embeddings into a joint vector space. – Attract-Repel embeddings: we used the pretrained English-Arabic embeddings from AttractRepel (Mrkši´c et al., 2017). Table 1 shows the system perfor"
N18-5006,J15-3002,0,0.0693925,"Missing"
N18-5006,P17-1006,0,0.0251397,"Missing"
P15-2113,P07-1098,1,0.451732,"al problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer s"
P15-2113,N10-1145,0,0.0528465,"answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve high"
P15-2113,S15-2047,1,0.84994,"Missing"
P15-2113,S15-2035,0,0.0819635,"1.45 65.57±1.54 76.23±0.45 76.43±0.92 75.05±0.70 75.61±0.63 75.71±0.71 Table 3: Precision, Recall, F1 , Accuracy computed at the comment level; F1,ta and Ata are averaged at the thread level. Precision, Recall, F1 , F1,ta are computed with respect to the good classifier on 5-fold cross-validation (mean±stand. dev.). 4.2 Experimental Setup As in the competition, the results are macroaveraged at class level. The results of the top 3 Our local classifiers are support vector machines systems are reported for comparison: JAIST (Tran (SVM) with C = 1 (Joachims, 1999), logistic et al., 2015), HITSZ (Hou et al., 2015) and regression with a Gaussian prior with variance 10, QCRI (Nicosia et al., 2015), where the latter refers and logistic ordinal regression (McCullagh, 1980). to our old system that we used for the competition. In order to capture long-range sequential depenThe two main observations are (i) using threaddencies, we use a second-order SVMhmm (Yu level features helps significantly; and (ii) the ordiand Joachims, 2008) (with C = 500 and nal regression model, which captures the idea that epsilon = 0.01) and a second-order linear-chain potential lies between good and bad, achieves at CRF, which con"
P15-2113,S15-2036,1,0.620958,"Missing"
P15-2113,D13-1044,1,0.903656,"Missing"
P15-2113,W01-0515,0,0.0237286,"can affect the label of the current answer, this dependency is too loose to have impact on the selection accuracy. In other words, labels should be used together with answers’ content to account for stronger and more effective dependencies. 2 Basic and Thread-Level Features 3.1 Baseline Features We measure lexical and syntactic similarity between q and c. We compute the similarity between word n-grams (n = [1, . . . , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also apply partial tree kernels (Moschitti, 2006) on shallow syntactic trees. We designed a set of heuristic features that might suggest whether c is good or not. Forty-four Boolean features express whether c (i) includes URLs or emails (2 feats.); (ii) contains the word “yes”, “sure”, “no”, “can”, “neither”, “okay”, and “sorry”, as well as symbols ‘?’ and ‘@’ (9 feats.); (iii) starts with “yes” (1 feat.); (iv) includes a sequence of three or more repeated characters or a word longer than fifteen characters (2 feats.); (v) belongs to one of the categories of the for"
P15-2113,W13-3509,1,0.800933,"g the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve higher accuracy. To test our hypothesis about the usefulness of thread-level information, we used"
P15-2113,D07-1002,0,0.0974152,"a Abstract This is a real problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and"
P15-2113,P08-1082,0,0.0417245,"n can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should b"
P15-2113,S15-2038,0,0.16032,"Missing"
P15-2113,C10-1131,0,0.223512,"Missing"
P15-2113,N13-1106,0,0.0861461,"Missing"
P18-4023,S16-1138,1,0.902898,"Missing"
P18-4023,S17-2003,1,0.899267,"Missing"
P18-4023,P15-2114,0,0.0320864,"onships together with both content- and usage-based features. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Regarding question ranking, Duan et al. (2008) searched for equivalent questions by considering the question’s focus. Zhou et al. (2011) used a (monolingual) phrase-based translation model and Wang et al. (2009) computed similarities on syntactic-trees. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approaches. The challenges of 2016 and 2017 included Task 3-A on comment re-ranking and Task 3-B on question re-ranking. For task 3-A, Tran et al. (2015) applied machine translation, topic models, embeddings, and similarities. Hou et al. (2015) and Nicosia et al. (2015) applied supervised models with lexical, syntactic and meta-data features. For task 3-B The top-three participants applied SVMs as learning models (Franco-Salvador et al., 2016; Ba"
P18-4023,P08-1019,0,0.0273251,"ly available to the research and industrial community by also providing our toolkit with tutorials and usage options for different degrees of user expertise. 2 Related Work One of the first approaches to answer ranking relied on metadata (Jeon et al., 2006) (e.g., click counts). Agichtein et al. (2008) explored a graph-based model of contributors relationships together with both content- and usage-based features. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Regarding question ranking, Duan et al. (2008) searched for equivalent questions by considering the question’s focus. Zhou et al. (2011) used a (monolingual) phrase-based translation model and Wang et al. (2009) computed similarities on syntactic-trees. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approaches. The challenges of 2016 and 2017 included Task 3-A on comment re-ranking and Task"
P18-4023,W14-5201,0,0.0578265,"Missing"
P18-4023,S16-1172,1,0.905097,"Missing"
P18-4023,S15-2036,1,0.897104,"Missing"
P18-4023,P15-1097,1,0.832041,"A) or a set of questions (Task 3-B). For example, r can be a linear function, r(x, x0 ) = w ~ · φ(x, x0 ), where w ~ is the model and φ() provides a feature vector representation of the pair, (x, x0 ). The vectors φ used by Barr´on-Cede˜no et al. (2016); Filice et al. (2016) are a combination of tree kernel similarity functions and features derived from similarity measures between the two comments/questions constituting one learning example, as well as features extracting information from the forum threads the comments/questions belong to. 3.1 Tree Kernel We use the kernel function defined by Filice et al. (2015):  K((x1 , x01 ), (x2 , x02 )) = TK tx01 (x1 ), tx02 (x2 )  + TK tx1 (x01 ), tx2 (x02 ) where TK is the Partial Tree Kernel by Moschitti (2006) and ty (x) is a function which enriches the tree x with information derived from its structural similarity with the tree y (see (Severyn and Moschitti, 2012; Filice et al., 2016) for details). 135 3.2 Feature Vectors module is designed to be replicated in multiple instances to achieve scalability. Each of these modules is a pipeline deployed as UIMA-AS service that listens to a queue of processing requests (registered on the broker). Each module can"
P18-4023,P17-4004,0,0.0645536,"Missing"
P18-4023,S17-2053,1,0.895072,"Missing"
P18-4023,S15-2038,0,0.023764,"idering the question’s focus. Zhou et al. (2011) used a (monolingual) phrase-based translation model and Wang et al. (2009) computed similarities on syntactic-trees. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approaches. The challenges of 2016 and 2017 included Task 3-A on comment re-ranking and Task 3-B on question re-ranking. For task 3-A, Tran et al. (2015) applied machine translation, topic models, embeddings, and similarities. Hou et al. (2015) and Nicosia et al. (2015) applied supervised models with lexical, syntactic and meta-data features. For task 3-B The top-three participants applied SVMs as learning models (Franco-Salvador et al., 2016; Barr´on-Cede˜no et al., 2016; Filice et al., 2016). Franco-Salvador et al. (2016) relied heavily on distributed representations and semantic information sources, such as Babelnet and Framenet. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use lexical similarities and tree kernels on parse t"
P18-4023,P16-4027,1,0.845066,"f the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 134–139 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics that SVM systems based on a combination of vectorial features and tree kernels perform consistently well on the different editions of the competition (Barr´on-Cede˜no et al., 2016; Filice et al., 2016, 2017): the systems described in those papers won Task 3-A both years, placed second and first on Task 3-B in years 2016 and 2017, respectively. The most related demonstration papers to ours are (Uryupina et al., 2016; R¨uckl´e and Gurevych, 2017). As ours, the system of Uryupina et al. (2016) is a UIMA-based pipeline. Yet in their case the input is a single text and the output is the result of different levels of textual annotation (e.g., tokens, syntactic information, or wikification). R¨uckl´e and Gurevych (2017) developed an architecture to perform question and answer reranking in cQA based on deep learning. Their main focus is the analysis of attention models in these tasks. and Arabic (Barr´on-Cede˜no et al., 2016) by simply adding basic linguistic modules, e.g., the syntactic parsers, for both langu"
P18-4023,S16-1126,0,0.0200807,"(2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approaches. The challenges of 2016 and 2017 included Task 3-A on comment re-ranking and Task 3-B on question re-ranking. For task 3-A, Tran et al. (2015) applied machine translation, topic models, embeddings, and similarities. Hou et al. (2015) and Nicosia et al. (2015) applied supervised models with lexical, syntactic and meta-data features. For task 3-B The top-three participants applied SVMs as learning models (Franco-Salvador et al., 2016; Barr´on-Cede˜no et al., 2016; Filice et al., 2016). Franco-Salvador et al. (2016) relied heavily on distributed representations and semantic information sources, such as Babelnet and Framenet. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use lexical similarities and tree kernels on parse trees. No statisticallysignificant differences were observed in the performance of these three systems. In summary, the results for both tasks show 3 Structural Linguistic Models for cQA In this section, we describe the components of the two learning systems. The ranking function for both task"
P18-4023,S15-2035,0,0.0312867,"on model and Wang et al. (2009) computed similarities on syntactic-trees. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approaches. The challenges of 2016 and 2017 included Task 3-A on comment re-ranking and Task 3-B on question re-ranking. For task 3-A, Tran et al. (2015) applied machine translation, topic models, embeddings, and similarities. Hou et al. (2015) and Nicosia et al. (2015) applied supervised models with lexical, syntactic and meta-data features. For task 3-B The top-three participants applied SVMs as learning models (Franco-Salvador et al., 2016; Barr´on-Cede˜no et al., 2016; Filice et al., 2016). Franco-Salvador et al. (2016) relied heavily on distributed representations and semantic information sources, such as Babelnet and Framenet. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use lexical similarities and tree kernels on parse trees. No statisticallysignificant differences were observed in the performance of these thr"
P18-4023,P11-1066,0,0.0188308,"utorials and usage options for different degrees of user expertise. 2 Related Work One of the first approaches to answer ranking relied on metadata (Jeon et al., 2006) (e.g., click counts). Agichtein et al. (2008) explored a graph-based model of contributors relationships together with both content- and usage-based features. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Regarding question ranking, Duan et al. (2008) searched for equivalent questions by considering the question’s focus. Zhou et al. (2011) used a (monolingual) phrase-based translation model and Wang et al. (2009) computed similarities on syntactic-trees. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approaches. The challenges of 2016 and 2017 included Task 3-A on comment re-ranking and Task 3-B on question re-ranking. For task 3-A, Tran et al. (2015) applied machine translation,"
P18-4023,D15-1068,1,0.908478,"Missing"
P18-4023,P15-1025,0,0.0144318,"s, for both languages. We make our software framework, based on UIMA technology, freely available to the research and industrial community by also providing our toolkit with tutorials and usage options for different degrees of user expertise. 2 Related Work One of the first approaches to answer ranking relied on metadata (Jeon et al., 2006) (e.g., click counts). Agichtein et al. (2008) explored a graph-based model of contributors relationships together with both content- and usage-based features. Some of the most recent proposals aim at classifying whole threads of answers (Joty et al., 2015; Zhou et al., 2015) rather than each answer in isolation. Regarding question ranking, Duan et al. (2008) searched for equivalent questions by considering the question’s focus. Zhou et al. (2011) used a (monolingual) phrase-based translation model and Wang et al. (2009) computed similarities on syntactic-trees. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). dos Santos et al. (2015) applied convolutional neural networks. The three editions of the SemEval Task 3 on cQA (Nakov et al., 2015, 2016, 2017) have triggered a manifold of approach"
R19-1141,D18-1389,1,0.867559,"d dataset. Section 4 describes our method and features. Section 5 presents the experiments and the evaluation results. Finally, Section 7 concludes and points to some possible directions for future work. 1229 Proceedings of Recent Advances in Natural Language Processing, pages 1229–1239, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 20"
R19-1141,N19-1216,1,0.885016,"Missing"
R19-1141,D19-1565,1,0.88107,"78-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answer"
R19-1141,P08-1118,0,0.0399865,"Missing"
R19-1141,S17-2006,0,0.0513316,"Missing"
R19-1141,P15-2139,0,0.0249286,"tweets and temporal information (Ma et al., 2016). We also want to explore other multi-task learning options, e.g., as described in (Ruder, 2017). Figure 2: Ablation experiment with the multi model. Each row is an experiment removing one target. Each column is the MAP difference with respect to the multi model for the corresponding target. It would be interesting to investigate the reasons why the NYT source does not benefit from the multi-task architecture. In order to adapt to this situation with a single model, we plan to experiment with a network with soft parameter sharing, e.g., as in (Duong et al., 2015). For example, we could create a chain of layers that back-propagate to the input using only single task targets and then add an auxiliary layer that is shared between the tasks on the side. In this way, the model would be able to turn off the multi-task learning completely for some of the sources. However, training such kind of model might require significantly more training data; semi-supervised training might be a possible solution. Acknowledgments We would like to thank the anonymous reviewer, whose constructive feedback has helped us improve the quality of this paper. This work is part of"
R19-1141,W09-0439,0,0.0126282,"CW-USPD-2016 corpus contains four debates, we perform 4-fold cross-validation, where each time we leave one debate out for testing, and we train on the remaining three debates. Moreover, in order to stabilize the results, we repeat each experiment three times with different random seeds and we report the average over these three reruns of the system.4 4 Having multiple reruns is a standard procedure to stabilize an optimization algorithm that is sensitive to the random seed, e.g., this strategy has been argued for when using MERT for tuning hyper-parameters in Statistical Machine Translation (Foster and Kuhn, 2009). In our neural model, we used ReLU units and a shared layer of size 300. For training, we used Stochastic Gradient Descent with Nesterov momentum,5 iterating for 100 epochs. Recall that our main objective is to prioritize the claims that should be selected for manual factchecking, which is best achieved by proposing a ranked list of claims. Thus, we have a ranking task, for which we use suitable information retrieval evaluation measures. In particular, we adopt Mean Average Precision (MAP) as our primary evaluation measure. We further report RPrecision, or R-Pr, and precision at k, or P@k,6 f"
R19-1141,gencheva-etal-2017-context,1,0.605665,"Missing"
R19-1141,S19-2147,0,0.0306601,"news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. The interested reader can learn more about “fake news” from the overview by Shu et al. (2017), which adopted a data min"
R19-1141,N18-5006,1,0.895733,"Missing"
R19-1141,J15-3002,0,0.0278902,"IDF-weighted bag of words, part-of-speech tags, the presence of named entities, sentiment scores, and sentence length (in number of tokens). Moreover, from (Gencheva et al., 2017), we further adopt lexicon features, e.g., for bias (Recasens et al., 2013), for sentiment (Liu et al., 2005), for assertiveness (Hooper, 1974), and for subjectivity; structural features, e.g., for location of the sentence within the debate/intervention; LDA topics (Blei et al., 2003); word embeddings, pretrained on Google News (Mikolov et al., 2013); and discourse relations with respect to the neighboring sentences (Joty et al., 2015). See (Hassan et al., 2015b; Gencheva et al., 2017) for more details about each of these feature types. After the input layer, comes a hidden layer that is shared between all tasks. It is followed by ten parallel task-specific hidden layers. During training, in the process of backpropagation, each task modifies the weights of its own task-specific layer and also of the shared layer. 1231 Figure 1: The architecture of our neural multi-task learning model, predicting whether each of the nine individual fact-checking organizations (tasks) would consider this sentence check-worthy and one cumulati"
R19-1141,D18-1388,0,0.0308285,"tural Language Processing, pages 1229–1239, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERifi"
R19-1141,W16-2117,0,0.0319884,"been collected. Thus, the task can be reduced to recognizing textual entailment (Dagan et al., 2009). 1230 de Marneffe et al. (2008) also looked for contradictions in text. They tried to classify the contradictions that can be found in a piece of text in two categories —those occurring via antonymy, negation, and date/number mismatch, and those arising from different world knowledge and lexical contrasts. The features that are selected for the task of contradiction detection include polarity, numbers, dates and time, antonymy, factivity, modality, structural, and relational features. Finally, Le et al. (2016) used deep learning. They argued that the top terms in claim vs. nonclaim sentences are highly overlapping in content, which is a problem for bag-of-words approaches. Thus, they used a Convolutional Neural Network, where each word is represented by its embedding and each named entity is replaced by its tag, e.g., person, organization, location. Unlike the above work, we mimic the selection strategy of one specific fact-checking organization by learning to jointly predict the selection choices by multiple such organizations. 3 Data In our experiments, we used the CW-USPD2016 dataset from our pr"
R19-1141,K15-1032,1,0.805395,"f Recent Advances in Natural Language Processing, pages 1229–1239, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fac"
R19-1141,S19-2149,1,0.839641,"l., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. The interested reader can learn more about “fake news” from the overview by Shu et al. (2017), which adopted a data mining perspective and focused on social media. Another recent survey (Thorne and Vlachos, 2018) took a factchecking perspective on “fake news” and related problems. Yet another survey was performed by Li et al. (2016), and it covered truth discovery in general. Moreover, there were two recent articles in Science: Lazer et al. (2018) offered a general overview and discussion on the science of “fake news”, while Vosoughi et al. (2018) focused on the proliferation o"
R19-1141,N13-1090,0,0.0171527,"sk of checkworthiness prediction. In particular, from (Hassan et al., 2015b), we adopt TF.IDF-weighted bag of words, part-of-speech tags, the presence of named entities, sentiment scores, and sentence length (in number of tokens). Moreover, from (Gencheva et al., 2017), we further adopt lexicon features, e.g., for bias (Recasens et al., 2013), for sentiment (Liu et al., 2005), for assertiveness (Hooper, 1974), and for subjectivity; structural features, e.g., for location of the sentence within the debate/intervention; LDA topics (Blei et al., 2003); word embeddings, pretrained on Google News (Mikolov et al., 2013); and discourse relations with respect to the neighboring sentences (Joty et al., 2015). See (Hassan et al., 2015b; Gencheva et al., 2017) for more details about each of these feature types. After the input layer, comes a hidden layer that is shared between all tasks. It is followed by ten parallel task-specific hidden layers. During training, in the process of backpropagation, each task modifies the weights of its own task-specific layer and also of the shared layer. 1231 Figure 1: The architecture of our neural multi-task learning model, predicting whether each of the nine individual fact-ch"
R19-1141,P18-1022,0,0.0483181,"related work. Section 3 describes the used dataset. Section 4 describes our method and features. Section 5 presents the experiments and the evaluation results. Finally, Section 7 concludes and points to some possible directions for future work. 1229 Proceedings of Recent Advances in Natural Language Processing, pages 1229–1239, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Na"
R19-1141,P13-1162,0,0.0835321,"hether each of the nine individual sources (tasks) would have selected it, and whether at least one of them would, which is the special task ANY. The input to our neural network consists of various domain-specific features that have been previously shown to work well for the task of checkworthiness prediction. In particular, from (Hassan et al., 2015b), we adopt TF.IDF-weighted bag of words, part-of-speech tags, the presence of named entities, sentiment scores, and sentence length (in number of tokens). Moreover, from (Gencheva et al., 2017), we further adopt lexicon features, e.g., for bias (Recasens et al., 2013), for sentiment (Liu et al., 2005), for assertiveness (Hooper, 1974), and for subjectivity; structural features, e.g., for location of the sentence within the debate/intervention; LDA topics (Blei et al., 2003); word embeddings, pretrained on Google News (Mikolov et al., 2013); and discourse relations with respect to the neighboring sentences (Joty et al., 2015). See (Hassan et al., 2015b; Gencheva et al., 2017) for more details about each of these feature types. After the input layer, comes a hidden layer that is shared between all tasks. It is followed by ten parallel task-specific hidden la"
R19-1141,C18-1283,0,0.0892411,"d et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. The interested reader can learn more about “fake news” from the overview by Shu et al. (2017), which adopted a data mining perspective and focused on social media. Another recent survey (Thorne and Vlachos, 2018) took a factchecking perspective on “fake news” and related problems. Yet another survey was performed by Li et al. (2016), and it covered truth discovery in general. Moreover, there were two recent articles in Science: Lazer et al. (2018) offered a general overview and discussion on the science of “fake news”, while Vosoughi et al. (2018) focused on the proliferation of true and false news online. The first work to target check-worthiness estimation, i.e., predicting which sentences in a given input text should be prioritized for factchecking, was the ClaimBuster system (Hassan et al., 2015b)"
R19-1141,N18-1074,0,0.0902678,"Missing"
R19-1141,W14-2508,0,0.076833,"oint that attracted wide public attention to the problem. By then, a number of organizations, e.g., FactCheck1 and Snopes2 among many others, launched factchecking initiatives. Yet, this proved to be a very demanding manual effort, and only a relatively small number of claims could be fact-checked. Thus, it is important to prioritize what to check. 1 2 http://www.factcheck.org/ http://www.snopes.com/ Llu´ıs M`arquez Amazon Core ML lluismv@amazon.com The task of detecting check-worthy claims has been recognized as an important stage in the process of fully automatic fact-checking. According to Vlachos and Riedel (2014) this is a multistep process that (i) extracts statements to be fact-checked, (ii) constructs appropriate questions, (iii) obtains the answers from relevant sources, and (iv) reaches a verdict using these answers. Hassan et al. (2015a) presented a similar vision, and in a follow up work they made check-worthiness an integral part of an end-to-end fact-checking system Hassan et al. (2017). Here, we approach the problem of mimicking the selection strategy of several renowned fact-checking organizations such as PolitiFact, FactCheck, ABC, CNN, NPR, NYT, Chicago Tribune, The Guardian, and The Wash"
R19-1141,D19-3038,1,0.810868,"Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova"
S13-1020,W05-0909,0,0.388265,"Missing"
S13-1020,P04-1077,0,0.118587,"Missing"
S13-1020,W05-0904,0,0.032249,"omatic translation, or vice versa. Some of the metrics are not symmetric so we compute similarity between s1 and s2 in both directions and average the resulting scores. The measures are computed with the Asiya Toolkit for Automatic MT Evaluation (Gim´enez and M`arquez, 2010b). The only pre-processing carried out was tokenization (Asiya performs additional inbox pre-processing operations, though). We consid2 We also tried with linear kernels, but RBF always obtained better results. 144 Syntactic Similarity Three metrics that estimate the similarity of the sentences over dependency parse trees (Liu and Gildea, 2005): DP-HWCMic-4 for grammatical categories chains, DP-HWCMir-4 over grammatical relations, and DP-Or (⋆) over words ruled by non-terminal nodes. Also, one measure that estimates the similarity over constituent parse trees: CP-STM4 (Liu and Gildea, 2005). Semantic Similarity Three measures that estimate the similarities over semantic roles (i.e. arguments and adjuncts): SR-Or , SR-Mr (⋆), and SR-Or (⋆). Additionally, two metrics that estimate similarities over discourse representations: DR-Or (⋆) and DR-Orp(⋆). 3 Asiya is available at http://asiya.lsi.upc.edu. Full descriptions of the metrics are"
S13-1020,P02-1040,0,0.0951332,"Missing"
S13-1020,2006.amta-papers.25,0,0.13064,"Missing"
S13-1020,S12-1060,0,0.109397,"Missing"
S15-2036,P14-1023,0,0.00431346,"entence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of th"
S15-2036,W10-2802,0,0.0170168,"tial tree kernel (Moschitti, 2006) to calculate the similarity between the question and the comment based on their corresponding shallow syntactic trees. These trees have word lemmata as leaves, then there is a POS tag node parent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimen"
S15-2036,W01-0515,0,0.415394,"rule-based. 2.1 Similarity Measures The similarity features measure the similarity sim(q, c) between the question and a target comment, assuming that high similarity signals a GOOD answer. We consider three kinds of similarity measures, which we describe below. 2.1.1 Lexical Similarity We compute the similarity between word n-gram representations (n = [1, . . . , 4]) of q and c, using the following lexical similarity measures (after stopword removal): greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: X sim(q, c) = idf (t) (1) t∈q∩c sim(q, c) = X t∈q∩c sim(q, c) = X t∈q∩c log(idf (t)) (2)   |C| log 1 + tf (t) (3) where idf (t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf (t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (200"
S15-2036,S15-2047,1,0.437926,"Missing"
S15-2036,N13-1090,0,0.0152512,"tar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of the comment thread. Whether a question includes further comments by the person who asked the original question or just several comments by the same user, or whether it belongs to a category in which a given kind of answer is expected, are all important factors. Therefore, we consider a set of featur"
S15-2036,S13-2053,0,0.0143676,"omments suggested visiting a Web site or contained an email address. Therefore, we included two boolean features to verify the presence of URLs or emails in c. Another feature captures the length of c, as longer (GOOD ) comments usually contain detailed information to answer a question. 2.5 Polarity These features, which we used for subtask B only, try to determine whether a comment is positive or negative, which could be associated with YES or NO answers. The polarity of a comment c is X pol(w) (5) pol(c) = w∈c where pol(w) is the polarity of word w in the NRC Hashtag Sentiment Lexicon v0.1 (Mohammad et al., 2013). We disregarded pol(w) if its absolute value was less than 1. We further use boolean features that check the existence of some keywords in the comment. Their values are set to true if c contains words like (i) yes, can, sure, wish, would, or (ii) no, not, neither. 2.6 User Profile With this set of features, we aim to model the behavior of the different participants in previous queries. Given comment c by user u, we consider the number of GOOD , BAD , POTENTIAL , and DIALOGUE comments u has produced before.4 We also consider the average word length of GOOD , BAD , POTENTIAL , and DIALOGUE comm"
S15-2036,D14-1162,0,0.0928882,"arent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help u"
S16-1138,P15-2113,1,0.112654,"Missing"
S16-1138,S15-2048,0,0.119863,"the appropriateness of the answers c ∈ Cq against q 0 . 1 http://alt.qcri.org/semeval2016/task3 Task 3 included these three tasks for English, whereas an adaptation of Task C was proposed for Arabic (Task D). The reader can refer to (Nakov et al., 2016) for a more detailed description of the tasks. Task A was also proposed in the SemEval2015 edition (Nakov et al., 2015).2 We designed systems for all tasks. We used the feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and com"
S16-1138,P15-4004,0,0.00840536,"the pairs as: K((t1 , t2 ), (u1 , u2 )) = T K(t1 , u1 ) + T K(t2 , u2 ), (1) where t and u are parse trees extracted from the text pair, i.e., either question and comment for task A or question and question for tasks B and D. 4 Submissions and Results We describe our primary submissions for the four tasks in Section 4.1. The contrastive submissions are discussed in Section 4.2. Table 1 shows our official competition results for both primary and contrastive submissions. In all submissions we employed Support Vector Machines (SVM) (Joachims, 1999) using either SVM-Light (Joachims, 1999), KeLP9 (Filice et al., 2015), or SVM-light-TK10 (Moschitti, 2006) (only the last two can handle tree kernels). 4.1 Task A. The submission consists in an SVM operating on two kernels: (i) the tree kernel described in Section 3.3, applied to the structures described by Tymoshenko and Moschitti (2015) without question and focus classification; (ii) a polynomial kernel of degree 3 applied to the feature vector that is a concatenation of the feature vector described in Section 3.1, and question and answer embeddings learned on the training set by the Convolutional Neural Network (CNN) described in (Severyn and Moschitti, 2015"
S16-1138,D15-1068,1,0.19243,"Missing"
S16-1138,N16-1084,1,0.885885,"Missing"
S16-1138,P14-5010,0,0.0035797,"found in (Nakov et al., 2016). 3 Approach In order to re-rank the comments according to their relevance, either against the forum questions or against the new questions, we train a binary SVM classifier and use its score as a measure of relevance. The classifier uses partial tree kernels (Moschitti, 2006) defined over shallow syntactic trees, along with other numeric features. We used the DKPro Core toolkit (Eckart de Castilho and Gurevych, 2014)5 for pre-processing the texts in English. More precisely, we used OpenNLP’s tokenizer, POS-tagger and chunk annotator6 , and Stanford’s lemmatizer (Manning et al., 2014), all accessible through DKPro Core. We used the MADAMIRA toolkit (Pasha et al., 2014) for segmenting Arabic texts. In order to split the texts into sentences, we used the Stanford splitter.7 For parsing Arabic texts into syntactic trees, we 3 http://www.qatarliving.com/forum https://www.webteb.com/, http://www. altibbi.com/, and http://consult.islamweb. net. 5 https://dkpro.github.io/dkpro-core/ 6 https://opennlp.apache.org/ 7 http://stanfordnlp.github.io/CoreNLP 4 used the Berkeley parser (Petrov and Klein, 2007). Following, we briefly describe the numeric features used in different tasks. 3"
S16-1138,N13-1090,0,0.00760434,"situations including the identification of potential dialogues, which usually represent a bunch of bad comments, or the position of the comment in the thread. We also considered the categories of the questions in the forum (as some of them tend to include more open-ended questions and even invite for discussion on ambiguous topics), as well as the occurrence of specific strings or the length of a comment. In-depth descriptions of these features are available in (Nicosia et al., 2015). For Arabic texts, we utilize the embedding vectors as obtained by Belinkov et al. (2015): employing word2vec (Mikolov et al., 2013) on the Arabic Gigaword corpus (Parker et al., 2011). More specifically, we concatenate the vectors representing a new question and an existing question in the question– answer pair, which is then fed to the SVM classifier. 3.2 Rank Feature The meta-information in the English corpus includes the position of the forum threads in the rank generated by the Google search engine for a given new question. We exploit this information in tasks B and C. We employ the inverse of such position as a feature and refer to it as the rank feature. 3.3 structed a syntactic tree for each comment or question. Ea"
S16-1138,P07-1098,1,0.0409837,"proposed for Arabic (Task D). The reader can refer to (Nakov et al., 2016) for a more detailed description of the tasks. Task A was also proposed in the SemEval2015 edition (Nakov et al., 2015).2 We designed systems for all tasks. We used the feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to"
S16-1138,S15-2047,1,0.299017,"; (B) to re-rank the set of questions Q according to their relevance against the new question q 0 ; and finally (C) to predict the appropriateness of the answers c ∈ Cq against q 0 . 1 http://alt.qcri.org/semeval2016/task3 Task 3 included these three tasks for English, whereas an adaptation of Task C was proposed for Arabic (Task D). The reader can refer to (Nakov et al., 2016) for a more detailed description of the tasks. Task A was also proposed in the SemEval2015 edition (Nakov et al., 2015).2 We designed systems for all tasks. We used the feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko e"
S16-1138,S15-2036,1,0.501211,"Missing"
S16-1138,P02-1040,0,0.114288,"ll training and development sets. The second contrastive submission consists of a rule-based system which relies on the outputs from tasks A and B. A comment is labeled as good if it is considered good with respect to the related question (Task A) and the related question is considered relevant with respect to the new question (Task B). The comment is considered bad otherwise. Task D. The contrastive systems did not use tree kernels. Our first contrastive run used only feature vectors. Our second contrastive run also used additional features borrowed from machine translation evaluation: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), Meteor (Lavie and Denkowski, 2009), NIST (Doddington, 2002), Precision and Recall, and length ratio between the question and the comment. 5 Results and Discussion Table 1 shows the results obtained in the four tasks. We achieved the second position for tasks A, B, and D. In Task A, tree kernels give no major boost, but without them our model would be cont2 , which 900 achieved the third position on the test set. The joint model cont1 , run on top of our primary system, was able to improve it by more than one point. We were not sure about the outcome of this model,"
S16-1138,pasha-etal-2014-madamira,0,0.0108564,"Missing"
S16-1138,D13-1044,1,0.0339644,"ms for all tasks. We used the feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to tackle the new challenges of the SemEval-2016 Task 3. Indeed, all our three systems using relational models based on tree kernels achieved the second official 2 Note that in that paper the naming convention is slightl"
S16-1138,W13-3509,1,0.135312,"feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to tackle the new challenges of the SemEval-2016 Task 3. Indeed, all our three systems using relational models based on tree kernels achieved the second official 2 Note that in that paper the naming convention is slightly different. The fresh"
S16-1138,2006.amta-papers.25,0,0.0422727,"ets. The second contrastive submission consists of a rule-based system which relies on the outputs from tasks A and B. A comment is labeled as good if it is considered good with respect to the related question (Task A) and the related question is considered relevant with respect to the new question (Task B). The comment is considered bad otherwise. Task D. The contrastive systems did not use tree kernels. Our first contrastive run used only feature vectors. Our second contrastive run also used additional features borrowed from machine translation evaluation: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), Meteor (Lavie and Denkowski, 2009), NIST (Doddington, 2002), Precision and Recall, and length ratio between the question and the comment. 5 Results and Discussion Table 1 shows the results obtained in the four tasks. We achieved the second position for tasks A, B, and D. In Task A, tree kernels give no major boost, but without them our model would be cont2 , which 900 achieved the third position on the test set. The joint model cont1 , run on top of our primary system, was able to improve it by more than one point. We were not sure about the outcome of this model, thus we preferred not to us"
S16-1138,E14-1070,1,0.868985,"t al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to tackle the new challenges of the SemEval-2016 Task 3. Indeed, all our three systems using relational models based on tree kernels achieved the second official 2 Note that in that paper the naming convention is slightly different. The fresh user question and the forum question are called"
S16-1138,N16-1152,1,0.718101,"eature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to tackle the new challenges of the SemEval-2016 Task 3. Indeed, all our three systems using relational models based on tree kernels achieved the second official 2 Note that in that paper the naming convention is slightly different. The fresh user question and the forum question are called “original” and “related”, respectively. 896 Proceedings of SemEval-2016, pages 896–903, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics position. In contrast, for Task C, we did n"
S16-1138,N07-1051,0,\N,Missing
S16-1138,W14-5201,0,\N,Missing
S17-2019,S16-1081,0,0.0491886,"enough before the deadline and we could not include the results. 3 145 2017 Track 1 2 3 4 5 6 total L–L0 ar–ar ar–en es–es es–en en–en tr–en Instances 1, 081 2, 162 1, 555 1, 595 14, 778 0∗ 21, 171 Pctge. 5.11 10.21 7.34 7.53 69.80 0.00 100 3 For training, we used all the annotated datasets released both in the current and in previous editions.6 Table 1 shows the size of the different language collections. Note the important imbalance: there are more than ten times more instances available in en only than in the rest of languages. We used the test set from the 2016 edition (only in English) (Agirre et al., 2016) as our internal test set. Using the features in Sections 2.1 to 2.6, we train two regressors by: Sys1 learning one SVM per each language pair Sys2 learning one single SVM for all the language pairs together. We experiment with a third system using all the extensions of Section 2.7 on XGBoost. The purpose of this system is to analyse and compare different assumptions made for Sys1 and Sys2: Sys3 learning one single XGB for all the language pairs with an extended set of features. Table 1: Instances provided in the history of STS. (∗ No training data exists for this pair.) package (Mikolov et al"
S17-2019,Q17-1024,0,0.0640267,"Missing"
S17-2019,W15-3402,1,0.841459,"Missing"
S17-2019,S17-2001,0,0.0323469,"sed six binary features that mark the languages of each pair. lang1, lang2 and lang3 are set to 1 if s is written in either ar, en, or es, respectively. The other three features, lang4, lang5, and lang6, provide the same information for t. For instance, the value for the six features for a pair en–ar would be 0 1 0 1 0 0. Introduction The Semantic Textual Similarity (STS) task poses the following challenge. Let s and t be two text snippets. Determine the degree of equivalence α(s, t) |α ∈ [0, 5]. Whereas 0 represents complete independence, 5 reflects semantic equivalence. The current edition (Cer et al., 2017) includes the monolingual ar–ar, en–en, and es– es, as well as the cross-language ar–en, es– en, and tr–enlanguage pairs. We use the twoletter ISO 639-1 codes: ar=Arabic, en=English, es=Spanish, and tr=Turkish. Multilinguality is the premise of the Lump approach: we use representations which lie towards language-independence as we aim to be able to approach similar tasks on other languages, paying the least possible effort. Our regression model relies on different kinds of features, from simple length-based and lexical similarities to more sophisticated embeddings and deep neural net internal"
S17-2019,2009.mtsummit-posters.15,0,0.0273006,"l neural machine translation (NMT) system to obtain a representation in a common space for sentences in all the languages. We build the NMT system in the same philosophy of Johnson et al. (2016) using and adapting the Nematus engine (Sennrich et al., 2016). The multilingual system is able to translate between any combination of languages ar, en, and es. It was trained on 60 k parallel sentences (20 k per language pair) using 512-dimensional word embeddings, 1024 hidden units, a minibatch of 200 samples, and applying Adadelta optimisation. The parallel corpus includes data from United Nations (Rafalovitch and Dale, 2009), Common Crawl2 , News Commentary3 and IWSLT.4 We are not interested in the translations but in the context vectors output of the hidden layers of the encoder, as these are supposed to have learnt an interlingua representation of the input. We compute the cosine similarity between 2048dimensional context vectors from the internal representation when the encoder is fed with s and t. Two independent systems, one trained with words and another one trained with lemmas5 provide our two features lN M T and wN M T . Lexical Similarities (5 feats.) We compute cosine similarities between character n-gr"
S17-2019,1992.tmi-1.7,0,0.718314,"epresentations of s and t, with n = [2, 5] (2grm,. . .,5grm). The pre-processing in this case is casefolding and diacritics removal. The fifth feature cog is the cosine similarity computed over “pseudo-cognate” representations. From an NLP point of view, cognates are “words that are similar across languages” (Manning and Sch¨utze, 1999). We relax this concept and consider as pseudocognates any words in two languages that share prefixes. To do so, we discard tokens shorter than four characters, unless they contain nonalphabetical characters, and cut off the resulting tokens to four characters (Simard et al., 1992). This kind of representations is used on European languages with similar alphabets (McNamee and Mayfield, 2004; Simard et al., 1992). We apply Buckwalter transliteration to texts in ar and remove vowels from the snippets written in latin alphabets. For the pseudo-cognates computations, we use three characters instead of four. 2.4 Context Vectors in a Neural Machine Translation Engine (2 feats.) 2.6 Embeddings for Babel Synsets (2 feats.) BabelNet is a multilingual semantic network connecting concepts via Babel synsets (Navigli and Ponzetto, 2012). Each concept, or word, is identified by its I"
S19-2176,D18-1389,1,0.901408,"Missing"
S19-2176,N19-1216,1,0.837505,"Missing"
S19-2176,N19-1423,0,0.0692818,"Missing"
S19-2176,S19-2145,0,0.0296054,"Jack Ryder team to SemEval-2019 Task 4 on Hyperpartisan News Detection. The task asked participants to predict whether a given article is hyperpartisan, i.e., extreme-left or extremeright. We propose an approach based on BERT with fine-tuning, which was ranked 7th out 28 teams on the distantly supervised dataset, where all articles from a hyperpartisan/nonhyperpartisan news outlet are considered to be hyperpartisan/non-hyperpartisan. On a manually annotated test dataset, where human annotators double-checked the labels, we were ranked 29th out of 42 teams. 1 Introduction SemEval-2019 Task 4 (Kiesel et al., 2019) asks to distinguish between articles that are extremely one-sided, i.e., extreme-left or extreme-right, and such that are not. The organizers provided two datasets: 1. By article: A small dataset of 645 manually annotated articles (BA in the following). 2. By publisher: A large dataset of 750,000 articles annotated using distant supervision, where an article is considered hyperpartisan if its source is labeled as such (BP in the following). The set is separated into 600,000 articles for training (BP-train) and 150,000 articles for validation (BP-val). Furthermore, two test sets, one annotated"
S19-2176,D18-1388,0,0.0217847,"Missing"
S19-2176,P18-1022,0,0.0439102,"Missing"
S19-2176,P17-1068,0,0.0567789,"Missing"
S19-2176,D17-1317,0,0.0383943,"Missing"
S19-2176,D13-1010,0,0.0818406,"Missing"
S19-2176,P14-1105,0,0.0706,"Missing"
S19-2182,D18-1389,1,0.904156,"Missing"
S19-2182,N19-1216,1,0.832379,"Missing"
S19-2182,N18-2004,1,0.896708,"Missing"
S19-2182,C18-1158,0,0.0258872,"al election (Brill, 2001; Finberg et al., 2002; Castillo et al., 2011; Baly et al., 2018a; Kulkarni et al., 2018; Mihaylov et al., 2018; Baly et al., 2019). Most approaches have focused on predicting credibility, bias or stance. Stance detection was considered as an intermediate step for detecting fake claims, where the veracity of a claim is checked by aggregating the stances of the retrieved relevant articles (Baly et al., 2018b; Nakov et al., 2019). Several stance detection models have been proposed including deep convolutional neural networks (Baird et al., 2017), multi-layer perceptrons (Hanselowski et al., 2018), and end-to-end memory networks (Mohtarami et al., 2018). 1041 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1041–1046 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics The stylometric analysis model of Koppel et al. (2007) was used by Potthast et al. (2018) to address hyperpartisanship. They used articles from nine news sources whose factuality has been manually verified by professional journalists. Writing style and complexity were also considered by Horne and Adal (2017) to differentiate real news from"
S19-2182,S19-2145,0,0.0557502,"distant supervision. Additional experiments showed that significant performance gains can be achieved with better feature pre-processing.1 1 Introduction The rise of social media has enabled people to easily share information with a large audience without regulations or quality control. This has allowed malicious users to spread disinformation and misinformation (a.k.a. “fake news”) at an unprecedented rate. Fake news is typically characterized as being hyperpartisan (one-sided), emotional and riddled with lies (Potthast et al., 2018). The SemEval-2019 Task 4 on Hyperpartisan News Detection (Kiesel et al., 2019) focused on the challenge of automatically identifying whether a text is hyperpartisan or not. While hyperpartisanship is defined as “exhibiting one or more of blind, prejudiced, or unreasoning allegiance to one party, faction, cause, or person”, we model this task as a binary document classification problem. Scholars have argued that all biased messages can be considered propagandistic, regardless of whether the bias was intentional or not (Ellul, 1965, p. XV). 1 Our system is available at https://github.com/ AbdulSaleh/QCRI-MIT-SemEval2019-Task4 Thus, we approached the task departing from an"
S19-2182,D18-1388,0,0.165557,"Missing"
S19-2182,N18-1070,1,0.913935,"Missing"
S19-2182,H05-1044,0,0.0505862,"1042 X biasi (Dj ) = count(cue, Dj ) cue∈BLi X wk ∈Dj (3) count(wk , Dj ) Lexicon-based Features. Rashkin et al. (2017) studied the occurrence of specific types of words in different kinds of articles, and showed that words from certain lexicons (e.g., negation and swear words) appear more frequently in propaganda, satire, and hoax articles than in trustworthy articles. We capture this by extracting features that reflect the frequency of words from particular lexicons. We use 18 lexicons from Wiktionary, Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001), Wilson’s subjectives (Wilson et al., 2005), Hyland’s hedges (Hyland, 2015), and Hooper’s assertives (Hooper, 1975). For each lexicon, we count the total number of words in the article that appear in the lexicon. This resulted in 18 features, one for each lexicon. Vocabulary Richness Potthast et al. (2018) showed that hyperpartisan outlets tend to use a writing style that is different from mainstream outlets. Different topic-independent features have been proposed to characterize the vocabulary richness, style and complexity of a text. For this task, we used the following vocabulary richness features: (i) type–token ratio (TTR), or the"
S19-2182,P18-1022,0,0.211987,"d by aggregating the stances of the retrieved relevant articles (Baly et al., 2018b; Nakov et al., 2019). Several stance detection models have been proposed including deep convolutional neural networks (Baird et al., 2017), multi-layer perceptrons (Hanselowski et al., 2018), and end-to-end memory networks (Mohtarami et al., 2018). 1041 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1041–1046 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics The stylometric analysis model of Koppel et al. (2007) was used by Potthast et al. (2018) to address hyperpartisanship. They used articles from nine news sources whose factuality has been manually verified by professional journalists. Writing style and complexity were also considered by Horne and Adal (2017) to differentiate real news from fake news and satire. They used features such as the number of occurrences of different part-of-speech tags, swearing and slang words, stop words, punctuation, and negation as stylistic markers. They also used a number of readability measures. Rashkin et al. (2017) focused on a multi-class setting (real news vs. satire vs. hoax vs. propaganda) a"
S19-2182,D17-1317,0,0.206809,"l Linguistics The stylometric analysis model of Koppel et al. (2007) was used by Potthast et al. (2018) to address hyperpartisanship. They used articles from nine news sources whose factuality has been manually verified by professional journalists. Writing style and complexity were also considered by Horne and Adal (2017) to differentiate real news from fake news and satire. They used features such as the number of occurrences of different part-of-speech tags, swearing and slang words, stop words, punctuation, and negation as stylistic markers. They also used a number of readability measures. Rashkin et al. (2017) focused on a multi-class setting (real news vs. satire vs. hoax vs. propaganda) and relied on word n-grams. Similarly to Potthast et al. (2018), we believe that there is an inherent style in propaganda, regardless of the source publishing it. Many stylistic features were proposed for authorship identification, i.e., the task of predicting whether a piece of text has been written by a particular author. One of the most successful representations for such a task are character-level n-grams (Stamatatos, 2009), and they turn out to represent some of our most important stylistic features. More det"
S19-2182,C18-1283,0,0.0315537,"e believe that there is an inherent style in propaganda, regardless of the source publishing it. Many stylistic features were proposed for authorship identification, i.e., the task of predicting whether a piece of text has been written by a particular author. One of the most successful representations for such a task are character-level n-grams (Stamatatos, 2009), and they turn out to represent some of our most important stylistic features. More details about research on fact-checking and the spread of fake news online can be found in recent surveys (Lazer et al., 2018; Vosoughi et al., 2018; Thorne and Vlachos, 2018). 3 System Description We developed our system for detecting hyperpartisanship in news articles by training a logistic regression classifier using features such as character and word n-grams, lexicon-based indicators, and readability and vocabulary richness measures. Below, we describe these features in detail. Character 3-grams. Stamatatos (2009) argued that, for tasks where the topic is irrelevant, character-level representations are more sensitive than token-level ones. We hypothesize that this applies to hyperpartisan news detection, since articles on both sides of the political spectrum m"
S19-2182,P02-1053,0,0.0234741,"= α + i:yi =0 xi , and we set the smoothing parameter α to 1. Finally, we calculate the vector:  r = log p/ k p k q/ k q k  (1) which is used to scale the TF.IDF features to create the NB-TF.IDF features as follows: x0i = r ◦ xi , ∀i (2) Bias Analysis We analyze the bias in the language used in the documents by (i) creating bias lexicons that contain left and right bias cues, and (ii) using these lexicons to compute two scores for each document, indicating the intensity of bias towards each ideology. To generate the list of cues that signal biased language, we use Semantic Orientation (SO) (Turney, 2002) to identify the words that are strongly associated with each of the left and right documents in the training dataset. Those SO values can be either positive or negative, indicating association with right or left biases, respectively. Then, we select words whose absolute SO value is ≥ 0.4 to create two bias lexicons: BLlef t and BLright . Finally, we use these lexicons to compute two bias scores per document according to Equation (3), where for each document Dj , the frequency of cues in the lexicon BLi that are present in Dj is normalized by the total number of words in Dj : 1042 X biasi (Dj"
S19-2182,P12-2018,0,0.0995675,"Missing"
sidorov-etal-2010-english,J90-2002,0,\N,Missing
sidorov-etal-2010-english,J03-1002,0,\N,Missing
sidorov-etal-2010-english,atserias-etal-2006-freeling,0,\N,Missing
W13-2215,W11-2107,0,0.0192041,"E models we used the data from the WMT13 shared task on quality estimation (System Selection Quality Estimation at Sentence Level task5 ), which contains the test sets from other WMT campaigns with human assessments. We used five groups of features, namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the follow"
W13-2215,W12-3133,1,0.860966,"Missing"
W13-2215,2012.amta-monomt.1,1,0.796291,"Missing"
W13-2215,2013.mtsummit-papers.9,1,0.774503,"Missing"
W13-2215,padro-etal-2010-freeling,0,0.0202206,"Missing"
W13-2215,W06-1607,0,0.0198766,"s with additional information, such as POS tags or lemmas. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing the construction of factor-specific language models with higher-order n-grams. Such language models can help to obtain syntactically more correct outputs. We used the standard models available in Moses as feature functions: relative frequencies, lexical weights, word and phrase penalties, wbe-msdbidirectional-fe reordering models, and two language models (one for surface and one for POS tags). Phrase scoring was computed using GoodTuring discounting (Foster et al., 2006). As aforementioned, we developed five factored Moses-based independent systems with different 134 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 134–140, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the"
W13-2215,P02-1040,0,0.0862502,"on Statistical Machine Translation, pages 134–140, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation"
W13-2215,D08-1090,0,0.0227644,"namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the following parameters: linear kernel, expanding the working set by 9 variables at each iteration, for a maximum of 50,000 iterations and with a cache size of 100 for kernel evaluations. The trade-off parameter was empirically set to 0.001. Table 2 sh"
W13-2215,2011.eamt-1.18,1,0.897081,"Missing"
W13-2215,P10-1063,0,0.0150842,"task5 ), which contains the test sets from other WMT campaigns with human assessments. We used five groups of features, namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the following parameters: linear kernel, expanding the working set by 9 variables at each iteration, for a maximum of 50,000 iterations and"
W13-2215,D11-1125,0,0.0166218,"nal Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation Task’s website1 . We used all the News corpora to busld the language model (LM). Firstly, a LM was built for every cor"
W13-2215,P07-2045,0,0.10399,"such as morphology generation, training sentence filtering, and domain adaptation through unit derivation. The results show a coherent improvement on TER, METEOR, NIST, and BLEU scores when compared to our baseline system. 1 Introduction 2 The TALP-UPC center (Center for Language and Speech Technologies and Applications at Universitat Polit`ecnica de Catalunya) focused on the English to Spanish translation of the WMT13 shared task. Our primary (contrastive) run is an internal system selection comprised of different training approaches (without CommonCrawl, unless stated): (a) Moses Baseline (Koehn et al., 2007b), (b) Moses Baseline + Morphology Generation (Formiga et al., 2012b), (c) Moses Baseline + News Adaptation (Henr´ıquez Q. et al., 2011), (d) Moses Baseline + News Adaptation + Morphology Generation , and (e) Moses Baseline + News Adaptation + Filtered CommonCrawl Adaptation (Barr´on-Cede˜no et al., 2013). Our secondary run includes is the full training strategy marked as (e) in the previous description. The main differences with respect to our last year’s participation (Formiga et al., 2012a) are: i) the inclusion of the CommonCrawl corpus, using Baseline system: Phrase-Based SMT Our contrib"
W13-2215,2005.mtsummit-papers.11,0,0.0437825,"s for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation Task’s website1 . We used all the News corpora to busld the language model (LM). Firstly, a LM was built for every corpus independently. Afterwards, they were combined to produce de final LM. For internal testing we used the News 2011 and News 2012 data and concatenated the remaining three years of News data as a single parallel corpus for development. We processed the corpora as in our participation to WMT12 (Formiga et al"
W13-2215,P03-1021,0,\N,Missing
W13-2215,2011.eamt-1.20,1,\N,Missing
W13-2244,2012.amta-papers.13,1,0.318131,"Missing"
W13-2244,2013.mtsummit-papers.9,1,0.683064,"Missing"
W13-2244,P10-1063,0,0.0136334,"indicators. Secondly, we used Random Forests (Breiman, 2001), the rationale was the same as ranking-topairwise implementation from SVMlight . However, SVMlight considers two different data preprocessing methods depending on the kernel of the classifier: LINEAR and RBF-Kernel. We used the same data-preprocessing algorithm from SVMlight in order to train a Random Forest classifier with ties (three classes: {0,-1,1}) based upon the pairwise relations. We used the Random Forests implementation of scikit-learn toolkit (Pedregosa et al., 2011) with 50 estimators. 3.2 Pseudo-Reference-based Features Soricut and Echihabi (2010) introduced the concept of pseudo-reference-based features (PR) for translation ranking estimation. The principle is that, in the lack of human-produced references, automatic ones are still good for differentiating good from bad translations. One or more secondary MT systems are required to generate translations starting from the same input, which are Once the classes are given by the Random For360 Note that a considerable amount of the features described in the baseline group (QQE and AQE) fall in this category. In this subsection we include some extra features we devised to capture source– t"
W13-2244,P12-3024,1,0.541573,"Missing"
W13-2244,W05-0635,0,0.0232696,"measure for “pseudo-prefixes” (considering only up to four initial characters for every token). 3.4 We interpolated different language models comprising the WMT’12 Monolingual corpora (EPPS, News, UN and Gigafrench for English). The interpolation weights were computed as to minimize the perplexity according to the WMT Translation Task test data (2008-2010)4 . The features are as follow: 5. Based on semantic information (SEM) Twelve features calculated with named entity- and semantic role-based evaluation measures (again, provided by A SIYA). Sentences are automatically annotated using SwiRL (Surdeanu and Turmo, 2005) and BIOS (Surdeanu et al., 2005). We used these features in the de-en subtask only. 9. Language Model Features (LM) Two log-probabilities of the translation candidate with respect to the above described interpolated language models over word forms and PoS labels. 6. Explicit semantic analysis (ESA) Two versions of explicit semantic analysis (Gabrilovich and Markovitch, 2007), a semantic similarity measure, built on top of Wikipedia (we used the opening paragraphs of 100k Wikipedia articles as in 2010). 3.3 Adapted Language-Model Features 4 Experiments and Results In this section we describe t"
W13-2244,P04-1077,0,0.0383445,"Missing"
W13-2244,N03-2021,0,0.0179653,"be calculated with any evaluation measure or text similarity function, which gives us all feature variants in this group. We consider the following PR-based features: 7. Alignment-based features (ALG / ALGPR) One measure calculated over the aligned words between a candidate translation and the source (ALG); and two measures based on the comparison between these alignments for two different translations (e.g., candidate and pseudo-reference) and the source (ALGPR).3 3. Derived from A SIYA’s metrics (APR) Twenty-three PR features, including GTM-l (l∈{1,2,3}) to reward different length matching (Melamed et al., 2003), four variants of ROUGE (-L, -S*, -SU* and -W) (Lin and Och, 2004), WER (Nießen et al., 2000), PER (Tillmann et al., 1997), TER, and TERbase (i.e., without stemming, synonymy look-up, nor paraphrase support) (Snover et al., 2009), and all the shallow and full parsing measures (i.e., constituency and dependency parsing, PoS, chunking and lemmas) that A SIYA provides either for Spanish or English as target languages. 8. Length model (LeM) A measure to estimate the quality likelihood of a candidate sentence by considering the “expected length” of a proper translation from the source. The measure"
W13-2244,niessen-etal-2000-evaluation,0,0.0675224,"re variants in this group. We consider the following PR-based features: 7. Alignment-based features (ALG / ALGPR) One measure calculated over the aligned words between a candidate translation and the source (ALG); and two measures based on the comparison between these alignments for two different translations (e.g., candidate and pseudo-reference) and the source (ALGPR).3 3. Derived from A SIYA’s metrics (APR) Twenty-three PR features, including GTM-l (l∈{1,2,3}) to reward different length matching (Melamed et al., 2003), four variants of ROUGE (-L, -S*, -SU* and -W) (Lin and Och, 2004), WER (Nießen et al., 2000), PER (Tillmann et al., 1997), TER, and TERbase (i.e., without stemming, synonymy look-up, nor paraphrase support) (Snover et al., 2009), and all the shallow and full parsing measures (i.e., constituency and dependency parsing, PoS, chunking and lemmas) that A SIYA provides either for Spanish or English as target languages. 8. Length model (LeM) A measure to estimate the quality likelihood of a candidate sentence by considering the “expected length” of a proper translation from the source. The measure was introduced by (Pouliquen et al., 2003) to identify document translations. We estimated it"
W14-3351,W07-0738,1,0.886748,"Missing"
W14-3351,P91-1022,0,0.304658,"text against the translations. The metrics can be divided into two subsets: those that do not require any external resources (Section 3.1) and those that depend on a parallel corpus (Section 3.2). 3.1 The length factor (LeM) is rooted in the fact that the length of a text and its translation tend to preserve a certain length correlation. For instance, translations from English into Spanish or French tend to be longer than their source. Similar measures were proposed during the statistical machine translation early days, both considering characterand word-level lengths (Gale and Church, 1993; Brown et al., 1991). Pouliquen et al. (2003) defines the length factor as: Language-Independent Resource-Free Metrics We opted for two characterisations that allow for the comparison of texts across languages without external resources nor language-related knowledge —as far as the languages use the same writing system.4 The first characterisation is character n-grams; proposed by McNamee and Mayfield (2004) for cross-language information retrieval between European languages. Texts are broken down into overlapping character sequences of length n, with 1-character shifting. We opt for case-folded bigrams (NGRAM-co"
W14-3351,candito-etal-2010-statistical,0,0.0198857,"may make unfair comparisons when they are not able to reflect 2 Reference-based Metrics We recently added a new set of metrics to A SIYA, which estimate the similarity between reference (ref ) and candidate (cand) translations. The met1 http://asiya.lsi.upc.edu 394 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 394–401, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics rics rely either on structural linguistic information (Section 2.1), on a semantic mapping (Section 2.2), or on word n-grams (Section 2.3). 2.1 Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gi"
W14-3351,P12-3024,1,0.897396,"Missing"
W14-3351,C10-2013,0,0.0192448,"may make unfair comparisons when they are not able to reflect 2 Reference-based Metrics We recently added a new set of metrics to A SIYA, which estimate the similarity between reference (ref ) and candidate (cand) translations. The met1 http://asiya.lsi.upc.edu 394 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 394–401, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics rics rely either on structural linguistic information (Section 2.1), on a semantic mapping (Section 2.2), or on word n-grams (Section 2.3). 2.1 Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gi"
W14-3351,P13-4031,1,0.877708,"Missing"
W14-3351,P05-1022,0,0.0280345,"for each part of speech. We also use NIST (Doddington, 2002) to compute accumulated scores over sequences of n = 1..5 parts of speech (SP-pNIST). Similarly, CP metrics analyse similarities between constituent parse trees associated to candidate and reference translations. For instance, CP-STMi5 and CP-STM4 compute, respectively, the proportion of (individual) length-5 and accumulated up to length-4 matching sub-paths of the syntactic tree (Liu and Gildea, 2005). CP-Oc(*) computes the lexical overlap averaged over all the phrase constituents. Constituent trees are obtained using the parsers of Charniak and Johnson (2005), 2.2 Explicit-Semantics Metric Additionally, we borrowed a metric originally proposed in the field of Information Retrieval: explicit semantic analysis (ESA) (Gabrilovich and Markovitch, 2007). ESA is a similarity metric that relies on a large corpus of general knowledge to represent texts. Our knowledge corpora are composed of ∼ 100K Wikipedia articles from 2010 for the following target languages: English, French and German. In this case, ref and cand translations are both mapped onto the Wikipedia collection W . The similarities between each text and every article a ∈ W are computed on the"
W14-3351,W11-2107,0,0.0387306,"that transliteration is a good short-cut when dealing with different writing systems (Barr´on-Cede˜no et al., 2014). 5 396 https://code.google.com/p/berkeleyaligner conducted considering the WMT13 Metrics Task dataset (Mach´acˇ ek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the A SIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). The metric sets included in UPC-IPA are light version"
W14-3351,W05-0904,0,0.179329,"al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gildea, 2005) up to length 3; and DPm-HWCMi c-3 computes the proportion of matching category-chains of length 3. Parsing-based Metrics Our initial set of parsing-based metrics is a followup of the proposal by Gim´enez and M`arquez (2010b): it leverages the structural information provided by linguistic processors to compute several similarity cues between two analyzed sentences. A SIYA includes plenty of metrics that capture syntactic and semantic aspects of a translation. New metrics based on linguistic structural information for French and German and upgraded versions of the parsers for English and Spanis"
W14-3351,W13-2202,0,0.0317518,"the alignments src–cand and src–ref (ALGNr); and (iii) the ratio of shared alignments between src– cand and src–ref (ALGNp). Parallel-Corpus Metrics We consider two metrics that make use of parallel corpora: length factor and alignment. 4 Experimental Results The tuning and selection of the different metrics to build UPC-IPA and UPC-STOUT was 4 Previous research showed that transliteration is a good short-cut when dealing with different writing systems (Barr´on-Cede˜no et al., 2014). 5 396 https://code.google.com/p/berkeleyaligner conducted considering the WMT13 Metrics Task dataset (Mach´acˇ ek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the A SIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those result"
W14-3351,W14-3336,0,0.026961,"ain over the baseline, the SEM ones do not. Finally, it merits some attention the good results achieved by the baseline for translations into English. We may remark here that our baseline included also the best performing state-of-the-art metrics, including all the variants of METEOR, that reported good results in the WMT13 challenge. Tables 6 and 7 show the official results obtained by UPC-IPA and UPC-STOUT in WMT14.8 The best and worst figures for each language pair are included for comparison —the worst performing submission at segment level is neglected as it seems to be a dummy (Mach´acˇ ek and Bojar, 2014 to appear). Both UPC-IPA and UPC-STOUT configurations resulted in different performances depending on the language pair. UPC-STOUT scored above the average for all the language pairs except for en–cs at both system and segment level, and en–ru at system level. Although the evaluation results are not directly comparable to the WMT13 ones, one can note that the results were notably better for pairs that involved Czech and Russian, and worse for those that involved French and German at system level. Analysing the impact of the evaluation methods and building comparable results in order to addres"
W14-3351,N03-2021,0,0.0556188,"ort-cut when dealing with different writing systems (Barr´on-Cede˜no et al., 2014). 5 396 https://code.google.com/p/berkeleyaligner conducted considering the WMT13 Metrics Task dataset (Mach´acˇ ek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the A SIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). The metric sets included in UPC-IPA are light versions of the UPC-STOUT ones. The"
W14-3351,niessen-etal-2000-evaluation,0,0.0602984,"e.google.com/p/berkeleyaligner conducted considering the WMT13 Metrics Task dataset (Mach´acˇ ek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the A SIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). The metric sets included in UPC-IPA are light versions of the UPC-STOUT ones. They were composed following different criteria, depending on the translation direction. Parsing-based"
W14-3351,N07-1051,0,0.00964749,"ference-based Metrics We recently added a new set of metrics to A SIYA, which estimate the similarity between reference (ref ) and candidate (cand) translations. The met1 http://asiya.lsi.upc.edu 394 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 394–401, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics rics rely either on structural linguistic information (Section 2.1), on a semantic mapping (Section 2.2), or on word n-grams (Section 2.3). 2.1 Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gildea, 2005) up to length 3; and DPm-HWCMi c-3 computes the proportion"
W14-3351,P06-1055,0,0.00957742,"able to reflect 2 Reference-based Metrics We recently added a new set of metrics to A SIYA, which estimate the similarity between reference (ref ) and candidate (cand) translations. The met1 http://asiya.lsi.upc.edu 394 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 394–401, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics rics rely either on structural linguistic information (Section 2.1), on a semantic mapping (Section 2.2), or on word n-grams (Section 2.3). 2.1 Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gildea, 2005) up to length 3; and DPm-HWCMi c-"
W14-3351,1992.tmi-1.7,0,0.718175,"Missing"
W14-3351,W09-0441,0,0.105035,"ent writing systems (Barr´on-Cede˜no et al., 2014). 5 396 https://code.google.com/p/berkeleyaligner conducted considering the WMT13 Metrics Task dataset (Mach´acˇ ek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the A SIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). The metric sets included in UPC-IPA are light versions of the UPC-STOUT ones. They were composed following diffe"
W14-3351,J93-1004,0,\N,Missing
W14-3351,W09-0440,1,\N,Missing
W14-3351,W13-2201,0,\N,Missing
W15-3223,darwish-etal-2014-using,1,0.8246,"atching binary feature fires. We found such ranking scores to be a valuable addition in our experiments. To understand why, we note that they are able to neatly separate the different labels, with the following average scores: DIRECT 14.5, RELATED 12.3, and IRRELEVANT 10.5. 3.4 In addition to the machine learning approaches, we adapted our rule-based model, which ranked 2nd in the competition (Nicosia et al., 2015). The basic idea is to rank the comments according to their similarity and label the top ones as DIRECT . In this case our preprocessing consists of stemming, performed with QATARA (Darwish et al., 2014), and again stopword removal. In our implementation, the score of a comment is computed as 1 X score(c) = α · ω(t) + pos(t) |q |t∈q∩c Similarity This set of features measures the similarity sim(q, c) between a question and a comment, assuming that high similarity signals a DIRECT answer. We compute the similarity between word n-gram representations (n = [1, . . . , 4]) of q and c, using different lexical similarity measures: greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosi"
W15-3223,N13-1090,0,0.0265105,"cal setting, first discriminating between IRRELEVANT and NON-IRRELEVANT and then between DIRECT and RELATED ; and (ii) a multiclass classification setting. Their third approach was based on an ensemble of classifiers. 3.1 Vectors Our motivation for using word vectors for this task is that they convey a soft representation of word meanings. In contrast to similarity measures that are based on words, using word vectors has the potential to bridge over lack of lexical overlap between questions and answers. We start by creating word vectors from a large corpus of raw Arabic text. We use Word2Vec (Mikolov et al., 2013b; Mikolov et al., 2013a) with default settings for creating 100-dimensional vectors. We experimented with the Arabic Gigaword (Linguistic Data Consortium, 2011), containing newswire text, and with the King Saud University Corpus of Classical Arabic (KSUCCA), containing classical Arabic text (Alrabiah et al., 2013). Table 2 provides some statistics for these corpora. We were initially expecting KSUCCA to produce better results, beFinally, Mohamed et al., (2015) applied a decision tree whose output is composed of lexical and enriched representations of q and c: the terms in the texts are expand"
W15-3223,S15-2040,0,0.0217317,"exical overlap between questions and answers. We start by creating word vectors from a large corpus of raw Arabic text. We use Word2Vec (Mikolov et al., 2013b; Mikolov et al., 2013a) with default settings for creating 100-dimensional vectors. We experimented with the Arabic Gigaword (Linguistic Data Consortium, 2011), containing newswire text, and with the King Saud University Corpus of Classical Arabic (KSUCCA), containing classical Arabic text (Alrabiah et al., 2013). Table 2 provides some statistics for these corpora. We were initially expecting KSUCCA to produce better results, beFinally, Mohamed et al., (2015) applied a decision tree whose output is composed of lexical and enriched representations of q and c: the terms in the texts are expanded on the basis of a set of Quranic ontologies. The authors do not report the 185 cause its language should be more similar to the religious texts in the Fatwa corpus. However, in practice we found vectors trained on the Arabic Gigaword to perform better, possibly thanks to its larger coverage, so we report only results with the Gigaword corpus below. We noticed in preliminary experiments that many errors are due to lack of overlap in vocabulary between answers"
W15-3223,S15-2035,0,0.027656,"easures, statistical ranking, and rule-based ranking. We describe each kind in turn. Belinkov et al., (2015)’s best submission was very similar to the one of Nicosia et al., (2015): a ranking approach based on confidence values obtained by an SVM ranker (Joachims, 2006). Their second approach consisted of a multi-class linear SVM classifier relying on three feature families: (i) lexical similarities between q and c (similar to those applied by the previous team); (ii) word vector representations of q and c; and (iii) a ranking score for c produced by the SVM ranker. The two best approaches of Hou et al., (2015) used features representing different similarities between q and c, lengths of words and sentences, and the number of named-entities in c, among others. In this case [1,2,3]-grams were also considered as features, but with two differences with respect to the other participants: only the most frequent n-grams were used and a translated version to English was also included. They explored two strategies using SVMs in their top performing submissions: (i) a hierarchical setting, first discriminating between IRRELEVANT and NON-IRRELEVANT and then between DIRECT and RELATED ; and (ii) a multiclass c"
W15-3223,S15-2047,0,0.269072,"ence Laboratory, Doha, Qatar Cambridge, MA 02139, USA {albarron, hmubarak}@qf.org.qa belinkov@csail.mit.edu Abstract ing” (Nakov et al., 2015) and focus on the Arabic language. Our approach is treating each question– comment as an instance in a supervised learning scenario. We build a support vector machine (SVM) classifier that is using different kinds of features, including vector representations, similarity measures, and rankings. Our extensive feature set allows us to achieve better results than those of the winner of the competition: 79.25 F1 compared to 78.55, obtained by Nicosia et al. (2015). The rest of the paper is organized as follows. Section 2 describes the experimental framework —composed of the Fatwa corpus and the evaluation metrics— and overviews the different models proposed at competition time. Section 3 describes our model. Experiments and results are discussed in Section 4. Related work is discussed in Section 5. We summarize our contributions in Section 6, and include an error analysis in Appendix A. The task of answer selection in community question answering consists of identifying pertinent answers from a pool of user-generated comments related to a question. The"
W15-3223,S15-2036,1,0.877926,"Missing"
W15-3223,pasha-etal-2014-madamira,0,0.1324,"Missing"
W15-3223,W01-0515,0,0.0181742,"ATARA (Darwish et al., 2014), and again stopword removal. In our implementation, the score of a comment is computed as 1 X score(c) = α · ω(t) + pos(t) |q |t∈q∩c Similarity This set of features measures the similarity sim(q, c) between a question and a comment, assuming that high similarity signals a DIRECT answer. We compute the similarity between word n-gram representations (n = [1, . . . , 4]) of q and c, using different lexical similarity measures: greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. The preprocessing in this case consists only of stopword removal. Additionally, we further compute cosine similarity on lemmas and part-of-speech tags, both including and excluding stopwords. 3.3 Rule-based Ranking where ω(t) = 1 if t is a 1-gram, 4 if it is a 2-gram, and pos(t) represents the relative position of t in the question and is estimated as the length of q minus the position of t in q. That is, we give significantly more relevance to 2-grams and to those matching n-grams at the beginning of the question. We compute this score twice: once considering the subj"
W15-3223,S15-2048,1,\N,Missing
W15-3402,W06-2810,0,0.207288,"Missing"
W15-3402,cui-etal-2008-corpus,0,0.201057,"rpus using IR techniques. They use the characteristic vocabulary of the domain (100 terms extracted from an external in-domain corpus) to query a Lucene search engine4 over the whole encyclopædia. Our approach is completely different: we try to get along with Wikipedia’s structure with a strategy to walk through the category graph departing from a root or pseudo-root category, which defines our domain of interest. We empirically set a threshold to stop exploring the graph such that the included categories most likely represent an entire domain (cf. Section 3). This approach is more similar to Cui et al. (2008), who explore the Wiki-Graph and score every category in order to assess its likelihood of belonging to the domain. Other tools are being developed to extract corpora from Wikipedia. Linguatools5 released a comparable corpus extracted from Wikipedias in 253 language pairs. Unfortunately, neither their tool nor the applied methodology description are available. CatScan26 is a tool that allows to explore and search categories recursively. The Accurat toolkit (Pinnis et al., 2012; S¸tef˘anescu, Dan and Ion, Radu and Hunsicker, Sabine, 2012)7 aligns comparable documents and extracts parallel sente"
W15-3402,W04-3208,0,0.0142898,"for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all. 1 http://en.wikipedia.org/wiki/Help: Category 3 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 3–13, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics 2 Background Sport Comparability in multilingual corpora is a fuzzy concept that has received alternative definitions without reaching an overall consensus (Rapp, 1995; Eagles Document Eag–Tcwg–Ctyp, 1996; Fung, 1998; Fung and Cheung, 2004; Wu and Fung, 2005; McEnery and Xiao, 2007; Sharoff et al., 2013). Ideally, a comparable corpus should contain texts in multiple languages which are similar in terms of form and content. Regarding content, they should observe similar structure, function, and a long list of characteristics: register, field, tenor, mode, time, and dialect (Maia, 2003). Nevertheless, finding these characteristics in real-life data collections is virtually impossible. Therefore, we attach to the following simpler four-class classification (Skadin¸a et al., 2010): (i) Parallel texts are true and accurate translati"
W15-3402,W95-0114,0,0.0936815,"tatistical machine translation engines for specific domains. Our experiments on the English– Spanish pair in the domains of Computer Science, Science, and Sports show that our in-domain translator performs significantly better than a generic one when translating in-domain Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 1 Introduction Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. Comparable corpora were first used for extracting parallel lexicons (Rapp, 1995; Fung, 1995). Later they were used for feeding statistical machine translation (SMT) systems (Uszkoreit et al., 2010) and in multilingual retrieval models (Sch¨onhofen et al., 2007; Potthast et al., 2008). SMT systems estimate the statistical models from bilingual texts (Koehn, 2010). Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage. However, it is evident that no large collections of parallel texts for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available; in some othe"
W15-3402,P02-1040,0,0.0936058,"rallel corpora used to train the SMT systems (top rows) and of the sets used for development and test. CS Sc Sp Un Comp. Europarl 27.99 34.00 30.02 30.63 – c3g cog monoen ¯ S·len union 38.81 57.32 54.27 56.14 64.65 40.53 56.17 52.96 57.40 62.95 46.94 57.60 55.74 58.39 62.65 43.68 58.14 55.17 58.80 64.47 43.68 54.89 52.45 56.78 – Table 8: BLEU scores obtained on the Wikipedia test sets for the 20 specialised systems described in Section 5. A comparison column (Comp.) where all the systems are trained with corpora of the same size is also included (see text). against the BLEU evaluation metric (Papineni et al., 2002). Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a lexicalised reordering. (i) Training systems with Wikipedia or Europarl for domain-specific translation. Table 8 shows the evaluation results on WPtest. All the specialised systems obtain significant improvements with respect to the Europarl system, regardless of their size. For instance, the worst specialised system (c3g with only 95,715 sentences for CS) outperforms by more than 10 points of BLEU the general Europarl translator. The mos"
W15-3402,W11-1212,0,0.211252,"ction (Erdmann et al., 2008), extraction of bilingual dictionaries (Yu and Tsujii, 2009), and identification of particular translations (Chu et al., 2014; Prochasson and Fung, 2011). Different cross-language NLP tasks have particularly taken advantage of Wikipedia. Articles have been used for query translation (Sch¨onhofen et al., 2007) and crosslanguage semantic representations for similarity estimation (Cimiano et al., 2009; Potthast et al., 2008; Sorg and Cimiano, 2012). The extraction of parallel corpora from Wikipedia has been a hot topic during the last years (Adafre and de Rijke, 2006; Patry and Langlais, 2011; Plamada and Volk, 2012; Smith et al., 2010; Tom´as et al., 2008; Yasuda and Sumita, 2008). 3 Domain-Specific Comparable Corpora Extraction In this section we describe our proposal to extract domain-specific comparable corpora from Wikipedia. The input to the pipeline is the top category of the domain (e.g., Sport). The terminology used in this description is as follows. Let c be a Wikipedia category and c∗ be the top category of a domain. Let a be a Wikipedia article; a ∈ c if a contains c among its categories. Let G be the Wikipedia category graph. Vocabulary definition. The domain vocabula"
W15-3402,2005.mtsummit-papers.11,0,0.108055,"to be used for training SMT systems. Some standard parallel corpora have the same order of magnitude. For tasks other than MT, where the precision on the extracted pairs can be more important than the recall, one can obtain cleaner corpora by using a threshold that maximises precision instead of F1 . Evaluation: Statistical Machine Translation Task In this section we validate the quality of the obtained corpora by studying its impact on statistical machine translation. There are several parallel corpora for the English–Spanish language pair. We select as a general-purpose corpus Europarl v7 (Koehn, 2005), with 1.97M parallel sentences. The order of magnitude is similar to the largest corpus we have extracted from Wikipedia, so we can compare the results in a size-independent way. If our corpus extracted from Wikipedia was made up with parallel fragments of the desired domain, it should be the most adequate to translate these domains. If the quality of the parallel fragments was acceptable, it should also help when translating out-of-domain texts. In order to test these hypotheses we analyse three settings: (i) train SMT systems only with Wikipedia (WP) or Europarl (EP) to translate domain-spe"
W15-3402,J10-4005,0,0.0131287,"n Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 1 Introduction Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. Comparable corpora were first used for extracting parallel lexicons (Rapp, 1995; Fung, 1995). Later they were used for feeding statistical machine translation (SMT) systems (Uszkoreit et al., 2010) and in multilingual retrieval models (Sch¨onhofen et al., 2007; Potthast et al., 2008). SMT systems estimate the statistical models from bilingual texts (Koehn, 2010). Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage. However, it is evident that no large collections of parallel texts for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all. 1 http://en.wikipedia.org/wiki/Help: Category 3 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 3–13, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics 2 Background"
W15-3402,J03-1002,0,0.00628859,"eliminate duplicates from this corpus, the size of the union is close to the sum of the individual corpora. This indicates that every similarity measure selects a different set of parallel fragments. Beside the specialised corpus for each domain, we build a larger corpus with all the data (Un). Again, duplicate fragments coming from articles belonging to more than one domain are removed. SMT systems are trained using standard freely available software. We estimate a 5-gram language model using interpolated Kneser–Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with Moses (Koehn et al., 2007). We optimise the feature weights of the model with Minimum Error Rate Training (MERT) (Och, 2003) c3g cog monoen ¯ S·len union WPdev WPtest GNOME CS Sc Sp Un 95,715 182,283 210,664 120,835 577,428 723,760 1,213,965 1,367,169 956,346 3,847,381 334,828 451,324 461,237 389,975 1,181,664 883,366 1,430,962 1,638,777 1,160,977 4,948,241 300 500 1000 300 500 – 300 500 – 900 1500 – Table 7: Number of sentences of the Wikipedia parallel corpora used to train the SMT systems (top rows) and of the sets used for development"
W15-3402,P11-1133,0,0.0249799,"nally, the most related tool to ours: CorpusPedia8 extracts non-aligned, softly-aligned, and strongly-aligned comparable corpora from Wikipedia (Otero and L´opez, 2010). The difference with respect to our model is that they only consider the articles associated to one specific category and not to an entire domain. The inter-connection among Wikipedia editions in different languages has been exploited for multiple tasks including lexicon induction (Erdmann et al., 2008), extraction of bilingual dictionaries (Yu and Tsujii, 2009), and identification of particular translations (Chu et al., 2014; Prochasson and Fung, 2011). Different cross-language NLP tasks have particularly taken advantage of Wikipedia. Articles have been used for query translation (Sch¨onhofen et al., 2007) and crosslanguage semantic representations for similarity estimation (Cimiano et al., 2009; Potthast et al., 2008; Sorg and Cimiano, 2012). The extraction of parallel corpora from Wikipedia has been a hot topic during the last years (Adafre and de Rijke, 2006; Patry and Langlais, 2011; Plamada and Volk, 2012; Smith et al., 2010; Tom´as et al., 2008; Yasuda and Sumita, 2008). 3 Domain-Specific Comparable Corpora Extraction In this section"
W15-3402,P11-2000,0,0.224925,"Missing"
W15-3402,P95-1050,0,0.107949,"m to train statistical machine translation engines for specific domains. Our experiments on the English– Spanish pair in the domains of Computer Science, Science, and Sports show that our in-domain translator performs significantly better than a generic one when translating in-domain Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 1 Introduction Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. Comparable corpora were first used for extracting parallel lexicons (Rapp, 1995; Fung, 1995). Later they were used for feeding statistical machine translation (SMT) systems (Uszkoreit et al., 2010) and in multilingual retrieval models (Sch¨onhofen et al., 2007; Potthast et al., 2008). SMT systems estimate the statistical models from bilingual texts (Koehn, 2010). Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage. However, it is evident that no large collections of parallel texts for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available;"
W15-3402,C10-1124,0,0.028803,"Spanish pair in the domains of Computer Science, Science, and Sports show that our in-domain translator performs significantly better than a generic one when translating in-domain Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 1 Introduction Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. Comparable corpora were first used for extracting parallel lexicons (Rapp, 1995; Fung, 1995). Later they were used for feeding statistical machine translation (SMT) systems (Uszkoreit et al., 2010) and in multilingual retrieval models (Sch¨onhofen et al., 2007; Potthast et al., 2008). SMT systems estimate the statistical models from bilingual texts (Koehn, 2010). Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage. However, it is evident that no large collections of parallel texts for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all. 1 http://en.wikipedia.org/wiki/Help: Category 3 Proceedings of"
W15-3402,1992.tmi-1.7,0,0.831818,"Missing"
W15-3402,I05-1023,0,0.0315766,"nguage pairs exist. In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all. 1 http://en.wikipedia.org/wiki/Help: Category 3 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 3–13, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics 2 Background Sport Comparability in multilingual corpora is a fuzzy concept that has received alternative definitions without reaching an overall consensus (Rapp, 1995; Eagles Document Eag–Tcwg–Ctyp, 1996; Fung, 1998; Fung and Cheung, 2004; Wu and Fung, 2005; McEnery and Xiao, 2007; Sharoff et al., 2013). Ideally, a comparable corpus should contain texts in multiple languages which are similar in terms of form and content. Regarding content, they should observe similar structure, function, and a long list of characteristics: register, field, tenor, mode, time, and dialect (Maia, 2003). Nevertheless, finding these characteristics in real-life data collections is virtually impossible. Therefore, we attach to the following simpler four-class classification (Skadin¸a et al., 2010): (i) Parallel texts are true and accurate translations or approximate"
W15-3402,2009.mtsummit-posters.26,0,0.0424596,"comparable documents and extracts parallel sentences, lexicons, and named entities. Finally, the most related tool to ours: CorpusPedia8 extracts non-aligned, softly-aligned, and strongly-aligned comparable corpora from Wikipedia (Otero and L´opez, 2010). The difference with respect to our model is that they only consider the articles associated to one specific category and not to an entire domain. The inter-connection among Wikipedia editions in different languages has been exploited for multiple tasks including lexicon induction (Erdmann et al., 2008), extraction of bilingual dictionaries (Yu and Tsujii, 2009), and identification of particular translations (Chu et al., 2014; Prochasson and Fung, 2011). Different cross-language NLP tasks have particularly taken advantage of Wikipedia. Articles have been used for query translation (Sch¨onhofen et al., 2007) and crosslanguage semantic representations for similarity estimation (Cimiano et al., 2009; Potthast et al., 2008; Sorg and Cimiano, 2012). The extraction of parallel corpora from Wikipedia has been a hot topic during the last years (Adafre and de Rijke, 2006; Patry and Langlais, 2011; Plamada and Volk, 2012; Smith et al., 2010; Tom´as et al., 200"
W15-3402,N10-1063,0,0.0207021,"ual dictionaries (Yu and Tsujii, 2009), and identification of particular translations (Chu et al., 2014; Prochasson and Fung, 2011). Different cross-language NLP tasks have particularly taken advantage of Wikipedia. Articles have been used for query translation (Sch¨onhofen et al., 2007) and crosslanguage semantic representations for similarity estimation (Cimiano et al., 2009; Potthast et al., 2008; Sorg and Cimiano, 2012). The extraction of parallel corpora from Wikipedia has been a hot topic during the last years (Adafre and de Rijke, 2006; Patry and Langlais, 2011; Plamada and Volk, 2012; Smith et al., 2010; Tom´as et al., 2008; Yasuda and Sumita, 2008). 3 Domain-Specific Comparable Corpora Extraction In this section we describe our proposal to extract domain-specific comparable corpora from Wikipedia. The input to the pipeline is the top category of the domain (e.g., Sport). The terminology used in this description is as follows. Let c be a Wikipedia category and c∗ be the top category of a domain. Let a be a Wikipedia article; a ∈ c if a contains c among its categories. Let G be the Wikipedia category graph. Vocabulary definition. The domain vocabulary represents the set of terms that better c"
W15-3402,zesch-etal-2008-extracting,0,0.105477,"Missing"
W15-3402,2012.eamt-1.37,0,0.108348,"Missing"
W15-3402,P07-2045,0,\N,Missing
W15-3402,P12-3016,0,\N,Missing
W15-3402,tiedemann-2012-parallel,0,\N,Missing
W15-3402,P03-1021,0,\N,Missing
