2005.sigdial-1.13,P94-1002,0,0.119128,"that end, we first briefly discuss the conceptualizations – and motivations behind those conceptualizations – that have arisen in the related fields of segmenting text and monologue. We then discuss previous work in segmenting discourse, our own motivations, and finally (in section 2.1.1) outline an annotation schema derived from these motivations. Text and Monologues The segmenting of text documents is often motivated by information retrieval tasks – for instance, so that a single appropriate segment can be returned matching a query. In some cases, topic boundaries are hand-annotated, as in (Hearst, 1994). However, topic boundaries are often artificially created by concatenating multiple articles together, as in (Galley et al., 2003; Choi, 2000). Moreover, since text is written linearly, usually with clearly punctuated boundaries in the form of sentences and paragraphs, it is natural to assume that topic boundaries will occur at such places. Thus, such “natural” boundaries both define and limit the search space. In addition to text, there has been much research in segmenting non-conversational speech; essentially monologues or series of monologues. For example, much work has been done on autom"
2005.sigdial-1.13,J97-1003,0,0.183084,"ively defined over the concatenated article boundaries. News broadcasts tend to consist primarily of scripted speech – with little spontaneity – produced by highly practiced professionals (though some work has also been done on more spontaneous monologues, see (Passonneau and Litman, 1997)). Topic boundaries in news broadcasts are designed to be obvious, with unambiguous shifts from one story to the next. In both domains, automatic segmentation algorithms tend to rely primarily on lexical co-occurrence statistics to calculate a measure of lexical cohesion between chunks of text (Hearst, 1994; Hearst, 1997). In the case of monologue, prosodic cues are often utilized as well (T¨ur et al., 2001; Hirschberg and Nakatani, 1998). Discourse When turning to spontaneous discourse, most previous work has followed this text/monologue approach: for example, when (Galley et al., 2003) annotated 25 meetings in the ICSI Meeting corpus for topics, the discourse was represented linearly as a series of non-overlapping utterances, topics were represented as a linear sequence of segments, and topic boundaries were allowed only at speaker changes. Although we are aware of one project in which hierarchical topic ann"
2005.sigdial-1.13,J97-1005,0,0.0258596,"done on automatically segmenting broadcast news, e.g. (T¨ur et al., 2001; Beeferman et al., 1999; Allan et al., 1998). The tasks of segmenting text and monologue are similar in that both tend to have fairly well defined topic structure. In the case of artificial text corpora created through concatenation, topic boundaries can be objectively defined over the concatenated article boundaries. News broadcasts tend to consist primarily of scripted speech – with little spontaneity – produced by highly practiced professionals (though some work has also been done on more spontaneous monologues, see (Passonneau and Litman, 1997)). Topic boundaries in news broadcasts are designed to be obvious, with unambiguous shifts from one story to the next. In both domains, automatic segmentation algorithms tend to rely primarily on lexical co-occurrence statistics to calculate a measure of lexical cohesion between chunks of text (Hearst, 1994; Hearst, 1997). In the case of monologue, prosodic cues are often utilized as well (T¨ur et al., 2001; Hirschberg and Nakatani, 1998). Discourse When turning to spontaneous discourse, most previous work has followed this text/monologue approach: for example, when (Galley et al., 2003) annot"
2005.sigdial-1.13,J96-2004,0,0.0347136,"ut establishing a defined reference. We will likely employ different strategies in the future for establishing a reference segmentation which incorporates minor boundaries. 3.4 Evaluating inter-annotator agreement In this section we present the results of evaluating agreement between our two annotators and compare multiple agreement metrics. The results show variance among meetings, suggesting that the topic segmentation task may be ill-formed for certain classes of meetings. The current standard metric for measuring interannotator agreement in classification tasks is the kappa statistic (K) (Carletta, 1996). While K is a good measure of how well annotators can agree on pinpointing topic breaks at time points, it does not accomodate nearmiss break assignments in which annotators label different nearby time points as topic breaks. For the evaluation of segmentation algorithms specifically, two metrics are most commonly used: Pk (Beeferman et al., 1999) and WindowDiff (W D) (Pevzner and Hearst, 2002). These were designed principally to evaluate text segmentation algorithms that operate at sentence boundaries, but can be applied to continuous-time segmentations through the use of windowing. Pk accom"
2005.sigdial-1.13,J02-1002,0,0.0638887,"ggesting that the topic segmentation task may be ill-formed for certain classes of meetings. The current standard metric for measuring interannotator agreement in classification tasks is the kappa statistic (K) (Carletta, 1996). While K is a good measure of how well annotators can agree on pinpointing topic breaks at time points, it does not accomodate nearmiss break assignments in which annotators label different nearby time points as topic breaks. For the evaluation of segmentation algorithms specifically, two metrics are most commonly used: Pk (Beeferman et al., 1999) and WindowDiff (W D) (Pevzner and Hearst, 2002). These were designed principally to evaluate text segmentation algorithms that operate at sentence boundaries, but can be applied to continuous-time segmentations through the use of windowing. Pk accommodates near-miss labelings by considering how likely two time points are to be assigned to the same topic, while W D further refines this notion by measuring the number of intervening topic breaks between a time point assigned by annotators to distinct topics. Each metric provides a reasonable, though different, evaluation of inter-annotator agreement. Results given below show a high degree of"
2005.sigdial-1.13,A00-2004,0,0.0190751,"lds of segmenting text and monologue. We then discuss previous work in segmenting discourse, our own motivations, and finally (in section 2.1.1) outline an annotation schema derived from these motivations. Text and Monologues The segmenting of text documents is often motivated by information retrieval tasks – for instance, so that a single appropriate segment can be returned matching a query. In some cases, topic boundaries are hand-annotated, as in (Hearst, 1994). However, topic boundaries are often artificially created by concatenating multiple articles together, as in (Galley et al., 2003; Choi, 2000). Moreover, since text is written linearly, usually with clearly punctuated boundaries in the form of sentences and paragraphs, it is natural to assume that topic boundaries will occur at such places. Thus, such “natural” boundaries both define and limit the search space. In addition to text, there has been much research in segmenting non-conversational speech; essentially monologues or series of monologues. For example, much work has been done on automatically segmenting broadcast news, e.g. (T¨ur et al., 2001; Beeferman et al., 1999; Allan et al., 1998). The tasks of segmenting text and mono"
2005.sigdial-1.13,P03-1071,0,0.451374,") and the ISL Meeting Corpus (Burger et al., 2002) because both contain high-quality close-talking microphone recordings of conversational speech in a meeting environment, as well as word-level transcriptions and utterance-level timing information. We focused mainly on the ICSI corpus because its contents most closely matched our task of processing fairly informal, officestyle meetings. In addition, extensive annotations have already been completed on the ICSI corpus, including: dialogue acts (Shriberg et al., 2004), “hot spots” (Wrede and Shriberg, 2003), and some work on topic segmentation (Galley et al., 2003; Carletta and Kilgour, 2004). 2.1 Topic Segmentations A significant challenge in spoken discourse segmentation is providing a concrete definition of the problem – the desired concepts of both topic and segmentation. To that end, we first briefly discuss the conceptualizations – and motivations behind those conceptualizations – that have arisen in the related fields of segmenting text and monologue. We then discuss previous work in segmenting discourse, our own motivations, and finally (in section 2.1.1) outline an annotation schema derived from these motivations. Text and Monologues The segme"
2005.sigdial-1.13,W04-2319,0,0.0553681,"d an application-driven annotation schema. We worked with the ICSI Meeting corpus (Janin et al., 2003) and the ISL Meeting Corpus (Burger et al., 2002) because both contain high-quality close-talking microphone recordings of conversational speech in a meeting environment, as well as word-level transcriptions and utterance-level timing information. We focused mainly on the ICSI corpus because its contents most closely matched our task of processing fairly informal, officestyle meetings. In addition, extensive annotations have already been completed on the ICSI corpus, including: dialogue acts (Shriberg et al., 2004), “hot spots” (Wrede and Shriberg, 2003), and some work on topic segmentation (Galley et al., 2003; Carletta and Kilgour, 2004). 2.1 Topic Segmentations A significant challenge in spoken discourse segmentation is providing a concrete definition of the problem – the desired concepts of both topic and segmentation. To that end, we first briefly discuss the conceptualizations – and motivations behind those conceptualizations – that have arisen in the related fields of segmenting text and monologue. We then discuss previous work in segmenting discourse, our own motivations, and finally (in section"
2005.sigdial-1.13,J01-1002,0,0.126584,"Missing"
2007.sigdial-1.17,1993.iwpt-1.12,0,0.0164248,"bedded in the semantic slots. For research in which elaborated expressions are considered, the coverage is typically small. Another thread of research is targeted at broad coverage but simple dialogs, which is exemplified by the work at AT&T [Gorin et al 1997]. While extending the research on the collaborative aspects, our effort specifically focuses on dealing with the conversational phenomena in multitasking and distracting environments, specifically imperfect input and imperfect memory. While dealing with imperfect input can be traced back far in time [Carbonell and Hayes, 1983; Weng 1993; Lavie & Tomita 1993; He and Young 2003], the CHAT system integrates models ranging from disfluency, partial and full proper names, shallow semantic parsing, and deep structural parsing. The interpretation only occurs when all the contextual information and alternatives are gathered. For the imperfect memory issue, we explore information presentation and other strategies to enable the user to access the information comfortably. All these approaches and strategies lead to high task completion rate and dialog efficiency as well as user satisfaction across the three domains, especially for the navigation. Collective"
2007.sigdial-1.17,W05-1519,1,0.779738,"results; finally, we conclude with a comparison with other work. 2 The CHAT System and Its Functionality The CHAT system has adopted many state-of-art technologies and has grown beyond its heritages over the years. This progress is reflected in several core aspects, including the spoken language understanding (SLU) module, the dialog manager (DM), the content optimizer (CO), the knowledge management (KM), the response generation (RG), as well as the overall system architecture. The SLU module integrates multiple understanding strategies with components such as edit region detection algorithm [Zhang and Weng, 2005; Zhang et al 2006]1, partial name identifier, shallow semantic parser, and deep structural parser. This approach enables understanding at finer levels when faced with imperfect input from the distracted multi-tasking user, and/or from speech recognition errors. The DM, originated from the CSLI dialog manager [Lemon et al 2002], follows the informationstate-update approach [Larsson and Traum 2000]. It uses a dialog move tree to keep track of multiple dialog threads and multiple applications [Mirkovic and Cavedon 2005; Purver et al 2006]. The latest version also supports mixed initiative dialog"
2007.sigdial-1.17,J83-3001,0,\N,Missing
2007.sigdial-1.17,W07-0305,0,\N,Missing
2007.sigdial-1.17,P06-1071,1,\N,Missing
2007.sigdial-1.17,J74-3000,0,\N,Missing
2007.sigdial-1.17,J74-2000,0,\N,Missing
2007.sigdial-1.17,J74-1000,0,\N,Missing
2007.sigdial-1.17,W05-1626,1,\N,Missing
2007.sigdial-1.4,W04-1008,0,0.0256616,"utputs. For the timeframes, this produces 18 Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, pages 18–25, c Antwerp, September 2007. 2007 Association for Computational Linguistics more informative results than the alternative of presenting the entire 1-best utterance transcriptions. 2 Background Subdialogue Detection User studies show that participants regard action items as one of a meeting’s most important outputs (Lisowska et al., 2004; Banerjee et al., 2005). However, spoken action item detection seems to be a relatively new task. There is related work with email text: (Corston-Oliver et al., 2004; Bennett and Carbonell, 2005) both showed success classifying sentences or entire messages as action item- or task-related. Performance was reasonable, with f-scores around 0.6 for sentences and 0.8 for whole messages; the features used included lexical, syntactic and semantic features (n-grams, PoS-tags, named entities) as well as more emailspecific features (e.g. header information). However, applying the same methods to dialogue data is problematic. Morgan et al. (2006) applied a similar method to a portion of the ICSI Meeting Corpus (Janin et al., 2003) annotated for action items by Gruen"
2007.sigdial-1.4,P93-1008,1,0.575081,"ssen, 1997). NOMLEX (Macleod et al., 1998) provides syntactic information for event nominalizations and a mapping from noun arguments to VerbNet syntactic positions; this allows us to give nominalizations a semantics compatible with verb events, and assert selectional restrictions. To add proper names, we used US Census data for people, KnowItAll (Downey et al., 2007) for companies, and WSJ data for person and organization names. Proper names account for about 1/3 of the entries in the lexicon. These resources are combined and converted to the Prolog-based format used in the Gemini framework (Dowding et al., 1993), which includes a fast bottom-up robust parser in which syntactic and semantic information is applied interleaved. To facilitate extracting semantic features, we use Minimal Recursion Semantics (Copestake et al., 2005), a flat semantic representation; we have also modified Gemini to parse WCNs as well as flat transcriptions. Gemini computes parse probabilities on the contextfree background of the grammar; in these experiments, probabilities were trained on WSJ data. 4.2 Experiments Our parsing approach intentionally produces multiple short fragments rather than one full utterance parse. Combi"
2007.sigdial-1.4,P03-1071,0,0.0376942,"ntifies potential summarizing phrases, and show that for some task properties these can be more informative than plain utterance transcriptions. 1 Introduction Multi-party conversation, usually in the form of meetings, is the primary way to share information and make decisions in organized work environments. There is growing interest in the development of automatic methods to extract and analyze the information content of meetings in various ways, including automatic transcription, targeted browsing, and topic detection and segmentation – see (Stolcke et al., 2005; Tucker and Whittaker, 2005; Galley et al., 2003), amongst others. In this paper we are interested in identifying action items – public commitments to perform a ∗ This work was supported by the CALO project (DARPA grant NBCH-D-03-0010). We also thank Gokhan T¨ur, Andreas Stolcke and Liz Shriberg for provision of ASR output and dialogue act tags for the ICSI corpus. Section 3 discusses the detection of subdialogues – short passages of conversation in which the action items are typically discussed, summarized, agreed and committed to – using a hierarchical classifier which exploits local dialogue structure. Multiple independent sub-classifiers"
2007.sigdial-1.4,C94-1042,0,0.0189211,"mar, however, is small: as our data is highly ungrammatical, disfluent and errorful, we have developed a semantic parser that attempts to find basic predicate-argument structures of the major phrase types S, VP, NP, and PP, not necessarily trying to find larger structures (such as coordination and relative clauses) where reliability would be low. Lexical Resources Our lexicon is built from publicly available lexical resources for English, including COMLEX, VerbNet, WordNet, and NOMLEX. Others have shared this basic approach (Shi and Mihalcea, 2005; Crouch and King, 2005; Swift, 2005). COMLEX (Grishman et al., 1994) provides detailed morphological and syntactic information for the 40,000 most common words of English, as well as basic lexical information (e.g. adjective gradability, verb subcategorization, noun mass/count nature). VerbNet (Kipper et al., 2000) provides semantic information for 5,000 verbs, including frames and thematic roles, along with syntactic mappings and selectional restrictions for role fillers. WordNet (Miller, 1995) then provides us with another 15,539 nouns, and the semantic class information for all nouns. These semantic classes are hand-aligned to the selectional classes used i"
2007.sigdial-1.4,2005.sigdial-1.13,1,0.820435,"2004; Bennett and Carbonell, 2005) both showed success classifying sentences or entire messages as action item- or task-related. Performance was reasonable, with f-scores around 0.6 for sentences and 0.8 for whole messages; the features used included lexical, syntactic and semantic features (n-grams, PoS-tags, named entities) as well as more emailspecific features (e.g. header information). However, applying the same methods to dialogue data is problematic. Morgan et al. (2006) applied a similar method to a portion of the ICSI Meeting Corpus (Janin et al., 2003) annotated for action items by Gruenstein et al. (2005). While they found that similar lexical, syntactic and contextual features were useful (together with other dialoguespecific features, including dialogue act type and prosodic information), performance was poor, with f-scores limited to approximately 0.3, even given manual transcripts and dialogue act tags. One major reason for this is the fragmented nature of conversational decision-making: in contrast to email text, the descriptions of tasks and their properties tend not to come in single sentences, but may be distributed over many utterances. These utterances may take many different forms a"
2007.sigdial-1.4,P07-2027,1,0.827568,"Ideally, this summary should contain at least the identity of the owner, a description of the task, and a specification of the timeframe. Ownership may occasionally be expressed by explicit use of a name, but is more often specified through the interaction itself – proposals of ownership usually either volunteer the speaker “I guess I’ll . . . ” or request commitment from the addressee “Could you maybe . . . ”. Establishing identity therefore becomes a problem of speaker and addressee identification, which we leave aside for now, but see e.g. (Katzenmaier et al., 2004; Jovanovic et al., 2006; Gupta et al., 2007). Timeframe and task, however, are expressed explicitly; but detecting the relevant utterances only gets us part of the way. Example (1) shows an utterance containing a task description: (1) What I have down for action items is we’re supposed to find out about our human subject D description Arguably the best phrase within this utterance to describe the task is find out about our human subject, as opposed to other larger or smaller phrases. Notably, although the utterance contains the phrase action item — likely a strong clue to the detection of this utterance as action item-related — this phr"
2007.sigdial-1.4,N07-1004,0,0.0492964,"and action item discussions could be hypothesized using a simple heuristic to detect clusters of multiple classes. However, this was only evaluated on a small corpus of simulated meetings (5 c.10-minute meetings, simulated by actors given a detailed scenario), and only on gold-standard manual transcriptions. The first half of this paper applies that proposal to a larger, less domain-specific, naturally-occurring dataset, and also extends it to include the learning of a super-classifier from data. Note that while previous work in the detection and modelling of decisions (Verbree et al., 2006; Hsueh and Moore, 2007) is related, the tasks are not the same. Firstly, our job is to identify public commitments to tasks, rather than general decisions about strategy, or decisions not to do anything (see e.g. Hsueh and Moore (2007)’s example Fig. 1). Secondly, our data is essentially opendomain, making e.g. simple lexical cues less useful than they are in a domain with repeated fixed topics. Note also that our results are not directly comparable with those of Hsueh and Moore (2007), who detect decision-making acts from a human-extracted summary rather than a raw meeting transcript, making positive examples much"
2007.sigdial-1.4,E06-1022,0,0.0675213,"Missing"
2007.sigdial-1.4,lisowska-etal-2004-user,0,0.130481,"esources and tailored to the particular problem of parsing speech recognition output, and show how a regression model can be used to rank the candidate parser outputs. For the timeframes, this produces 18 Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, pages 18–25, c Antwerp, September 2007. 2007 Association for Computational Linguistics more informative results than the alternative of presenting the entire 1-best utterance transcriptions. 2 Background Subdialogue Detection User studies show that participants regard action items as one of a meeting’s most important outputs (Lisowska et al., 2004; Banerjee et al., 2005). However, spoken action item detection seems to be a relatively new task. There is related work with email text: (Corston-Oliver et al., 2004; Bennett and Carbonell, 2005) both showed success classifying sentences or entire messages as action item- or task-related. Performance was reasonable, with f-scores around 0.6 for sentences and 0.8 for whole messages; the features used included lexical, syntactic and semantic features (n-grams, PoS-tags, named entities) as well as more emailspecific features (e.g. header information). However, applying the same methods to dialog"
2007.sigdial-1.4,W06-1314,0,0.153464,". However, spoken action item detection seems to be a relatively new task. There is related work with email text: (Corston-Oliver et al., 2004; Bennett and Carbonell, 2005) both showed success classifying sentences or entire messages as action item- or task-related. Performance was reasonable, with f-scores around 0.6 for sentences and 0.8 for whole messages; the features used included lexical, syntactic and semantic features (n-grams, PoS-tags, named entities) as well as more emailspecific features (e.g. header information). However, applying the same methods to dialogue data is problematic. Morgan et al. (2006) applied a similar method to a portion of the ICSI Meeting Corpus (Janin et al., 2003) annotated for action items by Gruenstein et al. (2005). While they found that similar lexical, syntactic and contextual features were useful (together with other dialoguespecific features, including dialogue act type and prosodic information), performance was poor, with f-scores limited to approximately 0.3, even given manual transcripts and dialogue act tags. One major reason for this is the fragmented nature of conversational decision-making: in contrast to email text, the descriptions of tasks and their p"
2007.sigdial-1.4,W04-2319,0,0.0108683,"Missing"
2007.sigdial-1.4,H93-1008,1,\N,Missing
2007.sigdial-1.40,P04-1085,0,0.0388402,"use a label of 4 to represent addressing to the entire group. Baseline. We can build two baselines. The Next Speaker baseline always predicts the addressee to be the next (different) speaker (i.e. a label of 1). The Previous Speaker baseline predicts the addressee to be the most recent previous different speaker. Features. We expect that the structure of the dialog gives the most indicative cues to addressee: forward-looking dialog acts are likely to influence the addressee to speak next, while backward-looking acts might address a recent speaker. We therefore use similar features to those of Galley et al. (2004) for the related task of identifying the first half of an adjacency pair. However, since their task was retrospective, their features all involve facts about the previous discourse context. We therefore adapt the approach to examine features of subsequent as well as preceding utterances. For each utterance and potential addressee, we examine the pair made up of the original utterance A and the next (or previous) utterance B spoken by that potential addressee. We then extract features of the pair which might indicate the degree of relatedness of the utterances, including their overlap, separati"
2007.sigdial-1.40,P07-2027,1,0.87513,"Missing"
2007.sigdial-1.40,E06-1022,0,0.340956,"Missing"
2007.sigdial-1.40,E06-1007,0,0.0598088,"Missing"
2007.sigdial-1.40,J93-3003,0,\N,Missing
2020.aacl-srw.16,P19-1448,0,0.526258,"al., 2017), TypeSQL (Yu et al.), SQLova (Hwang et al., 2019), HydraNet (Lyu et al., 2020), X-SQL (He et al., 2019), Coarse2Fine (Dong and Lapata, 2018) and others. Among them, the SQLNet and TypeSQL models designed for WikiSQL have been transferred to Spider; however, their performance drops significantly. SyntaxSQLNet (Yu et al., 2018a) is the first model designed for Spider and, based on a similar idea, uses independent modules to predict different clauses. However its performance is less effective than some later models based on one unified grammar-based decoder modules (Guo et al., 2019; Bogin et al., 2019a). Although these later models are based on one unified module, they also treat SQL structure generation and filling the schema items as separate processes. SQL structure generation depends on analysis of the sentence, while filling the schema items depends on the similarity between schema items and sentence tokens. For example, in Table 2, we test the top models (RAT-SQL (Wang et al., 2020), IRNET (Guo et al., 2019), and GNN (Bogin et al., 2019a)) in the Spider leaderboard, and all these models tend to generate wrong predictions of the type shown. This type of example can be found in the Spi"
2020.aacl-srw.16,D19-1378,0,0.0227895,"Missing"
2020.aacl-srw.16,N19-1423,0,0.0914313,"Missing"
2020.aacl-srw.16,P18-1068,0,0.0208132,"mns and tables). In WikiSQL, because the dataset only contains simple SQL, most models decompose the SQL synthesis into several independent classification sub-tasks. Each sub-task employs an independent classifier taking the entire sentence as input. For example, one classifier would be used to determine which column is the column in SELECT clause, and another separate classifier to determine which aggregation function is correct. These models include: SQLNet (Xu et al., 2017), TypeSQL (Yu et al.), SQLova (Hwang et al., 2019), HydraNet (Lyu et al., 2020), X-SQL (He et al., 2019), Coarse2Fine (Dong and Lapata, 2018) and others. Among them, the SQLNet and TypeSQL models designed for WikiSQL have been transferred to Spider; however, their performance drops significantly. SyntaxSQLNet (Yu et al., 2018a) is the first model designed for Spider and, based on a similar idea, uses independent modules to predict different clauses. However its performance is less effective than some later models based on one unified grammar-based decoder modules (Guo et al., 2019; Bogin et al., 2019a). Although these later models are based on one unified module, they also treat SQL structure generation and filling the schema items"
2020.aacl-srw.16,P19-1444,0,0.472846,"ude: SQLNet (Xu et al., 2017), TypeSQL (Yu et al.), SQLova (Hwang et al., 2019), HydraNet (Lyu et al., 2020), X-SQL (He et al., 2019), Coarse2Fine (Dong and Lapata, 2018) and others. Among them, the SQLNet and TypeSQL models designed for WikiSQL have been transferred to Spider; however, their performance drops significantly. SyntaxSQLNet (Yu et al., 2018a) is the first model designed for Spider and, based on a similar idea, uses independent modules to predict different clauses. However its performance is less effective than some later models based on one unified grammar-based decoder modules (Guo et al., 2019; Bogin et al., 2019a). Although these later models are based on one unified module, they also treat SQL structure generation and filling the schema items as separate processes. SQL structure generation depends on analysis of the sentence, while filling the schema items depends on the similarity between schema items and sentence tokens. For example, in Table 2, we test the top models (RAT-SQL (Wang et al., 2020), IRNET (Guo et al., 2019), and GNN (Bogin et al., 2019a)) in the Spider leaderboard, and all these models tend to generate wrong predictions of the type shown. This type of example can"
2020.aacl-srw.16,P17-1089,0,0.081315,"ue. Any text-to-SQL model with decent performance needs a schema linking value. In Paradigm One approaches, only the schema items strongly related to the question tokens (with high schema linking value) will be filled into the SQL structure. In Paradigm Two, schema linking helps to generate the schema related labels. 4.2 Schema Linking Construction There are different ways to construct a schema linking. The most common method is to train a neural network model that gives a higher similarity score to the link between a word token in a question and a schema item when they have the same meaning (Iyer et al., 2017). This method is widely used but may have different implementation details. Some works implement extra schema linking by recognizing the columns and the tables mentioned in a question before training the model (Guo et al., 2019; Bogin et al., 2019a; Wang et al., 2020). It should be noted that Guo et al. (2019) and Wang 110 et al. (2020) name the extra schema linking as schema linking in the paper while Bogin et al. (2019a) do not mention this extra schema linking but implement it in the code. Extra schema linking is essential in these models because in the ablation study of IRNet and RAT-SQL b"
2020.aacl-srw.16,W00-1317,0,0.0512039,"Missing"
2020.aacl-srw.16,2020.acl-main.677,0,0.69757,"similar idea, uses independent modules to predict different clauses. However its performance is less effective than some later models based on one unified grammar-based decoder modules (Guo et al., 2019; Bogin et al., 2019a). Although these later models are based on one unified module, they also treat SQL structure generation and filling the schema items as separate processes. SQL structure generation depends on analysis of the sentence, while filling the schema items depends on the similarity between schema items and sentence tokens. For example, in Table 2, we test the top models (RAT-SQL (Wang et al., 2020), IRNET (Guo et al., 2019), and GNN (Bogin et al., 2019a)) in the Spider leaderboard, and all these models tend to generate wrong predictions of the type shown. This type of example can be found in the Spider development set where the database id is ‘concert singer’. The example shows that although based on a unified module, there is no strong interaction between generating SQL struc109 Natural Language Question: What is the average miles per gallon of the cars with 4 cylinders? Paradigm One: Step 1) Generate SQL Structure ‘SELECT avg( ) FROM WHERE = Step 2) Fill the schema items mpg cars_data"
2020.aacl-srw.16,N18-2093,0,0.0483393,"Missing"
2020.aacl-srw.16,D18-1193,0,0.225251,"w of Cross-Domain Text-to-SQL Models Yujian Gan Matthew Purver John R. Woodward School of Electronic Engineering and Computer Science Queen Mary University of London Mile End Road, London E1 4NS, UK {y.gan,m.purver,j.woodward}@qmul.ac.uk Abstract leading to the impression that the text-to-SQL problem has been solved. However, WikiSQL’s complexity is limited: its SQL queries only cover a single SELECT column and aggregation, together with relatively simple selection predicates in the WHERE clauses, thus lacking in terms of complex SQL queries. To facilitate the study of complex SQL generation, Yu et al. (2018b) introduced Spider, a large-scale cross-domain text-to-SQL benchmark with complex SQL queries. Experiments on Spider show previous models designed for WikiSQL suffer a significant performance drop. In this paper, we discuss the top models for the WikiSQL and Spider benchmarks. Since relatively high generation accuracy has already been achieved for the WikiSQL benchmark, and the SQL structures in Spider cover all SQL structures in WikiSQL, we focus more on models designed for Spider. This paper starts from the comparison of the overall paradigms of the models and then discusses the key module"
2020.aacl-srw.16,D18-1425,0,0.0839818,"Missing"
2020.lrec-1.720,N19-1423,0,0.0238089,"es; covers not only discrete differences in word sense but more subtle, graded changes in meaning; and covers not only a well-resourced language (English) but a number of less-resourced languages. We define the task and evaluation metrics, outline the dataset collection methodology, and describe the status of the dataset so far. Keywords: corpus, annotation, semantics, similarity, context, salience, context-dependence 1. Introduction Recent work in language modelling and word embeddings has led to a sharp increase in use of context-dependent models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). These models, by providing representations of words which depend on the surrounding context, allow us to take account of the effects not only of discrete differences in word sense but of the more graded effects of context. However, evaluation of these models has generally been in terms of either their performance as language models, or their effect on downstream tasks such as sentiment classification (Peters et al., 2018): there are few resources available which allow evaluation in terms of the properties of the embeddings themselves, or in terms of their ability to model human perceptions o"
2020.lrec-1.720,J15-4004,0,0.742447,"s of context. However, evaluation of these models has generally been in terms of either their performance as language models, or their effect on downstream tasks such as sentiment classification (Peters et al., 2018): there are few resources available which allow evaluation in terms of the properties of the embeddings themselves, or in terms of their ability to model human perceptions of meaning. There are established methods to evaluate word embedding models intrinsically via their ability to reflect human similarity judgements (see e.g. WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015)) or model analogies (Mikolov et al., 2013); however, these have generally ignored context and treated words in isolation. The few that do provide context (e.g. SCWS (Huang et al., 2012) and WiC (Pilehvar and Camacho-Collados, 2019)) focus on word sense and discrete effects, thus missing some of the effects that context has on words in general, and some of the benefits of context-dependent models. To evaluate current models, we need a way to evaluate their ability to reflect similarity judgements in context: how well do they model the effects that context has on word meaning? In this paper we"
2020.lrec-1.720,P12-1092,0,0.926591,"ification (Peters et al., 2018): there are few resources available which allow evaluation in terms of the properties of the embeddings themselves, or in terms of their ability to model human perceptions of meaning. There are established methods to evaluate word embedding models intrinsically via their ability to reflect human similarity judgements (see e.g. WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015)) or model analogies (Mikolov et al., 2013); however, these have generally ignored context and treated words in isolation. The few that do provide context (e.g. SCWS (Huang et al., 2012) and WiC (Pilehvar and Camacho-Collados, 2019)) focus on word sense and discrete effects, thus missing some of the effects that context has on words in general, and some of the benefits of context-dependent models. To evaluate current models, we need a way to evaluate their ability to reflect similarity judgements in context: how well do they model the effects that context has on word meaning? In this paper we present our ongoing efforts to define and build a new dataset that tries to fill that gap: CoSimLex (Armendariz et al., 2020). CoSimLex builds on the familiar pairwise, graded similarity"
2020.lrec-1.720,Q17-1022,0,0.0793333,"Missing"
2020.lrec-1.720,N18-1202,0,0.0456083,"ext-dependent similarity measures; covers not only discrete differences in word sense but more subtle, graded changes in meaning; and covers not only a well-resourced language (English) but a number of less-resourced languages. We define the task and evaluation metrics, outline the dataset collection methodology, and describe the status of the dataset so far. Keywords: corpus, annotation, semantics, similarity, context, salience, context-dependence 1. Introduction Recent work in language modelling and word embeddings has led to a sharp increase in use of context-dependent models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). These models, by providing representations of words which depend on the surrounding context, allow us to take account of the effects not only of discrete differences in word sense but of the more graded effects of context. However, evaluation of these models has generally been in terms of either their performance as language models, or their effect on downstream tasks such as sentiment classification (Peters et al., 2018): there are few resources available which allow evaluation in terms of the properties of the embeddings themselves, or in terms of their abili"
2020.lrec-1.720,N19-1128,0,0.151127,"8): there are few resources available which allow evaluation in terms of the properties of the embeddings themselves, or in terms of their ability to model human perceptions of meaning. There are established methods to evaluate word embedding models intrinsically via their ability to reflect human similarity judgements (see e.g. WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015)) or model analogies (Mikolov et al., 2013); however, these have generally ignored context and treated words in isolation. The few that do provide context (e.g. SCWS (Huang et al., 2012) and WiC (Pilehvar and Camacho-Collados, 2019)) focus on word sense and discrete effects, thus missing some of the effects that context has on words in general, and some of the benefits of context-dependent models. To evaluate current models, we need a way to evaluate their ability to reflect similarity judgements in context: how well do they model the effects that context has on word meaning? In this paper we present our ongoing efforts to define and build a new dataset that tries to fill that gap: CoSimLex (Armendariz et al., 2020). CoSimLex builds on the familiar pairwise, graded similarity task of SimLex-999, but extends it to pairs o"
2020.lrec-1.720,W17-0228,0,0.068048,"nses and cannot therefore capture continuous effects of context in the judgements of similarity between different words. These datasets are also available only in English, and do not allow models to be evaluated across different languages. 3. Dataset and Task Design CoSimLex will be based on pairs of words from SimLex999 (Hill et al., 2015); the reliability and common use of this dataset makes it a good starting point and allows comparison of judgements and model outputs to the contextindependent case. For Croatian and Finnish we use existing translations of Simlex-999 (Mrkˇsi´c et al., 2017; Venekoski and Vankka, 2017; Kittask, 2019). In the case of Slovene, we have produced our own new translation (Pollak et al., 2020), following the methodology used by Mrkˇsi´c et al. (2017) for Croatian. The English dataset consists of 333 pairs; the Croatian, Finnish and Slovene datasets of 111 pairs each. Each pair is rated within two different contexts, giving a total of 1554 scores of contextual similarity. This poses a difficult task: to find suitable, organically occurring contexts for each pair; this task is more pronounced for languages with less resources, and as a result the selection of pairs is different for"
2020.nlpcovid19-2.7,Q16-1033,0,0.0263165,"depression scales (Rude et al. 2004). Such findings confirmed that language can be an indicator of an individual’s psychological state (Bucci & Freedman 1981) which lead to the development of Linguistic Enquiry and Word Count (LIWC) software (Pennebaker et al. 2003, Tausczik & Pennebaker 2010) which allows users to evaluate texts based on word counts in a variety of categories. More recent and larger scale computational linguistics have been applied in conversational counselling by utilising data from an SMS service where vulnerable users can engage in therapeutic discussion with counsellors (Althoff et al. 2016). For a more in-depth review of uses of natural language processing (NLP) techniques applied in mental health the reader is referred to Trotzek et al. (2018). 2.2 Social Media as a Platform for Mental Health Monitoring The widespread engagement in social media platforms by users coupled with the availability of platforms’ data enables researchers to extract population-level health information that make it possible to track diseases, medications and symptoms (Paul & Dredze 2011). The use of social media data is attractive to researchers not only due to its vast domain coverage but also due to t"
2020.semeval-1.3,C18-1139,0,0.0425101,"imLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differe"
2020.semeval-1.3,2020.semeval-1.37,0,0.0306792,"h Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask 1, finished second in the two Croatian subtasks and performed competitively in the English ones. To conclude with this group JUSTMasters (Al-Khdour et al., 2020) tested several models, parameters and their own strategy to combine models. They achieved very good performance, especially in the English Subtask 2. However, in order to optimise their system, they made many more submissions than allowed in the competition; we therefore leave them out of the official ranking. With a more multilingual approach, BabelEncoding (Costella Pessutto et al., 2020) proposed a solution in which they translated the contexts and target words to many languages and then used a weighted combination of monolingual pretrained non contextualised embeddings and BERT embeddings"
2020.semeval-1.3,2020.lrec-1.720,1,0.566046,"Missing"
2020.semeval-1.3,2020.semeval-1.38,0,0.0192666,"l at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene. The starting point of CitiusNLP (Gamallo, 2020) was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn’t seem to make use of it. They created a linguistically motivated system that relied in dependency to create predictions. However, its performance was considerably worse than BERT’s and their actual submissions are based on a standard BERT model. Finally, the Will_Go team (Bao et al., 2020) looked at different ways to measure similarity between embeddings, mixing euclidean distance with the most common cosine similarity and several others not 44 described in their paper. The combination works well, they achieved a second place in the English Subtask 1 and won the Finnish Subtask 1. 8 Conclusion We resented the SemEval-2020 Task on Graded Word Similarity in Context and introduced our new dataset CoSimLex. We provided the motivation behind their design choices and described the annotation process. The task received a good number of submissions and system description papers (15 and"
2020.semeval-1.3,S17-2002,1,0.894409,"me word, and labelled as to whether the word sense in the two examples/contexts is the same or different. This forces engagement with the context; it also creates a task in which context-independent models like word2vec “would perform no better than a random baseline”; and inter-rater agreement scores are much more healthy. However, as the dataset focuses on discrete word senses, it cannot capture graded effects of context. These datasets are also available only in English. Multi-lingual similarity datasets exist: in SemEval2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity, Camacho-Collados et al. (2017) used five different languages, and even used pairs in which each word was presented in a different language. A more recent Multi-SimLex dataset (Vuli´c et al., 2020) comprises similarity ratings for 1,888 concept pairs aligned across 13 typologically diverse languages. However, the pairs in both datasets were annotated out of context, preventing analysis of contextual effects. 38 3 Task Description Our dataset is based on pairs of words from SimLex-999 (Hill et al., 2015). Each instance is a naturallyoccurring context, taken from Wikipedia, in which both words in the pair appear, labelled wit"
2020.semeval-1.3,2020.semeval-1.35,0,0.0334679,"al influence. As an example, ukWaC-subs was created by substituting target words by either: a correct substitute; a word that could be the right substitute in other circumstances but it is not in this context; or a random word. The datasets included WiC, which when used to fine tune the model resulted in the best performance for Subtask1, giving them a third place. The approach works very well, giving a very consistent performance in all categories, and significantly improving the non fine-tuned model from a ρ=0.715 and 0.661 per subtask, to a ρ=0.760 and 0.718 respectively. Ferryman’s focus (Chen et al., 2020) was clearly the English Subtask 1, which they won with a modification of BERT in which they fed the TF-IDF score of the words to the model, thus incorporating information about the general importance of words. The system does very well at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene. The starting point of CitiusNLP (Gamallo, 2020) was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn’t seem to make use of it. They created"
2020.semeval-1.3,P19-4007,1,0.829844,"e to optimise their system with more than the competition’s limit of 9 submissions. neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables 2 and 3). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts"
2020.semeval-1.3,2020.semeval-1.5,0,0.204677,"Missing"
2020.semeval-1.3,P19-1285,0,0.02456,"ty scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask"
2020.semeval-1.3,N19-1423,0,0.0961329,"w dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two short text passages. Systems beat the baselines by significant margins, but few did well in more than one language or subtask. Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect. 1 Introduction Contextualised word embeddings, produced by models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in iso"
2020.semeval-1.3,J13-3003,0,0.173635,"luation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential contexts only in the English language. The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of SimLex-999, but extends it to pairs of words as they occur in context; sp"
2020.semeval-1.3,2020.semeval-1.34,0,0.0283968,"roving the non fine-tuned model from a ρ=0.715 and 0.661 per subtask, to a ρ=0.760 and 0.718 respectively. Ferryman’s focus (Chen et al., 2020) was clearly the English Subtask 1, which they won with a modification of BERT in which they fed the TF-IDF score of the words to the model, thus incorporating information about the general importance of words. The system does very well at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene. The starting point of CitiusNLP (Gamallo, 2020) was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn’t seem to make use of it. They created a linguistically motivated system that relied in dependency to create predictions. However, its performance was considerably worse than BERT’s and their actual submissions are based on a standard BERT model. Finally, the Will_Go team (Bao et al., 2020) looked at different ways to measure similarity between embeddings, mixing euclidean distance with the most common cosine similarity and several others not 44 described in their paper. The combination works well, they a"
2020.semeval-1.3,2020.semeval-1.17,0,0.0328571,"nally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the similarity scores themselves. The biggest weakness of the approach is their reliance on linguistic resources that don’t exist for most languages other than English. Related to these systems, the submission by MineriaUNAM (Gomez-Adorno et al., 2020) won the English Subtask 2. They proposed a system in which they calculated K-Means inspired centroids from the words in the context and used them to modify the original SimLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed prediction"
2020.semeval-1.3,2020.semeval-1.16,0,0.0416405,"rom the words in the context and used them to modify the original SimLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, su"
2020.semeval-1.3,J15-4004,0,0.527307,"used to good effect. 1 Introduction Contextualised word embeddings, produced by models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential"
2020.semeval-1.3,P12-1092,0,0.573882,"eam tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential contexts only in the English language. The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of S"
2020.semeval-1.3,2020.emnlp-main.283,0,0.0166095,"submissions. neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables 2 and 3). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The"
2020.semeval-1.3,P19-1569,0,0.0310581,"notator against the average of the rest. JUSTMasters is not part of the official ranking since they were able to optimise their system with more than the competition’s limit of 9 submissions. neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables 2 and 3). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new"
2020.semeval-1.3,2020.semeval-1.33,0,0.0312792,"set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the similarity scores themselves. The biggest weakness of the approach is their reliance on linguistic resources that don’t exist for most languages other than English. Related to these systems, the submission by MineriaUNAM (Gomez-Adorno et al., 2020) won the English Subtask 2. They pr"
2020.semeval-1.3,2020.semeval-1.36,0,0.0172085,"ss is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask 1, finished second in the two Croatian subtasks and performed competitively in the English ones. To conclude with this group JUSTMasters (Al-Khdour et al., 2020) tested several models, parameters and their own strategy to combine models. They achieved very good performance, esp"
2020.semeval-1.3,Q17-1022,1,0.867881,"Missing"
2020.semeval-1.3,N18-1202,0,0.17532,"system description papers. A new dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two short text passages. Systems beat the baselines by significant margins, but few did well in more than one language or subtask. Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect. 1 Introduction Contextualised word embeddings, produced by models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datas"
2020.semeval-1.3,N19-1128,1,0.870958,"ew resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential contexts only in the English language. The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of SimLex-999, but extends it to pairs of words"
2020.semeval-1.3,2020.semeval-1.18,0,0.0318793,"languages and then used a weighted combination of monolingual pretrained non contextualised embeddings and BERT embeddings. Their idea is that the translation not only brings new resources but the process itself can produce useful information, for example to disambiguate. The approach works very well for the less resourced languages, being clearly the best system in that category, in both Subtask 1 and 2. Their system won Subtask 1 and 2 for Croatian (by a healthy margin) and 2 for Slovene, ending third in the Slovene Subtask 1 and third and second in the two Finnish ones. The MultiSem team (Soler and Apidianaki, 2020) collected 5 different datasets in order to fine-tune their BERT models, most of them automatically generated from previous datasets to increase contextual influence. As an example, ukWaC-subs was created by substituting target words by either: a correct substitute; a word that could be the right substitute in other circumstances but it is not in this context; or a random word. The datasets included WiC, which when used to fine tune the model resulted in the best performance for Subtask1, giving them a third place. The approach works very well, giving a very consistent performance in all categ"
2020.semeval-1.3,2020.semeval-1.19,0,0.0366991,"l discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the si"
2020.semeval-1.3,2020.lrec-1.582,1,0.835923,"Missing"
2020.semeval-1.3,W17-0228,0,0.0235541,"ilarity of words and the effect that context has on it. Good context-independent models could theoretically give reasonably competitive results in this task, however we still expect context-dependent models to have a considerable advantage. 4 Dataset CoSimLex (Armendariz et al., 2020) is based on pairs of words from SimLex-999 (Hill et al., 2015); the reliability and common use of SimLex makes it a good starting point and allows comparison of judgements and model outputs to the context-independent case. For Croatian and Finnish we use existing translations of SimLex-999 (Mrkši´c et al., 2017; Venekoski and Vankka, 2017; Kittask, 2019). In the case of Slovene, we have produced our own new translation,1 following Mrkši´c et al. (2017)’s methodology for Croatian. The dataset consists of 340 pairs in English, 112 in Croatian, 111 in Slovene and 24 in Finnish. Each pair is rated within two different contexts, giving a total of 1174 scores of contextual similarity. This poses a difficult task: to find suitable, organically occurring contexts; this task is even more challenging for languages with less resources, and as a result the selection of pairs is different for each language. Each line of CoSimLex is made of"
2020.semeval-1.3,2020.cl-4.5,1,0.887547,"Missing"
2020.tacl-1.20,P18-2073,1,0.601412,"ore document context, using broader and deeper models (e.g., Devlin et al., 2019; Yang et al., 2019). While most language modeling is restricted to individual sentences, models can benefit from using additional context (Khandelwal et al., 2018). However, despite the importance of context, few psycholinguistic or computational studies systematically investigate how context affects acceptability, or the ability of language models to predict human acceptability judgments. Two recent studies that explore the impact of document context on acceptability judgments both identify a compression effect (Bernardy et al., 2018; Bizzoni and Lappin, 2019). Sentences perceived to be low in acceptability when judged without context receive a boost in acceptability when judged within context. Conversely, those with high out-of-context acceptability see a reduction in acceptability when context is presented. It is unclear what causes this compression effect. Is it a result of cognitive load, imposed by additional We study the influence of context on sentence acceptability. First we compare the acceptability ratings of sentences judged in isolation, with a relevant context, and with an irrelevant context. Our results show"
2020.tacl-1.20,W19-0414,1,0.88034,"sing broader and deeper models (e.g., Devlin et al., 2019; Yang et al., 2019). While most language modeling is restricted to individual sentences, models can benefit from using additional context (Khandelwal et al., 2018). However, despite the importance of context, few psycholinguistic or computational studies systematically investigate how context affects acceptability, or the ability of language models to predict human acceptability judgments. Two recent studies that explore the impact of document context on acceptability judgments both identify a compression effect (Bernardy et al., 2018; Bizzoni and Lappin, 2019). Sentences perceived to be low in acceptability when judged without context receive a boost in acceptability when judged within context. Conversely, those with high out-of-context acceptability see a reduction in acceptability when context is presented. It is unclear what causes this compression effect. Is it a result of cognitive load, imposed by additional We study the influence of context on sentence acceptability. First we compare the acceptability ratings of sentences judged in isolation, with a relevant context, and with an irrelevant context. Our results show that context induces a cog"
2020.tacl-1.20,P19-1285,0,0.0240984,"pus, Giga5 ClueWeb, Common Crawl Table 1: Language models and their configurations. order (e.g., x4 and x1 ), and so the model always sees some context words for prediction. As XLNET is trained to work with different factorization orders during training, it has experienced both full/bidirectional context and partial/ unidirectional context, allowing it to adapt to tasks that have access to full context (e.g., most language understanding tasks), as well as those that do not (e.g., left-to-right generation). Another innovation of XLNET is that it incorporates the segment recurrence mechanism of Dai et al. (2019). This mechanism is inspired by truncated backpropagation through time used for training RNNs, where the initial state of a sequence is initialized with the final state from the previous sequence. The segment recurrence mechanism works in a similar way, by caching the hidden states of the transformer blocks from the previous sequence, and allowing the current sequence to attend to them during training. This permits XLNET to model long-range dependencies beyond its maximum sequence length. We use the largest pre-trained model (‘‘XLNetLarge’’),15 which has a similar number of parameters to our B"
2020.tacl-1.20,J15-4004,0,0.0445344,"users are rating the English sentences ≥ 3.0 consistently. For the second and third experiments, we also check that users are selecting the topics appropriately. In each HIT one context paragraph has one real topic (from the topic model), and three fake topics with randomly sampled words as the candidate topics. Users who fail to identify the real topic above a confidence level are filtered out. Across the three experiments, over three quarters of workers passed our filtering conditions. To calibrate for the differences in rating scale between users, we follow the postprocessing procedure of Hill et al. (2015), where we calculate the average rating for each user and the overall average (by taking the mean of all average ratings), and decrease (increase) the ratings of a user by 1.0 if their average rating is greater (smaller) than the overall average by 1.0.7 To reduce the impact of outliers, for each sentence we also remove ratings that are more than 2 standard deviations away from the mean.8 2.2 Results and Discussion We present scatter plots to compare the mean ratings for the three different contexts (H∅ , H+ , and H− ) in Figure 1. The black line represents the diagonal, and the red line repre"
2020.tacl-1.20,P82-1020,0,0.795466,"Missing"
2020.tacl-1.20,N19-1423,0,0.451204,"from raw texts, we have prima facie support for an alternative view of language acquisition that does not rely on a categorical grammaticality component. It is generally assumed that our perception of sentence acceptability is influenced by context. Sentences that may appear odd in isolation can become natural in some environments, and sentences that seem perfectly well formed in some contexts are odd in others. On the computational side, much recent progress in language modeling has been achieved through the ability to incorporate more document context, using broader and deeper models (e.g., Devlin et al., 2019; Yang et al., 2019). While most language modeling is restricted to individual sentences, models can benefit from using additional context (Khandelwal et al., 2018). However, despite the importance of context, few psycholinguistic or computational studies systematically investigate how context affects acceptability, or the ability of language models to predict human acceptability judgments. Two recent studies that explore the impact of document context on acceptability judgments both identify a compression effect (Bernardy et al., 2018; Bizzoni and Lappin, 2019). Sentences perceived to be low"
2020.tacl-1.20,P18-1027,0,0.0180897,"generally assumed that our perception of sentence acceptability is influenced by context. Sentences that may appear odd in isolation can become natural in some environments, and sentences that seem perfectly well formed in some contexts are odd in others. On the computational side, much recent progress in language modeling has been achieved through the ability to incorporate more document context, using broader and deeper models (e.g., Devlin et al., 2019; Yang et al., 2019). While most language modeling is restricted to individual sentences, models can benefit from using additional context (Khandelwal et al., 2018). However, despite the importance of context, few psycholinguistic or computational studies systematically investigate how context affects acceptability, or the ability of language models to predict human acceptability judgments. Two recent studies that explore the impact of document context on acceptability judgments both identify a compression effect (Bernardy et al., 2018; Bizzoni and Lappin, 2019). Sentences perceived to be low in acceptability when judged without context receive a boost in acceptability when judged within context. Conversely, those with high out-of-context acceptability s"
2020.tacl-1.20,C12-1173,0,0.0644447,"Missing"
2020.tacl-1.20,D18-2012,0,0.0554596,"Missing"
2020.tacl-1.20,P12-1101,0,0.0687942,"To modulate for these factors we introduce simple normalization techniques. Table 2 presents five methods to map sentence probabilities to acceptability measures: LP, MeanLP, PenLP, NormLP, and SLOR. LP is the unnormalized log probability. Both MeanLP and PenLP are normalized on sentence length, but PenLP scales length with an exponent (α) to dampen the impact of large values (Wu et al., 2016; Vaswani et al., 2017). We set α = 0.8 in our experiments. NormLP normalizes using unigram Q|s| sentence probability (i.e., Pu (s) = i=0 P (wi )), while SLOR utilizes both length and unigram probability (Pauls and Klein, 2012). When computing sentence probability we have the option of including the context paragraph that the human annotators see (Section 2). We use the superscripts ∅, +, − to denote a model using no context, real context, and random context, respectively (e.g., LSTM∅ , LSTM+ , and LSTM− ). Note that these variants are created at test time, and are all based on the same trained model (e.g., LSTM). For all models except TDLM, incorporating the context paragraph is trivial. We simply prepend it to the target sentence before computing the latter’s probability. For TDLM+ or TDLM− , the context paragraph"
2020.tacl-1.20,P17-1033,1,0.61961,"ion. The two sets of experiments provide insights into the cognitive aspects of sentence processing and central issues in the computational modeling of text and discourse. 1 Introduction Sentence acceptability is the extent to which a sentence appears natural to native speakers of a language. Linguists have often used this property to motivate grammatical theories. Computational language processing has traditionally been more concerned with likelihood—the probability of a sentence being produced or encountered. The question of whether and how these properties are related is a fundamental one. Lau et al. (2017b) experiment with unsupervised language models to predict acceptability, and they obtained an encouraging correlation with human ratings. 296 Transactions of the Association for Computational Linguistics, vol. 8, pp. 296–310, 2020. https://doi.org/10.1162/tacl a 00315 Action Editor: George Foster. Submission batch: 10/2019; Revision batch: 1/2020; Published 6/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. processing demands, or is it the consequence of an attempt to identify a discourse relation between context and sentence? We address these que"
2020.tacl-1.20,P15-1156,1,0.937034,"prediction), and use these models to infer the probabilities of our test sentences. To accommodate sentence length and lexical frequency we experiment with several simple normalization methods, converting probabilities to acceptability measures (Section 3.2). The acceptability measures are the final output of our models; they are what we use to compare to human acceptability ratings. 3.1 Language Models Our first model is an LSTM language model (LSTM: Hochreiter and Schmidhuber, 1997; Mikolov et al., 2010). Recurrent neural network models (RNNs) have been shown to be competitive in this task (Lau et al., 2015; Bernardy et al., 2018), and they serve as our baseline. Our second model is a joint topic and language model (TDLM: Lau et al., 2017a). TDLM combines topic model with language model in a single model, drawing on the idea that the topical context of a sentence can help word prediction in the language model. The topic model is fashioned as an auto-encoder, where the input is the document’s word sequence and it is processed by convolutional layers to produce a topic vector to predict the input words. The language model 10 We follow the procedure detailed in https:// statisticsbyjim.com/regressi"
2020.tacl-1.20,P16-1162,0,0.00980138,"ped to embeddings (along with their positions) and fed to multiple layers of ‘‘transformer blocks’’ before the target word is predicted. Much of its power resides in these transformer blocks: Each provides a multi-headed self-attention unit over all input words, allowing it to capture multiple dependencies between words, while avoiding the need for recurrence. With no need to process a sentence in sequence, the model parallelizes more efficiently, and scales in a way that RNNs cannot. GPT2 is trained on WebText, which consists of over 8 million web documents, and uses Byte Pair Encoding (BPE: Sennrich et al., 2016) for tokenization (casing preserved). BPE produces sub-word units, a middle ground between word and character, and it provides better coverage for unseen words. We use the released medium-sized model (‘‘Medium’’) for our experiments.12 Our second transformer is BERT (Devlin et al., 2019). Unlike GPT2, BERT is not a typical language model, in the sense that it has access to both left and right context words when predicting the target word.13 Hence, it encodes context in a bidirectional manner. To train BERT, Devlin et al. (2019) propose a masked language model objective, where a random proporti"
2020.tacl-1.20,P14-5010,0,0.00379249,"rd units, a middle ground between word and character, and it provides better coverage for unseen words. We use the released medium-sized model (‘‘Medium’’) for our experiments.12 Our second transformer is BERT (Devlin et al., 2019). Unlike GPT2, BERT is not a typical language model, in the sense that it has access to both left and right context words when predicting the target word.13 Hence, it encodes context in a bidirectional manner. To train BERT, Devlin et al. (2019) propose a masked language model objective, where a random proportion of input words are masked 11 We use Stanford CoreNLP (Manning et al., 2014) to tokenize words and sentences. Rare words are replaced by a special UNK symbol. 12 https://github.com/openai/gpt-2. 13 Note that context is burdened with two senses in the paper. It can mean the preceding sentences of a target sentence, or the neighbouring words of a target word. The intended sense should be apparent from the usage. 14 300 https://github.com/google-research/bert. Configuration Architecture Encoding #Param. Casing Size BERTUCS RNN RNN Transformer Transformer Transformer Unidir. Unidir. Unidir. Bidir. Bidir. 60M 80M 340M 340M 340M Uncased Uncased Cased Cased Uncased 0.2GB 0.2"
2020.tacl-1.20,W19-2304,0,0.0184689,"ver, that sentence probability computed this way is not a true probability value: These probabilities do not sum to 1.0 over all sentences. Equation (1), in contrast, does guarantee true probabilities. Intuitively, the sentence probability computed with this bidirectional formulation is a measure 16 Technically we can mask all right context words and predict the target words one at a time, but because the model is never trained in this way, we found that it performs poorly in preliminary experiments. of the model’s confidence in the likelihood of the sentence. To compute the true probability, Wang and Cho (2019) show that we need to sum the pre-softmax weights for each token to score a sentence, and then divide the score by the total score of all sentences. As it is impractical to compute the total score of all sentences (an infinite set), the true sentence probabilities for these bidirectional models are intractable. We use our non-normalized confidence scores as stand-ins for these probabilities. For XLNET, we also compute sentence probability this way, applying bidirectional context, and we denote it as XLNETBI . Note that XLNETUNI and XLNETBI are based on the same trained model. They differ only"
2021.acl-long.195,2020.deelio-1.3,0,0.0539819,"Missing"
2021.acl-long.195,D18-1316,0,0.0347609,"Missing"
2021.acl-long.195,P19-1448,0,0.204974,"test set is not publicly accessible, and thus Spider-Syn does not contain a test set. Both Spider and Spider-Syn contain 7000 training and 1034 development samples respectively, where there are 146 databases for training and 20 for development. The SQL queries and schema annotations between Spider and Spider-Syn are the same; the difference is that the questions in Spider-Syn are modified from Spider by synonym substitution. Models are evaluated using the official exact matching accuracy metric of Spider. We first evaluate open-source models that reach competitive performance on Spider: GNN (Bogin et al., 2019a), IRNet (Guo et al., 2019) and RATSQL (Wang et al., 2020), on the Spider-Syn development set. We then evaluate our approaches with RAT-SQL+BERT model (denoted as RAT-SQLB ) on both Spider-Syn and Spider development set. We examine the robustness of following approaches for synonym substitution: • SPR: Indicate that the model is trained on the Spider dataset. • SPRSYN : Indicate that the model is trained on the Spider-Syn dataset . • SPRSPR&SYN : Indicate that the model is trained on both Spider and Spider-Syn datasets. • ADVBERT : To improve the robustness of text-toSQL models, we use advers"
2021.acl-long.195,D19-1378,0,0.0323664,"Missing"
2021.acl-long.195,P18-2006,0,0.0237286,"r-based models. We follow the standard adversarial training pipeline that iteratively generates adversarial examples, and trains the model on the dataset augmented with these adversarial examples. When generating adversarial examples for training, we aim to generate samples that align with the SpiderSyn principles, rather than arbitrary adversarial perturbations. We describe the details of adversarial example generation below. 3.2.1 Generating Adversarial Examples We choose BERT-Attack to generate the adversarial examples. Different from other word substitution methods (Mrkˇsi´c et al., 2016; Ebrahimi et al., 2018; Wei and Zou, 2019), BERT-Attack model considers the entire NL question when generating words for synonym substitution. Such a sentence-based method can generate different synonyms for the same word in different context. For example, the 2508 Input with domain information： Input without domain information： Which chief 's name has the substring ' Ha ' ? Which brain rain 's name has the substring ' Ha ' ? BERT-Attack BERT-Attack [CLS] Which head 's name has the substring ' Ha ' ? [SEP] How many heads of the departments are older than 56 ? [SEP] [CLS] Which head 's name has the substring ' Ha '"
2021.acl-long.195,2020.emnlp-main.500,0,0.0110758,"advantage of this method is that it does not require additional training, and could apply to existing models trained without synonym substitution questions. Annotating multiple schema words could be done automatically or manually, and we compare them in Section 4. 3.2 Adversarial Training Motivated by the idea of adversarial training that can improve the robustness of machine learning models against adversarial attacks (Madry et al., 2018; Morris et al., 2020), we implement adversarial training using the current open-source SOTA model RAT-SQL (Wang et al., 2020). We use the BERT-Attack model (Li et al., 2020) to generate adversarial examples, and implement the entire training process based on the TextAttack framework (Morris et al., 2020). TextAttack provides 82 pre-trained models, including word-level LSTM, word-level CNN, BERT-Attack, and other pre-trained Transformer-based models. We follow the standard adversarial training pipeline that iteratively generates adversarial examples, and trains the model on the dataset augmented with these adversarial examples. When generating adversarial examples for training, we aim to generate samples that align with the SpiderSyn principles, rather than arbitr"
2021.acl-long.195,2020.emnlp-demos.16,0,0.0560065,"Missing"
2021.acl-long.195,P19-1444,0,0.0855807,"essible, and thus Spider-Syn does not contain a test set. Both Spider and Spider-Syn contain 7000 training and 1034 development samples respectively, where there are 146 databases for training and 20 for development. The SQL queries and schema annotations between Spider and Spider-Syn are the same; the difference is that the questions in Spider-Syn are modified from Spider by synonym substitution. Models are evaluated using the official exact matching accuracy metric of Spider. We first evaluate open-source models that reach competitive performance on Spider: GNN (Bogin et al., 2019a), IRNet (Guo et al., 2019) and RATSQL (Wang et al., 2020), on the Spider-Syn development set. We then evaluate our approaches with RAT-SQL+BERT model (denoted as RAT-SQLB ) on both Spider-Syn and Spider development set. We examine the robustness of following approaches for synonym substitution: • SPR: Indicate that the model is trained on the Spider dataset. • SPRSYN : Indicate that the model is trained on the Spider-Syn dataset . • SPRSPR&SYN : Indicate that the model is trained on both Spider and Spider-Syn datasets. • ADVBERT : To improve the robustness of text-toSQL models, we use adversarial training methods to de"
2021.acl-long.195,N16-1018,0,0.0582999,"Missing"
2021.acl-long.195,P17-1089,0,0.196913,"Missing"
2021.acl-long.195,D14-1162,0,0.0837468,"Missing"
2021.acl-long.195,2020.intexsempar-1.5,0,0.0542513,"Missing"
2021.acl-long.195,P19-1103,0,0.0566433,"Missing"
2021.acl-long.195,W00-1317,0,0.189336,"Missing"
2021.acl-long.195,2020.acl-main.677,0,0.602223,"without changing the model input format. The main advantage of this method is that it does not require additional training, and could apply to existing models trained without synonym substitution questions. Annotating multiple schema words could be done automatically or manually, and we compare them in Section 4. 3.2 Adversarial Training Motivated by the idea of adversarial training that can improve the robustness of machine learning models against adversarial attacks (Madry et al., 2018; Morris et al., 2020), we implement adversarial training using the current open-source SOTA model RAT-SQL (Wang et al., 2020). We use the BERT-Attack model (Li et al., 2020) to generate adversarial examples, and implement the entire training process based on the TextAttack framework (Morris et al., 2020). TextAttack provides 82 pre-trained models, including word-level LSTM, word-level CNN, BERT-Attack, and other pre-trained Transformer-based models. We follow the standard adversarial training pipeline that iteratively generates adversarial examples, and trains the model on the dataset augmented with these adversarial examples. When generating adversarial examples for training, we aim to generate samples that align w"
2021.acl-long.195,D19-1670,0,0.0188178,"ow the standard adversarial training pipeline that iteratively generates adversarial examples, and trains the model on the dataset augmented with these adversarial examples. When generating adversarial examples for training, we aim to generate samples that align with the SpiderSyn principles, rather than arbitrary adversarial perturbations. We describe the details of adversarial example generation below. 3.2.1 Generating Adversarial Examples We choose BERT-Attack to generate the adversarial examples. Different from other word substitution methods (Mrkˇsi´c et al., 2016; Ebrahimi et al., 2018; Wei and Zou, 2019), BERT-Attack model considers the entire NL question when generating words for synonym substitution. Such a sentence-based method can generate different synonyms for the same word in different context. For example, the 2508 Input with domain information： Input without domain information： Which chief 's name has the substring ' Ha ' ? Which brain rain 's name has the substring ' Ha ' ? BERT-Attack BERT-Attack [CLS] Which head 's name has the substring ' Ha ' ? [SEP] How many heads of the departments are older than 56 ? [SEP] [CLS] Which head 's name has the substring ' Ha ' ? [SEP] Figure 5: In"
2021.acl-long.195,D18-1425,0,0.166751,"Missing"
2021.acl-long.195,2020.acl-demos.24,0,0.0614491,"Missing"
2021.acl-long.195,P19-1559,0,0.0357321,"Missing"
2021.acl-long.195,D19-1537,0,0.0419117,"Missing"
2021.emnlp-main.702,P19-1448,0,0.0178455,"stand that the user queries two • SpiderT : 125 examples drawn from the Spider columns by an omitted expression. training set. T2 requires the models to infer the correct • SpiderD : 535 examples drawn from the Spider queries, e.g., if the T2 utterance in Table 1 modidevelopment set. fied from `date of birth&apos; to `age&apos;, the model • Spider-DK: Spider-DK development set with should output desc not asc. Note that the Spider 535 examples. training set contains both `date of birth&apos; and We evaluate open-source models that reach com`age&apos; along with `old to young&apos;. petitive performance on Spider: GNN (Bogin et al., 2019), IRNet (Guo et al., 2019), RAT-SQL (Wang T3 requires the models to recognize the cell value synonym substitution. Some synonym sub- et al., 2020) with and without BERT (Devlin et al., 2019), and RAT-SQL + GAP (Shi et al., 2020).We stitutions base on their adjective form, such as present their results of the 265 Spider-DK do`singer whose country is France&apos; and main knowledge examples and analyze their perfor`French singer&apos;. Although the number of T4 is the least in Spider- mance in each knowledge type. Our evaluation is DK, it is not uncommon in the Spider training set. based on the exact matc"
2021.emnlp-main.702,2021.naacl-main.105,0,0.048342,"Missing"
2021.emnlp-main.702,N19-1423,0,0.0211668,"amples drawn from the Spider queries, e.g., if the T2 utterance in Table 1 modidevelopment set. fied from `date of birth&apos; to `age&apos;, the model • Spider-DK: Spider-DK development set with should output desc not asc. Note that the Spider 535 examples. training set contains both `date of birth&apos; and We evaluate open-source models that reach com`age&apos; along with `old to young&apos;. petitive performance on Spider: GNN (Bogin et al., 2019), IRNet (Guo et al., 2019), RAT-SQL (Wang T3 requires the models to recognize the cell value synonym substitution. Some synonym sub- et al., 2020) with and without BERT (Devlin et al., 2019), and RAT-SQL + GAP (Shi et al., 2020).We stitutions base on their adjective form, such as present their results of the 265 Spider-DK do`singer whose country is France&apos; and main knowledge examples and analyze their perfor`French singer&apos;. Although the number of T4 is the least in Spider- mance in each knowledge type. Our evaluation is DK, it is not uncommon in the Spider training set. based on the exact match metric defined in the original Spider benchmark, which measures whether Unlike the GeoQuery major example mentioned above, T4 only includes the conditions whose col- the predicted query wi"
2021.emnlp-main.702,2021.acl-long.195,1,0.858875,"Missing"
2021.emnlp-main.702,2020.aacl-srw.16,1,0.779385,"L ... in the order of birth date. ... order of their birth date from old to young. Modify both NL and SQL Compute the average age of dogs. select avg(age) from dogs Compute the average age of abandoned dogs. select avg(age) from dogs where abandoned_y = 1 Table 2: Examples of Spider question and/or SQL modifications made in Spider-DK. boolean-like type, but the difficulty is that the word varies in different domains. Although T5 seems simple and does not seem to contain domain knowledge, the models that generate SQL structure and schema items separately are easy to mispredict in T5. A review (Gan et al., 2020) shows that most models follow the separate generation pattern, i.e., these models may use the same word twice in both generating schema items and SQL structure. Because, in other domain training data, the models learn to generate a max() function when the utterance contains a word max. Therefore, these models may use the word max twice to generate the max(max_speed) for T5 utterance instead of a simple max_speed. edge required by these datasets is even hard to infer for experienced programmers. For example, we asked five computer science graduate students to write the SQL query for the questi"
2021.emnlp-main.702,P19-1444,0,0.0182674,"two • SpiderT : 125 examples drawn from the Spider columns by an omitted expression. training set. T2 requires the models to infer the correct • SpiderD : 535 examples drawn from the Spider queries, e.g., if the T2 utterance in Table 1 modidevelopment set. fied from `date of birth&apos; to `age&apos;, the model • Spider-DK: Spider-DK development set with should output desc not asc. Note that the Spider 535 examples. training set contains both `date of birth&apos; and We evaluate open-source models that reach com`age&apos; along with `old to young&apos;. petitive performance on Spider: GNN (Bogin et al., 2019), IRNet (Guo et al., 2019), RAT-SQL (Wang T3 requires the models to recognize the cell value synonym substitution. Some synonym sub- et al., 2020) with and without BERT (Devlin et al., 2019), and RAT-SQL + GAP (Shi et al., 2020).We stitutions base on their adjective form, such as present their results of the 265 Spider-DK do`singer whose country is France&apos; and main knowledge examples and analyze their perfor`French singer&apos;. Although the number of T4 is the least in Spider- mance in each knowledge type. Our evaluation is DK, it is not uncommon in the Spider training set. based on the exact match metric defined in the or"
2021.emnlp-main.702,P17-1089,0,0.0297212,"e Word Generate a Condition How many students got accepted after the tryout? ... from tryout where decision=""yes"" T5 NL SQL Easy to Conflict with other Domains ... with max speed higher than 1000. ... where max_speed &gt; 1000 Table 1: Five types of domain knowledge extracted from Spider training set. We name them as T1 to T5. 2.1 Spider-DK Dataset Overview Domain Knowledge Different SQL databases could require very different domain knowledge. As shown in (Suhr et al., 2020), the state-of-the-art models on Spider achieve much worse performance on earlier SQL benchmarks such as ATIS and GeoQuery (Iyer et al., 2017; Zelle and Mooney, 1996). However, we argue that the failure of generalization is expected to some extent, because without seeing in-domain examples, some domain knowlWe construct the Spider-DK benchmark by selecting samples from the Spider development set that require domain knowledge understanding, and we also manually modify some samples to incorporate domain knowledge. The purpose of building Spider-DK is to simulate the scenario where specific domain knowledge is involved in the users’ utterance query. Domain knowledge is often used unnoticedly, which makes some domain knowledge 8927 2 T"
2021.emnlp-main.702,2020.findings-emnlp.225,0,0.0750425,"Missing"
2021.emnlp-main.702,D14-1162,0,0.0858535,"Missing"
2021.emnlp-main.702,2021.spnlp-1.2,0,0.0251086,"Missing"
2021.emnlp-main.702,2021.acl-long.75,0,0.0734211,"Missing"
2021.emnlp-main.702,speer-havasi-2012-representing,0,0.0303761,"el to output a desc age order. The unbalance training data may lead the curacies of different domain knowledge types in model to prefer outputting a desc order even its Table 4. RAT-SQL + GAP unsurprisingly achieves the best performance on all examples and outper- column is the `date of birth&apos;. forms other models from T2 to T5. However, IRNet The second reason is that the model has insufsurprisingly obtains an overall accuracy close to ficient generalization ability for similar problems. the RAT-SQL + BERT, because IRNet integrates a Many training examples belong to the T3 and T4. ConceptNet (Speer and Havasi, 2012) to recognize However, these examples can not cover all cases. the country, state, and city synonyms, which can For example, the training data may not or rarely improve its accuracy in T3. The GNN and RAT- contain examples where the USA is substituted SQL perform relatively poorly on T3 because they with the United States, but we expect the models do not have extra knowledge components such as can still handle these examples correctly. The third ConceptNet. Besides, GNN trains its embeddings reason is that a word will be used twice to generate from scratch, and RAT-SQL uses GLOVE (Pen- schema"
2021.emnlp-main.702,2020.acl-main.742,0,0.0724753,"Missing"
2021.emnlp-main.702,2020.acl-main.677,0,0.0403356,"Missing"
2021.emnlp-main.702,D18-1425,0,0.0241971,"ese models may use the word max twice to generate the max(max_speed) for T5 utterance instead of a simple max_speed. edge required by these datasets is even hard to infer for experienced programmers. For example, we asked five computer science graduate students to write the SQL query for the question 3 Experiments `how many major cities are there?&apos; in GeoQuery, but none of them gave the correct answer. 3.1 Experimental Setup This question requires the domain knowledge that We evaluate the previous state-of-the-art models major means `population &gt; 150000&apos;, which is on the Spider-DK and Spider (Yu et al., 2018). As hard to infer without looking at GeoQuery training discussed in Section 2.1, the Spider test set is not set. Therefore, while acquiring general-purpose do- publicly accessible, and thus Spider-DK does not main knowledge is also important, we believe that contain a test set. We extracted 535 examples correthe failure of generalization to questions requiring sponding to Spider-DK from Spider for evaluation similar domain knowledge to the training set could instead of using a whole Spider development set be more problematic, which motivates our design for better comparison. In addition, we s"
2021.findings-emnlp.174,P19-1448,0,0.198351,"or’ |‘except’ |‘intersect’ W_Oper = ‘between’ |‘=’ |‘>’ |‘<’ |‘>=’ |‘<=’ |‘! =’ |‘in’ |‘like’ |‘is’ |‘exists’ |‘not in’ |‘not like’ |‘not between’ |‘is not’ |‘join’ ; Cond_R Cond_L = = NUMBER |STRING |Column ; Column |“@” ; Order_By = Column , [ DESC |ASC ] , [ LIMIT , NUMBER ] [ ‘and’ , NUMBER ] ; |‘union’ |‘sub’ ; Table 1: The main grammar of NatSQL. Here we highlight the differences of production rules from SQL. question. To achieve this goal, some existing neural networks implement a schema linking mechanism, by recognizing the tables and columns mentioned in a question (Guo et al., 2019; Bogin et al., 2019a; Wang et al., 2020). Schema linking is essential for text-to-SQL tasks. As shown in the ablation study of IRNet (Guo et al., 2019) and RAT-SQL (Wang et al., 2020), removing the schema linking results in a dramatic decrease in performance. The importance of schema linking raises a question about generating schema items not mentioned in the question. Some models use graph neural networks to find these unmentioned schema items, and some models delete unmentioned schema items based on the IR; e.g., in Figure 1, the IRs remove the JOIN ON and GROUP BY clauses with the unmentioned schema items. be"
2021.findings-emnlp.174,D19-1378,0,0.0323004,"Missing"
2021.findings-emnlp.174,P18-1068,0,0.0337788,"Missing"
2021.findings-emnlp.174,P18-1033,0,0.038811,"Missing"
2021.findings-emnlp.174,P17-1089,0,0.0487027,"Missing"
2021.findings-emnlp.174,D19-1624,0,0.017212,"SQLNet (Yu et al., 2018a), several types of IR have been developed for textto-SQL models on the Spider dataset. The main limitation of SyntaxSQLNet is that it removes the FROM and JOIN ON clauses, which may result in the failure to find the correct table when converted to SQL. For example, in Figure 1, SyntaxSQLNet IR misses the inventory table, thus it cannot generate the correct JOIN ON clause that appears in the original SQL. The IR for RAT-SQL (Wang et al., 2020) is mostly close to SQL, and it avoids missing tables since it only removes the JOIN ON clause from SQL. Zhong et al. (2020) and Lee (2019) also utilize an IR that is similar to the IR in RAT-SQL and SyntaxSQLNet. Guo et al. (2019) introduced SemQL, an intermediate language, to facilitate SQL prediction. 3 NatSQL As with NatSQL, SemQL removes the keywords 3.1 Overview FROM, JOIN ON, GROUP BY, HAVING from SQL. Table 1 presents the grammar specification of Nat- Although SemQL and NatSQL remove both FROM SQL. NatSQL only retains the SELECT, WHERE and JOIN ON clauses, SemQL and NatSQL avoid and ORDER BY clauses from SQL, dispensing with missing a table by moving the table into the ‘*’ other clauses such as GROUP BY, HAVING, FROM, col"
2021.findings-emnlp.174,C04-1021,0,0.173886,"Missing"
2021.findings-emnlp.174,2021.spnlp-1.2,0,0.0139543,"on in Appendix B). that need to be searched. The slots can appear in: WHERE clause, WHERE clause in a subquery, WHERE clause after set operators, HAVING clause, etc. On the other hand, when there are multiple value slots, it is easier to confuse where to fill. For example, in Figure 5, the two different questions correspond to the same SQL query, making it hard to copy the right values from the question to SQL. Because the condition value slots of NatSQL only appear in the WHERE clause, generating condition values becomes much easier, as shown in Figure 5. Unlike the models (Lin et al., 2020; Rubin and Berant, 2021) trained to copy the values from questions to SQL queries, NatSQL simply copies the possible values (numbers or database cell values) from questions to SQL in the order of appearance without training. This feature enables the models designed only for the Spider exact match metrics to generate executable SQL. for training, 20 for development and 40 for testing. We first evaluate the gold NatSQL and other IRs using the exact match and execution match metrics in (Yu et al., 2018b). Exact match measures whether the predicted query without condition values as a whole is equivalent to the gold query"
2021.findings-emnlp.174,W00-1317,0,0.412889,"Missing"
2021.findings-emnlp.174,2020.acl-main.677,0,0.113743,"QL). Specifically, with complex and nested SQL queries, on which NatSQL preserves the core functionalities of previous models designed for WikiSQL suffer a SQL, while it simplifies the queries as folsignificant performance drop. lows: (1) dispensing with operators and keyTo synthesize SQL queries with more comwords such as GROUP BY, HAVING, FROM, plex structures, intermediate representation (IR) JOIN ON, which are usually hard to find counterparts for in the text descriptions; (2) reis widely employed by the previous SOTA models moving the need for nested subqueries and on the Spider dataset (Wang et al., 2020; Guo et al., set operators; and (3) making schema link2019; Yu et al., 2018a; Shi et al., 2020). However, ing easier by reducing the required number of previous IRs are either too complicated or have limschema items. On Spider, a challenging textited coverage of SQL structures. Besides, although to-SQL benchmark that contains complex and the existing IRs eliminate part of the mismatch nested SQL queries, we demonstrate that Natbetween intent expressed in NL and the implemenSQL outperforms other IRs, and significantly tation details in SQL, there is still some mismatch improves the performance"
2021.findings-emnlp.174,J82-3002,0,0.705653,"Missing"
2021.findings-emnlp.174,2020.findings-emnlp.438,0,0.0374868,"(see more discussion in Appendix B). that need to be searched. The slots can appear in: WHERE clause, WHERE clause in a subquery, WHERE clause after set operators, HAVING clause, etc. On the other hand, when there are multiple value slots, it is easier to confuse where to fill. For example, in Figure 5, the two different questions correspond to the same SQL query, making it hard to copy the right values from the question to SQL. Because the condition value slots of NatSQL only appear in the WHERE clause, generating condition values becomes much easier, as shown in Figure 5. Unlike the models (Lin et al., 2020; Rubin and Berant, 2021) trained to copy the values from questions to SQL queries, NatSQL simply copies the possible values (numbers or database cell values) from questions to SQL in the order of appearance without training. This feature enables the models designed only for the Spider exact match metrics to generate executable SQL. for training, 20 for development and 40 for testing. We first evaluate the gold NatSQL and other IRs using the exact match and execution match metrics in (Yu et al., 2018b). Exact match measures whether the predicted query without condition values as a whole is equ"
2021.findings-emnlp.174,D18-1193,0,0.240886,"ersity of London UC Berkeley Jožef Stefan Institute 4 5 Guangxi University of Finance and Economics University of Leicester {y.gan,m.purver,j.woodward}@qmul.ac.uk xinyun.chen@berkeley.edu john.drake@leicester.ac.uk {jinxia_xie,qiaofuzhang}@hotmail.com Abstract queries in this benchmark only cover a single SELECT column and aggregation, as well as WHERE Addressing the mismatch between natural lanconditions, it does not represent the true complexity guage descriptions and the corresponding SQL of SQL generation. To facilitate more realistic evalqueries is a key challenge for text-to-SQL uation, Yu et al. (2018b) introduced Spider, the first translation. To bridge this gap, we propose an SQL intermediate representation (IR) large-scale cross-domain text-to-SQL benchmark called Natural SQL (NatSQL). Specifically, with complex and nested SQL queries, on which NatSQL preserves the core functionalities of previous models designed for WikiSQL suffer a SQL, while it simplifies the queries as folsignificant performance drop. lows: (1) dispensing with operators and keyTo synthesize SQL queries with more comwords such as GROUP BY, HAVING, FROM, plex structures, intermediate representation (IR) JOIN ON, which"
2021.findings-emnlp.174,D18-1425,0,0.0369622,"Missing"
2021.findings-emnlp.174,2020.emnlp-main.558,0,0.018616,"son Starting from SyntaxSQLNet (Yu et al., 2018a), several types of IR have been developed for textto-SQL models on the Spider dataset. The main limitation of SyntaxSQLNet is that it removes the FROM and JOIN ON clauses, which may result in the failure to find the correct table when converted to SQL. For example, in Figure 1, SyntaxSQLNet IR misses the inventory table, thus it cannot generate the correct JOIN ON clause that appears in the original SQL. The IR for RAT-SQL (Wang et al., 2020) is mostly close to SQL, and it avoids missing tables since it only removes the JOIN ON clause from SQL. Zhong et al. (2020) and Lee (2019) also utilize an IR that is similar to the IR in RAT-SQL and SyntaxSQLNet. Guo et al. (2019) introduced SemQL, an intermediate language, to facilitate SQL prediction. 3 NatSQL As with NatSQL, SemQL removes the keywords 3.1 Overview FROM, JOIN ON, GROUP BY, HAVING from SQL. Table 1 presents the grammar specification of Nat- Although SemQL and NatSQL remove both FROM SQL. NatSQL only retains the SELECT, WHERE and JOIN ON clauses, SemQL and NatSQL avoid and ORDER BY clauses from SQL, dispensing with missing a table by moving the table into the ‘*’ other clauses such as GROUP BY, HA"
2021.hackashop-1.14,2020.conll-1.35,1,0.709935,"arge manually labeled datasets are scarce. RaKUn9 (Škrlj et al., 2019) offers unsupervised detection and exploration of keyphrases. It transforms a document collection into a network, which is pruned to keep only the most relevant nodes. The nodes are ranked, prioritizing nodes corresponding to individual keywords and paths (keyphrases comprised of multiple words). Being unsupervised, RaKUn is well suited for less-resourced languages where expensive pre-training is not possible. 2.2.2 Named Entity Recognition10 The Named Entity Recognition (NER) system is based on the architecture proposed by Boros et al. (2020). It consists of fine-tuned BERT with two additional Transformer blocks (Vaswani et al., 2017). We provided models capable of predicting three types of named entities (Location, Organisation and Person) for eight European languages: Croatian, Estonian, Finnish, Latvian, Lithuanian, Russian, Slovene and Swedish. These models were trained using the WikiANN corpus (Pan et al., 2017), specifically using the training, development and testing partitions provided by Rahimi et al. (2019). Regarding BERT, for Croatian and Slovene we used CroSloEngual BERT (Ulˇcar and RobnikŠikonja, 2020); for Finnish a"
2021.hackashop-1.14,2021.hackashop-1.4,1,0.900804,"in the Russian 19 https://github.com/EMBEDDIA/crosslingual-classification-of-tweetsentiment 20 http://hdl.handle.net/11356/1054 21 http://newseye-wp5.cs.helsinki.fi: 4220/documentation/ 22 https://github.com/EMBEDDIA/ evolutionary-algorithm-for-NLG Task-specific News Datasets For the purposes of the hackashop, a set of taskspecific datasets were also gathered. 3.2.1 Keyword Extraction Datasplits For the keyword extraction challenge, we created train and test data splits, given as article IDs from datasets in Section 3.1. The number of articles for Estonian, Latvian, Russian and Croatian (see Koloski et al. (2021a) for details) are: 23 http://hdl.handle.net/11356/1408 http://hdl.handle.net/11356/1409 25 http://hdl.handle.net/11356/1410 26 http://urn.fi/urn:nbn:fi:lb2019041501 27 http://urn.fi/urn:nbn:fi:lb2020031201 24 103 3.3.1 • Croatian: 32,223 train, 3,582 test; • Estonian: 10,750 train, 7,747 test; This dataset is an archive of reader comments on the Ekspress Meedia news site from 2009–2019, containing approximately 31M comments, mostly in Estonian language, with some in Russian. The dataset is publicly available in CLARIN.32 • Russian: 13,831 train, 11,475 test; • Latvian: 13,133 train, 11,641 t"
2021.hackashop-1.14,2021.hackashop-1.16,1,0.833054,"Missing"
2021.hackashop-1.14,2021.hackashop-1.18,0,0.07548,"Missing"
2021.hackashop-1.14,W17-3528,1,0.823537,"lable in CLARIN.24 3.1.3 24sata News Archive (in Croatian) 24sata is the biggest Croatian news publisher, owned by the Styria Media Group. The 24sata news portal consists of a daily news portal and several smaller portals covering news on specific topics, such as automotive news, health, culinary content, and lifestyle advice. The dataset contains over 650,000 articles in Croatian between 2007– 2019, as well as assigned tags. The dataset is publicly available in CLARIN.25 Template-Based NLG System for Automated Journalism The rule-based natural language generation system—similar in concept to Leppänen et al. (2017)—produces news texts in Finnish and English from statistical data obtained from EuroStat. The system provides the text inputs used in the NLG challenges, described in Section 4.3. Access to the tool is provided through an API.21 Creative Language Generation We provide a framework22 to help in generation of creative language using an evolutionary algorithm (Alnajjar and Toivonen, 2020). 3 Datasets For the purposes of the hackashop, the EMBEDDIA media partners released their news archives, the majority of which are now being made publicly available for use after the project. 3.1 General EMBEDDIA"
2021.hackashop-1.14,N19-1423,0,0.157471,"d, and shared in the cloud. The main aim of ClowdFlows is to foster sharing of workflow solutions in order to simplify the replication and adaptation of shared work. It is suitable for prototyping, demonstrating new approaches, and exposing solutions to potential users who are not proficient in programming but would like to experiment with their own datasets and different tool parameter settings. 2 4 5 100 https://docs.texta.ee/ https://cf3.ijs.si/ 2.1.3 BERT Embeddings CroSloEngual6 FinEst7 BERT and BERT (Ulˇcar and Robnik-Šikonja, 2020) are trilingual models, based on the BERT architecture (Devlin et al., 2019), created in the EMBEDDIA project to facilitate easy cross-lingual transfer. Both models are trained on three languages: one of them being English as a resource-rich language, CroSloEngual BERT was trained on Croatian, Slovenian, and English data, while FinEst BERT was trained on Finnish, Estonian, and English data. The advantage of multi-lingual models over monolingual models is that they can be used for cross-lingual knowledge transfer, e.g., a model for a task for which very little data is available in a target language such as Croatian or Estonian can be trained on English (with more data"
2021.hackashop-1.14,2021.hackashop-1.17,1,0.812362,"Missing"
2021.hackashop-1.14,D09-1092,0,0.0997614,"Missing"
2021.hackashop-1.14,P17-1178,0,0.0249213,"ed, RaKUn is well suited for less-resourced languages where expensive pre-training is not possible. 2.2.2 Named Entity Recognition10 The Named Entity Recognition (NER) system is based on the architecture proposed by Boros et al. (2020). It consists of fine-tuned BERT with two additional Transformer blocks (Vaswani et al., 2017). We provided models capable of predicting three types of named entities (Location, Organisation and Person) for eight European languages: Croatian, Estonian, Finnish, Latvian, Lithuanian, Russian, Slovene and Swedish. These models were trained using the WikiANN corpus (Pan et al., 2017), specifically using the training, development and testing partitions provided by Rahimi et al. (2019). Regarding BERT, for Croatian and Slovene we used CroSloEngual BERT (Ulˇcar and RobnikŠikonja, 2020); for Finnish and Estonian FinEst BERT (Ulˇcar and Robnik-Šikonja, 2020); for Russian RuBERT (Kuratov and Arkhipov, 2019); for Swedish Swedish BERT (Malmsten et al., 2020); for Latvian and Lithuanian Multilingual BERT (Devlin et al., 2019). 2.2.3 Diachronic News Analysis11 TNT-KID8 (Transformer-based Neural Tagger for Keyword Identification, Martinc et al., 2020) is a supervised tool for extrac"
2021.hackashop-1.14,2021.hackashop-1.5,1,0.750065,"ghlighting. 12 https://github.com/EMBEDDIA/crosslingual-linking 13 https://github.com/EMBEDDIA/crosslingual-linking 14 https://github.com/EMBEDDIA/ multilingual_dtm 15 https://github.com/EMBEDDIA/TeMoCo News Sentiment Analysis16 2.3 News Comment Analysis Tools Several of the tools in the sections above can also be applied to comments. We describe the following comment-specific tools: comment moderation, bot and gender detection, and sentiment analysis tools. 2.3.1 Comment Moderation17 Our comment moderation tool flags inappropriate comments that should be blocked from appearing on news sites (Pelicon et al., 2021a,b). It uses multilingual BERT (Devlin et al., 2019) and the trilingual EMBEDDIA BERT models (Section 2.1.3). The models were trained on combinations of five datasets: Croatian and Estonian (see Section 3.3 and details in Shekhar et al. (2020)), Slovenian (Ljubeši´c et al., 2019), English (Zampieri et al., 2019), and German (Wiegand et al., 2018). For Croatian, we also provide a model to predict which rule is violated, based on the moderation policy of 24 sata, the biggest Croatian news publisher (see Section 3.3.3). 2.3.2 Bot and Gender Detection18 An author profiling tool for gender classif"
2021.hackashop-1.14,P19-1015,0,0.0139053,"2.2.2 Named Entity Recognition10 The Named Entity Recognition (NER) system is based on the architecture proposed by Boros et al. (2020). It consists of fine-tuned BERT with two additional Transformer blocks (Vaswani et al., 2017). We provided models capable of predicting three types of named entities (Location, Organisation and Person) for eight European languages: Croatian, Estonian, Finnish, Latvian, Lithuanian, Russian, Slovene and Swedish. These models were trained using the WikiANN corpus (Pan et al., 2017), specifically using the training, development and testing partitions provided by Rahimi et al. (2019). Regarding BERT, for Croatian and Slovene we used CroSloEngual BERT (Ulˇcar and RobnikŠikonja, 2020); for Finnish and Estonian FinEst BERT (Ulˇcar and Robnik-Šikonja, 2020); for Russian RuBERT (Kuratov and Arkhipov, 2019); for Swedish Swedish BERT (Malmsten et al., 2020); for Latvian and Lithuanian Multilingual BERT (Devlin et al., 2019). 2.2.3 Diachronic News Analysis11 TNT-KID8 (Transformer-based Neural Tagger for Keyword Identification, Martinc et al., 2020) is a supervised tool for extracting keywords from The tool for diachronic semantic shift detection (Martinc et al., 2019a) leverages"
2021.hackashop-1.14,2021.hackashop-1.19,0,0.0188473,"ent challenges related to comment analysis: • One team automated news comment moderation. They compiled and labeled a dataset of English news and social posts, and experimented with cross-lingual transfer of comment labels from English and subsequent supervised machine learning on Croatian and Estonian news comments (Korenˇci´c et al., 2021). • Another team looked at the diversity of news comment recommendations, motivated by democratic debate. They implemented a novel metric based on theories of democracy and used it to compare recommendation strategies of New York Times comments in English (Reuver and Mattis, 2021). Finally, one team worked on a generation task: • The team experimented with several methods for generating headlines, given the contents of a news story. They found that headlines formulated as questions about the story’s content tend to be both informative and enticing. 6 Conclusions This paper presents the contributions of the EMBEDDIA project, including a large variety of tools, new datasets of news articles and comments from the media partners, as well as challenges that were proposed to the participants of the EACL 2021 Hackathon on News Media Content Analysis and Automated Report Gener"
2021.hackashop-1.14,2021.hackashop-1.15,0,0.0178673,"and to wrap up the work at the end. Ample support on tools, 37 https://github.com/EMBEDDIA/embeddianlg-output-corpus 106 models, data and challenges was provided by the EMBEDDIA experts via several channels. The six teams all picked up different challenges and set themselves specific goals. Reports from five teams are included in these proceedings. Three teams worked on news content analysis: • One team developed a COVID-19 news dashboard to visualise sentiment in pandemicrelated news. The dashboard uses a multilingual BERT model to analyze news headlines in different languages across Europe (Robertson et al., 2021). • Methods for cross-border news discovery were developed by another team using multilingual topic models. Their tool discovers Latvian news that could interest Estonian readers (Koloski et al., 2021b). • A third team used sentiment and viewpoint analysis to study attitudes related to LGBTIQ+ in Slovenian news. Their results suggest that political affiliation of media outlets can affect sentiment towards and framing of LGBTIQ+specific topics (Martinc et al., 2021). Two teams looked at different challenges related to comment analysis: • One team automated news comment moderation. They compiled"
2021.hackashop-1.14,N19-1144,0,0.0196093,"be applied to comments. We describe the following comment-specific tools: comment moderation, bot and gender detection, and sentiment analysis tools. 2.3.1 Comment Moderation17 Our comment moderation tool flags inappropriate comments that should be blocked from appearing on news sites (Pelicon et al., 2021a,b). It uses multilingual BERT (Devlin et al., 2019) and the trilingual EMBEDDIA BERT models (Section 2.1.3). The models were trained on combinations of five datasets: Croatian and Estonian (see Section 3.3 and details in Shekhar et al. (2020)), Slovenian (Ljubeši´c et al., 2019), English (Zampieri et al., 2019), and German (Wiegand et al., 2018). For Croatian, we also provide a model to predict which rule is violated, based on the moderation policy of 24 sata, the biggest Croatian news publisher (see Section 3.3.3). 2.3.2 Bot and Gender Detection18 An author profiling tool for gender classification and bot detection in Spanish and English, trained on Twitter data (Martinc et al., 2019b), was developed for the PAN 2019 author profiling shared task (Rangel and Rosso, 2019). It uses a two-step approach: in the first step distinguishing between bots and humans, and in the second step determining the gen"
2021.hackashop-1.14,R19-1159,1,0.819443,"ee tools dealing with news topics: PTM, PDTM and TeMoCo. The first two use topics to link articles across languages, and the third one visualizes distributions of topics over time. PTM12 (Polylingual Topic Model, Mimno et al., 2009) can be used to train cross-lingual topic models and obtain cross-lingual topic vectors for news articles. These vectors can be used to link news articles across languages. An ensemble of crosslingual topic vectors and document embeddings can outperform stand-alone methods for crosslingual news linking (Zosa et al., 2020).13 PDTM14 (Polylingual Dynamic Topic Model, Zosa and Granroth-Wilding, 2019) is an extension of the Dynamic Topic Model (Blei and Lafferty, 2006) for multiple languages. This model can track the evolution of topics over time aligned across multiple languages. TeMoCo15 (Temporal Topic Visualisation, Sheehan et al., 2019, 2020) visualizes changes in topic distribution and associated keywords in a document or collection of articles. The tool can investigate a single document or a corpus which has been temporally annotated (e.g., a transcript or corpus of dated articles). The user can examine an overview of a dataset, processed into time and topic segments. The changes in"
2021.hackashop-1.14,2020.clssts-1.6,1,0.827694,"positive, negative, and neutral. Topic Analysis We present three tools dealing with news topics: PTM, PDTM and TeMoCo. The first two use topics to link articles across languages, and the third one visualizes distributions of topics over time. PTM12 (Polylingual Topic Model, Mimno et al., 2009) can be used to train cross-lingual topic models and obtain cross-lingual topic vectors for news articles. These vectors can be used to link news articles across languages. An ensemble of crosslingual topic vectors and document embeddings can outperform stand-alone methods for crosslingual news linking (Zosa et al., 2020).13 PDTM14 (Polylingual Dynamic Topic Model, Zosa and Granroth-Wilding, 2019) is an extension of the Dynamic Topic Model (Blei and Lafferty, 2006) for multiple languages. This model can track the evolution of topics over time aligned across multiple languages. TeMoCo15 (Temporal Topic Visualisation, Sheehan et al., 2019, 2020) visualizes changes in topic distribution and associated keywords in a document or collection of articles. The tool can investigate a single document or a corpus which has been temporally annotated (e.g., a transcript or corpus of dated articles). The user can examine an"
2021.hackashop-1.5,Q19-1038,0,0.159082,"ere is a need for a general model that could be used in content filtering systems to automatically detect such discourse. Since the majority of research in the area of offensive language and hate speech detection is currently done in monolingual settings, we performed a preliminary study to assess the feasibility of the proposed zero-shot cross-lingual transfer for this task. Two approaches are tested in this study. The first uses multilingual Bidirectional Encoder Representations from Transformers (BERT, Devlin et al., 2019). The second uses Language-Agnostic SEntence Representations (LASER, Artetxe and Schwenk, 2019), a system built specifically for zero-shot cross-lingual transfer using multilingual sentence embeddings. Our best performing model is available online and can be used for detecting offensive content in less-resourced languages with no available training data. We present a system for zero-shot crosslingual offensive language and hate speech classification. The system was trained on English datasets and tested on a task of detecting hate speech and offensive social media content in a number of languages without any additional training. Experiments show an impressive ability of both models to g"
2021.hackashop-1.5,S19-2007,0,0.0650014,"Missing"
2021.hackashop-1.5,P19-4007,0,0.0455279,"Missing"
2021.hackashop-1.5,N19-1423,0,0.0279266,"tional manual inspection of the content (Schmidt and Wiegand, 2017). As a consequence, there is a need for a general model that could be used in content filtering systems to automatically detect such discourse. Since the majority of research in the area of offensive language and hate speech detection is currently done in monolingual settings, we performed a preliminary study to assess the feasibility of the proposed zero-shot cross-lingual transfer for this task. Two approaches are tested in this study. The first uses multilingual Bidirectional Encoder Representations from Transformers (BERT, Devlin et al., 2019). The second uses Language-Agnostic SEntence Representations (LASER, Artetxe and Schwenk, 2019), a system built specifically for zero-shot cross-lingual transfer using multilingual sentence embeddings. Our best performing model is available online and can be used for detecting offensive content in less-resourced languages with no available training data. We present a system for zero-shot crosslingual offensive language and hate speech classification. The system was trained on English datasets and tested on a task of detecting hate speech and offensive social media content in a number of langua"
2021.hackashop-1.5,2020.coling-main.559,0,0.0479891,"Missing"
2021.hackashop-1.5,P18-2061,0,0.035549,"Missing"
2021.hackashop-1.5,W19-3506,0,0.0975429,"egand et al., 2018), which also contains manually labeled tweets. Both datasets use hierarchical annotation schemes for annotating hate speech content. For our purposes, we employed only the annotations on the first level which classify tweets into two classes, offensive and not offensive. We trained the hate speech classifiers on the English training set from the HatEval dataset (Basile et al., 2019). For evaluation, we used the English and Spanish (ES) test sets from the HatEval competition, the German (DE) IGW hate speech dataset (Ross et al., 2016), an Indonesian (ID) hate speech dataset (Ibrohim and Budi, 2019) and the Arabic (AR) hate speech dataset LHSAB (Mulki et al., 2019). Each of the test datasets had binary labels that denoted the presence or absence of hate speech, except for the Arabic test set, which modeled hate speech as a three-class task, with labels denoting absence of hate speech, abusive language and hateful language. Since the authors themselves acknowledge there is a fine line between abusive and hateful language, we felt confident to join them into one class that denotes the presence of hate speech in a tweet. Tweets in the German IGW dataset included hate speech labels from two"
2021.hackashop-1.5,W19-3512,0,0.0202673,"datasets use hierarchical annotation schemes for annotating hate speech content. For our purposes, we employed only the annotations on the first level which classify tweets into two classes, offensive and not offensive. We trained the hate speech classifiers on the English training set from the HatEval dataset (Basile et al., 2019). For evaluation, we used the English and Spanish (ES) test sets from the HatEval competition, the German (DE) IGW hate speech dataset (Ross et al., 2016), an Indonesian (ID) hate speech dataset (Ibrohim and Budi, 2019) and the Arabic (AR) hate speech dataset LHSAB (Mulki et al., 2019). Each of the test datasets had binary labels that denoted the presence or absence of hate speech, except for the Arabic test set, which modeled hate speech as a three-class task, with labels denoting absence of hate speech, abusive language and hateful language. Since the authors themselves acknowledge there is a fine line between abusive and hateful language, we felt confident to join them into one class that denotes the presence of hate speech in a tweet. Tweets in the German IGW dataset included hate speech labels from two annotators and no common label, so we decided to evaluate only on t"
2021.hackashop-1.5,N19-1144,0,0.163622,"e without any additional language specific training. In this study we present an offensive language classifier available through a REST API which leverages the cross-lingual capabilities of these systems. Due to the exponential growth of social media content, the amount of offensive language 2 Related work The large majority of research on hate speech is monolingual, with English still the most popular language due to data availability (Wulczyn et al., 2017; Davidson et al., 2017), and a number of English-only shared tasks organized on the topic of hate or offensive speech (e.g., OffenseEval, Zampieri et al., 2019b). Lately, the focus has been shifting to other languages, with several shared tasks organized that cover other languages besides English, e.g. OffenseEval 2020 (Zampieri et al., 2020), EVALITA 2018 (Bai et al., 2018) and GermEval 2018 (Wiegand et al., 2018). 30 Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation, pages 30–34 April 19, 2021 © Association for Computational Linguistics 3 For example, the EVALITA 2018 shared task (Bai et al., 2018) covered hate speech in Italian social media, the GermEval 2018 (Wiegand et al., 2018) shared tasks explo"
2021.hackashop-1.5,D19-1474,0,0.0201665,"which modeled hate speech as a three-class task, with labels denoting absence of hate speech, abusive language and hateful language. Since the authors themselves acknowledge there is a fine line between abusive and hateful language, we felt confident to join them into one class that denotes the presence of hate speech in a tweet. Tweets in the German IGW dataset included hate speech labels from two annotators and no common label, so we decided to evaluate only on those tweets where the two annotators agreed. The statistics of the datasets that were used in this study are reported in Table 1. Ousidhoum et al. (2019) conduct multilingual hate speech studies by testing a number of traditional bag-of-words and neural models on a multilingual dataset containing English, French and Arabic tweets that were manually labeled with six class hostility labels (abusive, hateful, offensive, disrespectful, fearful, normal). They report that multilingual models outperform monolingual models on some of the tasks. Shekhar et al. (2020) study multilingual comment filtering for newspaper comments in Croatian and Estonian. Another multilingual approach was proposed by Schneider et al. (2018), who used multilingual MUSE embe"
2021.hackashop-1.5,S19-2010,0,0.145595,"e without any additional language specific training. In this study we present an offensive language classifier available through a REST API which leverages the cross-lingual capabilities of these systems. Due to the exponential growth of social media content, the amount of offensive language 2 Related work The large majority of research on hate speech is monolingual, with English still the most popular language due to data availability (Wulczyn et al., 2017; Davidson et al., 2017), and a number of English-only shared tasks organized on the topic of hate or offensive speech (e.g., OffenseEval, Zampieri et al., 2019b). Lately, the focus has been shifting to other languages, with several shared tasks organized that cover other languages besides English, e.g. OffenseEval 2020 (Zampieri et al., 2020), EVALITA 2018 (Bai et al., 2018) and GermEval 2018 (Wiegand et al., 2018). 30 Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation, pages 30–34 April 19, 2021 © Association for Computational Linguistics 3 For example, the EVALITA 2018 shared task (Bai et al., 2018) covered hate speech in Italian social media, the GermEval 2018 (Wiegand et al., 2018) shared tasks explo"
2021.hackashop-1.5,P19-2051,0,0.0170866,"et al. (2018), who used multilingual MUSE embeddings (Lample et al., 2018) in order to extend the GermEval 2018 German train set with more English data. They report that no improvements in accuracy were achieved with this approach. Cross-lingual hate speech identification is even less researched than the multilingual task. The so-called bleaching approach (van der Goot et al., 2018) was used by Basile and Rubagotti (2018) to conduct cross-lingual experiments between Italian and English at EVALITA 2018 misogyny identification task. The only other study we are aware of is a very recent study by Pamungkas and Patti (2019) proposing an LSTM joint-learning model with multilingual MUSE embeddings. Google Translate is used for translation in order to create a bilingual train and test input data. Bassignana et al. (2018) report that the use of a multilingual lexicon of hate words, HurtLex, slightly improves the performance of misogyny identification systems. Closest to our work is that of Glavaˇs et al. (2020), who propose a dataset called XHATE-999 to evaluate abusive language detection in a multi-domain and multilingual setting. 4 Classification models and methodology Our models were trained and evaluated on two"
2021.hackashop-1.5,W17-1101,0,0.172285,"ss-lingual Content Filtering: Offensive Language and Hate Speech Detection Andraˇz Pelicon1 , Ravi Shekhar3 , Matej Martinc1,2 1,2 ˇ Blaˇz Skrlj , Matthew Purver2,3 , Senja Pollak2 1 Joˇzef Stefan International Postgraduate School 2 Joˇzef Stefan Institute, Ljubljana, Slovenia 3 Computational Linguistics Lab, Queen Mary University of London, UK {andraz.pelicon,matej.martinc,blaz.skrlj,senja.pollak}@ijs.si {r.shekhar,m.purver}@qmul.ac.uk Abstract and hate speech has seen a steep increase and its identification and removal is no longer manageable by traditional manual inspection of the content (Schmidt and Wiegand, 2017). As a consequence, there is a need for a general model that could be used in content filtering systems to automatically detect such discourse. Since the majority of research in the area of offensive language and hate speech detection is currently done in monolingual settings, we performed a preliminary study to assess the feasibility of the proposed zero-shot cross-lingual transfer for this task. Two approaches are tested in this study. The first uses multilingual Bidirectional Encoder Representations from Transformers (BERT, Devlin et al., 2019). The second uses Language-Agnostic SEntence Re"
2021.sigdial-1.32,2020.lrec-1.147,0,0.0374835,"e signal-non-understanding, simple yesanswers and clarification requests in cognitively impaired patients’ conversations. Computational work that leverages these features is rare, however. Many diagnosis classification models include some signals associated with non-understanding (e.g. Fraser et al., 2016; Broderick et al., 2018) but only as part of large general language feature sets. One reason for this is that many studies use data that contains little interaction: the commonly used DementiaBank Pitt corpus, for example, contains conversations of a very one-sided nature. In a recent study, Farzana et al. (2020) developed an annotation scheme with 26 DAs based on ISO standard (Bunt, 2011) on DementiaBank data set to facilitate automated cognitive health screening from conversational interviews. They investigated phenomena like clarification request but some of the tags are specific to Cookie Theft Picture description task (Goodglass et al., 2001) and are not very general. Some recent work uses a more truly interactive approach: Luz et al. (2018) use a probabilistic graphical model to classify AD patients in the CCC corpus, although they use pauses and vocalisation times rather than any DA information"
2021.sigdial-1.32,W13-3214,0,0.0339827,"Missing"
2021.sigdial-1.32,L18-1307,0,0.0269097,"Missing"
2021.sigdial-1.32,N16-1062,0,0.0440721,"Missing"
2021.sigdial-1.32,W01-1616,1,0.520129,"with the standard DAMSL tagset (Stolcke et al., 2000) and adapt it. Based on the clinical studies described in Section 2, we keep 17 specific DA tags of interest from DAMSL; split 2 of them each into 2 sub-categories; and collapse all other tags into a single other tag, giving a total of 20 tags. The two new DA tags are clarification-request (qc) and statementanswer (sa): clarification-request (qc) is a subcategory of signal-non-understanding (br) which 292 Figure 1: Model architecture for DA classification with one utterance and one DA as context. requests more specific information (see e.g. Purver et al., 2001; Rodr´ıguez and Schlangen, 2004); while statement-answer (sa) is a sub-category of declarative-statement (sd) used as an answer to a wh-question (qw), open-question (qo) or orquestion (qr). The full tagset1 is shown in Table 1. 4.2 the home or community settings. Each patient is interviewed by a different interviewer. The CCC includes some uniform questions that are collectionspecific for people specific to health conditions, diseases, and cognitively-impaired speakers with dementia. It is transcribed but not annotated with DA tags. Access to the data was granted after ethical review by the b"
2021.sigdial-1.32,N19-1373,0,0.0538055,", when training on gold-standard labels but using previously predicted DAs as context during testing — a more realistic approach in real-time systems — we achieve reasonable performance which improves as the context window increases, suggesting that further improvements may be gained by using more predicted DA 295 labels as context. Our interest, of course, is not in macro-average figures but in predicting the distribution over the individual DA classes. We therefore, examine the class-wise prediction scores, showing a selection of classes in Figure 2. We note that performance exceeds that of Raheja and Tetreault (2019) (see Section 2) by a very large margin in all cases. Classwise results for each class in our tagset can be found in supplementary materials. form ‘yes’ to represent a backchannel. Some surface forms of backchannels are also present in the CCC dataset but did not occur in SwDA, and are thus misclassified when testing on CCC. We further analyzed the effect of adding utterance/DA context on individual DA classes, with results shown in Figure 3 and Figure 4. Yes-answer (ny) recall improved from 0.22 to 0.58 when including only one preceding utterance, and is further improved to 0.75 by adding the"
2021.sigdial-1.32,J00-3003,0,0.766712,"Missing"
2021.sigdial-1.32,W17-5530,0,0.0272314,"Missing"
2021.sigdial-1.32,D14-1162,0,0.0850638,"s a sequence of utterances U = {U1 , U2 , U3 , ..., Un } paired with a sequence of DA labels Y = {da1 , da2 , da3 , ..., dan }; each utterance Ut ∈ U is a sequence of words Ut = {wt1 , wt2 , ..., wtm }. Figure 1 shows the overall architecture of our model in which Ut represents the current utterance and Ut−1 represents the previous utterance. We use word embeddings to extract the lexical feature representations from the transcripts, converting the utterances from word sequences into sequences of word vectors. We compared the use of randomly initialised embeddings, GloVe pretrained embeddings (Pennington et al., 2014), GloVe embeddings trained on SwDA and CCC corpus, and ELMo embeddings (Peters et al., 2018). This word representation layer feeds into a BiLSTM, producing a representation of an utterance as a sequence of hidden vectors ht = {h1t , h2t , ..., hm t }. We use an attention mechanism to weight these and aggregate them into a single utterance representation, an attention vector ct is representing the whole utterance Ut . We then concatenate the vector for the current utterance ct with various combinations of information from previous context: the previous utterance vector ct−1 , previous DA (dat−1"
2021.sigdial-1.32,N18-1202,0,0.0321128,"{da1 , da2 , da3 , ..., dan }; each utterance Ut ∈ U is a sequence of words Ut = {wt1 , wt2 , ..., wtm }. Figure 1 shows the overall architecture of our model in which Ut represents the current utterance and Ut−1 represents the previous utterance. We use word embeddings to extract the lexical feature representations from the transcripts, converting the utterances from word sequences into sequences of word vectors. We compared the use of randomly initialised embeddings, GloVe pretrained embeddings (Pennington et al., 2014), GloVe embeddings trained on SwDA and CCC corpus, and ELMo embeddings (Peters et al., 2018). This word representation layer feeds into a BiLSTM, producing a representation of an utterance as a sequence of hidden vectors ht = {h1t , h2t , ..., hm t }. We use an attention mechanism to weight these and aggregate them into a single utterance representation, an attention vector ct is representing the whole utterance Ut . We then concatenate the vector for the current utterance ct with various combinations of information from previous context: the previous utterance vector ct−1 , previous DA (dat−1 ) (gold-standard or predicted, see Section 4), and their preceding neighbours ct−2 , dat−2"
2021.sigdial-1.56,W09-3934,0,0.0358397,"thub.com/mladenk42/decibert 542 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 542–547 July 29–31, 2021. ©2021 Association for Computational Linguistics a model using support vector machines (SVMs) to classify each DDA, using the outputs to predict decision discussion regions. Similarly, Frampton et al. (2009) explore real-time decision detection. Further improvements have been shown via more explicit modeling of decision-making dialogue structure, encoded as probabilistic graphical models, and including non-lexical and prosodic features (Bui et al., 2009; Bui and Peters, 2010), but at the cost of assuming a fixed structure to a discussion rather than learning it from data. In contrast to related work, our primary focus is exploring the, thus far unaddressed, topic bias issues rather than maximum performance. Consequently, we opt for simpler models that use only the text without additional features. We include one traditional and one deep learning based model. 3 Dataset We use the dataset introduced by Fern´andez et al. (2008), an annotated subset of transcripts from the AMI meeting corpus (McCowan et al., 2005) covering 17 meetings in which a"
2021.sigdial-1.56,P10-2057,0,0.0795916,"rdings or transcripts can be a valuable resource, but we need automatic processing and summarization methods if we are to be able to quickly search and retrieve the information we need. According to user surveys, the primary requirement of users from a meeting summarization system is a record of the decisions made (Lisowska et al., 2004; Banerjee et al., 2005). It can allow tracking of decisions and the reasoning behind them, as well as alternatives that were proposed and discussed. Previous work on the task of automatic decision detection (e.g. Hsueh and Moore, 2007; Fern´andez et al., 2008; Bui and Peters, 2010) shows that the problem is challenging: performance is limited (Fern´andez et al., 2008) unless strong assumptions about the nature of the data are made (Bui and Peters, 2010). E.g., assuming particular structure of the dialogue, rather than learning it from data. One reason for this is the lack of large datasets for the task. Here, we show that previous models are also affected by another issue resulting from lack of data: topic bias. The intuition behind this problem is that the models might pick up on words that decisions are about instead of words that generally indicate decision making. A"
2021.sigdial-1.56,N19-1423,0,0.00687255,"able 1: Utterance counts and percentages for the three DDA categories – Issue (I), Resolution (R), and Agreement (A), with examples. of each utterance. Then, we use a similar baseline as the one in (Fern´andez et al., 2008). We include context by extending the vector of each utterance with vectors of nearby utterances in a context window of size N around it. We feed the extended representations into a logistic regression classifier. BERT-LSTM As the basis of our deep learning approach we use BERT, a popular transformerbased language model shown to perform well across a diverse range of tasks (Devlin et al., 2019). Specifically we use SentenceBERT (Reimers and Gurevych, 2019) to generate a 768-dimensional vector representation for each utterance. To generate a prediction for utterance uk at position k, given a context window of size N , we consider the sequence of BERT vector representations for utterances uk− N ...uk+ N , of length N . We run 2 2 a bidirectional long-short term memory (LSTM, Hochreiter and Schmidhuber, 1997) network over this sequence, yielding N hidden state outputs.2 Each output is fed into 3 separate linear + softmax layers, producing three separate binary decisions, one for each D"
2021.sigdial-1.56,W08-0125,1,0.646842,"Missing"
2021.sigdial-1.56,D09-1118,0,0.0338028,"features; but in this domain performance is lower (Hsueh and Moore, 2007). Fern´andez et al. (2008) improve on this by considering the structure of the decision-making dialogue: they propose a set of decision-specific dialogue acts (DDAs) and 1 https://github.com/mladenk42/decibert 542 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 542–547 July 29–31, 2021. ©2021 Association for Computational Linguistics a model using support vector machines (SVMs) to classify each DDA, using the outputs to predict decision discussion regions. Similarly, Frampton et al. (2009) explore real-time decision detection. Further improvements have been shown via more explicit modeling of decision-making dialogue structure, encoded as probabilistic graphical models, and including non-lexical and prosodic features (Bui et al., 2009; Bui and Peters, 2010), but at the cost of assuming a fixed structure to a discussion rather than learning it from data. In contrast to related work, our primary focus is exploring the, thus far unaddressed, topic bias issues rather than maximum performance. Consequently, we opt for simpler models that use only the text without additional features"
2021.sigdial-1.56,D19-1011,0,0.0300882,"Missing"
2021.sigdial-1.56,N07-1004,0,0.185732,"such meetings in the form of video or audio recordings or transcripts can be a valuable resource, but we need automatic processing and summarization methods if we are to be able to quickly search and retrieve the information we need. According to user surveys, the primary requirement of users from a meeting summarization system is a record of the decisions made (Lisowska et al., 2004; Banerjee et al., 2005). It can allow tracking of decisions and the reasoning behind them, as well as alternatives that were proposed and discussed. Previous work on the task of automatic decision detection (e.g. Hsueh and Moore, 2007; Fern´andez et al., 2008; Bui and Peters, 2010) shows that the problem is challenging: performance is limited (Fern´andez et al., 2008) unless strong assumptions about the nature of the data are made (Bui and Peters, 2010). E.g., assuming particular structure of the dialogue, rather than learning it from data. One reason for this is the lack of large datasets for the task. Here, we show that previous models are also affected by another issue resulting from lack of data: topic bias. The intuition behind this problem is that the models might pick up on words that decisions are about instead of"
2021.sigdial-1.56,lisowska-etal-2004-user,0,0.0651602,"xplore this by removing topic information from the train data. We show that this resolves the bias issues to an extent and, surprisingly, sometimes even boosts performance. 1 Intro We spend a lot of our time in meetings. Recordings of such meetings in the form of video or audio recordings or transcripts can be a valuable resource, but we need automatic processing and summarization methods if we are to be able to quickly search and retrieve the information we need. According to user surveys, the primary requirement of users from a meeting summarization system is a record of the decisions made (Lisowska et al., 2004; Banerjee et al., 2005). It can allow tracking of decisions and the reasoning behind them, as well as alternatives that were proposed and discussed. Previous work on the task of automatic decision detection (e.g. Hsueh and Moore, 2007; Fern´andez et al., 2008; Bui and Peters, 2010) shows that the problem is challenging: performance is limited (Fern´andez et al., 2008) unless strong assumptions about the nature of the data are made (Bui and Peters, 2010). E.g., assuming particular structure of the dialogue, rather than learning it from data. One reason for this is the lack of large datasets fo"
2021.sigdial-1.56,D19-1410,0,0.0138638,"DA categories – Issue (I), Resolution (R), and Agreement (A), with examples. of each utterance. Then, we use a similar baseline as the one in (Fern´andez et al., 2008). We include context by extending the vector of each utterance with vectors of nearby utterances in a context window of size N around it. We feed the extended representations into a logistic regression classifier. BERT-LSTM As the basis of our deep learning approach we use BERT, a popular transformerbased language model shown to perform well across a diverse range of tasks (Devlin et al., 2019). Specifically we use SentenceBERT (Reimers and Gurevych, 2019) to generate a 768-dimensional vector representation for each utterance. To generate a prediction for utterance uk at position k, given a context window of size N , we consider the sequence of BERT vector representations for utterances uk− N ...uk+ N , of length N . We run 2 2 a bidirectional long-short term memory (LSTM, Hochreiter and Schmidhuber, 1997) network over this sequence, yielding N hidden state outputs.2 Each output is fed into 3 separate linear + softmax layers, producing three separate binary decisions, one for each DDA class.3 Thus, for each utterance we obtain, as a byproduct,"
2021.sigdial-1.56,N16-3020,0,0.0357163,"cific vocabulary; and are absent for the A category, which uses much fewer topic words. The SEG scores also modestly increase, as small improvements for individual utterances have some influence on the overall output. To better understand this phenomenon in the Figure 2: Feature influences derived by the LIME method for BERT-LSTM models without (left) and with (right) masking. Positive influence values denote a word is pushing the prediction towards the positive category, and vice versa for negative ones. Rows represent utterances. BERT-LSTM model, we applied the LIME feature analysis method (Ribeiro et al., 2016). Figure 2 illustrates the results for two utterances. For the first utterance, we see that after masking, the model relies much more on the word decision than on the domain-specific words chip or print. In this case masking corrected the output of the model from 0 to 1. In the second utterance, however, shifting the focus from the domain-specific backup to the more general Are we going to phrase, while seemingly desirable, causes a mistake changing the prediction from 1 to 0. We hypothesize this is due to lack of data to learn all decision indicative phrases properly. These insights and the r"
2021.sigdial-1.56,2020.emnlp-main.66,0,0.0274193,"he cross-entropy loss, weighted to account for the highly imbalanced number of positive and negative examples in each category.4 We use this as it works with multilabel annotations. When making predictions with this model for utterance uk with respect to class c, we run the above model for a context window of size N around uk and take the center prediction, i.e. yc,k,k . Since the goal of this paper is to explore bias, rather than maximize performance, we stick to this simpler deep learning approach and leave the investigation of more complex alternatives, such as dialog oriented models from (Wu et al., 2020; Gu et al., 2020) to future work. Both models are implemented using Scikit-learn (Pedregosa et al., 2011) and PyTorch (Paszke et al., 2019). The hyperparameters and other training details of all models are provided in Section 5. 4.2 Evaluation metrics The models are evaluated using the metrics of Fern´andez et al. (2008), with two evaluation setups described below. Utterance level evaluation (ULE) This approach is implemented as described by Hsueh and Moore (2007). In essence it is a lenient variant of F-score that works on the level of individual utterances but tolerates a level of misalignm"
D14-1009,P00-1037,0,0.135401,"tors, verbatim repeats withstanding, agreement is often poor (Hough and Purver, 2013; Shriberg, 1994). Assigning and evaluating repair (not just reparandum) structures will allow repair interpretation in future; however, work to date evaluates only reparandum detection. 3.1 Our approach To address the above, we propose an alternative to (Johnson and Charniak, 2004; Zwarts et al., 2010)’s noisy channel model. While the model elegantly captures intuitions about parallelism in repairs and modelling fluency, it relies on stringmatching, motivated in a similar way to automatic spelling correction (Brill and Moore, 2000): it assumes a speaker chooses to utter fluent utterance X according to some prior distribution P (X), but a noisy channel causes them instead to utter a noisy Y according to channel model P (Y |X). Estimating P (Y |X) directly from observed data is difficult due to sparsity of repair instances, so a transducer is trained on the rough copy alignments between reparandum and repair. This approach succeeds because repetition and simple substitution repairs are very common; but repair as a psychological process is not driven by string alignment, and deletes, restarts and rarer substitution forms a"
D14-1009,W13-2604,0,0.0613221,"ed}rpstart ...rpend ]... ...{ed}... “uh” ed S0 S1 “John” S2 “likes” ed S3 “uh” ed S0 S1 “John” S2 “likes” “loves” S1 “likes” rmstart rmend S1 “likes” rmstart rmend S1 S3 “loves” ed rpstart rpsub end S4 rpsub end ed S3 T4 “Mary” “uh” “loves” ed rpstart rpsub end rmstart S2 We derive the basic information-theoretic features required using n-gram language models, as they have a long history of information theoretic analysis (Shannon, 1948) and provide reproducible results without forcing commitment to one particular grammar formalism. Following recent work on modelling grammaticality judgements (Clark et al., 2013), we implement several modifications to standard language models to develop our basic measures of fluency and uncertainty. For our main fluent language models we train a trigram model with Kneser-Ney smoothing (Kneser and Ney, 1995) on the words and POS tags of the standard Switchboard training data (all files with conversation numbers beginning sw2*,sw3* in the Penn Treebank III release), consisting of ≈100K utterances, ≈600K words. We follow (Johnson and Charniak, 2004) by cleaning the data of disfluencies (i.e. edit terms and reparanda), to approximate a ‘fluent’ language pos model. We call"
D14-1009,W04-3241,0,0.0829987,"Missing"
D14-1009,N09-2028,0,0.0214418,"propose new repair processing evaluation standards. 1 Introduction Self-repairs in spontaneous speech are annotated according to a well established three-phase structure from (Shriberg, 1994) onwards, and as described in Meteer et al. (1995)’s Switchboard corpus annotation handbook: John [ likes + {uh} loves ] Mary |{z } |{z } |{z } reparandum interregnum 2 Previous work Qian and Liu (2013) achieve the state of the art in Switchboard corpus self-repair detection, with an F-score for detecting reparandum words of 0.841 using a three-step weighted Max-Margin Markov network approach. Similarly, Georgila (2009) uses Integer Linear Programming post-processing of a CRF to achieve F-scores over 0.8 for reparandum start and repair start detection. However neither approach can operate incrementally. Recently, there has been increased interest in left-to-right repair detection: Rasooli and Tetreault (2014) and Honnibal and Johnson (2014) present dependency parsing systems with reparandum detection which perform similarly, the latter equalling Qian and Liu (2013)’s F-score at 0.841. However, while operating left-to-right, these systems are not designed or evaluated for their incremental performance. The us"
D14-1009,C08-1072,0,0.0498907,"Missing"
D14-1009,N13-1102,0,0.185339,"cy on a par with state-of-the-art incremental repair detection methods, but with better incremental accuracy, faster time-to-detection and less computational overhead. We evaluate its performance using incremental metrics and propose new repair processing evaluation standards. 1 Introduction Self-repairs in spontaneous speech are annotated according to a well established three-phase structure from (Shriberg, 1994) onwards, and as described in Meteer et al. (1995)’s Switchboard corpus annotation handbook: John [ likes + {uh} loves ] Mary |{z } |{z } |{z } reparandum interregnum 2 Previous work Qian and Liu (2013) achieve the state of the art in Switchboard corpus self-repair detection, with an F-score for detecting reparandum words of 0.841 using a three-step weighted Max-Margin Markov network approach. Similarly, Georgila (2009) uses Integer Linear Programming post-processing of a CRF to achieve F-scores over 0.8 for reparandum start and repair start detection. However neither approach can operate incrementally. Recently, there has been increased interest in left-to-right repair detection: Rasooli and Tetreault (2014) and Honnibal and Johnson (2014) present dependency parsing systems with reparandum"
D14-1009,J99-4003,0,0.645063,"y report an F-score of 0.578 at one word back from the current prefix boundary, increasing word-by-word until 6 words back where it reaches 0.770. These results are the point-of-departure for our work. 3 Challenges and Approach In this section we summarize the challenges for incremental repair detection: computational complexity, repair hypothesis stability, latency of detection and repair structure identification. In 3.1 we explain how we address these. Computational complexity Approaches to detecting repair structures often use chart storage (Zwarts et al., 2010; Johnson and Charniak, 2004; Heeman and Allen, 1999), which poses a computational overhead: if considering all possible boundary points for a repair structure’s 3 phases beginning on any word, for prefixes of length n the number of hypotheses can grow in the order O(n4 ). Exploring a subset of this space is necessary for assigning entire repair structures as in (1) above, rather than just detecting reparanda: the (Johnson and Charniak, 2004; Zwarts et al., 2010) noisy-channel detector is the only system that applies such structures but the potential runtime complexity in decoding these with their STAG repair parser is O(n5 ). In their approach,"
D14-1009,E14-4010,0,0.0672262,"loves ] Mary |{z } |{z } |{z } reparandum interregnum 2 Previous work Qian and Liu (2013) achieve the state of the art in Switchboard corpus self-repair detection, with an F-score for detecting reparandum words of 0.841 using a three-step weighted Max-Margin Markov network approach. Similarly, Georgila (2009) uses Integer Linear Programming post-processing of a CRF to achieve F-scores over 0.8 for reparandum start and repair start detection. However neither approach can operate incrementally. Recently, there has been increased interest in left-to-right repair detection: Rasooli and Tetreault (2014) and Honnibal and Johnson (2014) present dependency parsing systems with reparandum detection which perform similarly, the latter equalling Qian and Liu (2013)’s F-score at 0.841. However, while operating left-to-right, these systems are not designed or evaluated for their incremental performance. The use of beam search over (1) repair From a dialogue systems perspective, detecting repairs and assigning them the appropriate structure is vital for robust natural language understanding (NLU) in interactive systems. Downgrading the commitment of reparandum phases and assigning appropriate interre"
D14-1009,Q14-1011,0,0.0319367,"ry |{z } |{z } |{z } reparandum interregnum 2 Previous work Qian and Liu (2013) achieve the state of the art in Switchboard corpus self-repair detection, with an F-score for detecting reparandum words of 0.841 using a three-step weighted Max-Margin Markov network approach. Similarly, Georgila (2009) uses Integer Linear Programming post-processing of a CRF to achieve F-scores over 0.8 for reparandum start and repair start detection. However neither approach can operate incrementally. Recently, there has been increased interest in left-to-right repair detection: Rasooli and Tetreault (2014) and Honnibal and Johnson (2014) present dependency parsing systems with reparandum detection which perform similarly, the latter equalling Qian and Liu (2013)’s F-score at 0.841. However, while operating left-to-right, these systems are not designed or evaluated for their incremental performance. The use of beam search over (1) repair From a dialogue systems perspective, detecting repairs and assigning them the appropriate structure is vital for robust natural language understanding (NLU) in interactive systems. Downgrading the commitment of reparandum phases and assigning appropriate interregnum and repair phases permits c"
D14-1009,D09-1034,0,0.0768812,"Missing"
D14-1009,P11-1071,0,0.169943,"for prefixes of length n the number of hypotheses can grow in the order O(n4 ). Exploring a subset of this space is necessary for assigning entire repair structures as in (1) above, rather than just detecting reparanda: the (Johnson and Charniak, 2004; Zwarts et al., 2010) noisy-channel detector is the only system that applies such structures but the potential runtime complexity in decoding these with their STAG repair parser is O(n5 ). In their approach, complexity is mitigated by imposing a maximum repair length (12 words), and also by using beam search with re-ranking (Lease et al., 2006; Zwarts and Johnson, 2011). If we wish to include full decoding of the repair’s structure (as argued by Hough and Purver (2013) as necessary for full interpretation) whilst taking a strictly incremental and time-critical perspective, reducing this complexity by minimizing the size of this search space is crucial. The model we consider most suitable for incremental dialogue systems so far is Zwarts et al. (2010)’s incremental version of Johnson and Charniak (2004)’s noisy channel repair detector, as it incrementally applies structural repair analyses (rather than just identifying reparanda) and is evaluated for its incr"
D14-1009,C10-1154,0,0.0787872,"Missing"
D14-1079,D10-1115,0,0.558187,"is paper. This is because, as mentioned in the introduction, their vectors and composition operators are task-specific. These are trained directly to achieve specific objectives in certain pre-determined tasks. We are interested in vector and composition operators that work for any compositional task, and which can be combined with results in linguistics and formal semantics to provide generalisable models that can canonically extend to complex semantic phenomena. The third (i.e. the grammatical) approach promises a way to achieve this, and has been instantiated in various ways in the work of Baroni and Zamparelli (2010),Grefenstette and Sadrzadeh (2011a), and Kartsaklis et al. (2012). (1) t=1 −c≤j≤c,j6=0 Optimizing the above function, for example, produces vectors which maximise the conditional probability of observing words in a context around the target word wt , where c is the size of the training window, and w1 w2 , · · · wT a sequence of words forming a training instance. Therefore, the resulting vectors will capture the distributional intuition and can express degrees of lexical similarity. This method has an obvious advantage compared to co-occurrence method: since now the context is predicted, the mo"
D14-1079,P14-1023,0,0.16069,", although choice of compositional method is important; on the largerscale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks. 1 Matthew Purver1 Introduction Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks. While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder. The purpose of this paper is to provide a more complete picture regarding the potential of neu708 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708–719, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics they are rather similar to each other (both of them are"
D14-1079,P06-4018,0,0.0121831,"nd paraphrase detection tasks. It seems clear that neural word embeddings have an advantage when used in tasks for which they have been trained; our main questions here are whether they outperform co-occurrence based alternatives across the board; and which approach lends itself better to composition using general mathematical operators. To partially an7 F-scores use the standard definition F = 2(precision ∗ recall )/(precision + recall ). 8 The dataset and a Python interface to it are available at http://compprag.christopherpotts.net/ swda.html 9 We use WordNetLemmatizer of the NLTK library (Bird, 2006). 715 swer this question, we can compare model behaviour against the baselines in isolation. For the disambiguation and sentence similarity tasks the baseline is the similarity between verbs only, ignoring the context—see above. For the paraphrase task, we take the global vector-based similarity reported in (Mihalcea et al., 2006): 0.65 accuracy and 0.75 F-score. For the dialogue act tagging task the baseline is the accuracy of the bag-of-unigrams model in (Milajevs and Purver, 2014): 0.60. Sections 5.1 and 5.2 show that although the best choice of vector representation might vary, for small-s"
D14-1079,D11-1129,1,0.1916,"g compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others). In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)). In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in d"
D14-1079,D12-1050,0,0.151787,"s, which show robust, stable performance across the tasks. 1 Matthew Purver1 Introduction Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks. While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder. The purpose of this paper is to provide a more complete picture regarding the potential of neu708 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708–719, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics they are rather similar to each other (both of them are humans) and dissimilar to words such as dog, pavement or idea. The same applies at the phrase and sentence level: “dogs chase cats”"
D14-1079,W11-2507,1,0.170523,"g compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others). In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)). In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in d"
D14-1079,W13-3214,0,0.013977,"ter Science Parks Road, Oxford, UK {d.milajevs,m.sadrzadeh,m.purver}@qmul.ac.uk Abstract dimitri.kartsaklis@cs.ox.ac.uk ral word embeddings in compositional tasks, and meaningfully compare them with the traditional distributional approach based on co-occurrence counts. We are especially interested in investigating the performance of neural word vectors in compositional models involving general mathematical composition operators, rather than in the more task- or domain-specific deep-learning compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others). In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused"
D14-1079,P14-1062,0,0.135867,"nes word vectors by vector addition or point-wise multiplication (Mitchell and Lapata, 2008)—as this is independent of word order, it cannot capture the difference between the two sentences “dogs chase cats” and “cats chase dogs”. The second approach has generally been implemented using some form of deep learning, and captures word order, but not by necessarily caring about the grammatical structure of the sentence. Here, one works by recursively building and combining vectors for subsequences of words within the sentence using e.g. autoencoders (Socher et al., 2012) or convolutional filters (Kalchbrenner et al., 2014). We do not consider this approach in this paper. This is because, as mentioned in the introduction, their vectors and composition operators are task-specific. These are trained directly to achieve specific objectives in certain pre-determined tasks. We are interested in vector and composition operators that work for any compositional task, and which can be combined with results in linguistics and formal semantics to provide generalisable models that can canonically extend to complex semantic phenomena. The third (i.e. the grammatical) approach promises a way to achieve this, and has been inst"
D14-1079,D13-1166,1,0.906618,"Missing"
D14-1079,N13-1090,0,0.623016,"es (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)). In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in detail below. The general picture we get from the results is that in almost all cases the neural vectors are more effective than the traditional approaches. We proceed as follows: Section 2 provides a concise introduction to distributional word representations in natural language processing. Section We provide a comparative study between neural word representations and traditional vector spaces based on cooccurrence counts, in a number of compositional tasks. We use"
D14-1079,W14-1505,1,0.91639,"hine translation metrics and three constituent classifiers (Madnani et al., 2012). The multiplicative model gives lower results than the additive model across all vector spaces. The KS14 vector space shows the steadiest performance, with a drop in accuracy of only 0.04 and no drop in F-score, while for the GS11 and NWE spaces both accuracy and F-score experienced drops by more than 0.20. 5.4 sification decision). The results are shown in the second part of Table 5. Un-lemmatized NWE addition gave the best accuracy (0.63) and F-score (0.60) (averaged over tag classes), i.e. similar results to (Milajevs and Purver, 2014)—although note that the dimensionality of our NWE vectors is 10 times lower than theirs. Multiplicative NWE outperformed the corresponding model in (Milajevs and Purver, 2014). In general, addition consistently outperforms multiplication for all the models. Lemmatization dramatically lowers tagging accuracy: the lemmatized GS11, KS14 and NWE models perform much worse than un-lemmatized NWE, suggesting that morphological features are important for this task. Dialogue act tagging As our last experiment, we evaluate the word spaces on a dialogue act tagging task (Stolcke et al., 2000) over the Sw"
D14-1079,C12-2054,1,0.875957,"ctors and composition operators are task-specific. These are trained directly to achieve specific objectives in certain pre-determined tasks. We are interested in vector and composition operators that work for any compositional task, and which can be combined with results in linguistics and formal semantics to provide generalisable models that can canonically extend to complex semantic phenomena. The third (i.e. the grammatical) approach promises a way to achieve this, and has been instantiated in various ways in the work of Baroni and Zamparelli (2010),Grefenstette and Sadrzadeh (2011a), and Kartsaklis et al. (2012). (1) t=1 −c≤j≤c,j6=0 Optimizing the above function, for example, produces vectors which maximise the conditional probability of observing words in a context around the target word wt , where c is the size of the training window, and w1 w2 , · · · wT a sequence of words forming a training instance. Therefore, the resulting vectors will capture the distributional intuition and can express degrees of lexical similarity. This method has an obvious advantage compared to co-occurrence method: since now the context is predicted, the model in principle can be much more robust in data sparsity problem"
D14-1079,P08-1028,0,0.225372,"nvolving general mathematical composition operators, rather than in the more task- or domain-specific deep-learning compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others). In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)). In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with tw"
D14-1079,W14-1503,0,0.166348,"he words Mary, John, girl, boy and idea. The words philosophy, book and school signify vector space dimensions. As the vector for John is closer to Mary than it is to idea in the vector space—a direct consequence of the fact that John’s contexts are similar to Mary’s and dissimilar to idea’s—we can infer that John is semantically more similar to Mary than to idea. Many variants of this approach exist: performance on word similarity tasks has been shown to be improved by replacing raw counts with weighted values (e.g. mutual information)—see (Turney et al., 2010) and below for discussion, and (Kiela and Clark, 2014) for a detailed comparison. Formal semantics Formal approaches to the semantics of natural language have long built upon the classical idea of compositionality – that the meaning of a sentence is a function of the meanings of its parts (Frege, 1892). In compositional type-logical approaches, predicateargument structures representing phrases and sentences are built from their constituent parts by βreduction within the lambda calculus framework (Montague, 1970): for example, given a representation of John as john 0 and sleeps as λx.sleep 0 (x), the meaning of the sentence “John sleeps” can be co"
D14-1079,E14-1025,0,0.0308022,"e e.g. Kartsaklis and Sadrzadeh (2013). Since the creation and manipulation of tensors of order higher than 2 is difficult, one can work with simplified versions of tensors, faithful to their underlying mathematical basis; these have found intuitive interpretations, e.g. see Grefenstette and Sadrzadeh (2011a), Kartsaklis and Sadrzadeh (2014). In such cases, ? becomes a combination of a range of operations such as ×, ⊗, , and +. 4 Semantic word spaces Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to (Kiela and Clark, 2014; Polajnar and Clark, 2014) for recent studies). We instantiate two co-occurrence-based vectors spaces with different underlying corpora and weighting schemes. Specific models In the current paper we will experiment with a variety of models. In Table 2, we present these models in terms of their composition operators and a reference to the main paper in 711 Method Sentence Addition Multiplication w1 w2 · · · wn w1 w2 · · · wn Relational Kronecker Sbj Verb Obj Sbj Verb Obj Copy object Copy subject Frob. add. Frob. mult. Frob. outer Sbj Verb Obj Sbj Verb Obj Sbj Verb Obj Sbj Verb Obj Sbj Verb Obj Linear algebraic formula −"
D14-1079,C94-1103,0,0.0546427,"oach is extremely powerful because it can capture complex aspects of meaning such as quantifiers and their interaction (see e.g. (Copestake et al., 2005)), and enables inference using well studied and developed logical methods (see e.g. (Bos and Gabsdil, 2000)). Mary John girl boy idea Distributional hypothesis However, such formal approaches are less able to express similarity in meaning. We would like to capture the intuition that while John and Mary are distinct, philosophy book school 0 4 0 0 10 10 60 19 12 47 22 59 93 164 39 Table 1: Word co-occurrence frequencies extracted from the BNC (Leech et al., 1994). 709 Neural word embeddings Deep learning techniques exploit the distributional hypothesis differently. Instead of relying on observed cooccurrence frequencies, a neural language model is trained to maximise some objective function related to e.g. the probability of observing the surrounding words in some context (Mikolov et al., 2013b): T 1X T X log p(wt+j |wt ) extending this to the sentence level and to more complex semantic phenomena, though, depends on their applicability within compositional models, which is the subject of the next section. 3 Compositional models Compositional distribut"
D14-1079,W14-1618,0,0.346919,"ors are competitive, although choice of compositional method is important; on the largerscale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks. 1 Matthew Purver1 Introduction Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks. While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder. The purpose of this paper is to provide a more complete picture regarding the potential of neu708 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708–719, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics they are rather similar to each o"
D14-1079,N12-1019,0,0.0152209,"core Accuracy F-Score MSR addition MSR multiplication 0.65 0.75 0.62 0.52 0.79 0.58 0.70 0.66 0.80 0.80 0.73 0.42 0.82 0.34 0.72 0.41 0.81 0.36 SWDA addition SWDA multiplication 0.60 0.58 0.35 0.32 0.35 0.16 0.40 0.39 0.35 0.33 0.63 0.58 0.60 0.53 0.44 0.43 0.40 0.38 Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results significantly outperform corresponding nearest competitors (for accuracy): p &lt; 0.05, χ2 test. and 0.84 F-score7 ) by the time of this writing has been obtained using 8 machine translation metrics and three constituent classifiers (Madnani et al., 2012). The multiplicative model gives lower results than the additive model across all vector spaces. The KS14 vector space shows the steadiest performance, with a drop in accuracy of only 0.04 and no drop in F-score, while for the GS11 and NWE spaces both accuracy and F-score experienced drops by more than 0.20. 5.4 sification decision). The results are shown in the second part of Table 5. Un-lemmatized NWE addition gave the best accuracy (0.63) and F-score (0.60) (averaged over tag classes), i.e. similar results to (Milajevs and Purver, 2014)—although note that the dimensionality of our NWE vecto"
D14-1079,D12-1110,0,0.355018,"rd Department of Computer Science Parks Road, Oxford, UK {d.milajevs,m.sadrzadeh,m.purver}@qmul.ac.uk Abstract dimitri.kartsaklis@cs.ox.ac.uk ral word embeddings in compositional tasks, and meaningfully compare them with the traditional distributional approach based on co-occurrence counts. We are especially interested in investigating the performance of neural word vectors in compositional models involving general mathematical composition operators, rather than in the more task- or domain-specific deep-learning compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others). In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzad"
D14-1079,J00-3003,0,0.0395555,"Missing"
D14-1079,J06-3003,0,0.0122014,"ulting vectors will capture the distributional intuition and can express degrees of lexical similarity. This method has an obvious advantage compared to co-occurrence method: since now the context is predicted, the model in principle can be much more robust in data sparsity problems, which is always an important issue for cooccurrence word spaces. Additionally, neural vectors have also proven successful in other tasks (Mikolov et al., 2013c), since they seem to encode not only attributional similarity (the degree to which similar words are close to each other), but also relational similarity (Turney, 2006). For example, it is possible to extract the singular:plural relation (apple:apples, car:cars) using vector subtraction: −−−→ −−−−→ −→ −−→ apple − apples ≈ car − cars Perhaps even more importantly, semantic relationships are preserved in a very intuitive way: −−→ −−→ −−−→ −−−−→ king − man ≈ queen − woman allowing the formation of analogy queries similar −−→ −→ + − −−−→ = ?, obtaining − −−→ as to king − − man woman queen the result.1 Both neural and co-occurrence-based approaches have advantages over classical formal approaches in their ability to capture lexical semantics and degrees of simila"
D14-1079,W08-2222,0,\N,Missing
E12-1049,O04-3004,0,0.182412,"Missing"
E12-1049,D10-1124,0,0.0238599,"Missing"
E12-1049,W09-1904,0,0.0283578,"e amounts of data, often consisting of very short texts, written in unconventional style and without accompanying metadata, audio/video signals or access to the author for disambiguation, how can we easily produce a gold-standard labelling for training and/or for evaluation and test? One possible solution that is becoming popular is crowd-sourcing the labelling task, as the easy access to very large numbers of annotators provided by tools such as Amazon’s Mechanical Turk can help with the problem of dataset size; however, this has its own attendant problems of annotator reliability (see e.g. (Hsueh et al., 2009)), and cannot directly help with the inherent problem of ambiguity – using many annotators does not guarantee that they can understand or correctly assign the author’s intended interpretation or emotional state. In this paper, we investigate a different approach via distant supervision (see e.g. (Mintz et al., 2009)). By using conventional markers of emotional content within the texts themselves as a surrogate for explicit labels, we can quickly retrieve large subsets of (noisily) labelled data. This approach has the advantage of giving us direct access to the authors’ own intended interpretat"
E12-1049,P09-1113,0,0.00714962,"coming popular is crowd-sourcing the labelling task, as the easy access to very large numbers of annotators provided by tools such as Amazon’s Mechanical Turk can help with the problem of dataset size; however, this has its own attendant problems of annotator reliability (see e.g. (Hsueh et al., 2009)), and cannot directly help with the inherent problem of ambiguity – using many annotators does not guarantee that they can understand or correctly assign the author’s intended interpretation or emotional state. In this paper, we investigate a different approach via distant supervision (see e.g. (Mintz et al., 2009)). By using conventional markers of emotional content within the texts themselves as a surrogate for explicit labels, we can quickly retrieve large subsets of (noisily) labelled data. This approach has the advantage of giving us direct access to the authors’ own intended interpretation or emotional state, without relying on thirdparty annotators. Of course, the labels themselves may be noisy: ambiguous, vague or not having a direct correspondence with the desired classification. We therefore experiment with multiple such conventions with apparently similar meanings – here, emoticons (following"
E12-1049,pak-paroubek-2010-twitter,0,0.0533837,"ions of multiple classes (rather than purely positive or negative sentiment) in such short texts; and secondly, to investigate the use of distant supervision to quickly bootstrap large datasets and classifiers without the need for manual annotation. Text classification according to emotion and sentiment is a well-established research area. In this and other areas of text analysis and classification, recent years have seen a rise in use of data from online sources and social media, as these provide very large, often freely available datasets (see e.g. (Eisenstein et al., 2010; Go et al., 2009; Pak and Paroubek, 2010) amongst many others). However, one of the challenges this poses is that of data annotation: given very large amounts of data, often consisting of very short texts, written in unconventional style and without accompanying metadata, audio/video signals or access to the author for disambiguation, how can we easily produce a gold-standard labelling for training and/or for evaluation and test? One possible solution that is becoming popular is crowd-sourcing the labelling task, as the easy access to very large numbers of annotators provided by tools such as Amazon’s Mechanical Turk can help with th"
E12-1049,P05-2008,0,0.299218,"By using conventional markers of emotional content within the texts themselves as a surrogate for explicit labels, we can quickly retrieve large subsets of (noisily) labelled data. This approach has the advantage of giving us direct access to the authors’ own intended interpretation or emotional state, without relying on thirdparty annotators. Of course, the labels themselves may be noisy: ambiguous, vague or not having a direct correspondence with the desired classification. We therefore experiment with multiple such conventions with apparently similar meanings – here, emoticons (following (Read, 2005)) and Twitter hashtags – allowing us to examine the similarity of classifiers trained on independent labels but intended to detect the same underlying class. We also investigate the precision and correspondence of particular labels with the desired emotion classes by testing on a small set of man482 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 482–491, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics ually labelled data. We show that the success of this approach depends on both the conven"
N07-4012,W02-0216,1,0.833017,"se. The DM module mediates and manages interac23 NAACL HLT Demonstration Program, pages 23–24, c Rochester, New York, USA, April 2007. 2007 Association for Computational Linguistics tion. It uses an information-state-update approach to maintain dialog context, which is then used to interpret incoming utterances (including fragments and revisions), resolve NPs, construct salient responses, track issues, etc. Dialog states can also be used to bias SR expectation and improve SR performance, as has been performed in previous applications of the DM. Detailed descriptions of the DM can be found in (Lemon et al., 2002) (Mirkovic and Cavedon, 2005). The Knowledge Manager (KM) controls access to knowledge base sources (such as domain knowledge and device information) and their updates. Domain knowledge is structured according to domaindependent ontologies. The current KM makes use of OWL, a W3C standard, to represent the ontological relationships between domain entities. The Content Optimization module acts as an intermediary between the dialog management module and the knowledge management module and controls the amount of content and provides recommendations to user. It receives queries in the form of seman"
P06-1003,P94-1002,0,0.211398,"we sample a new distribution over topics. This pattern of dependency is produced by associating a binary switching variable with each utterance, indicating whether its topic is the same as that of the previous utterance. The joint states of all the switching variables define segments that should be semantically coherent, because their words are generated by the same topic vector. We will first describe this generative model in more detail, and then discuss inference in this model. Work on automatic topic segmentation of text and monologue has been prolific, with a variety of approaches used. (Hearst, 1994) uses a measure of lexical cohesion between adjoining paragraphs in text; (Reynar, 1999) and (Beeferman et al., 1999) combine a variety of features such as statistical language modelling, cue phrases, discourse information and the presence of pronouns or named entities to segment broadcast news; (Maskey and Hirschberg, 2003) use entirely non-lexical features. Recent advances have used generative models, allowing lexical models of the topics themselves to be built while segmenting (Imai et al., 1997; Barzilay and Lee, 2004), and we take a similar approach here, although with some important diff"
P06-1003,lisowska-etal-2004-user,0,0.0115266,"r text or monologue, and most address only one of the two issues (usually for the very good reason that the dataset itself provides the other, for example by the explicit separation of individual documents or news stories in a collection). Spoken multi-party meetings pose a difficult problem: firstly, neither the 2 Background and Related Work In this paper we are interested in spoken discourse, and in particular multi-party human-human meetings. Our overall aim is to produce information which can be used to summarize, browse and/or retrieve the information contained in meetings. User studies (Lisowska et al., 2004; Banerjee et al., 2005) have shown that topic information is important here: people are likely to want to know 17 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 17–24, c Sydney, July 2006. 2006 Association for Computational Linguistics which topics were discussed in a particular meeting, as well as have access to the discussion on particular topics in which they are interested. Of course, this requires both identification of the topics discussed, and segmentation into the periods of topically related discussion. tion and"
P06-1003,N04-1015,0,0.486294,"tion of text and monologue has been prolific, with a variety of approaches used. (Hearst, 1994) uses a measure of lexical cohesion between adjoining paragraphs in text; (Reynar, 1999) and (Beeferman et al., 1999) combine a variety of features such as statistical language modelling, cue phrases, discourse information and the presence of pronouns or named entities to segment broadcast news; (Maskey and Hirschberg, 2003) use entirely non-lexical features. Recent advances have used generative models, allowing lexical models of the topics themselves to be built while segmenting (Imai et al., 1997; Barzilay and Lee, 2004), and we take a similar approach here, although with some important differences detailed below. Turning to multi-party discourse and meetings, however, most previous work on automatic segmentation (Reiter and Rigoll, 2004; Dielmann and Renals, 2004; Banerjee and Rudnicky, 2004), treats segments as representing meeting phases or events which characterize the type or style of discourse taking place (presentation, briefing, discussion etc.), rather than the topic or subject matter. While we expect some correlation between these two types of segmentation, they are clearly different problems. Howev"
P06-1003,J02-1002,0,0.323619,"Missing"
P06-1003,P99-1046,0,0.0137341,"ciating a binary switching variable with each utterance, indicating whether its topic is the same as that of the previous utterance. The joint states of all the switching variables define segments that should be semantically coherent, because their words are generated by the same topic vector. We will first describe this generative model in more detail, and then discuss inference in this model. Work on automatic topic segmentation of text and monologue has been prolific, with a variety of approaches used. (Hearst, 1994) uses a measure of lexical cohesion between adjoining paragraphs in text; (Reynar, 1999) and (Beeferman et al., 1999) combine a variety of features such as statistical language modelling, cue phrases, discourse information and the presence of pronouns or named entities to segment broadcast news; (Maskey and Hirschberg, 2003) use entirely non-lexical features. Recent advances have used generative models, allowing lexical models of the topics themselves to be built while segmenting (Imai et al., 1997; Barzilay and Lee, 2004), and we take a similar approach here, although with some important differences detailed below. Turning to multi-party discourse and meetings, however, most pre"
P06-1003,P03-1071,0,\N,Missing
P07-2027,E06-1022,0,0.14175,"Missing"
P07-2027,H94-1020,0,0.0378558,"resents our features in detail. N 2 N 2 2 2 2 2 Table 1: Number of cases found. originally referential use (as the original addressee may not be the current addressee – see example (4)). We allowed a separate class for genuinely ambiguous cases. Switchboard explicitly tags “you know” when used as a discourse marker; as this (generic) case is common and seems trivial we removed it from our data. B: (4) A: Features All features used for classifier experiments were extracted from the Switchboard LDC Treebank 3 release, which includes transcripts, part of speech information using the Penn tagset (Marcus et al., 1994) and dialog act tags (Jurafsky et al., 1997). Features fell into four main categories:2 sentential features which capture lexical features of the utterance itself; part-of-speech features which capture shallow syntactic patterns; dialog act features capturing the discourse function of the current utterance and surrounding context; and context features which give oracle information (i.e., the correct generic/referential label) about preceding uses 2 46 46 46 2 2 Well, uh, I guess probably the last one I went to I met so many people that I had not seen in probably ten, over ten years. It was lik"
P07-2027,E06-1007,0,0.178699,"Missing"
P07-2027,W04-2319,0,0.0257365,"me. Oh yeah uh O K. It can also be important to distinguish between singular and plural reference, as in example (2) where the task is assigned to more than one person: A: (2) B: So y- so you guys will send to the rest of us um a version of um, this, and - the - uh, description With sugge- yeah, suggested improvements and - Use of “you” might therefore help us both in de∗ This work was supported by the CALO project (DARPA grant NBCH-D-03-0010) and ONR (MURI award N000140510388). The authors also thank John Niekrasz for annotating our test data. 1 (1,2) are taken from the ICSI Meeting Corpus (Shriberg et al., 2004); (3,4) from Switchboard (Godfrey et al., 1992). tecting the fact that a task is being assigned, and in identifying the owner. While there is an increasing body of work concerning addressee identification (Katzenmaier et al., 2004; Jovanovic et al., 2006), there is very little investigating the problem of second-person pronoun resolution, and it is this that we address here. Most cases of “you” do not in fact refer to the addressee but are generic, as in example (3); automatic referentiality classification is therefore very important. B: (3) 2 Well, usually what you do is just wait until you t"
P07-2027,J00-3003,0,0.16253,"Missing"
P16-3009,P14-1023,0,0.0478223,"London, UK {d.milajevs,m.sadrzadeh,m.purver}@qmul.ac.uk Abstract neighbor words should correspond to them. Levy et al. (2015) propose optimisations for co-occurrence-based distributional models, using parameters adopted from predictive models (Mikolov et al., 2013): shifting and context distribution smoothing. Their experiments and thus their parameter recommendations use highdimensional vector spaces with word vector dimensionality of almost 200K, and many recent state-of-the-art results in lexical distributional semantics have been obtained using vectors with similarly high dimensionality (Baroni et al., 2014; Kiela and Clark, 2014; Lapesa and Evert, 2014). In contrast, much work on compositional distributional semantics employs vectors with much fewer dimensions: e.g. 2K (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Milajevs et al., 2014), 3K (Dinu and Lapata, 2010; Milajevs and Purver, 2014) or 10K (Polajnar and Clark, 2014; Baroni and Zamparelli, 2010). The most common reason thereof is that these models assign tensors to functional words. For a vector space V with k dimensions, a tensor V ⊗V · · ·⊗V of rank n has k n dimensions. Adjectives and intransitive verbs have tenso"
P16-3009,J90-1003,0,0.449298,"same sparsity as the space with PMI values. 2.2 Shifted PMI (neg) Many approaches use only positive PMI values, as negative PMI values may not positively contribute to model performance and sparser matrices are more computationally tractable (Turney and Pantel, 2010). This can be generalised to an additional cutoff parameter k (neg) following Levy et al. (2015), giving our third PMI variant (abbreviated as SPMI):2 2.1 PMI variants (discr) SPMIk = max(0, PMI(x, y) − log k) Most co-occurrence weighting schemes in distributional semantics are based on point-wise mutual information (PMI, see e.g. Church and Hanks (1990), Turney and Pantel (2010), Levy and Goldberg (2014)): P (x, y) P (x)P (y) 1K, 2K, 3K, 5K 10K, 20K, 30K, 40K, 50K PMI, CPMI, SPMI, SCPMI 1, n, log n 0.2, 0.5, 0.7, 1, 1.4, 2, 5, 7 global, 1, 0.75 Cosine, Correlation Table 1: Model parameters and their values. Parameters PMI(x, y) = log Values (3) When k = 1 SPMI is equivalent to positive PMI. k &gt; 1 increases the underlying matrix sparsity by keeping only highly associated co-occurrence pairs. k &lt; 1 decreases the underlying matrix sparsity by including some unassociated cooccurrence pairs, which are usually excluded due to unreliability of prob"
P16-3009,P93-1022,0,0.56185,"(2010), Levy and Goldberg (2014)): P (x, y) P (x)P (y) 1K, 2K, 3K, 5K 10K, 20K, 30K, 40K, 50K PMI, CPMI, SPMI, SCPMI 1, n, log n 0.2, 0.5, 0.7, 1, 1.4, 2, 5, 7 global, 1, 0.75 Cosine, Correlation Table 1: Model parameters and their values. Parameters PMI(x, y) = log Values (3) When k = 1 SPMI is equivalent to positive PMI. k &gt; 1 increases the underlying matrix sparsity by keeping only highly associated co-occurrence pairs. k &lt; 1 decreases the underlying matrix sparsity by including some unassociated cooccurrence pairs, which are usually excluded due to unreliability of probability estimates (Dagan et al., 1993). We can apply the same idea to CPMI: (1) As commonly done, we replace the infinite PMI values,1 which arise when P (x, y) = 0, with zeroes and use PMI hereafter to refer to a weighting with this fix. SCPMIk = max(0, CPMI(x, y) − log 2k) (4) 1 We assume that the probability of a single token is always greater than zero as it appears in the corpus at least once. 2 SPMI is different from CPMI because log ( ) P (x,y) (x,y) log k = log P (x)(P = log 1 + PP(x)P . (y)k (y) 59 P (x,y) P (x)P (y) − 0.40 freq = 1 |discr = pmi freq = 1 |discr = cpmi freq = 1 |discr = spmi freq = 1 |discr = scpmi freq ="
P16-3009,D10-1113,0,0.0234887,"context distribution smoothing. Their experiments and thus their parameter recommendations use highdimensional vector spaces with word vector dimensionality of almost 200K, and many recent state-of-the-art results in lexical distributional semantics have been obtained using vectors with similarly high dimensionality (Baroni et al., 2014; Kiela and Clark, 2014; Lapesa and Evert, 2014). In contrast, much work on compositional distributional semantics employs vectors with much fewer dimensions: e.g. 2K (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Milajevs et al., 2014), 3K (Dinu and Lapata, 2010; Milajevs and Purver, 2014) or 10K (Polajnar and Clark, 2014; Baroni and Zamparelli, 2010). The most common reason thereof is that these models assign tensors to functional words. For a vector space V with k dimensions, a tensor V ⊗V · · ·⊗V of rank n has k n dimensions. Adjectives and intransitive verbs have tensors of rank 2, transitive verbs are of rank 3; for coordinators, the rank can go up to 7. Taking k = 200K already results in a highly intractable tensor of 8 × 1015 dimensions for a transitive verb. An alternative way of obtaining a vector space with few dimensions, usually with just"
P16-3009,D11-1129,1,0.892091,"Missing"
P16-3009,W14-1505,1,0.850144,"oothing. Their experiments and thus their parameter recommendations use highdimensional vector spaces with word vector dimensionality of almost 200K, and many recent state-of-the-art results in lexical distributional semantics have been obtained using vectors with similarly high dimensionality (Baroni et al., 2014; Kiela and Clark, 2014; Lapesa and Evert, 2014). In contrast, much work on compositional distributional semantics employs vectors with much fewer dimensions: e.g. 2K (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Milajevs et al., 2014), 3K (Dinu and Lapata, 2010; Milajevs and Purver, 2014) or 10K (Polajnar and Clark, 2014; Baroni and Zamparelli, 2010). The most common reason thereof is that these models assign tensors to functional words. For a vector space V with k dimensions, a tensor V ⊗V · · ·⊗V of rank n has k n dimensions. Adjectives and intransitive verbs have tensors of rank 2, transitive verbs are of rank 3; for coordinators, the rank can go up to 7. Taking k = 200K already results in a highly intractable tensor of 8 × 1015 dimensions for a transitive verb. An alternative way of obtaining a vector space with few dimensions, usually with just 100–500, is the use of SVD"
P16-3009,J15-4004,0,0.0862951,"Missing"
P16-3009,D14-1079,1,0.912213,"et al., 2013): shifting and context distribution smoothing. Their experiments and thus their parameter recommendations use highdimensional vector spaces with word vector dimensionality of almost 200K, and many recent state-of-the-art results in lexical distributional semantics have been obtained using vectors with similarly high dimensionality (Baroni et al., 2014; Kiela and Clark, 2014; Lapesa and Evert, 2014). In contrast, much work on compositional distributional semantics employs vectors with much fewer dimensions: e.g. 2K (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Milajevs et al., 2014), 3K (Dinu and Lapata, 2010; Milajevs and Purver, 2014) or 10K (Polajnar and Clark, 2014; Baroni and Zamparelli, 2010). The most common reason thereof is that these models assign tensors to functional words. For a vector space V with k dimensions, a tensor V ⊗V · · ·⊗V of rank n has k n dimensions. Adjectives and intransitive verbs have tensors of rank 2, transitive verbs are of rank 3; for coordinators, the rank can go up to 7. Taking k = 200K already results in a highly intractable tensor of 8 × 1015 dimensions for a transitive verb. An alternative way of obtaining a vector space with few di"
P16-3009,W14-1503,0,0.175957,"vs,m.sadrzadeh,m.purver}@qmul.ac.uk Abstract neighbor words should correspond to them. Levy et al. (2015) propose optimisations for co-occurrence-based distributional models, using parameters adopted from predictive models (Mikolov et al., 2013): shifting and context distribution smoothing. Their experiments and thus their parameter recommendations use highdimensional vector spaces with word vector dimensionality of almost 200K, and many recent state-of-the-art results in lexical distributional semantics have been obtained using vectors with similarly high dimensionality (Baroni et al., 2014; Kiela and Clark, 2014; Lapesa and Evert, 2014). In contrast, much work on compositional distributional semantics employs vectors with much fewer dimensions: e.g. 2K (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Milajevs et al., 2014), 3K (Dinu and Lapata, 2010; Milajevs and Purver, 2014) or 10K (Polajnar and Clark, 2014; Baroni and Zamparelli, 2010). The most common reason thereof is that these models assign tensors to functional words. For a vector space V with k dimensions, a tensor V ⊗V · · ·⊗V of rank n has k n dimensions. Adjectives and intransitive verbs have tensors of rank 2, transitiv"
P16-3009,D14-1162,0,0.0798361,"ssign tensors to functional words. For a vector space V with k dimensions, a tensor V ⊗V · · ·⊗V of rank n has k n dimensions. Adjectives and intransitive verbs have tensors of rank 2, transitive verbs are of rank 3; for coordinators, the rank can go up to 7. Taking k = 200K already results in a highly intractable tensor of 8 × 1015 dimensions for a transitive verb. An alternative way of obtaining a vector space with few dimensions, usually with just 100–500, is the use of SVD as a part of Latent Semantic Analysis (Dumais, 2004) or another models such as SGNS (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). However, these models take more time to instantiate in comparison to weighting of a co-occurrence matrix, bring more parameters to explore and produce vector spaces with uninterpretable dimensions (vector space dimension interpretation is used by some lexical modPrevious optimisations of parameters affecting the word-context association measure used in distributional vector space models have focused either on highdimensional vectors with hundreds of thousands of dimensions, or dense vectors with dimensionality of a few hundreds; but dimensionality of a few thousands is often applied in compo"
P16-3009,Q14-1041,0,0.0192707,"}@qmul.ac.uk Abstract neighbor words should correspond to them. Levy et al. (2015) propose optimisations for co-occurrence-based distributional models, using parameters adopted from predictive models (Mikolov et al., 2013): shifting and context distribution smoothing. Their experiments and thus their parameter recommendations use highdimensional vector spaces with word vector dimensionality of almost 200K, and many recent state-of-the-art results in lexical distributional semantics have been obtained using vectors with similarly high dimensionality (Baroni et al., 2014; Kiela and Clark, 2014; Lapesa and Evert, 2014). In contrast, much work on compositional distributional semantics employs vectors with much fewer dimensions: e.g. 2K (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Milajevs et al., 2014), 3K (Dinu and Lapata, 2010; Milajevs and Purver, 2014) or 10K (Polajnar and Clark, 2014; Baroni and Zamparelli, 2010). The most common reason thereof is that these models assign tensors to functional words. For a vector space V with k dimensions, a tensor V ⊗V · · ·⊗V of rank n has k n dimensions. Adjectives and intransitive verbs have tensors of rank 2, transitive verbs are of rank 3; fo"
P16-3009,E14-1025,0,0.0228947,"their parameter recommendations use highdimensional vector spaces with word vector dimensionality of almost 200K, and many recent state-of-the-art results in lexical distributional semantics have been obtained using vectors with similarly high dimensionality (Baroni et al., 2014; Kiela and Clark, 2014; Lapesa and Evert, 2014). In contrast, much work on compositional distributional semantics employs vectors with much fewer dimensions: e.g. 2K (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Milajevs et al., 2014), 3K (Dinu and Lapata, 2010; Milajevs and Purver, 2014) or 10K (Polajnar and Clark, 2014; Baroni and Zamparelli, 2010). The most common reason thereof is that these models assign tensors to functional words. For a vector space V with k dimensions, a tensor V ⊗V · · ·⊗V of rank n has k n dimensions. Adjectives and intransitive verbs have tensors of rank 2, transitive verbs are of rank 3; for coordinators, the rank can go up to 7. Taking k = 200K already results in a highly intractable tensor of 8 × 1015 dimensions for a transitive verb. An alternative way of obtaining a vector space with few dimensions, usually with just 100–500, is the use of SVD as a part of Latent Semantic Anal"
P16-3009,Q15-1016,0,0.169177,"counts. Another fundamental question in vector space design is the vector space dimensionality and what 58 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics – Student Research Workshop, pages 58–64, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics els, for example, McGregor et al. (2015), and the passage from formal semantics to tensor models relies on it (Coecke et al., 2010)). In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of Levy et al. (2015) for comparison. The mismatch of recent experiments with nondense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions: • To what extent does model performance depend on vector dimensionality? • Do parameters influence 200K and 1K dimensional models similarly? Can the findings of Levy et al. (2015) be directly applied to models with a few thousand dimensions? • If not, can we derive suitable parameter selection heuristics which take account of dimensionality? To answer these questions, we perform a systematic study of distributional models"
P16-3009,D10-1115,0,\N,Missing
U05-1032,W03-2123,0,0.0342734,"evice and activity status, together with a set of update rules defining the effect of dialogue moves on the state (e.g. adding new information and referents for anaphora resolution, and triggering new tasks, activities and system responses). This approach allows more complex dialogue types with advanced strategies for context-dependent utterance interpretation (including fragments and revisions), NP resolution, issue tracking and improved speech-recognizer performance (Lemon and Gruenstein, 2004). 2.2 The CSLI Dialogue Manager Generic ISU toolkits (e.g. TrindiKit (Traum et al., 1999), DIPPER (Bos et al., 2003)) provide general data structures for representing state and a language for specifying update rules, but the specific state and rules used are left to the individual application. The CDM is a specific implementation of an ISU dialoguemanagement system, providing data structures and processes for update specifically designed as suitable to activity-oriented dialogue, but adaptible to different applications and domains. The two central components of the CDM information state are the Dialogue Move Tree (DMT) and the Activity Tree. The DMT represents the dialogue context and history, with each dia"
U05-1032,W01-1609,0,0.0268773,"requests, interpreting observations, and otherwise supporting the agent in the performance of the activity. Systems engaging in such dialogue characteristically require deep knowledge about the task domain and the devices/agents they provide access to, in order to know what information is critical to the tasks, and know what information about task performance is appropriate to provide to the user. CSLI has been developing activityoriented dialogue systems for a number of years, for applications such as multimodal control of robotic devices (Lemon et al., 2002), speechenabled tutoring systems (Fry et al., 2001), and conversational interaction with in-car devices (Weng et al., 2004). The dialogue system architecture (Figure 1) centers around the CSLI Dialogue Manager, which can be used with various different external components: speech-recognizer, NL parser, NL generation, speech-synthesizer, as well as connections to external application-specific 234 components such as ontologies or knowledge bases, and the dialogue-enabled devices themselves. Clean interfaces and representationneutral processes enable the CDM to be used relatively seamlessly with different NL components, while interaction with exte"
U05-1032,P04-1044,0,0.101265,"ly corrected. Confirmation and clarification behaviour can also be governed not only by ASR or parse confidence, but by the overall score. Proceedings of the Australasian Language Technology Workshop 2005, pages 233–240, Sydney, Australia, December 2005. 233 Related Approaches Rayner et al. (1994) combine speech recognition confidence scores with various intra-utterance linguistic features to re-order n-best hypotheses; Chotimongkol and Rudnicky (2001) also include move bigram statistics. Walker et al. (2000) use similar feature combination to identify misrecognised utterances. More recently, Gabsdil and Lemon (2004) also include pragmatic information such as NP resolution, and simultaneously choose from an n-best list while identifying misrecognition. They also divide misrecognised utterances into two overall confidence ranges, one for outright rejection and one for confirmation/clarification. Similarly Gabsdil and Bos (2003) combine acoustic confidences with semantic information, and Schlangen (2004) with bridging reference resolution, in order to allow clarification on an integrated basis. All of these approaches assume a single-device setting and hence no ambiguity of move type once the correct word s"
U05-1032,W02-0216,0,0.0777472,"participates in specifying the activity, clarifying requests, interpreting observations, and otherwise supporting the agent in the performance of the activity. Systems engaging in such dialogue characteristically require deep knowledge about the task domain and the devices/agents they provide access to, in order to know what information is critical to the tasks, and know what information about task performance is appropriate to provide to the user. CSLI has been developing activityoriented dialogue systems for a number of years, for applications such as multimodal control of robotic devices (Lemon et al., 2002), speechenabled tutoring systems (Fry et al., 2001), and conversational interaction with in-car devices (Weng et al., 2004). The dialogue system architecture (Figure 1) centers around the CSLI Dialogue Manager, which can be used with various different external components: speech-recognizer, NL parser, NL generation, speech-synthesizer, as well as connections to external application-specific 234 components such as ontologies or knowledge bases, and the dialogue-enabled devices themselves. Clean interfaces and representationneutral processes enable the CDM to be used relatively seamlessly with d"
U05-1032,H94-1040,0,0.0520732,"formation sources, including context. The highest scoring result overall may not correspond to the highest-confidence result from the ASR or parser n-best list alone, but n-best lists are effectively re-ordered based on device and dialogue context, allowing parsing errors such as incorrect PP-attachment to be automatically corrected. Confirmation and clarification behaviour can also be governed not only by ASR or parse confidence, but by the overall score. Proceedings of the Australasian Language Technology Workshop 2005, pages 233–240, Sydney, Australia, December 2005. 233 Related Approaches Rayner et al. (1994) combine speech recognition confidence scores with various intra-utterance linguistic features to re-order n-best hypotheses; Chotimongkol and Rudnicky (2001) also include move bigram statistics. Walker et al. (2000) use similar feature combination to identify misrecognised utterances. More recently, Gabsdil and Lemon (2004) also include pragmatic information such as NP resolution, and simultaneously choose from an n-best list while identifying misrecognition. They also divide misrecognised utterances into two overall confidence ranges, one for outright rejection and one for confirmation/clari"
U05-1032,W01-1618,0,0.0504671,"Missing"
U05-1032,W04-2325,0,0.0143152,"der n-best hypotheses; Chotimongkol and Rudnicky (2001) also include move bigram statistics. Walker et al. (2000) use similar feature combination to identify misrecognised utterances. More recently, Gabsdil and Lemon (2004) also include pragmatic information such as NP resolution, and simultaneously choose from an n-best list while identifying misrecognition. They also divide misrecognised utterances into two overall confidence ranges, one for outright rejection and one for confirmation/clarification. Similarly Gabsdil and Bos (2003) combine acoustic confidences with semantic information, and Schlangen (2004) with bridging reference resolution, in order to allow clarification on an integrated basis. All of these approaches assume a single-device setting and hence no ambiguity of move type once the correct word string or parse has been identified. Here we extend these approaches to allow a principled choice of move/device pairing. 2 Background 2.1 Dialogue Manager Architecture Our focus is on activity-oriented dialogue, discussing tasks or activities that are jointly performed by a human and one or more intelligent devices or agents. By “joint activity”, we mean that the human participates in speci"
W01-1616,J96-2004,0,0.0568195,"nscripts. For this experiment, a sub-portion of the dialogue transcripts was used consisting of c. 150,000 words. To maintain a spread across dialogue domain, region, speaker age etc., this subportion was created by taking a 200-speakerturn section from 59 transcripts. All CRs within this sub-corpus were identi ed and tagged, using the markup scheme and decision process described in 4.2 and 4.3 below. At time of writing this process has been performed by only one (expert) user { our intention is to con rm results by comparing with those obtained by naive users, using e.g. the kappa statistic (Carletta, 1996) to assess reliability. Initial identi cation of CRs was performed using SCoRE (Purver, 2001), a search engine developed speci cally for this purpose (in particular, to allow searches for repeated words between speaker turns, and to display dialogue in an intuitive manner). However, in order to ensure that all clari cational phenomena were captured, the nal search and markup were performed manually. 4.2 Markup Scheme The markup scheme used evolved during the markup process as new CR mechanisms were identi ed, and the nal scheme was as described here. A multi-layered approach was 17 We suspect"
W01-1616,W00-0302,0,0.164307,"Missing"
W01-1616,P01-1031,1,\N,Missing
W02-0222,W96-0213,0,\N,Missing
W02-0222,P98-1014,0,\N,Missing
W02-0222,C98-1014,0,\N,Missing
W02-0222,P01-1031,0,\N,Missing
W03-2103,P01-1031,1,0.847503,"We present some results from an investigation using the BNC which show some general correlations between clarification request type, likelihood of answering, answer type and distance between question and answer. We then describe a new experimental technique for integrating manipulations into text-based synchronous dialogue, and give more specific results concerning the effect of word category and level of grounding on interpretation and response type. 1 Introduction Requesting clarification is a vital part of the communicative process and has received attention from both the formal semantic (Ginzburg and Cooper, 2001; Ginzburg and Cooper, forthcoming) and conversation analytic traditions (Schegloff, 1987), but little in the computational dialogue system community. In theory, a perfect dialogue system should be able to interpret and deal with clarification requests (CRs) made by the user in order to elicit clarification of some part of a system utterance, and be able to request clarification itelf of some part of a user utterance. This is no easy task – CRs may take many 2 Department of Computer Science Queen Mary, University of London London E1 4NS, UK different forms (often highly elliptical), and can be"
W03-2103,J97-3003,0,0.0535506,"on, and then displays the original version to the originator client, and the processed (or artificially generated) version to the other client. The server records all turns, together with each key press from both clients, for later analysis. This data is also used on the fly to control the speed and capitalisation of artificially generated turns, to be as realistic a simulation of the relevant subject as possible. NLP Component The NLP component consists of a Perl text-processing module which communicates with various external NLP modules as required: PoS tagging can be performed using LTPOS (Mikheev, 1997), word rarity/frequency tagging using a custom tagger based on the BNC (Kilgarriff, 1997), and synonym generation using WordNet (Fellbaum, 1998). Experimental parameters are specified as a set of rules which are applied to each word in turn. Preconditions for the application of the rule can be specified in terms of PoS, word frequency and the word itself, together with contextual factors such as the time since the last artificial turn was generated, and a probability threshold to prevent behaviour appearing too regular. The effect of the rule can be to transform the word in question (by substi"
W03-2103,W01-1616,1,0.883946,"Missing"
W03-2103,W02-0222,1,0.842662,"tionally attempted to avoid the necessity for CR interpretation by making system utterances as clear and precise as possible, and avoid having to generate all but the most simple CRs by using robust shallow methods of interpretation or by relying on highly domain-dependent lexicons and grammars. However, as systems become more human-like, it seems likely that we will have to cope with user CRs at some stage; and the ability to generate system CRs can be useful in order to repair misunderstanding, disambiguate other utterances, and learn new words – see (Knight, 1996; Dusan and Flanagan, 2002; Purver, 2002). The investigations presented here had two main aims: to examine (a) how CRs are interpreted, and (b) how they are responded to. The two are clearly dependent – the response must depend on the interpretation – but there are many other influencing factors such as CR form, context and level of grounding. Answers to (a) should help us with the following questions: • What factors can help us disambiguate and correctly interpret user CRs? • What factors should govern generation of system CRs such that they are correctly interpreted by the user? Answers to (b) should help with the following related"
W03-2311,C88-2128,0,\N,Missing
W03-2311,P01-1028,0,\N,Missing
W03-2311,P97-1026,0,\N,Missing
W03-2311,P89-1002,0,\N,Missing
W04-0312,W03-2311,1,0.883377,"Missing"
W04-0312,P97-1026,0,0.0308922,"he treatment of linked structures (for relatives and other modifiers) and quantification, and more relevantly to improve the incrementality of the generation process: we do not adopt the proposal of O&P to speed up generation by use of a restricted multiset of lexical entries selected on the basis of goal tree features, which prevents strictly incremental generation and excludes modification of the goal tree. 7 In building n-tuples of trees corresponding to predicate-argument structures, the system is similar to LTAG formalisms (Joshi and Kulick, 1997). However, unlike LTAG systems (see e.g. (Stone and Doran, 1997)), both parsing and generation are not head-driven, but fully (word-by-word) incremental. This has the advantage of allowing fully incremental models for all languages, matching psycholinguistic observations (Ferreira, 1996). 8 Strictly speaking, scope statements should be included in these n-tuples – for now we consider them as part of the tree. this set can then be taken to consist of: (a) a similar triple hT0 , W0 , A0 i given by the previous sentence, where T0 is its semantic tree representation, W0 and A0 the sequences of words and actions that were used in building it; and (b) the triple"
W06-3405,W04-1008,0,0.031968,"se tends to reflect this, and we attempt to exploit this fact here. 3 Baseline Experiments We applied Gruenstein et al. (2005)’s flat annotation schema to transcripts from a sequence of 5 short related meetings with 3 participants recorded as part of the CALO project. Each meeting was simulated in that its participants were given a scenario, but was not scripted. In order to avoid entirely dataor scenario-specific results (and also to provide an acceptable amount of training data), we then added a random selection of 6 ICSI and 1 ISL meetings from Gruenstein et al. (2005)’s annotations. Like (Corston-Oliver et al., 2004) we used support vector machines (Vapnik, 1995) via the classifier SVMlight (Joachims, 1999). Their full set of features are not available to us, but we experimented with combinations of words and n-grams and assessed classification performance via a 5-fold validation on each of the CALO meetings. In each case, we trained classifiers on the other 4 meetings in the CALO sequence, plus the fixed ICSI/ISL training selection. Performance (per utterance, on the binary classification problem) is shown in Table 1; overall f-score 32 figures are poor even on these short meetings. These figures were ob"
W06-3405,P93-1008,0,0.106833,"Missing"
W06-3405,2005.sigdial-1.13,1,0.865412,"lass Task included any sentence containing items that seemed appropriate to add to an ongoing todo list. They report good inter-annotator agreement over their general tagging exercise (κ > 0.8), although individual figures for the Task class are not given. They then concentrated on Task sentences, establishing a set of predictive features (in which word n-grams emerged as “highly predictive”) and achieved reasonable per-sentence classification performance (with f-scores around 0.6). We investigated automatic action item detection from transcripts of multi-party meetings. Unlike previous work (Gruenstein et al., 2005), we use a new hierarchical annotation scheme based on the roles utterances play in the action item assignment process, and propose an approach to automatic detection that promises improved classification accuracy while enabling the extraction of useful information for summarization and reporting. 1 Background Introduction Action items are specific kinds of decisions common in multi-party meetings, characterized by the concrete assignment of tasks together with certain properties such as an associated timeframe and reponsible party. Our aims are firstly to automatically detect the regions of d"
W06-3405,W02-0811,0,0.0277325,"5 9 Precision 0.31 0.36 0.28 0.20 0.19 Recall 0.50 0.33 0.55 0.60 0.67 F-Score 0.38 0.35 0.37 0.30 0.30 Table 1: Baseline Classification Performance 4 Hierarchical Annotations Two problems are apparent: firstly, accuracy is lower than desired; secondly, identifying utterances related to action items does not allow us to actually identify those action items and extract their properties (deadline, owner etc.). But if the utterances related to these properties form distinct sub-classes which have their own distinct features, treating them separately and combining the results (along the lines of (Klein et al., 2002)) might allow better performance, while also identifying the utterances where each property’s value is extracted. Thus, we produced an annotation schema which distinguishes among these four classes. The first three correspond to the discussion and assignment of the individual properties of the action item (task description, timeframe and owner); the final agreement class covers utterances which explicitly show that the action item is agreed upon. Since the task description subclass extracts a description of the task, it must include any utterances that specify the action to be performed, inclu"
W06-3405,W04-2319,0,0.0362987,"fy the important properties of the action items (such as the associated tasks and deadlines) that would foster concise and informative semantically-based reporting (for example, adding task specifications to a user’s to-do list). We believe both of these aims are facilitated by taking into account the roles different utterances play in the decision-making process – in short, a shallow notion of discourse structure. While there are related tags for dialogue act tagging schema – like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al., 2004) which includes a commit tag – these classes are too general to allow identification of action items specifically. One comparable attempt in spoken discourse took a flat approach, annotating utterances as action-item-related or not (Gruenstein et al., 2005) over the ICSI and ISL meeting corpora (Janin et al., 2003; Burger et al., 2002). Their inter-annotator agreement was low (κ = .36). While this may have been partly due to their methods, it is notable that (Core and Allen, 1997) reported even lower agreement (κ = .15) on their Commit dialogue acts. Morgan et al. (forthcoming) then used these"
W06-3405,H93-1008,0,\N,Missing
W06-3405,S01-1021,0,\N,Missing
W08-0125,P04-1085,0,0.220016,"assistants that process, understand, summarize and report the output of meetings; meeting tracking systems that assist in implementing decisions; and group decision support systems that, for instance, help in constructing group memory (Romano and Nunamaker, 2001; Post et al., 2004; Voss et al., 2007). Previously researchers have focused on the interactive aspects of argumentative and decisionmaking dialogue, tackling issues such as the detection of agreement and disagreement and the level of emotional involvement of conversational participants (Hillard et al., 2003; Wrede and Shriberg, 2003; Galley et al., 2004; Gatica-Perez et al., 2005). From a perhaps more formal perspective, Verbree et al. (2006) have created an argumentation scheme intended to support automatic production of argument structure diagrams from decision-oriented meeting transcripts. Only Hsueh and Moore (2007a; 2007b), however, have specifically investigated the automatic detection of decisions. Using the AMI Meeting Corpus, Hsueh and Moore (2007b) attempt to identify the dialogue acts (DAs) in a meeting transcript that are “decisionrelated”. The authors define these DAs on the basis of two kinds of manually created summaries: an e"
W08-0125,N03-2012,0,0.316116,"range of applications, such as automatic meeting assistants that process, understand, summarize and report the output of meetings; meeting tracking systems that assist in implementing decisions; and group decision support systems that, for instance, help in constructing group memory (Romano and Nunamaker, 2001; Post et al., 2004; Voss et al., 2007). Previously researchers have focused on the interactive aspects of argumentative and decisionmaking dialogue, tackling issues such as the detection of agreement and disagreement and the level of emotional involvement of conversational participants (Hillard et al., 2003; Wrede and Shriberg, 2003; Galley et al., 2004; Gatica-Perez et al., 2005). From a perhaps more formal perspective, Verbree et al. (2006) have created an argumentation scheme intended to support automatic production of argument structure diagrams from decision-oriented meeting transcripts. Only Hsueh and Moore (2007a; 2007b), however, have specifically investigated the automatic detection of decisions. Using the AMI Meeting Corpus, Hsueh and Moore (2007b) attempt to identify the dialogue acts (DAs) in a meeting transcript that are “decisionrelated”. The authors define these DAs on the basis o"
W08-0125,N07-1004,0,0.365105,"that were made and the trains of reasoning that led to those decisions. Such a capability would allow work groups to keep track of courses of action that were shelved or rejected, and could allow new team members to get quickly up to speed. Thanks to the recent availability of substantial meeting corpora—such as the ISL (Burger et al., 2002), ICSI (Janin et al., 2004), and AMI (McCowan et al., 2005) Meeting Corpora—current research on the structure of decision-making dialogue and its use for automatic decision detection has helped to bring this vision closer to reality (Verbree et al., 2006; Hsueh and Moore, 2007b). Our aim here is to further that research by applying a simple notion of dialogue structure to the task of automatically detecting decisions in multiparty dialogue. A central hypothesis underlying our approach is that this task is best addressed by taking into account the roles that different utterances play in the decision-making process. Our claim is that this approach facilitates both the detection of regions of discourse where decisions are discussed and adopted, and also the identification of important aspects of the decision discussions themselves, thus opening the way to better and m"
W08-0125,lisowska-etal-2004-user,0,0.256809,"Missing"
W08-0125,2007.sigdial-1.4,1,0.731463,"cisions in multiparty dialogue. A central hypothesis underlying our approach is that this task is best addressed by taking into account the roles that different utterances play in the decision-making process. Our claim is that this approach facilitates both the detection of regions of discourse where decisions are discussed and adopted, and also the identification of important aspects of the decision discussions themselves, thus opening the way to better and more concise reporting. In the next section, we describe prior work on related efforts, including our own work on action item detection (Purver et al., 2007). Sections 3 and 4 then present our decision annotation scheme, which distinguishes several types of decision-related dialogue acts (DAs), and the corpus used as data (in this study a section of the AMI Meeting Corpus). Next, in Section 5, we describe our experimental methodology, including the basic conception of our classification approach, the features we used in classification, and our evaluation metrics. Section 6 then presents our results, obtained with a hierarchical classifier that first trains individual sub-classifiers to detect the different types of decision DAs, and then uses a su"
W08-0125,N07-4009,1,\N,Missing
W09-3937,E09-1085,0,0.0189901,"utterance has more than one split point, some portions may therefore act as the continuation for one split point, and the antecedent for the next. 3.2 General Our first interest is in the general statistics regarding SUs: how often do they occur, and what is the balance between same- and otherperson splits? Do they usually fall into the specific categories (with specific preferred split points) examined by e.g. Lerner (1991), or can the split point be anywhere? Dialogue Models We are not aware of any system/model which treats other-person splits, but same-person ones are now being looked at. Skantze and Schlangen (2009) present an incremental system design (for a limited domain) which can react to user feedback, e.g., backchannels, and resume with utterance completion if interrupted. Some related empirical work regarding the issue of turn-switch addressed here is also presented in Schlangen (2006) but the emphasis there centered mostly on prosodic rather than grammar/theorybased factors. 3 3.1 Questions Completeness For a grammatical treatment of SUs, as well as for implementing parsing/production mechanisms for their processing, we need to know about the likely completeness of antecedent and continuation (i"
W09-3937,J96-2004,0,\N,Missing
W09-3937,W09-3713,1,\N,Missing
W09-3937,E09-1000,0,\N,Missing
W09-3944,E09-1032,1,0.722128,"Missing"
W09-3944,2007.sigdial-1.40,1,0.884909,"Missing"
W09-3944,E06-1022,0,0.174502,"Missing"
W11-0144,W10-4342,0,0.0225779,"ls. 1 Introduction Many dialogue phenomena seem to motivate an incremental view of language processing: for example, a participant’s ability to change hearer/speaker role mid-sentence to produce or interpret backchannels, or complete or continue an utterance (see e.g. Yngve, 1970; Lerner, 2004, amongst many others). Much recent research in dialogue systems has pursued this line, resulting in frameworks for incremental dialogue processing (Schlangen and Skantze, 2009) and systems capable of mid-utterance backchannels (Skantze and Schlangen, 2009) or utterance completions (DeVault et al., 2009; Buß et al., 2010). However, to date there has been little focus on semantics, with the systems produced either operating in domains in which semantic representation is not required (Skantze and Schlangen, 2009), or using variants of domain-specific canned lexical or phrasal matching (Buß et al., 2010). Our intention is to extend this work to finer-grained and more domain-general notions of grammar and semantics, by using an incremental grammatical framework, Dynamic Syntax (DS, Kempson et al., 2001) together with the structured semantic representation provided by Type Theory with Records (TTR, see e.g. Cooper,"
W11-0144,W09-3902,0,0.0220966,"requests or backchannels. 1 Introduction Many dialogue phenomena seem to motivate an incremental view of language processing: for example, a participant’s ability to change hearer/speaker role mid-sentence to produce or interpret backchannels, or complete or continue an utterance (see e.g. Yngve, 1970; Lerner, 2004, amongst many others). Much recent research in dialogue systems has pursued this line, resulting in frameworks for incremental dialogue processing (Schlangen and Skantze, 2009) and systems capable of mid-utterance backchannels (Skantze and Schlangen, 2009) or utterance completions (DeVault et al., 2009; Buß et al., 2010). However, to date there has been little focus on semantics, with the systems produced either operating in domains in which semantic representation is not required (Skantze and Schlangen, 2009), or using variants of domain-specific canned lexical or phrasal matching (Buß et al., 2010). Our intention is to extend this work to finer-grained and more domain-general notions of grammar and semantics, by using an incremental grammatical framework, Dynamic Syntax (DS, Kempson et al., 2001) together with the structured semantic representation provided by Type Theory with Records (TT"
W11-0144,E09-1081,0,0.408253,"ialogue system capable of handling inherently incremental phenomena such as split utterances, adjuncts, and mid-sentence clarification requests or backchannels. 1 Introduction Many dialogue phenomena seem to motivate an incremental view of language processing: for example, a participant’s ability to change hearer/speaker role mid-sentence to produce or interpret backchannels, or complete or continue an utterance (see e.g. Yngve, 1970; Lerner, 2004, amongst many others). Much recent research in dialogue systems has pursued this line, resulting in frameworks for incremental dialogue processing (Schlangen and Skantze, 2009) and systems capable of mid-utterance backchannels (Skantze and Schlangen, 2009) or utterance completions (DeVault et al., 2009; Buß et al., 2010). However, to date there has been little focus on semantics, with the systems produced either operating in domains in which semantic representation is not required (Skantze and Schlangen, 2009), or using variants of domain-specific canned lexical or phrasal matching (Buß et al., 2010). Our intention is to extend this work to finer-grained and more domain-general notions of grammar and semantics, by using an incremental grammatical framework, Dynamic"
W11-0144,W10-4301,0,0.356918,"complexity. 3 There are similarities to chart parsing here: the tree graph edges spanning a state graph edge could be seen as corresponding to chart edges spanning a substring, with the tree nodes in the state Si as the agenda. However, the lack of a notion of syntactic constituency means no direct equivalent for the active/passive edge distinction; a detailed comparison is still to be carried out. 2 367 4 Dialogue System The DyLan parser has now been integrated into a working dialogue system by implementation as an Interpreter module in the Java-based incremental dialogue framework Jindigo (Skantze and Hjalmarsson, 2010). Jindigo follows Schlangen and Skantze (2009)’s abstract architecture specification and is specifically designed to handle units smaller than fully sentential utterances; one of its specific implementations is a travel agent system, and our module integrates semantic interpretation into this. As set out by Schlangen and Skantze (2009)’s specification, our Interpreter’s essential components are a left buffer (LB), processor and right buffer (RB). Incremental units (IUs) of various types are posted from the RB of one module to the LB of another; for our module, the LB-IUs are ASR word hypothese"
W11-0144,E09-1085,0,0.342813,"t utterances, adjuncts, and mid-sentence clarification requests or backchannels. 1 Introduction Many dialogue phenomena seem to motivate an incremental view of language processing: for example, a participant’s ability to change hearer/speaker role mid-sentence to produce or interpret backchannels, or complete or continue an utterance (see e.g. Yngve, 1970; Lerner, 2004, amongst many others). Much recent research in dialogue systems has pursued this line, resulting in frameworks for incremental dialogue processing (Schlangen and Skantze, 2009) and systems capable of mid-utterance backchannels (Skantze and Schlangen, 2009) or utterance completions (DeVault et al., 2009; Buß et al., 2010). However, to date there has been little focus on semantics, with the systems produced either operating in domains in which semantic representation is not required (Skantze and Schlangen, 2009), or using variants of domain-specific canned lexical or phrasal matching (Buß et al., 2010). Our intention is to extend this work to finer-grained and more domain-general notions of grammar and semantics, by using an incremental grammatical framework, Dynamic Syntax (DS, Kempson et al., 2001) together with the structured semantic represen"
W13-0110,J07-4004,0,0.0340082,"each word contributes to the target semantics. However, taking advantage of the DS formalism, we make two novel contributions: first, we bring an added dimension of incrementality: not only is learning sentence-by-sentence incremental, but the grammar learned is word-by-word incremental, commensurate with psycholinguistic results showing incrementality to be a fundamental feature of human parsing and production Lombardo and Sturt (1997); Ferreira and Swets (2002). While incremental parsing algorithms for standard grammar formalisms have seen much research (Hale, 2001; Collins and Roark, 2004; Clark and Curran, 2007), to the best of our knowledge, a learning system for an explicitly incremental grammar is yet to be presented. Second, by using a grammar in which syntax and parsing context are defined in terms of the growth of semantic structures, we can learn lexical entries for items such as pronouns the constraints on which depend on semantic context. ?T y(t) ?T y(t) ?T y(e), ?T y(e → t) ♦ −→ “john” T y(e), ?T y(e → t), ♦ john T y(t), ♦, upset′ (john′ )(mary ′ ) ?T y(t) −→ “upset” T y(e), john ?T y(e → t) −→ “mary” ?T y(e), T y(e → (e → t)), λyλx.upset′(x)(y) ♦ T y(e), john T y(e → t), λx.upset′ (x)(mary"
W13-0110,P04-1015,0,0.133854,"con which specifies what each word contributes to the target semantics. However, taking advantage of the DS formalism, we make two novel contributions: first, we bring an added dimension of incrementality: not only is learning sentence-by-sentence incremental, but the grammar learned is word-by-word incremental, commensurate with psycholinguistic results showing incrementality to be a fundamental feature of human parsing and production Lombardo and Sturt (1997); Ferreira and Swets (2002). While incremental parsing algorithms for standard grammar formalisms have seen much research (Hale, 2001; Collins and Roark, 2004; Clark and Curran, 2007), to the best of our knowledge, a learning system for an explicitly incremental grammar is yet to be presented. Second, by using a grammar in which syntax and parsing context are defined in terms of the growth of semantic structures, we can learn lexical entries for items such as pronouns the constraints on which depend on semantic context. ?T y(t) ?T y(t) ?T y(e), ?T y(e → t) ♦ −→ “john” T y(e), ?T y(e → t), ♦ john T y(t), ♦, upset′ (john′ )(mary ′ ) ?T y(t) −→ “upset” T y(e), john ?T y(e → t) −→ “mary” ?T y(e), T y(e → (e → t)), λyλx.upset′(x)(y) ♦ T y(e), john T y(e"
W13-0110,N01-1021,0,0.0964955,"lding a lexicon which specifies what each word contributes to the target semantics. However, taking advantage of the DS formalism, we make two novel contributions: first, we bring an added dimension of incrementality: not only is learning sentence-by-sentence incremental, but the grammar learned is word-by-word incremental, commensurate with psycholinguistic results showing incrementality to be a fundamental feature of human parsing and production Lombardo and Sturt (1997); Ferreira and Swets (2002). While incremental parsing algorithms for standard grammar formalisms have seen much research (Hale, 2001; Collins and Roark, 2004; Clark and Curran, 2007), to the best of our knowledge, a learning system for an explicitly incremental grammar is yet to be presented. Second, by using a grammar in which syntax and parsing context are defined in terms of the growth of semantic structures, we can learn lexical entries for items such as pronouns the constraints on which depend on semantic context. ?T y(t) ?T y(t) ?T y(e), ?T y(e → t) ♦ −→ “john” T y(e), ?T y(e → t), ♦ john T y(t), ♦, upset′ (john′ )(mary ′ ) ?T y(t) −→ “upset” T y(e), john ?T y(e → t) −→ “mary” ?T y(e), T y(e → (e → t)), λyλx.upset′(x"
W13-0110,E12-1024,0,0.0976326,"Missing"
W13-0110,D10-1119,0,0.0413457,"(Pereira and Schabes, 1992). More recently, another interesting line of work has emerged: lightly supervised learning guided by semantic rather than syntactic annotation, using sentence-level propositional logical form rather than detailed word-level annotation (more justifiably arguable to be ‘available’ to a human learner in a realworld situation, with some idea of what a string in an unknown language could mean). This has been successfully applied in Combinatorial Categorial Grammar (Steedman, 2000), as it tightly couples compositional semantics with syntax (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010, 2012); as CCG is a lexicalist framework, grammar learning involves inducing a lexicon assigning to each word its syntactic and semantic contribution. Moreover, the grammar is learnt ground-up in an ‘incremental’ fashion, in the sense that the learner collects data over time and does the learning sentence by sentence. Here we follow this spirit, inducing grammar from a propositional meaning representation and building a lexicon which specifies what each word contributes to the target semantics. However, taking advantage of the DS formalism, we make two novel contributions: first, we bring an"
W13-0110,P92-1017,0,0.592465,"models). Unsupervised methods, on the other hand, proceed from unannotated raw data; they are thus closer to the human language acquisition setting, but have seen less success. In its pure form —positive data only, without bias— unsupervised learning has been demonstrated to be computationally too complex (‘unlearnable’) in the worst case (Gold, 1967). Successful approaches involve some prior learning or bias, e.g. a fixed set of known lexical categories, a probability distribution bias (Klein and Manning, 2005) or a hybrid, semi-supervised method with shallower (e.g. POS-tagging) annotation (Pereira and Schabes, 1992). More recently, another interesting line of work has emerged: lightly supervised learning guided by semantic rather than syntactic annotation, using sentence-level propositional logical form rather than detailed word-level annotation (more justifiably arguable to be ‘available’ to a human learner in a realworld situation, with some idea of what a string in an unknown language could mean). This has been successfully applied in Combinatorial Categorial Grammar (Steedman, 2000), as it tightly couples compositional semantics with syntax (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010, 20"
W13-0110,W11-0144,1,0.511927,"s a lexical action. Different paths through the DAG represent different parsing strategies, which may succeed or fail depending on how the utterance is continued. Here, the path T0 − T3 will succeed if ‘John’ is the subject of an upcoming verb (“John upset Mary”); T0 − T4 will succeed if ‘John’ turns out to be a left-dislocated object (“John, Mary upset”). This DAG makes up the parse state at any point, and contains all information available to the parser. This includes semantic tree and tree-transition information taken to make up the linguistic context for ellipsis and pronominal construal (Purver et al., 2011). It also provides us with a basis for probabilistic parsing (see Sato, 2011): given a conditional probability distribution P (a|w, T ) over possible actions a given a word w and (some set of features of) the current partial tree T , the DAG can then be incrementally constructed and traversed in a best-first, breadth-first or beam parsing manner. 4 Learning lexical actions 4.1 Problem Statement Our task here is data-driven, probabilistic learning of lexical actions for all the words occurring in the corpus. Throughout, we will assume that the (language-independent) computational actions are kn"
W13-0110,D07-1071,0,0.387838,"r (e.g. POS-tagging) annotation (Pereira and Schabes, 1992). More recently, another interesting line of work has emerged: lightly supervised learning guided by semantic rather than syntactic annotation, using sentence-level propositional logical form rather than detailed word-level annotation (more justifiably arguable to be ‘available’ to a human learner in a realworld situation, with some idea of what a string in an unknown language could mean). This has been successfully applied in Combinatorial Categorial Grammar (Steedman, 2000), as it tightly couples compositional semantics with syntax (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010, 2012); as CCG is a lexicalist framework, grammar learning involves inducing a lexicon assigning to each word its syntactic and semantic contribution. Moreover, the grammar is learnt ground-up in an ‘incremental’ fashion, in the sense that the learner collects data over time and does the learning sentence by sentence. Here we follow this spirit, inducing grammar from a propositional meaning representation and building a lexicon which specifies what each word contributes to the target semantics. However, taking advantage of the DS formalism, we make two novel contribu"
W13-0110,H92-1024,0,\N,Missing
W13-0402,W06-3407,0,0.0499452,"Missing"
W13-0402,W12-1610,1,0.562274,"Missing"
W13-0402,P06-1003,1,0.858797,"Missing"
W13-2611,W08-2227,0,0.0600054,"Missing"
W13-2611,basile-etal-2012-developing,0,0.0459806,"Missing"
W13-2611,J07-4004,0,0.0267382,"e actions and structures, and can be modelled on the same DAG with the addition only of a goal tree; partial trees are checked for subsumption of the goal at each stage. The framework therefore inherently provides both parsing and generation that are word-by-word incremental and interchangeable, commensurate with psycholinguistic results (Lombardo and Sturt, 1997; Ferreira and Swets, 2002) and suitable for modelling dialogue (Howes et al., 2012). While standard grammar formalisms can of course also be used with incremental parsing or generation algorithms (Hale, 2001; Collins and Roark, 2004; Clark and Curran, 2007), their string-based grammaticality and lack of inherent parsing-generation interoperability means examples such as (1) remain problematic. 3 Method Our task here is to learn an incremental DS grammar; following Kwiatkowski et al. (2012), we assume as input a set of sentences paired with their semantic LFs. Eshghi et al. (2013) outline a method for inducing DS grammars from semantic DS trees (e.g. Fig. 1), in which possible lexical entries are incrementally hypothesized, constrained by subsumption of the target tree for the sentence. Here, however, this structured tree information is not avail"
W13-2611,W13-0110,1,0.519406,"sed directly in incremental probabilistic parsing and generation. We test this on child-directed utterances from the CHILDES corpus, and show that it results in good coverage and semantic accuracy, without requiring annotation at the word level or any independent notion of syntax. One such formalism is Dynamic Syntax (DS) (Kempson et al., 2001; Cann et al., 2005); it recognises no intermediate layer of syntax, but instead reflects grammatical constraints via constraints on the word-by-word incremental construction of meaning, underpinned by attendant concepts of underspecification and update. Eshghi et al. (2013) describe a method for inducing a probabilistic DS lexicon from sentences paired with DS semantic trees (see below) representing not only their meaning, but their functionargument structure with fine-grained typing information. They apply their method only to an artificial corpus generated using a known lexicon. Here, we build on that work to induce a lexicon from real child-directed utterances paired with less structured Logical Forms in the form of TTR Record Types (Cooper, 2005), thus providing less supervision. By assuming only the availability of a small set of general compositional seman"
W13-2611,P04-1015,0,0.0430162,"tion uses exactly the same actions and structures, and can be modelled on the same DAG with the addition only of a goal tree; partial trees are checked for subsumption of the goal at each stage. The framework therefore inherently provides both parsing and generation that are word-by-word incremental and interchangeable, commensurate with psycholinguistic results (Lombardo and Sturt, 1997; Ferreira and Swets, 2002) and suitable for modelling dialogue (Howes et al., 2012). While standard grammar formalisms can of course also be used with incremental parsing or generation algorithms (Hale, 2001; Collins and Roark, 2004; Clark and Curran, 2007), their string-based grammaticality and lack of inherent parsing-generation interoperability means examples such as (1) remain problematic. 3 Method Our task here is to learn an incremental DS grammar; following Kwiatkowski et al. (2012), we assume as input a set of sentences paired with their semantic LFs. Eshghi et al. (2013) outline a method for inducing DS grammars from semantic DS trees (e.g. Fig. 1), in which possible lexical entries are incrementally hypothesized, constrained by subsumption of the target tree for the sentence. Here, however, this structured tree"
W13-2611,N01-1021,0,0.0933761,"nner. Generation uses exactly the same actions and structures, and can be modelled on the same DAG with the addition only of a goal tree; partial trees are checked for subsumption of the goal at each stage. The framework therefore inherently provides both parsing and generation that are word-by-word incremental and interchangeable, commensurate with psycholinguistic results (Lombardo and Sturt, 1997; Ferreira and Swets, 2002) and suitable for modelling dialogue (Howes et al., 2012). While standard grammar formalisms can of course also be used with incremental parsing or generation algorithms (Hale, 2001; Collins and Roark, 2004; Clark and Curran, 2007), their string-based grammaticality and lack of inherent parsing-generation interoperability means examples such as (1) remain problematic. 3 Method Our task here is to learn an incremental DS grammar; following Kwiatkowski et al. (2012), we assume as input a set of sentences paired with their semantic LFs. Eshghi et al. (2013) outline a method for inducing DS grammars from semantic DS trees (e.g. Fig. 1), in which possible lexical entries are incrementally hypothesized, constrained by subsumption of the target tree for the sentence. Here, howe"
W13-2611,W12-1610,1,0.866203,"Missing"
W13-2611,D10-1119,0,0.0214787,"nd joint focus of attention with an adult (see e.g. (Saxton, 1997)). Given this, the problem she is faced with is one of separating out the contribution of each individual linguistic token to the overall meaning of an uttered linguistic expression (i.e. decomposition), while maintaining and generalising over several such hypotheses acquired through time as she is exposed to more utterances involving each token. This has been successfully applied in Combinatorial Categorial Grammar (CCG) (Steedman, 2000), as it tightly couples compositional semantics with syntax (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2012); as CCG is a lexicalist framework, grammar learning involves inducing a lexicon assigning to each word its syntactic and semantic contribution. Moreover, the grammar is learnt incrementally, in the sense that the learner collects data over time and does the learning sentence by sentence. Following this approach, Eshghi et al. (2013) outline a method for inducing a DS grammar from semantic LFs. This brings an added dimension of incrementality: not only is learning sentence-by-sentence incremental, but the grammar learned is inherently word-by-word incremental (see se"
W13-2611,E12-1024,0,0.199192,"n with an adult (see e.g. (Saxton, 1997)). Given this, the problem she is faced with is one of separating out the contribution of each individual linguistic token to the overall meaning of an uttered linguistic expression (i.e. decomposition), while maintaining and generalising over several such hypotheses acquired through time as she is exposed to more utterances involving each token. This has been successfully applied in Combinatorial Categorial Grammar (CCG) (Steedman, 2000), as it tightly couples compositional semantics with syntax (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2012); as CCG is a lexicalist framework, grammar learning involves inducing a lexicon assigning to each word its syntactic and semantic contribution. Moreover, the grammar is learnt incrementally, in the sense that the learner collects data over time and does the learning sentence by sentence. Following this approach, Eshghi et al. (2013) outline a method for inducing a DS grammar from semantic LFs. This brings an added dimension of incrementality: not only is learning sentence-by-sentence incremental, but the grammar learned is inherently word-by-word incremental (see section 2.2 below). However,"
W13-2611,P92-1017,0,0.393852,"learning models. Fully unsupervised methods, on the other hand, proceed from unannotated raw data; they are thus closer to the human language acquisition setting, but have seen less success. In its pure form —positive data only, without bias— unsupervised learning is computationally too complex (‘unlearnable’) in the worst case (Gold, 1967). Successful approaches involve some prior learning or bias (see (Clark and Lappin, 2011)) e.g. a set of known lexical categories, a probability distribution bias (Klein and Manning, 2005) or a semisupervised method with shallower (e.g. POS-tag) annotation (Pereira and Schabes, 1992). Another point on the spectrum is lightly supervised learning: providing information which constrains learning but with little or no lexicosyntactic detail. One possibility is the use of semantic annotation, using sentence-level propositional Logical Forms (LF). It seems more cognitively plausible, as the learner can be said to be able to understand, at least in part, the meaning 95 Computational actions These form a small, fixed set, which we assume as given here. Some merely encode the properties of the lambda calculus and the logical tree formalism itself, LoFT (Blackburn and Meyer-Viol, 1"
W13-2611,W11-0144,1,0.458278,"of a small set of general compositional semantic operations, reflecting the properties of the lambda calculus and the logic of finite trees, we ensure that the lexical entries learnt include the grammatical constraints and corresponding compositional semantic structure of the language. Our method exhibits incrementality in two senses: incremental learning, with the grammar being extended and refined as each new sentence becomes available; resulting in an inherently incremental, probabilistic grammar for parsing and production, suitable for use in state-of-the-art incremental dialogue systems (Purver et al., 2011) and for modelling humanhuman dialogue. 1 Introduction Human language processing has long been thought to function incrementally, both in parsing and production (Crocker et al., 2000; Ferreira, 1996). This incrementality gives rise to many characteristic phenomena in conversational dialogue, including unfinished utterances, interruptions and compound contributions constructed by more than one participant, which pose problems for standard grammar formalisms (Howes et al., 2012). In particular, examples such as (1) suggest that a suitable formalism would be one which defines grammaticality not i"
W13-2611,sagae-etal-2004-adding,0,0.0364346,") = i=1 n Y ′ θw (hji ) i 1 Z n Y ′ θw (hji ) i (2) (3) {j|h,w∈HTj } i=1 Incremental update The estimate of θw is now updated incrementally at each training example: N is a weighted average of the the new estimate θw N −1 and the new value from previous estimate θw ′′ from equation (3): the current example θw 1 ′′ N − 1 N−1 θw (h) + θw (h) N N       : e  : t     : e          Corpus We tested our approach on a section of the Eve corpus within CHILDES (MacWhinney, 2000), a series of English child-directed utterances, annotated with LFs by Kwiatkowski et al. (2012) following Sagae et al. (2004)’s syntactic annotation. We convert these LFs into semantically equivalent RTs; e.g. Fig 8 shows the conversion to a record type for “He doesn’t have a hat”. Importantly, our representations remove all part-of-speech or syntactic information; e.g. the subject, object and indirect object predicates function as purely semantic role information expressing an event’s participants. This includes e.g. do-aux(e) in (8), which is taken merely to represent temporal/aspectual information about the event, and could be part of any word hypothesis. From this corpus we selected 500 short utterance-record ty"
W13-2611,D07-1071,0,0.044727,"and (2) helpful interaction, and joint focus of attention with an adult (see e.g. (Saxton, 1997)). Given this, the problem she is faced with is one of separating out the contribution of each individual linguistic token to the overall meaning of an uttered linguistic expression (i.e. decomposition), while maintaining and generalising over several such hypotheses acquired through time as she is exposed to more utterances involving each token. This has been successfully applied in Combinatorial Categorial Grammar (CCG) (Steedman, 2000), as it tightly couples compositional semantics with syntax (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2012); as CCG is a lexicalist framework, grammar learning involves inducing a lexicon assigning to each word its syntactic and semantic contribution. Moreover, the grammar is learnt incrementally, in the sense that the learner collects data over time and does the learning sentence by sentence. Following this approach, Eshghi et al. (2013) outline a method for inducing a DS grammar from semantic LFs. This brings an added dimension of incrementality: not only is learning sentence-by-sentence incremental, but the grammar learned is inherently word-b"
W14-1410,W14-1409,0,0.387518,"mplex) record and T can be a record type – e.g. s could represent a dialogue gameboard state and T could be a dialogue gameboard state type (Ginzburg, 2012; Cooper, 2012). As TTR is highly flexible with a rich type system, variants have been considered with types corresponding to real-numbervalued perceptual judgements used in conjunction with linguistic context, such as visual perceptual information (Larsson, 2011; Dobnik et al., 2012), demonstrating its potential for embodied learning systems. The possibility of integration of perceptron learning (Larsson, 2011) and naive Bayes classifiers (Cooper et al., 2014) into TTR show how linguistic processing and probabilistic conceptual inference can be treated in a uniform way within the same representation system. Probabilistic TTR as described by Cooper et al. (2014) replaces the categorical s ∶ T judgement with the real number valued p(s ∶ T ) = v where v ∈ [0,1]. The authors show how standard probability theoretic and Bayesian equations can be applied to TTR judgements and how an agent might learn from experience in a simple classification game. The agent is presented with instances of We present an adaptation of recent work on probabilistic Type Theor"
W14-1410,J12-1006,0,0.050884,"Missing"
W14-1410,E09-1022,0,0.0249213,"pJ (s ∶ R1 ∣ s ∶ R1 ) = 1 and R1 ∧R2 = R1 ∧R3 = ⊥, the distribution remains unchanged. The system’s processing models how listeners reason about the revocation itself rather than predicting the outcome through positive evidence alone, in line with (Brennan and Schober, 2001)’s results. Learning in a dialogue While not our focus here, lattice G’s probabilities can be updated through observations after its initial construction. If a reference game is played over several rounds, the choice of referring expression can change based on mutually salient functions from words to situations- see e.g. (DeVault and Stone, 2009). Our currently frequentist approach to learning is: given an observation of an existing RT Ri is made with probability v, then ∥Ri ∥J , the overall denominator P (J) , and the nodes in the upward path from Ri to ⊤ are incremented by v. The approach could be converted to Bayesian update learning by using the prior probabilities in G for calculating v before it is added. Furthermore, observations can be added to G that include novel RTs: due to the DAG structure of G, their subtype ordering and probability effects can be integrated efficiently. 6 Extensions Dialogue and self-repair in the wild"
W14-1410,W11-0144,1,0.929377,"essing Julian Hough and Matthew Purver Cognitive Science Research Group School of Electronic Engineering and Computer Science Queen Mary University of London {j.hough,m.purver}@qmul.ac.uk Abstract 2 Previous Work Type Theory with Records (TTR) (Betarte and Tasistro, 1998; Cooper, 2005) is a rich type theory which has become widely used in dialogue models, including information state models for a variety of phenomena such as clarification requests (Ginzburg, 2012; Cooper, 2012) and nonsentential fragments (Fern´andez, 2006). It has also been shown to be useful for incremental semantic parsing (Purver et al., 2011), incremental generation (Hough and Purver, 2012), and recently for grammar induction (Eshghi et al., 2013). While the technical details will be given in section 3, the central judgement in type theory s ∶ T (that a given object s is of type T ) is extended in TTR so that s can be a (potentially complex) record and T can be a record type – e.g. s could represent a dialogue gameboard state and T could be a dialogue gameboard state type (Ginzburg, 2012; Cooper, 2012). As TTR is highly flexible with a rich type system, variants have been considered with types corresponding to real-numbervalued pe"
W14-1410,W13-2611,1,0.566306,"d Computer Science Queen Mary University of London {j.hough,m.purver}@qmul.ac.uk Abstract 2 Previous Work Type Theory with Records (TTR) (Betarte and Tasistro, 1998; Cooper, 2005) is a rich type theory which has become widely used in dialogue models, including information state models for a variety of phenomena such as clarification requests (Ginzburg, 2012; Cooper, 2012) and nonsentential fragments (Fern´andez, 2006). It has also been shown to be useful for incremental semantic parsing (Purver et al., 2011), incremental generation (Hough and Purver, 2012), and recently for grammar induction (Eshghi et al., 2013). While the technical details will be given in section 3, the central judgement in type theory s ∶ T (that a given object s is of type T ) is extended in TTR so that s can be a (potentially complex) record and T can be a record type – e.g. s could represent a dialogue gameboard state and T could be a dialogue gameboard state type (Ginzburg, 2012; Cooper, 2012). As TTR is highly flexible with a rich type system, variants have been considered with types corresponding to real-numbervalued perceptual judgements used in conjunction with linguistic context, such as visual perceptual information (Lar"
W14-1410,E09-1081,0,0.158874,"head in all RTs which corresponds to the DS tree node type. We also assume a neo-Davidsonian representation of predicates, with fields corresponding to an event term and to each semantic role; this allows all available semantic information to be specified incrementally in a strict subtyping relation (e.g. providing the subj() field when subject but not object has been parsed) – see Figure 2. We implement DS-TTR parsing and genera5 tion mechanisms in the DyLan dialogue system within Jindigo (Skantze and Hjalmarsson, 2010), a Java-based implementation of the incremental unit (IU) framework of (Schlangen and Skantze, 2009). In this framework, each module has input and output IUs which can be added as edges between vertices in module buffer graphs, and become committed should the appropriate conditions be fulfilled, a notion which becomes important in light of hypothesis change and repair situations. Dependency relations between different graphs within and between modules can be specified by groundedIn links (see (Schlangen and Skantze, 2009) for details). The DyLan interpreter module (Purver et al., 2011) uses Sato (2011)’s insight that the context of DS parsing can be characterized in terms of a Directed Acycl"
W14-1410,W10-4301,0,0.177224,"he parser’s previous maximal output. Following (Eshghi et al., 2013), DS-TTR tree nodes include a field head in all RTs which corresponds to the DS tree node type. We also assume a neo-Davidsonian representation of predicates, with fields corresponding to an event term and to each semantic role; this allows all available semantic information to be specified incrementally in a strict subtyping relation (e.g. providing the subj() field when subject but not object has been parsed) – see Figure 2. We implement DS-TTR parsing and genera5 tion mechanisms in the DyLan dialogue system within Jindigo (Skantze and Hjalmarsson, 2010), a Java-based implementation of the incremental unit (IU) framework of (Schlangen and Skantze, 2009). In this framework, each module has input and output IUs which can be added as edges between vertices in module buffer graphs, and become committed should the appropriate conditions be fulfilled, a notion which becomes important in light of hypothesis change and repair situations. Dependency relations between different graphs within and between modules can be specified by groundedIn links (see (Schlangen and Skantze, 2009) for details). The DyLan interpreter module (Purver et al., 2011) uses S"
W14-1505,N09-1003,0,0.108038,"Missing"
W14-1505,W98-0319,0,0.204206,"s specific to the corpus at hand. Here, in contrast, we use an external unlabeled resource to obtain a model of word meaning, composing words to obtain representations for utterances, and rely on training data only for discourse learning for the tagging task itself. Introduction One of the fundamental tasks in automatic dialogue processing is dialogue act tagging: labelling each utterance with a tag relating to its function in the dialogue and effect on the emerging context: greeting, query, statement etc (see e.g. (Core, 1998)). Although factors such as intonation also play a role (see e.g. (Jurafsky et al., 1998)), one of the most important sources of information in this task is the semantic meaning of an utterance, and this is reflected in the fact that people use similar words when they perform similar utterance acts. For example, utterances which state opinion (tagged sv in the standard DAMSL schema, see below) often include words such as “I think”, “I believe”, “I guess” etc. Hence, a similarity-based model of meaning — for instance, a distributional We proceed as follows. First, we discuss related work by introducing distributional semantics and describe common approaches for dialogue act tagging"
W14-1505,W13-3214,0,0.118591,"vector composition) are required. 1 Here, we extend bag-of-word models common in previous approaches (Serafin et al., 2003) with simple compositional distributional operations (Mitchell and Lapata, 2008) and examine the improvements gained. These improvements suggest that distributional information does improve performance, but that more sophisticated compositional operations such as matrix multiplication (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011) should provide further benefits. The state of the art is a supervised method based on Recurrent Convolutional Neural Networks (Kalchbrenner and Blunsom, 2013). This method learns both the sentence model and the discourse model from the same training corpus, making it hard to understand how much of the contribution comes from the inclusion of distributional word meaning, and how much from learning patterns specific to the corpus at hand. Here, in contrast, we use an external unlabeled resource to obtain a model of word meaning, composing words to obtain representations for utterances, and rely on training data only for discourse learning for the tagging task itself. Introduction One of the fundamental tasks in automatic dialogue processing is dialog"
W14-1505,D10-1115,0,0.714759,"simple models of compositionality fail to capture crucial information from word and utterance sequence; more advanced approaches (e.g. sequence- or grammar-driven, such as categorical, word vector composition) are required. 1 Here, we extend bag-of-word models common in previous approaches (Serafin et al., 2003) with simple compositional distributional operations (Mitchell and Lapata, 2008) and examine the improvements gained. These improvements suggest that distributional information does improve performance, but that more sophisticated compositional operations such as matrix multiplication (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011) should provide further benefits. The state of the art is a supervised method based on Recurrent Convolutional Neural Networks (Kalchbrenner and Blunsom, 2013). This method learns both the sentence model and the discourse model from the same training corpus, making it hard to understand how much of the contribution comes from the inclusion of distributional word meaning, and how much from learning patterns specific to the corpus at hand. Here, in contrast, we use an external unlabeled resource to obtain a model of word meaning, composing words to obtain repre"
W14-1505,P12-3029,0,0.0315625,"Missing"
W14-1505,P08-1028,0,0.82599,"ogue act sequence, and distributional information to the performance, and compare with the current state of the art approaches. Our experiment suggests that that distributional information is useful for dialogue act tagging but that simple models of compositionality fail to capture crucial information from word and utterance sequence; more advanced approaches (e.g. sequence- or grammar-driven, such as categorical, word vector composition) are required. 1 Here, we extend bag-of-word models common in previous approaches (Serafin et al., 2003) with simple compositional distributional operations (Mitchell and Lapata, 2008) and examine the improvements gained. These improvements suggest that distributional information does improve performance, but that more sophisticated compositional operations such as matrix multiplication (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011) should provide further benefits. The state of the art is a supervised method based on Recurrent Convolutional Neural Networks (Kalchbrenner and Blunsom, 2013). This method learns both the sentence model and the discourse model from the same training corpus, making it hard to understand how much of the contribution comes from the"
W14-1505,D10-1113,0,0.121379,"+ What kind of experience [ do you, + do you ] have, then with child care? / : I think, ] + {F uh, } I wonder if that worked. / A o A qw : Okay. : So What kind of experience do you do you have then with child care? B qyˆd: I guess I think uh I wonder if that worked. (a) A conversation with interrupted utterances. (b) A preprocessed conversation. Figure 1: A example of interrupted utterances from Switchboard and their transformation. sults, see (Mitchell and Lapata, 2008; Agirre et al., 2009) for various approaches. It is also common to apply dimensionality reduction to get higher performance (Dinu and Lapata, 2010; Baroni and Zamparelli, 2010). As target words we select all the words in our (Switchboard) training split. As context words we choose the 3000 most frequent words in the Google Ngram Corpus, excluding the 100 most frequent. To obtain co-occurrence frequencies from ngrams we sum up the frequency of a 5-gram over the years, treat the word in the middle as a target, and the other words as its contexts. For normalization, we experiment with a vector space based on raw co-occurrences; a vector space where frequencies are weighted using tf-idf; and another one with the number of dimensions reduced"
W14-1505,N03-2032,0,0.0878159,"Missing"
W14-1505,D12-1110,0,0.203784,"Missing"
W14-1505,P10-1097,0,0.0535982,"Missing"
W14-1505,P13-1045,0,0.0422794,"d, together with inter-utterance features, such as the sequence of utterance tags being used previously. To capture both of these aspects, sequence models 3 Utterance models In this paper, we investigate the extent to which distributional representations, word order infor41 However, this raises the question of how to compose these word vectors into a single representation for an utterance. Various approaches to compositional vector space modelling have been successfully applied to capture the meaning of a phrase in a range of tasks (Mitchell and Lapata, 2008; Grefenstette and Sadrzadeh, 2011; Socher et al., 2013). In this work, we follow (Mitchell and Lapata, 2008) and apply vector addition and pointwise multiplication to obtain the vector of an utterance from the words it consists of. This has the advantage of simplicity and domain-generality, requiring no sentence grammar (problematic for the non-canonical language in dialogue) or training on a specific corpus to obtain the appropriate compositionality operators or associative model; but has the disadvantage of losing word order information. The corresponding models are referred as addition and multiplication in Table 1 and Table 2. mation, and utte"
W14-1505,J00-3003,0,0.563942,"Missing"
W14-1505,J09-3004,0,0.0264259,"Missing"
W14-1505,D11-1129,0,\N,Missing
W14-3202,O04-3004,0,0.222429,"to 14, with a mean of 5.011 (s.d. 2.73). For all of the measures based on the transcripts, as outlined below, we included all text typed by both the therapist and the patient. In addition to the transcripts themselves, each patient normally filled out two questionnaires prior to each session with their therapist. These are described below. bust but require training from large datasets. Recent research has attempted finer-grained distinctions, e.g. detecting specific emotions such as anger, surprise, fear etc; again, approaches can be characterised as dictionary-based or machinelearning-based (Chuang and Wu, 2004; Seol et al., 2008; Purver and Battersby, 2012; De Choudhury et al., 2012). The resulting sentiment or emotion ratings have been widely used to determine aspects of personality and mental state in various domains. In social media text, Quercia et al. (2011; 2012) found correlations between sentiment and levels of popularity, influence and general wellbeing; O’Connor et al. (2010) with measures of public opinion. Closer to our application, Liakata et al. (2012) show that these methods can be applied to analyse emotion in suicide notes. 2.4 Research questions 3.2 Here, similar to (DeVault et al"
W14-3202,W13-4032,0,0.137971,"chizophrenia both conversation structure (how communication proceeds in therapy), and content (what is talked about), can affect outcomes (McCabe et al., 2013a; John et al., under review). NLP research has now begun to examine both. Wallace et al. (2013) model speech acts to characterise doctor-patient consultations on medication adherence; Angus et al. (2012) use unsupervised topic models to visualise shared content in clinical dialogue; Cretchley et al. (2010) use a similar approach for a qualititative analysis of topic and communication style between patients with schizophrenia and carers. DeVault et al. (2013) 2.3 Sentiment and emotion analysis One aspect of conversation process and style is the affect or emotion present. NLP research has generally approached this via the task of sentiment detection, distinguishing positive from negative (and sometimes neutral) stance (Pang and Lee, 2008). Methods generally take either a knowledge-rich approach (relying on e.g. dictionaries of sentiment-carrying words (Pennebaker et al., 2007)), or a data-rich approach via (usually supervised) machine learning over datasets of sentiment-carrying text (e.g. Socher et al. (2013)). The former can provide deeper insigh"
W14-3202,D08-1035,0,0.0154456,"effective, involve time-consuming hand-coding of data (Beattie et al., 2009; John et al., under review); NLP techniques can reduce this requirement. Unsupervised probabilistic models (e.g. Latent Dirichlet Allocation (LDA) Blei et al. (2003) and variants) have been widely applied to learn topics (word distributions) from the data itself, connecting words with similar meanings and even distinguishing between uses of words with multiple meanings (Steyvers and Griffiths, 2007). Such techniques have been applied successfully to structured dialogue e.g. meetings and tutorials (Purver et al., 2006; Eisenstein and Barzilay, 2008), and more recently to dialogues in the clinical domain (Cretchley et al., 2010; Howes et al., 2013), with topics found to identify important themes within therapy conversation such as medication, symptoms, family and social issues, and to correlate with outcomes. Background Computational analysis & mental health Research into computer-based diagnosis in mental health goes back at least to the 1960s – see (Overall and Hollister, 1964; Hirschfeld et al., 1974) amongst others – but most systems rely on doctoror patient-reported data rather than naturally occurring language. Much recent work simi"
W14-3202,D13-1170,0,0.0096846,"Missing"
W14-3202,D13-1182,0,0.0161001,"s when discussing conditions or treatment, e.g. discovering topics and opinions from online doctor ratings (Paul et al., 2013) or social media (Paul and Drezde, 2011). However, aspects of the communication during treatment itself are also associated with patient outcomes (Ong et al., 1995). In the mental health domain, recent work suggests that, for patients with schizophrenia both conversation structure (how communication proceeds in therapy), and content (what is talked about), can affect outcomes (McCabe et al., 2013a; John et al., under review). NLP research has now begun to examine both. Wallace et al. (2013) model speech acts to characterise doctor-patient consultations on medication adherence; Angus et al. (2012) use unsupervised topic models to visualise shared content in clinical dialogue; Cretchley et al. (2010) use a similar approach for a qualititative analysis of topic and communication style between patients with schizophrenia and carers. DeVault et al. (2013) 2.3 Sentiment and emotion analysis One aspect of conversation process and style is the affect or emotion present. NLP research has generally approached this via the task of sentiment detection, distinguishing positive from negative"
W14-3202,E12-1049,1,0.837095,"For all of the measures based on the transcripts, as outlined below, we included all text typed by both the therapist and the patient. In addition to the transcripts themselves, each patient normally filled out two questionnaires prior to each session with their therapist. These are described below. bust but require training from large datasets. Recent research has attempted finer-grained distinctions, e.g. detecting specific emotions such as anger, surprise, fear etc; again, approaches can be characterised as dictionary-based or machinelearning-based (Chuang and Wu, 2004; Seol et al., 2008; Purver and Battersby, 2012; De Choudhury et al., 2012). The resulting sentiment or emotion ratings have been widely used to determine aspects of personality and mental state in various domains. In social media text, Quercia et al. (2011; 2012) found correlations between sentiment and levels of popularity, influence and general wellbeing; O’Connor et al. (2010) with measures of public opinion. Closer to our application, Liakata et al. (2012) show that these methods can be applied to analyse emotion in suicide notes. 2.4 Research questions 3.2 Here, similar to (DeVault et al., 2013; Howes et al., 2013), our primary quest"
W14-3202,P06-1003,1,0.770684,"Missing"
W14-5318,N10-1027,0,0.181064,"ctors with entries for all observed word (token) unigrams, and character ngrams of lengths 1-3; feature values are counts (raw term frequencies) normalised 2 http://corporavm.uni-koeln.de/vardial/ 156 by the text length in tokens or characters respectively. We then train a single multi-class linear-kernel support vector machine using LIBLINEAR (Fan et al., 2008) with the language identifiers (en-US, en-UK, hr, bs, sr etc.) as labels. SVMs are well-suited to high-dimensional feature spaces; and SVMs with ngrams of these lengths have shown good performance in other language identification work (Baldwin and Lui, 2010). Features were given numerical indices corresponding to the unique ngram type (i.e. we used a feature dictionary with no hashing). No feature selection or frequency cutoff was used. No part-of-speech tagging or grammatical analysis was attempted; no external language resources or tools were used other than described above. Development and testing Development and test set texts were tokenised and featurised using the same process; feature indices were taken from the dictionary generated during training, with unseen ngram types ignored. LIBLINEAR was then used to predict the most likely languag"
W14-5318,P14-1062,0,0.0256173,"Missing"
W14-5318,U13-1003,0,0.303156,"Missing"
W14-5318,Q14-1003,0,0.0259247,") Czech (cz), Slovakian (sk) Brazilian Portuguese (pt-BR), European Portuguese (pt-PT) Peninsular Spain (es-ES), Argentine Spanish (es-AR) American English (en-US), British English (en-GB) Table 1: Languages and groups in the DSL Shared Task. However, problems were discovered in labelling the languages in Group F, and an evaluation for groups A-E was therefore performed separately; we discuss only this latter task and evaluation here. Related Work Classification approaches based on character or byte sequences have shown success in providing general models of language identification; see e.g. (Lui et al., 2014). In more specific experiments into discriminating between pairs or triples of similar languages, many researchers have found that word-based features can aid accuracy; but classification method and feature choice vary widely. When distinguishing Malay from Indonesian, Ranaivo-Malanc¸on (2006) combines character n-gram frequencies with heuristics based on number format and lists of words unique to each language. Ljubeˇsi´c et al. (2007) use a character trigram-based probabilistic language model, again in combination with a unique-word list, to distinguish between Croatian, Serbian and Slovenia"
W14-5318,C12-1160,0,0.150669,"Missing"
W14-5318,W14-5307,0,0.184451,"Missing"
W15-0130,W09-3936,0,0.0615722,"ted as a full propositional intention without some loss of the impression or effect shared in context (example 3). (3) 2 A: A: B: B: B: B: 1 2a 2b 2c 2d 2e Nirma was at the party. Nirma! / Nirma? / Who? (Why Nirma? / Sorry, I meant Irma.) Nirma eh? I knew it. Nirma! Oh how nice. Nirma. . . Wait, I know this name. Nirma, Nirma, I see. Overlapping talk is shown in aligned square brackets. 262 Models of grounding have been recently explored in practical dialogue systems. However, these systems either explore the positioning of backchannels based on low-level features (e.g. Cathcart et al., 2003; Gravano and Hirschberg, 2009); or rely on a notion of feedback that incorporates reasoning about the intentions, mental states or goals of one’s interlocutor (e.g. Visser et al., 2014; Buschmeier and Kopp, 2013; Wang et al., 2011). The former type of system may allow a dialogue model to sound ‘more human’, but do not give any insight into why feedback occurs where it does; and, in the latter, full intention recognition requires a level of complexity that corpus studies on repair (Purver et al., 2003; Colman and Healey, 2011), backchannels (Kjellmer, 2009), and conversational analysis of multiparty dialogue (Goodwin, 1979;"
W15-0130,A00-2001,0,0.097613,"acts, or intention recognition. On this account, if the two coordination pointers are on the same DAG node, any (sub-)utterance on the DAG path from this doubly pointed node back to the root can be taken to be grounded. More generally, the intersection of the q → root path and ♦ → root path is grounded; the remaining ♦ → root path is as yet ungrounded; and other paths are repaired (see below). Divergence of the two pointers represents the source for forward momentum in dialogue: something must be done by either of the parties in order to align these pointers. This is conceptually analogous to Matheson et al. (2000)’s notion of obligation or Ginzburg (2012)’s discursive potential, but operates without recourse to dialogue acts such as acceptance, or rejection. As will become clear below, linguistic signs of rejection here do not necessarily correspond to denying, or discussing a proposition, but instead indicate abandonment of a DAG branch. Under this general view, linguistic elements can unproblematically operate sub-propositionally, for instance, establishing a referent before the proposition involving that referent is complete. Confirmed Repaired 3.1 Local A: The doctor (a) B: Chorlton? A: Chorlton, m"
W15-0130,W11-0144,1,0.7735,"(e → t) x= john : e ,♦ head=x : e Figure 2: Lexical action for the word ‘John’ 264 “john” pred T2 “likes” john intro T 1 T0 link-adj *-adj T5 thin comp pred T9 T8 T7 likes T 13 link-adj T3 T4 abort john T6 thin T 11 comp T 10 abort T 12 abort Figure 3: DS parsing as a graph: actions (edges) are transitions between partial trees (nodes). 2.2 Type Theory with Records Type Theory with Records (TTR) is an extension of standard type theory shown useful in semantics and dialogue modelling (Cooper, 2005; Ginzburg, 2012). For us here, it provides the logical formalism in which meanings are expressed (Purver et al., 2011; Hough and Purver, 2012; Eshghi et al., 2012). Given its fine-grained, structured representations (see below), it has also been used to encode the linguistic, and non-linguistic context of an utterance (Purver et al., 2010; Dobnik et al., 2012). This is important for us here since such an integration provides for non-linguistic elicitations and provisions of feedback. In TTR, logical forms are specified as record types (RTs), sequences of fields of the form [ l : T ] containing a label l and a type T . RTs can be witnessed (i.e. judged true) by records of that type, where a record is a sequen"
W15-0130,W01-1616,1,\N,Missing
W16-5508,N09-1035,0,0.0321612,"stem itself as statistical properties inherent to the underlying corpus of sonnets. This process of building a space of potential subspaces is coupled with a phonological model which similarly uses an information theoretic metric to try to capture the way in which word-sounds are expected to co-occur in poetry. This model is also constructed from a statistical model of a corpus, in this instance a corpus containing about 1500 English language sonnets.1 These sonnets are rendered into a format containing both phonemic and syllabic information, based on a syllabified version of the CMU Arpabet (Bartlett et al., 2009). Frequencies of phonemic co-occurrence Ci (pa , pb ) are then tabulated, where the count C is the total number of times phoneme pb occurs i syllables in front of phoneme pb in a line of a poem. Once all frequencies for all lines in all poems in the corpus are compiled, these statistics are converted into mutual information measures, formulated here with Ci (T ) representing the total number of phonemes occurring i syllables apart and Ci (pa ) and C− i(pb ) standing for the independent frequencies at which phonemes pa and pb occur i or −i syllables away respectively from any other syllable: Pi"
W16-5508,W09-2005,0,0.433808,"istical mode of production, (Toivanen et al., 2012) describe a poetry generating system which discovers semantic relationships based on word co-occurrence statistics in a large scale corpus. In addition to this statistical technique for modelling semantics, this system imposes additional syntactic and phonological constraints on its output, and in this regard is comparable with the system described in this paper. Also within the general family of statistical, corpus based models, Haiku generation in particular has been a target for vector space model approaches to computational poetry. Gaiku (Netzer et al., 2009), for instance, uses a combination of human generated word association norms and sequences of syntax derived from a statistical analysis of a corpus of existing haiku to generate new haiku which are designed to be as meaningful, grammatical, and poetic as possible. The First Sally system for Haiku generation (Droog-Hayes and Wiggins, 2015) uses a distributional semantic model, based on an analysis of word co-occurrences in a large scale textual corpus, to generate sets of conceptually related words, and in this regard is closely related to the semantic element of the new system described in Se"
W17-4210,P13-2080,0,0.0273234,"ment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supported by insufficient evidence. This could be viewed as very close to the task of the detection of incongruent headlines, where the headline represents an argument which is not supported by claims in the text. Further, we could approach incongruence as a semantic issue and look to existing work on contradiction (De Marneffe et al., 2008), contrast (Harabagiu et al., 2006) and entailment recognition (Levy et al., 2013). In doing so, we may well discover several sub-types of incongruence which may fall into different semantic categories. (13) A sausage a day could lead to cancer: Pancreatic cancer warning over processed meat (14) Rise of the hugger mugger: Sociable thieves who cuddle while they rob (15) £100 to play truant! Schools accused of bribing worst pupils to stay away when Ofsted inspectors call Molek-Kozakowska (2013) views sensationalism as a discourse strategy used to repackage information in a more exciting, extraordinary or interesting way, via the presence of several discourse illocutions (e.g."
W17-4210,P00-1041,0,0.0467958,"of headline incongruence detection is best approached in parts: to analyse complex relationships between a headline and an entire news article is likely to be extremely difficult, not least because of their very different lengths and levels of linguistic complexity. This could therefore be facilitated with the extraction of key quotes (Pouliquen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supported by insufficient evidence. This could be viewed as very close to the task of the detection of incongruent headlines, where the headline represents an argument which is not supported by claim"
W17-4210,S16-1003,0,0.0362797,"eadlines this paper aims to highlight: incongruent headlines do not necessarily adhere to an identifiable style in their surface form, but rather must be identified in relation to the text they represent. This presents significant problems for the NLP approaches so far discussed. 13 As Molek-Kozakowska (2013) used only one news source (the Daily Mail), this list may be specific to this particular newspaper’s voice and/or the knowledge, subjectivity and demographic range of the annotators. 14 See Hoffman and Justicz (2016, Appendices 1-4). 59 Finally, stance detection (Augenstein et al., 2016; Mohammad et al., 2016) has been applied in the Fake News Challenge (FNC-1)15 as a means of exploring whether different articles agree or disagree with a given headline or claim, to aid in the task of fact checking. Stance is certainly relevant to task of incongruence detection, but we argue that it is not sufficient for our task, as the headlinearticle relation may be incongruent in ways separate from (dis)agreement. Beyond the headlinearticle pair itself, however, stance detection could be used to analyse engagement and interaction with an article on social media, given that early indications suggest that users ar"
W17-4210,E17-1092,0,0.0219404,"quen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supported by insufficient evidence. This could be viewed as very close to the task of the detection of incongruent headlines, where the headline represents an argument which is not supported by claims in the text. Further, we could approach incongruence as a semantic issue and look to existing work on contradiction (De Marneffe et al., 2008), contrast (Harabagiu et al., 2006) and entailment recognition (Levy et al., 2013). In doing so, we may well discover several sub-types of incongruence which may fall into different semantic categories. ("
W17-4210,E17-3010,0,0.094443,"ethodology, the source of the headline-article pair may well prove to be a useful feature in the broader classification process, which we will explore experimentally in future work. Arguably, the task of headline incongruence detection is best approached in parts: to analyse complex relationships between a headline and an entire news article is likely to be extremely difficult, not least because of their very different lengths and levels of linguistic complexity. This could therefore be facilitated with the extraction of key quotes (Pouliquen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supp"
W17-4210,P08-1118,0,0.0662678,"Missing"
W17-4210,W03-0501,0,0.299577,"approached in parts: to analyse complex relationships between a headline and an entire news article is likely to be extremely difficult, not least because of their very different lengths and levels of linguistic complexity. This could therefore be facilitated with the extraction of key quotes (Pouliquen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supported by insufficient evidence. This could be viewed as very close to the task of the detection of incongruent headlines, where the headline represents an argument which is not supported by claims in the text. Further, we could approach"
W17-4210,D15-1312,0,0.129872,"n conjunction with other methodology, the source of the headline-article pair may well prove to be a useful feature in the broader classification process, which we will explore experimentally in future work. Arguably, the task of headline incongruence detection is best approached in parts: to analyse complex relationships between a headline and an entire news article is likely to be extremely difficult, not least because of their very different lengths and levels of linguistic complexity. This could therefore be facilitated with the extraction of key quotes (Pouliquen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the ident"
W17-4210,N16-1138,0,0.0443085,"not sufficient for our task, as the headlinearticle relation may be incongruent in ways separate from (dis)agreement. Beyond the headlinearticle pair itself, however, stance detection could be used to analyse engagement and interaction with an article on social media, given that early indications suggest that users are compelled to alert others when they notice that a headline is misleading. 4 Discusses: The body text discuss the same topic as the headline, but does not take a position. Unrelated: The body text discusses a different topic than the headline. Built on the data set described in Ferreira and Vlachos (2016), which is collected from rumour tracking website, Emergent17 , the corpus contains approximately 50,000 annotated headlinebody pairs. A manual analysis of the first 50 body IDs led to a number of observations on the applicability of this data set to the problem of headline incongruence. Firstly, the ‘headline’ in a pair is the claim from the original post on the website, and is as such not necessarily a gold-standard headline. In addition, a single ‘headline’ can occur with multiple article bodies, and vice versa, which means that the original relation between the two is not captured. In our"
W17-4210,E17-4007,0,\N,Missing
W17-6813,J10-4006,0,0.0470999,"iding the smaller constituent by the larger. We also examine the angles 6 V ON , 6 V 0 M 0 N 0 , 6 V M N , and 6 V AN . Our objective is to establish mechanisms for systematically gauging the geometric relationships between the word-vectors corresponding to word-pairs, as well as the relative relationships between these word-vectors and some anchor points within a given subspace. With regard to these anchor points, it is important to note that, unlike typical distributional semantic methods which build normalised spaces through either the factorisation of a matrix of co-occurrence statistics (Baroni and Lenci, 2010; Pennington et al., 2014) or the application of neural networks for the learning of abstract word-vectors across iterations of a corpus (Mikolov et al., 2013), our spaces are not normalised, and so there may be considerable variance in terms of the distribution of values across different dimensions. Our case is that, in non-normalised context-specific subspaces, we should be able to find a richer range of geometric features with which to analyse various semantic properties of words relevant to the specific context determining a given projection. In fact, the contextually indifferent nature of"
W17-6813,jezek-quochi-2010-capturing,1,0.824587,"in the paper. (3) They finished their cake. (*drinking, eating) It has been noted, however, that linguistic and situational contexts play a crucial role in the interpretation of coercions: for example, in the corpus fragment in (4), taken from the EnTenTen corpus, the context triggers a different interpretation for wine (preparing, making) as object of finish. In other words, the “reconstructed hidden event” may be assigned contextually. (4) So unless the winemakers add tannin by finishing the wine in oak ... Extensive corpus work on both English and Italian data (Pustejovsky and Jezek, 2008; Jezek and Quochi, 2010, inter alia) has shown that coercion in predicate-argument composition is particularly frequent with certain verb classes, including event-selecting verbs (attend, cancel, organize) of which aspectual verbs constitutes a subclass (finish, interrupt, start, continue), perception verbs (hear, listen), communication verbs (announce, inform), directed motion verbs (arrive, reach), and verbs indicating motion performed using a vehicle (land). Data on mismatches between expected type and argument type offer several options of linguistic modelling. Pustejovsky (2011) for example proposes a two-layer"
W17-6813,J03-2004,0,0.0456456,"mposition, the semantics of the argument plays a crucial role in two ways. First, it provides the semantic purport on which selection or coercion may apply;2 second, in the presence of a coercion environment, it constrains the resulting interpretation. While the default interpretation of (2) is “drinking”, the one in (3) is “eating”; in other words, different nouns grant privileged access to different activities, particularly those which are most frequently performed with the entities they denote. 1 Such cases of coercion to event are referred to as logical metonymies (see Verspoor, 1997, and Lapata and Lascarides, 2003). 2 Coercions are not always successful; that is, some predicate-argument combinations are not interpretable. Constraints on interpretability are clearly related to cognition and the way we conceptualize entities and relations among them, an aspect we will return to later in the paper. (3) They finished their cake. (*drinking, eating) It has been noted, however, that linguistic and situational contexts play a crucial role in the interpretation of coercions: for example, in the corpus fragment in (4), taken from the EnTenTen corpus, the context triggers a different interpretation for wine (prep"
W17-6813,E14-1051,0,0.0330813,"e learning of abstract word-vectors across iterations of a corpus (Mikolov et al., 2013), our spaces are not normalised, and so there may be considerable variance in terms of the distribution of values across different dimensions. Our case is that, in non-normalised context-specific subspaces, we should be able to find a richer range of geometric features with which to analyse various semantic properties of words relevant to the specific context determining a given projection. In fact, the contextually indifferent nature of co-occurrence based models subjected to principal component analysis (Lebret and Collobert, 2014), the aforementioned neural network models, and hybrid models applying both word counting and neural network techniques (Pennington et al., 2014) are a motivation for the model we describe in this paper. While these established methodologies have achieved impressive results on a variety of language processing tasks, the representations composing them are static and abstract, and are therefore not susceptible to the online influence of contextual factors at play in our dimension selection techniques. Our case is that, for a phenomenon such as coercion, we require, as Pustejovsky (1995) has put"
W17-6813,J88-2003,0,0.303578,"ribe the results of a logistic regression model applied to the geometric features generated by our subspace projection technique, trained on the labelled coercion data described in Section 2. Section 5 will analyse these results, examining the way that the typical geometry of semantic relationships shift as they move from selectional to coerced uses. 2 Background: Coercive Verbs, Susceptible Nouns Coercion as a theoretical tool has been used in linguistic studies to account for several kinds of semantic shifts occurring in different linguistic structures. For example, aspectual type coercion (Moens and Steedman, 1988) identifies the shift occurring when a predicate denoting an event type is coerced to a different type by contextual triggers, as in (1a), where the punctual adverb suddenly coerces the predicate know from State to Transition. Grinding in the nominal domain (Copestake and Briscoe, 1995) consists of a mass construal of a count noun, as for pillow in (1b), which is coerced to mass by the quantifier some. Finally, coercion by construction (Michaelis, 2004) identifies a shift in the meaning of a verb as a result of its insertion in a specific construction, as in the causative construction in (1c)."
W17-6813,D14-1162,0,0.0826465,"tuent by the larger. We also examine the angles 6 V ON , 6 V 0 M 0 N 0 , 6 V M N , and 6 V AN . Our objective is to establish mechanisms for systematically gauging the geometric relationships between the word-vectors corresponding to word-pairs, as well as the relative relationships between these word-vectors and some anchor points within a given subspace. With regard to these anchor points, it is important to note that, unlike typical distributional semantic methods which build normalised spaces through either the factorisation of a matrix of co-occurrence statistics (Baroni and Lenci, 2010; Pennington et al., 2014) or the application of neural networks for the learning of abstract word-vectors across iterations of a corpus (Mikolov et al., 2013), our spaces are not normalised, and so there may be considerable variance in terms of the distribution of values across different dimensions. Our case is that, in non-normalised context-specific subspaces, we should be able to find a richer range of geometric features with which to analyse various semantic properties of words relevant to the specific context determining a given projection. In fact, the contextually indifferent nature of co-occurrence based model"
W17-6813,J91-4003,0,0.450965,"as in the causative construction in (1c). (1) a. She suddenly knew it. b. Give me some pillow. c. He barked them back to work. In this paper, we focus on semantic coercion in predicate-argument combination, intended as the compositional mechanisms that resolves an apparent mismatch between the semantic type expected by a predicate for a specific argument position (in one or more of its specific senses, should the predicate be polysemous) and the semantic type of the argument filler, by adjusting the type of the argument to satisfy the type requirement of the function (argument type coercion; Pustejovsky, 1991). An example is (2), where wine is coerced to an Activity (drinking) as a result of the semantic requirements the predicate imposes on its object, i.e. finish applies to an activity.1 (2) When they finished the wine, he stood up. (drinking) In predicate-argument composition, the semantics of the argument plays a crucial role in two ways. First, it provides the semantic purport on which selection or coercion may apply;2 second, in the presence of a coercion environment, it constrains the resulting interpretation. While the default interpretation of (2) is “drinking”, the one in (3) is “eating”;"
W17-6813,W04-1908,0,0.0448285,"typing (coercion), and classify it accordingly.4 A dataset was produced for both English and Italian, using the methodology described by Pustejovsky et al. (2010).5 First, five coercive verbs that impose semantic typing on one of their arguments in at least one of their senses (arrive, cancel, deny, finish, and hear) were selected by examining the data from the BNC, using the Sketch Engine corpus query tool. Sense inventories were compiled for each verb using OntoNotes as a reference. For each sense, a set of type templates was identified following the Corpus Pattern Analysis (CPA) technique (Pustejovsky et al., 2004; Hanks, 2013): every argument in the syntactic pattern associated with a given sense was assigned a type specification. The coercive senses of the chosen verbs were associated with type templates. Type templates and senses for the five verbs are summarized below: (5) a. HUMAN arrive at LOCATION (reach a destination or goal) b. HUMAN cancel EVENT (call off) c. HUMAN deny PROPOSITION (maintain that something is untrue) d. HUMAN finish EVENT (complete an activity) e. HUMAN hear SOUND (perceive physical sound) A set of sentences was randomly extracted for each target verb from the BNC. The extrac"
W17-6813,S10-1005,1,0.918064,"d in type theory, including complex types such as the one associated with book, which comprises a physical as well as an informational component. Coercion detection has been addressed as a specific NLP task in the context of SemEval 20103 , with the goal of testing the ability of computational models to identify whether the type that a verb selects is satisfied directly by the argument (selection), or whether the argument must change type to satisfy the verb typing (coercion), and classify it accordingly.4 A dataset was produced for both English and Italian, using the methodology described by Pustejovsky et al. (2010).5 First, five coercive verbs that impose semantic typing on one of their arguments in at least one of their senses (arrive, cancel, deny, finish, and hear) were selected by examining the data from the BNC, using the Sketch Engine corpus query tool. Sense inventories were compiled for each verb using OntoNotes as a reference. For each sense, a set of type templates was identified following the Corpus Pattern Analysis (CPA) technique (Pustejovsky et al., 2004; Hanks, 2013): every argument in the syntactic pattern associated with a given sense was assigned a type specification. The coercive sens"
W17-6813,S10-1056,0,0.0923558,"81 Table 3: Coercion identification scores on test data, based on a logistic regression on various dimension selection techniques in a 5x5 word co-occurrence window, 200 dimensional model built from training data, as well as scores for baselines. Methods using information about the identity of words priorly observed in selectional or coercive relationships are reported in italics. both cases, we consider cosine distance between the word-vectors in the spaces as the singular metric of relationships between words, in line with results reported through the NLP literature. The method described by Roberts and Harabagiu (2010) learns classes for nouns based on analysis of entailment relationships within WordNet. Combined with a statistical analysis of word and named entity co-occurrences, this approach essentially seeks to recapitulate the semantic class information available in knowledge bases in order to identify instances where coercion is indicated by verb-object class mismatches. We take as our main point of comparison the results reported on this dataset by Roberts and Harabagiu (2011), who develop a probabilistic model for coercion detection based within the latent Dirichlet allocation paradigm (Blei et al.,"
W17-6813,D11-1091,0,0.107882,"ular metric of relationships between words, in line with results reported through the NLP literature. The method described by Roberts and Harabagiu (2010) learns classes for nouns based on analysis of entailment relationships within WordNet. Combined with a statistical analysis of word and named entity co-occurrences, this approach essentially seeks to recapitulate the semantic class information available in knowledge bases in order to identify instances where coercion is indicated by verb-object class mismatches. We take as our main point of comparison the results reported on this dataset by Roberts and Harabagiu (2011), who develop a probabilistic model for coercion detection based within the latent Dirichlet allocation paradigm (Blei et al., 2003). In this later work the authors establish probability distributions for classes that can be taken as an argument by a verb V , and likewise for classes that can be assigned to an object N , and then calculate the summation of the joint probabilities of V taking a word of the same class as N as an argument, learning a threshold below which the value of this summation indicates coercion. The distributions themselves are learned through observations of predicate-arg"
W17-6813,S13-1040,0,0.0311794,"5; Wilson and Sperber, 2012). The computational representation of contextual shifts in lexical semantics presents a particularly significant challenge, in that it, at first glance, requires the establishment of a rule-based system for indicating the open-ended ways in which rules may be broken. Approaches have typically relied on the construction, in some way or another, of categorical conceptual representations – ontologies – designed for the transfer of properties between classes. So, for instance, motivated by the cognitive linguistic conceptual metaphor model of Lakoff and Johnson (1980), Shutova (2013) uses clustering techniques to define classes based on a statistical analysis of dependency relationship in a parsed corpus, and then uses class transgressions in verb-noun relationships to detect metaphor. Alternatively, Veale and Hao (2008) draw inspiration from the conceptual blending work of Fauconnier and Turner (2003) in their description of a system that combines information extracted from the WordNet knowledge base with statistical corpus analysis in order to treat metaphor as the porting of categorical information between conceptual domains. Of particular relevance to the research pre"
W17-6813,C08-1119,0,0.0386544,"ndicating the open-ended ways in which rules may be broken. Approaches have typically relied on the construction, in some way or another, of categorical conceptual representations – ontologies – designed for the transfer of properties between classes. So, for instance, motivated by the cognitive linguistic conceptual metaphor model of Lakoff and Johnson (1980), Shutova (2013) uses clustering techniques to define classes based on a statistical analysis of dependency relationship in a parsed corpus, and then uses class transgressions in verb-noun relationships to detect metaphor. Alternatively, Veale and Hao (2008) draw inspiration from the conceptual blending work of Fauconnier and Turner (2003) in their description of a system that combines information extracted from the WordNet knowledge base with statistical corpus analysis in order to treat metaphor as the porting of categorical information between conceptual domains. Of particular relevance to the research presented here is the work of Shutova et al. (2013), who likewise use a combination of corpus analysis and knowledge base extraction to predict classes of words in order to identify instances of logical metonymy. Notwithstanding the impressive r"
W17-6813,C04-1133,0,\N,Missing
