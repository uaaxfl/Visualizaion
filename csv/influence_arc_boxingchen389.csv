2005.iwslt-1.11,2004.iwslt-evaluation.8,1,\N,Missing
2005.iwslt-1.11,J93-2003,0,\N,Missing
2005.iwslt-1.11,J96-1002,0,\N,Missing
2005.iwslt-1.11,J00-2004,0,\N,Missing
2005.iwslt-1.11,J03-1005,0,\N,Missing
2005.iwslt-1.11,takezawa-etal-2002-toward,0,\N,Missing
2005.iwslt-1.11,P00-1056,0,\N,Missing
2005.iwslt-1.11,2004.iwslt-papers.2,1,\N,Missing
2006.iwslt-evaluation.7,W06-3110,0,0.0903759,"ments. Briefly, the CLA computes an association score between all possible word pairs within the parallel corpus, and then applies a greedy algorithm to compute the best word-alignment for each sentence pair • question feature, i.e. a binary feature which triggers when text ends with a question mark and starts with one of the typical starting words of question sentences found in training data • frequency of its n-grams (n=1,2,3,4) within the N-best translations • ratio between the target and source length • 2,3,5-grams target LMs • n-gram posterior probabilities within the N-best translations [4] • sentence length posterior probabilities [4] • word/block reordering probabilities (Section 2.2) 2. System Description • ratio of the source length and the number of source phrases (Section 2.3) ITC-irst SMT system [1] implements a log-linear model and features a two-step decoding strategy. In the first pass, a dynamic programming beam search algorithm generates N-best translation hypotheses for each source sentence. In the secThe first six feature functions were used in our system in IWSLT-2005. The two posterior probabilities represent a 53 Table 1: Statistics of training, development and"
2006.iwslt-evaluation.7,2006.iwslt-papers.4,1,0.889045,". Preprocessing tokenization txt-to-digit lower-casing Chi-to-Eng Chinese English x x x x – x Jpn-to-Eng Japanese English x x – – – x refinement of the counts of n-grams in N-best lists we employed for the 2005 campaign. The last two features are new and are presented in the following. Ara-to-Eng Arabic English x x – – – x Ita-to-Eng Italian English x x x x x x hrules (˜ e, f , a) = K 1 X log Pr(ri ) K i=1 (1) where ri is a matching rule, Pr(ri ) its probability and K the number of the reordering patterns matching the given source/target pair. 2.2. Word/Block Reordering In the companion paper [5], the use of rules for modeling word reordering phenomena in phrase-based SMT is proposed. Reordering rules consist of two sides: the left-handside (lhs), which is a word-based pattern, and the right-handside (rhs), which corresponds to a possible reordering of that pattern. Different rules can share the lhs, because the same pattern can be reordered in more than one way. Rules are automatically extracted from word aligned training data and weighted according to observed statistics. Rules can reorder sequences of single words or a pair of blocks of words. A block is a sequence of source words"
2006.iwslt-evaluation.7,takezawa-etal-2002-toward,0,0.137694,"Missing"
2006.iwslt-evaluation.7,P00-1056,0,0.124776,"2 20.04 5.445 21.07 5.465 Ita-to-Eng BLEU NIST 35.24 7.779 35.59 7.859 – – – – 37.47 8.075 tuation, etc. For both the modules we use the disambig tool,1 as suggested in the instructions supplied by the evaluation organizers. The English training data have been employed to train the language models of the two modules. 3.3. Development: Baseline and Improvements The setup of baselines includes the use of phrases up to 8 words and monotonic search. We extracted phrases and estimated the phrase translation models from the intersection of direct and inverse IBM alignments, expanded as suggested in [7]. Improvements were carried out by introducing novelties in an incremental way, monitoring performance on development sets. Upgrades involve: (i) the addition of CLA wordalignments and (ii) of IBM union word-alignments [3], (iii) the execution of non-monotonic search, and (iv) the application of the rescoring module. Non-monotonic search uses constraints on word reordering defined by means of the maximum vacancy number (MVN) and maximum vacancy distance (MVD) parameters. The following setting was used for experiments: 3.1. Preprocessing The most important stage of preprocessing is tokenization"
2006.iwslt-evaluation.7,W05-0835,0,0.122059,"lations is generated for each source sentence by means of a beam-search decoder; in the second pass, N-best lists are rescored and reranked exploiting additional feature functions. Main updates brought to the 2005 system involve novel additional features which are here described. Results on development sets are analyzed and commented. 2.1. Rescoring Models The feature functions and search constraints adopted for decoding are quite standard: phrase and word translation models, 4-gram language model, fertility model, IBM reordering constraints, beam search. A detailed description is provided in [1, 2]. On the other hand, SMT systems often differ a lot in the models employed for rescoring N-best candidates. Here the list of those we apply: 1. Introduction • direct and inverse IBM model 1 and 3 lexicons, over all possible alignments In this paper, we report on the participation of ITC-irst to the evaluation campaign of the International Workshop on Spoken Language Translation 2006. We submitted runs under the Open Data conditions for all the language pairs: Arabic-to-English, Chinese-to-English, Japanese-to-English and Italian-to-English. For each language pair, we performed translations of"
2006.iwslt-evaluation.7,2005.iwslt-1.11,1,\N,Missing
2006.iwslt-papers.4,J99-4005,0,0.203932,"slation task. On other language pairs which differ a lot in the word order, the use of lexicalized rules allows to observe significant improvements as well. 1. Introduction In Machine Translation (MT), one of the main problems to handle is word reordering. Informally, a word is “reordered” when it and its translation occupy different positions within the corresponding sentences. In Statistical Machine Translation (SMT) [1], word reordering is faced from two points of view: constraints and modeling. If arbitrary word-reorderings are permitted, the exact decoding problem was shown to be NP-hard [2]; it can be made polynomial-time by introducing proper constraints, such as IBM constraints [3] and Inversion Transduction Grammars (ITG) constraints [4, 5]. It is worth to notice that both types of constraints are linguistically blind, i.e. they are unable to tune the number of allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausib"
2006.iwslt-papers.4,J97-3002,0,0.157164,"ll. 1. Introduction In Machine Translation (MT), one of the main problems to handle is word reordering. Informally, a word is “reordered” when it and its translation occupy different positions within the corresponding sentences. In Statistical Machine Translation (SMT) [1], word reordering is faced from two points of view: constraints and modeling. If arbitrary word-reorderings are permitted, the exact decoding problem was shown to be NP-hard [2]; it can be made polynomial-time by introducing proper constraints, such as IBM constraints [3] and Inversion Transduction Grammars (ITG) constraints [4, 5]. It is worth to notice that both types of constraints are linguistically blind, i.e. they are unable to tune the number of allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target"
2006.iwslt-papers.4,N04-1021,0,0.0472691,"allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-s"
2006.iwslt-papers.4,N04-4026,0,0.0427816,"allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-s"
2006.iwslt-papers.4,P05-1069,0,0.0683208,"allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-s"
2006.iwslt-papers.4,2004.iwslt-evaluation.6,0,0.0148907,"s. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-side of rules can be plain words or POS’s; moreover, rules can reorder sequences of single units or a pair of unit blocks. In a two-stage SMT system like our one, reordering rules could be exploited"
2006.iwslt-papers.4,P02-1038,0,0.332166,"we have worked with. Section 3 introduces the new reordering method. Sections 4 and 5 present the experimental results and future application, respectively. Some conclusions are drawn in Section 6. 2. The Phrase-based SMT System Given a string f in the source language, the goal of statistical machine translation is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrase-based translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model [11, 12] and by introducing the concept of word alignment [1], the optimal translation can be searched for with the criterion: ˜ e∗ = arg max max ˜ e 182 a R X r=1 λr hr (˜ e, f , a), source string decoder target string WG N−best Rescoring : extractor (from WG) a b c d b a b Figure 1: Architecture of the ITC-irst SMT system. The decoder produces a word-graph (WG) of translation hypotheses. In single-stage translation the most probable string is output. In two-stage decoding, N-best translations are extracted, re-scored, and re-ranked by applying additional feature functions. where ˜ e represents a str"
2006.iwslt-papers.4,takezawa-etal-2002-toward,0,0.0601658,"ORDER BlockREORDER BlockREORDER BlockREORDER BlockREORDER BlockREORDER BlockREORDER BlockREORDER /rr •/vmodal /rr /vmodal /rr /vmodal * * * * [/rr /vmodal] [/rr /vmodal] [/rr /vmodal] [/rr /vmodal] * * * * * * /v * * * * [/v] [/v] * * [/v] [/v] * where K is the number of the reordering patterns matching the given source/target pair. If for a matching POS pattern there is no reordering suggestion matching the actual translation, a small probability is used in the sum (2). 4. Experiments 4.1. Translation Tasks and Data Experiments were carried out on the Basic Traveling Expression Corpus (BTEC) [19]. BTEC is a multilingual speech corpus that contains translation pairs taken from phrase books for tourists. We conducted experiments on three language pairs: Chinese-to-English, Japanese-to-English and Arabic-to-English. On the Chinese-to-English direction, both POS and lexicalized rules were tested. On the contrary, for the other three language pairs, only lexicalized rules were experimented: lexicalized rules were extracted and applied in the same way as POS rules, with the only obvious difference that here the lhs is defined on words instead of POS’s. Detailed statistics on BTEC training d"
2006.iwslt-papers.4,W03-1709,0,0.0362555,"that here the lhs is defined on words instead of POS’s. Detailed statistics on BTEC training data are reported in Table 2. Data sets distributed for the CSTAR 2003 Evaluation Campaign were employed for development, while testing was performed on sets of IWSLT 2004 and IWSLT 2005 Evaluation Campaigns, and on one of the development sets (devset4) distributed for the IWSLT 2006 Evaluation Campaign. For each source sentence of those sets, 16 or 7 references are available. Detailed statistics are reported in Table 3. The Chinese word segmentation and POS tagging were performed by means of ICTCLAS [20] for the experiments involving POS rules. In the experiments using lexicalized rules, Chinese and Japanese were re-segmented with an inhouse word segmentation tool. We found that this permits to smooth inconsistencies of the manual segmentation. All texts were finally tokenized and put in lower case. In the following, translation performance are provided in terms of BLEU [21] and NIST1 scores, computed in the 1 http://www.nist.gov/speech/tests/mt/ /ng * * /ng /ng * * [/ng [/ng [/ng [/ng [/ng ¡/v * * /v /v * * /v] /v] /v] /v] /v] /wfullstop * * * * * * * * * * [/wfullstop] #21: #12: #12: #10: #"
2006.iwslt-papers.4,W05-0835,0,0.105966,"ed to model different aspects of the translation process. Figure 1 illustrates how the translation of an input string is performed by the ITC-irst SMT system. In the first stage, a beam search algorithm (decoder) computes a word graph of translation hypotheses. Hence, either the best translation hypothesis is directly extracted from the word graph and output, or an N-best list of translations is computed by means of an exact algorithm [13]. The N-best translations are then reranked by applying additional feature functions and the top ranking translation is finally output. The search algorithm [14] exploits dynamic programming, i.e. the optimal solution is computed by expanding and recombining previously computed partial theories. The target string is extended step by step by covering new source positions until all of them are covered. For each added target phrase, a source phrase within the source string is chosen, and the corresponding score is computed on the basis of its position and phrase-to-phrase translation probabilities. The fluency of the added target phrase with respect to its left context is evaluated by a 4-gram language model. Some exceptions are also managed: target phra"
2006.iwslt-papers.4,P03-1021,0,0.0875236,"e less promising and constraints are set to possible word re-ordering. Word re-ordering constraints are applied during translation each time a new source position is covered, by limiting the number of vacant positions on the left and the distance from the left most vacant position. The log-linear model on which both the search algorithm and the rescoring stage work embeds feature functions whose parameters are either estimated from data or empirically fixed. The scaling factors λ of the log-linear model are instead estimated on a development set, by applying a minimum error training procedure [15, 16]. The language model feature function is estimated on unsegmented monolingual texts. c a c d b Figure 2: Example illustrating the concept of block. The phrase-to-phrase probability feature is estimated from phrase-pair statistics extracted from word-aligned parallel texts. Alignments are computed with the GIZA++ software tool [17]. Phrase pairs are extracted from the segment pairs by means of the algorithm described in [18]. The distortion model feature function is a fixed negative exponential function computed on the distance between the current and the previously translated source phrases. 3"
2006.iwslt-papers.4,2004.iwslt-papers.2,1,0.770981,"e less promising and constraints are set to possible word re-ordering. Word re-ordering constraints are applied during translation each time a new source position is covered, by limiting the number of vacant positions on the left and the distance from the left most vacant position. The log-linear model on which both the search algorithm and the rescoring stage work embeds feature functions whose parameters are either estimated from data or empirically fixed. The scaling factors λ of the log-linear model are instead estimated on a development set, by applying a minimum error training procedure [15, 16]. The language model feature function is estimated on unsegmented monolingual texts. c a c d b Figure 2: Example illustrating the concept of block. The phrase-to-phrase probability feature is estimated from phrase-pair statistics extracted from word-aligned parallel texts. Alignments are computed with the GIZA++ software tool [17]. Phrase pairs are extracted from the segment pairs by means of the algorithm described in [18]. The distortion model feature function is a fixed negative exponential function computed on the distance between the current and the previously translated source phrases. 3"
2006.iwslt-papers.4,P00-1056,0,0.0950438,"hm and the rescoring stage work embeds feature functions whose parameters are either estimated from data or empirically fixed. The scaling factors λ of the log-linear model are instead estimated on a development set, by applying a minimum error training procedure [15, 16]. The language model feature function is estimated on unsegmented monolingual texts. c a c d b Figure 2: Example illustrating the concept of block. The phrase-to-phrase probability feature is estimated from phrase-pair statistics extracted from word-aligned parallel texts. Alignments are computed with the GIZA++ software tool [17]. Phrase pairs are extracted from the segment pairs by means of the algorithm described in [18]. The distortion model feature function is a fixed negative exponential function computed on the distance between the current and the previously translated source phrases. 3. Reordering Rules In order to overcome the limitations of our simple distortion model, we propose to enrich the translation process with reordering rules as defined in the following. Units on which rules work can be words (lexicalized rules) or POS’s (POS rules). Rules can suggest/constraint reorderings at the level of either sin"
2006.iwslt-papers.4,J93-2003,0,\N,Missing
2006.iwslt-papers.4,J96-1002,0,\N,Missing
2006.iwslt-papers.4,P02-1040,0,\N,Missing
2006.iwslt-papers.4,2005.iwslt-1.11,1,\N,Missing
2007.iwslt-1.8,N03-1017,0,0.00869946,"Missing"
2007.iwslt-1.8,W05-0835,0,0.0370063,"Missing"
2007.iwslt-1.8,P07-2045,0,0.0176492,"Missing"
2007.iwslt-1.8,P02-1038,0,0.254499,"Missing"
2007.iwslt-1.8,J93-2003,0,0.0108048,"Missing"
2007.iwslt-1.8,P03-1021,0,0.0319677,"Missing"
2007.iwslt-1.8,J03-1002,0,0.00866112,"Missing"
2007.iwslt-1.8,2007.mtsummit-papers.71,1,0.82372,"Missing"
2007.iwslt-1.8,2006.iwslt-papers.4,1,0.902165,"Missing"
2007.iwslt-1.8,W06-3110,0,0.310267,"Missing"
2007.iwslt-1.8,takezawa-etal-2002-toward,0,0.150368,"Missing"
2007.iwslt-1.8,W03-1730,0,0.0270664,"Missing"
2007.iwslt-1.8,2006.iwslt-evaluation.7,1,\N,Missing
2007.iwslt-1.8,J96-1002,0,\N,Missing
2007.iwslt-1.8,2005.iwslt-1.11,1,\N,Missing
2007.mtsummit-papers.15,J96-1002,0,0.0149963,"organized as follow. Section 2 presents the phrase-based SMT system we work with. Section 3 introduces the new hypotheses generation algorithm. Section 4 describes experiments and analyzes results. A discussion and conclusions end the paper. 2. SMT Process Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996; Och and Ney, 2002) and by introducing the concept of word alignment (Brown et al., 1993), the optimal translation can be searched for with the criterion: ˜ e∗ = arg max max ˜ e a R X λr hr (˜ e, f , a), r=1 where ˜ e represents a string of phrases in the target language, a an alignment from the words in f to the phrases in ˜ e, and hr (˜ e, f , a) r = 1, . . . , R are feature functions, designed to model different aspects of the translation process. We performed the “argmax” operation of the above equation by means of the decoder available in Moses,1 an open source toolkit for SMT. Besides t"
2007.mtsummit-papers.15,J93-2003,0,0.0168755,"3 introduces the new hypotheses generation algorithm. Section 4 describes experiments and analyzes results. A discussion and conclusions end the paper. 2. SMT Process Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996; Och and Ney, 2002) and by introducing the concept of word alignment (Brown et al., 1993), the optimal translation can be searched for with the criterion: ˜ e∗ = arg max max ˜ e a R X λr hr (˜ e, f , a), r=1 where ˜ e represents a string of phrases in the target language, a an alignment from the words in f to the phrases in ˜ e, and hr (˜ e, f , a) r = 1, . . . , R are feature functions, designed to model different aspects of the translation process. We performed the “argmax” operation of the above equation by means of the decoder available in Moses,1 an open source toolkit for SMT. Besides the decoder, Moses provides tools for training translation and lexicalized reordering model"
2007.mtsummit-papers.15,E06-1032,0,0.0127946,"er’s constraints. For instance, it is easy to verify that a low-order LM (e.g. a bigram LM) permits long word movements and the creation of phrases which are not contained in the phrase-table. The algorithm we have proposed for expanding N-best lists is simple and intuitive, but nevertheless effective in improving the quality of a two-pass SMT system. In fact, small but consistent improvements were measured on various evaluation sets on the challenging Chinese-to-English NIST MT task over a state-of-the-art performing baseline. We are aware of the criticisms about the BLEU score expressed in (Callison-Burch et al., 2006), which could particularly apply to our technique. In their paper, Burch et. al showed that for a translation output there are many possible variants, based on word permutations, that would each receive a similar BLEU score. Some variations could even correspond to higher scores but not to any genuine improvement in translation quality. The same could indeed apply to the translations computed by the expansion step, which in practice generates new word arrangements from the N-best list. We cannot reject this claim, as we did not manually inspect all the translations. However, from one hand our"
2007.mtsummit-papers.15,2006.iwslt-papers.4,1,0.898932,"Missing"
2007.mtsummit-papers.15,W05-0835,0,0.0183761,"ist indeed among the N-best ones. In this paper we present a technique to expand existing N-best lists in order to increase their potential of containing better translations. New entries are generated by means of a word-based n-gram language model estimated on the N-best entries. Experimental results on the NIST Chinese-to-English task show that better N-best lists can be obtained which also result in systematic BLEU score improvements in the re-scoring step. 1. Introduction In Statistical Machine Translation (SMT), performance improvements are often reported by applying two processing steps (Federico and Bertoldi, 2005; Koehn et al., 2003). In the first step, a decoding algorithms is applied that generates an N-best list of translation hypotheses. In the second step, the final translation is computed by re-ranking the Nbest translations through additional scores, computed with more sophisticated feature functions. Clearly, a fundamental assumption of the two step approach is that the generated N-best list contains better translations than the best one found by the decoder. The aim of the additional feature functions is indeed to reward better translations found among the N-best entries of the decoder. The r"
2007.mtsummit-papers.15,N03-1017,0,0.00526933,"nes. In this paper we present a technique to expand existing N-best lists in order to increase their potential of containing better translations. New entries are generated by means of a word-based n-gram language model estimated on the N-best entries. Experimental results on the NIST Chinese-to-English task show that better N-best lists can be obtained which also result in systematic BLEU score improvements in the re-scoring step. 1. Introduction In Statistical Machine Translation (SMT), performance improvements are often reported by applying two processing steps (Federico and Bertoldi, 2005; Koehn et al., 2003). In the first step, a decoding algorithms is applied that generates an N-best list of translation hypotheses. In the second step, the final translation is computed by re-ranking the Nbest translations through additional scores, computed with more sophisticated feature functions. Clearly, a fundamental assumption of the two step approach is that the generated N-best list contains better translations than the best one found by the decoder. The aim of the additional feature functions is indeed to reward better translations found among the N-best entries of the decoder. The reason for applying tw"
2007.mtsummit-papers.15,2005.iwslt-1.8,0,0.0157904,"coding: besides “large” and “giga” LMs, a third 4-gram LM was trained on the English side of the development sets (“dev”). MT performance are provided in terms of case-insensitive BLEU and NIST scores, as computed with the NIST scoring tool. 4.2. Setup For the generation of N-best translation lists, we run Moses with the maximum reordering distance set to 6 and the following feature functions: • phrase translation model, with phrases including at most 7 words • 3 LMs, namely “dev”, “large” and “giga” • lexicalized distortion model, trained specifying the option “orientation-bidirectional-fe” (Koehn et al., 2005) • word and phrase penalties, for balancing the length of translations with that of input sentences. Once N-best lists are available, they are expanded through the algorithm described in Section 3 by setting n = 4. Then, original (1) and updated (2) lists are re-scored and then re-ranked by applying some of the following feature functions, as specified in brackets: • n-gram posterior probabilities within the expanded N+M-best translations (2) • sentence length posterior probabilities within the original N-best translations (Zens and Ney, 2006) (1, 2) • sentence length posterior probabilities w"
2007.mtsummit-papers.15,J00-2004,0,0.245198,"Missing"
2007.mtsummit-papers.15,P02-1038,0,0.0289396,"Section 2 presents the phrase-based SMT system we work with. Section 3 introduces the new hypotheses generation algorithm. Section 4 describes experiments and analyzes results. A discussion and conclusions end the paper. 2. SMT Process Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996; Och and Ney, 2002) and by introducing the concept of word alignment (Brown et al., 1993), the optimal translation can be searched for with the criterion: ˜ e∗ = arg max max ˜ e a R X λr hr (˜ e, f , a), r=1 where ˜ e represents a string of phrases in the target language, a an alignment from the words in f to the phrases in ˜ e, and hr (˜ e, f , a) r = 1, . . . , R are feature functions, designed to model different aspects of the translation process. We performed the “argmax” operation of the above equation by means of the decoder available in Moses,1 an open source toolkit for SMT. Besides the decoder, Moses pr"
2007.mtsummit-papers.15,W06-3110,0,0.460989,"t contains better translations than the best one found by the decoder. The aim of the additional feature functions is indeed to reward better translations found among the N-best entries of the decoder. The reason for applying two steps instead of one is that not all available feature functions can be efficiently implemented into the decoder. In fact, not all of them can be decomposed into local scores that can be computed on partial translation hypotheses. Moreover, recently feature functions have been proposed that are estimated directly on the N-best list. In particular, (Chen et al., 2005; Zens and Ney, 2006) have recently reported performance improvements by computing posterior probabilities through n-gram language models (LMs) estimated on the N-best translations. This paper proposes an intermediate step in the chain. Before applying re-scoring, the N-best list is further expanded by applying a generative statistical n-gram LM, estimated on the N-best list itself. In particular, the LM is used to generate M new and different target strings, that do not occur in the N-best list. We applied this technique to a well performing baseline for Chinese-to-English translation, trained under the largedata"
2008.iwslt-evaluation.6,E06-1005,0,0.054695,"system combination methods that we have explored. The performance on development and test sets are reported in detail in the paper. The system has shown competitive performance with respect to the BLEU and METEOR measures in Chinese-English Challenge and BTEC tasks. 1. Introduction This paper describes the machine translation (MT) system and approach explored by the Institute for Infocomm Research (I2R) for the International Workshop on Spoken Language Translation (IWSLT) 2008. We submitted runs under the open data conditions for Chinese-to-English BTEC and Challenge tasks. System combination [1, 2, 3] has demonstrated its advantage in the recent machine translation evaluation campaign [4, 5]. In our system, a multi-pass SMT approach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders"
2008.iwslt-evaluation.6,N07-1029,0,0.0312943,"system combination methods that we have explored. The performance on development and test sets are reported in detail in the paper. The system has shown competitive performance with respect to the BLEU and METEOR measures in Chinese-English Challenge and BTEC tasks. 1. Introduction This paper describes the machine translation (MT) system and approach explored by the Institute for Infocomm Research (I2R) for the International Workshop on Spoken Language Translation (IWSLT) 2008. We submitted runs under the open data conditions for Chinese-to-English BTEC and Challenge tasks. System combination [1, 2, 3] has demonstrated its advantage in the recent machine translation evaluation campaign [4, 5]. In our system, a multi-pass SMT approach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders"
2008.iwslt-evaluation.6,P07-2045,0,0.0105658,"ed its advantage in the recent machine translation evaluation campaign [4, 5]. In our system, a multi-pass SMT approach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders, namely Moses [6] (an open source phrasebased MT system), JosHUa [7] (a hierarchical phrase-based translation system) and Tranyu [8] (an in-house linguistically-annotated BTG-based decoder) to generate 2Nbest lists of hypotheses for each decoder. Then each 2N-best lists are rescored and re-ranked with additional feature functions. The 1-best and top N-best lists of these re-ranked lists are then used in system combination1. Secondly, we construct system combination in two-stage. In the first stage, two strategies are applied. The first strategy is n-gram expansion by which we spawn new translation entries thro"
2008.iwslt-evaluation.6,W08-0402,0,0.0242585,"evaluation campaign [4, 5]. In our system, a multi-pass SMT approach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders, namely Moses [6] (an open source phrasebased MT system), JosHUa [7] (a hierarchical phrase-based translation system) and Tranyu [8] (an in-house linguistically-annotated BTG-based decoder) to generate 2Nbest lists of hypotheses for each decoder. Then each 2N-best lists are rescored and re-ranked with additional feature functions. The 1-best and top N-best lists of these re-ranked lists are then used in system combination1. Secondly, we construct system combination in two-stage. In the first stage, two strategies are applied. The first strategy is n-gram expansion by which we spawn new translation entries through a word-based n-gram language model estimated on"
2008.iwslt-evaluation.6,P06-1066,1,0.866553,"oach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders, namely Moses [6] (an open source phrasebased MT system), JosHUa [7] (a hierarchical phrase-based translation system) and Tranyu [8] (an in-house linguistically-annotated BTG-based decoder) to generate 2Nbest lists of hypotheses for each decoder. Then each 2N-best lists are rescored and re-ranked with additional feature functions. The 1-best and top N-best lists of these re-ranked lists are then used in system combination1. Secondly, we construct system combination in two-stage. In the first stage, two strategies are applied. The first strategy is n-gram expansion by which we spawn new translation entries through a word-based n-gram language model estimated on the input hypotheses. Then input hypotheses and the newly-gener"
2008.iwslt-evaluation.6,J03-1002,0,0.00526627,"Section 3 details the rescoring models. Section 4 describes three system combination strategies: n-gram expansion, simple cascading and weighted voting. Section 5 reports the experimental setups and results while Section 6 concludes the paper. 2. The SMT Models To integrate the advantages of the state-of-the-art translation methods, we use three different SMT models, phrase-based, hierarchical phrase-based and linguistically-annotated BTGbased in the first pass to generate N-best hypotheses. The three methods share the some common features: word alignment of training data obtained from GIZA++ [9], Language model(s) (LM) trained using SRILM toolkit [10] with modified Kneser-Ney smoothing method [11]. 2.1. Phrasal translation system Phrase-based SMT systems are usually modeled through a log-linear framework [12]. By introducing the hidden word alignment variable a [13], the optimal translation can be searched for based on the following criterion: M e * = arg max(∑ m =1 λm hm (e, f , a)) where e is a string of phrases in the target language, the source language string of phrases, feature functions, weights 1 Since our system combination method n-gram expansion [3] is based on a gener"
2008.iwslt-evaluation.6,P02-1038,0,0.0901849,"ion 6 concludes the paper. 2. The SMT Models To integrate the advantages of the state-of-the-art translation methods, we use three different SMT models, phrase-based, hierarchical phrase-based and linguistically-annotated BTGbased in the first pass to generate N-best hypotheses. The three methods share the some common features: word alignment of training data obtained from GIZA++ [9], Language model(s) (LM) trained using SRILM toolkit [10] with modified Kneser-Ney smoothing method [11]. 2.1. Phrasal translation system Phrase-based SMT systems are usually modeled through a log-linear framework [12]. By introducing the hidden word alignment variable a [13], the optimal translation can be searched for based on the following criterion: M e * = arg max(∑ m =1 λm hm (e, f , a)) where e is a string of phrases in the target language, the source language string of phrases, feature functions, weights 1 Since our system combination method n-gram expansion [3] is based on a generative language model that is trained on the input hypotheses lists, the hypotheses quality is very important to the performance of the n-gram expansion method. Therefore, we filter out N-worse hypotheses from the 2N-be"
2008.iwslt-evaluation.6,P03-1021,0,0.044622,"m =1 λm hm (e, f , a)) where e is a string of phrases in the target language, the source language string of phrases, feature functions, weights 1 Since our system combination method n-gram expansion [3] is based on a generative language model that is trained on the input hypotheses lists, the hypotheses quality is very important to the performance of the n-gram expansion method. Therefore, we filter out N-worse hypotheses from the 2N-best lists before passing them to the n-gram expansion model. (1) e ,a λm hm (e, f , a ) f is are are typically optimized to maximize the scoring function [14]. Our phrasal translation system is based on the Moses open source package [6]. IBM word reordering constraints [15] are applied during decoding to reduce the computational - 46 - Proceedings of IWSLT 2008, Hawaii - U.S.A. Figure 1: system architecture complexity. The other models and feature functions employed by Moses decoder are: • Translation model(s) (TM), direct phrase/word based translation model and inverse • Distortion model, which assigns a cost linear to the reordering distance, the cost is based on the number of source words which are skipped when translating a new source phrase •"
2008.iwslt-evaluation.6,2005.iwslt-1.8,0,0.0381389,"s based on the Moses open source package [6]. IBM word reordering constraints [15] are applied during decoding to reduce the computational - 46 - Proceedings of IWSLT 2008, Hawaii - U.S.A. Figure 1: system architecture complexity. The other models and feature functions employed by Moses decoder are: • Translation model(s) (TM), direct phrase/word based translation model and inverse • Distortion model, which assigns a cost linear to the reordering distance, the cost is based on the number of source words which are skipped when translating a new source phrase • Lexicalized word reordering model [16] (RM) • Word and phrase penalties, which count the numbers of words and phrases in the target string The translation model, reordering model and feature weights are trained and optimized using Moses training and tuning toolkits. Two different N-best lists are generated by the same Moses decoder with the same source input but different preprocessing. 2.2. Hierarchical phrase-based translation system Hierarchical phrase-based translation method is a typical formally syntax-based translation modeling method. Empirically, it has demonstrated better performance than the phrase-based method because"
2008.iwslt-evaluation.6,J07-2003,0,0.103377,"ses in the target string The translation model, reordering model and feature weights are trained and optimized using Moses training and tuning toolkits. Two different N-best lists are generated by the same Moses decoder with the same source input but different preprocessing. 2.2. Hierarchical phrase-based translation system Hierarchical phrase-based translation method is a typical formally syntax-based translation modeling method. Empirically, it has demonstrated better performance than the phrase-based method because it permits phrases with gaps by generalizing the normal phrase-based models [17, 7]. Formally, the hierarchical phrase-based translation model is a weighted synchronous context free grammar. In our system combination framework, for the hierarchical phrase-based translation component, we use the default setting as discussed in [17] for training and tuning and use JosHUa [7]’s implementation for decoding. 2.3. Linguistically annotated BTG-based system Tranyu is an in-house formally and linguistically syntaxbased SMT system, which adopts the bracketing transduction grammars (BTG) as the fundamental framework for phrase translation and reordering. The BTG lexical rules (A --&gt; x/"
2008.iwslt-evaluation.6,P08-2038,1,0.868481,"el uses boundary words of neighboring phrases as features [8], which we call the boundary words based reordering model (BWR). The second model uses linguistic annotations of each BTG node as features, which are automatically learned by projecting source-side parse trees onto the corresponding binary trees generated by BTG. We call the second model the linguistically annotated reordering model (LAR). Based on these two reordering models, we developed two variations of Tranyu. The first variation Tranyu1 only uses the BWR model [8] while the second variation Tranyu2 uses both BWR and LAR models [18]. 3. Rescoring models Rescoring operation plays a very important role in our system. A rich global feature functions set benefits our system greatly. The rescoring models are the same ones which were used in our SMT system for IWSLT 2007 [4]. We apply the - 47 - Proceedings of IWSLT 2008, Hawaii - U.S.A. following feature functions. Weights of feature functions are optimized by the MERT tool in Moses package. • direct and inverse IBM model 1 and 3 and two combined systems. The feature weight of each system is tuned over the development set. If all the weights are set to 1, then we call it simp"
2008.iwslt-evaluation.6,2006.iwslt-papers.4,1,0.869059,"system greatly. The rescoring models are the same ones which were used in our SMT system for IWSLT 2007 [4]. We apply the - 47 - Proceedings of IWSLT 2008, Hawaii - U.S.A. following feature functions. Weights of feature functions are optimized by the MERT tool in Moses package. • direct and inverse IBM model 1 and 3 and two combined systems. The feature weight of each system is tuned over the development set. If all the weights are set to 1, then we call it simple voting. • association scores, i.e. hyper-geometric distribution probabilities and mutual information • lexicalized reordering rule [19] • 6-gram target language model and 8-gram target wordclass based LM, word-classes are clustered by GIZA++ • length ratio between source and target sentence • question feature [20] • Linear sum of n-grams (n=1,2,3,4) relative frequencies within all translations [20] • n-gram and sentence length posterior probabilities within the N-best translations [21] 5. Experiments We participated Chinese-to-English BTEC task (BT) and Challenge task (CT) in open data track for IWSLT 2008. 5.1. Preprocessing Preprocessing includes Chinese word segmentation, English tokenization, and transformation of numbers"
2008.iwslt-evaluation.6,J93-2003,0,\N,Missing
2008.iwslt-evaluation.6,C08-1014,1,\N,Missing
2008.iwslt-evaluation.6,2007.iwslt-1.8,1,\N,Missing
2008.iwslt-evaluation.6,P02-1040,0,\N,Missing
2008.iwslt-evaluation.6,W06-3110,0,\N,Missing
2008.iwslt-evaluation.6,takezawa-etal-2002-toward,0,\N,Missing
2008.iwslt-evaluation.6,2005.iwslt-1.11,1,\N,Missing
2008.iwslt-evaluation.6,W03-1730,1,\N,Missing
2009.mtsummit-papers.2,J93-2003,0,0.011587,",  is the frequency of source phrase  and target phrase  linked to each other. Most of the longer phrases are seen only once in the training corpus. Therefore, relative frequencies overestimate the phrase translation probabilities. To overcome the overestimation problem, lexical weights are introduced. There are various approaches for estimating the lexical weights. The first approach is introSMT Baseline Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable  (Brown et al., 1993). M ~ ~ e * = arg max(∑m=1 λm H m (~ e , f , a) ) e, a (1) ~ H m (~ e , f , a) are feature functions, and weights λm are typically optimized to maximize the scoring function (Och, 2003). The SMT system we applied in our experiments is Portage, a state-of-the-art phrase-based translation system (Ueffing et al., 2007). The models and feature functions which are employed by the decoder are: • Phrase translation model(s) estimated on the word alignments generated using IBM model 2 and HMMs (Vogel et al., 1996), with standard phrase conditional translation probabilities and lexical weights being em"
2009.mtsummit-papers.2,W05-0801,0,0.014402,"occur; otherwise 0. ability assuming a binomial distribution. Without simplification, this probability can be expressed by: &quot;BC 3,   D 8 81  ED E 8,  81,  ? F G 8  (9) However, because this probability is very small, we follow (Johnson et al., 2007) in computing the negative of the natural logs of the p-value associated with the hypergeometric distribution as the feature functions: HBC ,   0log ∑L &quot;  (10) ,8,  BC Therefore, the higher the value of HBC ,  , the stronger the association between phrase and phrase  . 4) Link probability (Moore, 2005) is the conditional probability of the phrase pair being linked given that they co-occur in a given sentence pair. It is estimated as:   ,  :&quot;,   8 ,  (11) where ,   is the number of times that and  are linked in sentence pairs. 3.3 Phrase-table smoothing As (Foster et al., 2006) shows, the phrase table can be improved by applying smoothing techniques. A motivation for this is our observation that the phrase pairs which co-occur only once: 3,  1 are amazingly frequent in the phrase table even when the training corpus is very large. To compensate for this ov"
2009.mtsummit-papers.2,P03-1021,0,0.00640825,"overestimate the phrase translation probabilities. To overcome the overestimation problem, lexical weights are introduced. There are various approaches for estimating the lexical weights. The first approach is introSMT Baseline Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable  (Brown et al., 1993). M ~ ~ e * = arg max(∑m=1 λm H m (~ e , f , a) ) e, a (1) ~ H m (~ e , f , a) are feature functions, and weights λm are typically optimized to maximize the scoring function (Och, 2003). The SMT system we applied in our experiments is Portage, a state-of-the-art phrase-based translation system (Ueffing et al., 2007). The models and feature functions which are employed by the decoder are: • Phrase translation model(s) estimated on the word alignments generated using IBM model 2 and HMMs (Vogel et al., 1996), with standard phrase conditional translation probabilities and lexical weights being employed; • Distortion model, which assigns a penalty based on the number of source words which are skipped when generating a new target phrase; • Language model(s) trained using SRILM to"
2009.mtsummit-papers.2,P02-1038,0,0.0597455,"hrases in the target language,  is the source language string, 2 where ,  is the frequency of source phrase  and target phrase  linked to each other. Most of the longer phrases are seen only once in the training corpus. Therefore, relative frequencies overestimate the phrase translation probabilities. To overcome the overestimation problem, lexical weights are introduced. There are various approaches for estimating the lexical weights. The first approach is introSMT Baseline Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable  (Brown et al., 1993). M ~ ~ e * = arg max(∑m=1 λm H m (~ e , f , a) ) e, a (1) ~ H m (~ e , f , a) are feature functions, and weights λm are typically optimized to maximize the scoring function (Och, 2003). The SMT system we applied in our experiments is Portage, a state-of-the-art phrase-based translation system (Ueffing et al., 2007). The models and feature functions which are employed by the decoder are: • Phrase translation model(s) estimated on the word alignments generated using IBM model 2 and HMMs (Vogel et al., 1996), with standard"
2009.mtsummit-papers.2,J03-1002,0,0.011046,"Missing"
2009.mtsummit-papers.2,P02-1040,0,0.0760108,"Missing"
2009.mtsummit-papers.2,P96-1041,0,0.128888,"Missing"
2009.mtsummit-papers.2,P08-1010,0,0.0264371,"Missing"
2009.mtsummit-papers.2,J96-1001,0,0.197505,"Missing"
2009.mtsummit-papers.2,J93-1003,0,0.216972,"Missing"
2009.mtsummit-papers.2,P06-1091,0,0.025513,"Missing"
2009.mtsummit-papers.2,W06-1607,1,0.856654,"Missing"
2009.mtsummit-papers.2,D07-1103,1,0.848548,"imes among N sentencepairs by chance, given the marginal frequencies 3 and 3 . We computed this prob1 There are at least three ways to count the number of co-occurrence of  and  , if one or both of them have more than one occurrence in a given sentence pair (Melamed, 1998). We choose to count the cooccurrence as 1 if they both occur; otherwise 0. ability assuming a binomial distribution. Without simplification, this probability can be expressed by: &quot;BC 3,   D 8 81  ED E 8,  81,  ? F G 8  (9) However, because this probability is very small, we follow (Johnson et al., 2007) in computing the negative of the natural logs of the p-value associated with the hypergeometric distribution as the feature functions: HBC ,   0log ∑L &quot;  (10) ,8,  BC Therefore, the higher the value of HBC ,  , the stronger the association between phrase and phrase  . 4) Link probability (Moore, 2005) is the conditional probability of the phrase pair being linked given that they co-occur in a given sentence pair. It is estimated as:   ,  :&quot;,   8 ,  (11) where ,   is the number of times that and  are linked in sentence pairs. 3.3 Phra"
2009.mtsummit-papers.2,N03-1017,0,0.0126144,"the features to predict the usefulness of a new association feature given the existing features. 1 Introduction Phrase-based translation is one of the dominant current approaches to statistical machine translation (SMT). A phrase translation model, incorporated in a data structure known as a phrase table, is the most important component of a phrasebased SMT system, since the translations are generated by concatenating target phrases stored in the phrase table. The pairs of source and corresponding target phrases are extracted from the word-aligned bilingual training corpus (Och and Ney, 2003; Koehn et al., 2003). These phrase pairs together with useful feature functions, are called collectively a phrase translation model or phrase table. A phrase translation model embedded in a state-of-the-art phrase-based SMT system normally exploits the feature functions involving conditional translation probabilities and lexical weights (Koehn et al., 2003). The phrase-based conditional translation probabilities are estimated from the relative frequencies of the source and target phrase in a given phrase pair. To avoid over-training, lexical weights are used to validate the quality of a phrase translation pair. T"
2009.mtsummit-papers.2,P06-1096,0,0.03176,"Missing"
2009.mtsummit-papers.2,W04-3243,0,0.0167998,"into account. The statistics can be organized in a contingency table, e.g. in Table 1. When collecting the statistics of the data, we only need to count 3,  , 3 , 3  and N; the other counts could be easily calculated accordingly. Then, we may compute the following association features: 1) Dice coefficient (Dice, 1945) as in Equation (7). It compares the co-occurrence count of phrase pair and  with the sum of the independent occurrence counts of and  . 45,  678,  8 98  (7) 2) Log-likelihood-ratio (Dunning, 1993) as in Equation (8) which is presented by Moore (2004). ::; ,  ?78?, ? 3? ,  ? =>   ∑  ?)&,1+  ?)& ,1 + 8?78 ? ∑ (8) where ? and  ? are variables ranging over  1A and & , 1 + respectively, the values @,  3 ? ,  ? is the joint count for the values of ? and  ?, 3?  and 3 ?  are the frequencies of values of ? and  ?. 3) Hyper-geometric distribution is the probability of the phrase-pair globally cooccurring 3,   times among N sentencepairs by chance, given the marginal frequencies 3 and 3 . We computed this prob1 There are at least three ways to count the number of co"
2009.mtsummit-papers.2,C96-2141,0,0.433628,"inear framework (Och and Ney, 2002) by introducing the hidden word alignment variable  (Brown et al., 1993). M ~ ~ e * = arg max(∑m=1 λm H m (~ e , f , a) ) e, a (1) ~ H m (~ e , f , a) are feature functions, and weights λm are typically optimized to maximize the scoring function (Och, 2003). The SMT system we applied in our experiments is Portage, a state-of-the-art phrase-based translation system (Ueffing et al., 2007). The models and feature functions which are employed by the decoder are: • Phrase translation model(s) estimated on the word alignments generated using IBM model 2 and HMMs (Vogel et al., 1996), with standard phrase conditional translation probabilities and lexical weights being employed; • Distortion model, which assigns a penalty based on the number of source words which are skipped when generating a new target phrase; • Language model(s) trained using SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing method (Chen et al, 1996); • Word/phrase penalties. 3 3.1 Phrase Translation Model with Association-based Features Traditional phrase-table features A typical phrase translation model exploits features estimating phrase conditional translation probabilities and lexical"
2009.mtsummit-papers.2,2003.mtsummit-papers.53,0,0.0461404,"Missing"
2009.mtsummit-papers.2,N04-1033,0,0.0438114,"Missing"
2009.mtsummit-papers.2,W04-3227,0,0.0445291,"Missing"
2011.iwslt-evaluation.19,P07-1004,0,0.0497399,"Missing"
2011.iwslt-evaluation.19,D08-1076,0,0.0846177,"Missing"
2011.iwslt-evaluation.19,D07-1103,1,0.763628,"Missing"
2011.iwslt-evaluation.19,N03-1017,0,0.0718706,"Missing"
2011.iwslt-evaluation.19,C10-1069,1,0.846539,"Missing"
2011.iwslt-evaluation.19,P05-1074,0,0.0938211,"Missing"
2011.iwslt-evaluation.19,N06-1003,0,0.100229,"Missing"
2011.iwslt-evaluation.19,D10-1064,0,\N,Missing
2011.iwslt-evaluation.19,J93-2003,0,\N,Missing
2011.iwslt-evaluation.19,C96-2141,0,\N,Missing
2011.iwslt-evaluation.19,D09-1040,0,\N,Missing
2011.iwslt-evaluation.19,P02-1040,0,\N,Missing
2011.iwslt-evaluation.19,W06-1607,1,\N,Missing
2011.iwslt-evaluation.19,N04-1033,0,\N,Missing
2011.mtsummit-papers.30,J93-2003,0,0.05186,"SRF = Ȝ1 ×log[c(s,t)]+Ȝ2 ×log[c(s)]+Ȝ3×log[c(t)]? (7) MERT can still choose λ’s that are equivalent to using the two RF estimates. In principle, nothing is lost and a degree of freedom, achieved by dropping the additive constraint, is gained (but this extra degree of freedom may lead to search errors). The obvious objection is that this “3-count” replacement for the two RF features doesn’t model probabilities. However, the inclusion of PRF(t|s) among features can’t be justified probabilistically either. Originally, the objective function for SMT was derived via Bayes’s Theorem as P(s|t)×P(t) (Brown et al., 1993). The inclusion of P(t|s) happened later – it’s a heuristic that defies Bayes’s Theorem (Och and Ney, 2002). Once the forward and backward estimates have been unpacked into their three constituent counts, these counts can be transformed and generalized slightly by adding or subtracting constants (while ensuring the logarithm is defined). Therefore, we have two different log-linear feature sets obtained from the three basic statistics: 3-count: {log[c(s,t)], log[c(s)], log[c(t)]} Generalized 3-count: {log[c(s,t)+k1], log[c(s)+k2], log[c(t)+k3]}; where k1, k2, k3&gt; -11. “Generalized 3-count” is r"
2011.mtsummit-papers.30,D08-1076,0,0.0305775,"tice MERT details We evaluated several new and several known techniques with our in-house phrase-based SMT system, whose decoder resembles Moses (Koehn et al., 2007). In addition to phrase count features, all systems had forward and backward lexical probabilities, of the type described in (Zens and Ney, 2004), and lexicalized and distance-based distortion models. The LW estimates employed in our experiments are based on (Zens and Ney, 2004); Foster et al. (2006) found this to be the most effective lexical smoothing technique. Weights on the feature functions are found by lattice MERT (LMERT) (Macherey et al., 2008). These authors pruned the lattices output by their decoder; they also aggregated lattices over iterations (clarified via personal communication with W. Macherey). By contrast, an earlier version of LMERT employed by our group (Larkin et al. 2010) did not involve pruning or aggregation. Initially, we followed the Larkin et al. algorithm: this provides rapid convergence to reasonable optima. However, we decided that some aggregation should be tried to discourage random walk behaviour. In experiments for this paper, we found that without lattice aggregation, adding features led to worse optima o"
2011.mtsummit-papers.30,P03-1021,0,0.0244482,") ≤ τ ; N [c ( s , t ) ≤ τ ] = ® ¯0 otherwise. (1) (2) c ( s, t ) − D + α (t ) × Pb ( s ) c(t ) (6) This feature “punishes” low-frequency phrase pairs. In the paper (Mauser et al., 2007), three different low-frequency features were used, with the three values of τ lying in the interval between 0.9 and 3.0 (the system in the paper allows fractional values of c(s,t)). 3 Unpacking and Transforming Feature Functions Taking just the two RF features from Equation (3), we have: SRF = Ȝ1×log[PRF(t|s)]+ Ȝ2 ×log[PRF(s|t)] The λ’s are often estimated by the minimum error rate training (MERT) algorithm (Och, 2003). For the following two, the implementation details are as in (Foster et al., 2006). Good-Turing: observed counts c are modified according to the formula (Church and Gale, 1991): cg = (c + 1) nc +1 / nc (4) 270 = Ȝ1×(log[c(s,t)]–log[c(s)])+Ȝ2×(log[c(s,t)]–log[c(t)]) = (Ȝ1+Ȝ2)× log[c(s,t)]–Ȝ1×log[c(s)]–Ȝ2×log[c(t)]. This is a combination of three terms, with an additive constraint. Wouldn’t it be simpler to fit the following expression: SRF = Ȝ1 ×log[c(s,t)]+Ȝ2 ×log[c(s)]+Ȝ3×log[c(t)]? (7) MERT can still choose λ’s that are equivalent to using the two RF estimates. In principle, nothing is lost"
2011.mtsummit-papers.30,P02-1038,0,0.650975,"e frequency (RF) estimates of “forward” probability PRF(t|s) and “backward” probability PRF(s|t) are c ( s, t ) PRF (t |s) = c ( s) c ( s, t ) PRF ( s |t ) = c (t ) where cg is a modified count replacing c in subsequent RF estimates, and nc is the number of events having count c. Kneser-Ney (modified): an absolute discounting variant with PKN (s |t ) = These RF estimates are often combined with “lexical weighting” (LW) estimates of the same probabilities PLW(t|s) and PLW(s|t), based on cooccurrence counts of the individual words making up s and t. Thus, the TM score is typically of this form (Och and Ney, 2002): STM = Ȝ1×log[PRF(t|s)]+ Ȝ2×log[PRF(s|t)]+ (3) Ȝ3 ×log[PLW(t|s)]+Ȝ4×log[PLW(s|t)]. (5) where α (t ) = D × n1+ (*, t ) / c(t ) , and Pb ( s ) = n1+ ( s,*) / ¦s n1+ (s,*) . Here, n1+(*,t) is the number of unique sourcelanguage phrases t is aligned with; n1+(s,*) has an analogous definition. PKN(t|s) is defined symmetrically. Kneser-Ney gives a bonus to phrase pairs (s,t) such that s and t have been aligned to many different phrases. Modified Kneser-Ney defines different discounts D depending on the value of c(s,t). We used “KN3”, where D1 is used when c(s,t) = 1, D2 when c(s,t) = 2, and D3 when"
2011.mtsummit-papers.30,J03-1002,0,0.00498293,"is solves the memory and speed problems and provides better performance. 4.2 Results were obtained for Chinese-to-English (CE), and French-to-English (FE). There were two CE data conditions. The first is the small data condition where only the FBIS 3 corpus (10.5M target words) is used to train the translation model. For this condition, we built the phrase table using two phrase extractors: the inhouse one extracts phrase pairs from merged counts of symmetrized IBM model 2 (Brown et al., 1993) and HMM (Vogel et al., 1996) word alignments, while the other one extracts phrase pairs from GIZA++ (Och and Ney 2003) IBM model 4 word alignments (in all other experiments, we only used the in-house extractor). The second is the large data condition where the pa3 272 Data LDC2003E14 rallel training data are from the NIST4 2009 CE evaluation (112.6M target words). We used the same two language models (LMs) for both CE conditions: a 5-gram LM trained on the target side of the large data corpus, and a 6-gram LM trained on the English Gigaword v4 corpus. We used the same development and test sets for the two CE data conditions. The development set comprises mainly data from the NIST 2005 test set, and also some"
2011.mtsummit-papers.30,D07-1103,1,0.912224,"Missing"
2011.mtsummit-papers.30,N03-1017,0,0.0136316,"owing. Section 2 will introduce some existing smoothing techniques. In section 3, we will unpack and transform the two RF feature functions and Kneser-Ney phrase table smoothing. Section 4 is the experiments and discussion. Section 5 ends the paper with conclusion and future work. 2 Existing Smoothing Techniques The phrase table consists of conditional probabilities of co-occurrence for source-language phrases s and target-language phrases t. “Relative frequency” (RF) estimates for these probabilities are obtained from a phrase pair extraction procedure applied to a bilingual training corpus (Koehn et al., 2003). Let c(s) be the count of a source phrase s, c(t) the count of a target phrase t, and c(s,t) the number of times s and t are aligned to form a phrase pair. Relative frequency (RF) estimates of “forward” probability PRF(t|s) and “backward” probability PRF(s|t) are c ( s, t ) PRF (t |s) = c ( s) c ( s, t ) PRF ( s |t ) = c (t ) where cg is a modified count replacing c in subsequent RF estimates, and nc is the number of events having count c. Kneser-Ney (modified): an absolute discounting variant with PKN (s |t ) = These RF estimates are often combined with “lexical weighting” (LW) estimates of"
2011.mtsummit-papers.30,W04-3250,0,0.109273,"e abbreviation for each system, we give in brackets 4 http://www.nist.gov/speech/tests/mt (http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_Const rainedResources.pdf provides the list of resources from which large data was drawn). 5 http://www.statmt.org/wmt10/ 273 the number of log-linear plus other weights that must be tuned for the non-lexical phrase count component of each: e.g., KN3 has two probability estimates, with associated log-linear weights λ1 and λ2 tuned by MERT, and three discounts D1, D2, and D3 (shared by forward and backward probabilities), giving (2+3) weights. Following (Koehn, 2004), we use the bootstrap-resampling test to do significance testing. In Table 1-3, Symbols ** or * indicates that the result is significant better than the baseline at level p&lt;0.01 or p&lt;0.05 respectively. In-house extractor system (#wts) RF (2+0) 3CT (3+0) RF+LF3 (5+0) GT (2+0) RF+ELF (3+0) Gen3CT (3+3) Gen3CT+2EN (5+3) Gen3CT+Gen2EN (5+5) KN3 (2+3) KN3+ELF (3+3) NIST test 2006 2008 29.85 23.57 29.48 23.24 29.87 23.60 29.91 23.61 30.46** 24.16** 30.21 23.62 30.47** 23.90* 30.76** 24.36** 27.56 +0.85 30.91** 24.53** 27.72 +1.01 GIZA++ extractor system (#wts) RF (2+0) 3CT (3+0) RF+LF3 (5+0) GT (2+"
2011.mtsummit-papers.30,W10-1717,1,0.881935,"Missing"
2013.mtsummit-papers.23,D11-1033,0,0.0819155,"to automate the identification of coherent sub-corpora. (Aminzadeh et al., 2012) examine the interaction between adaptation and MAP smoothing, and compare different combining techniques. Other ways of combining various component models have also been proposed, including “fill-up” with a predefined preference order (Bisazza et al., 2011), using multiple decoder paths (Koehn and Schroeder, 2007), and ensemble combination of complete models in the decoder (Razmara et al., 2012). Mixture approaches have also been used for LM adaptation (Foster and Kuhn, 2007). Finally, data selection approaches (Axelrod et al., 2011) can be seen as an extreme form of mixture modeling in which weights are either 0 or 1. 5 Conclusion Our main aim in this paper has been to investigate whether SMT features inspired by various prototypical linear mixtures can be used to simulate discriminative training of linear mixture models for TM adaptation. Because of the fact that linear mixtures occur within the log-probabilities assigned to phrase pairs, they are not accessible to standard tuning algorithms without non-trivial modification, and hence are usually trained nondiscriminatively for maximum likelihood. Our simulation is loos"
2013.mtsummit-papers.23,2011.iwslt-evaluation.18,0,0.0330396,"have been refined by (Sennrich, 2012b), who applied them to multiple sub-corpora and investigated alternative techniques such as count weighting to capture the amount of subcorpus-internal evidence for each phrase pair. Sennrich (2012a) reports interesting results using clustering to automate the identification of coherent sub-corpora. (Aminzadeh et al., 2012) examine the interaction between adaptation and MAP smoothing, and compare different combining techniques. Other ways of combining various component models have also been proposed, including “fill-up” with a predefined preference order (Bisazza et al., 2011), using multiple decoder paths (Koehn and Schroeder, 2007), and ensemble combination of complete models in the decoder (Razmara et al., 2012). Mixture approaches have also been used for LM adaptation (Foster and Kuhn, 2007). Finally, data selection approaches (Axelrod et al., 2011) can be seen as an extreme form of mixture modeling in which weights are either 0 or 1. 5 Conclusion Our main aim in this paper has been to investigate whether SMT features inspired by various prototypical linear mixtures can be used to simulate discriminative training of linear mixture models for TM adaptation. Beca"
2013.mtsummit-papers.23,2011.mtsummit-papers.30,1,0.785655,"bc gale bn gale nw gale wl hkh hkl hkn isi ne lex others nw fbis sinorama unv2 PT size (# pairs) 4.4M 3.6M 4.6M 1.8M 1.4M 62.2M 8.2M 26.6M 26.1M 1.9M 7.8M 19.6M 17.0M 360.1M weights mixtm-ml mixtm-samp 0.033 0.142 0.022 0.091 0.059 0.126 0.082 0.279 0.017 0.176 0.120 0.008 0.002 0.024 0.035 0.021 0.045 0.011 0.003 0.030 0.050 0.042 0.142 0.026 0.053 0.020 0.341 0.005 Table 4: Comparison of weights assigned to Chinese sub-corpora by standard and sampled ML mixture models. Table 3: Mixture TM Adaptation. ditional phrase pair estimates in both directions were obtained using Kneser-Ney smoothing (Chen et al., 2011), and were weighted (each directional estimate separately) for adaptation. We also used lexical estimates in both directions which were not weighted (no sub-corpus-specific values were available). Additional features included a hierarchical lexical reordering model (Galley and Manning, 2008) (6 features), standard distortion and word penalties (2 features), a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM (14 features total). The decoder used a distortion limit of 7, and at most 30 translations for each source phrase. The system was tuned with batch"
2013.mtsummit-papers.23,N13-1114,1,0.88535,"Missing"
2013.mtsummit-papers.23,N12-1047,1,0.842261,"hted (each directional estimate separately) for adaptation. We also used lexical estimates in both directions which were not weighted (no sub-corpus-specific values were available). Additional features included a hierarchical lexical reordering model (Galley and Manning, 2008) (6 features), standard distortion and word penalties (2 features), a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM (14 features total). The decoder used a distortion limit of 7, and at most 30 translations for each source phrase. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 corpus Results Our first experiment compares a non-adapted baseline, trained on all available corpora, with standard maximum likelihood (ML) mixture TM adaptation as described in section 2.1 and the sampled variant described in section 2.2. The results are presented in table 3, in the form of BLEU scores averaged over both test sets for each language pair. ML mix187 ture adaptation (mixtm-ml) yields significant gains of 0.6 and 0.9 for Arabic and Chinese over the unadapted model. However, in our setting, this underperforms assigning uniform weights to all component models (mixtm-uni). Eq"
2013.mtsummit-papers.23,D08-1024,0,0.0797579,"Missing"
2013.mtsummit-papers.23,W07-0717,1,0.958969,"ues intended to address this problem have attracted significant research attention in recent years. One simple and popular approach is mixture adaptation, in which models trained on various sub-corpora are weighted according to their relevance for the current domain. Mixture adaptation is most effective when there are many heterogeneous sub-corpora, at least some of which are not too different from the test domain.1 It typically requires only a small in-domain sample—a development or test set—in order to obtain weights, and has been shown to work well with all major SMT model types: language (Foster and Kuhn, 2007), translation (Koehn and Schroeder, 2007; Foster et al., 2010; Sennrich, 2012b), and reordering (Chen et al., 2013). There are two main types of mixture model: linear mixtures, which combine weighted probabilities from component models additively; and loglinear mixtures, which combine multiplicatively. Linear mixtures frequently perform better (Foster et al., 2010), especially when there are a relatively large number of sub-corpora, and when the models derived from them are not smooth. This is likely due to the “veto power” that any component model exercises within a log-linear combination: it"
2013.mtsummit-papers.23,D10-1044,1,0.891722,"much better on the dev set than simplex. We conclude that it is very unlikely that a more sophisticated optimizer working with a better objective than raw BLEU would be able to find a set of linear weights that outperform the test results of our features.6 6 As noted above, however, it almost certainly does not accomplish this feat while remaining within the space of linear combinations. 188 4 Related Work Domain adaptation for SMT is currently a very active topic, encompassing a wide variety of approaches. TM linear mixture models of the kind we study here were first proposed by Foster et al (2010), and have been refined by (Sennrich, 2012b), who applied them to multiple sub-corpora and investigated alternative techniques such as count weighting to capture the amount of subcorpus-internal evidence for each phrase pair. Sennrich (2012a) reports interesting results using clustering to automate the identification of coherent sub-corpora. (Aminzadeh et al., 2012) examine the interaction between adaptation and MAP smoothing, and compare different combining techniques. Other ways of combining various component models have also been proposed, including “fill-up” with a predefined preference or"
2013.mtsummit-papers.23,D08-1089,0,0.0891952,"0.035 0.021 0.045 0.011 0.003 0.030 0.050 0.042 0.142 0.026 0.053 0.020 0.341 0.005 Table 4: Comparison of weights assigned to Chinese sub-corpora by standard and sampled ML mixture models. Table 3: Mixture TM Adaptation. ditional phrase pair estimates in both directions were obtained using Kneser-Ney smoothing (Chen et al., 2011), and were weighted (each directional estimate separately) for adaptation. We also used lexical estimates in both directions which were not weighted (no sub-corpus-specific values were available). Additional features included a hierarchical lexical reordering model (Galley and Manning, 2008) (6 features), standard distortion and word penalties (2 features), a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM (14 features total). The decoder used a distortion limit of 7, and at most 30 translations for each source phrase. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 corpus Results Our first experiment compares a non-adapted baseline, trained on all available corpora, with standard maximum likelihood (ML) mixture TM adaptation as described in section 2.1 and the sampled variant described in section 2.2. The re"
2013.mtsummit-papers.23,W12-3154,0,0.053233,"ownweighting them severely, thereby discarding whatever useful information they might possess. Although linear mixtures are attractive for adaptation, they have the potential disadvantage that it is difficult to tune mixture weights directly for an SMT error metric such as BLEU. This is because, in order to allow for decoder factoring, models must be mixed at the local level, ie over ngrams or phrase/rule pairs. Thus the linear mixtures oc1 The assumption that train and test domains are fairly similar is shared by most current work on SMT domain adaptation, which focusses on modifying scores. Haddow and Koehn (2012) show that coverage problems dominate scoring problems when train and test are more distant. Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 183–190. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. cur inside the normal log probabilities that are assigned to these entities, making mixture weights inaccessible to standard tuning algorithms. Notice that log-linear mixtures do not suffer from this problem, since c"
2013.mtsummit-papers.23,W07-0733,0,0.432798,"have attracted significant research attention in recent years. One simple and popular approach is mixture adaptation, in which models trained on various sub-corpora are weighted according to their relevance for the current domain. Mixture adaptation is most effective when there are many heterogeneous sub-corpora, at least some of which are not too different from the test domain.1 It typically requires only a small in-domain sample—a development or test set—in order to obtain weights, and has been shown to work well with all major SMT model types: language (Foster and Kuhn, 2007), translation (Koehn and Schroeder, 2007; Foster et al., 2010; Sennrich, 2012b), and reordering (Chen et al., 2013). There are two main types of mixture model: linear mixtures, which combine weighted probabilities from component models additively; and loglinear mixtures, which combine multiplicatively. Linear mixtures frequently perform better (Foster et al., 2010), especially when there are a relatively large number of sub-corpora, and when the models derived from them are not smooth. This is likely due to the “veto power” that any component model exercises within a log-linear combination: it can suppress hypotheses by assigning th"
2013.mtsummit-papers.23,P07-2045,0,0.00636067,"Missing"
2013.mtsummit-papers.23,P03-1021,0,0.0290162,"solution to the problem of setting linear mixture weights is to sidestep it by optimizing some other criterion, typically dev-set likelihood (Sennrich, 2012b), instead of BLEU. This achieves good empirical results, but leaves open the question of whether tuning linear weights directly for BLEU would work better, as one might naturally expect. This question—which to our knowledge has not yet been answered in the literature—is the one we address in this paper. Modifying standard tuning algorithms to accommodate local linear mixtures presents a challenge that varies with the algorithm. In MERT (Och, 2003) for instance, one could replace exact line maximization within Powell’s algorithm with a general-purpose line optimizer capable of handling local linear mixtures at the same time as the usual log-linear weights. For expected BLEU methods (Rosti et al., 2011), handling local weights would require computing gradient information for them. For MIRA (Chiang et al., 2008), the required modifications are somewhat less obvious. Rather than tackling the problem directly by attempting to modify our tuning algorithm, we opted for a simpler indirect approach in which features are used to simulate a discr"
2013.mtsummit-papers.23,P12-1099,1,0.872569,"ting to capture the amount of subcorpus-internal evidence for each phrase pair. Sennrich (2012a) reports interesting results using clustering to automate the identification of coherent sub-corpora. (Aminzadeh et al., 2012) examine the interaction between adaptation and MAP smoothing, and compare different combining techniques. Other ways of combining various component models have also been proposed, including “fill-up” with a predefined preference order (Bisazza et al., 2011), using multiple decoder paths (Koehn and Schroeder, 2007), and ensemble combination of complete models in the decoder (Razmara et al., 2012). Mixture approaches have also been used for LM adaptation (Foster and Kuhn, 2007). Finally, data selection approaches (Axelrod et al., 2011) can be seen as an extreme form of mixture modeling in which weights are either 0 or 1. 5 Conclusion Our main aim in this paper has been to investigate whether SMT features inspired by various prototypical linear mixtures can be used to simulate discriminative training of linear mixture models for TM adaptation. Because of the fact that linear mixtures occur within the log-probabilities assigned to phrase pairs, they are not accessible to standard tuning"
2013.mtsummit-papers.23,W11-2119,0,0.0207874,"ether tuning linear weights directly for BLEU would work better, as one might naturally expect. This question—which to our knowledge has not yet been answered in the literature—is the one we address in this paper. Modifying standard tuning algorithms to accommodate local linear mixtures presents a challenge that varies with the algorithm. In MERT (Och, 2003) for instance, one could replace exact line maximization within Powell’s algorithm with a general-purpose line optimizer capable of handling local linear mixtures at the same time as the usual log-linear weights. For expected BLEU methods (Rosti et al., 2011), handling local weights would require computing gradient information for them. For MIRA (Chiang et al., 2008), the required modifications are somewhat less obvious. Rather than tackling the problem directly by attempting to modify our tuning algorithm, we opted for a simpler indirect approach in which features are used to simulate a discriminatively-trained linear mixture, relying on the ability of MIRA to handle large feature sets. Our aim in doing so is not to achieve a close mathematical approximation, but rather to use features to capture the kinds of information we expect to be inherent"
2013.mtsummit-papers.23,2012.eamt-1.43,0,0.50844,"recent years. One simple and popular approach is mixture adaptation, in which models trained on various sub-corpora are weighted according to their relevance for the current domain. Mixture adaptation is most effective when there are many heterogeneous sub-corpora, at least some of which are not too different from the test domain.1 It typically requires only a small in-domain sample—a development or test set—in order to obtain weights, and has been shown to work well with all major SMT model types: language (Foster and Kuhn, 2007), translation (Koehn and Schroeder, 2007; Foster et al., 2010; Sennrich, 2012b), and reordering (Chen et al., 2013). There are two main types of mixture model: linear mixtures, which combine weighted probabilities from component models additively; and loglinear mixtures, which combine multiplicatively. Linear mixtures frequently perform better (Foster et al., 2010), especially when there are a relatively large number of sub-corpora, and when the models derived from them are not smooth. This is likely due to the “veto power” that any component model exercises within a log-linear combination: it can suppress hypotheses by assigning them low probabilities. To avoid the co"
2013.mtsummit-papers.23,E12-1055,0,0.566545,"recent years. One simple and popular approach is mixture adaptation, in which models trained on various sub-corpora are weighted according to their relevance for the current domain. Mixture adaptation is most effective when there are many heterogeneous sub-corpora, at least some of which are not too different from the test domain.1 It typically requires only a small in-domain sample—a development or test set—in order to obtain weights, and has been shown to work well with all major SMT model types: language (Foster and Kuhn, 2007), translation (Koehn and Schroeder, 2007; Foster et al., 2010; Sennrich, 2012b), and reordering (Chen et al., 2013). There are two main types of mixture model: linear mixtures, which combine weighted probabilities from component models additively; and loglinear mixtures, which combine multiplicatively. Linear mixtures frequently perform better (Foster et al., 2010), especially when there are a relatively large number of sub-corpora, and when the models derived from them are not smooth. This is likely due to the “veto power” that any component model exercises within a log-linear combination: it can suppress hypotheses by assigning them low probabilities. To avoid the co"
2014.amta-researchers.10,D11-1033,0,0.0469667,"Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. not-include decision about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets line"
2014.amta-researchers.10,W09-0432,0,0.0871166,"e six methods (those other than the original VSM), with improvements for these five all in the range +1.7-2.0 BLEU; 3) combining some of these techniques yields further significant improvement. The best combination yields improvements of +2.6-3.3 BLEU over the baseline. 2 Reviewing of SMT adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. n"
2014.amta-researchers.10,P13-1141,0,0.0260984,"s. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing ne"
2014.amta-researchers.10,N13-1114,1,0.258614,"c-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline. 1 Introduction The translation of a source-language expression to a target language might differ across genres, topics, national origins, and dialects, or the author’s or publication’s style. The word “domain” is often used to indicate a particular combination of all these factors (Chen et al., 2013b). Statistical machine translation (SMT) systems are trained on bilingual parallel and monolingual data. The training data vary across domains, and translations across domains are unreliable. Therefore, we can often get better performance by adapting the SMT system to the test domain. Domain adaptation (DA) techniques for SMT systems have been widely studied. Approaches that have been tried for SMT model adaptation include self-training, data selection, data weighting, context-based DA, and topic-based DA. etc. We will review these techniques in the next Section. Among all these approaches, d"
2014.amta-researchers.10,P13-1126,1,0.392167,"c-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline. 1 Introduction The translation of a source-language expression to a target language might differ across genres, topics, national origins, and dialects, or the author’s or publication’s style. The word “domain” is often used to indicate a particular combination of all these factors (Chen et al., 2013b). Statistical machine translation (SMT) systems are trained on bilingual parallel and monolingual data. The training data vary across domains, and translations across domains are unreliable. Therefore, we can often get better performance by adapting the SMT system to the test domain. Domain adaptation (DA) techniques for SMT systems have been widely studied. Approaches that have been tried for SMT model adaptation include self-training, data selection, data weighting, context-based DA, and topic-based DA. etc. We will review these techniques in the next Section. Among all these approaches, d"
2014.amta-researchers.10,2011.mtsummit-papers.30,1,0.752758,"development and test sets. We use the evaluation sets from NIST 06, 08, and 09 as our development set and two test sets, respectively. All Chinese and Arabic dev and test sets have 4 references. 5.2 System Experiments were carried out with an in-house, state-of-the-art phrase-based system. The whole corpora were first word-aligned using IBM2, HMM, and IBM4 models, then split to subcorpora according to genre and origin; the phrase table was the union of phrase pairs extracted from these alignments, with a length limit of 7. The translation model (TM) was Kneser-Ney smoothed in both directions (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7, lexical weighting in both directions, word count, a distance-based reordering model, a 4-gram language model (LM) trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 5.3 Results This paper has described three modifications to existing techniques: smoothing for log-linear mixtures and two new versions of VSM - grouped VSM and distributional VSM. First, we did Al-Onaizan & Simar"
2014.amta-researchers.10,P08-2040,1,0.834358,"st improvements are for five of the six methods (those other than the original VSM), with improvements for these five all in the range +1.7-2.0 BLEU; 3) combining some of these techniques yields further significant improvement. The best combination yields improvements of +2.6-3.3 BLEU over the baseline. 2 Reviewing of SMT adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data select"
2014.amta-researchers.10,N13-1003,0,0.0205184,"aseline, original VSM adaptation and distributional VSM adaptation. Table 5 compares original VSM and distributional VSM. It shows that the original VSM adaptation reported in (Chen et al., 2013b) does yield improvement over a non-adaptive baseline. However, if instead of computing a domain similarity score, we directly maximize BLEU on the dev set by tuning the weights of distribution features, we got further significant improvements. On the Chinese task, the further improvements were +0.7-0.8 BLEU, while on Arabic, the further improvements were smaller but still significant: +0.3-0.4 BLEU. (Cherry, 2013) showed that in the case of reordering features, directly maximizing BLEU outperforms maximum entropy optimization; the experiments in Table 5 yield a similar conclusion. Given that recently developed tuning algorithms such as MIRA can handle a very large feature set, we may consider having all possible features directly tuned to maximize BLEU (or similar criteria). Now, we compare all six DA techniques, Table 6 reports the results. All techniques improved on all test sets across two language pairs over the non-adaptive baseline, and all these improvements are significant. From the average abs"
2014.amta-researchers.10,N12-1047,1,0.887245,"Missing"
2014.amta-researchers.10,P11-2080,0,0.184624,"or Translation Model Adaptation Boxing Chen Roland Kuhn George Foster Boxing.Chen@nrc-cnrc.gc.ca Roland.Kuhn@nrc-cnrc.gc.ca George.Foster@nrc-cnrc.gc.ca National Research Council Canada, Ottawa, Canada Abstract In this paper, we propose two extensions to the vector space model (VSM) adaptation technique (Chen et al., 2013b) for statistical machine translation (SMT), both of which result in significant improvements. We also systematically compare the VSM techniques to three mixture model adaptation techniques: linear mixture, log-linear mixture (Foster and Kuhn, 2007), and provenance features (Chiang et al., 2011). Experiments on NIST Chinese-to-English and Arabic-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline. 1 Introduction The translation of a source-language expression to a target language might differ across genres, topics, national origins, and dialects, or the author’s or publication’s style. The word “domain” is often used to indica"
2014.amta-researchers.10,D13-1107,0,0.0120428,"better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for each sentence in the training data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phra"
2014.amta-researchers.10,P11-2071,0,0.152169,"Missing"
2014.amta-researchers.10,D12-1025,0,0.0440133,"lations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing new senses in new domains (Daume III and Jagarlamudi, 2011; Irvine et al., 2013). (Dou and Knight, 2012; Zhang and Zong, 2013) learned bilingual lexica or phrase tables from in-domain monolingual data with a decipherment method, then incorporated them into the SMT system. 3 Mixture model adaptation There are several adaptation scenarios for SMT, of which the most common is 1) The training material is heterogeneous, with some parts of it that are not too far from the test domain; 2) A bilingual dev set drawn from the test domain is available. A common approach to DA for this scenario is: 1) split the training data into sub-corpora by domain; 2) train sub-models or features on each sub-corpus; 3)"
2014.amta-researchers.10,P13-2119,0,0.0500397,"can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. not-include decision about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets linearly or log-linearl"
2014.amta-researchers.10,P12-2023,0,0.0197208,"in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing new senses in new domains (Daume III and Jagarlamudi, 2011; Irvine et al., 2013). (Dou and Knight, 2012; Zhang and Zong, 2013) learned bilingual lexica or phrase tables from in-domain monolingual data with a decipherment method, then incorporated them into th"
2014.amta-researchers.10,2012.iwslt-papers.17,0,0.0367148,"Missing"
2014.amta-researchers.10,D10-1044,1,0.962878,"on about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets linearly or log-linearly (Foster and Kuhn, 2007). Log-linear mixture DA can employ the same discriminative tuning algorithm used to combine the log-linear high-level components of a typical SMT system (the translation model, language model, reordering model, etc.), but linear mixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixt"
2014.amta-researchers.10,W07-0717,1,0.867177,"mparison of Mixture and Vector Space Techniques for Translation Model Adaptation Boxing Chen Roland Kuhn George Foster Boxing.Chen@nrc-cnrc.gc.ca Roland.Kuhn@nrc-cnrc.gc.ca George.Foster@nrc-cnrc.gc.ca National Research Council Canada, Ottawa, Canada Abstract In this paper, we propose two extensions to the vector space model (VSM) adaptation technique (Chen et al., 2013b) for statistical machine translation (SMT), both of which result in significant improvements. We also systematically compare the VSM techniques to three mixture model adaptation techniques: linear mixture, log-linear mixture (Foster and Kuhn, 2007), and provenance features (Chiang et al., 2011). Experiments on NIST Chinese-to-English and Arabic-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline. 1 Introduction The translation of a source-language expression to a target language might differ across genres, topics, national origins, and dialects, or the author’s or publication’s s"
2014.amta-researchers.10,W06-1607,1,0.711799,"are then learned under the standard SMT log-linear framework. Experiments (see 5.3) show that this simple smoothing greatly improves the performance of log-linear mixture adaptation. 1 We use the models trained on the whole training data to align the dev set. This can be done with mgiza (http://www.kyloo.net/software/doku.php/mgiza) Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 127 3.3 Provenance features Provenance features (Chiang et al., 2011) are applied to lexical weights. There are slight variations in computing lexical weights (Foster et al., 2006), they all use forward and backward word translation probabilities T (s|t) and T (t|s) estimated from the word-aligned parallel text. The conditional probability for word pair (s, t) in a translation table is computed as below: count(s, t) . s count(si , t) T (s|t) = P (4) i We adopt the approach proposed in (Zens and Ney, 2004) to compute the lexical weights. It assumes that all source words are conditionally independent: plw (s|t) = n Y p(si |t) (5) i=1 and adopts a “noisy-or” combination, so that p(si |t) = 1 − m Y (1 − T (si |tj )) (6) j=1 where n and m are number of source and target word"
2014.amta-researchers.10,D08-1089,0,0.0150038,"as our development set and two test sets, respectively. All Chinese and Arabic dev and test sets have 4 references. 5.2 System Experiments were carried out with an in-house, state-of-the-art phrase-based system. The whole corpora were first word-aligned using IBM2, HMM, and IBM4 models, then split to subcorpora according to genre and origin; the phrase table was the union of phrase pairs extracted from these alignments, with a length limit of 7. The translation model (TM) was Kneser-Ney smoothed in both directions (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7, lexical weighting in both directions, word count, a distance-based reordering model, a 4-gram language model (LM) trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 5.3 Results This paper has described three modifications to existing techniques: smoothing for log-linear mixtures and two new versions of VSM - grouped VSM and distributional VSM. First, we did Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors"
2014.amta-researchers.10,2013.mtsummit-papers.23,1,0.900528,"Missing"
2014.amta-researchers.10,D11-1084,0,0.0212675,"from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which e"
2014.amta-researchers.10,D13-1109,0,0.187192,"Missing"
2014.amta-researchers.10,koen-2004-pharaoh,0,0.0610681,"-lin w/ smooth Chinese-English MT06 MT08 36.0 29.4 35.8 29.0 37.9**++ 31.1**++ Arabic-English MT08 MT09 46.4 49.2 47.7** 50.0** 48.2**+ 50.6**++ Table 3: Results of log-linear mixtures with or without smoothing. */** or +/++ means result is significantly better than baseline or system without smoothing (p < 0.05 or p < 0.01, respectively). experiments to see if these three modifications yield statistically significant improvements over the original techniques. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrap-resampling test for significance testing. We set the λ in Equation 3 and Equation 7 to 0.01 by looking at performance on the Arabic-English dev set. Table 3 shows the results of log-linear mixture model with or without smoothing. Without smoothing, log-linear mixture model DA hurt the performance of Chinese-to-English on both test sets, but gave moderate improvements on Arabic-to-English. The Chinese task has more (14) sub-corpora than the Arabic one, and they seem more diverse; as suggested earlier, the “veto power” of small sub-corpora seems to be particularly harmful"
2014.amta-researchers.10,W07-0733,0,0.0353986,"is can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets linearly or log-linearly (Foster and Kuhn, 2007). Log-linear mixture DA can employ the same discriminative tuning algorithm used to combine the log-linear high-level components of a typical SMT system (the translation model, language model, reordering model, etc.), but linear mixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for ea"
2014.amta-researchers.10,D07-1036,0,0.299343,"Missing"
2014.amta-researchers.10,D09-1074,0,0.268931,"uhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for each sentence in the training data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data"
2014.amta-researchers.10,P10-2041,0,0.0572885,"adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. not-include decision about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different"
2014.amta-researchers.10,P02-1040,0,0.100025,"=broadcast conversation, bn=broadcast news, ng=newsgroup, wl=weblog. baseline log-lin w/o smooth log-lin w/ smooth Chinese-English MT06 MT08 36.0 29.4 35.8 29.0 37.9**++ 31.1**++ Arabic-English MT08 MT09 46.4 49.2 47.7** 50.0** 48.2**+ 50.6**++ Table 3: Results of log-linear mixtures with or without smoothing. */** or +/++ means result is significantly better than baseline or system without smoothing (p < 0.05 or p < 0.01, respectively). experiments to see if these three modifications yield statistically significant improvements over the original techniques. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrap-resampling test for significance testing. We set the λ in Equation 3 and Equation 7 to 0.01 by looking at performance on the Arabic-English dev set. Table 3 shows the results of log-linear mixture model with or without smoothing. Without smoothing, log-linear mixture model DA hurt the performance of Chinese-to-English on both test sets, but gave moderate improvements on Arabic-to-English. The Chinese task has more (14) sub-corpora than the Arabic one, and they seem more diverse; as"
2014.amta-researchers.10,P12-1099,1,0.850794,"stem (the translation model, language model, reordering model, etc.), but linear mixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for each sentence in the training data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approac"
2014.amta-researchers.10,2008.iwslt-papers.6,0,0.124773,"for five of the six methods (those other than the original VSM), with improvements for these five all in the range +1.7-2.0 BLEU; 3) combining some of these techniques yields further significant improvement. The best combination yields improvements of +2.6-3.3 BLEU over the baseline. 2 Reviewing of SMT adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of"
2014.amta-researchers.10,E12-1055,0,0.0466168,"ence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets linearly or log-linearly (Foster and Kuhn, 2007). Log-linear mixture DA can employ the same discriminative tuning algorithm used to combine the log-linear high-level components of a typical SMT system (the translation model, language model, reordering model, etc.), but linear mixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method"
2014.amta-researchers.10,P13-1082,0,0.0200048,"ixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for each sentence in the training data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev"
2014.amta-researchers.10,P07-1066,0,0.0266431,"he in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing new senses in new domains (Daume III and Jagarlamudi, 2011; Irvine et al., 2013). (Dou and Knight, 2012; Zhang and Zong, 2013) learned bilingual lexica or phrase ta"
2014.amta-researchers.10,W10-2602,0,0.0203879,"data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and K"
2014.amta-researchers.10,J07-1003,0,0.303587,"Missing"
2014.amta-researchers.10,N04-1033,0,0.0435798,"iza) Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 127 3.3 Provenance features Provenance features (Chiang et al., 2011) are applied to lexical weights. There are slight variations in computing lexical weights (Foster et al., 2006), they all use forward and backward word translation probabilities T (s|t) and T (t|s) estimated from the word-aligned parallel text. The conditional probability for word pair (s, t) in a translation table is computed as below: count(s, t) . s count(si , t) T (s|t) = P (4) i We adopt the approach proposed in (Zens and Ney, 2004) to compute the lexical weights. It assumes that all source words are conditionally independent: plw (s|t) = n Y p(si |t) (5) i=1 and adopts a “noisy-or” combination, so that p(si |t) = 1 − m Y (1 − T (si |tj )) (6) j=1 where n and m are number of source and target words in the phrase pair (s, t) respectively. To compute the provenance features, we first estimate the word translation tables T (s|t) and T (t|s) trained on the N sub-corpora. However, many word pairs are unseen for the word translation table of a given sub-corpus. Following (Chiang et al., 2011), we smooth the translation tables:"
2014.amta-researchers.10,P13-1140,0,0.155795,"domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing new senses in new domains (Daume III and Jagarlamudi, 2011; Irvine et al., 2013). (Dou and Knight, 2012; Zhang and Zong, 2013) learned bilingual lexica or phrase tables from in-domain monolingual data with a decipherment method, then incorporated them into the SMT system. 3 Mixture model adaptation There are several adaptation scenarios for SMT, of which the most common is 1) The training material is heterogeneous, with some parts of it that are not too far from the test domain; 2) A bilingual dev set drawn from the test domain is available. A common approach to DA for this scenario is: 1) split the training data into sub-corpora by domain; 2) train sub-models or features on each sub-corpus; 3) weight these sub-model"
2014.amta-researchers.10,C04-1059,0,0.214758,"over the baseline. 2 Reviewing of SMT adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. not-include decision about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixtur"
2014.amta-researchers.10,P13-2122,0,\N,Missing
2016.amta-researchers.8,D11-1033,0,0.260007,"high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity betwee"
2016.amta-researchers.8,W15-3003,0,0.0599708,"ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domai"
2016.amta-researchers.8,K16-1031,1,0.892142,"15) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT system. Experiments described in that paper, covering three different types of test domain and four language directions, show that this SSCNN method yields signiﬁcantly higher BLEU scores for the"
2016.amta-researchers.8,W12-3131,0,0.0978263,"an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO"
2016.amta-researchers.8,P14-1129,0,0.0302692,"a subset of data to be used for training an SMT system from a bilingual corpus, the user must specify the number N of sentence pairs to be chosen. The N sentence pairs with the highest global scores S(s, t) will be selected. This method is symmetrical - the roles of the source-language and target-language sides of the corpus are the same - and bilingual, because the IBM model 1 measures the degree to which each target sentence t is a good translation of its partner s, and vice versa. 2.2 Data Selection with Neural Net Joint Model (NNJM) The Neural Network Joint Model (NNJM), as described in (Devlin et al., 2014), is a joint language and translation model based on a feedforward neural net (NN). It incorporats a wide span of contextual information from the source sentence, in addition to the traditional n-gram information from preceding target-language words. Speciﬁcally, when scoring a target word wi , the NNJM inputs not only the n − 1 preceding words wi−n+1 , ..., wi−1 , but also 2m + 1 source words: the source word si most closely aligned with wi along with the m source words si−m , ..., si−1 to the left of si and the m source words si+1 , ..., si+m to the right of si . The NNJMs used in our experi"
2016.amta-researchers.8,P13-2119,0,0.133955,"picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language mod"
2016.amta-researchers.8,2015.mtsummit-papers.10,0,0.686015,"of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT system. Experiments described in that paper, covering three different types of test domain"
2016.amta-researchers.8,2012.amta-papers.7,1,0.926825,"Missing"
2016.amta-researchers.8,2010.eamt-1.26,0,0.0690526,"ent over 3 points on an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URF"
2016.amta-researchers.8,N15-1011,0,0.0239437,"de up of equal contributions from these four models. Since this metric contains information about the translation relationship between each source sentence and its target counterpart, and since the ways in which the source and target languages are used are mirror images of each other, the NNJM data selection method is both bilingual and symmetrical. 2.3 Data Selection with Semi-Supervised CNN As described in more detail in (Chen and Huang, 2016), we were inspired by the success of convolutional neural networks (CNNs) applied to image and text classiﬁcation (Krizhevsky et al., 2012; Kim, 2014; Johnson and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al"
2016.amta-researchers.8,P14-1062,0,0.00850774,"on and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al., 2014; Johnson and Zhang, 2015b; Wang et al., 2015). Chen and Huang (2016) use CNNs to classify sentence pairs to in-domain and out-of-domain sentence pairs. In many of these studies, the ﬁrst layer of the network converts words to word embeddings using table lookup; the embeddings are sometimes pre-trained on an unnlabeled data. The embeddings remain ﬁxed during subsequent model training. A CNN trained with small number of labled data and pre-trained word embeddings on large unlabeled data is termed “semi-supervised”. Because we were interested in data selection scenarios where only small amounts"
2016.amta-researchers.8,D14-1181,0,0.00555345,"pair is made up of equal contributions from these four models. Since this metric contains information about the translation relationship between each source sentence and its target counterpart, and since the ways in which the source and target languages are used are mirror images of each other, the NNJM data selection method is both bilingual and symmetrical. 2.3 Data Selection with Semi-Supervised CNN As described in more detail in (Chen and Huang, 2016), we were inspired by the success of convolutional neural networks (CNNs) applied to image and text classiﬁcation (Krizhevsky et al., 2012; Kim, 2014; Johnson and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation"
2016.amta-researchers.8,W04-3250,0,0.208233,"f-the-art data selection method for domain adaptation (Axelrod et al., 2011). The “sum LM” variant uses the sum of the source and target LM scores for a sentence pair. 2. SSCNN: Data selection by semi-supervised CNN based on monolingual tokens (Section 2.3) 3. IBM-LM: Data selection by both IBM and language models (Section 2.1) 4. NNJM: Data selection by neural network joint models (Section 2.2) 5. Bi-SSCNN: Data selection by bitoken based semi-supervised CNN (Section 2.4) 3.3 Experimental results We evaluated the system using the BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we apply the bootstrap resampling test to do signiﬁcance testing. Table 2 summarizes the results for each task. The number of selected sentence pairs for each language pair (1.8 million pairs for Chinese-to-English, and 1.4 million pairs for Arabic-to-English) was decided on the basis of tests on held-out data using the IBM-LM method. That is, 1.8 million was the value of N that maximized the BLEU score of the ﬁnal SMT system when IBM-LM was used to select N sentence pairs as training data for Chinese-to-English, and 1.4 had the same property for Arabic-to-English. In the table, the bilingua"
2016.amta-researchers.8,D07-1036,0,0.0723693,"Missing"
2016.amta-researchers.8,2011.iwslt-papers.5,0,0.0415047,"_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT s"
2016.amta-researchers.8,P10-2041,0,0.263994,"election, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language m"
2016.amta-researchers.8,J05-4003,0,0.19056,"pairs, can beneﬁt NMT much more than SMT.We observed a BLEU improvement over 3 points on an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-lang"
2016.amta-researchers.8,W11-2124,0,0.186448,"nce. Essentially, it scores the extent to which both the source and target sentence are in-domain, but does not in any way penalize bad translations. We say that such a method is “symmetric”: it incorporates equal amounts of information from the source and the target language, but it is not “bilingual”: it does not incorporate information about the quality of translations. The main motivation for this paper is to explore CNN-based data selection techniques that are bilingual. It is based on semi-supervised CNNs that use bitokens as units instead of source or target words (Marino et al., 2006; Niehues et al., 2011). For the bitoken semi-supervised CNN, we should use the abbreviation “Bi-SSCNN”. We also experiment with the bilingual method that combines IBM model 1 and language model (LM) scores and neural network joint model. In this paper, we carried out experiments reported on two language pairs: Chinese-toEnglish and Arabic-to-English. We ﬁx the number of training sentences to be chosen for the data selection techniques so that they can be fairly compared, and measure the BLEU score on test data from the resulting MT systems. It turns out that three techniques have roughly the same performance in ter"
2016.amta-researchers.8,P02-1040,0,0.0972421,"ence as the criterion. This is considered to be a state-of-the-art data selection method for domain adaptation (Axelrod et al., 2011). The “sum LM” variant uses the sum of the source and target LM scores for a sentence pair. 2. SSCNN: Data selection by semi-supervised CNN based on monolingual tokens (Section 2.3) 3. IBM-LM: Data selection by both IBM and language models (Section 2.1) 4. NNJM: Data selection by neural network joint models (Section 2.2) 5. Bi-SSCNN: Data selection by bitoken based semi-supervised CNN (Section 2.4) 3.3 Experimental results We evaluated the system using the BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we apply the bootstrap resampling test to do signiﬁcance testing. Table 2 summarizes the results for each task. The number of selected sentence pairs for each language pair (1.8 million pairs for Chinese-to-English, and 1.4 million pairs for Arabic-to-English) was decided on the basis of tests on held-out data using the IBM-LM method. That is, 1.8 million was the value of N that maximized the BLEU score of the ﬁnal SMT system when IBM-LM was used to select N sentence pairs as training data for Chinese-to-English, and 1.4 had the same property fo"
2016.amta-researchers.8,W16-2323,0,0.0611686,"Missing"
2016.amta-researchers.8,2014.amta-researchers.3,1,0.757845,"h SSCNNs that take as input the bitokens of (Marino et al., 2006; Niehues et al., 2011). 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Figure 2: Bitoken sequence. The paper (Niehues et al., 2011) describes a “bilingual language model” (biLM): the idea that SMT systems would beneﬁt from wider contextual information from the source sentence. BiLMs provide this context by aligning each target word in the training data with source words to create bitokens. An n-gram bitoken LM for the sequence of target words is then trained. Figure 2 (taken from (Stewart et al., 2014)) shows how a bitoken sequence is obtained from a word-aligned sentence pair for the English to French language pair. Unaligned target words (e.g., French word “d´’’ in the example) are aligned with NULL. Unaligned source words (e.g., “very”) are dropped. A source word aligned with more than one target word (e.g., “we”) aligned with two instances of “nous” is duplicated: each target word aligned with it receives a copy of that source word. The word embeddings for bitokens are learned directly by word2vec, treating each bitoken as a word. For instance, in the French sentence shown in Figure 2,"
2016.amta-researchers.8,P15-2058,0,0.0114968,"ng sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al., 2014; Johnson and Zhang, 2015b; Wang et al., 2015). Chen and Huang (2016) use CNNs to classify sentence pairs to in-domain and out-of-domain sentence pairs. In many of these studies, the ﬁrst layer of the network converts words to word embeddings using table lookup; the embeddings are sometimes pre-trained on an unnlabeled data. The embeddings remain ﬁxed during subsequent model training. A CNN trained with small number of labled data and pre-trained word embeddings on large unlabeled data is termed “semi-supervised”. Because we were interested in data selection scenarios where only small amounts of in-domain data are available, we chose to u"
2016.amta-researchers.8,I08-2088,0,0.0573278,"e two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the d"
2016.amta-researchers.8,C04-1059,0,0.0497328,"will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for b"
2020.acl-main.143,D16-1250,0,0.0197347,"methods, including word-by-word translation, unsupervised MT, and cross-lingual embedding transformation. On distant language pairs that unsupervised MT struggled to be effective, AT and Bi-view AT perform remarkably better. 2 Related Work The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (U"
2020.acl-main.143,P18-1073,0,0.158206,"hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences1 . 1 Introduction Motivated by a monolingual speaker acquiring translation ability by referring to a bilingual dictionary, we propose a novel MT task that no parallel sentences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency word"
2020.acl-main.143,D18-1399,0,0.0376008,"Missing"
2020.acl-main.143,P19-1019,0,0.062444,"language pairs. 4.5 Experimental Results: with Cross-lingual Pretraining The bottom part of Table 2 reports performances of UNMT with XLM, which conducts the crosslingual pretraining on concatenated non-parallel corpora (Lample and Conneau, 2019), and performances of our AT/Bi-view AT with the anchored cross-lingual pretraining, i.e., ACP. The results show that our proposed AT approaches are still superior when equipped with the cross-lingual pretraining. UNMT obtains great improvement when combined with XLM, achieving state-of-the-art unsupervised MT performance better than Unsupervised SMT (Artetxe et al., 2019) and Unsupervised NMT (Lample et al., 2018b) across close and distant language pairs. 1575 Effect on Bilingual Word Embeddings As shown in Figure 2, we depict the word embeddings of some sampled words in English-Chinese after our Bi-view AT. The dimensions of the embedding vectors are reduced to two by using T-SNE and are visualized by the visualization tool in Tensorflow10 . We sample the English words that are not covered by the dictionary at first, then search their nearest Chinese neighbors in the embedding space. It shows that the words which constitute a new ground-truth translation pair"
2020.acl-main.143,J82-2005,0,0.709748,"Missing"
2020.acl-main.143,D16-1162,0,0.164289,"8b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generated by unsupervised BLI for initializing an unsupervised MT system (Artetxe et al., 2018c,b; Lample et al., 2018a), we use the ground-truth bilingual dictionary and apply it throughout the training process. We propose Anchored Training (AT) to tackle this task. Since word representations are learned over monolingual corpora wi"
2020.acl-main.143,P18-1008,0,0.0261085,"that no parallel sentences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generate"
2020.acl-main.143,E17-1088,0,0.0146801,"of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary. Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and Søgaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017). 3 Our Approach There are multiple freely available bilingual dictionaries such as Muse dictionary2 (Conneau et al., 2018), Wiktionary3 , and PanLex4 . We adopt Muse dictionary which contains 110 large-scale groundtruth bilingual dictionaries. We propose to inject the bilingual dictionary into the MT training by placing anchoring points on the large scale monolingual corpora to drive the semantic spaces of both languages becoming closer so that MT training without parallel sentences becomes easier. We present the proposed Anchored Training (AT) and Bi-view AT in the following. 3.1 Anchored Tr"
2020.acl-main.143,D16-1136,0,0.0209789,"orms remarkably bad on distant language pairs in which aligning the embeddings of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary. Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and Søgaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017). 3 Our Approach There are multiple freely available bilingual dictionaries such as Muse dictionary2 (Conneau et al., 2018), Wiktionary3 , and PanLex4 . We adopt Muse dictionary which contains 110 large-scale groundtruth bilingual dictionaries. We propose to inject the bilingual dictionary into the MT training by placing anchoring points on the large scale monolingual corpora to drive the semantic spaces of both languages becoming closer so that MT training without parallel sentences becomes easier. We present the pr"
2020.acl-main.143,E14-1049,0,0.031871,"aries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al."
2020.acl-main.143,N15-1157,0,0.0293103,"g process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the embeddings of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary. Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and Søgaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017). 3 Our Approach There are multiple freely available bilingual dictionaries such as Muse dictionary2 (Conneau et al., 2018), Wiktionary3 , and PanLex4 . We adopt Muse dictionary which contains 110 large-scale groundtruth bilingual dictionaries. We propose to inject the bilingual dictionary into the MT training by placing anchoring points on the large scale monolingual corpora to drive the semantic spaces of both languages becoming closer s"
2020.acl-main.143,N16-1156,0,0.0134234,"erform remarkably better. 2 Related Work The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionar"
2020.acl-main.143,D18-1549,0,0.0509903,"Missing"
2020.acl-main.143,P15-1027,0,0.0208843,"ive, AT and Bi-view AT perform remarkably better. 2 Related Work The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the"
2020.acl-main.143,N15-1028,0,0.0210411,"orks are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al., 2018c; Lample et"
2020.acl-main.143,P16-1009,0,0.242082,"entences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generated by unsupervised BLI f"
2020.acl-main.143,P16-1162,0,0.424255,"entences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generated by unsupervised BLI f"
2020.acl-main.143,P19-1119,0,0.0252407,"through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al., 2018c; Lample et al., 2018a) or abandon the artificial dictionary by using joint BPE so that multiple BPE units can be shared by both languages (Lample et al., 2018b). We use the ground-truth dictionary instead and apply it throughout a novel training process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the embeddings of both si"
2020.acl-main.143,W14-1613,0,0.0293924,"by both languages (Lample et al., 2018b). We use the ground-truth dictionary instead and apply it throughout a novel training process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the embeddings of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary. Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and Søgaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017). 3 Our Approach There are multiple freely available bilingual dictionaries such as Muse dictionary2 (Conneau et al., 2018), Wiktionary3 , and PanLex4 . We adopt Muse dictionary which contains 110 large-scale groundtruth bilingual dictionaries. We propose to inject the bilingual dictionary into the MT training b"
2020.acl-main.143,N15-1104,0,0.0287906,"uggled to be effective, AT and Bi-view AT perform remarkably better. 2 Related Work The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference i"
2020.acl-main.143,P18-1005,0,0.0184947,"sformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al., 2018c; Lample et al., 2018a) or abandon the artificial dictionary by using joint BPE so that multiple BPE units can be shared by both languages (Lample et al., 2018b). We use the ground-truth dictionary instead and apply it throughout a novel training process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the em"
2020.coling-main.399,2020.acl-main.692,0,0.0160525,"nd figures out the exact meaning of “compense” . With respect to Case 2, the french query “veja chaussur” is failed to be translated to “doka shoes” by APE and our domain transfer model. Our final DTDA model can correctly translate the word “veja” because our method alleviates the modification errors with the union of the repaired data with out-of-domain corpus for training. 5.5 Universality of The Proposed Method In addition, we further examine the proposed method on two widely used domain translation tasks, i.e., English-German Law and Subtitles. We follow the common experimental setting in Aharoni and Goldberg (2020). Experimental results are concluded in Table 8. For data augmentation, we extract in-domain monolingual data following Tiedemann (2012). Obviously, the domain transfer based data augmentation 4529 Source Reference Transformer Forward-Translation Automatic Post Editing Our Model (DTDA) Case1 talons compense wedge heels heels offsets heels offsets wedge heels wedge heels Case2 veja chaussur veja shoes veja shoes veja shoes doka shoes veja shoes Table 7: Case study on French-English translation results produced by different methods. Train Data Model Transformer Forward-Translation (Zhang and Zon"
2020.coling-main.399,D17-1151,0,0.0242807,"rds as in queries. This discrepancy makes the synthetic parallel data readable for human but unsuitable for the downstream retrieval task (Zhou et al., 2012; Sarwar et al., 2019). To this end, we attempt to alleviate this problem by exploiting a domain transfer model to refine out-of-domain data to in-domain queries. 4523 Domain Transfer There are several studies focus on domain adaption or automatic post-editing (APE) for machine translation. Typically, a widely used method is to train an NMT model on general domain data and subsequently finetune it on the in-domain data (Luong et al., 2015; Britz et al., 2017). However, such kind of approaches are denounced to lead to catastrophic forgetting (Kirkpatrick et al., 2017) and rely on massive training tricks. Considering the APE model, Correia and Martins (2019) recently propose an encoder-to-decoder architecture which is initialized from Bert (Devlin et al., 2019), demonstrating the effectiveness of the pre-trained language model on APE tasks. Despite their success, this series of work directly revises the final translation, which has a potential risk of generating incorrect modifications. Different to these studies which leveraging training tricks or"
2020.coling-main.399,P19-1292,0,0.0870089,"t and irregular one. For semantic meaning, the word ”basket” is translated into “basketball” in out-of-domain scenario, but “sneakers” in a real-world E-Commerce search engine. training and inference of QT model. To the best of our knowledge, this is the first study that extend data augmentation with an additional refinement procedure. Moreover, we introduce a novel domain transfer architecture based on a pre-trained cross-lingual language model – Bert (Devlin et al., 2019). Contrast to common refining model that contains two encoders for synthetic sentence pairs and a decoder for generation (Correia and Martins, 2019), our model unifies these components via coordinating and mixing self- and cross-attention layers, resulting in faster processing speed and more adequate architecture to conform to the pre-trained language model. The refinement procedure is fine-tuned by a semi-supervised strategy, which leverages a large amount of monolingual queries and a small number of bilingual query samples that manually generated by human translators. Researchers may concern about why we employ the forward-translation rather than the backward ones for data augmentation. In a real-world search engine, users with differen"
2020.coling-main.399,N19-1423,0,0.500422,"t used to train the teacher translation system is relatively long with complete sentence constituents and syntax, while search query is a short and irregular one. For semantic meaning, the word ”basket” is translated into “basketball” in out-of-domain scenario, but “sneakers” in a real-world E-Commerce search engine. training and inference of QT model. To the best of our knowledge, this is the first study that extend data augmentation with an additional refinement procedure. Moreover, we introduce a novel domain transfer architecture based on a pre-trained cross-lingual language model – Bert (Devlin et al., 2019). Contrast to common refining model that contains two encoders for synthetic sentence pairs and a decoder for generation (Correia and Martins, 2019), our model unifies these components via coordinating and mixing self- and cross-attention layers, resulting in faster processing speed and more adequate architecture to conform to the pre-trained language model. The refinement procedure is fine-tuned by a semi-supervised strategy, which leverages a large amount of monolingual queries and a small number of bilingual query samples that manually generated by human translators. Researchers may concern"
2020.coling-main.399,eisele-chen-2010-multiun,0,0.0119999,"s.com2 ). Then, these queries are translated from English to French/Spanish and next backwards from French/Spanish to English. The intermediate French/Spanish translations are preserved. The monolingual English queries are treated as groundtruth target. In this way, we obtain 27.9M artificial training triplets called ”Artificial data“ for French-English and Spanish-English tasks respectively as shown in Table 1. • Training Dataset for Query Translation In addition to involving the corpus for training domain transfer model, we also extract all available parallel sentences from MultiUN dataset (Eisele and Chen, 2010) as our out-of-domain bilingual corpus. The large-scale monolingual queries are obtained from Aliexpress.com, involving 43.1M french queries and 57.3M spanish queries. These queries are translated by a general machine translation system (Google) to form the original synthetic parallel query pairs. These generated data are then revised by our domain transfer model to in-domain parallel query corpus. The details are presented in Table 1. • Development and Evaluation Dataset We collect 10K French and Spanish search queries from Aliexpress store websites in two countries: Spanish and France, and t"
2020.coling-main.399,W16-2378,0,0.0183671,"h 30K merge operations. The datasets are described as follows: • Training Dataset for Domain transfer In order to train our domain transfer model, a triple dataset need to be constructed, involving a monolingual search query, its machine translation result and ground-truth target. We build this dataset in two ways. Firstly, similar to Negri et al. (2018), we translate the source side of high-quality bilingual query samples (e.g. FrenchEnglish) created by human translators and utilize the generated translation results to constitute the triple dataset called ”Manual data”. Secondly, inspired by Junczys-Dowmunt and Grundkiewicz (2016), we create pseudo training triplets by round-trip translations from only monolingual English queries. These monolingual English queries are randomly extracted from the search log of a real-world E-Commerce website (Aliexpress.com2 ). Then, these queries are translated from English to French/Spanish and next backwards from French/Spanish to English. The intermediate French/Spanish translations are preserved. The monolingual English queries are treated as groundtruth target. In this way, we obtain 27.9M artificial training triplets called ”Artificial data“ for French-English and Spanish-English"
2020.coling-main.399,P07-2045,0,0.00713617,"Att(Hn−1 (10) Y j , Hd ) + HY j , 4525 where Hn−1 means combining different types of representations from the monolingual query X, its d 0 translation result Y as well as previous target tokens before position j. Note that, self- and mixedattention in the same layer share their parameters. 4 Experiments 4.1 Datasets To evaluate the effectiveness of the proposed domain transfer based data augmentation method (DTDA), we conduct experiments on two query translation tasks, including French-English and Spanish-English language pairs. All datasets are tokenized and truecased with the Moses toolkit (Koehn et al., 2007), and splited into sub-word units with a joint BPE model (Sennrich et al., 2016b) with 30K merge operations. The datasets are described as follows: • Training Dataset for Domain transfer In order to train our domain transfer model, a triple dataset need to be constructed, involving a monolingual search query, its machine translation result and ground-truth target. We build this dataset in two ways. Firstly, similar to Negri et al. (2018), we translate the source side of high-quality bilingual query samples (e.g. FrenchEnglish) created by human translators and utilize the generated translation"
2020.coling-main.399,W04-3250,0,0.169001,"ing (Correia and Martins, 2019) 96.72M 96.72M 182M 96.72M 96.72M 84.26M 84.26M 155.3M 84.26M 84.26M 31.32 30.64 30.91 31.83 32.92 55.20 54.48 55.31 56.17 58.45 43.26 42.56 43.11 44.00 45.69 Our Model (DTDA) 96.72M 84.26M 34.05† 60.23† 47.14 Table 2: BLEU scores on Spanish-English and French-English query translation tasks. Our DTDA model yields better translation performance on the examined tasks than other effective methods. AVG indicates the average BLEU scores on test sets. † represents our system is significantly better than best comparable system (p<0.01), tested by bootstrap resampling (Koehn, 2004). • Backward-Translation: The Transformer model reinforced with the backward-translation corpus (Sennrich et al., 2016a). • Forward- & Backward-Translation: The Transformer model trained with both forward and backward translation corpus (Park et al., 2017). • Fine-tuning with Forward-Translation Data: The Transformer model fine-tuned with forwardtranslation corpus. • Automatic Post Editing: The system utilizing APE to directly revise the final translations produced from the above out-of-domain Transformer model (Correia and Martins, 2019). 4.3 Implementation Details • Query Translation: Neural"
2020.coling-main.399,P09-5002,0,0.0159433,"over the output of the encoder HeN :  Dnd = Ln Att(Cnd , HeN ) + Cnd . (4) Here, Att(Cnd , HeN ) denotes attending the top encoder layer HeN with Cnd as the output of self-attention layer in decoder. The top layer of the decoder HdN is used to generate the final output sequence. 2.2 Related Work Query Translation Query translation has attracted increasing attention since its translation quality significantly affects the retrieval results (Wu and He, 2010). Existing studies mainly focus on traditional translation models, e.g. bilingual dictionaries or statistical machine translation systems (Koehn, 2009; Och and Ney, 2002; Gao et al., 2001). Among them, Clinchant and Renders (2007) adapt the initial dictionary to a query-specific dictionary with pseudo relevance feedback methods. Nikoulina et al. (2012) propose to finetune the general translation model using a set of parallel queries. As NMT has shown superiorities in a variety of translation tasks, Sarwar et al. (2019) propose a multi-task learning approach to train a neural-based query translation model with a relevance-based auxiliary task. However, both of these approaches require either expensive language resources or a large amount of"
2020.coling-main.399,D15-1166,0,0.521812,"al., 2015; Gehring et al., 2017). Formally, the learning objective is to minimize the following loss function over the training N , with the size being N: corpus D = {(xn , yn )}n=1 L = E(xn ,yn )∼D [− log P(yn |xn ; θ)], (1) where xn and yn indicate the source and target sides of the n-th example in training data. θ denotes the trainable parameters of NMT model. As an advanced NMT model, Transformer (Vaswani et al., 2017) 4522 builds an encoder-decoder framework merely using self-attention networks (Lin et al., 2017; Vaswani et al., 2017) and cross-attention networks (Bahdanau et al., 2015; Luong et al., 2015). The encoder is composed of a stack of N identical layers, each of which has two sub-layers. The first sub-layer is a self-attention network, and the second one is a position-wise fully connected feed-forward network. A residual connection (He et al., 2016) is employed around each of two sub-layers, followed by layer normalization (Ba et al., 2016). Formally, the output of the first sub-layer Cne and the second sub-layer Hne are sequentially calculated as: n−1 n−1  Cne = Ln Att(Hn−1 , e , He ) + He n n n He = Ln Ffn(Ce ) + Ce , (2) (3) where Att(·), Ln(·), and Ffn(·) are respectively self-a"
2020.coling-main.399,L18-1004,0,0.0189277,"on two query translation tasks, including French-English and Spanish-English language pairs. All datasets are tokenized and truecased with the Moses toolkit (Koehn et al., 2007), and splited into sub-word units with a joint BPE model (Sennrich et al., 2016b) with 30K merge operations. The datasets are described as follows: • Training Dataset for Domain transfer In order to train our domain transfer model, a triple dataset need to be constructed, involving a monolingual search query, its machine translation result and ground-truth target. We build this dataset in two ways. Firstly, similar to Negri et al. (2018), we translate the source side of high-quality bilingual query samples (e.g. FrenchEnglish) created by human translators and utilize the generated translation results to constitute the triple dataset called ”Manual data”. Secondly, inspired by Junczys-Dowmunt and Grundkiewicz (2016), we create pseudo training triplets by round-trip translations from only monolingual English queries. These monolingual English queries are randomly extracted from the search log of a real-world E-Commerce website (Aliexpress.com2 ). Then, these queries are translated from English to French/Spanish and next backwar"
2020.coling-main.399,E12-1012,0,0.0281742,"decoder. The top layer of the decoder HdN is used to generate the final output sequence. 2.2 Related Work Query Translation Query translation has attracted increasing attention since its translation quality significantly affects the retrieval results (Wu and He, 2010). Existing studies mainly focus on traditional translation models, e.g. bilingual dictionaries or statistical machine translation systems (Koehn, 2009; Och and Ney, 2002; Gao et al., 2001). Among them, Clinchant and Renders (2007) adapt the initial dictionary to a query-specific dictionary with pseudo relevance feedback methods. Nikoulina et al. (2012) propose to finetune the general translation model using a set of parallel queries. As NMT has shown superiorities in a variety of translation tasks, Sarwar et al. (2019) propose a multi-task learning approach to train a neural-based query translation model with a relevance-based auxiliary task. However, both of these approaches require either expensive language resources or a large amount of parallel data, which are generally inconsistent with the domain of the queries. The lack of parallel query corpus restricts further improvement on translation quality. Data Augmentation Such kind of low-r"
2020.coling-main.399,P02-1038,0,0.356167,"put of the encoder HeN :  Dnd = Ln Att(Cnd , HeN ) + Cnd . (4) Here, Att(Cnd , HeN ) denotes attending the top encoder layer HeN with Cnd as the output of self-attention layer in decoder. The top layer of the decoder HdN is used to generate the final output sequence. 2.2 Related Work Query Translation Query translation has attracted increasing attention since its translation quality significantly affects the retrieval results (Wu and He, 2010). Existing studies mainly focus on traditional translation models, e.g. bilingual dictionaries or statistical machine translation systems (Koehn, 2009; Och and Ney, 2002; Gao et al., 2001). Among them, Clinchant and Renders (2007) adapt the initial dictionary to a query-specific dictionary with pseudo relevance feedback methods. Nikoulina et al. (2012) propose to finetune the general translation model using a set of parallel queries. As NMT has shown superiorities in a variety of translation tasks, Sarwar et al. (2019) propose a multi-task learning approach to train a neural-based query translation model with a relevance-based auxiliary task. However, both of these approaches require either expensive language resources or a large amount of parallel data, whic"
2020.coling-main.399,W18-6301,0,0.0174206,"ds on both translation quality and retrieval accuracy.1 1 Introduction Cross-lingual information retrieval (CLIR) can have separate query translation (QT), information retrieval (IR), as well as machine-learned ranking stages. Among them, QT stage takes a multilingual user query as input and returns the translation candidates in language of documents for the downstream retrieval. To this end, QT plays a key role and its output significantly affects the retrieval results (Wu and He, 2010). Recently, neural machine translation (NMT) has shown their superiority in a variety of translation tasks (Ott et al., 2018; Hassan et al., 2018). Several studies begin to explore the feasibility and improvements of NMT for QT task (Sarwar et al., 2019; Sharma and Mittal, 2019). However, a well-performed NMT model depends on extensive language resources (Popel and Bojar, 2018; Ott et al., 2018; Wan et al., 2020), while there are few available parallel query corpus for neural QT training, limiting further improvement on translation quality. An alternative way to alleviate this problem is to produce pseudo parallel data from large-scale monolingual queries using a translation model, which refers to data augmentation"
2020.coling-main.399,P02-1040,0,0.106634,"d Martins, 2019). 4.3 Implementation Details • Query Translation: Neural QT model is based upon the Transformer architecture implemented on the open-source toolkit Tensor2Tensor (Vaswani et al., 2018). Adam optimizer (Kingma and Ba, 2015) is applied with an initial learning rate 0.1. The size of hidden dimension and feed-forward layer are set to 512 and 2048 respectively. Encoder and decoder have 6 layers with 8 heads multihead attention. Dropout is 0.1 and batch size involves 4096 tokens. Beam size is 4 for inference. We evaluate query translation tasks with tokenized case-insensitive BLEU3 (Papineni et al., 2002). • Domain Transfer: Instead of a random initialization, the self-attention and mixed attention layers is initialized with the weights of the corresponding self-attention layers of Pre-trained M-Bert (Devlin et al., 2019). Similar to M-Bert, our domain transfer model is composed of 12 layers with hidden size 768 and 12 attention heads. The feed-forward layer size is set to 3072. Adam optimizer is 3 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl 4527 Languages Domain Transfer Model Transformer (Correia and Martins, 2019) Ours Parameters ES-EN FR-EN 33.62 3"
2020.coling-main.399,2020.clssts-1.1,0,0.0150771,"a producer, the teacher translation system is trained using out-of-domain texts which consists of long and syntax-compliant text rather than short keywords as in queries, as illustrated in Figure 1. Such kind of discrepancy makes the data producer tend to generate readable texts for human but unaware candidates for the downstream retrieval task (Zhou et al., 2012; Sarwar et al., 2019). The augmented data are therefore insufficient to teach QT model how to explicitly generate an in-domain query, leading to weak correlation between translation and retrieval qualities (Yarmohammadi et al., 2019; Rubino, 2020; Bi et al., 2020). In this paper, we tackle this problem by proposing a domain transfer based data augmentation method. Specifically, we first build the pseudo parallel corpus by translating large-scale source queries into the target language of retrieval documents using a general NMT engine. The out-of-domain candidates is then be revised to in-domain queries by a domain transfer model, thus eliminating mismatch between the 1 The code and datasets are available at https://github.com/starryskyyl/DTDA.git 4521 Proceedings of the 28th International Conference on Computational Linguistics, pages"
2020.coling-main.399,P19-1639,0,0.196895,"arate query translation (QT), information retrieval (IR), as well as machine-learned ranking stages. Among them, QT stage takes a multilingual user query as input and returns the translation candidates in language of documents for the downstream retrieval. To this end, QT plays a key role and its output significantly affects the retrieval results (Wu and He, 2010). Recently, neural machine translation (NMT) has shown their superiority in a variety of translation tasks (Ott et al., 2018; Hassan et al., 2018). Several studies begin to explore the feasibility and improvements of NMT for QT task (Sarwar et al., 2019; Sharma and Mittal, 2019). However, a well-performed NMT model depends on extensive language resources (Popel and Bojar, 2018; Ott et al., 2018; Wan et al., 2020), while there are few available parallel query corpus for neural QT training, limiting further improvement on translation quality. An alternative way to alleviate this problem is to produce pseudo parallel data from large-scale monolingual queries using a translation model, which refers to data augmentation (Sennrich et al., 2016a; Yao et al., 2020). Nevertheless, as a synthesis data producer, the teacher translation system is traine"
2020.coling-main.399,P16-1009,0,0.443121,"Hassan et al., 2018). Several studies begin to explore the feasibility and improvements of NMT for QT task (Sarwar et al., 2019; Sharma and Mittal, 2019). However, a well-performed NMT model depends on extensive language resources (Popel and Bojar, 2018; Ott et al., 2018; Wan et al., 2020), while there are few available parallel query corpus for neural QT training, limiting further improvement on translation quality. An alternative way to alleviate this problem is to produce pseudo parallel data from large-scale monolingual queries using a translation model, which refers to data augmentation (Sennrich et al., 2016a; Yao et al., 2020). Nevertheless, as a synthesis data producer, the teacher translation system is trained using out-of-domain texts which consists of long and syntax-compliant text rather than short keywords as in queries, as illustrated in Figure 1. Such kind of discrepancy makes the data producer tend to generate readable texts for human but unaware candidates for the downstream retrieval task (Zhou et al., 2012; Sarwar et al., 2019). The augmented data are therefore insufficient to teach QT model how to explicitly generate an in-domain query, leading to weak correlation between translatio"
2020.coling-main.399,P16-1162,0,0.653174,"Hassan et al., 2018). Several studies begin to explore the feasibility and improvements of NMT for QT task (Sarwar et al., 2019; Sharma and Mittal, 2019). However, a well-performed NMT model depends on extensive language resources (Popel and Bojar, 2018; Ott et al., 2018; Wan et al., 2020), while there are few available parallel query corpus for neural QT training, limiting further improvement on translation quality. An alternative way to alleviate this problem is to produce pseudo parallel data from large-scale monolingual queries using a translation model, which refers to data augmentation (Sennrich et al., 2016a; Yao et al., 2020). Nevertheless, as a synthesis data producer, the teacher translation system is trained using out-of-domain texts which consists of long and syntax-compliant text rather than short keywords as in queries, as illustrated in Figure 1. Such kind of discrepancy makes the data producer tend to generate readable texts for human but unaware candidates for the downstream retrieval task (Zhou et al., 2012; Sarwar et al., 2019). The augmented data are therefore insufficient to teach QT model how to explicitly generate an in-domain query, leading to weak correlation between translatio"
2020.coling-main.399,tiedemann-2012-parallel,0,0.0407705,"” by APE and our domain transfer model. Our final DTDA model can correctly translate the word “veja” because our method alleviates the modification errors with the union of the repaired data with out-of-domain corpus for training. 5.5 Universality of The Proposed Method In addition, we further examine the proposed method on two widely used domain translation tasks, i.e., English-German Law and Subtitles. We follow the common experimental setting in Aharoni and Goldberg (2020). Experimental results are concluded in Table 8. For data augmentation, we extract in-domain monolingual data following Tiedemann (2012). Obviously, the domain transfer based data augmentation 4529 Source Reference Transformer Forward-Translation Automatic Post Editing Our Model (DTDA) Case1 talons compense wedge heels heels offsets heels offsets wedge heels wedge heels Case2 veja chaussur veja shoes veja shoes veja shoes doka shoes veja shoes Table 7: Case study on French-English translation results produced by different methods. Train Data Model Transformer Forward-Translation (Zhang and Zong, 2016) Our Model (DTDA) Law 0.47M 2.39M 2.39M Subtitles 0.5M 5.5M 5.5M BLEU-4 Law 41.34 42.58 43.24† Subtitles 24.37 24.95 25.6† AVG 3"
2020.coling-main.399,W18-1819,0,0.171365,"ation 2-3), and trained on large-scale multilingual data. As a representative refining model, Correia and Martins (2019) involve two encoders for synthetic sentence pair and one decoder for generation, both of which are initialized by M-Bert. However, such kind of model assigns additional components for sentence draft encoding which lead to complicated model architecture and reduction of processing speed. Besides, the additional cross-attention layers in decoder (Equation 4) extract features from the top encoding layer, which fail to be initialized from M-Bert. Partially inspired by He et al. (2018) who succeed via sharing layer-wise parameters of encoder and decoder in machine translation tasks, we introduce a novel 4524 domain transfer approach to revise the synthetic parallel queries to in-domain data. As shown in Figure 3, our model unifies components via coordinating and mixing self-attention and cross-attention layers. This offers the model ability to interact features among inputs layer by layer. The proposed architecture conforms to the pre-trained language model, resulting in better initialization from these pre-trained models. In this work, we use M-Bert for initializing the se"
2020.coling-main.399,2020.emnlp-main.80,1,0.534012,"act meaning of “compense” . With respect to Case 2, the french query “veja chaussur” is failed to be translated to “doka shoes” by APE and our domain transfer model. Our final DTDA model can correctly translate the word “veja” because our method alleviates the modification errors with the union of the repaired data with out-of-domain corpus for training. 5.5 Universality of The Proposed Method In addition, we further examine the proposed method on two widely used domain translation tasks, i.e., English-German Law and Subtitles. We follow the common experimental setting in Aharoni and Goldberg (2020). Experimental results are concluded in Table 8. For data augmentation, we extract in-domain monolingual data following Tiedemann (2012). Obviously, the domain transfer based data augmentation 4529 Source Reference Transformer Forward-Translation Automatic Post Editing Our Model (DTDA) Case1 talons compense wedge heels heels offsets heels offsets wedge heels wedge heels Case2 veja chaussur veja shoes veja shoes veja shoes doka shoes veja shoes Table 7: Case study on French-English translation results produced by different methods. Train Data Model Transformer Forward-Translation (Zhang and Zon"
2020.coling-main.399,W19-6602,0,0.0112776,"theless, as a synthesis data producer, the teacher translation system is trained using out-of-domain texts which consists of long and syntax-compliant text rather than short keywords as in queries, as illustrated in Figure 1. Such kind of discrepancy makes the data producer tend to generate readable texts for human but unaware candidates for the downstream retrieval task (Zhou et al., 2012; Sarwar et al., 2019). The augmented data are therefore insufficient to teach QT model how to explicitly generate an in-domain query, leading to weak correlation between translation and retrieval qualities (Yarmohammadi et al., 2019; Rubino, 2020; Bi et al., 2020). In this paper, we tackle this problem by proposing a domain transfer based data augmentation method. Specifically, we first build the pseudo parallel corpus by translating large-scale source queries into the target language of retrieval documents using a general NMT engine. The out-of-domain candidates is then be revised to in-domain queries by a domain transfer model, thus eliminating mismatch between the 1 The code and datasets are available at https://github.com/starryskyyl/DTDA.git 4521 Proceedings of the 28th International Conference on Computational Ling"
2020.coling-main.399,D16-1160,0,0.287966,"nt of parallel data, which are generally inconsistent with the domain of the queries. The lack of parallel query corpus restricts further improvement on translation quality. Data Augmentation Such kind of low-resource translation task has became an open problem in NMT community. Several data augmentation methods which generate the synthetic parallel corpus DO are introduced to alleviate this problem: L = E(xn ,yn )∼(D+DO ) [− log P(yn |xn ; θ)]. (5) Sennrich et al. (2016a) propose an effective approach to augment the parallel training data with backtranslations of target-side sentences, while Zhang and Zong (2016) assign a self-learning algorithm to make full use of source-side monolingual data and generate the synthetic parallel corpus to enlarge the bilingual training data. Park et al. (2017) use synthetic corpus generated from both sides monolingual sentences as an efficient alternative to real parallel data. Nevertheless, both the translation models used to generate synthesis data are trained utilizing out-of-domain corpus which is composed of long and fluent texts other than short keywords as in queries. This discrepancy makes the synthetic parallel data readable for human but unsuitable for the d"
2020.coling-main.399,C12-1164,0,0.1914,"alternative way to alleviate this problem is to produce pseudo parallel data from large-scale monolingual queries using a translation model, which refers to data augmentation (Sennrich et al., 2016a; Yao et al., 2020). Nevertheless, as a synthesis data producer, the teacher translation system is trained using out-of-domain texts which consists of long and syntax-compliant text rather than short keywords as in queries, as illustrated in Figure 1. Such kind of discrepancy makes the data producer tend to generate readable texts for human but unaware candidates for the downstream retrieval task (Zhou et al., 2012; Sarwar et al., 2019). The augmented data are therefore insufficient to teach QT model how to explicitly generate an in-domain query, leading to weak correlation between translation and retrieval qualities (Yarmohammadi et al., 2019; Rubino, 2020; Bi et al., 2020). In this paper, we tackle this problem by proposing a domain transfer based data augmentation method. Specifically, we first build the pseudo parallel corpus by translating large-scale source queries into the target language of retrieval documents using a general NMT engine. The out-of-domain candidates is then be revised to in-doma"
2020.coling-main.399,2020.acl-main.620,1,0.894324,"Missing"
2020.emnlp-main.474,2020.emnlp-main.475,0,0.188631,"Missing"
2020.emnlp-main.474,D19-1147,0,0.640025,"oisy because they are generated by imperOne straightforward and effective solution for fect out-of-domain systems, resulting in the unsupervised domain adaptation is to build inpoor performance of domain adaptation. To domain synthetic parallel data, including copyaddress this issue, we propose a novel itering monolingual target sentences to the source ative domain-repaired back-translation frameside (Currey et al., 2017) or back-translation of work, which introduces the Domain-Repair in-domain monolingual target sentences (Sennrich (DR) model to refine translations in synthetic et al., 2016; Dou et al., 2019). Although the backbilingual data. To this end, we construct corresponding data for the DR model training translation approach has proven the superior effecby round-trip translating the monolingual sentiveness in exploiting monolingual data, directly tences, and then design the unified training applying this method in this scenario brings lowframework to optimize paired DR and NMT quality in-domain synthetic data. Table 1 gives models jointly. Experiments on adapting NMT two incorrect translation sentences generated by models between specific domains and from the back-translation method. The m"
2020.emnlp-main.474,D18-1045,0,0.0133486,"gual data simultaneously by semi-supervised learning (Cheng et al., 2016), dual learning (He et al., 2016) and joint training (Zhang et al., 2018; Hoang et al., 2018). Our method utilizes both source and target data as well, with different that we use monolingual data to train bidirectional DR models, and then these models are used to fix pseudo data. As back-translation is widely considered more effective than the self-training method, several works find that performance of back-translation degrades due to the less rich translation or domain mismatch at the source side of the synthetic data (Edunov et al., 2018; Caswell et al., 2019). Edunov et al. (2018) attempt to use sampling instead of maximum a-posterior when decoding with the reverse direction model. Imamura et al. (2018) add noises to the results of beam search. Caswell et al. (2019) propose to add a tag token at the source side of the synthetic data. Unlike their methods, our method leverages the DR model to re-generate the source side of the synthetic data, which can also increase translation diversity and mitigate the effect of different domains. 3 Iterative Domain-Repaired Back-Translation In this section, we first illustrate the overview"
2020.emnlp-main.474,W17-4713,0,0.0205198,"s. One research line is to extract pseudo in-domain data from large amounts of outdomain parallel data. Bic¸ici and Yuret (2011) use an in-domain held-out set to obtain parallel sentences from out-domain parallel sentences by computing n-gram overlaps. Instead, Moore and Lewis (2010), Axelrod et al. (2011) and Duh et al. (2013) use LMs score to select data similar to in-domain text. Recently, Chen et al. (2017) train a domain classifier to weight the out-domain training samples. There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, these methods cannot always guarantee to find domain-specific samples from out-domain data. Another research direction is to exploit plenty of in-domain monolingual data, e.g., integrating a language model during decoding (C¸aglar G¨ulc¸ehre et al., 2015), copy method (Currey et al., 2017), back-translation (Sennrich et al., 2016) or obtaining domain-aware feature embedding via an auxiliary language modeling (Dou et al., 2019). Among 5885 these approaches, back-translation is a widely used and effective method in exploiting monolingual data. Our proposed meth"
2020.emnlp-main.474,D11-1033,0,0.0193719,"repair and round-trip translation procedures, which are used to generate corresponding training data for NMT and DR models respectively. 2 Related Work Since in-domain parallel corpora are usually hard to obtain, many studies attempt to improve the performance of NMT models without any in-domain parallel sentences. One research line is to extract pseudo in-domain data from large amounts of outdomain parallel data. Bic¸ici and Yuret (2011) use an in-domain held-out set to obtain parallel sentences from out-domain parallel sentences by computing n-gram overlaps. Instead, Moore and Lewis (2010), Axelrod et al. (2011) and Duh et al. (2013) use LMs score to select data similar to in-domain text. Recently, Chen et al. (2017) train a domain classifier to weight the out-domain training samples. There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, these methods cannot always guarantee to find domain-specific samples from out-domain data. Another research direction is to exploit plenty of in-domain monolingual data, e.g., integrating a language model during decoding (C¸aglar G¨ulc¸ehre et al., 2"
2020.emnlp-main.474,N19-1191,0,0.0582107,"to extract pseudo in-domain data from large amounts of outdomain parallel data. Bic¸ici and Yuret (2011) use an in-domain held-out set to obtain parallel sentences from out-domain parallel sentences by computing n-gram overlaps. Instead, Moore and Lewis (2010), Axelrod et al. (2011) and Duh et al. (2013) use LMs score to select data similar to in-domain text. Recently, Chen et al. (2017) train a domain classifier to weight the out-domain training samples. There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, these methods cannot always guarantee to find domain-specific samples from out-domain data. Another research direction is to exploit plenty of in-domain monolingual data, e.g., integrating a language model during decoding (C¸aglar G¨ulc¸ehre et al., 2015), copy method (Currey et al., 2017), back-translation (Sennrich et al., 2016) or obtaining domain-aware feature embedding via an auxiliary language modeling (Dou et al., 2019). Among 5885 these approaches, back-translation is a widely used and effective method in exploiting monolingual data. Our proposed method is also based on back"
2020.emnlp-main.474,W11-2131,0,0.0802961,"Missing"
2020.emnlp-main.474,W19-5206,0,0.0117516,"sly by semi-supervised learning (Cheng et al., 2016), dual learning (He et al., 2016) and joint training (Zhang et al., 2018; Hoang et al., 2018). Our method utilizes both source and target data as well, with different that we use monolingual data to train bidirectional DR models, and then these models are used to fix pseudo data. As back-translation is widely considered more effective than the self-training method, several works find that performance of back-translation degrades due to the less rich translation or domain mismatch at the source side of the synthetic data (Edunov et al., 2018; Caswell et al., 2019). Edunov et al. (2018) attempt to use sampling instead of maximum a-posterior when decoding with the reverse direction model. Imamura et al. (2018) add noises to the results of beam search. Caswell et al. (2019) propose to add a tag token at the source side of the synthetic data. Unlike their methods, our method leverages the DR model to re-generate the source side of the synthetic data, which can also increase translation diversity and mitigate the effect of different domains. 3 Iterative Domain-Repaired Back-Translation In this section, we first illustrate the overview of iter-DRBT framework"
2020.emnlp-main.474,W17-3205,1,0.851988,"nd DR models respectively. 2 Related Work Since in-domain parallel corpora are usually hard to obtain, many studies attempt to improve the performance of NMT models without any in-domain parallel sentences. One research line is to extract pseudo in-domain data from large amounts of outdomain parallel data. Bic¸ici and Yuret (2011) use an in-domain held-out set to obtain parallel sentences from out-domain parallel sentences by computing n-gram overlaps. Instead, Moore and Lewis (2010), Axelrod et al. (2011) and Duh et al. (2013) use LMs score to select data similar to in-domain text. Recently, Chen et al. (2017) train a domain classifier to weight the out-domain training samples. There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, these methods cannot always guarantee to find domain-specific samples from out-domain data. Another research direction is to exploit plenty of in-domain monolingual data, e.g., integrating a language model during decoding (C¸aglar G¨ulc¸ehre et al., 2015), copy method (Currey et al., 2017), back-translation (Sennrich et al., 2016) or obtaining domain-aware"
2020.emnlp-main.474,P16-1185,0,0.0211942,"sed and effective method in exploiting monolingual data. Our proposed method is also based on backtranslation and makes the most of it by improving the data quality with the DR model. The methods of exploiting monolingual data in NMT can be naturally applied in unsupervised domain adaptation. Some studies are working on exploiting source-side monolingual data by selftraining (Zhang and Zong, 2016; Chinea-R´ıos et al., 2017) or pre-training (Yang et al., 2019; Weng et al., 2020; Ji et al., 2020), and leveraging both source and target monolingual data simultaneously by semi-supervised learning (Cheng et al., 2016), dual learning (He et al., 2016) and joint training (Zhang et al., 2018; Hoang et al., 2018). Our method utilizes both source and target data as well, with different that we use monolingual data to train bidirectional DR models, and then these models are used to fix pseudo data. As back-translation is widely considered more effective than the self-training method, several works find that performance of back-translation degrades due to the less rich translation or domain mismatch at the source side of the synthetic data (Edunov et al., 2018; Caswell et al., 2019). Edunov et al. (2018) attempt"
2020.emnlp-main.474,W17-4714,0,0.0361986,"Missing"
2020.emnlp-main.474,W11-2123,0,0.065603,"Missing"
2020.emnlp-main.474,C18-1111,0,0.0611793,"Missing"
2020.emnlp-main.474,W18-2703,0,0.158487,"backtranslation and makes the most of it by improving the data quality with the DR model. The methods of exploiting monolingual data in NMT can be naturally applied in unsupervised domain adaptation. Some studies are working on exploiting source-side monolingual data by selftraining (Zhang and Zong, 2016; Chinea-R´ıos et al., 2017) or pre-training (Yang et al., 2019; Weng et al., 2020; Ji et al., 2020), and leveraging both source and target monolingual data simultaneously by semi-supervised learning (Cheng et al., 2016), dual learning (He et al., 2016) and joint training (Zhang et al., 2018; Hoang et al., 2018). Our method utilizes both source and target data as well, with different that we use monolingual data to train bidirectional DR models, and then these models are used to fix pseudo data. As back-translation is widely considered more effective than the self-training method, several works find that performance of back-translation degrades due to the less rich translation or domain mismatch at the source side of the synthetic data (Edunov et al., 2018; Caswell et al., 2019). Edunov et al. (2018) attempt to use sampling instead of maximum a-posterior when decoding with the reverse direction model"
2020.emnlp-main.474,W17-4715,0,0.176996,"xploiting in-domain monolarge amounts of out-of-domain bilingual data and lingual data with the back-translation method. However, the synthetic parallel data is very in-domain monolingual data. noisy because they are generated by imperOne straightforward and effective solution for fect out-of-domain systems, resulting in the unsupervised domain adaptation is to build inpoor performance of domain adaptation. To domain synthetic parallel data, including copyaddress this issue, we propose a novel itering monolingual target sentences to the source ative domain-repaired back-translation frameside (Currey et al., 2017) or back-translation of work, which introduces the Domain-Repair in-domain monolingual target sentences (Sennrich (DR) model to refine translations in synthetic et al., 2016; Dou et al., 2019). Although the backbilingual data. To this end, we construct corresponding data for the DR model training translation approach has proven the superior effecby round-trip translating the monolingual sentiveness in exploiting monolingual data, directly tences, and then design the unified training applying this method in this scenario brings lowframework to optimize paired DR and NMT quality in-domain synthe"
2020.emnlp-main.474,P19-1286,0,0.457523,"AW 377,114 187,550 189,564 4,233 4,063 MEDICAL 328,132 171,906 156,226 1,141 1,272 Table 2: Statistics on bilingual, monolingual, development and test data of medical and law domains. ing mapping rules, which helps to better fix the synthetic parallel data in the next iteration. Note that we fine-tune the NMT and DR models in each iteration to speed up the whole training process. 4 Experiments 4.1 Setup Datasets. To evaluate the performance of our proposed method, we adopt a multi-domain dataset released by Koehn and Knowles (2017), which is further built as an unaligned monolingual corpus in Hu et al. (2019). However, there are two issues in the train/dev/test splits used in Hu et al. (2019). First, Ma et al. (2019) and Dou et al. (2020) find that some same sentence pairs exist between the training and test data. Second, Hu et al. (2019) randomly shuffle the bi-text data and split it into halves, which may bring more overlap than in natural monolingual data, i.e., bilingual sentences from a document are probably selected into monolingual data (e.g., one sentence on the source split and its translation on the target split). To address the impact of the above two issues, we re-collect in-domain mon"
2020.emnlp-main.474,W18-2707,0,0.014862,"Our method utilizes both source and target data as well, with different that we use monolingual data to train bidirectional DR models, and then these models are used to fix pseudo data. As back-translation is widely considered more effective than the self-training method, several works find that performance of back-translation degrades due to the less rich translation or domain mismatch at the source side of the synthetic data (Edunov et al., 2018; Caswell et al., 2019). Edunov et al. (2018) attempt to use sampling instead of maximum a-posterior when decoding with the reverse direction model. Imamura et al. (2018) add noises to the results of beam search. Caswell et al. (2019) propose to add a tag token at the source side of the synthetic data. Unlike their methods, our method leverages the DR model to re-generate the source side of the synthetic data, which can also increase translation diversity and mitigate the effect of different domains. 3 Iterative Domain-Repaired Back-Translation In this section, we first illustrate the overview of iter-DRBT framework, then describe the architecture of DR model and the joint training strategy. 3.1 Overview Suppose that we have non-parallel in-domain monolingual"
2020.emnlp-main.474,W17-3204,0,0.227114,"ve performance when large amounts of parallel sentences are available (Wu et al., 2016; this issue. Specifically, the DR model is designed Vaswani et al., 2017; Hassan et al., 2018). However, to re-generate in-domain source sentences given some previous works have shown that NMT mod- the synthetic data. In this way, the pseudo parallel data’s source side can be re-written with the els perform poorly in specific domains, especially when they are trained on the corpora from very dis- in-domain style, and some wrong translations are fixed. To optimize the DR model, we use the roundtinct domains (Koehn and Knowles, 2017; Chu and trip translation of monolingual source sentences to Wang, 2018). The fine-tuning method (Luong and construct the corresponding training data. Manning, 2015) is a popular way to mitigate the Since source monolingual data is involved, it is 1 Our code is released in natural to extend the back-translation method to https://github.com/whr94621/ Iterative-Domain-Repaired-Back-Translation bidirectional setting (Zhang et al., 2018), which 5884 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5884–5893, c November 16–20, 2020. 2020 Association for"
2020.emnlp-main.474,2015.iwslt-evaluation.11,0,0.119382,"Missing"
2020.emnlp-main.474,D19-6109,0,0.0218559,"ngual, monolingual, development and test data of medical and law domains. ing mapping rules, which helps to better fix the synthetic parallel data in the next iteration. Note that we fine-tune the NMT and DR models in each iteration to speed up the whole training process. 4 Experiments 4.1 Setup Datasets. To evaluate the performance of our proposed method, we adopt a multi-domain dataset released by Koehn and Knowles (2017), which is further built as an unaligned monolingual corpus in Hu et al. (2019). However, there are two issues in the train/dev/test splits used in Hu et al. (2019). First, Ma et al. (2019) and Dou et al. (2020) find that some same sentence pairs exist between the training and test data. Second, Hu et al. (2019) randomly shuffle the bi-text data and split it into halves, which may bring more overlap than in natural monolingual data, i.e., bilingual sentences from a document are probably selected into monolingual data (e.g., one sentence on the source split and its translation on the target split). To address the impact of the above two issues, we re-collect in-domain monolingual data and test sets in the following steps: • Download the XML files from OPUS2 , extract parallel cor"
2020.emnlp-main.474,P10-2041,0,0.0360641,"consists of translation repair and round-trip translation procedures, which are used to generate corresponding training data for NMT and DR models respectively. 2 Related Work Since in-domain parallel corpora are usually hard to obtain, many studies attempt to improve the performance of NMT models without any in-domain parallel sentences. One research line is to extract pseudo in-domain data from large amounts of outdomain parallel data. Bic¸ici and Yuret (2011) use an in-domain held-out set to obtain parallel sentences from out-domain parallel sentences by computing n-gram overlaps. Instead, Moore and Lewis (2010), Axelrod et al. (2011) and Duh et al. (2013) use LMs score to select data similar to in-domain text. Recently, Chen et al. (2017) train a domain classifier to weight the out-domain training samples. There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, these methods cannot always guarantee to find domain-specific samples from out-domain data. Another research direction is to exploit plenty of in-domain monolingual data, e.g., integrating a language model during decoding (C¸agl"
2020.emnlp-main.474,N19-4007,0,0.0208288,"tage. The DR model significantly improves its quality, which improves the effectiveness of back-translation. Improvement of Lexical Translation. We then assess the change in lexical translation at the source side of synthetic data before and after domain repair. Based on the frequency of words that appear in the out-of-domain training data, we allocate target side words of development sets into three buckets (< 1, [1, 20) and ≥ 20, which represent zeroshot words, few-shot words, and frequent words, respectively), and compute the word translation f-scores within each bucket. We use compare-mt (Neubig et al., 2019) to do all the analysis and plot the results in Figure 4. We can see that the synthetic data repaired by DR models show better word translation in all the buckets. It is worth noting that the improvement of word translation f-scores on zero/few-shot (< 20) words dramatically exceeds that on frequent words, which shows that DR models are especially good at repairing in-domain lexical mistranslations. Improvement of Domain Consistent Style. We further evaluate how can DR models remedy the domain mismatch issue at the source side of back5890 SRC: REF: w/o DR: w/ DR: SRC: REF: w/o DR: w/ DR: Arzne"
2020.emnlp-main.474,tiedemann-2012-parallel,0,0.0857042,"intly update DR and NMT models. More particularly, in the translation repair stage, the back-translated synthetic data can be well rewritten as in-domain sentences by the well-trained DR models to further improve NMT models. Then enhanced NMT models run the round-trip translation on monolingual data to build domain-mapping data, which helps DR models better identify mistakes made by the latest NMT models. This training process is iteratively carried out to make full use of the advantage of DR models to improve NMT models. We evaluate our proposed method on GermanEnglish multi-domain datasets (Tiedemann, 2012). Experimental results on adapting NMT models between specific domains and from the general domain to specific domains show that our proposed method obtains 15.79 and 4.47 BLEU improvements on average over unadapted models and backtranslation, respectively. Further analysis demonstrates the ability of DR models to repair the synthetic parallel data. 9= X {$7 (&) , *7 & , $ DR2:; 7 (1,8)→1 Round-trip Translation X∗ = & ,* 7&} DR2(8,1)→8 7 2:; NMT/→1 Table 1: Two incorrect medical translations caused by the law-domain NMT model in German-English multidomain datasets (Tiedemann, 2012), in which “"
2020.emnlp-main.474,D16-1160,0,0.024803,"Currey et al., 2017), back-translation (Sennrich et al., 2016) or obtaining domain-aware feature embedding via an auxiliary language modeling (Dou et al., 2019). Among 5885 these approaches, back-translation is a widely used and effective method in exploiting monolingual data. Our proposed method is also based on backtranslation and makes the most of it by improving the data quality with the DR model. The methods of exploiting monolingual data in NMT can be naturally applied in unsupervised domain adaptation. Some studies are working on exploiting source-side monolingual data by selftraining (Zhang and Zong, 2016; Chinea-R´ıos et al., 2017) or pre-training (Yang et al., 2019; Weng et al., 2020; Ji et al., 2020), and leveraging both source and target monolingual data simultaneously by semi-supervised learning (Cheng et al., 2016), dual learning (He et al., 2016) and joint training (Zhang et al., 2018; Hoang et al., 2018). Our method utilizes both source and target data as well, with different that we use monolingual data to train bidirectional DR models, and then these models are used to fix pseudo data. As back-translation is widely considered more effective than the self-training method, several work"
2020.emnlp-main.474,N19-4009,0,0.0192682,"fically, the number of layers in the encoder and decoder is set to 6, with 8 attention heads in each layer. Each layer in both encoder and decoder has the same dimension of input and output dmodel = 512, dimension of feed-forward layer’s inner-layer dhidden = 2048. Besides, DR models follow the same setting as the NMT model. The Adam (Kingma and Ba, 2014) algorithm is used to update DR and NMT models. For training initial NMT and DR models, following the setting of Hu et al. (2019), we set the dropout as 0.1 and the label smoothing coefficient as 0.2. Besides, we adopt the setting of Fairseq (Ott et al., 2019) on IWSLT’14 German to English to fine-tune NMT and DR models. During training, we schedule the learning rate with the inverse square root decay scheme, in which the warm-up step is set as 4000, and the maximum learning rate is set as 1e-3 and 5e-4 for pre-training and fine-tuning, respectively. For the joint training strategy, we set the maximum iteration number T in Algorithm 1 as 2 for balancing speed and performance. In practice, we train our framework on 2 Tesla P100 GPUs for all tasks, and it takes 2 days to finish the whole training. Methods. We compare our approach with several baselin"
2020.emnlp-main.474,P02-1040,0,0.112995,"Missing"
2020.emnlp-main.474,W18-6319,0,0.0250981,"Missing"
2020.emnlp-main.474,P16-1009,0,0.0612378,"to in-domain text. Recently, Chen et al. (2017) train a domain classifier to weight the out-domain training samples. There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, these methods cannot always guarantee to find domain-specific samples from out-domain data. Another research direction is to exploit plenty of in-domain monolingual data, e.g., integrating a language model during decoding (C¸aglar G¨ulc¸ehre et al., 2015), copy method (Currey et al., 2017), back-translation (Sennrich et al., 2016) or obtaining domain-aware feature embedding via an auxiliary language modeling (Dou et al., 2019). Among 5885 these approaches, back-translation is a widely used and effective method in exploiting monolingual data. Our proposed method is also based on backtranslation and makes the most of it by improving the data quality with the DR model. The methods of exploiting monolingual data in NMT can be naturally applied in unsupervised domain adaptation. Some studies are working on exploiting source-side monolingual data by selftraining (Zhang and Zong, 2016; Chinea-R´ıos et al., 2017) or pre-traini"
2020.emnlp-main.80,P18-1008,0,0.0343184,"g self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.1 1 Introduction Neural machine translation (NMT) has achieved promising results with the use of various optimization tricks (Hassan et al., 2018; Chen et al., 2018; Xu et al., 2019; Li et al., 2020; Yang et al., 2020). In spite of that, these techniques lead to increased training time and massive hyper-parameters, making the development of a well-performed system expensive (Popel and Bojar, 2018; Ott et al., 2018). As an alternative mitigation, curriculum learning (CL, Elman, 1993; Bengio et al., 2009) has shown its effectiveness on speeding up the convergence and stabilizing the NMT model training (Zhang et al., 2018; Platanios et al., 2019). CL teaches NMT model from easy examples to complex ones rather than equally considering all samples, where the"
2020.emnlp-main.80,P18-1069,0,0.0830225,"h place distributions over the weights of network. For efficiency, we adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016) to approximate Bayesian inference. Given current NMT model parameterized by θ and a mini-batch consisting of N sentence pairs {(x1 , y1 ), · · · , (xN , yN )}, we first perform M passes through the network, where the m-th pass θˆm randomly deactivates part of neurons. Thus, each example yields M sets of conditional probabilities. The lower variance of translation probabilities reflects higher confidence that the model has with respect to the instance (Dong et al., 2018; Wang et al., 2019). We propose multi-granularity strategies for confidence estimation: n ,θ ˆm ) denotes the variwhere Var{P (yjn |xn , y<j ance of the translation probability with respect to yjn . Similar to sentence-level strategy, the confidence scores of tokens are normalized as: exp(βˆjn ) βjn = PJ , ˆn t=1 exp(βt ) (4) where J indicates the length of target sentence yn . 2.2 Training Strategy A larger confidence score indicates that the current model is confident on the corresponding example. Therefore, the model should learn more from the predicted loss. In order to govern the learnin"
2020.emnlp-main.80,N19-1208,0,0.0303738,"ds on the number of training step. 10k BLEU score At the early stage of the study, the model learns more from confident samples, thus accelerating the training. The hesitant samples are not completely ignorant, but relatively few can be learned. As training proceeds, the loss of high-confidence samples gradually reduce, and the model will pay more attention on “complex” samples with low prediction accuracy, thus raising their confidence. In this way, the loss of different samples are dynamically revised, eventually balancing the learning. Contrast to related studies (Zhang et al., 2018, 2019; Kumar et al., 2019; Platanios et al., 2019) which adopt CL into NMT with predefined patterns, the superiority of our model lies in its flexibility on both learning emphasis and strategy. Several researchers may concern about the processing speed when integrating Monte Carlo Dropout sampling. Contrary to prior studies which estimate confidence during inference (Dong et al., 2018; Wang et al., 2019), we only perform forward propagation M = 5 times in training time, which avoids the auto-regressive decoding for efficiency. 20k 40k 80k End 27 25 23 21 −3 −2 −1 0 k 1 2 3 Figure 2: Affects of k on best performance af"
2020.emnlp-main.80,D18-1317,1,0.891192,"Missing"
2020.emnlp-main.80,2020.acl-main.41,1,0.837622,"idering all samples, where the keys lie in the definition of “diffi∗ Baosong Yang and Derek F. Wong are co-corresponding authors. Work was done when Yu Wan was interning at DAMO Academy, Alibaba Group. 1 Our codes: https://github.com/NLP2CT/SPL for NMT. culty” and the strategy of curricula design (Krueger and Dayan, 2009; Kocmi and Bojar, 2017). Existing studies artificially determine data difficulty according to prior linguistic knowledge such as sentence length (SL) and word rarity (WR) (Platanios et al., 2019; Zhang et al., 2019; Zhou et al., 2020), and manually tune the learning schedule (Liu et al., 2020; Fomicheva et al., 2020). However, neither there exists a clear distinction between easy and hard examples (Kumar et al., 2010), nor these human intuitions exactly conform to effective model training (Zhang et al., 2018). Instead, we resolve this problem by introducing self-paced learning (Kumar et al., 2010), where the emphasis of learning can be dynamically determined by model itself rather than human intuitions. Specifically, our model measures the level of confidence on each training example (Gal and Ghahramani, 2016; Xiao and Wang, 2019), where an easy sample is actually the one of high"
2020.emnlp-main.80,W18-6301,0,0.15898,"ion tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.1 1 Introduction Neural machine translation (NMT) has achieved promising results with the use of various optimization tricks (Hassan et al., 2018; Chen et al., 2018; Xu et al., 2019; Li et al., 2020; Yang et al., 2020). In spite of that, these techniques lead to increased training time and massive hyper-parameters, making the development of a well-performed system expensive (Popel and Bojar, 2018; Ott et al., 2018). As an alternative mitigation, curriculum learning (CL, Elman, 1993; Bengio et al., 2009) has shown its effectiveness on speeding up the convergence and stabilizing the NMT model training (Zhang et al., 2018; Platanios et al., 2019). CL teaches NMT model from easy examples to complex ones rather than equally considering all samples, where the keys lie in the definition of “diffi∗ Baosong Yang and Derek F. Wong are co-corresponding authors. Work was done when Yu Wan was interning at DAMO Academy, Alibaba Group. 1 Our codes: https://github.com/NLP2CT/SPL for NMT. culty” and the strategy of curr"
2020.emnlp-main.80,N19-1119,0,0.356016,"ine translation (NMT) has achieved promising results with the use of various optimization tricks (Hassan et al., 2018; Chen et al., 2018; Xu et al., 2019; Li et al., 2020; Yang et al., 2020). In spite of that, these techniques lead to increased training time and massive hyper-parameters, making the development of a well-performed system expensive (Popel and Bojar, 2018; Ott et al., 2018). As an alternative mitigation, curriculum learning (CL, Elman, 1993; Bengio et al., 2009) has shown its effectiveness on speeding up the convergence and stabilizing the NMT model training (Zhang et al., 2018; Platanios et al., 2019). CL teaches NMT model from easy examples to complex ones rather than equally considering all samples, where the keys lie in the definition of “diffi∗ Baosong Yang and Derek F. Wong are co-corresponding authors. Work was done when Yu Wan was interning at DAMO Academy, Alibaba Group. 1 Our codes: https://github.com/NLP2CT/SPL for NMT. culty” and the strategy of curricula design (Krueger and Dayan, 2009; Kocmi and Bojar, 2017). Existing studies artificially determine data difficulty according to prior linguistic knowledge such as sentence length (SL) and word rarity (WR) (Platanios et al., 2019;"
2020.emnlp-main.80,P10-1063,0,0.0353053,"ning through regulating the training loss, as illustrated in Fig. 1. 2.1 αn = PN 24 . 22 t α t=1 exp(ˆ ) (2) 20 Token-Level Confidence (TLC) 5000Intuitively, 50000 95000 confidence scores can be evaluated at more Training Step fine-grained level. We extend our model into token-level so as to estimate the confidence on translating each element in target sentence yn . The confidence βˆjn of the j-th token yjn is: n ˆm M βˆjn = (1 − Var{P (yjn |xn , y<j , θ )}m=1 )k , (3) Confidence Estimation We propose to determine the learning emphasis according to the model confidence (Ueffing and Ney, 2005; Soricut and Echihabi, 2010), which quantifies whether the current model is confident or hesitant on translating the training samples. The model confidence can be quantified by Bayesian neural networks (Buntine and Weigend, 1991; Neal, 1996), which place distributions over the weights of network. For efficiency, we adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016) to approximate Bayesian inference. Given current NMT model parameterized by θ and a mini-batch consisting of N sentence pairs {(x1 , y1 ), · · · , (xN , yN )}, we first perform M passes through the network, where the m-th pass θˆm random"
2020.emnlp-main.80,H05-1096,0,0.0843226,"ntrol the focus of learning through regulating the training loss, as illustrated in Fig. 1. 2.1 αn = PN 24 . 22 t α t=1 exp(ˆ ) (2) 20 Token-Level Confidence (TLC) 5000Intuitively, 50000 95000 confidence scores can be evaluated at more Training Step fine-grained level. We extend our model into token-level so as to estimate the confidence on translating each element in target sentence yn . The confidence βˆjn of the j-th token yjn is: n ˆm M βˆjn = (1 − Var{P (yjn |xn , y<j , θ )}m=1 )k , (3) Confidence Estimation We propose to determine the learning emphasis according to the model confidence (Ueffing and Ney, 2005; Soricut and Echihabi, 2010), which quantifies whether the current model is confident or hesitant on translating the training samples. The model confidence can be quantified by Bayesian neural networks (Buntine and Weigend, 1991; Neal, 1996), which place distributions over the weights of network. For efficiency, we adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016) to approximate Bayesian inference. Given current NMT model parameterized by θ and a mini-batch consisting of N sentence pairs {(x1 , y1 ), · · · , (xN , yN )}, we first perform M passes through the network, w"
2020.emnlp-main.80,D19-1073,0,0.13871,"ns over the weights of network. For efficiency, we adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016) to approximate Bayesian inference. Given current NMT model parameterized by θ and a mini-batch consisting of N sentence pairs {(x1 , y1 ), · · · , (xN , yN )}, we first perform M passes through the network, where the m-th pass θˆm randomly deactivates part of neurons. Thus, each example yields M sets of conditional probabilities. The lower variance of translation probabilities reflects higher confidence that the model has with respect to the instance (Dong et al., 2018; Wang et al., 2019). We propose multi-granularity strategies for confidence estimation: n ,θ ˆm ) denotes the variwhere Var{P (yjn |xn , y<j ance of the translation probability with respect to yjn . Similar to sentence-level strategy, the confidence scores of tokens are normalized as: exp(βˆjn ) βjn = PJ , ˆn t=1 exp(βt ) (4) where J indicates the length of target sentence yn . 2.2 Training Strategy A larger confidence score indicates that the current model is confident on the corresponding example. Therefore, the model should learn more from the predicted loss. In order to govern the learning schedule automatic"
2020.emnlp-main.80,P19-1295,1,0.827329,"ng, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.1 1 Introduction Neural machine translation (NMT) has achieved promising results with the use of various optimization tricks (Hassan et al., 2018; Chen et al., 2018; Xu et al., 2019; Li et al., 2020; Yang et al., 2020). In spite of that, these techniques lead to increased training time and massive hyper-parameters, making the development of a well-performed system expensive (Popel and Bojar, 2018; Ott et al., 2018). As an alternative mitigation, curriculum learning (CL, Elman, 1993; Bengio et al., 2009) has shown its effectiveness on speeding up the convergence and stabilizing the NMT model training (Zhang et al., 2018; Platanios et al., 2019). CL teaches NMT model from easy examples to complex ones rather than equally considering all samples, where the keys lie in the d"
2020.emnlp-main.80,2020.acl-main.620,1,0.767745,"l from easy examples to complex ones rather than equally considering all samples, where the keys lie in the definition of “diffi∗ Baosong Yang and Derek F. Wong are co-corresponding authors. Work was done when Yu Wan was interning at DAMO Academy, Alibaba Group. 1 Our codes: https://github.com/NLP2CT/SPL for NMT. culty” and the strategy of curricula design (Krueger and Dayan, 2009; Kocmi and Bojar, 2017). Existing studies artificially determine data difficulty according to prior linguistic knowledge such as sentence length (SL) and word rarity (WR) (Platanios et al., 2019; Zhang et al., 2019; Zhou et al., 2020), and manually tune the learning schedule (Liu et al., 2020; Fomicheva et al., 2020). However, neither there exists a clear distinction between easy and hard examples (Kumar et al., 2010), nor these human intuitions exactly conform to effective model training (Zhang et al., 2018). Instead, we resolve this problem by introducing self-paced learning (Kumar et al., 2010), where the emphasis of learning can be dynamically determined by model itself rather than human intuitions. Specifically, our model measures the level of confidence on each training example (Gal and Ghahramani, 2016; Xiao and Wan"
2020.emnlp-main.81,N19-1000,0,0.252319,"Missing"
2020.emnlp-main.81,D18-1325,0,0.0421655,"versations or documents, the original mode of translating one sentence at a time ignores the discourse phenomena (Voita et al., 2019a,b), introducing undesirable behaviors such as inconsistent pronouns across different translated sentences. Document-level NMT, as a more realistic translation task in these scenarios, has been systematically ∗ corresponding author. investigated in the machine translation community. Most literatures focused on looking back a fixed number of previous source or target sentences as the document-level context (Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Voita et al., 2019a,b). Some latest works innovatively attempted to either get the most out of the entire document context or dynamically select the suitable context (Maruf and Haffari, 2018; Yang et al., 2019a; Maruf et al., 2019; Jiang et al., 2019). Because of the scarcity of document training data, the benefit gained from such an approach, as reflected in BLEU, is usually limited. We therefore elect to pay attention to the context in the previous n sentences only where n is a small number and usually does not cover the entire document. Almost all of the latest studies chose the standard"
2020.emnlp-main.81,Q18-1029,0,0.0490181,"slating text with long-range dependencies, such as in conversations or documents, the original mode of translating one sentence at a time ignores the discourse phenomena (Voita et al., 2019a,b), introducing undesirable behaviors such as inconsistent pronouns across different translated sentences. Document-level NMT, as a more realistic translation task in these scenarios, has been systematically ∗ corresponding author. investigated in the machine translation community. Most literatures focused on looking back a fixed number of previous source or target sentences as the document-level context (Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Voita et al., 2019a,b). Some latest works innovatively attempted to either get the most out of the entire document context or dynamically select the suitable context (Maruf and Haffari, 2018; Yang et al., 2019a; Maruf et al., 2019; Jiang et al., 2019). Because of the scarcity of document training data, the benefit gained from such an approach, as reflected in BLEU, is usually limited. We therefore elect to pay attention to the context in the previous n sentences only where n is a small number and usually does not cover the enti"
2020.emnlp-main.81,D19-1081,0,0.233273,"Missing"
2020.emnlp-main.81,P19-1116,0,0.178489,"Missing"
2020.emnlp-main.81,D18-1049,0,0.0381941,"Missing"
2021.acl-long.222,N18-1118,0,0.0171027,"SR alone. Table 6: Performance (BLEU scores) on dev and test sets of ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model t"
2021.acl-long.222,2012.eamt-1.60,0,0.0139028,"parallel dataset and the monolingual document and segment words into sub-words by a BPE model with 30K (25K) operations (Sennrich et al., 2016). Fine-tuning data settings. For ZH-EN, we have one translation task on news domain. The document-level parallel corpus of training set include 41K documents with 780K sentence pairs.8 We use the NIST MT 2006 dataset as the development set, and combine the NIST MT 2002, 2003, 2004, 2005, 2008 datasets as test set.. For EN-DE, we test three translation tasks in domains of TED talks, News-Commentary and Europarl. • TED, which is from IWSLT 2017 MT track (Cettolo et al., 2012). We combine test2016 and test2017 as our test set while the rest as the development set. Experimentation To test the effect of our approach in leveraging sentence-level parallel dataset and monolingual documents, we carry out experiments on Chineseto-English (ZH-EN) and English-to-German (ENDE) translation. 4.1 Experimental Settings Pre-training data settings. The ZH-EN sentence-level parallel dataset contains 2.0M sentence pairs with 54.8M Chinese words and 60.8M English words.4 We use WMT14 EN-DE 4 It consists of LDC2002E18, LDC2003E07, LDC2003E14, news part of LDC2004T08, LDC2002T01, LDC20"
2021.acl-long.222,Q17-1024,0,0.049404,"Missing"
2021.acl-long.222,W19-5321,0,0.0420252,"Missing"
2021.acl-long.222,2020.emnlp-main.175,0,0.0462114,"ory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the encoder. They simply concatenate sentences within a document int"
2021.acl-long.222,P17-4012,0,0.0297593,"rovement of averaged 1.36 BLEU and 1.72 Meteor. Moreover, when we use documents as translation units, our models (i.e., #3 Ours-doc) achieve further improvement by modeling document-level context. Compared to previous studies, it also shows that our approach surpasses all context-aware baselines on ZH-EN and EN-DE (TED) tasks and achieves the state-ofthe-art on average. In the scenario where both sentence-level parallel dataset and monolingual documents are used,12 similar performance trends also hold. For example, #5 Ours-sent significantly exceeds Transformer Model settings. We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it.11 For all translation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., Ng , Ne , and Nd in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respectively. The number of heads in multi-head attention is 8 and the dropout rate is 0.1. In pre-training, we train the models for 500K steps on four V100 GPUs with batch-size 8192. We use Adam (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98 for optimization, and learning rate as 1, the warm-up step as 16K."
2021.acl-long.222,P07-2045,0,0.0126851,"Missing"
2021.acl-long.222,C18-1050,1,0.815965,"example, will be X respectively. Table 7 compares the performance when the pre-training task is of CA-MSR objective or combination of CA-GSR and CA-MSR.It Related Work We describe related studies in the following two perspectives. 6.1 5.4 Test 50.03 49.73 shows the combining objective achieves better performance than using CA-MSR alone. Table 6: Performance (BLEU scores) on dev and test sets of ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several atte"
2021.acl-long.222,W07-0734,0,0.0412222,"on test sets. Bi-sent/Mo-doc indicates if the models are pretrained on sentence-level parallel dataset or monolingual documents (7 for no and 3 for yes). Ours-sent/Ours-doc indicates that we use sentences or documents as input units, i.e., performing sentence-level NMT or context-aware NMT. Scores are obtained by running their source code with our model settings. • Europarl, which is extracted from the Europarl v7. The training, development and test sets are obtained through randomly splitting the corpus. Evaluation. For evaluation, we use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate translation quality. All above EN-DE document-level parallel datasets are downloaded from Maruf et al. (2019).10 Similar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pretraining datasets. See Appendix A for more statistics of the fine-tuning datasets. Main results. Table 1 shows the performance of our approach, where Ours-sent and Ours-doc indicate the performance achieved by our approach when we use sentences or documents as input units, re"
2021.acl-long.222,N19-1423,0,0.0188033,"context-aware NMT performance in the scenarios where the document-level parallel dataset is scale-limited, or even not available. On the one hand, sentence-level parallel dataset is a natural resource to use. For example, Zhang et al. (2018) propose a two-stage training strategy for context-aware NMT by pre-training the model on a sentencelevel parallel dataset. On the other hand, JunczysDowmunt (2019) leverage large-scale source-side monolingual documents, in which they simply concatenate sentences within a document into a long sequence and explore multi-task training via the BERT-objective (Devlin et al., 2019) on the encoder. Due to that different models are usually required to model sentences and documents, however, it is challenging to effectively take them both in a single model. In order to effectively and simultaneously model 2 We note that not all, but many context-aware NMT models contain a context encoder to extract global context information from the document. 2851 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2851–2861 August 1–6, 2021. ©2021 Association for Computat"
2021.acl-long.222,2020.acl-main.703,0,0.0431285,"tencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the encoder. They simply concatenate sentences within a document into a long sequence, which is different from our approach. 6.2 Pre-training for Document-Level NMT While there are substantial studies on improving sentence-level NMT with pre-training, we limit ourselves here to pre-training for document-level (context-aware) NMT. BART (Lewis et al., 2020) is a denoising auto-encoder model which learns to reconstruct the original document from a noised version. Inspired by BART, mBART (Liu et al., 2858 2020) is a model trained on a mixed corpus containing monolingual documents of different languages. Both BART and mBART concatenate sentences in one document into a long sequence, and thus fall into a standard sequence-to-sequence (seq2seq) framework. This is very different from our cross-task pre-training, in which we combine both context-agnostic learning and context-aware learning in a single model. 7 Conclusion In order to leverage both large"
2021.acl-long.222,2020.wmt-1.71,0,0.0581681,"f ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (20"
2021.acl-long.222,2020.tacl-1.47,0,0.0324015,"Missing"
2021.acl-long.222,P18-1118,0,0.0816908,"cument-level perspectives. Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents. 1 Introduction Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context. Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few). However, the performance of contextaware NMT still suffers from the size of parallel document dataset. On the one hand, unlike ∗ Corresponding Author: Junhui Li. If not specified, monolingual documents are all for sourceside through this paper. 1 sentence-level translation models which could be well trained on large-scale sentence-level parallel datasets, the translation models of context-aware NMT may result in insufficient training. On the other hand, with only scale-limited source-side documents, the context encoders may fail to effectively extract use"
2021.acl-long.222,N19-1313,0,0.223252,". Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents. 1 Introduction Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context. Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few). However, the performance of contextaware NMT still suffers from the size of parallel document dataset. On the one hand, unlike ∗ Corresponding Author: Junhui Li. If not specified, monolingual documents are all for sourceside through this paper. 1 sentence-level translation models which could be well trained on large-scale sentence-level parallel datasets, the translation models of context-aware NMT may result in insufficient training. On the other hand, with only scale-limited source-side documents, the context encoders may fail to effectively extract useful context from the"
2021.acl-long.222,D18-1325,0,0.0263752,"Missing"
2021.acl-long.222,P02-1040,0,0.109336,"rformance (BLEU and Meteor scores) on test sets. Bi-sent/Mo-doc indicates if the models are pretrained on sentence-level parallel dataset or monolingual documents (7 for no and 3 for yes). Ours-sent/Ours-doc indicates that we use sentences or documents as input units, i.e., performing sentence-level NMT or context-aware NMT. Scores are obtained by running their source code with our model settings. • Europarl, which is extracted from the Europarl v7. The training, development and test sets are obtained through randomly splitting the corpus. Evaluation. For evaluation, we use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate translation quality. All above EN-DE document-level parallel datasets are downloaded from Maruf et al. (2019).10 Similar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pretraining datasets. See Appendix A for more statistics of the fine-tuning datasets. Main results. Table 1 shows the performance of our approach, where Ours-sent and Ours-doc indicate the performance achieved by our approach when we use sent"
2021.acl-long.222,P16-1162,0,0.127889,"Missing"
2021.acl-long.222,D19-1168,1,0.714836,"storation. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the enco"
2021.acl-long.222,W17-4811,0,0.022607,"from both sentencelevel and document-level perspectives. Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents. 1 Introduction Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context. Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few). However, the performance of contextaware NMT still suffers from the size of parallel document dataset. On the one hand, unlike ∗ Corresponding Author: Junhui Li. If not specified, monolingual documents are all for sourceside through this paper. 1 sentence-level translation models which could be well trained on large-scale sentence-level parallel datasets, the translation models of context-aware NMT may result in insufficient training. On the other hand, with only scale-limited source-side documents, the context encoders may fail t"
2021.acl-long.222,W17-4802,0,0.0221713,"Missing"
2021.acl-long.222,D19-1164,0,0.0578752,"ed studies, we lowercase English sentences in ZHEN while truecase English and German sentences in EN-DE. 8 It consists of LDC2002T01, LDC2004T07, LDC2005T06, LDC2005T10, LDC2009T02, LDC2009T15, LDC2010T03. Note that they are also included in ZH-EN parallel dataset. 9 http://www.casmacat.eu/corpus/news-co mmentary.html 2855 Model # ZH-EN BLEU Meteor 7 40.32 27.93 7 40.83 28.19 7 41.01 28.37 7 7 40.92 28.25 7 39.64 27.56 7 40.73 27.97 7 41.27 28.46 3 46.30 32.91 3 49.58 35.97 3 50.03 36.50 Bi- Mosent doc DocT (Zhang et al., 2018) 7 HAN (Miculicich et al., 2018) 7 SAN (Maruf et al., 2019) 7 QCN (Yang et al., 2019) 7 MCN (Zheng et al., 2020) 7 #1 Transformer 7 7 #2 Ours-sent #3 Ours-doc 7 #4 Transformer 3 #5 Ours-sent 3 #6 Ours-doc 3 EN-DE (TED) BLEU Meteor 24.00 44.69 24.58 45.48 24.42 45.26 25.19 46.09 25.10 23.02 43.66 24.75 45.83 25.31 46.30 26.94 47.06 28.73 48.80 29.31 49.40 EN-DE (News) BLEU Meteor 23.08 42.40 25.03 44.02 24.84 44.17 22.37 41.88 24.91 22.03 41.37 24.19 43.96 24.70 44.38 26.80 46.99 28.41 48.52 29.01 48.83 EN-DE (Europarl) BLEU Meteor 29.32 46.72 28.60 46.09 29.75 47.22 29.82 47.86 30.40 28.65 45.83 29.10 47.55 30.07 47.93 29.90 47.50 30.61 48.29 31.52 49.02 Avg. BLEU Meteor 29.18"
2021.acl-long.222,2020.tacl-1.23,0,0.0232631,"g et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the encoder. They simply concatenate sentences within a document into a long sequence, which is different from our approach. 6.2 Pre-training for Document-Level NMT While there are substantial studies on improving sentence-level NMT with pre-training, we limit ourselves here to pre-training for document-level (context-aware) NMT. BART (Lewis et al., 2020) is a denoising auto-encoder model which"
2021.acl-long.222,D18-1049,1,0.913664,"o break the corpus bottleneck for context-aware NMT by leveraging both largescale sentence-level parallel dataset and monolingual documents. Specifically, we aim to use the former to boost the performance of translation models while employ the latter to enhance the context encoders’ capability of capturing useful context information. There have been several attempts to boost context-aware NMT performance in the scenarios where the document-level parallel dataset is scale-limited, or even not available. On the one hand, sentence-level parallel dataset is a natural resource to use. For example, Zhang et al. (2018) propose a two-stage training strategy for context-aware NMT by pre-training the model on a sentencelevel parallel dataset. On the other hand, JunczysDowmunt (2019) leverage large-scale source-side monolingual documents, in which they simply concatenate sentences within a document into a long sequence and explore multi-task training via the BERT-objective (Devlin et al., 2019) on the encoder. Due to that different models are usually required to model sentences and documents, however, it is challenging to effectively take them both in a single model. In order to effectively and simultaneously m"
2021.acl-long.222,Q18-1029,0,0.0560714,"Missing"
2021.acl-long.222,D19-1081,0,0.0570404,"el translation as a fine-tuning task. Table 3 compares the performance with respect to different fine-tuning strategies and different input units in inferring. When we use documents as input units in inferring, the joint fine-tuning strategy provides no advantage. However, when the input units are sentences, the joint fine-tuning strategy outperforms the one not including sentence-level translation in fine-tuning. 5.2 Analysis of Discourse Phenomena We also want to examine whether the proposed approach actually learns to utilize document context to resolve discourse inconsistencies. Following Voita et al. (2019b) and Zheng et al. (2020), we use the same datasets to train model and contrastive test set for the evaluation of discourse phenomena for English-Russian by Voita et al. (2019b). There are four test sets in the suite regarding deixis, lexicon consistency, ellipsis (inflection and verb phrase). Each testset contains groups of contrastive examples consisting of a positive translation with correct discourse phenomenon and negative translations with incorrect phenomena. The goal is to figure out if a model is more likely to generate a cor2857 Model Trans. Ours Bi-sent Mo-doc 7 7 7 7 Dev 67.30 68."
2021.acl-long.222,P19-1116,0,0.0302677,"Missing"
2021.acl-long.222,P18-1117,0,0.0178409,"rformance (BLEU scores) on dev and test sets of ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-le"
2021.acl-long.222,D17-1301,0,0.0138548,"ares the performance when the pre-training task is of CA-MSR objective or combination of CA-GSR and CA-MSR.It Related Work We describe related studies in the following two perspectives. 6.1 5.4 Test 50.03 49.73 shows the combining objective achieves better performance than using CA-MSR alone. Table 6: Performance (BLEU scores) on dev and test sets of ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monoli"
2021.acl-long.267,K16-1002,0,0.0407819,"Missing"
2021.acl-long.267,J93-2003,0,0.0782466,"Missing"
2021.acl-long.267,P05-1033,0,0.147096,"separately. As shown in Table 6, when we replace the combined attention on top 2 layers with group attention, the performance drops by 0.22, 0.09, and 0.75 d-BLEU on TED, News, and Europarl, respectively. When we replace the combined attention with global attention, the performance decrease is enlarged to 0.84, 0.69, and 1.00 d-BLEU, respectively. These results demonstrate the necessity of combined attention for integrating local and global context information. 6 Related Work The unit of translation has evolved from word (Brown et al., 1993; Vogel et al., 1996) to phrase (Koehn et al., 2003; Chiang, 2005, 2007) and further to sentence (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) in the MT literature. The trend shows that larger units of translation, when represented properly, can lead to improved translation quality. A line of document-level MT extends translation unit to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 2018; Zhang et al., 2020; Ma et al., 2020). However, these approaches are limited within a short context of maximum four sentences. Recent studies extend the translation unit to whole document (Junczys-Dowmunt, 2019; Liu et a"
2021.acl-long.267,J07-2003,0,0.501204,"Missing"
2021.acl-long.267,P05-1066,0,0.291163,"Missing"
2021.acl-long.267,W15-4908,0,0.0442761,"Missing"
2021.acl-long.267,D11-1084,0,0.0365643,"w that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets. 1 Context Source Target Decoder Translation1 Translation2 Source Encoder Source1 … Target Decoder Source2 … (b) Multi-sentence Translation Translation1 Translation2 …      … Source1 Source2  … … (c) G-Transformer (Doc-by-doc Translation) Figure 1: Overview of model structures for documentlevel machine translation. Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when con"
2021.acl-long.267,P14-6007,0,0.188194,"ia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when conducting document translation (Hardmeier, 2014; L¨aubli et al., 2018). Despite that neural models achieve competitive performances on sentence* Corresponding author. Source Encoder (a) Sentence-by-sentence Translation Introduction ∗ Context Encoder level MT, the performance of document-level MT is still far from satisfactory. Existing methods can be mainly classified into two categories. The first category translates a document sentence by sentence using a sequence-tosequence neural model (Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Zheng et al., 2020). Document-level context is integrated into sentence-translation b"
2021.acl-long.267,P13-4033,0,0.0299596,"r converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets. 1 Context Source Target Decoder Translation1 Translation2 Source Encoder Source1 … Target Decoder Source2 … (b) Multi-sentence Translation Translation1 Translation2 …      … Source1 Source2  … … (c) G-Transformer (Doc-by-doc Translation) Figure 1: Overview of model structures for documentlevel machine translation. Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when conducting document transla"
2021.acl-long.267,P19-1356,0,0.139735,"e attention range may also result in unstable training and slow down the training process. 3.3 Conclusion The above experiments show that training failure on Transformer can be caused by local minima. Additionally, the oscillation of attention range may make it worse. During training process, the attention module needs to identify relevant tokens from whole sequence to attend to. Assuming that the sequence length is N , the complexity of the attention distribution increases when N grows from sentence-level to document-level. We propose to use locality properties (Rizzi, 2013; Hardmeier, 2014; Jawahar et al., 2019) of both the language itself and the translation task as a constraint in Transformer, regulating the hypothesis space of the self-attention and target-to-source attention, using a simple group tag method. 4 G-Transformer An example of G-Transformer is shown in Figure 6, where the input document contains more than 3 sentences. As can be seen from the figure, G-Transformer extends Transformer by augmenting the input and output with group tags (Bao and Zhang, 2021). In particular, each token is assigned a group tag, indicating its sentential index. While 3445 source group tags can be assigned det"
2021.acl-long.267,W19-5321,0,0.114867,"complexity. Second, more importantly, information exchange cannot be made between the current sentence and its document context in the same encoding module. The second category extends the translation unit from a single sentence to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 3442 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3442–3455 August 1–6, 2021. ©2021 Association for Computational Linguistics 2018; Zhang et al., 2020) and the whole document (Junczys-Dowmunt, 2019; Liu et al., 2020). Recently, it has been shown that when the translation unit increases from one sentence to four sentences, the performance improves (Zhang et al., 2020; Scherrer et al., 2019). However, when the whole document is encoded as a single unit for sequence to sequence translation, direct supervised training has been shown to fail (Liu et al., 2020). As a solution, either large-scale pre-training (Liu et al., 2020) or data augmentation (Junczys-Dowmunt, 2019) has been used as a solution, leading to improved performance. These methods are shown in Figure 1(b). One limitation of suc"
2021.acl-long.267,D13-1176,0,0.0689615,"lace the combined attention on top 2 layers with group attention, the performance drops by 0.22, 0.09, and 0.75 d-BLEU on TED, News, and Europarl, respectively. When we replace the combined attention with global attention, the performance decrease is enlarged to 0.84, 0.69, and 1.00 d-BLEU, respectively. These results demonstrate the necessity of combined attention for integrating local and global context information. 6 Related Work The unit of translation has evolved from word (Brown et al., 1993; Vogel et al., 1996) to phrase (Koehn et al., 2003; Chiang, 2005, 2007) and further to sentence (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) in the MT literature. The trend shows that larger units of translation, when represented properly, can lead to improved translation quality. A line of document-level MT extends translation unit to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 2018; Zhang et al., 2020; Ma et al., 2020). However, these approaches are limited within a short context of maximum four sentences. Recent studies extend the translation unit to whole document (Junczys-Dowmunt, 2019; Liu et al., 2020), using large augmented dataset or pretrained models."
2021.acl-long.267,P07-2045,0,0.0140967,"mentary v11 for training, which is document-delimited and sentence-aligned. newstest2015 is used for development, and newstest2016 for testing. Europarl. The corpus is extracted from Europarl v7, where sentences are segmented and aligned using additional information. The train, dev and test sets are randomly split from the corpus. The detailed statistics of these corpora are shown in Table 1. We pre-process the documents by splitting them into instances with up-to 512 tokens, taking a sentence as one instance if its length exceeds 512 tokens. We tokenize and truecase the sentences with MOSES (Koehn et al., 2007) tools, applying BPE (Sennrich et al., 2016) with 30000 merging operations. We consider three standard model configurations. Base Model. Following the standard Transformer base model (Vaswani et al., 2017), we use 6 layers, 8 heads, 512 dimension outputs, and 2048 3443 Language Dataset En-De TED News Europarl #Sentences train/dev/test 0.21M/9K/2.3K 0.24M/2K/3K 1.67M/3.6K/5.1K #Documents train/dev/test 1.7K/92/22 6K/80/154 118K/239/359 #Instances train/dev/test 11K/483/123 18.5K/172/263 162K/346/498 Avg #Sents/Inst train/dev/test 18.3/18.5/18.3 12.8/12.6/11.3 10.3/10.4/10.3 Avg #Tokens/Inst tra"
2021.acl-long.267,W17-3204,0,0.0197758,", leading to improved performance. These methods are shown in Figure 1(b). One limitation of such methods is that they require much more training time due to the necessity of data augmentation. Intuitively, encoding the whole input document as a single unit allows the best integration of context information when translating the current sentence. However, little work has been done investigating the underlying reason why it is difficult to train such a document-level NMT model. One remote clue is that as the input sequence grows larger, the input becomes more sparse (Pouget-Abadie et al., 2014; Koehn and Knowles, 2017). To gain more understanding, we make dedicated experiments on the influence of input length, data scale and model size for Transformer (Section 3), finding that a Transformer model can fail to converge when training with long sequences, small datasets, or big model size. We further find that for the failed cases, the model gets stuck at local minima during training. In such situation, the attention weights from the decoder to the encoder are flat, with large entropy values. This can be because that larger input sequences increase the challenge for focusing on a local span to translate when ge"
2021.acl-long.267,N03-1017,0,0.157576,"Missing"
2021.acl-long.267,D18-1512,0,0.0527802,"Missing"
2021.acl-long.267,2020.acl-main.703,0,0.0381462,"1.7K/92/22 6K/80/154 118K/239/359 #Instances train/dev/test 11K/483/123 18.5K/172/263 162K/346/498 Avg #Sents/Inst train/dev/test 18.3/18.5/18.3 12.8/12.6/11.3 10.3/10.4/10.3 Avg #Tokens/Inst train/dev/test 436/428/429 380/355/321 320/326/323 Table 1: En-De datasets for evaluation. 64 128 256 Tokens 512 1024 Loss 5K 10K 20K Instances 40K 80K 160K dimension hidden vectors. Big Model. We follow the standard Transformer big model (Vaswani et al., 2017), using 6 layers, 16 heads, 1024 dimension outputs, and 4096 dimension hidden vectors. Large Model. We use the same settings of BART large model (Lewis et al., 2020), which involves 12 layers, 16 heads, 1024 dimension outputs, and 4096 dimension hidden vectors. We use s-BLEU and d-BLEU (Liu et al., 2020) as the metrics. The detailed descriptions are in Appendix A. Transformer and Long Inputs We empirically study Transformer (see Appendix B) on the datasets. We run each experiment five times using different random seeds, reporting the average score for comparison. Failure Reproduction Input Length. We use the Base model and fixed dataset for this comparison. We split both the training and testing documents from Europarl dataset into instances with input le"
2021.acl-long.267,N19-4009,0,0.0609932,"Missing"
2021.acl-long.267,L18-1275,0,0.0196206,"d-BLEU. Here, rnd. denotes the model trained using randomly initialized parameters. Method TED News Europarl Drop G-Transformer (rnd.) Combined attention 25.84 25.23 33.87 25.62 25.14 33.12 -0.35 Only group attention Only global attention 25.00 24.54 32.87 -0.84 Table 6: Separate effect of group and global attention reporting in d-BLEU. Here, rnd. denotes the model trained using randomly initialized parameters. Russion (En-Ru) for deixis and ellipsis. We follow the Transformer concat baseline (Voita et al., 2019b) and use both 6M sentence pairs and 1.5M document pairs from OpenSubtitles2018 (Lison et al., 2018) to train our model. The results are shown in Table 4. G-Transformer outperforms Transformer baseline concat (Voita et al., 2019b) with a large margin on three discourse features, indicating a better leverage of the source-side context. When compared to previous model LSTM-T, G-Transformer achieves a better ellipsis on both infl. and VP. However, the score on deixis is still lower, which indicates a potential direction that we can investigate in further study. Word-dropout. As shown in Table 5, worddropout (Appendix C.1) contributes about 0.37 dBLEU on average. Its contribution to TED and News"
2021.acl-long.267,2020.tacl-1.47,0,0.234542,"ores for both nonpretraining and pre-training settings on three benchmark datasets. 1 Context Source Target Decoder Translation1 Translation2 Source Encoder Source1 … Target Decoder Source2 … (b) Multi-sentence Translation Translation1 Translation2 …      … Source1 Source2  … … (c) G-Transformer (Doc-by-doc Translation) Figure 1: Overview of model structures for documentlevel machine translation. Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when conducting document translation (Hardmeier, 2014; L¨aubli et al., 2018). Despite that neural models achieve compe"
2021.acl-long.267,2020.acl-main.321,0,0.0889397,"K = GX . Decoder. We use one group multi-head attention module for self-attention and another group multihead attention module for cross-attention. Similar to the encoder, we assign the same group-tag sequence to the key and value of the self-attention, that GQ = GK = GY , but use different group-tag sequences for cross-attention that GQ = GY and GK = GX . 3446 Method TED News s-BLEU d-BLEU s-BLEU d-BLEU S ENT N MT (Vaswani et al., 2017) 23.10 22.40 HAN (Miculicich et al., 2018b) 24.58 25.03 SAN (Maruf et al., 2019) 24.42 24.84 25.10 24.91 Hybrid Context (Zheng et al., 2020) Flat-Transformer (Ma et al., 2020) 24.87 23.55 Transformer on sent (baseline) 24.82 25.19 Transformer on doc (baseline) 0.76 0.60 G-Transformer random initialized (ours) 23.53 25.84* 23.55 25.23* G-Transformer fine-tuned on sent Transformer (ours) 25.12 27.17* 25.52 27.11* Fine-tuning on Pre-trained Model Flat-Transformer+BERT (Ma et al., 2020) 26.61 24.52 G-Transformer+BERT (ours) 26.81 26.14 Transformer on sent fine-tuned on BART (baseline) 27.78 29.90 Transformer on doc fine-tuned on BART (baseline) 28.29 30.49 G-Transformer fine-tuned on BART (ours) 28.06 30.03* 30.34* 31.71* Europarl s-BLEU d-BLEU 29.40 28.60 29.75 30.40"
2021.acl-long.267,P18-1118,0,0.0153541,"mum four sentences. Recent studies extend the translation unit to whole document (Junczys-Dowmunt, 2019; Liu et al., 2020), using large augmented dataset or pretrained models. Liu et al. (2020) shows that Transformer trained directly on documentlevel dataset can fail, resulting in unreasonably low BLEU scores. Following these studies, we also model translation on the whole document. We solve the training challenge using a novel locality bias with group tags. Another line of work make document-level machine translation sentence by sentence, using additional components to represent the context (Maruf and Haffari, 2018; Zheng et al., 2020; Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Yang et al., 2019). Different from these approaches, G-Transformer uses a generic design for both source and context, translating whole document in one beam search instead of sentence-by-sentence. Some methods use a two-pass strategy, generating sentence translation first, integrating context information through a post-editing model (Voita et al., 2019a; Yu et al., 2020). In contrast, G-Transformer uses a single model, which reduces the complexity for both training and inference. The locality bias we introd"
2021.acl-long.267,N19-1313,0,0.240338,"e-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets. 1 Context Source Target Decoder Translation1 Translation2 Source Encoder Source1 … Target Decoder Source2 … (b) Multi-sentence Translation Translation1 Translation2 …      … Source1 Source2  … … (c) G-Transformer (Doc-by-doc Translation) Figure 1: Overview of model structures for documentlevel machine translation. Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when conducting document translation (Hardmeier, 2014; L¨aubli et al., 2018). Despite that neural m"
2021.acl-long.267,D18-1325,0,0.104196,"former, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets. 1 Context Source Target Decoder Translation1 Translation2 Source Encoder Source1 … Target Decoder Source2 … (b) Multi-sentence Translation Translation1 Translation2 …      … Source1 Source2  … … (c) G-Transformer (Doc-by-doc Translation) Figure 1: Overview of model structures for documentlevel machine translation. Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when conducting document translation (Hardmeier, 2014; L¨aubli et al., 2018)."
2021.acl-long.267,D19-6506,0,0.0407945,"Missing"
2021.acl-long.267,P16-1162,0,0.0636249,"nt-delimited and sentence-aligned. newstest2015 is used for development, and newstest2016 for testing. Europarl. The corpus is extracted from Europarl v7, where sentences are segmented and aligned using additional information. The train, dev and test sets are randomly split from the corpus. The detailed statistics of these corpora are shown in Table 1. We pre-process the documents by splitting them into instances with up-to 512 tokens, taking a sentence as one instance if its length exceeds 512 tokens. We tokenize and truecase the sentences with MOSES (Koehn et al., 2007) tools, applying BPE (Sennrich et al., 2016) with 30000 merging operations. We consider three standard model configurations. Base Model. Following the standard Transformer base model (Vaswani et al., 2017), we use 6 layers, 8 heads, 512 dimension outputs, and 2048 3443 Language Dataset En-De TED News Europarl #Sentences train/dev/test 0.21M/9K/2.3K 0.24M/2K/3K 1.67M/3.6K/5.1K #Documents train/dev/test 1.7K/92/22 6K/80/154 118K/239/359 #Instances train/dev/test 11K/483/123 18.5K/172/263 162K/346/498 Avg #Sents/Inst train/dev/test 18.3/18.5/18.3 12.8/12.6/11.3 10.3/10.4/10.3 Avg #Tokens/Inst train/dev/test 436/428/429 380/355/321 320/326/"
2021.acl-long.267,W17-4811,0,0.120714,"f et al., 2019; Zheng et al., 2020). Document-level context is integrated into sentence-translation by introducing additional context encoder. The structure of such a model is shown in Figure 1(a). These methods suffer from two limitations. First, the context needs to be encoded separately for translating each sentence, which adds to the runtime complexity. Second, more importantly, information exchange cannot be made between the current sentence and its document context in the same encoding module. The second category extends the translation unit from a single sentence to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 3442 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3442–3455 August 1–6, 2021. ©2021 Association for Computational Linguistics 2018; Zhang et al., 2020) and the whole document (Junczys-Dowmunt, 2019; Liu et al., 2020). Recently, it has been shown that when the translation unit increases from one sentence to four sentences, the performance improves (Zhang et al., 2020; Scherrer et al., 2019). However, when the whole document is encoded as a single unit fo"
2021.acl-long.267,Q17-1007,0,0.0141943,"ces tends to be constant, the time and memory complexities of group attention are approximately O(N ), making training and inference on very long inputs feasible. 4.2 Combined Attention We use only group attention on lower layers for local sentence representation, and combined attention on top layers for integrating local and global context information. We use the standard multihead attention in Eq 5 for global context, naming it global multi-head attention (GlobalMHA). Group multi-head attention in Eq 8 and global multi-head attention are combined using a gate-sum module (Zhang et al., 2016; Tu et al., 2017) HL HG g H = GroupMHA(Q, K, V, GQ , GK ), = GlobalMHA(Q, K, V ), = sigmoid([HL , HG ]W + b), = HL g + HG (1 − g), (10) where W and b are linear projection parameters, and denotes element-wise multiplication. Previous study (Jawahar et al., 2019) shows that the lower layers of Transformer catch more local syntactic relations, while the higher layers represent longer distance relations. Based on these findings, we use combined attention only on the top layers for integrating local and global context. By this design, on lower layers, the sentences are isolated from each other, while on top layers"
2021.acl-long.267,C96-2141,0,0.609057,", we study the effect of group and global attention separately. As shown in Table 6, when we replace the combined attention on top 2 layers with group attention, the performance drops by 0.22, 0.09, and 0.75 d-BLEU on TED, News, and Europarl, respectively. When we replace the combined attention with global attention, the performance decrease is enlarged to 0.84, 0.69, and 1.00 d-BLEU, respectively. These results demonstrate the necessity of combined attention for integrating local and global context information. 6 Related Work The unit of translation has evolved from word (Brown et al., 1993; Vogel et al., 1996) to phrase (Koehn et al., 2003; Chiang, 2005, 2007) and further to sentence (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) in the MT literature. The trend shows that larger units of translation, when represented properly, can lead to improved translation quality. A line of document-level MT extends translation unit to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 2018; Zhang et al., 2020; Ma et al., 2020). However, these approaches are limited within a short context of maximum four sentences. Recent studies extend the translation unit to who"
2021.acl-long.267,D19-1081,0,0.0225201,"Missing"
2021.acl-long.267,P19-1116,0,0.0264584,"Missing"
2021.acl-long.267,D19-1164,0,0.0121706,"al., 2020), using large augmented dataset or pretrained models. Liu et al. (2020) shows that Transformer trained directly on documentlevel dataset can fail, resulting in unreasonably low BLEU scores. Following these studies, we also model translation on the whole document. We solve the training challenge using a novel locality bias with group tags. Another line of work make document-level machine translation sentence by sentence, using additional components to represent the context (Maruf and Haffari, 2018; Zheng et al., 2020; Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Yang et al., 2019). Different from these approaches, G-Transformer uses a generic design for both source and context, translating whole document in one beam search instead of sentence-by-sentence. Some methods use a two-pass strategy, generating sentence translation first, integrating context information through a post-editing model (Voita et al., 2019a; Yu et al., 2020). In contrast, G-Transformer uses a single model, which reduces the complexity for both training and inference. The locality bias we introduce to G-Transformer is different from the ones in Longformer (Beltagy et al., 2020) and Reformer (Kitaev"
2021.acl-long.267,D18-1049,0,0.0297389,"Missing"
2021.acl-long.267,2020.emnlp-main.81,1,0.825661,"Missing"
2021.acl-short.47,N19-1191,0,0.0546008,"ed with only a few training samples. On four benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively filter out the noises in retrieval results and significantly outperforms the vanilla kNN-MT model. Even more noteworthy is that the Meta-k Network learned on one domain could be directly applied to other domains and obtain consistent improvements, illustrating the generality of our method. Our implementation is open-sourced at https://github. com/zhengxxn/adaptive-knn-mt. 1 Introduction Retrieval-based methods (Gu et al., 2018; Zhang et al., 2018; Bapna and Firat, 2019; Khandelwal et al., 2020a) are increasingly receiving attentions from the machine translation (MT) community recently. These approaches complement advanced neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Hassan et al., 2018) to alleviate the performance degradation when translating out-of-domain sentences (Dou et al., 2019; Wei et al., 2020), rare words (Koehn and Knowles, ∗ Corresponding author. 2017), etc. The ability of accessing any provided datastore during translation makes them scalable, adaptable and interpretable. kNN-MT,"
2021.acl-short.47,D19-1147,0,0.0961514,"ts, illustrating the generality of our method. Our implementation is open-sourced at https://github. com/zhengxxn/adaptive-knn-mt. 1 Introduction Retrieval-based methods (Gu et al., 2018; Zhang et al., 2018; Bapna and Firat, 2019; Khandelwal et al., 2020a) are increasingly receiving attentions from the machine translation (MT) community recently. These approaches complement advanced neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Hassan et al., 2018) to alleviate the performance degradation when translating out-of-domain sentences (Dou et al., 2019; Wei et al., 2020), rare words (Koehn and Knowles, ∗ Corresponding author. 2017), etc. The ability of accessing any provided datastore during translation makes them scalable, adaptable and interpretable. kNN-MT, recently proposed in (Khandelwal et al., 2020a), equips a pre-trained NMT model with a kNN classifier over a datastore of cached context representations and corresponding target tokens, providing a simple yet effective strategy to utilize cached contextual information in inference. However, the hyper-parameter k is fixed for all cases, which raises some potential problems. Intuitively"
2021.acl-short.47,2020.findings-emnlp.307,0,0.137873,"Missing"
2021.acl-short.47,W17-3204,0,0.0286012,"Missing"
2021.acl-short.47,W19-5333,0,0.023106,"proposed model as Adaptive kNN-MT (A) and compare it with two baselines. One of that is vanilla kNN-MT (V) and the other is uniform kNN-MT (U) where we set equal confidence for each kNN prediction. Datasets and Evaluation Metric. We use the same multi-domain dataset as the baseline (Khandelwal et al., 2020a), and consider domains including IT, Medical, Koran, and Law in our experiments. The sentence statistics of datasets are illustrated in Table 1. The Moses toolkit1 is used to tokenize the sentences and split the words into subword units (Sennrich et al., 2016) with the bpecodes provided by Ng et al. (2019). We use SacreBLEU2 to measure all results with case-sensitive detokenized BLEU (Papineni et al., 2002). Dataset Train Dev Test IT Medical Koran Laws 222, 927 2000 2000 248, 009 2000 2000 17, 982 2000 2000 467, 309 2000 2000 Table 1: Statistics of dataset in different domains. (3) Prediction. Instead of introducing the hyperparameter λ as Equation (2), we aggregate the NMT model and different kNN predictions with the output of the Meta-k Network to obtain the final prediction: p(yt |x, yˆ<t ) = 4 pMeta (ki ) · pki NN (yt |x, yˆ<t ), ki ∈S (4) where pki NN indicates the ki Nearest Neighbor pred"
2021.acl-short.47,N19-4009,0,0.0149402,"2002). Dataset Train Dev Test IT Medical Koran Laws 222, 927 2000 2000 248, 009 2000 2000 17, 982 2000 2000 467, 309 2000 2000 Table 1: Statistics of dataset in different domains. (3) Prediction. Instead of introducing the hyperparameter λ as Equation (2), we aggregate the NMT model and different kNN predictions with the output of the Meta-k Network to obtain the final prediction: p(yt |x, yˆ<t ) = 4 pMeta (ki ) · pki NN (yt |x, yˆ<t ), ki ∈S (4) where pki NN indicates the ki Nearest Neighbor prediction results calculated as Equation (1). Implementation Details. We adopt the fairseq toolkit3 (Ott et al., 2019) and faiss4 (Johnson et al., 2017) to replicate kNN-MT and implement our model. We apply the WMT’19 German-English news translation task winner model (Ng et al., 2019) as the pre-trained NMT model which is also used by Khandelwal et al. (2020a). For kNN-MT, we carefully tune the hyper-parameter λ in Equation (2) and report the best scores for each domain. More details are included in the supplementary materials. For our method, the hidden size of the two-layer FFN in Meta-k Network is set to 32. We Training. We fix the pre-trained NMT model and only optimize the Meta-k Network by minimizing th"
2021.acl-short.47,P02-1040,0,0.113175,"NN-MT (V) and the other is uniform kNN-MT (U) where we set equal confidence for each kNN prediction. Datasets and Evaluation Metric. We use the same multi-domain dataset as the baseline (Khandelwal et al., 2020a), and consider domains including IT, Medical, Koran, and Law in our experiments. The sentence statistics of datasets are illustrated in Table 1. The Moses toolkit1 is used to tokenize the sentences and split the words into subword units (Sennrich et al., 2016) with the bpecodes provided by Ng et al. (2019). We use SacreBLEU2 to measure all results with case-sensitive detokenized BLEU (Papineni et al., 2002). Dataset Train Dev Test IT Medical Koran Laws 222, 927 2000 2000 248, 009 2000 2000 17, 982 2000 2000 467, 309 2000 2000 Table 1: Statistics of dataset in different domains. (3) Prediction. Instead of introducing the hyperparameter λ as Equation (2), we aggregate the NMT model and different kNN predictions with the output of the Meta-k Network to obtain the final prediction: p(yt |x, yˆ<t ) = 4 pMeta (ki ) · pki NN (yt |x, yˆ<t ), ki ∈S (4) where pki NN indicates the ki Nearest Neighbor prediction results calculated as Equation (1). Implementation Details. We adopt the fairseq toolkit3 (Ott e"
2021.acl-short.47,P16-1162,0,0.0804927,"ithout training on any in-domain data. We denote the proposed model as Adaptive kNN-MT (A) and compare it with two baselines. One of that is vanilla kNN-MT (V) and the other is uniform kNN-MT (U) where we set equal confidence for each kNN prediction. Datasets and Evaluation Metric. We use the same multi-domain dataset as the baseline (Khandelwal et al., 2020a), and consider domains including IT, Medical, Koran, and Law in our experiments. The sentence statistics of datasets are illustrated in Table 1. The Moses toolkit1 is used to tokenize the sentences and split the words into subword units (Sennrich et al., 2016) with the bpecodes provided by Ng et al. (2019). We use SacreBLEU2 to measure all results with case-sensitive detokenized BLEU (Papineni et al., 2002). Dataset Train Dev Test IT Medical Koran Laws 222, 927 2000 2000 248, 009 2000 2000 17, 982 2000 2000 467, 309 2000 2000 Table 1: Statistics of dataset in different domains. (3) Prediction. Instead of introducing the hyperparameter λ as Equation (2), we aggregate the NMT model and different kNN predictions with the output of the Meta-k Network to obtain the final prediction: p(yt |x, yˆ<t ) = 4 pMeta (ki ) · pki NN (yt |x, yˆ<t ), ki ∈S (4) wher"
2021.acl-short.47,2020.emnlp-main.474,1,0.841791,"he generality of our method. Our implementation is open-sourced at https://github. com/zhengxxn/adaptive-knn-mt. 1 Introduction Retrieval-based methods (Gu et al., 2018; Zhang et al., 2018; Bapna and Firat, 2019; Khandelwal et al., 2020a) are increasingly receiving attentions from the machine translation (MT) community recently. These approaches complement advanced neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Hassan et al., 2018) to alleviate the performance degradation when translating out-of-domain sentences (Dou et al., 2019; Wei et al., 2020), rare words (Koehn and Knowles, ∗ Corresponding author. 2017), etc. The ability of accessing any provided datastore during translation makes them scalable, adaptable and interpretable. kNN-MT, recently proposed in (Khandelwal et al., 2020a), equips a pre-trained NMT model with a kNN classifier over a datastore of cached context representations and corresponding target tokens, providing a simple yet effective strategy to utilize cached contextual information in inference. However, the hyper-parameter k is fixed for all cases, which raises some potential problems. Intuitively, the retrieved nei"
2021.acl-short.47,N18-1120,0,0.301745,"be efficiently trained with only a few training samples. On four benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively filter out the noises in retrieval results and significantly outperforms the vanilla kNN-MT model. Even more noteworthy is that the Meta-k Network learned on one domain could be directly applied to other domains and obtain consistent improvements, illustrating the generality of our method. Our implementation is open-sourced at https://github. com/zhengxxn/adaptive-knn-mt. 1 Introduction Retrieval-based methods (Gu et al., 2018; Zhang et al., 2018; Bapna and Firat, 2019; Khandelwal et al., 2020a) are increasingly receiving attentions from the machine translation (MT) community recently. These approaches complement advanced neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Hassan et al., 2018) to alleviate the performance degradation when translating out-of-domain sentences (Dou et al., 2019; Wei et al., 2020), rare words (Koehn and Knowles, ∗ Corresponding author. 2017), etc. The ability of accessing any provided datastore during translation makes them scalable, adaptable and"
2021.emnlp-main.325,E17-2076,0,0.0218532,"guage. It is a multi-modality We leverage this idea and adapt it to sequence task, closely related to automatic speech recogni- tasks. Our main contributions are: 1. We protion (ASR) and machine translation (MT). ST has pose a jointly-trainable mutual-learning paradigm, a wide range of applications, such as video subti- which improves the distillation method by training tling (Saboo and Baumann, 2019), real-time lecture together. The search space of MT and ST are both translation (Müller et al., 2016), and protection of enlarged, providing the potential for a more robust endangered languages (Bansal et al., 2017). local optima. 2. We further improve our mutualDespite the recent success in end-to-end (E2E) learning method by integrating cyclical annealing models, currently such systems still face the issue schedule, which alleviates the KL vanishing probof labelled training data insufficiency (Sperber and lem suffered by many time-series tasks (Fu et al., Paulik, 2020). A recent popular advance in E2E ST 2019; Bowman et al., 2016; Higgins et al., 2017). 3. is the use of knowledge distillation (KD), which We implement extensive experiments on MuST-C can provide an effective paradigm for transfer- En-Fr,"
2021.emnlp-main.325,K16-1002,0,0.0174672,"ogether. The search space of MT and ST are both translation (Müller et al., 2016), and protection of enlarged, providing the potential for a more robust endangered languages (Bansal et al., 2017). local optima. 2. We further improve our mutualDespite the recent success in end-to-end (E2E) learning method by integrating cyclical annealing models, currently such systems still face the issue schedule, which alleviates the KL vanishing probof labelled training data insufficiency (Sperber and lem suffered by many time-series tasks (Fu et al., Paulik, 2020). A recent popular advance in E2E ST 2019; Bowman et al., 2016; Higgins et al., 2017). 3. is the use of knowledge distillation (KD), which We implement extensive experiments on MuST-C can provide an effective paradigm for transfer- En-Fr, En-Es datasets and illustrate the advantage ring knowledge from rich-resource to low-resource of our model by empirically comparing with a cas∗ caded model, a knowledge distillation (KD) model Majority of this work was conducted during Jiawei Zhao’s research internship at DAMO Academy, Alibaba. and a multi-task learning (MTL) model. The ex3989 Proceedings of the 2021 Conference on Empirical Methods in Natural Language P"
2021.emnlp-main.325,N19-1202,0,0.0619948,"Missing"
2021.emnlp-main.325,N16-1109,0,0.0300824,"ethods in Natural Language Processing, pages 3989–3994 c November 7–11, 2021. 2021 Association for Computational Linguistics perimental results show our model can effectively leverage the transcript and the auxiliary MT task, and we provide competitive results in all experiments. In addition, as a side benefit, the performance of the MT model also improves. 2 2.1 KL1 = KL(pmt ||pst ) = KL2 = KL(pst ||pmt ) = End-to-End Speech Translation L=− logP (y|x; θ) N X pjst ln j=1 E2E ST learns a single model, which directly maps features extracted from speech signal to a target language text sequence (Duong et al., 2016; Weiss et al., 2017). More concretely, given a sample pair (x, y) from the training set D corresponding to speech signal features and translated target sentence, the ST model is trained by minimising the negative log likelihood (NLL) loss, L: pjmt pjmt ln pjst j=1 Model Description X N X pjst pjmt (4) (5) where N represents the length of the output sentence. We adopt NLL loss as the reconstruction loss, denoted by LCst for ST and LCmt for MT: LCst = − N X yi ln (pist |yi ) (6) yi ln (pimt |yi ) (7) i=1 LCmt = − (1) N X i=1 (x,y)∈D E2E models consist of an encoder that encodes speech input as"
2021.emnlp-main.325,N19-1021,0,0.0208855,"ST and MT (LCst and LCmt ) and KL divergence between outputs of ST and MT (KL1 and KL2 ). The training process is described by Algorithm 1. We propose to train ST and MT models iteratively until convergence. In each iteration, there are two steps: 1. MT model is frozen and the parameters of ST model are updated; 2. ST model is frozen and the parameters of MT model are updated. KL vanishing issue: Leveraging KL divergence for mimicry loss in our mutual-learning strategy can suffer from the vanishing issue, which has been observed in other applications, for example in variational auto-encoders (Fu et al., 2019). We mitigate this by adopting a cyclical annealing schedule for β, which has been proposed for this purpose in the context of variational auto-encoders (Fu et al., 2019). More concretely, β in Eq. 8 changes periodically during training iterations, as described by Eq. 11:  r RC , r &lt;= RC βt = (11) 1, r &gt; RC where t represents the current training iteration and r is defined as: r = mod(t − 1, C) (12) The training process is effectively split into many cycles with each cycle lasting C iterations. In each cycle βt progressively increases from 0 to 1 during RC iterations and then stays at 1 for t"
2021.emnlp-main.325,P18-1007,0,0.0249757,"i Gangi et al., 2019a), using the two mostused language pairs: English-to-French (En-Fr) and English-to-Spanish (En-Es). En-Fr dataset contains 500 hours of speech and 280k sentences. EnEs dataset contains 504 hours of speech and 270k sentences. Pre-processing We implement the same data pre-processing steps as described in fairseq speechto-text framework (Wang et al., 2020). Specifically, we extract 80-channel log Mel-filterbank features. The training samples that are larger than 3000 frames are removed. For both, input and target texts, we employ newly proposed subword regularisation method (Kudo, 2018) to build a vocabulary with a size of 8000. We also experiment with a jointly-trained shared vocabulary of size 8000. 3.2 Architecture and Evaluation Details For ST task we use a stack of 2 1D convolutional layers (kernel size 5, stride 2), followed by 12 Transformer layers of size 2048 as the encoder. We use 6 stacked Transformer layers with size 512 as the decoder. For MT task we use 12 stacked Transformer layers with size 2048 as the encoder and 6 stacked transformer layer with size 2048 as the decoder. Evaluation is based on the standard implementation of BLEU score, SACREBLEU (Post, 2018)"
2021.emnlp-main.325,2020.iwslt-1.8,0,0.0810234,"Missing"
2021.emnlp-main.325,N16-3017,0,0.0696636,"Missing"
2021.emnlp-main.325,W18-6319,0,0.012074,"Kudo, 2018) to build a vocabulary with a size of 8000. We also experiment with a jointly-trained shared vocabulary of size 8000. 3.2 Architecture and Evaluation Details For ST task we use a stack of 2 1D convolutional layers (kernel size 5, stride 2), followed by 12 Transformer layers of size 2048 as the encoder. We use 6 stacked Transformer layers with size 512 as the decoder. For MT task we use 12 stacked Transformer layers with size 2048 as the encoder and 6 stacked transformer layer with size 2048 as the decoder. Evaluation is based on the standard implementation of BLEU score, SACREBLEU (Post, 2018), with beam size of 5. The maximum number of tokens in each batch is set to 40000. Dataset We evaluate the proposed framework on the popular MuST-C multilingual speech translation cor3991 1 https://ict.fbk.eu/must-c/ 4 4.1 Results and Analysis Comparison with a Cascaded Model To form a cascaded model, we first train a Transformer-based E2E ASR model using speech inputs and English transcripts. We then train an MT model using English transcripts and target sentences. In inference mode, we first use ASR to generate intermediate text representation, then we pass this to the MT system and calculat"
2021.emnlp-main.325,W19-5210,0,0.0204583,"nformation from multiple models and allow effective dual knowledge transfer in image processSpeech translation (ST) aims to translate speech sig- ing tasks (Zhang et al., 2018; Zhao et al., 2021). nals into a foreign language. It is a multi-modality We leverage this idea and adapt it to sequence task, closely related to automatic speech recogni- tasks. Our main contributions are: 1. We protion (ASR) and machine translation (MT). ST has pose a jointly-trainable mutual-learning paradigm, a wide range of applications, such as video subti- which improves the distillation method by training tling (Saboo and Baumann, 2019), real-time lecture together. The search space of MT and ST are both translation (Müller et al., 2016), and protection of enlarged, providing the potential for a more robust endangered languages (Bansal et al., 2017). local optima. 2. We further improve our mutualDespite the recent success in end-to-end (E2E) learning method by integrating cyclical annealing models, currently such systems still face the issue schedule, which alleviates the KL vanishing probof labelled training data insufficiency (Sperber and lem suffered by many time-series tasks (Fu et al., Paulik, 2020). A recent popular adv"
2021.emnlp-main.325,2020.acl-main.661,0,0.0255737,"Missing"
2021.emnlp-main.325,2020.aacl-demo.6,0,0.0234506,"s. In each cycle βt progressively increases from 0 to 1 during RC iterations and then stays at 1 for the remaining (1 − R)C iterations. With R = 0.5 and C = 5000, we are able to mitigate KL vanishing issue and train. 3 3.1 Experiments pus1 (Di Gangi et al., 2019a), using the two mostused language pairs: English-to-French (En-Fr) and English-to-Spanish (En-Es). En-Fr dataset contains 500 hours of speech and 280k sentences. EnEs dataset contains 504 hours of speech and 270k sentences. Pre-processing We implement the same data pre-processing steps as described in fairseq speechto-text framework (Wang et al., 2020). Specifically, we extract 80-channel log Mel-filterbank features. The training samples that are larger than 3000 frames are removed. For both, input and target texts, we employ newly proposed subword regularisation method (Kudo, 2018) to build a vocabulary with a size of 8000. We also experiment with a jointly-trained shared vocabulary of size 8000. 3.2 Architecture and Evaluation Details For ST task we use a stack of 2 1D convolutional layers (kernel size 5, stride 2), followed by 12 Transformer layers of size 2048 as the encoder. We use 6 stacked Transformer layers with size 512 as the deco"
2021.emnlp-main.325,P19-1649,1,0.846176,"loss as the reconstruction loss, denoted by LCst for ST and LCmt for MT: LCst = − N X yi ln (pist |yi ) (6) yi ln (pimt |yi ) (7) i=1 LCmt = − (1) N X i=1 (x,y)∈D E2E models consist of an encoder that encodes speech input as an intermediate representation, and a decoder that decodes this intermediate representation to a probability distribution over the target text feature space. In the past, the encoder and decoder were based on recurrent neural network architecture, but most recent work utilises Transformerbased architecture (Berard et al., 2016; Weiss et al., 2017; Di Gangi et al., 2019b; Zhang et al., 2019; Vila et al., 2018). where yi denotes the ith token in the target sentence. The overall mutual-learning training loss is a combination of the weighted mimicry loss and the reconstruction losses, as described by Eq. 8. The proposed mutual-learning scenario is illustrated in Figure 1. 2.2 Algorithm 1 Training Strategy Input: training set, ST network parameters θst (with ASR pre-trained encoder), pre-trained MT network parameters θmt repeat t=t+1 1. Compute pst and pmt for one mini batch 2. Freeze θmt , compute the gradient and update θst Mutual-Learning Model definition: Given a parallel data s"
2021.findings-acl.281,P19-1020,0,0.0150394,"T02, NIST03, NIST04, NIST05, NIST08 as the test sets. For the English-French translation task, we use the IWSLT 2016 corpus with 230k sentence pairs for training, test2012 for validation, and test2013 and test2014 as the test sets. All models are based on the Transformer architecture. Details of the data processing, model configuration, and training settings can be found in the appendix. We compare with the following methods: ˜ (j) ), z˜(k) ), ω Lm ˜ )] mixup = E[KL(Dec&gt;k (Enc&gt;j (h 3186 • The vanilla Transformer model (Vaswani et al., 2017). • The virtual adversarial regularization method in (Sano et al., 2019), which adds a proportion of normalized gradient to the source and target word embeddings for adversarial training. METHOD Vaswani et al. (2017) Sano et al. (2019) Cheng et al. (2019) Cheng et al. (2020) Our method MT06 44.57 45.75 46.95 49.26 49.43 MT02 45.49 46.37 47.06 49.03 49.54 Chinese-English MT03 MT04 44.55 46.20 45.02 46.49 46.48 47.39 47.96 48.86 50.34 49.46 MT05 44.96 45.88 46.58 49.88 49.04 MT08 35.11 35.90 37.38 39.63 39.19 English-French test13 test14 40.88 37.79 41.67 38.72 41.76 39.46 43.03 40.91 44.58 41.56 Table 1: Comparison of main results with different robust training met"
2021.findings-acl.281,D18-1100,0,0.0135491,"ning but with a slightly different use of words or expressions. Some studies (Belinkov and Bisk, 2017) have shown that the performance of NMT models can drop significantly when small perturbations are added to input sentences. This problem can be attributed to overfitting as it is difficult to reliably model the translation distribution for the part of input space that has little or no training samples. There have been several attempts to address this problem by filling the space via data augmentation. One direction is to create new training samples by adding perturbations at the token level (Wang et al., 2018; Belinkov and Bisk, 2017; Sperber et al., 2017; Ebrahimi et al., 2018; Li et al., 2019; Cheng et al., 2018, 2019, 2020; Levy et al., 2019), through either token insertion, deletion, and substitution operations or introducing noises to token embedding vectors. Among these approaches, Cheng et al. (2019) demonstrated the effectiveness of incorporating adversarial training samples that are natural sentences, with their semantic relevance to the original sentence safeguarded by language modeling. Cheng et al. (2020) achieved further improvement by creating more diverse but virtual sentences by mi"
2021.findings-emnlp.358,2020.acl-main.692,0,0.0199973,")∈(X ,Y) t (5) where θ is the parameters of all adapter layers. Note that we keep original parameters in the pre-trained NMT model fixed during training to avoid the performance degradation of the NMT model in the inference stage. pre-trained NMT model with adapter layers forward passes these pairs to create an in-domain datastore. When translating in-domain sentences, we utilize the original NMT model and kNN retrieval on the in-domain datastore to perform online domain adaptation as Equation (3). 4 Experiments 4.1 Setup Datasets and Evaluation Metric. We use the same multi-domain dataset as Aharoni and Goldberg (2020) to evaluate the effectiveness of our proposed model and consider domains including IT, Medical, Koran, and Law in our experiments. We extract target-side data in the training sets to perform unsupervised domain adaptation while keeping the dev and test sets unchanged. Besides, WMT’19 News data1 is used for training the adapters in our method as well as the reverse translation model for back-translation. The sentence statistics of all datasets are illustrated in table 1. The Moses toolkit2 is used to tokenize the sentences and we split the words into subword units (Sennrich et al., 2016b) with"
2021.findings-emnlp.358,D19-1147,0,0.0160741,"nario that utilizes large amounts of representation of this task to the ideal repmonolingual in-domain data. One straightforward resentation of translation task. Experiments and effective solution for unsupervised domain on multi-domain datasets demonstrate that our proposed approach significantly improves the adaptation is to build in-domain synthetic parallel translation accuracy with target-side monolindata via back-translation of monolingual target sengual data, while achieving comparable perfortences (Sennrich et al., 2016a; Zhang et al., 2018b; mance with back-translation. Our implementaDou et al., 2019; Wei et al., 2020). Although this tion is open-sourced at https://github. approach has proven superior effectiveness in excom/zhengxxn/UDA-KNN. ploiting monolingual data, applying it in kNN-MT requires an additional reverse model and brings the 1 Introduction extra cost of generating back-translation, making Non-parametric methods (Gu et al., 2018; Zhang the adaptation of kNN-MT more complicated and et al., 2018a; Bapna and Firat, 2019a; Khandel- time-consuming in practice. wal et al., 2020; Zheng et al., 2021) have recently In this paper, we propose a novel Unsupervised been successfully app"
2021.findings-emnlp.358,W17-3204,0,0.0648933,"Missing"
2021.findings-emnlp.358,W19-5333,0,0.0206117,"model and consider domains including IT, Medical, Koran, and Law in our experiments. We extract target-side data in the training sets to perform unsupervised domain adaptation while keeping the dev and test sets unchanged. Besides, WMT’19 News data1 is used for training the adapters in our method as well as the reverse translation model for back-translation. The sentence statistics of all datasets are illustrated in table 1. The Moses toolkit2 is used to tokenize the sentences and we split the words into subword units (Sennrich et al., 2016b) with the codes provided by the pre-trained model (Ng et al., 2019). We use SacreBLEU3 to measure all results with casesensitive detokenized BLEU (Papineni et al., 2002). Dataset Train Dev Test WMT19’News IT Medical Koran Laws 37, 079, 168 10, 000 - 222, 927 2000 2000 248, 009 2000 2000 17, 982 2000 2000 467, 309 2000 2000 Table 1: Statistics of dataset in different domains. Methods. We compare our proposed approach with several baselines: • Basic NMT: A general domain model is directly used to evaluate on in-domain test sets. • Empty-kNN: The source-side of synthetic bilingual data is always set to <EOS> token. • Copy-kNN: Each target sentence is copied to s"
2021.findings-emnlp.358,P02-1040,0,0.110604,"target-side data in the training sets to perform unsupervised domain adaptation while keeping the dev and test sets unchanged. Besides, WMT’19 News data1 is used for training the adapters in our method as well as the reverse translation model for back-translation. The sentence statistics of all datasets are illustrated in table 1. The Moses toolkit2 is used to tokenize the sentences and we split the words into subword units (Sennrich et al., 2016b) with the codes provided by the pre-trained model (Ng et al., 2019). We use SacreBLEU3 to measure all results with casesensitive detokenized BLEU (Papineni et al., 2002). Dataset Train Dev Test WMT19’News IT Medical Koran Laws 37, 079, 168 10, 000 - 222, 927 2000 2000 248, 009 2000 2000 17, 982 2000 2000 467, 309 2000 2000 Table 1: Statistics of dataset in different domains. Methods. We compare our proposed approach with several baselines: • Basic NMT: A general domain model is directly used to evaluate on in-domain test sets. • Empty-kNN: The source-side of synthetic bilingual data is always set to <EOS> token. • Copy-kNN: Each target sentence is copied to source-side to produce synthetic bilingual data. This is a special case of our method without model tra"
2021.findings-emnlp.358,P16-1009,0,0.22508,"f kNN-MT on unsupervised domain original NMT model to map the token-level adaptation scenario that utilizes large amounts of representation of this task to the ideal repmonolingual in-domain data. One straightforward resentation of translation task. Experiments and effective solution for unsupervised domain on multi-domain datasets demonstrate that our proposed approach significantly improves the adaptation is to build in-domain synthetic parallel translation accuracy with target-side monolindata via back-translation of monolingual target sengual data, while achieving comparable perfortences (Sennrich et al., 2016a; Zhang et al., 2018b; mance with back-translation. Our implementaDou et al., 2019; Wei et al., 2020). Although this tion is open-sourced at https://github. approach has proven superior effectiveness in excom/zhengxxn/UDA-KNN. ploiting monolingual data, applying it in kNN-MT requires an additional reverse model and brings the 1 Introduction extra cost of generating back-translation, making Non-parametric methods (Gu et al., 2018; Zhang the adaptation of kNN-MT more complicated and et al., 2018a; Bapna and Firat, 2019a; Khandel- time-consuming in practice. wal et al., 2020; Zheng et al., 2021)"
2021.findings-emnlp.358,P16-1162,0,0.272461,"f kNN-MT on unsupervised domain original NMT model to map the token-level adaptation scenario that utilizes large amounts of representation of this task to the ideal repmonolingual in-domain data. One straightforward resentation of translation task. Experiments and effective solution for unsupervised domain on multi-domain datasets demonstrate that our proposed approach significantly improves the adaptation is to build in-domain synthetic parallel translation accuracy with target-side monolindata via back-translation of monolingual target sengual data, while achieving comparable perfortences (Sennrich et al., 2016a; Zhang et al., 2018b; mance with back-translation. Our implementaDou et al., 2019; Wei et al., 2020). Although this tion is open-sourced at https://github. approach has proven superior effectiveness in excom/zhengxxn/UDA-KNN. ploiting monolingual data, applying it in kNN-MT requires an additional reverse model and brings the 1 Introduction extra cost of generating back-translation, making Non-parametric methods (Gu et al., 2018; Zhang the adaptation of kNN-MT more complicated and et al., 2018a; Bapna and Firat, 2019a; Khandel- time-consuming in practice. wal et al., 2020; Zheng et al., 2021)"
2021.findings-emnlp.358,N19-1209,0,0.0395328,"Missing"
2021.findings-emnlp.358,2020.emnlp-main.474,1,0.790641,"s large amounts of representation of this task to the ideal repmonolingual in-domain data. One straightforward resentation of translation task. Experiments and effective solution for unsupervised domain on multi-domain datasets demonstrate that our proposed approach significantly improves the adaptation is to build in-domain synthetic parallel translation accuracy with target-side monolindata via back-translation of monolingual target sengual data, while achieving comparable perfortences (Sennrich et al., 2016a; Zhang et al., 2018b; mance with back-translation. Our implementaDou et al., 2019; Wei et al., 2020). Although this tion is open-sourced at https://github. approach has proven superior effectiveness in excom/zhengxxn/UDA-KNN. ploiting monolingual data, applying it in kNN-MT requires an additional reverse model and brings the 1 Introduction extra cost of generating back-translation, making Non-parametric methods (Gu et al., 2018; Zhang the adaptation of kNN-MT more complicated and et al., 2018a; Bapna and Firat, 2019a; Khandel- time-consuming in practice. wal et al., 2020; Zheng et al., 2021) have recently In this paper, we propose a novel Unsupervised been successfully applied to neural mach"
2021.findings-emnlp.358,N18-1120,0,0.0193051,"domain original NMT model to map the token-level adaptation scenario that utilizes large amounts of representation of this task to the ideal repmonolingual in-domain data. One straightforward resentation of translation task. Experiments and effective solution for unsupervised domain on multi-domain datasets demonstrate that our proposed approach significantly improves the adaptation is to build in-domain synthetic parallel translation accuracy with target-side monolindata via back-translation of monolingual target sengual data, while achieving comparable perfortences (Sennrich et al., 2016a; Zhang et al., 2018b; mance with back-translation. Our implementaDou et al., 2019; Wei et al., 2020). Although this tion is open-sourced at https://github. approach has proven superior effectiveness in excom/zhengxxn/UDA-KNN. ploiting monolingual data, applying it in kNN-MT requires an additional reverse model and brings the 1 Introduction extra cost of generating back-translation, making Non-parametric methods (Gu et al., 2018; Zhang the adaptation of kNN-MT more complicated and et al., 2018a; Bapna and Firat, 2019a; Khandel- time-consuming in practice. wal et al., 2020; Zheng et al., 2021) have recently In thi"
2021.findings-emnlp.358,2021.acl-short.47,1,0.595004,"nnrich et al., 2016a; Zhang et al., 2018b; mance with back-translation. Our implementaDou et al., 2019; Wei et al., 2020). Although this tion is open-sourced at https://github. approach has proven superior effectiveness in excom/zhengxxn/UDA-KNN. ploiting monolingual data, applying it in kNN-MT requires an additional reverse model and brings the 1 Introduction extra cost of generating back-translation, making Non-parametric methods (Gu et al., 2018; Zhang the adaptation of kNN-MT more complicated and et al., 2018a; Bapna and Firat, 2019a; Khandel- time-consuming in practice. wal et al., 2020; Zheng et al., 2021) have recently In this paper, we propose a novel Unsupervised been successfully applied to neural machine transDomain Adaptation framework based on kNN-MT lation (NMT). These approaches complement ad(UDA-kNN). The UDA-kNN aims at directly levervanced NMT models (Sutskever et al., 2014; Bahaging the monolingual target-side data to generate danau et al., 2015; Vaswani et al., 2017; Hassan the corresponding datastore, and encouraging it to et al., 2018) with external memory to alleviate the play a similar role with the real bilingual in-domain performance degradation when translating out-ofdata,"
2021.findings-emnlp.366,Q17-1024,0,0.415371,"resentational invariance across languages. Ji et al. (2020) build 1 Introduction up a universal encoder for different languages via bridge language model pre-training, while Liu et al. Multilingual neural machine translation (NMT) sys- (2021) disentangle positional information in multem concatenates multiple language pairs into one tilingual NMT to obtain language-agnostic represingle neural-based model, enabling translation on sentations. Besides, Gu et al. (2019) point out that multiple language directions (Firat et al., 2016; the conventional multilingual NMT model heavily Ha et al., 2016; Johnson et al., 2017; Kudugunta captures spurious correlations between the output et al., 2019; Arivazhagan et al., 2019b; Zhang language and language invariant semantics due to et al., 2020). Besides, the multilingual NMT sys- the maximum likelihood training objective, making tem can achieve translation on unseen language it hard to generate a reasonable translation in an unpairs in training, and we refer to this setting as seen language. Then they investigate the effectivezero-shot NMT. This finding is promising that ness of decoder pre-training and back-translation zero-shot translation halves the decoding tim"
2021.findings-emnlp.366,2020.tacl-1.47,0,0.0366994,"Missing"
2021.findings-emnlp.366,N19-4009,0,0.0143908,"019): pre-training the decoder as a multilingual language model, then training the MNMT model initialized with the pre-trained decoder; (iii) MNMT-RC (Liu et al., 2021): removing residual connections in an encoder layer to disentangle positional information. We re-implement all baseline methods, following the same experimental settings to make fair comparison with our method. Experimental Details. We choose standard Transformer-base (Vaswani et al., 2017) architecture to conduct experiments on all baseline and proposed methods, with nlayer = 6, nhead = 8, dembed = 512. We use faiseq toolkit1 (Ott et al., 2019) for fast implementations and experiments. We deploy Adam (Kingma and Ba, 2015) (β1 = 0.9, β2 = 0.98) optimizer and train all models with lr = 0.0005, twarmup = 4000, dropout = Datasets. We evaluate the proposed method on two benchmark machine translation datasets, Eu4323 1 https://github.com/pytorch/fairseq Ar, Zh, Ru ↔ En MultiUN ← Ar-Ru → ← → ← → Zero Avg. Parallel Avg. MNMT LM+MNMT MNMT-RS 17.9 22.0 20.8 13.4 29.3 26.1 16.1 20.3 20.3 29.5 42.7 37.9 12.1 24.3 24.2 30.3 42.1 37.4 19.9 30.1 27.8 49.2 48.9 49.9 MNMT+DN (Ours) 24.6 33.0 24.6 47.2 30.0 46.1 34.3 50.1 Model Ar-Zh Ru-Zh BLEU Table"
2021.findings-emnlp.366,P02-1040,0,0.110797,"endix A.2 for detailed derivations. Once we complement P (h|y) into three subtasks mentioned before, this gap could be further reduced, resulting in better performance on zeroshot translation directions. 4 4.1 Language Pairs Train Dev & Test Europarl De-En, Fr-En 1.8M 2000 MultiUN Ar-En, Zh-En, Ru-En 2M 4000 Table 2: Data statistics of Europarl and MultiUN, in which we sub-sampled 2M samples for each languagepair in MultiUN. P (y|h, x)P (h|x) (x,y)∈D∗ h = Dataset Experiments Experimental Settings roparl and MultiUN. The data statistics of two selected datasets are summarized in Table 2. BLEU (Papineni et al., 2002) is used as the metric for evaluating translation quality. For Europarl dataset, we select three European languages, Germany (De), French (Fr) and English (En). We remove all parallel sentences between De and Fr to ensure the zero-shot setting. We use WMT devtest2006 as validation set and test2006 as test set. For MultiUN, four languages are selected, Arabic (Ar), Chinese (Zh), Russian (Ru), and English (En). The selected languages are distributed in various language families, making the zero-shot language transfer more difficult. We use MultiUN standard validation and test sets to report the"
2021.findings-emnlp.366,P18-1006,0,0.0215615,"(h|h) P (h|h) log P (y|h)P (h|x) P (h|h) (4) Eh∼P (h|h) log P (y|h) (x,y)∈D∗ − KL(P (h|h)||P (h|x)) ∗ ∗ = P (Y |X; D , P (h|h)), where we assume that P (y|h, x) ≈ P (y|h) due to the semantic equivalence of languages h and x. With above equation, the original objective is transformed into optimizing three sub-tasks P (h|x), P (y|h) and P (h|h). Incorporating the denoising autoencoder objective into the translation objective of multilingual NMT model helps minimize the KL-divergence terms, thus implicitly maximizing the lower bound of probability distributions of zero-shot directions. Following Ren et al. (2018), the gap between P ∗ (Y |X; D∗ , P (h|h)) and P (Y |X; D∗ ) can be calculated as follow: ∆ := P (Y |X; D∗ ) − P ∗ (Y |X; D∗ , P (h|h)) X X P (h|h)P (y|x) = P (h|h) log P (y|h)P (h|x) (x,y)∈D∗ h X ≈ KL(P (h|h)||P (h|y)), (5) (x,y)∈D∗ where we leverage an additional approximation that P (h|x, y) ≈ P (h|y) due to the semantic equivalence. Refer to Appendix A.2 for detailed derivations. Once we complement P (h|y) into three subtasks mentioned before, this gap could be further reduced, resulting in better performance on zeroshot translation directions. 4 4.1 Language Pairs Train Dev & Test Europar"
2021.findings-emnlp.366,P16-1162,0,0.155921,"Missing"
2021.findings-emnlp.366,2005.mtsummit-papers.11,0,0.239811,"Missing"
2021.findings-emnlp.366,P07-2045,0,0.0140517,"Missing"
2021.findings-emnlp.366,D19-1167,0,0.0581843,"Missing"
2021.findings-emnlp.366,2020.acl-main.148,0,0.0329012,"Missing"
2021.findings-emnlp.366,L16-1561,0,0.0135522,"ion can be significantly alleviated by directly replacing the original German sentence with a noisy target English sentence in training data, thereby guiding the model to learn the correct mapping between language IDs and output language. Besides, we analyze our proposed method by treating pivot language as latent variables and find that our approach actually implicitly maximizes the probability distributions for zero-shot translation directions. We evaluate the proposed method on two public multilingual datasets with several English-centric language-pairs, Europarl (Koehn, 2005) and MultiUN (Ziemski et al., 2016). Experimental results demonstrate that our proposed method not only achieves significant improvement over vanilla multilingual NMT on zero-shot directions, but also outperforms previous state-of-the-art methods. 2 Multilingual NMT The multilingual NMT system (Johnson et al., 2017) combines different language directions into one single translation model. Due to data limitations of non-English languages, multilingual NMT systems are mostly trained on large-scale Englishcentric corpus via maximizing the likelihood over all available language pairs S: X Lm (θ) = log P (y|x, j; θ), (1) (i,j)∈S,(x,"
2021.findings-emnlp.366,2020.acl-main.703,0,0.0405138,"age ID, leading to unreasonable mutual information between language semantic of “X/Y/...” and output language of English. To address this problem, we introduce a denoising sequence-to-sequence task, in which we directly replace the original input sentence with a noisy target English sentence in training data. In this way, previous mutual information can be significantly reduced, while enhancing the relationship between language IDs and output language. Specifically, we simply use all English sentences in parallel data to construct the denoising English corpus DEN via text infilling operation (Lewis et al., 2020). Then we optimize the multilingual NMT model via maximizing the original translation objective Lm (θ) and denoising autoencoder objective Ld (θ): X Ld (θ) = log P (y|y, j; θ), (2) j=&lt;2en>,(y,y)∈DEN where (i, j) ∈ S are the sampled source language La (θ) = Lm (θ) + Ld (θ). (3) ID and target language ID in all available language pairs, Di,j represents for the corresponding paral- Latent Variable Perspective. As for zero-shot lel data, and θ is the model parameter. The tar- translation, we actually aim at directly fitting the get language ID is appended as the initial token probability distribut"
2021.naacl-main.281,N18-1118,0,0.0357623,"Missing"
2021.naacl-main.281,N19-1423,0,0.355819,"f: 版式1(D) shows the illustration of downstream fine-tuning. (A) CST, (B) ISG, and (C) PST. The lower-right sub-figure datasets and results illustrate that our approach can achieve state-of-the-art performance, which is able to outperform a variety of baselines. 2 Related Work Document machine translation (Doc-MT) aims to translate the source sentence into another different language in the presence of additional contextual information. The mainstream advances of this research field can be divided into three lines: uniencoder structure, dual-encoder structure, and pretrained models. Yang et al. (2019) introduce a query-guided capsule networks into document-level translation to capture high-level capsules related to the current source sentence. Ma et al. (2020) proposes a unified encoder to process the concatenated source information that only attends to the source sentence at the top of encoder blocks. Dual-encoder structure. This line of work tends to adopt two encoders or another components to model the source sentences and the document-level contexts. Wang et al. (2017) summarize the source history in a hierarchical way and then integrate the historical representation into translation m"
2021.naacl-main.281,C18-1051,0,0.016388,"evaluate the ability of the mod- uses a hierarchical attention network (HAN) with els to exploit previous source and target sentences. two levels of abstraction: word level abstraction Kuang et al. (2018) utilizes dynamic or topic cache allows attention to words in previous sentences, to model coherence for Doc-MT by capturing con- and sentence level abstraction allows access to reletextual information either from recently translated vant previous sentences. Source and target context sentences or the entire document. Going a step both can be exploited. Voita et al. (2019b) introfurther, they (Kuang and Xiong, 2018) presents an duces a two-pass framework that first translates inter-sentence gate model to encode two adjacent each sentence with a context-agnostic model, and sentences and controls the amount of information then refines it using context of several previous senflowing from the preceding sentence to the transla- tences. Furthermore,Voita et al. (2019a) presents tion of the current sentence with an inter-sentence a monolingual Doc-Repair model that performs gate. Tu et al. (2018) augments translation model automatic post-editing on a sequence of sentencewith a cache-like memory network that sto"
2021.naacl-main.281,C18-1050,1,0.825769,"dexplores multiple different concatenation strate- den variables. Zhang et al. (2018) introduces a gies and proves that the translation with extended light context encoder to represent source context source achieves the best performance. Bawden and performs information fusion with the unidirecet al. (2018) presents several new discourse test- tional multi-head attention. Werlen et al. (2018) sets, which aims to evaluate the ability of the mod- uses a hierarchical attention network (HAN) with els to exploit previous source and target sentences. two levels of abstraction: word level abstraction Kuang et al. (2018) utilizes dynamic or topic cache allows attention to words in previous sentences, to model coherence for Doc-MT by capturing con- and sentence level abstraction allows access to reletextual information either from recently translated vant previous sentences. Source and target context sentences or the entire document. Going a step both can be exploited. Voita et al. (2019b) introfurther, they (Kuang and Xiong, 2018) presents an duces a two-pass framework that first translates inter-sentence gate model to encode two adjacent each sentence with a context-agnostic model, and sentences and controls"
2021.naacl-main.281,D18-2012,0,0.0256094,"PM-based detoken on the generated texts and then use Moses to re-tokenize all texts like the baselines. 4 4.3 i 4.1 Experiments Settings We train Transformer consisting of 12 encoder and 12 decoder layers with 1024 hidden size on 16 heads. We adopt the public mBART.CC25 released by Liu et al. (2020) as the initialization. For CST task, the pre-training data consists of: TED, Europarl, News Commentary and Rapid corpus. The monolingual target documents used in ISG task are extracted from Wikipedia. For PST task, we sample bilingual sentences in NewsCrawl utill 2018. We use sentence piece model (Kudo and Richardson, 2018) to tokenize all data. Gradient accumulation is used to simulate the batch size of 128K tokens. We use Adam optimizer with linear learning rate decay. The learning rate and dropout is set to 3e−5 and 0.1, respectively. We set λ in Eq. 1 to 0.01. We evaluate on three EN-DE Doc-MT datasets provided by Maruf et al. (2019): TED, News, and Europarl and perform limited grid-search of hyperparameter. Main Results Table 2 shows the performance of different systems. Results first confirm that large-scale pre-training can effectively accomplish model transferring and advance the performance of Doc-MT. B"
2021.naacl-main.281,2020.acl-main.322,0,0.120028,"roach can achieve state-of-the-art performance, which is able to outperform a variety of baselines. 2 Related Work Document machine translation (Doc-MT) aims to translate the source sentence into another different language in the presence of additional contextual information. The mainstream advances of this research field can be divided into three lines: uniencoder structure, dual-encoder structure, and pretrained models. Yang et al. (2019) introduce a query-guided capsule networks into document-level translation to capture high-level capsules related to the current source sentence. Ma et al. (2020) proposes a unified encoder to process the concatenated source information that only attends to the source sentence at the top of encoder blocks. Dual-encoder structure. This line of work tends to adopt two encoders or another components to model the source sentences and the document-level contexts. Wang et al. (2017) summarize the source history in a hierarchical way and then integrate the historical representation into translation model Uni-encoder structure. This line of research with multiple strategies. Maruf and Haffari (2018) aims at performing Doc-MT based on a univer- takes both sourc"
2021.naacl-main.281,2020.tacl-1.47,0,0.0134306,"the related capsulese. where λ is a hyperparameter weighting the importance of old LM tasks compared to new MT task, and i labels each parameter. The final loss J for fine-tuning is the sum of negative log-likelihood in all pre-training tasks and newly introduced R, i.e., J = LCST + LISG + LPST + R. We summarize the key information of our approach in Table 1, which also shows the available data of different tasks. Pretrained models. Flat-Transformer (Ma et al., 2020) apply BERT as the initialization of encoder. We also implement the parallel sentence translationbased pre-training with mBART (Liu et al., 2020) initialization as the most comparable baseline. To have a fair comparison, we adopt multiBLEU as the evaluation metric. We first conduct SPM-based detoken on the generated texts and then use Moses to re-tokenize all texts like the baselines. 4 4.3 i 4.1 Experiments Settings We train Transformer consisting of 12 encoder and 12 decoder layers with 1024 hidden size on 16 heads. We adopt the public mBART.CC25 released by Liu et al. (2020) as the initialization. For CST task, the pre-training data consists of: TED, Europarl, News Commentary and Rapid corpus. The monolingual target documents used i"
2021.naacl-main.281,2020.acl-main.321,0,0.216757,"roach can achieve state-of-the-art performance, which is able to outperform a variety of baselines. 2 Related Work Document machine translation (Doc-MT) aims to translate the source sentence into another different language in the presence of additional contextual information. The mainstream advances of this research field can be divided into three lines: uniencoder structure, dual-encoder structure, and pretrained models. Yang et al. (2019) introduce a query-guided capsule networks into document-level translation to capture high-level capsules related to the current source sentence. Ma et al. (2020) proposes a unified encoder to process the concatenated source information that only attends to the source sentence at the top of encoder blocks. Dual-encoder structure. This line of work tends to adopt two encoders or another components to model the source sentences and the document-level contexts. Wang et al. (2017) summarize the source history in a hierarchical way and then integrate the historical representation into translation model Uni-encoder structure. This line of research with multiple strategies. Maruf and Haffari (2018) aims at performing Doc-MT based on a univer- takes both sourc"
2021.naacl-main.281,P18-1118,0,0.0167974,"o capture high-level capsules related to the current source sentence. Ma et al. (2020) proposes a unified encoder to process the concatenated source information that only attends to the source sentence at the top of encoder blocks. Dual-encoder structure. This line of work tends to adopt two encoders or another components to model the source sentences and the document-level contexts. Wang et al. (2017) summarize the source history in a hierarchical way and then integrate the historical representation into translation model Uni-encoder structure. This line of research with multiple strategies. Maruf and Haffari (2018) aims at performing Doc-MT based on a univer- takes both source and target document context into sal Transformer, which takes the concatenation of account using memory networks, which modeling the additional contexts and the source sentence Doc-MT as a structured prediction problem with as the input. Tiedemann and Scherrer (2017) inter-dependencies among the observed and hidexplores multiple different concatenation strate- den variables. Zhang et al. (2018) introduces a gies and proves that the translation with extended light context encoder to represent source context source achieves the best"
2021.naacl-main.281,N19-1313,0,0.249116,"nsla- tences. Furthermore,Voita et al. (2019a) presents tion of the current sentence with an inter-sentence a monolingual Doc-Repair model that performs gate. Tu et al. (2018) augments translation model automatic post-editing on a sequence of sentencewith a cache-like memory network that stores re- level translations to correct inconsistencies among cent hidden representations as translation history. them. Li et al. (2020) investigates multi-encoder 3590 approaches in Doc-MT and find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. Maruf et al. (2019) presents a hierarchical context-aware translation model, which selectively focus on relevant sentences in the document context and then attends to key words in those sentences. 3 Methodology Following prior work (Ma et al., 2020), we translate the i-th source sentence xi into the i-th target sentence yi in the presence of extra source contexts c = (xi−1 , xi+1 ), where xi−1 and xi+1 refer to the predecessor and successor of xi respectively. We adopt Transformer as the model architecture of pre-training and machine translation. The model is trained by minimizing the negative log-likelihood of"
2021.naacl-main.281,P02-1040,0,0.109511,"Missing"
2021.naacl-main.281,W17-4811,0,0.169435,"ines. consists of three pre-training tasks, whose sketch is presented in Figure 1. Specifically, the cross sen1 Introduction tence translation task (CST in Figure 1 (A)) strives to generate the target sentence in the absence of the Document machine translation (Doc-MT) aims at utilizing the surrounding contexts of the source sen- source sentence and only based on the source contence to tackle some linguistic consistency prob- texts. With such a goal, the model is encouraged to maximize the utilization of extra contexts. To caplems (e.g., deixis, ellipsis, and lexical cohesion) in translation (Tiedemann and Scherrer, 2017). How- ture interactions between multiple sentences in the target document so that the discourse phenomena ever, due to the introduction of extra contexts, it can be modeled, we conduct inter sentence genalso presents several intractable challenges: eration (ISG in Figure 1 (B)) that aims to predict (1) Data scarcity of document-level bilingual corpora. Since most bilingual corpora are pre- the inter sentence based on the target surrounding contexts. This task can be regarded as discourse served by sentence, well-aligned document-level data is relatively scarce (Zhang et al., 2018), espe- lang"
2021.naacl-main.281,Q18-1029,0,0.12971,"mains. Such a data sparsity not only impairs the effective train- coder of the translation model. We also introduce ing of neural machine translation (NMT) models, parallel sentence translation (PST in Figure 1 (C)) to alleviate the lack of doc-level bilingual corpora but also tends to result in potential overfitting. and achieve knowledge transfer from abundant sent(2) Effective utilization of valuable information contained in extra contexts. Although some ef- level parallel data to limited doc-level parallel data. In order to avoid the catastrophic forgetting of preforts (Wang et al., 2017; Tu et al., 2018) have trained model in downstream fine-tuning, elastic strived to incorporate contextual information via various architectures, they only observe minor per- weight consolidation (EWC) regularization is introduced to further enhance the model performance. formance gains compared with traditional sentence machine translation (Sent-MT). Recent work (Li We perform the evaluation on three benchmark 3589 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3589–3595 June 6–11, 2021. ©2021 Association for"
2021.naacl-main.281,D19-1081,0,0.158409,"f: 版式1(D) shows the illustration of downstream fine-tuning. (A) CST, (B) ISG, and (C) PST. The lower-right sub-figure datasets and results illustrate that our approach can achieve state-of-the-art performance, which is able to outperform a variety of baselines. 2 Related Work Document machine translation (Doc-MT) aims to translate the source sentence into another different language in the presence of additional contextual information. The mainstream advances of this research field can be divided into three lines: uniencoder structure, dual-encoder structure, and pretrained models. Yang et al. (2019) introduce a query-guided capsule networks into document-level translation to capture high-level capsules related to the current source sentence. Ma et al. (2020) proposes a unified encoder to process the concatenated source information that only attends to the source sentence at the top of encoder blocks. Dual-encoder structure. This line of work tends to adopt two encoders or another components to model the source sentences and the document-level contexts. Wang et al. (2017) summarize the source history in a hierarchical way and then integrate the historical representation into translation m"
2021.naacl-main.281,P19-1116,0,0.198689,"f: 版式1(D) shows the illustration of downstream fine-tuning. (A) CST, (B) ISG, and (C) PST. The lower-right sub-figure datasets and results illustrate that our approach can achieve state-of-the-art performance, which is able to outperform a variety of baselines. 2 Related Work Document machine translation (Doc-MT) aims to translate the source sentence into another different language in the presence of additional contextual information. The mainstream advances of this research field can be divided into three lines: uniencoder structure, dual-encoder structure, and pretrained models. Yang et al. (2019) introduce a query-guided capsule networks into document-level translation to capture high-level capsules related to the current source sentence. Ma et al. (2020) proposes a unified encoder to process the concatenated source information that only attends to the source sentence at the top of encoder blocks. Dual-encoder structure. This line of work tends to adopt two encoders or another components to model the source sentences and the document-level contexts. Wang et al. (2017) summarize the source history in a hierarchical way and then integrate the historical representation into translation m"
2021.naacl-main.281,D17-1301,0,0.0821492,"rce languages or domains. Such a data sparsity not only impairs the effective train- coder of the translation model. We also introduce ing of neural machine translation (NMT) models, parallel sentence translation (PST in Figure 1 (C)) to alleviate the lack of doc-level bilingual corpora but also tends to result in potential overfitting. and achieve knowledge transfer from abundant sent(2) Effective utilization of valuable information contained in extra contexts. Although some ef- level parallel data to limited doc-level parallel data. In order to avoid the catastrophic forgetting of preforts (Wang et al., 2017; Tu et al., 2018) have trained model in downstream fine-tuning, elastic strived to incorporate contextual information via various architectures, they only observe minor per- weight consolidation (EWC) regularization is introduced to further enhance the model performance. formance gains compared with traditional sentence machine translation (Sent-MT). Recent work (Li We perform the evaluation on three benchmark 3589 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3589–3595 June 6–11, 2021. ©20"
2021.naacl-main.281,D18-1325,0,0.24655,"ount using memory networks, which modeling the additional contexts and the source sentence Doc-MT as a structured prediction problem with as the input. Tiedemann and Scherrer (2017) inter-dependencies among the observed and hidexplores multiple different concatenation strate- den variables. Zhang et al. (2018) introduces a gies and proves that the translation with extended light context encoder to represent source context source achieves the best performance. Bawden and performs information fusion with the unidirecet al. (2018) presents several new discourse test- tional multi-head attention. Werlen et al. (2018) sets, which aims to evaluate the ability of the mod- uses a hierarchical attention network (HAN) with els to exploit previous source and target sentences. two levels of abstraction: word level abstraction Kuang et al. (2018) utilizes dynamic or topic cache allows attention to words in previous sentences, to model coherence for Doc-MT by capturing con- and sentence level abstraction allows access to reletextual information either from recently translated vant previous sentences. Source and target context sentences or the entire document. Going a step both can be exploited. Voita et al. (2019b)"
2021.naacl-main.281,D19-1164,0,0.168352,"ks consist of: 版式1(D) shows the illustration of downstream fine-tuning. (A) CST, (B) ISG, and (C) PST. The lower-right sub-figure datasets and results illustrate that our approach can achieve state-of-the-art performance, which is able to outperform a variety of baselines. 2 Related Work Document machine translation (Doc-MT) aims to translate the source sentence into another different language in the presence of additional contextual information. The mainstream advances of this research field can be divided into three lines: uniencoder structure, dual-encoder structure, and pretrained models. Yang et al. (2019) introduce a query-guided capsule networks into document-level translation to capture high-level capsules related to the current source sentence. Ma et al. (2020) proposes a unified encoder to process the concatenated source information that only attends to the source sentence at the top of encoder blocks. Dual-encoder structure. This line of work tends to adopt two encoders or another components to model the source sentences and the document-level contexts. Wang et al. (2017) summarize the source history in a hierarchical way and then integrate the historical representation into translation m"
2021.naacl-main.281,D18-1049,0,0.0810126,"el capsules related to the current source sentence. Ma et al. (2020) proposes a unified encoder to process the concatenated source information that only attends to the source sentence at the top of encoder blocks. Dual-encoder structure. This line of work tends to adopt two encoders or another components to model the source sentences and the document-level contexts. Wang et al. (2017) summarize the source history in a hierarchical way and then integrate the historical representation into translation model Uni-encoder structure. This line of research with multiple strategies. Maruf and Haffari (2018) aims at performing Doc-MT based on a univer- takes both source and target document context into sal Transformer, which takes the concatenation of account using memory networks, which modeling the additional contexts and the source sentence Doc-MT as a structured prediction problem with as the input. Tiedemann and Scherrer (2017) inter-dependencies among the observed and hidexplores multiple different concatenation strate- den variables. Zhang et al. (2018) introduces a gies and proves that the translation with extended light context encoder to represent source context source achieves the best"
2021.naacl-main.310,2020.acl-main.692,0,0.0484041,"Missing"
2021.naacl-main.310,J90-2002,0,0.892835,"ed to the competitive models. In summary, the prime contributions of this paper are as follows: • We propose a novel continual learning framework for neural machine translation. Compared with existing works, we consider a more general scenario where the training is comprised of multiple stages. 2 2.1 Related Works Neural Machine Translation The task of machine translation is to automatically translate a written text from one natural language into another. Early machine translation systems are mostly built upon statistical learning techniques, which mainly rely on various count-based features (Brown et al., 1990; Och, 2003; Koehn et al., 2007). Recently, statistical machine translation (SMT) has largely been superseded by neural machine translation (NMT), which tackles machine translation with deep neural networks (Luong et al., 2015; Vaswani et al., 2017). Most NMT models either use LSTM (Luong et al., 2015) or Transformer (Vaswani et al., 2017) architectures. NMT systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-"
2021.naacl-main.310,W18-2705,0,0.273296,"llation-based methods transfer important knowledge from an old model to a new model • Experimental results in three different settings through a teacher-student framework. Usually, a all show that the proposed method obtains su- modified cross-entropy loss is adopted to preserve perior performance compared to competitive the knowledge of the old model. models.2 In the field of natural language processing, there 2 are some researches on solving catastrophic forCodes and data will be released once this paper gets accepted. getting problem in lifelong learning (Freitag and 3965 Al-Onaizan, 2016; Khayrallah et al., 2018; Saunders et al., 2019; Thompson et al., 2019). However, these works only consider the scenario of one-stage incremental training. To the best of our knowledge, there is no previous work that takes into account the scenario in which the training consists of multiple stages. Domain adaptation learning (or transfer learning) is a task similar to continual learning. The difference is that domain adaptation learning only cares about the performance of in-domain data, while continual learning cares about not only the performance on in-domain data, but also the performance on out-of-domain data. 3"
2021.naacl-main.310,P07-2045,0,0.0258947,"n summary, the prime contributions of this paper are as follows: • We propose a novel continual learning framework for neural machine translation. Compared with existing works, we consider a more general scenario where the training is comprised of multiple stages. 2 2.1 Related Works Neural Machine Translation The task of machine translation is to automatically translate a written text from one natural language into another. Early machine translation systems are mostly built upon statistical learning techniques, which mainly rely on various count-based features (Brown et al., 1990; Och, 2003; Koehn et al., 2007). Recently, statistical machine translation (SMT) has largely been superseded by neural machine translation (NMT), which tackles machine translation with deep neural networks (Luong et al., 2015; Vaswani et al., 2017). Most NMT models either use LSTM (Luong et al., 2015) or Transformer (Vaswani et al., 2017) architectures. NMT systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-domain corpus via continual trai"
2021.naacl-main.310,2015.iwslt-evaluation.11,0,0.0278057,"ne translation (SMT) has largely been superseded by neural machine translation (NMT), which tackles machine translation with deep neural networks (Luong et al., 2015; Vaswani et al., 2017). Most NMT models either use LSTM (Luong et al., 2015) or Transformer (Vaswani et al., 2017) architectures. NMT systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-domain corpus via continual training (Sennrich et al., 2016; Luong and Manning, 2015). However, this suffers from the problem of catastrophic forgetting (French, 1993) that the performance of the model on the general domain has decreased drastically. In this work, we aim to mitigate the catastrophic forgetting for NMT models. As for the bias in NMT systems, Michel and Neubig (2018) 2018 adapt the bias of the output softmax to build a personalized NMT model. Different from their work, we propose to elinamate the bias in the output layer. 2.2 Continual Learning Most of continual learning models are proposed for computer vision tasks. These models mainly fall into parameter-based"
2021.naacl-main.310,D15-1166,0,0.0618651,"general scenario where the training is comprised of multiple stages. 2 2.1 Related Works Neural Machine Translation The task of machine translation is to automatically translate a written text from one natural language into another. Early machine translation systems are mostly built upon statistical learning techniques, which mainly rely on various count-based features (Brown et al., 1990; Och, 2003; Koehn et al., 2007). Recently, statistical machine translation (SMT) has largely been superseded by neural machine translation (NMT), which tackles machine translation with deep neural networks (Luong et al., 2015; Vaswani et al., 2017). Most NMT models either use LSTM (Luong et al., 2015) or Transformer (Vaswani et al., 2017) architectures. NMT systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-domain corpus via continual training (Sennrich et al., 2016; Luong and Manning, 2015). However, this suffers from the problem of catastrophic forgetting (French, 1993) that the performance of the model on the general domain ha"
2021.naacl-main.310,P18-2050,0,0.0174674,"systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-domain corpus via continual training (Sennrich et al., 2016; Luong and Manning, 2015). However, this suffers from the problem of catastrophic forgetting (French, 1993) that the performance of the model on the general domain has decreased drastically. In this work, we aim to mitigate the catastrophic forgetting for NMT models. As for the bias in NMT systems, Michel and Neubig (2018) 2018 adapt the bias of the output softmax to build a personalized NMT model. Different from their work, we propose to elinamate the bias in the output layer. 2.2 Continual Learning Most of continual learning models are proposed for computer vision tasks. These models mainly fall into parameter-based methods (Aljundi et al., 2018; Kirkpatrick et al., 2016; Zenke et al., 2017) and distillation-based methods (Aljundi et al., 2017; • We propose a novel method to alleviate the Triki et al., 2017; Hou et al., 2018, 2019; Wu et al., problem of catastrophic forgetting in a system- 2019). The paramete"
2021.naacl-main.310,N18-1031,0,0.0177045,"inear Projection To reveal the bias weights phenomenon in the linear projection in continual training, we conduct a test that first trains an English-German NMT model on an IT-related corpus, and then fine-tunes it on law-related corpus.3 We find that after fine-tuning on law-related data, the model will no longer generate IT-specific words even we feed an IT-related 3 The number of training samples for IT and law corpus are 232K and 205K respectively. 3967 3.3.2 Weight Normalization for Bias Correction Based on the above observation, we propose to add a weight normalization module similar to Nguyen and Chiang (2018) in the linear projection. Concretly, we normalize the weights for all words by: θˆi = θi / kθi k (10) and compute the probability of generating each word as: Figure 1: The changes of η with the training of Model1 and Model-2. This figure shows that directly finetuning the model on new data will cause the biased weights problem. source sentence to the model. As a consequence, the model performs extremely poorly on the IT test set. We hypothesize that the model reduces the old words’ probability by shrinking their corresponding weights in the last linear projection Θ. To verify this, we train t"
2021.naacl-main.310,P03-1021,0,0.10873,"e models. In summary, the prime contributions of this paper are as follows: • We propose a novel continual learning framework for neural machine translation. Compared with existing works, we consider a more general scenario where the training is comprised of multiple stages. 2 2.1 Related Works Neural Machine Translation The task of machine translation is to automatically translate a written text from one natural language into another. Early machine translation systems are mostly built upon statistical learning techniques, which mainly rely on various count-based features (Brown et al., 1990; Och, 2003; Koehn et al., 2007). Recently, statistical machine translation (SMT) has largely been superseded by neural machine translation (NMT), which tackles machine translation with deep neural networks (Luong et al., 2015; Vaswani et al., 2017). Most NMT models either use LSTM (Luong et al., 2015) or Transformer (Vaswani et al., 2017) architectures. NMT systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-domain corp"
2021.naacl-main.310,N19-4009,0,0.0203816,"or the EWC-based • w/o dynamic knowledge distillation It re- model. moves the dynamic knowledge distillation By comparing results of our model with module from the proposed model. the knowledge distillation-based and EWC regularization-based methods, we can see that our • w/o bias correction It removes the bias cormodel outperforms them in all cases. The proposed rection module from the proposed model. model achieves an average improvement of 0.3 and 0.8 BLEU scores compared to the knowledge 4.3 Implementation Details distillation-based and EWC regularization-based We use the Fairseq toolkit (Ott et al., 2019) to imple- methods, respectively. ment the proposed model. We process the text into The above results confirm the finding of prior subword units by using the subword-nmt toolkit10 . works that the learning-without-forgetting strateWe adopt the transformer (Vaswani et al., 2017) gies can benefit the continual training, and demonas the model architecture. We set the model’s hidstrate that the proposed method adds more gains. den size, feed-forward hidden size to 512, 2048, We also study the effect of α in Eq. 3. A small and set the number of layers and the number of value of α indicates that the"
2021.naacl-main.310,P19-1022,0,0.0346712,"Missing"
2021.naacl-main.310,P16-1009,0,0.0339867,"ntly, statistical machine translation (SMT) has largely been superseded by neural machine translation (NMT), which tackles machine translation with deep neural networks (Luong et al., 2015; Vaswani et al., 2017). Most NMT models either use LSTM (Luong et al., 2015) or Transformer (Vaswani et al., 2017) architectures. NMT systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-domain corpus via continual training (Sennrich et al., 2016; Luong and Manning, 2015). However, this suffers from the problem of catastrophic forgetting (French, 1993) that the performance of the model on the general domain has decreased drastically. In this work, we aim to mitigate the catastrophic forgetting for NMT models. As for the bias in NMT systems, Michel and Neubig (2018) 2018 adapt the bias of the output softmax to build a personalized NMT model. Different from their work, we propose to elinamate the bias in the output layer. 2.2 Continual Learning Most of continual learning models are proposed for computer vision tasks. These models mainly"
2021.naacl-main.310,N19-1209,0,0.0261324,"ge from an old model to a new model • Experimental results in three different settings through a teacher-student framework. Usually, a all show that the proposed method obtains su- modified cross-entropy loss is adopted to preserve perior performance compared to competitive the knowledge of the old model. models.2 In the field of natural language processing, there 2 are some researches on solving catastrophic forCodes and data will be released once this paper gets accepted. getting problem in lifelong learning (Freitag and 3965 Al-Onaizan, 2016; Khayrallah et al., 2018; Saunders et al., 2019; Thompson et al., 2019). However, these works only consider the scenario of one-stage incremental training. To the best of our knowledge, there is no previous work that takes into account the scenario in which the training consists of multiple stages. Domain adaptation learning (or transfer learning) is a task similar to continual learning. The difference is that domain adaptation learning only cares about the performance of in-domain data, while continual learning cares about not only the performance on in-domain data, but also the performance on out-of-domain data. 3 Methods 3.1 Overall Given a bilingual translati"
C04-1183,ahrenberg-etal-2000-evaluation,0,\N,Missing
C04-1183,C02-1002,0,\N,Missing
C04-1183,E03-1026,0,\N,Missing
C04-1183,P03-1058,0,\N,Missing
C04-1183,P00-1050,0,\N,Missing
C08-1014,J93-2003,0,0.0123935,"Missing"
C08-1014,2006.iwslt-papers.4,1,0.884309,"d either from different methods or same decoder with different models, local feature functions of each hypothesis are not directly comparable, and thus inadequate for rescoring. We hence exploit rich global feature functions in the rescoring models to compensate the loss of local feature functions. We apply the following 10 feature functions and optimize the weight of each feature function using the tool in Moses package. • direct and inverse IBM model 1 and 3 • association score, i.e. hyper-geometric distribution probabilities and mutual information • lexicalized word/block reordering rules (Chen et al., 2006) • 6-gram target LM • 8-gram target word-class based LM, wordclasses are clustered by GIZA++ • length ratio between source and target sentence • question feature (Chen et al., 2005) • linear sum of n-grams relative frequencies within N-best translations (Chen et al., 2005) • n-gram posterior probabilities within the Nbest translations (Zens and Ney, 2006) • sentence length posterior probabilities (Zens and Ney, 2006) 108 5 Experiments 5.1 data Tasks Train We carried out two sets of experiments on two different datasets. One is in spoken language domain while the other is on newswire corpus. Bo"
C08-1014,2007.mtsummit-papers.15,1,0.910881,"are generated by the decoder and the 1-best translation is returned after rescored with additional feature functions. e,a m =1 where e is a string of phrases in the target language, f is the source language string of phrases, Figure 2: Structure of a three-pass machine translation system with the new regeneration pass. The original N-best translations list (Nbest1) is expanded to generate a new N-best translations list (N-best2) before the rescoring pass. lation and reordering models that are trained on the source-to-target N-best translations generated in the first pass. N-gram expansion (Chen et al., 2007) regenerates more hypotheses by continuously expanding the partial hypotheses through an n-gram language model trained on the original N-best translations. And confusion network generates new hypotheses based on confusion network decoding (Matusov et al., 2006), where the confusion network is built on the original N-best translations. Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b). Researchers have used confusion network to compute cons"
C08-1014,N03-1017,0,0.0070794,"le system. We explore three different methods to implement the regeneration process: redecoding, n-gram expansion, and confusion network-based regeneration. Experiments on Chinese-to-English NIST and IWSLT tasks show that all three methods obtain consistent improvements. Moreover, the combination of the three strategies achieves further improvements and outperforms the baseline by 0.81 BLEU-score on IWSLT’06, 0.57 on NIST’03, 0.61 on NIST’05 test set respectively. 1 Introduction State-of-the-art Statistical Machine Translation (SMT) systems usually adopt a two-pass search strategy (Och, 2003; Koehn, et al., 2003) as shown in Figure 1. In the first pass, a decoding algorithm is applied to generate an N-best list of translation hypotheses, while in the second pass, the final translation is selected by rescoring and re-ranking the N-best translations through additional feature functions. The fundamental assumption behind using a second pass is that the generated N-best list may contain better transla© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. tions than the best choice foun"
C08-1014,takezawa-etal-2002-toward,0,0.0637026,"ure (Chen et al., 2005) • linear sum of n-grams relative frequencies within N-best translations (Chen et al., 2005) • n-gram posterior probabilities within the Nbest translations (Zens and Ney, 2006) • sentence length posterior probabilities (Zens and Ney, 2006) 108 5 Experiments 5.1 data Tasks Train We carried out two sets of experiments on two different datasets. One is in spoken language domain while the other is on newswire corpus. Both experiments are on Chinese-to-English translation. Experiments on spoken language domain were carried out on the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2002) Chinese- to-English data augmented with HITcorpus 1 . BTEC is a multilingual speech corpus which contains sentences spoken by tourists. 40K sentence-pairs are used in our experiment. HITcorpus is a balanced corpus and has 500K sentence-pairs in total. We selected 360K sentencepairs that are more similar to BTEC data according to its sub-topic. Additionally, the English sentences of Tanaka corpus 2 were also used to train our LM. We ran experiments on an IWSLT 3 challenge track which uses IWSLT2006 4 DEV clean text set as development set and IWSLT-2006 TEST clean text as test set. Table 1 summ"
C08-1014,W06-3110,0,0.342533,"stem can be improved from two aspects, i.e. scoring models and the quality of the N-best hypotheses. Rescoring pass improves the performance of machine translation by enhancing the scoring models with more global sophisticated and discriminative feature functions. The idea for applying two passes instead of one is that some global feature functions cannot be easily decomposed into local scores and computed during decoding. Furthermore, rescoring allows some feature functions, such as word and n-gram posterior probabilities, to be estimated on the N-best list (Ueffing, 2003; Chen et al., 2005; Zens and Ney, 2006). In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b). We have instead chosen to regenerate new hypotheses from the original N-best list, a technique which we call regeneration. Regeneration is an intermediate pass bet"
C08-1014,P07-2045,0,0.0336091,"Missing"
C08-1014,C04-1183,1,0.757767,"are ranked, choosing the first best hypothesis as the skeleton is straightforward in our work. Aligning words: As a confusion network can be easily built from a one-to-one alignment, we develop our algorithm based on the one-to-one assumption and use competitive linking algorithm (Melamed, 2000) for our word alignment. Firstly, an association score is computed for every possible word pair from the skeleton and sentence to be aligned. Then a greedy algorithm is applied to select the best word-alignment. In this paper, we use a linear combination of multiple association scores, as suggested in (Kraif and Chen, 2004). As the two sentences to be aligned are in the same language, the association scores are computed on the following four clues. They are cognate (S1), word class (S2), synonyms (S3), and position difference (S4). The four scores are linearly combined with empirically determined weights as shown is Equation 2. 4 S ( f j , e i ) = ∑ λk × S k (2) k =1 Reordering words: After word alignment, the words in all other hypotheses are reordered to match the word order of the skeleton. The aligned words are reordered according to their alignment indices. The unaligned words are reordered in two strategie"
C08-1014,E06-1005,0,0.607335,"posed into local scores and computed during decoding. Furthermore, rescoring allows some feature functions, such as word and n-gram posterior probabilities, to be estimated on the N-best list (Ueffing, 2003; Chen et al., 2005; Zens and Ney, 2006). In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b). We have instead chosen to regenerate new hypotheses from the original N-best list, a technique which we call regeneration. Regeneration is an intermediate pass between decoding and rescoring as depicted in Figure 2. Given the original N-best list (N-best1) generated by the decoder, this regeneration pass creates new translation hypotheses from this list to form another N-best list (N-best2). These two N-best lists are then combined and given to the rescoring pass to derive the best translation. We implement three methods to regener"
C08-1014,J00-2004,0,0.0187612,"ther hypotheses on average as the skeleton. Bangalore et al. (2001) used a WER based alignment and Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate 107 (TER) based alignment to build the confusion network. Choosing alignment reference: Since the Nbest translations are ranked, choosing the first best hypothesis as the skeleton is straightforward in our work. Aligning words: As a confusion network can be easily built from a one-to-one alignment, we develop our algorithm based on the one-to-one assumption and use competitive linking algorithm (Melamed, 2000) for our word alignment. Firstly, an association score is computed for every possible word pair from the skeleton and sentence to be aligned. Then a greedy algorithm is applied to select the best word-alignment. In this paper, we use a linear combination of multiple association scores, as suggested in (Kraif and Chen, 2004). As the two sentences to be aligned are in the same language, the association scores are computed on the following four clues. They are cognate (S1), word class (S2), synonyms (S3), and position difference (S4). The four scores are linearly combined with empirically determi"
C08-1014,P03-1021,0,0.120474,"from a single system. We explore three different methods to implement the regeneration process: redecoding, n-gram expansion, and confusion network-based regeneration. Experiments on Chinese-to-English NIST and IWSLT tasks show that all three methods obtain consistent improvements. Moreover, the combination of the three strategies achieves further improvements and outperforms the baseline by 0.81 BLEU-score on IWSLT’06, 0.57 on NIST’03, 0.61 on NIST’05 test set respectively. 1 Introduction State-of-the-art Statistical Machine Translation (SMT) systems usually adopt a two-pass search strategy (Och, 2003; Koehn, et al., 2003) as shown in Figure 1. In the first pass, a decoding algorithm is applied to generate an N-best list of translation hypotheses, while in the second pass, the final translation is selected by rescoring and re-ranking the N-best translations through additional feature functions. The fundamental assumption behind using a second pass is that the generated N-best list may contain better transla© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. tions tha"
C08-1014,J03-1002,0,0.00995435,"Missing"
C08-1014,P02-1040,0,0.0763935,"Missing"
C08-1014,N07-1029,0,0.441059,"ing decoding. Furthermore, rescoring allows some feature functions, such as word and n-gram posterior probabilities, to be estimated on the N-best list (Ueffing, 2003; Chen et al., 2005; Zens and Ney, 2006). In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b). We have instead chosen to regenerate new hypotheses from the original N-best list, a technique which we call regeneration. Regeneration is an intermediate pass between decoding and rescoring as depicted in Figure 2. Given the original N-best list (N-best1) generated by the decoder, this regeneration pass creates new translation hypotheses from this list to form another N-best list (N-best2). These two N-best lists are then combined and given to the rescoring pass to derive the best translation. We implement three methods to regenerate new hypotheses: re-decoding, n-gra"
C08-1014,P07-1040,0,0.487111,"ing decoding. Furthermore, rescoring allows some feature functions, such as word and n-gram posterior probabilities, to be estimated on the N-best list (Ueffing, 2003; Chen et al., 2005; Zens and Ney, 2006). In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance. This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b). We have instead chosen to regenerate new hypotheses from the original N-best list, a technique which we call regeneration. Regeneration is an intermediate pass between decoding and rescoring as depicted in Figure 2. Given the original N-best list (N-best1) generated by the decoder, this regeneration pass creates new translation hypotheses from this list to form another N-best list (N-best2). These two N-best lists are then combined and given to the rescoring pass to derive the best translation. We implement three methods to regenerate new hypotheses: re-decoding, n-gra"
C10-1069,N06-1003,0,0.104358,"enk et al. (2007) on continuous space Ngram models, where a neural network is employed to smooth translation probabilities. However, both Schwenk et al.’s smoothing technique 608 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 608–616, Beijing, August 2010 and the system to which it is applied are quite different from ours. Phrase clustering is also somewhat related to work on paraphrases for SMT. Key papers in this area include (Bannard and Callison-Burch, 2005), which pioneered the extraction of paraphrases from bilingual parallel corpora, (Callison-Burch et al., 2006) which showed that paraphrase generation could improve SMT performance, (Callison-Burch, 2008) and (Zhao et al., 2008) which showed how to improve the quality of paraphrases, and (Marton et al., 2009) which derived paraphrases from monolingual data using distributional information. Paraphrases typically help SMT systems trained on under 100K sentence pairs the most. The phrase clustering algorithm in this paper outputs groups of source-language and targetlanguage phrases with similar meanings: paraphrases. However, previous work on paraphrases for SMT has aimed at finding translations for sour"
C10-1069,W08-2119,0,0.0531444,"Missing"
C10-1069,J10-4005,0,0.0154983,"n with small (less than 100K sentence pairs) amounts of training data, contrary to what one would expect from the paraphrasing literature. We have only begun to explore the original smoothing approach described here. 1 Introduction and Related Work The source-language and target-language “phrases” employed by many statistical machine translation (SMT) systems are anomalous: they are arbitrary sequences of contiguous words extracted by complex heuristics from a bilingual corpus, satisfying no formal linguistic criteria. Nevertheless, phrase-based systems perform better than word-based systems (Koehn 2010, pp. 127-129). In this paper, we look at what happens when we cluster together these anomalous but useful entities. Here, we apply phrase clustering to obtain better estimates for “backward” probability P(s|t) and “forward” probability P(t|s), where s is a source-language phrase, t is a target-language phrase, and phrase pair (s,t) was seen at least once in training data. The current work is thus related to work on smoothing P(s|t) and P(t|s) – see (Foster et al., 2006). The relative frequency estimates for P(s|t) and P(t|s) are PRF (s |t ) =# (s, t ) /# t and PRF (t |s ) =# (s , t ) /# s , w"
C10-1069,D08-1076,0,0.0318877,"word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a loglinear feature combination that includes language models, the distortion components, relative frequency estimators PRF(s|t) and PRF(t|s) and lexical weight estimators PLW(s|t) and PLW(t|s). The PLW() components are based on (Zens and Ney, 2004); Foster et al. (2006) found this to be the most effective lexical smoothing technique. The phrasecluster-based components PPC(s|t) and PPC(t|s) are incorporated as additional loglinear feature functions. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 4.1 Data We evaluated our method on C-E and F-E tasks. For each pair, we carried out experiments on training corpora of different sizes. C-E data were from the NIST1 2009 evaluation; all the allowed bilingual corpora except the UN corpus, Hong Kong Hansard and Hong Kong Law corpus were used to estimate the translation model. For C-E, we trained two 5-gram language models: the first on the English side of the parallel data, and the second on the English Gigaword corpus. Our C-E development set is made up mainly of data from the NIST 2005 test set; it also includes some balanced-genre web-text"
C10-1069,P02-1040,0,0.0842533,"reference is provided for each source input sentence. Two language models are used in this task: one is the English side of the parallel data, and the second is the English side of the GigaFrEn corpus. Table 2 summarizes the training, development and test corpora for F-E tasks. 4.2 Amount of clustering and metric For both C-E and E-F, we assumed that phrases seen only once in training data couldn’t be clustered reliably, so we prevented these “count 1” phrases from participating in clustering. The key 2 http://www.statmt.org/wmt10/ 613 Results and discussion Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Our first experiment evaluated the effects of the phrase clustering features given various amounts of training data. Figure 4 gives the BLEU score improvements for the two language pairs, with results for each pair averaged over two test sets (training data size shown as #sentences). The improvement is largest for medium amounts of training data. Since the F-E training data has more words per sentence than C-E, the two peaks would have been closer together if we’d put #words on the x axis: improvements for both tasks peak aroun"
C10-1069,N04-1033,0,0.051161,"partial French phrase cluster 4 We carried out experiments on a standard onepass phrase-based SMT system with a phrase table derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a loglinear feature combination that includes language models, the distortion components, relative frequency estimators PRF(s|t) and PRF(t|s) and lexical weight estimators PLW(s|t) and PLW(t|s). The PLW() components are based on (Zens and Ney, 2004); Foster et al. (2006) found this to be the most effective lexical smoothing technique. The phrasecluster-based components PPC(s|t) and PPC(t|s) are incorporated as additional loglinear feature functions. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 4.1 Data We evaluated our method on C-E and F-E tasks. For each pair, we carried out experiments on training corpora of different sizes. C-E data were from the NIST1 2009 evaluation; all the allowed bilingual corpora except the UN corpus, Hong Kong Hansard and Hong Kong Law corpus were used to estimate the transla"
C10-1069,P08-1089,0,0.0638985,"ver, both Schwenk et al.’s smoothing technique 608 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 608–616, Beijing, August 2010 and the system to which it is applied are quite different from ours. Phrase clustering is also somewhat related to work on paraphrases for SMT. Key papers in this area include (Bannard and Callison-Burch, 2005), which pioneered the extraction of paraphrases from bilingual parallel corpora, (Callison-Burch et al., 2006) which showed that paraphrase generation could improve SMT performance, (Callison-Burch, 2008) and (Zhao et al., 2008) which showed how to improve the quality of paraphrases, and (Marton et al., 2009) which derived paraphrases from monolingual data using distributional information. Paraphrases typically help SMT systems trained on under 100K sentence pairs the most. The phrase clustering algorithm in this paper outputs groups of source-language and targetlanguage phrases with similar meanings: paraphrases. However, previous work on paraphrases for SMT has aimed at finding translations for source-language phrases in the system’s input that weren’t seen during system training. Our approach is completely useless"
C10-1069,D09-1040,0,\N,Missing
C10-1069,D08-1021,0,\N,Missing
C10-1069,D07-1045,0,\N,Missing
C10-1069,W06-1607,1,\N,Missing
C10-1069,P05-1074,0,\N,Missing
C10-1069,P05-1033,0,\N,Missing
C10-1069,P07-1019,0,\N,Missing
E06-2002,2005.mtsummit-posters.19,0,0.0175623,"del-4. Starting from the parallel training corpus, provided with direct and inverted alignments, the socalled union alignment (Och and Ney, 2003) is computed. Phrase-pairs are extracted from each sentence pair which correspond to sub-intervals of the source and target positions, J and I, such that the union alignment links all positions of J into I and all positions of I into J. In general, phrases are extracted with maximum length in the source and target defined by the parameters Jmax and Imax . All such phrase-pairs are efficiently computed by an 2 algorithm with complexity O(lImax Jmax ) (Cettolo et al., 2005). Given all phrase-pairs extracted from the training corpus, lexicon probabilities and fertility probabilities are estimated. Target language models (LMs) used by the decoder and rescoring modules are, respectively, estimated from 3-gram and 4-gram statistics by applying the modified Kneser-Ney smoothing method (Goodman and Chen, 1998). LMs are estimated with an in-house software toolkit which also provides a compact binary representation of the LM which is used by the decoder. 3 Demo Architecture Figure 1 shows the two-layer architecture of the demo. At the bottom lie the programs that provid"
E06-2002,W05-0835,0,0.0137666,"interface of the demo is described. 1 http://www.nist.gov/speech/tests/mt/ http://www.slt.atr.jp/IWSLT2004/ 3 http://www.is.cs.cmu.edu/iwslt2005/ 2 Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f ) can be determined through suitable real valued functions (called features) hr (e, f , a), r = 1 . . . R, and takes the parametric form: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases e˜1 . . . e˜l . For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi , which sets its first position; the tablet f˜, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any 91 source word. Moreover, uncovered source positions are associated to a special target word (null) according to specific fertility and permutatio"
E06-2002,N03-1017,0,0.00726484,"ion 5 the Web-based interface of the demo is described. 1 http://www.nist.gov/speech/tests/mt/ http://www.slt.atr.jp/IWSLT2004/ 3 http://www.is.cs.cmu.edu/iwslt2005/ 2 Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f ) can be determined through suitable real valued functions (called features) hr (e, f , a), r = 1 . . . R, and takes the parametric form: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases e˜1 . . . e˜l . For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi , which sets its first position; the tablet f˜, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any 91 source word. Moreover, uncovered source positions are associated to a special target word (null) according to spe"
E06-2002,J03-1002,0,0.00280669,"of generated theories some approximations are introduced during the search: less promising theories are pruned off (beam search) and a new source position is selected by limiting the number of vacant positions on the left-hand and the distance from the left most vacant position (re-ordering constraints). 2.3 Phrase extraction and model training Training of the phrase-based translation model requires a parallel corpus provided with wordalignments in both directions, i.e. from source to target positions, and viceversa. This preprocessing step can be accomplished by applying the GIZA++ toolkit (Och and Ney, 2003) that provides Viterbi alignments based on IBM Model-4. Starting from the parallel training corpus, provided with direct and inverted alignments, the socalled union alignment (Och and Ney, 2003) is computed. Phrase-pairs are extracted from each sentence pair which correspond to sub-intervals of the source and target positions, J and I, such that the union alignment links all positions of J into I and all positions of I into J. In general, phrases are extracted with maximum length in the source and target defined by the parameters Jmax and Imax . All such phrase-pairs are efficiently computed b"
E06-2002,takezawa-etal-2002-toward,0,0.0141419,"concerned, all the languages are encoded in UTF8: this allows to manage the processing phase in an uniform way and to render graphically different character sets. 4 The supported language-pairs Although there is no theoretical limit to the number of supported language-pairs, the current version of the demo provides translations to English from three source languages: Arabic, Chinese and The Arabic-to-English system has been trained with the data provided by the International Workshop on Spoken Language Translation 2005 The context is that of the Basic Traveling Expression Corpus (BTEC) task (Takezawa et al., 2002). BTEC is a multilingual speech corpus which contains sentences coming from phrase books for tourists. Training set includes 20k sentences containing 159K Arabic and 182K English running words; vocabulary size is 18K for Arabic, 7K for English. Chinese-to-English (Newswire) The Chinese-to-English system has been trained with the data provided by the NIST MT Evaluation Campaign 2005 , large-data condition. In this case parallel data are mainly news-wires provided by news agencies. Training set includes 71M Chinese and 77M English running words; vocabulary size is 157K for Chinese, 214K for Engl"
E06-2002,J96-1002,0,0.0194882,"se-based Statistical Machine Translation system which can be accessed by means of a Web page. Section 2 presents the general log-linear framework to SMT and gives an overview of our phrase-based SMT system. In section 3 the software architecture of the demo is outlined. Section 4 focuses on the currently supported language-pairs: Arabic-to-English, Chinese-toEnglish and Spanish-to-English. In section 5 the Web-based interface of the demo is described. 1 http://www.nist.gov/speech/tests/mt/ http://www.slt.atr.jp/IWSLT2004/ 3 http://www.is.cs.cmu.edu/iwslt2005/ 2 Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f ) can be determined through suitable real valued functions (called features) hr (e, f , a), r = 1 . . . R, and takes the parametric form: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases e˜1 . . . e˜l . For each target phrase e˜ the corresponding source phrase within the source string is"
E06-2002,J93-2003,0,0.00522389,"and Spanish-to-English. In section 5 the Web-based interface of the demo is described. 1 http://www.nist.gov/speech/tests/mt/ http://www.slt.atr.jp/IWSLT2004/ 3 http://www.is.cs.cmu.edu/iwslt2005/ 2 Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f ) can be determined through suitable real valued functions (called features) hr (e, f , a), r = 1 . . . R, and takes the parametric form: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases e˜1 . . . e˜l . For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi , which sets its first position; the tablet f˜, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any 91 source word. Moreover, uncovered source positions are associated to a special targ"
E06-2002,2005.iwslt-1.11,1,\N,Missing
E06-2002,P03-1021,0,\N,Missing
E14-1064,C10-1081,0,0.0159755,"cern here is different ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) to improve translation, e.g., through improving reordering. Also, Chen et al. (2010) have leveraged sense similarity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem & Approach 3.1 Consis"
E14-1064,D08-1014,0,0.0607651,"Missing"
E14-1064,P07-1123,0,0.407461,"bilingual sentiment-annotated data for our study. It suits our purpose here of exploring the basic role of sentiment for translation. Also, such a method has been reported to achieve a good cross-domain performance (Taboada et al., 2011) comparable with that of other state-of-the-art models. Translation for sentiment analysis A very interesting line of research has leveraged labeled data in a resource-rich language (e.g., English) to help sentiment analysis in a resource-poorer language. This includes the idea of constructing sentiment lexicons automatically by using a translation dictionary (Mihalcea et al., 2007), as well as the idea of utilizing parallel corpora or automatically translated documents to incorporate sentiment-labeled data from different languages (Wan, 2009; Mihalcea et al., 2007). Our concern here is different ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation Th"
E14-1064,P02-1038,0,0.157896,"ine the role of sentiment consistency in two ways: designing features for the translation model and using them for re-ranking. Before discussing the details of our features, we briefly recap phrase-based SMT for completeness. Given a source sentence f, the goal of statistical machine translation is to select a target language string e which maximizes the posterior probability P(e|f). In a phrase-based SMT system, the translation unit is the phrases, where a ""phrase"" is a sequence of words. Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable a (Brown et al., 1993). ~ M (2) e~*  arg max (m1 m H m (e~, f , a))  F2: if only one side contains sentiment units;  F3: if the source side contains sentiment units;  F4: if the target side contains sentiment units. Sentiment polarity The second group of features check the sentiment polarity. These features are still binary; they check if the polarities of the source and target side are the same.  F5: if the two sides of the pair (f, e) have the same polarity;  F6: if at least one side has a neutral sentiment;  F7: if the polarity is"
E14-1064,J93-2003,0,0.0888328,"r the translation model and using them for re-ranking. Before discussing the details of our features, we briefly recap phrase-based SMT for completeness. Given a source sentence f, the goal of statistical machine translation is to select a target language string e which maximizes the posterior probability P(e|f). In a phrase-based SMT system, the translation unit is the phrases, where a ""phrase"" is a sequence of words. Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable a (Brown et al., 1993). ~ M (2) e~*  arg max (m1 m H m (e~, f , a))  F2: if only one side contains sentiment units;  F3: if the source side contains sentiment units;  F4: if the target side contains sentiment units. Sentiment polarity The second group of features check the sentiment polarity. These features are still binary; they check if the polarities of the source and target side are the same.  F5: if the two sides of the pair (f, e) have the same polarity;  F6: if at least one side has a neutral sentiment;  F7: if the polarity is opposite on the two sides, i.e., one is positive and one is negative. e,"
E14-1064,P03-1021,0,0.0266612,"ent polarity The second group of features check the sentiment polarity. These features are still binary; they check if the polarities of the source and target side are the same.  F5: if the two sides of the pair (f, e) have the same polarity;  F6: if at least one side has a neutral sentiment;  F7: if the polarity is opposite on the two sides, i.e., one is positive and one is negative. e,a where e~ is a string of phrases in the target language, ~ f is the source language string, ~ H m (e~, f , a) are feature functions, and weights m are typically optimized to maximize the scoring function (Och, 2003). 3.4 Feature design In Section 3.2 above, we have discussed our lexicon-based approach, which leverages lexiconbased sentiment consistency. Below, we describe the specific features we designed for our experi~ ments. For a phrase pair ( f , e~ ) or a sentence pair (f, e) 6 , we propose the following four groups of consistency features. Subjectivity The first group of features is designed to check the subjectivity of a phrase or a sentence pair (f, e). This set of features examines if the source or target side contains sentiment units. As the name suggests, these features only capture if subjec"
E14-1064,P05-1033,0,0.120749,"a et al., 2011) and (Zhang et al., 2012), for our bilingual task. We suspect that better sentiment modeling may further improve the general translation performance or the quality of sentiment in translation. We will discuss some directions we think interesting in the future work section. 3.3 Incorporating sentiment consistency into phrase-based SMT In this paper, we focus on exploring sentiment consistency for phrase-based SMT. However, the approach might be used in other translation framework. For example, consistency may be considered in the variables used in hierarchical translation rules (Chiang, 2005). 4 The expression “very not good” is ungrammatical in English. However, in Chinese, it is possible to have this kind of expression, such as “很不漂亮”, whose transliteration is “very not beautiful”, meaning “very ugly”. 5 Note that when sentiment-annotated training data are available, merg(.) can be trained, e.g., if assuming it to be the widely-used (log-) linear form. 609 We will examine the role of sentiment consistency in two ways: designing features for the translation model and using them for re-ranking. Before discussing the details of our features, we briefly recap phrase-based SMT for co"
E14-1064,W02-1011,0,0.0234612,"Missing"
E14-1064,P10-1086,1,0.855261,"e SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) to improve translation, e.g., through improving reordering. Also, Chen et al. (2010) have leveraged sense similarity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem & Approach 3.1 Consistency of sentiment Ideally, sentiment should be properly preserved in high-quality translation. An inte"
E14-1064,P02-1040,0,0.0929004,"-toEnglish translation task. The training data are from NIST Open MT 2012. All allowed bilingual corpora were used to train the translation model and reordering models. There are about 283M target word tokens. The development (dev) set comprised mainly data from the NIST 2005 test set, and also some balanced-genre web-text from NIST training data. Evaluation was performed on NIST 2006 and 2008, which have 1,664 and 1,357 sentences, 39.7K and 33.7K source words respectively. Four references were provided for all dev and test sets. 4.2 Results Our evaluation metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores on two test sets NIST06 and NIST08. Following (Koehn, 2004), we use the bootstrap resampling test to do significance testing. In Table 4-6, the sign * and ** denote statistically significant gains over the baseline at the p &lt; 0.05 and p &lt; 0.01 level, respectively. 8 The Stanford POS tagger (Toutanova et al., 2003) was used to tag phrase and sentence pairs for this purpose. 611 NIST06 Baseline 35.1 +feat. group1 35.6** +feat. group2 35.3* +feat. group3 35.3 +feat. group4 35.5* +feat. group1+2 35.8** +feat. group1+2+3 36.1**"
E14-1064,2011.mtsummit-papers.30,1,0.743356,"r of them;  F14: if both sides have an odd number of negation words not appearing outside any sentiment units, or if both sides have an even number of such negation words; 4 Experiments 4.1 Translation experimental settings Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). We conducted experiments on NIST Chinese-toEnglish translation task. The training data are from NIST Open MT 2012. All allowed bilingual corpora were used to train the translation model and reordering models. There are about 2"
E14-1064,J11-2001,0,0.0726138,"r Computational Linguistics, pages 607–615, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics The granularities of text have spanned from words and phrases to passages and documents. Sentiment analysis has been approached mainly as an unsupervised or supervised problem, although the middle ground, semi-supervised approaches, exists. In this paper, we take a lexiconbased, unsupervised approach to considering sentiment consistency for translation, although the translation system itself is supervised. The advantages of such an approach have been discussed in (Taboada et al., 2011). Briefly, it is good at capturing the basic sentiment expressions common to different domains, and certainly it requires no bilingual sentiment-annotated data for our study. It suits our purpose here of exploring the basic role of sentiment for translation. Also, such a method has been reported to achieve a good cross-domain performance (Taboada et al., 2011) comparable with that of other state-of-the-art models. Translation for sentiment analysis A very interesting line of research has leveraged labeled data in a resource-rich language (e.g., English) to help sentiment analysis in a resource"
E14-1064,N12-1047,0,0.0188096,"BM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). We conducted experiments on NIST Chinese-toEnglish translation task. The training data are from NIST Open MT 2012. All allowed bilingual corpora were used to train the translation model and reordering models. There are about 283M target word tokens. The development (dev) set comprised mainly data from the NIST 2005 test set, and also some balanced-genre web-text from NIST training data. Evaluation was performed on NIST 2006 and 2008, which have 1,664 and 1,357 sentences, 39.7K and 33.7K source words respectively. Four references were provided for all dev and test sets. 4.2 Results Our evalua"
E14-1064,N03-1033,0,0.0671445,"Missing"
E14-1064,P02-1053,0,0.01648,"Missing"
E14-1064,D08-1089,0,0.0214746,"ppearing outside any sentiment units, or if both sides have an even number of such negation words; 4 Experiments 4.1 Translation experimental settings Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). We conducted experiments on NIST Chinese-toEnglish translation task. The training data are from NIST Open MT 2012. All allowed bilingual corpora were used to train the translation model and reordering models. There are about 283M target word tokens. The development (dev) set comprised mainly data from the"
E14-1064,C96-2141,0,0.499625,"Missing"
E14-1064,P97-1023,0,0.0462066,"Missing"
E14-1064,P09-1027,0,0.0493368,"chieve a good cross-domain performance (Taboada et al., 2011) comparable with that of other state-of-the-art models. Translation for sentiment analysis A very interesting line of research has leveraged labeled data in a resource-rich language (e.g., English) to help sentiment analysis in a resource-poorer language. This includes the idea of constructing sentiment lexicons automatically by using a translation dictionary (Mihalcea et al., 2007), as well as the idea of utilizing parallel corpora or automatically translated documents to incorporate sentiment-labeled data from different languages (Wan, 2009; Mihalcea et al., 2007). Our concern here is different ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging se"
E14-1064,W04-3250,0,0.099985,"odel and reordering models. There are about 283M target word tokens. The development (dev) set comprised mainly data from the NIST 2005 test set, and also some balanced-genre web-text from NIST training data. Evaluation was performed on NIST 2006 and 2008, which have 1,664 and 1,357 sentences, 39.7K and 33.7K source words respectively. Four references were provided for all dev and test sets. 4.2 Results Our evaluation metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores on two test sets NIST06 and NIST08. Following (Koehn, 2004), we use the bootstrap resampling test to do significance testing. In Table 4-6, the sign * and ** denote statistically significant gains over the baseline at the p &lt; 0.05 and p &lt; 0.01 level, respectively. 8 The Stanford POS tagger (Toutanova et al., 2003) was used to tag phrase and sentence pairs for this purpose. 611 NIST06 Baseline 35.1 +feat. group1 35.6** +feat. group2 35.3* +feat. group3 35.3 +feat. group4 35.5* +feat. group1+2 35.8** +feat. group1+2+3 36.1** +feat. group1+2+3+4 36.2** NIST08 28.4 29.0** 28.7* 28.7* 28.8* 29.1** 29.3** 29.4** Avg. 31.7 32.3 32.0 32.0 32.1 32.5 32.7 32.8"
E14-1064,H05-1044,0,0.113648,"istency, we use a lexicon-based approach to sentiment analysis. Based on this, we design four groups of features to represent the consistency. The basic idea of the lexicon-based approach is first identifying the sentiment words, intensifiers, and negation words with lexicons, and then calculating the sentiment value using manually designed formulas. To this end, we adapted the approaches of (Taboada et al., 2011) and (Zhang et al., 2012) so as to use the same formulas to analyze the sentiment on both the source and the target side. The English and Chinese sentiment lexicons we used are from (Wilson et al. 2005) and (Xu and Lin, 2007), respectively. We further use 75 English in608 tensifiers listed in (Benzinger, 1971; page 171) and 81 Chinese intensifiers from (Zhang et al., 2012). We use 17 English and 13 Chinese negation words. Similar to (Taboada et al., 2011) and (Zhang et al., 2012), we assigned a numerical score to each sentiment word, intensifier, and negation word. More specifically, one of the five values: -0.8, -0.4, 0, 0.4, and 0.8, was assigned to each sentiment word in both the source and target sentiment lexicons, according to the strength information annotated in these lexicons. The s"
E14-1064,N09-2004,0,0.0233717,"al., 2007). Our concern here is different ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) to improve translation, e.g., through improving reordering. Also, Chen et al. (2010) have leveraged sense similarity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem"
E14-1064,P07-2045,0,0.00417343,"ures here additionally check the counts of negation words. This group of features is binary and triggered by the following conditions.  F12: if neither side of the pair (f, e) contain negation words;  F13: if both sides have an odd number of negation words or both sides have an even number of them;  F14: if both sides have an odd number of negation words not appearing outside any sentiment units, or if both sides have an even number of such negation words; 4 Experiments 4.1 Translation experimental settings Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The syst"
E14-1064,N13-1060,0,0.0147683,"― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) to improve translation, e.g., through improving reordering. Also, Chen et al. (2010) have leveraged sense similarity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem & Approach 3.1 Consistency of sentiment"
E14-1064,W12-3102,0,\N,Missing
K16-1031,P15-1061,0,0.0135941,"others. There is no previous work considering the scenario where only a tiny amount of in-domain data is available: this is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent, dense and low-dimensional word embeddings as inputs. (Kim, 2014) defines a one-layer CNN architecture with comparable performance to (Kalchbrenner et al., 2014)"
K16-1031,D11-1033,0,0.762012,"the test domain (in-domain). However, manually creating training data to match the test domain is not a preferred solution, because 1) sometimes the test domain is not known when training the model, and it could change from sentence to sentence; 2) even if the test domain is pre-determined, the resources required and slow turnaround in data collection process will still delay the system development process. Therefore, training data selection is widely used for domain adaptation in statistical machine translation (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015). Data selection techniques select monolingual or bilingual data that are similar to the indomain seed data based on some criteria, which are incorporated into the training data. The most successful data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011) train n-gram language models on in-domain text to select similar sentences from the large general-domain corpora according to the cross entropy. Furthermore, (Duh et al., 2013) obtained some gains by extending these approaches from n-gram models to recurrent neural network language model"
K16-1031,W15-3003,0,0.201493,"nually creating training data to match the test domain is not a preferred solution, because 1) sometimes the test domain is not known when training the model, and it could change from sentence to sentence; 2) even if the test domain is pre-determined, the resources required and slow turnaround in data collection process will still delay the system development process. Therefore, training data selection is widely used for domain adaptation in statistical machine translation (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015). Data selection techniques select monolingual or bilingual data that are similar to the indomain seed data based on some criteria, which are incorporated into the training data. The most successful data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011) train n-gram language models on in-domain text to select similar sentences from the large general-domain corpora according to the cross entropy. Furthermore, (Duh et al., 2013) obtained some gains by extending these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). To train the in"
K16-1031,D12-1025,0,0.0209149,"phrase pair based on vector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to compute weights for each instance in the training data; it can be applied to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010). If in-domain comparable data are available, (Daume III and Jagarlamudi, 2011; Irvine et al., 2013) propose mining translations from the comparable data to translate out-of-vocabulary (OOV) words and capture new senses for the new test domains. (Dou and Knight, 2012; Zhang and Zong, 2013) learn bilingual lexical or phrase tables from in-domain monolingual data with a decipherment method, then incorporate them into the SMT system. All the above approaches assume that either there is an in-domain (mono-lingual, parallel, or comparable) data set with a reasonable size available, or that some sub-corpora are closer to the test domain than others. There is no previous work considering the scenario where only a tiny amount of in-domain data is available: this is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wh"
K16-1031,P13-2119,0,0.586118,"main). However, manually creating training data to match the test domain is not a preferred solution, because 1) sometimes the test domain is not known when training the model, and it could change from sentence to sentence; 2) even if the test domain is pre-determined, the resources required and slow turnaround in data collection process will still delay the system development process. Therefore, training data selection is widely used for domain adaptation in statistical machine translation (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015). Data selection techniques select monolingual or bilingual data that are similar to the indomain seed data based on some criteria, which are incorporated into the training data. The most successful data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011) train n-gram language models on in-domain text to select similar sentences from the large general-domain corpora according to the cross entropy. Furthermore, (Duh et al., 2013) obtained some gains by extending these approaches from n-gram models to recurrent neural network language models (Mikolov et al.,"
K16-1031,W09-0432,0,0.032288,"used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Sch¨utze, 2015; Wang et al., 2015). 2 2.1 Related Work SMT adaptation techniques Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual parallel data and in-domain monolingual data. An MT system is first trained on bilingual general-domain data, then it is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data for language model or translation model training. Some early data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010) use in-domain monolingual data to select monolingual or bilingual data that are similar to the in-domain data according to some criterion. The state-of-th"
K16-1031,P15-2030,0,0.0196367,"t. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent, dense and low-dimensional word embeddings as inputs. (Kim, 2014) defines a one-layer CNN architecture with comparable performance to (Kalchbrenner et al., 2014). The word embeddings input to the CNN can be pre-trained, and treated as fixed input, or tuned for a specific task. (Johnson and Zhang, 2015b) extends their “one-hot” CNN in (Johnson and Zhang, 2015a) to take region embeddings trained on unlabeled dat"
K16-1031,W07-0717,0,0.0479909,"e and randomly select the same number of sentences from the general-domain training data as the negative sample to form the training sample for training the CNN classification model. This is a typical supervised learning setting. To compensate the limit of in-domain data size, we use word2vec (Mikolov et al., 2013) to learn the word embedding from a large amount of general-domain data. Together with the labeled data, these word embed315 and out-domain data, respectively. Data weighting approaches weight each data item according to its relevance to the in-domain data. Mixture model adaptation (Foster and Kuhn, 2007; Foster et al., 2010; Sennrich, 2012; Foster et al., 2013) assumes that the general-domain data can be clustered to several sub-corpora, with some parts that are not too far from test domain. It combines sub-models trained on different sub-corpus data sets linearly or log-linearly with different weights. Vector space model adaptation (Chen et al., 2013) has the same assumption, and it weights each phrase pair based on vector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to"
K16-1031,D10-1044,0,0.135851,"Missing"
K16-1031,P13-1126,1,0.925672,"Missing"
K16-1031,2013.mtsummit-papers.23,1,0.779631,"general-domain training data as the negative sample to form the training sample for training the CNN classification model. This is a typical supervised learning setting. To compensate the limit of in-domain data size, we use word2vec (Mikolov et al., 2013) to learn the word embedding from a large amount of general-domain data. Together with the labeled data, these word embed315 and out-domain data, respectively. Data weighting approaches weight each data item according to its relevance to the in-domain data. Mixture model adaptation (Foster and Kuhn, 2007; Foster et al., 2010; Sennrich, 2012; Foster et al., 2013) assumes that the general-domain data can be clustered to several sub-corpora, with some parts that are not too far from test domain. It combines sub-models trained on different sub-corpus data sets linearly or log-linearly with different weights. Vector space model adaptation (Chen et al., 2013) has the same assumption, and it weights each phrase pair based on vector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to compute weights for each instance in the training data; it"
K16-1031,2014.amta-researchers.10,1,0.823006,"Missing"
K16-1031,D13-1109,0,0.0494064,"Missing"
K16-1031,P15-1017,0,0.0161462,"a is available: this is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent, dense and low-dimensional word embeddings as inputs. (Kim, 2014) defines a one-layer CNN architecture with comparable performance to (Kalchbrenner et al., 2014). The word embeddings input to the CNN can be pre-trained, and treated as fixed input, or tuned fo"
K16-1031,N15-1011,0,0.463172,"there is an in-domain (mono-lingual, parallel, or comparable) data set with a reasonable size available, or that some sub-corpora are closer to the test domain than others. There is no previous work considering the scenario where only a tiny amount of in-domain data is available: this is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent"
K16-1031,P11-2071,0,0.0602114,"Missing"
K16-1031,P15-1026,0,0.0312173,"odel to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent, dense and low-dimensional word embeddings as inputs. (Kim, 2014) defines a one-layer CNN architecture with comparable performance to (Kalchbrenner et al., 2014). The word embeddings input to the CNN can be pre-trained, and treated as fixed input, or tuned for a specific task. (Johnson and Zhang, 2015b) extends their “one-hot” CNN in (Johnson"
K16-1031,P14-1062,0,0.357216,"augment the original model with semi-supervised convolutional neural networks for domain classification. Convolutional neural networks (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolution layers; each computation unit processes a small region of the input data. CNN has been very successful on image classification. When applying it to text input, the convolution layers process small regions of a document, i.e., a sequence of sentences or words. CNN has been gaining attention, and is now used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Sch¨utze, 2015; Wang et al., 2015). 2 2.1 Related Work SMT adaptation techniques Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual paralle"
K16-1031,D14-1181,0,0.00450541,"tem. All the above approaches assume that either there is an in-domain (mono-lingual, parallel, or comparable) data set with a reasonable size available, or that some sub-corpora are closer to the test domain than others. There is no previous work considering the scenario where only a tiny amount of in-domain data is available: this is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes"
K16-1031,D14-1162,0,0.080122,"entence is represented as a sequence of d-dimensional vectors, which is the input to a convolution network that generates feature vectors for each text segment. The segment vectors and one-hot vectors are fed into another convolution layer, which outputs the classification labels. The second network is trained with the labeled indomain/out-domain data. Therefore, Equation 1 is replaced with: One-hot CNN When applying CNN to NLP tasks, the first layer of the network takes word embeddings as input. Word embeddings can be pre-trained using tools such as word2vec (Mikolov et al., 2013) or GloV e (Pennington et al., 2014), in which case a table lookup is enough. Alternatively, these vectors can be learned from scratch as a step in the network training process. When there are enough in-domain data, training in-domain word embeddings is meaningful. However, when the in-domain data are limited, the word embeddings learned from these data are unreliable. In this case, the input sentence x can be represented with one-hot vectors where each vector’s length is the vocabulary size, value 1 at index i indicates word i appears in the sentence, and 0 indicates its absence. A CNN with one-hot vector input is called “one-h"
K16-1031,W04-3250,0,0.234449,"lection by recurrent neural network LM, with the RNNLM Toolkit (Duh et al., 2013) 4. comblm: Data selection by the combined LM using ngram & rnnlm (equal weight) (Duh et al., 2013). All systems are trained with a standard phrasebased SMT system with standard settings, i.e., GIZA++ alignment, phrase table Kneser-Ney smoothing, hierarchical reordering models, target side 4-gram language model, “gigaword” 5-gram language model for systems with English as the target language, etc. 5.3 Experimental results We evaluated the system using BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we use the bootstrap resampling test to do significance testing. Table 2 summarizes the results and numbers of the selected sentences for each task. First, we can see that all the data selection methods improved the performance over the baseline “alldata” with much less 6 The code and scripts for the three baselines are available at http://cl.naist.jp/ kevinduh/a/acl2013/. 7 For small amounts of data, Witten-Bell smoothing had performed better than Kneser-Ney smoothing in our experiments. 319 alldata ngram rnnlm comblm ohcnn sscnn zh2en #sent BLEU 12.2M 22.9 300K 25.3** 300K 25.6** 400K 25.7"
K16-1031,2008.iwslt-papers.6,0,0.0537453,"on, and is now used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Sch¨utze, 2015; Wang et al., 2015). 2 2.1 Related Work SMT adaptation techniques Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual parallel data and in-domain monolingual data. An MT system is first trained on bilingual general-domain data, then it is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data for language model or translation model training. Some early data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010) use in-domain monolingual data to select monolingual or bilingual data that are similar to the in-domain data according to s"
K16-1031,E12-1055,0,0.0452885,"tences from the general-domain training data as the negative sample to form the training sample for training the CNN classification model. This is a typical supervised learning setting. To compensate the limit of in-domain data size, we use word2vec (Mikolov et al., 2013) to learn the word embedding from a large amount of general-domain data. Together with the labeled data, these word embed315 and out-domain data, respectively. Data weighting approaches weight each data item according to its relevance to the in-domain data. Mixture model adaptation (Foster and Kuhn, 2007; Foster et al., 2010; Sennrich, 2012; Foster et al., 2013) assumes that the general-domain data can be clustered to several sub-corpora, with some parts that are not too far from test domain. It combines sub-models trained on different sub-corpus data sets linearly or log-linearly with different weights. Vector space model adaptation (Chen et al., 2013) has the same assumption, and it weights each phrase pair based on vector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to compute weights for each instance in"
K16-1031,J07-1003,0,0.030816,"as been gaining attention, and is now used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Sch¨utze, 2015; Wang et al., 2015). 2 2.1 Related Work SMT adaptation techniques Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual parallel data and in-domain monolingual data. An MT system is first trained on bilingual general-domain data, then it is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data for language model or translation model training. Some early data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010) use in-domain monolingual data to select monolingual or bilingual data that are similar to the in-domain data"
K16-1031,P15-2058,0,0.124317,"ication. Convolutional neural networks (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolution layers; each computation unit processes a small region of the input data. CNN has been very successful on image classification. When applying it to text input, the convolution layers process small regions of a document, i.e., a sequence of sentences or words. CNN has been gaining attention, and is now used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Sch¨utze, 2015; Wang et al., 2015). 2 2.1 Related Work SMT adaptation techniques Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual parallel data and in-domain monolingual data. An MT system is first trained on bilingual general"
K16-1031,D07-1036,0,0.150423,"Missing"
K16-1031,I08-2088,0,0.15319,"r performance by adapting the SMT system to the test domain (in-domain). However, manually creating training data to match the test domain is not a preferred solution, because 1) sometimes the test domain is not known when training the model, and it could change from sentence to sentence; 2) even if the test domain is pre-determined, the resources required and slow turnaround in data collection process will still delay the system development process. Therefore, training data selection is widely used for domain adaptation in statistical machine translation (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015). Data selection techniques select monolingual or bilingual data that are similar to the indomain seed data based on some criteria, which are incorporated into the training data. The most successful data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011) train n-gram language models on in-domain text to select similar sentences from the large general-domain corpora according to the cross entropy. Furthermore, (Duh et al., 2013) obtained some gains by extending these approaches from n-gram mode"
K16-1031,D09-1074,0,0.113093,"ta can be clustered to several sub-corpora, with some parts that are not too far from test domain. It combines sub-models trained on different sub-corpus data sets linearly or log-linearly with different weights. Vector space model adaptation (Chen et al., 2013) has the same assumption, and it weights each phrase pair based on vector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to compute weights for each instance in the training data; it can be applied to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010). If in-domain comparable data are available, (Daume III and Jagarlamudi, 2011; Irvine et al., 2013) propose mining translations from the comparable data to translate out-of-vocabulary (OOV) words and capture new senses for the new test domains. (Dou and Knight, 2012; Zhang and Zong, 2013) learn bilingual lexical or phrase tables from in-domain monolingual data with a decipherment method, then incorporate them into the SMT system. All the above approaches assume that either there is an in-domain (mono-lingual, parallel, or comparable) data set with a reaso"
K16-1031,N15-1091,0,0.0554186,"Missing"
K16-1031,C14-1220,0,0.0638379,"with semi-supervised convolutional neural networks for domain classification. Convolutional neural networks (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolution layers; each computation unit processes a small region of the input data. CNN has been very successful on image classification. When applying it to text input, the convolution layers process small regions of a document, i.e., a sequence of sentences or words. CNN has been gaining attention, and is now used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Sch¨utze, 2015; Wang et al., 2015). 2 2.1 Related Work SMT adaptation techniques Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual parallel data and in-domai"
K16-1031,P10-2041,0,0.530225,"ting the SMT system to the test domain (in-domain). However, manually creating training data to match the test domain is not a preferred solution, because 1) sometimes the test domain is not known when training the model, and it could change from sentence to sentence; 2) even if the test domain is pre-determined, the resources required and slow turnaround in data collection process will still delay the system development process. Therefore, training data selection is widely used for domain adaptation in statistical machine translation (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015). Data selection techniques select monolingual or bilingual data that are similar to the indomain seed data based on some criteria, which are incorporated into the training data. The most successful data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011) train n-gram language models on in-domain text to select similar sentences from the large general-domain corpora according to the cross entropy. Furthermore, (Duh et al., 2013) obtained some gains by extending these approaches from n-gram models to recurrent neural"
K16-1031,P13-1140,0,0.0152568,"ector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to compute weights for each instance in the training data; it can be applied to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010). If in-domain comparable data are available, (Daume III and Jagarlamudi, 2011; Irvine et al., 2013) propose mining translations from the comparable data to translate out-of-vocabulary (OOV) words and capture new senses for the new test domains. (Dou and Knight, 2012; Zhang and Zong, 2013) learn bilingual lexical or phrase tables from in-domain monolingual data with a decipherment method, then incorporate them into the SMT system. All the above approaches assume that either there is an in-domain (mono-lingual, parallel, or comparable) data set with a reasonable size available, or that some sub-corpora are closer to the test domain than others. There is no previous work considering the scenario where only a tiny amount of in-domain data is available: this is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wherever they are in the"
K16-1031,P15-2060,0,0.0241725,"s is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent, dense and low-dimensional word embeddings as inputs. (Kim, 2014) defines a one-layer CNN architecture with comparable performance to (Kalchbrenner et al., 2014). The word embeddings input to the CNN can be pre-trained, and treated as fixed input, or tuned for a specific task. (Johnson"
K16-1031,C04-1059,0,0.201474,"Missing"
K16-1031,P02-1040,0,0.0970612,"Bell 7 smoothing (Axelrod et al., 2011) 3. rnnlm: Data selection by recurrent neural network LM, with the RNNLM Toolkit (Duh et al., 2013) 4. comblm: Data selection by the combined LM using ngram & rnnlm (equal weight) (Duh et al., 2013). All systems are trained with a standard phrasebased SMT system with standard settings, i.e., GIZA++ alignment, phrase table Kneser-Ney smoothing, hierarchical reordering models, target side 4-gram language model, “gigaword” 5-gram language model for systems with English as the target language, etc. 5.3 Experimental results We evaluated the system using BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we use the bootstrap resampling test to do significance testing. Table 2 summarizes the results and numbers of the selected sentences for each task. First, we can see that all the data selection methods improved the performance over the baseline “alldata” with much less 6 The code and scripts for the three baselines are available at http://cl.naist.jp/ kevinduh/a/acl2013/. 7 For small amounts of data, Witten-Bell smoothing had performed better than Kneser-Ney smoothing in our experiments. 319 alldata ngram rnnlm comblm ohcnn sscnn zh2en #sent BL"
N13-1114,D11-1033,0,0.194028,"nearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set w"
N13-1114,W09-0432,0,0.0574768,"approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the senten"
N13-1114,P08-2040,1,0.901088,"7), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on"
N13-1114,2011.mtsummit-papers.30,1,0.81693,"Missing"
N13-1114,N12-1047,1,0.88809,"Missing"
N13-1114,W12-3125,0,0.0337185,"Missing"
N13-1114,W07-0717,1,0.955665,"adaptation: smoothing the in-domain sample, and weighting instances by document frequency. Applied to mixture RMs in our experiments, these techniques (especially smoothing) yield significant performance improvements. 1 In offline domain adaptation, the system is provided with a sample of translated sentences from the test domain prior to deployment. In a popular variant of offline adaptation, linear mixture model adaptation, each training corpus is used to generate a separate model component that forms part of a linear combination, and the sample is used to assign a weight to each component (Foster and Kuhn, 2007). If the sample resembles some of the corpora more than others, those corpora will receive higher weights in the combination. Introduction A phrase-based statistical machine translation (SMT) system typically has three main components: a translation model (TM) that contains information about how to translate word sequences (phrases) from the source language to the target language, a language model (LM) that contains information Previous research on domain adaptation for SMT has focused on the TM and the LM. Such research is easily motivated: translations across domains are unreliable. For exam"
N13-1114,D10-1044,1,0.922726,"Missing"
N13-1114,D08-1089,0,0.298034,"ically identifies three possible orientations for a newly chosen source phrase: monotone (M), swap (S), and discontinuous (D). The M orientation occurs when the newly chosen phrase is immediately to the right of the previously translated phrase in the source sentence, the S orientation occurs when the new phrase is immediately to the left of the previous phrase, and the D orientation covers all other cases.1 This type of RM is lexicalized: the estimated probabilities of M, S and D depend on the sourcelanguage and target-language words in both the previous phrase pair and the newly chosen one. Galley and Manning (2008) proposed a “hierarchical” lexicalized RM in which the orientation (M, S, or D) is determined not by individual phrase pairs, but by blocks. A block is the largest contiguous sequence of phrase pairs that satisfies the phrase pair consistency requirement of having no external links. Thus, classification of the orientation of a newly chosen phrase as M, S, or D is carried out as if the decoder always chose the longest possible source phrase in the past, and will choose the longest possible source phrase in the future. The RM used in this paper is hierarchical and lexicalized. For a given phrase"
N13-1114,C10-1056,0,0.020848,"al sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. All of"
N13-1114,W07-0733,0,0.0860924,"model (TM) and language model (LM) adaptation. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, data weighting, and phrase sense disambiguation. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data"
N13-1114,P07-2045,0,0.0214996,"Missing"
N13-1114,koen-2004-pharaoh,0,0.452653,"ve not done so. Finally, the paper analyzes reordering to see why RM adaptation works. There seem to be two factors at work. First, the reordering behaviour of words and phrases often differs dramatically from one bilingual corpus to another. Second, there are corpora (for instance, comparable corpora and bilingual lexicons) which may contain very valuable information for the TM, but which are poor sources of RM information; RM adaptation downweights information from these corpora significantly, and thus improves the overall quality of the RM. 2 Reordering Model In early SMT systems, such as (Koehn, 2004), changes in word order when a sentence is translated were modeled by means of a penalty that is in939 curred when the decoder chooses, as the next source phrase to be translated, a phrase that does not immediately follow the previously translated source sentence. Thus, the system penalizes deviations from monotone order, with the magnitude of the penalty being proportional to distance in the source sentence between the end of the previously translated source phrase and the start of the newly chosen source phrase. Many SMT systems, including our own, still use this distance-based penalty as a"
N13-1114,D07-1036,0,0.0862795,"Missing"
N13-1114,D09-1074,0,0.206693,"omain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system"
N13-1114,P10-2041,0,0.135061,"veral different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far fro"
N13-1114,P02-1040,0,0.0876064,"2.1* 33.0** 32.8** Arabic 46.8 47.0 47.5** 48.2** Table 5: Comparison of LM, TM, and RM adaptation. Table 3: Results for variants of RM adaptation. system LM+TM adaptation +RMA+dev-smoothing+DF Chinese 33.2 33.5 Arabic 47.7 48.4** Table 4: RM adaptation improves over a baseline containing adapted LMs and TMs. tem was tuned with batch lattice MIRA (Cherry and Foster, 2012). 4.3 Results For our main baseline, we simply concatenate all training data. We also tried augmenting this with separate log-linear features corresponding to subcorpus-specific RMs. Our metric is case-insensitvie IBM BLEU-4 (Papineni et al., 2002); we report BLEU scores averaged across both test sets. Following (Koehn, 2004), we use the bootstrap-resampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p < 0.05 and p < 0.01 levels, respectively. Table 3 shows that reordering model adaptation helps in both data settings. Adding either documentfrequency weighting (equation 4) or dev-set smoothing makes the improvement significant in both settings. Using both techniques together yields highly significant improvements. Our second experiment measures the improvement from RM adaptat"
N13-1114,2011.mtsummit-papers.2,0,0.0248563,"hen used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. All of the above work focuses on"
N13-1114,2008.iwslt-papers.6,0,0.0382513,"hree possible orientations for a newly chosen source phrase: monotone (M), swap (S), and discontinuous (D). The M orientation occurs when the newly chosen phrase is immediately to the right of the previously translated phrase in the source sentence, the S orientation occurs when the new phrase is immediately to the left of the previous phrase, and the D orientation covers all other cases.1 This type of RM is lexicalized: the estimated probabilities of M, S and D depend on the sourcelanguage and target-language words in both the previous phrase pair and the newly chosen one. Galley and Manning (2008) proposed a “hierarchical” lexicalized RM in which the orientation (M, S, or D) is determined not by individual phrase pairs, but by blocks. A block is the largest contiguous sequence of phrase pairs that satisfies the phrase pair consistency requirement of having no external links. Thus, classification of the orientation of a newly chosen phrase as M, S, or D is carried out as if the decoder always chose the longest possible source phrase in the past, and will choose the longest possible source phrase in the future. The RM used in this paper is hierarchical and lexicalized. For a given phrase"
N13-1114,2012.eamt-1.43,0,0.0613514,"ining data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. All of the above work focuses on either TM or LM d"
N13-1114,P05-1069,0,0.0313324,"ntence is translated were modeled by means of a penalty that is in939 curred when the decoder chooses, as the next source phrase to be translated, a phrase that does not immediately follow the previously translated source sentence. Thus, the system penalizes deviations from monotone order, with the magnitude of the penalty being proportional to distance in the source sentence between the end of the previously translated source phrase and the start of the newly chosen source phrase. Many SMT systems, including our own, still use this distance-based penalty as a feature. However, starting with (Tillmann and Zhang, 2005; Koehn et al., 2005), a more sophisticated type of reordering model has often been adopted as well, and has yielded consistent performance gains. This type of RM typically identifies three possible orientations for a newly chosen source phrase: monotone (M), swap (S), and discontinuous (D). The M orientation occurs when the newly chosen phrase is immediately to the right of the previously translated phrase in the source sentence, the S orientation occurs when the new phrase is immediately to the left of the previous phrase, and the D orientation covers all other cases.1 This type of RM is lex"
N13-1114,P07-1004,0,0.0230579,"(Foster and Kuhn, 2007), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich featu"
N13-1114,C04-1059,0,0.124106,"Missing"
O03-5004,P95-1032,0,0.23116,"llocations, multi-word phrases, multi-word terms etc.) is an * Center for Speech Interaction Technology Research, Institute of Acoustics, Chinese Academy of Sciences Address: 17 Zhongguancun Rd. Beijing 100080, China E-mail: {chenbx , dulm}@iis.ac.cn 78 Boxing Chen and Limin Du important aspect of the automatic alignment of bilingual corpus technology. Since the 1980’s, the technique of automatic alignment of a bilingual corpus has undergone great improvement; and during the mid- and late-1990’s, many researchers began to research the automatic construction of a bilingual translation lexicon [Fung 1995; Wu et al. 1995; Hiemstra 1996; Melamed 1996 etc.] Their works have focused on the alignment of single words. At the same time, the extraction of multi-word units in singular languages has been also studied. Church utilized mutual information to evaluate the degree of association between two words [Church 1990]; hence, mutual information has played an important role in multi-word unit extraction research, and it is used most often with this technology by means of a statistical method. Many researchers [Smadja 1993; Nagao et al. 1994; Kita et al. 1994; Zhou et al. 1995; Shimohata et al. 1997;"
O03-5004,C96-1089,0,0.0190145,"al. 1997; Yamamoto et al. 1998] have utilized mutual information (or the transformation of mutual information) as an important parameter to extract multi-word units. The shortcoming of these methods is that low frequency multi-word units are easy to eliminate, and the output of extraction mainly depends on the verification of suitable Bi-grams when the iterative algorithm initiates. Automatic extraction of bilingual multi-word units is based on the automatic extraction of bilingual word and multi-word units in singular languages. Research in this field has also proceeded [Smadja et al. 1996; Haruno et al. 1996; Melamed 1997 etc], but the problem with this approach is that it relies on statistical methods more than the characteristics of the language per se and is mainly limited to the extraction of noun phrases. Because of the above problems and the fact that Chinese-English corpuses are commonly small, we provide an algorithm that uses the average association score and normalized association score difference. We also apply the Local Bests algorithm, stopword filtration and longer unit preference methods to extract Chinese or English multi-word units. 1.2 The Object of Our Research In research on t"
O03-5004,P98-1117,0,0.0548359,"Missing"
O03-5004,1996.amta-1.13,0,0.0414865,"terms etc.) is an * Center for Speech Interaction Technology Research, Institute of Acoustics, Chinese Academy of Sciences Address: 17 Zhongguancun Rd. Beijing 100080, China E-mail: {chenbx , dulm}@iis.ac.cn 78 Boxing Chen and Limin Du important aspect of the automatic alignment of bilingual corpus technology. Since the 1980’s, the technique of automatic alignment of a bilingual corpus has undergone great improvement; and during the mid- and late-1990’s, many researchers began to research the automatic construction of a bilingual translation lexicon [Fung 1995; Wu et al. 1995; Hiemstra 1996; Melamed 1996 etc.] Their works have focused on the alignment of single words. At the same time, the extraction of multi-word units in singular languages has been also studied. Church utilized mutual information to evaluate the degree of association between two words [Church 1990]; hence, mutual information has played an important role in multi-word unit extraction research, and it is used most often with this technology by means of a statistical method. Many researchers [Smadja 1993; Nagao et al. 1994; Kita et al. 1994; Zhou et al. 1995; Shimohata et al. 1997; Yamamoto et al. 1998] have utilized mutual in"
O03-5004,W97-0311,0,0.0162625,"et al. 1998] have utilized mutual information (or the transformation of mutual information) as an important parameter to extract multi-word units. The shortcoming of these methods is that low frequency multi-word units are easy to eliminate, and the output of extraction mainly depends on the verification of suitable Bi-grams when the iterative algorithm initiates. Automatic extraction of bilingual multi-word units is based on the automatic extraction of bilingual word and multi-word units in singular languages. Research in this field has also proceeded [Smadja et al. 1996; Haruno et al. 1996; Melamed 1997 etc], but the problem with this approach is that it relies on statistical methods more than the characteristics of the language per se and is mainly limited to the extraction of noun phrases. Because of the above problems and the fact that Chinese-English corpuses are commonly small, we provide an algorithm that uses the average association score and normalized association score difference. We also apply the Local Bests algorithm, stopword filtration and longer unit preference methods to extract Chinese or English multi-word units. 1.2 The Object of Our Research In research on the results pro"
O03-5004,C94-1101,0,0.0658766,"Missing"
O03-5004,J93-1007,0,0.0492598,"began to research the automatic construction of a bilingual translation lexicon [Fung 1995; Wu et al. 1995; Hiemstra 1996; Melamed 1996 etc.] Their works have focused on the alignment of single words. At the same time, the extraction of multi-word units in singular languages has been also studied. Church utilized mutual information to evaluate the degree of association between two words [Church 1990]; hence, mutual information has played an important role in multi-word unit extraction research, and it is used most often with this technology by means of a statistical method. Many researchers [Smadja 1993; Nagao et al. 1994; Kita et al. 1994; Zhou et al. 1995; Shimohata et al. 1997; Yamamoto et al. 1998] have utilized mutual information (or the transformation of mutual information) as an important parameter to extract multi-word units. The shortcoming of these methods is that low frequency multi-word units are easy to eliminate, and the output of extraction mainly depends on the verification of suitable Bi-grams when the iterative algorithm initiates. Automatic extraction of bilingual multi-word units is based on the automatic extraction of bilingual word and multi-word units in singular langu"
O03-5004,J96-1001,0,0.121751,". 1995; Shimohata et al. 1997; Yamamoto et al. 1998] have utilized mutual information (or the transformation of mutual information) as an important parameter to extract multi-word units. The shortcoming of these methods is that low frequency multi-word units are easy to eliminate, and the output of extraction mainly depends on the verification of suitable Bi-grams when the iterative algorithm initiates. Automatic extraction of bilingual multi-word units is based on the automatic extraction of bilingual word and multi-word units in singular languages. Research in this field has also proceeded [Smadja et al. 1996; Haruno et al. 1996; Melamed 1997 etc], but the problem with this approach is that it relies on statistical methods more than the characteristics of the language per se and is mainly limited to the extraction of noun phrases. Because of the above problems and the fact that Chinese-English corpuses are commonly small, we provide an algorithm that uses the average association score and normalized association score difference. We also apply the Local Bests algorithm, stopword filtration and longer unit preference methods to extract Chinese or English multi-word units. 1.2 The Object of Our Resea"
O03-5004,W98-1104,0,0.0432112,"Missing"
O03-5004,P97-1061,0,\N,Missing
O03-5004,C98-1113,0,\N,Missing
O03-5004,W95-0111,0,\N,Missing
P08-2040,P07-2045,0,0.0115781,"ta again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this paper, we further exploit the potential of the N-best hypotheses and propose several schemes to derive the posterior knowledge from the N-best hypotheses, in an effort to enhance the language model, translation model, and source word reordering under a re-decoding framework of any phrase-based SMT system. 2 Self-Enhancement Knowledge with Posterior The self-enhancement system structure is shown in Figure 1. Our baseline system is set up using Moses (Koehn et al., 2007), a state-of-the-art phrase-base SMT open source package. In the followings, we detail the approaches to exploiting the three different kinds of posterior knowledge, namely, language model, translation model and word reordering. 157 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 157–160, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 4. Repeat step 1-3 for a fixed number of iterations. 2.2 Figure 1: Self-enhancement system structure, where TM is translation model, LM is language model, and RM is reordering model. 2.1 Language Model We consi"
P08-2040,P07-1091,0,0.0782534,"hence improve the translation model. The procedure for translation model selfenhancement can be summarized as follows. 1. Run decoding and extract N-best hypotheses. 2. Extract “good phrase-pairs” according to the hypotheses’ phrase-alignment information and append them to the original phrase table to generate a new phrase table. 3. Score the new phrase table to create a new translation model. 4. Optimize the weights of the decoder with the above new translation model. 5. Repeat step 1-4 for a fixed number of iterations. 2.3 Word Reordering Some previous work (Costa-jussà and Fonollosa, 2006; Li et al., 2007) have shown that reordering a source sentence to match the word order in its corresponding target sentence can produce better translations for a phrase-based SMT system. We bring this idea forward to our word reordering selfenhancement framework, which similarly translates a source sentence (S) to target sentence (T) in two stages: S → S ′ → T , where S ′ is the reordered source sentence. The phrase-alignment information in each hypothesis indicates the word reordering for source sentence. We select the word reordering with the highest posterior probability as the best word reordering for a gi"
P08-2040,C02-1164,0,0.139094,"approaches to exploiting the three different kinds of posterior knowledge, namely, language model, translation model and word reordering. 157 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 157–160, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 4. Repeat step 1-3 for a fixed number of iterations. 2.2 Figure 1: Self-enhancement system structure, where TM is translation model, LM is language model, and RM is reordering model. 2.1 Language Model We consider self-enhancement of language model as a language model adaptation problem similar to (Nakajima et al., 2002). The original monolingual target training data is regarded as general-domain data while the test data as a domain-specific data. Obviously, the real domain-specific target data (test data) is unavailable for training. In this work, the N-best hypotheses of the test set are used as a quasi-corpus to train a language model. This new language model trained on the quasi-corpus is then used together with the language model trained on the general-domain data (original training data) to produce a new list of N-best hypotheses under our self-enhancement framework. The feature function of the language"
P08-2040,P02-1040,0,0.0731461,"Missing"
P08-2040,2006.iwslt-papers.3,0,0.553265,"erence between the two models. Nakajima et al. used only 1-best hypothesis, while we use N-best hypotheses of test set as the quasicorpus to train the language model. In the work of (Costa-jussà and Fonollosa, 2006; Li et al., 2007) which similarly translates a source sentence (S) to target sentence (T) in two stages: S → S ′ → T , they derive S ′ from training data; while we obtain S ′ based on the occurrence frequency, i.e. posterior probability of each source word reordering in the N-best hypotheses list. An alternative solution for enhancing the translation model is through self-training (Ueffing, 2006; Ueffing et al., 2007) which re-trains the source-target N-best hypotheses together with the original training data, and thus differs from ours in the way of new phrase pairs extraction. We only supplement those phrase-pairs appeared in the Nbest hypotheses to the original phrase table. Further experiment showed that improvement obtained by self-training method is not as consistent on both development and test sets as that by our method. One possible reason is that in self-training, the entire translation model is adjusted with the addition of new phrase-pairs extracted from the source-target"
P08-2040,2003.mtsummit-papers.52,0,0.0248546,"he MT system as the first decoding has discarded many undesirable translation candidates. Thus, the knowledge captured in the N-best hypotheses, such as posterior probabilities for words, n-grams, phrase-pairs, and source word reorderings, etc. is more compatible with the source sentences and thus could potentially be used to improve the translation performance. Word posterior probabilities estimated from the N-best hypotheses have been widely used for confidence measure in automatic speech recognition (Wessel, 2002) and have also been adopted into machine translation. Blatz et al. (2003) and Ueffing et al. (2003) used word posterior probabilities to estimate the confidence of machine translation. Chen et al. (2005), Zens and Ney (2006) reported performance improvements by computing target ngrams posterior probabilities estimated on the Nbest hypotheses in a rescoring framework. Transductive learning method (Ueffing et al., 2007) which repeatedly re-trains the generated sourcetarget N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this paper, we further exploit the"
P08-2040,P07-1004,0,0.750203,"ed to improve the translation performance. Word posterior probabilities estimated from the N-best hypotheses have been widely used for confidence measure in automatic speech recognition (Wessel, 2002) and have also been adopted into machine translation. Blatz et al. (2003) and Ueffing et al. (2003) used word posterior probabilities to estimate the confidence of machine translation. Chen et al. (2005), Zens and Ney (2006) reported performance improvements by computing target ngrams posterior probabilities estimated on the Nbest hypotheses in a rescoring framework. Transductive learning method (Ueffing et al., 2007) which repeatedly re-trains the generated sourcetarget N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this paper, we further exploit the potential of the N-best hypotheses and propose several schemes to derive the posterior knowledge from the N-best hypotheses, in an effort to enhance the language model, translation model, and source word reordering under a re-decoding framework of any phrase-based SMT system. 2 Self-Enhancement Knowledge with Posterior T"
P08-2040,W06-3110,0,0.0123789,"-best hypotheses, such as posterior probabilities for words, n-grams, phrase-pairs, and source word reorderings, etc. is more compatible with the source sentences and thus could potentially be used to improve the translation performance. Word posterior probabilities estimated from the N-best hypotheses have been widely used for confidence measure in automatic speech recognition (Wessel, 2002) and have also been adopted into machine translation. Blatz et al. (2003) and Ueffing et al. (2003) used word posterior probabilities to estimate the confidence of machine translation. Chen et al. (2005), Zens and Ney (2006) reported performance improvements by computing target ngrams posterior probabilities estimated on the Nbest hypotheses in a rescoring framework. Transductive learning method (Ueffing et al., 2007) which repeatedly re-trains the generated sourcetarget N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this paper, we further exploit the potential of the N-best hypotheses and propose several schemes to derive the posterior knowledge from the N-best hypotheses,"
P08-2040,C04-1046,0,\N,Missing
P08-2040,W06-1609,0,\N,Missing
P08-2040,2005.iwslt-1.11,1,\N,Missing
P09-1106,C08-1005,0,0.185663,"Missing"
P09-1106,C08-1014,1,0.832383,"e (2005) proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alignment of hypotheses using synonyms as found in WordNet (Fellbaum, 1998) and a two-pass alignment strategy based on TER word alignment approach. He et al. (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. Although many methods have been attempted, no systematic comparison among them has been reported. A through and fair comparison among them would be of great meaning to t"
P09-1106,J07-2003,0,0.0421201,"the training, dev and test data for IWSLT and NIST tasks. task data Sent. Words Dev Sent. IWSLT Words Test Sent. Words Add. Words Train Sent. Words Dev Sent. NIST 2002 Words Test Sent. 2005 Words Add. Words Train Ch En 406K 4.4M 4.6M 489 489 × 7 5,896 45,449 500 500 × 7 6,296 51,227 1.7M 238K 7.0M 8.9M 878 878 × 4 23,248 108,616 1,082 1,082 × 4 30,544 141,915 61.5M Table 1: Statistics of training, dev and test data for IWSLT and NIST tasks. In both experiments, we used four systems, as listed in Table 2, they are phrase-based system Moses (Koehn et al., 2007), hierarchical phrasebased system (Chiang, 2007), BTG-based lexicalized reordering phrase-based system (Xiong et al., 2006) and a tree sequence alignment-based tree-to-tree translation system (Zhang et al., 2008). Each system for the same task is trained on the same data set. 4.2 Experiments setting For each system, we used the top 10 scored hypotheses to build the confusion network. Similar to (Rosti et al., 2007a), each word in the hypothesis is assigned with a rank-based score of 1/ (1 + r ) , where r is the rank of the hypothesis. And we assign the same weights to each system. For selecting the backbone, only the top hypothesis from eac"
P09-1106,D08-1011,0,0.634478,"um Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alignment of hypotheses using synonyms as found in WordNet (Fellbaum, 1998) and a two-pass alignment strategy based on TER word alignment approach. He et al. (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. Although many methods have been attempted, no systematic comparison among them has been reported. A through and fair comparison among them would be of great meaning to the MT sys941 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 941–948, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP tem combination research. In this paper, we implement a confusion network-based decoder. Based on this decoder, we compare four commonly used wo"
P09-1106,D07-1029,0,0.129059,"Missing"
P09-1106,P05-3026,0,0.0260536,"is. 3) Confusion network construction: to build a confusion network based on hypothesis alignments. 4) Confusion network decoding: to decode the best translation from a confusion network. Among the four steps, the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems (Rosti et al, 2007). Many techniques have been studied to address this issue. Bangalore et al. (2001) used the edit distance alignment algorithm which is extended to multiple strings to build confusion network, it only allows monotonic alignment. Jayaraman and Lavie (2005) proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et"
P09-1106,P08-2021,0,0.037903,"fusion network, it only allows monotonic alignment. Jayaraman and Lavie (2005) proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alignment of hypotheses using synonyms as found in WordNet (Fellbaum, 1998) and a two-pass alignment strategy based on TER word alignment approach. He et al. (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. Although many methods have been attempted, no systematic comparison among them has been reported. A th"
P09-1106,C04-1183,1,0.869719,"Missing"
P09-1106,P07-2045,0,0.00783503,"Missing"
P09-1106,N04-1022,0,0.147586,"cludes the paper. 2 Confusion network combination based system In order to compare different hypothesis alignment methods, we implement a confusion network decoding system as follows: Backbone selection: in the previous work, Matusov et al. (2006, 2008) let every hypothesis play the role of the backbone (also called “skeleton” or “alignment reference”) once. We follow the work of (Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; He et al., 2008) and choose the hypothesis that best agrees with other hypotheses on average as the backbone by applying Minimum Bayes Risk (MBR) decoding (Kumar and Byrne, 2004). TER score (Snover et al, 2006) is used as the loss function in MBR decoding. Given a hypothesis set H, the backbone can be computed using the following equation, where TER(•, •) returns the TER score of two hypotheses. Eb = arg min ∑ TER ( Eˆ , E ) Eˆ ∈H (1) E∈H Hypothesis alignment: all hypotheses are word-aligned to the corresponding backbone in a many-to-one manner. We apply four word alignment methods: GIZA++-based, TER-based, CLA-based, and IHMM-based word alignment algorithm. For each method, we will give details in the next section. Confusion network construction: confusion network is"
P09-1106,E06-1031,0,0.0608531,"Missing"
P09-1106,E06-1005,0,0.490885,"confusion network. Among the four steps, the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems (Rosti et al, 2007). Many techniques have been studied to address this issue. Bangalore et al. (2001) used the edit distance alignment algorithm which is extended to multiple strings to build confusion network, it only allows monotonic alignment. Jayaraman and Lavie (2005) proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alig"
P09-1106,J00-2004,0,0.132613,"allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alignment of hypotheses using synonyms as found in WordNet (Fellbaum, 1998) and a two-pass alignment strategy based on TER word alignment approach. He et al. (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. Although many methods have been attempted, no systematic comparison among them has been reported. A through and fair comparison among them would be of great meaning to the MT sys941 Proceedings of the 47th Annual Meeting of th"
P09-1106,P03-1021,0,0.0185165,"probabilities (arc scores of the confusion network), • N-gram frequencies (Chen et al., 2005), • N-gram posterior probabilities (Zens and Ney, 2006). Word alignment algorithms We compare four word alignment methods which are widely used in confusion network based system combination or bilingual parallel corpora word alignment. bigram ei′ei′+1 observed in the hypothesis list; e2 Word penalty, The n-grams used in the last two feature functions are collected from the original hypotheses list from each single system. The weights of feature functions are optimized to maximize the scoring measure (Och, 2003). where p (ei′ei′+1 ) is the occurrence probability of e1 • Hypothesis-to-backbone ment word alignGIZA++: Matusov et al. (2006, 2008) proposed using GIZA++ (Och and Ney, 2003) to align words between the backbone and hypothesis. This method uses enhanced HMM model bootstrapped from IBM Model-1 to estimate the alignment model. All hypotheses of the whole test set are collected to create sentence pairs for GIZA++ training. GIZA++ produces hypothesisbackbone many-to-1 word alignments. TER-based: TER-based word alignment method (Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b) is an exte"
P09-1106,J03-1002,0,0.00892965,"lgorithms We compare four word alignment methods which are widely used in confusion network based system combination or bilingual parallel corpora word alignment. bigram ei′ei′+1 observed in the hypothesis list; e2 Word penalty, The n-grams used in the last two feature functions are collected from the original hypotheses list from each single system. The weights of feature functions are optimized to maximize the scoring measure (Och, 2003). where p (ei′ei′+1 ) is the occurrence probability of e1 • Hypothesis-to-backbone ment word alignGIZA++: Matusov et al. (2006, 2008) proposed using GIZA++ (Och and Ney, 2003) to align words between the backbone and hypothesis. This method uses enhanced HMM model bootstrapped from IBM Model-1 to estimate the alignment model. All hypotheses of the whole test set are collected to create sentence pairs for GIZA++ training. GIZA++ produces hypothesisbackbone many-to-1 word alignments. TER-based: TER-based word alignment method (Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b) is an extension of multiple string matching algorithm based on Levenshtein edit distance (Bangalore et al., 2001). The TER (translation error rate) score (Snover et al., 2006) measures"
P09-1106,P02-1040,0,0.0768188,"Missing"
P09-1106,N07-1029,0,0.405741,"ur word alignment methods on both Chinese-to-English spoken and written language tasks. 1 Introduction Machine translation (MT) system combination technique leverages on multiple MT systems to achieve better performance by combining their outputs. Confusion network based system combination for machine translation has shown promising advantage compared with other techniques based system combination, such as sentence level hypothesis selection by voting and source sentence re-decoding using the phrases or translation models that are learned from the source sentences and target hypotheses pairs (Rosti et al., 2007a; Huang and Papineni, 2007). In general, the confusion network based system combination method for MT consists of four steps: 1) Backbone selection: to select a backbone (also called “skeleton”) from all hypotheses. The backbone defines the word orders of the final translation. 2) Hypothesis alignment: to build word-alignment between backbone and each hypothesis. 3) Confusion network construction: to build a confusion network based on hypothesis alignments. 4) Confusion network decoding: to decode the best translation from a confusion network. Among the four steps, the hypothesis alignment pr"
P09-1106,P07-1040,0,0.145783,"Missing"
P09-1106,W08-0329,0,0.0334511,"e. Bangalore et al. (2001) used the edit distance alignment algorithm which is extended to multiple strings to build confusion network, it only allows monotonic alignment. Jayaraman and Lavie (2005) proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alignment of hypotheses using synonyms as found in WordNet (Fellbaum, 1998) and a two-pass alignment strategy based on TER word alignment approach. He et al. (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a"
P09-1106,2006.amta-papers.25,0,0.473331,", 2007). Many techniques have been studied to address this issue. Bangalore et al. (2001) used the edit distance alignment algorithm which is extended to multiple strings to build confusion network, it only allows monotonic alignment. Jayaraman and Lavie (2005) proposed a heuristic-based matching algorithm which allows nonmonotonic alignments to align the words between the hypotheses. More recently, Matusov et al. (2006, 2008) used GIZA++ to produce word alignment for hypotheses pairs. Sim et al. (2007), Rosti et al. (2007a), and Rosti et al. (2007b) used minimum Translation Error Rate (TER) (Snover et al., 2006) alignment to build the confusion network. Rosti et al. (2008) extended TER algorithm which allows a confusion network as the reference to compute word alignment. Karakos et al. (2008) used ITG-based method for hypothesis alignment. Chen et al. (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. Ayan et al. (2008) proposed to improve alignment of hypotheses using synonyms as found in WordNet (Fellbaum, 1998) and a two-pass alignment strategy based on TER word alignment approach. He et al. (2008) proposed an IHMM-based word alignmen"
P09-1106,takezawa-etal-2002-toward,0,0.0321791,"two links share a same hypothesis or backbone word and also satisfy the constraints, we choose the link that with the highest similarity score. For example, in Figure 2, since MCS-based similarity scores S ( shot , shoot ) > S ( shot , the) , we choose alignment (a). 4 4.1 Experiments and results Tasks and single systems Experiments are carried out in two domains. One is in spoken language domain while the other is on newswire corpus. Both experiments are on Chinese-to-English translation. Experiments on spoken language domain were carried out on the Basic Traveling Expression Corpus (BTEC) (Takezawa et al., 2002) Chinese- to-English data augmented with HITcorpus1 . BTEC is a multilingual speech corpus which contains sentences spoken by tourists. 40K sentence-pairs are used in our experiment. HIT-corpus is a balanced corpus and has 500K sentence-pairs in total. We selected 360K sentence-pairs that are more similar to BTEC data according to its sub-topic. Additionally, the English sentences of Tanaka corpus2 were also used to train our language model. We ran experiments on an IWSLT challenge task which uses IWSLT20063 DEV clean text set as development set and IWSLT-2006 TEST clean text as test set. b Fi"
P09-1106,P06-1066,0,0.0199915,"ent. Words Dev Sent. IWSLT Words Test Sent. Words Add. Words Train Sent. Words Dev Sent. NIST 2002 Words Test Sent. 2005 Words Add. Words Train Ch En 406K 4.4M 4.6M 489 489 × 7 5,896 45,449 500 500 × 7 6,296 51,227 1.7M 238K 7.0M 8.9M 878 878 × 4 23,248 108,616 1,082 1,082 × 4 30,544 141,915 61.5M Table 1: Statistics of training, dev and test data for IWSLT and NIST tasks. In both experiments, we used four systems, as listed in Table 2, they are phrase-based system Moses (Koehn et al., 2007), hierarchical phrasebased system (Chiang, 2007), BTG-based lexicalized reordering phrase-based system (Xiong et al., 2006) and a tree sequence alignment-based tree-to-tree translation system (Zhang et al., 2008). Each system for the same task is trained on the same data set. 4.2 Experiments setting For each system, we used the top 10 scored hypotheses to build the confusion network. Similar to (Rosti et al., 2007a), each word in the hypothesis is assigned with a rank-based score of 1/ (1 + r ) , where r is the rank of the hypothesis. And we assign the same weights to each system. For selecting the backbone, only the top hypothesis from each system is considered as a candidate for the backbone. Concerning the four"
P09-1106,W06-3110,0,0.0565219,"Missing"
P09-1106,P08-1064,1,0.843256,"NIST 2002 Words Test Sent. 2005 Words Add. Words Train Ch En 406K 4.4M 4.6M 489 489 × 7 5,896 45,449 500 500 × 7 6,296 51,227 1.7M 238K 7.0M 8.9M 878 878 × 4 23,248 108,616 1,082 1,082 × 4 30,544 141,915 61.5M Table 1: Statistics of training, dev and test data for IWSLT and NIST tasks. In both experiments, we used four systems, as listed in Table 2, they are phrase-based system Moses (Koehn et al., 2007), hierarchical phrasebased system (Chiang, 2007), BTG-based lexicalized reordering phrase-based system (Xiong et al., 2006) and a tree sequence alignment-based tree-to-tree translation system (Zhang et al., 2008). Each system for the same task is trained on the same data set. 4.2 Experiments setting For each system, we used the top 10 scored hypotheses to build the confusion network. Similar to (Rosti et al., 2007a), each word in the hypothesis is assigned with a rank-based score of 1/ (1 + r ) , where r is the rank of the hypothesis. And we assign the same weights to each system. For selecting the backbone, only the top hypothesis from each system is considered as a candidate for the backbone. Concerning the four alignment methods, we use the default setting for GIZA++; and use toolkit TERCOM (Snover"
P09-1106,zhang-etal-2004-interpreting,0,0.0683296,"Missing"
P09-1106,2005.eamt-1.20,0,\N,Missing
P09-1106,2005.iwslt-1.11,1,\N,Missing
P09-1106,2006.iwslt-evaluation.15,0,\N,Missing
P10-1086,J93-2003,0,0.0245623,"MI (r , c ) is defined as: F ( r, c ) N w(r , c) = MI (r , c) = F (r ) F (c ) × log log N N log (3) F ( r , c) + k R ∑ ( F ( r , c) + k ) = F (r , c ) + k (5) F (c) + kR i i =1 where k is a tunable global smoothing constant, and R is the number of rules. 4 Similarity Functions There are many possibilities for calculating similarities between bags-of-words in different languages. We consider IBM model 1 probabilities and cosine distance similarity functions. 4.1 IBM Model 1 Probabilities For the IBM model 1 similarity function, we take the geometric mean of symmetrized conditional IBM model 1 (Brown et al., 1993) bag probabilities, as in Equation (6). (6) sim(α , γ ) = sqrt( P( B f |Be ) ⋅ P( Be |B f )) To compute P ( B f |Be ) , IBM model 1 as(2) where w f i and we j are values for each source 3.3 where N is the total frequency counts of all rules and their context words. Since we are using this value as a weight, following (Turney, 2001), we drop log, N and F (r ) . Thus (3) simplifies to: sumes that all source words are conditionally independent, so that: I P( B f |Be ) = ∏ p ( f i |Be ) (7) i =1 To compute, we use a “Noisy-OR” combination which has shown better performance than standard IBM model"
P10-1086,D07-1007,0,0.0353197,"nings. Although this bias is helpful on its own, possibly due to the mechanism we outline in section 6.1, it appears to have a synergistic effect when used along with the bilingual similarity feature. 3) Finally, we note that many of the features we use for capturing similarity, such as the context “the, of” for instantiations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source"
P10-1086,W09-2404,0,0.0273299,"Missing"
P10-1086,P07-1005,0,0.0426806,"is helpful on its own, possibly due to the mechanism we outline in section 6.1, it appears to have a synergistic effect when used along with the bilingual similarity feature. 3) Finally, we note that many of the features we use for capturing similarity, such as the context “the, of” for instantiations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target con"
P10-1086,P05-1033,0,0.813888,"s between the source and target sides of a translation rule to improve statistical machine translation performance. This work attempts to measure directly the sense similarity for units from different languages by comparing their contexts1. Our contribution includes proposing new bilingual sense similarity algorithms and applying them to machine translation. We chose a hierarchical phrase-based SMT system as our baseline; thus, the units involved in computation of sense similarities are hierarchical rules. 2 Hierarchical phrase-based MT system The hierarchical phrase-based translation method (Chiang, 2005; Chiang, 2007) is a formal syntaxbased translation modeling method; its translation model is a weighted synchronous context free grammar (SCFG). No explicit linguistic syntactic information appears in the model. An SCFG rule has the following form: X → α,γ , ~ where X is a non-terminal symbol shared by all the rules; each rule has at most two nonterminals. α ( γ ) is a source (target) string consisting of terminal and non-terminal symbols. ~ defines a one-to-one correspondence between non-terminals in α and γ . 1 There has been a lot of work (more details in Section 7) on applying word sense"
P10-1086,J07-2003,0,0.730767,"source and target sides of a translation rule to improve statistical machine translation performance. This work attempts to measure directly the sense similarity for units from different languages by comparing their contexts1. Our contribution includes proposing new bilingual sense similarity algorithms and applying them to machine translation. We chose a hierarchical phrase-based SMT system as our baseline; thus, the units involved in computation of sense similarities are hierarchical rules. 2 Hierarchical phrase-based MT system The hierarchical phrase-based translation method (Chiang, 2005; Chiang, 2007) is a formal syntaxbased translation modeling method; its translation model is a weighted synchronous context free grammar (SCFG). No explicit linguistic syntactic information appears in the model. An SCFG rule has the following form: X → α,γ , ~ where X is a non-terminal symbol shared by all the rules; each rule has at most two nonterminals. α ( γ ) is a source (target) string consisting of terminal and non-terminal symbols. ~ defines a one-to-one correspondence between non-terminals in α and γ . 1 There has been a lot of work (more details in Section 7) on applying word sense disambiguation"
P10-1086,N09-1025,0,0.020523,"the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similar"
P10-1086,J90-1003,0,0.0978788,"Missing"
P10-1086,W08-0302,0,0.0122429,"iations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilin"
P10-1086,C08-1041,0,0.0792135,"work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another wo"
P10-1086,P90-1034,0,0.286648,"ranslation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity functi"
P10-1086,N03-1017,0,0.0120534,"t 2 Rule 3 Context 3 Rule 4 Context 4 会议 他, 出席, 了 他 X1 会议 出席, 了 出席 了 他,会议 the meeting he, attended he X1 the meeting attended attended he, the, meeting Figure 1: example of hierarchical rule pairs and their context features. Rule frequencies are counted during rule extraction over word-aligned sentence pairs, and they are normalized to estimate features on rules. Following (Chiang, 2005; Chiang, 2007), 4 features are computed for each rule: • P (γ |α ) and P (α |γ ) are direct and inverse rule-based conditional probabilities; • Pw (γ |α ) and Pw (α |γ ) are direct and inverse lexical weights (Koehn et al., 2003). Empirically, this method has yielded better performance on language pairs such as ChineseEnglish than the phrase-based method because it permits phrases with gaps; it generalizes the normal phrase-based models in a way that allows long-distance reordering (Chiang, 2005; Chiang, 2007). We use the Joshua implementation of the method for decoding (Li et al., 2009). 3 Bag-of-Words Vector Space Model To compute the sense similarity via VSM, we follow the previous work (Lin, 1998) and represent the source and target side of a rule by feature vectors. In our work, each feature corresponds to a cont"
P10-1086,W09-0424,0,0.0124616,"llowing (Chiang, 2005; Chiang, 2007), 4 features are computed for each rule: • P (γ |α ) and P (α |γ ) are direct and inverse rule-based conditional probabilities; • Pw (γ |α ) and Pw (α |γ ) are direct and inverse lexical weights (Koehn et al., 2003). Empirically, this method has yielded better performance on language pairs such as ChineseEnglish than the phrase-based method because it permits phrases with gaps; it generalizes the normal phrase-based models in a way that allows long-distance reordering (Chiang, 2005; Chiang, 2007). We use the Joshua implementation of the method for decoding (Li et al., 2009). 3 Bag-of-Words Vector Space Model To compute the sense similarity via VSM, we follow the previous work (Lin, 1998) and represent the source and target side of a rule by feature vectors. In our work, each feature corresponds to a context word which co-occurs with the translation rule. 3.1 Context Features In the hierarchical phrase-based translation method, the translation rules are extracted by abstracting some words from an initial phrase pair (Chiang, 2005). Consider a rule with nonterminals on the source and target side; for a given instance of the rule (a particular phrase pair in the tr"
P10-1086,P98-2127,0,0.260401,"improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton"
P10-1086,D08-1010,0,0.12289,"s into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another words, WSD explicitly tries to cho"
P10-1086,D09-1022,0,0.0330004,"tic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or targ"
P10-1086,P03-1021,0,0.00622246,"on 3 from the full training data for α and for γ , recooc spectively. C f v v f = {w f1 , w f 2 ,..., w f I } v va = {waf1 , waf 2 ,..., waf I } 4.3 Cecooc ⊆ Cefull . Therefore, the original similarity functions are to compare the two context vectors built on full training data directly, as shown in Equation (13). sim(α , γ ) = sim(C ffull , Cefull ) (13) Then, we propose a new similarity function as follows: sim(α , γ ) = sim(C ffull , C cooc )λ1 ⋅ sim(C cooc , Cecooc )λ2 ⋅ sim(Cefull , Cecooc )λ3 (14) f f where the parameters λi (i=1,2,3) can be tuned via minimal error rate training (MERT) (Och, 2003). (11) α The standard cosine distance is defined as the v v inner product of the two vectors v f and va norγ malized by their norms. Based on Equation (10) and (11), it is easy to derive the similarity as follows: v v v f ⋅ va v v sim(α , γ ) = cos( v f , v a ) = v v |v f |⋅ |va | (12) J ∑∑ w = i =1 j =1 I fi Pr( f i |e j )we j I 2 sqrt (∑ w 2fi )sqrt (∑ wafi ) I =1 i =1 are the contexts for they satisfy the constraints: C cooc ⊆ C ffull and f Naïve Cosine Distance Similarity I cooc and C e α and γ when α and γ co-occur. Obviously, where p( f i |e j ) is a lexical probability (we use IBM model"
P10-1086,J07-2002,0,0.0108003,"rt hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton and McGill, 1983); other similarity functions include Euclid"
P10-1086,P02-1040,0,0.0849689,"sk. For German-to-English tasks, we used WMT 2006 4 data sets. The parallel training data contains 21 million target words; both the dev set and test set contain 2000 sentences; one reference is provided for each source input sentence. Only the target-language half of the parallel training data are used to train the language model in this task. 5.2 Results For the baseline, we train the translation model by following (Chiang, 2005; Chiang, 2007) and our decoder is Joshua 5 , an open-source hierarchical phrase-based machine translation system written in Java. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. By observing the results on dev set in the additional experiments, we first set the smoothing constant k in Equation (5) to 0.5. Then, we need to set the sizes of the vectors to balance the computing time and translation accu4 5 838 http://www.statmt.org/wmt06/ http://www.cs.jhu.edu/~ccb/joshua/index.html racy, i.e., we keep only the top N context words with the highest feature value for each side of a rule 6 . In the following, we use “Alg"
P10-1086,P99-1067,0,0.234566,", City Block distance (Bullinaria and Levy; 2007), and Dice and Jaccard coefficients (Frakes and Baeza-Yates, 1992), etc. Measures of monolingual sense similarity have been widely used in many applications, such as synonym recognizing (Landauer and Dumais, 1997), word clustering (Pantel and Lin 2002), word sense disambiguation (Yuret and Yatbaz 2009), etc. Use of the vector space model to compute sense similarity has also been adapted to the multilingual condition, based on the assumption that two terms with similar meanings often occur in comparable contexts across languages. Fung (1998) and Rapp (1999) adopted VSM for the application of extracting translation pairs from comparable or even unrelated corpora. The vectors in different languages are first mapped to a common space using an initial bilingual dictionary, and then compared. However, there is no previous work that uses the VSM to compute sense similarity for terms from parallel corpora. The sense similarities, i.e. the translation probabilities in a translation model, for units from parallel corpora are mainly based on the co-occurrence counts of the two units. Therefore, questions emerge: how good is the sense similarity computed v"
P10-1086,N09-2004,0,0.0398442,"ord-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another words, WSD explicitly tries to choose a translation given the current source context, while our work rates rule pairs i"
P10-1086,N04-1033,0,0.0655189,"on (6). (6) sim(α , γ ) = sqrt( P( B f |Be ) ⋅ P( Be |B f )) To compute P ( B f |Be ) , IBM model 1 as(2) where w f i and we j are values for each source 3.3 where N is the total frequency counts of all rules and their context words. Since we are using this value as a weight, following (Turney, 2001), we drop log, N and F (r ) . Thus (3) simplifies to: sumes that all source words are conditionally independent, so that: I P( B f |Be ) = ∏ p ( f i |Be ) (7) i =1 To compute, we use a “Noisy-OR” combination which has shown better performance than standard IBM model 1 probability, as described in (Zens and Ney, 2004): p( fi |Be ) = 1 − p( f i |Be ) (8) J p( fi |Be ) ≈ 1 − ∏ (1 − p ( f i |e j )) (9) j =1 where p( f i |Be ) is the probability that f i is not in the translation of Be , and is the IBM model 1 probability. 4.2 Vector Space Mapping A common way to calculate semantic similarity is by vector space cosine distance; we will also 836 use this similarity function in our algorithm. However, the two vectors in Equation (2) cannot be directly compared because the axes of their spaces represent different words in different languages, and also their dimensions I and J are not assured to be the same. There"
P10-1086,W04-3227,0,0.0214075,"836 use this similarity function in our algorithm. However, the two vectors in Equation (2) cannot be directly compared because the axes of their spaces represent different words in different languages, and also their dimensions I and J are not assured to be the same. Therefore, we need to first map a vector into the space of the other vector, so that the similarity can be calculated. Fung (1998) and Rapp (1999) map the vector onedimension-to-one-dimension (a context word is a dimension in each vector space) from one language to another language via an initial bilingual dictionary. We follow (Zhao et al., 2004) to do vector space mapping. Our goal is – given a source pattern – to distinguish between the senses of its associated target patterns. Therefore, we map all vectors in target language into the vector space in the source language. What we want is a representav tion va in the source language space of the target v v vector ve . To get va , we can let waf i , the weight of the ith source feature, be a linear combination over target features. That is to say, given a source feature weight for fi, each target feature weight is linked to it with some probability. So that we can calculate a transform"
P10-1086,J10-1004,0,\N,Missing
P10-1086,P07-1020,0,\N,Missing
P10-1086,C98-2122,0,\N,Missing
P10-1086,W04-3250,0,\N,Missing
P12-1098,W05-0909,0,0.328714,"Missing"
P12-1098,P11-1103,0,0.0149301,"), geometric (G), harmonic (H), and quadratic (Q) mean. If all of the values to be averaged are positive, the order is min ≤ H ≤ G ≤ A ≤ Q ≤ max , with equality holding if and only if all the values being averaged are equal. We chose the quadratic mean to combine precision and recall, as follows: Qmean( N ) = ( Pa ( N ) × SBP) 2 + ( Ra ( N ) × SRP) 2 2 (10) PORT uses permutations. These encode one-toone relations but not one-to-many, many-to-one, many-to-many or null relations, all of which can occur in word alignments. We constrain the forbidden types of relation to become one-to-one, as in (Birch and Osborne, 2011). Thus, in a one-tomany alignment, the single source word is forced to align with the first target word; in a many-to-one alignment, monotone order is assumed for the target words; and source words originally aligned to null are aligned to the target word position just after the previous source word’s target position. After the normalization above, suppose we have two permutations for the same source n-word input. E.g., let P1 = reference, P2 = hypothesis: P1: p11 p12 p13 p14 … p1i … p1n P2: p12 p22 p23 p24 … p2i … p2n Here, each pij is an integer denoting position in the original source (e.g."
P12-1098,W08-0309,0,0.136376,"Missing"
P12-1098,E06-1032,0,0.0653375,"Missing"
P12-1098,N10-1080,0,0.0668049,"Missing"
P12-1098,P08-1007,0,0.173175,"(Snover et al., 2006), and LRscore (Birch and Osborne, 2011) do not use external linguistic information; they are fast to compute (except TER). • • More sophisticated metrics such as RTE (Pado et al., 2009), DCU-LFG (He et al., 2010) and MEANT (Lo and Wu, 2011) use higher level syntactic or semantic analysis to score translations. Introduction Automatic evaluation metrics for machine translation (MT) quality are a key part of building statistical MT (SMT) systems. They play two 1 METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), TESLA (Liu et al., 2010), AMBER (Chen and Kuhn, 2011) and MTeRater (Parton et al., 2011) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging, paraphrasing tables or word root lists. PORT: Precision-Order-Recall Tunable metric. Among these metrics, BLEU is the most widely used for both evaluation and tuning. Many of the metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et 930 Proceedings of the 50th Annual Meeting of the Association for Computational Linguis"
P12-1098,W11-2105,1,0.653181,", 2011) do not use external linguistic information; they are fast to compute (except TER). • • More sophisticated metrics such as RTE (Pado et al., 2009), DCU-LFG (He et al., 2010) and MEANT (Lo and Wu, 2011) use higher level syntactic or semantic analysis to score translations. Introduction Automatic evaluation metrics for machine translation (MT) quality are a key part of building statistical MT (SMT) systems. They play two 1 METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), TESLA (Liu et al., 2010), AMBER (Chen and Kuhn, 2011) and MTeRater (Parton et al., 2011) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging, paraphrasing tables or word root lists. PORT: Precision-Order-Recall Tunable metric. Among these metrics, BLEU is the most widely used for both evaluation and tuning. Many of the metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et 930 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 930–939, c Jeju, Republic of Korea, 8-14 Ju"
P12-1098,D08-1064,0,0.0177585,"If there are multiple references, we use closest reference length for each translation hypothesis to compute the numbers of the reference n-grams. 2.1 BLEU BLEU is composed of precision Pg(N) and brevity penalty BP: BLEU = Pg ( N ) × BP (3) where Pg(N) is the geometric average of n-gram precisions 1  N N Pg ( N ) =  ∏ p (n)   n =1  (4) The BLEU brevity penalty punishes the score if the translation length len(T) is shorter than the reference length len(R); it is: BP = min 1.0, e1−len ( R ) / len (T ) (5) ( 2.2 ) PORT PORT has five components: precision, recall, strict brevity penalty (Chiang et al., 2008), strict redundancy penalty (Chen and Kuhn, 2011) and an ordering measure v. The design of PORT is based on exhaustive experiments on a development data set. We do not have room here to give a rationale for all the choices we made when we designed PORT. However, a later section (3.3) reconsiders some of these design decisions. 2.2.1 Precision and Recall The average precision and average recall used in PORT (unlike those used in BLEU) are the arithmetic average of n-gram precisions Pa(N) and recalls Ra(N): 1 N ∑ p ( n) N n=1 1 N Ra ( N ) = ∑ r (n) N n=1 Pa ( N ) = (6) (7) We use two penalties t"
P12-1098,W10-1751,0,0.0590586,"Missing"
P12-1098,W10-1753,0,0.0305202,"Missing"
P12-1098,D10-1092,0,0.111313,"Missing"
P12-1098,P07-2045,0,0.00743306,"an tokens and 50.8M English tokens. We translate both German-to-English (deen) and English-to-German (en-de). The two conditions both use an LM trained on the target side of the parallel training data, and de-en also uses the English Gigaword 5-gram LM. News test 2008 set is used as dev set; News test 2009, 2010, 2011 are used as test sets. One reference is provided for all dev and test sets. 2 3 LDC2003E14 http://www.nist.gov/speech/tests/mt 934 All experiments were carried out with α in Eq. (17) set to 0.25, and involved only lowercase European-language text. They were performed with MOSES (Koehn et al., 2007), whose decoder includes lexicalized reordering, translation models, language models, and word and phrase penalties. Tuning was done with n-best MERT, which is available in MOSES. In all tuning experiments, both BLEU and PORT performed lower case matching of n-grams up to n = 4. We also conducted experiments with tuning on a version of BLEU that incorporates SBP (Chiang et al., 2008) as a baseline. The results of original IBM BLEU and BLEU with SBP were tied; to save space, we only report results for original IBM BLEU here. 3.2.2 Comparisons with automatic metrics First, let us see if BLEU-tun"
P12-1098,W10-1754,0,0.104194,"LRscore (Birch and Osborne, 2011) do not use external linguistic information; they are fast to compute (except TER). • • More sophisticated metrics such as RTE (Pado et al., 2009), DCU-LFG (He et al., 2010) and MEANT (Lo and Wu, 2011) use higher level syntactic or semantic analysis to score translations. Introduction Automatic evaluation metrics for machine translation (MT) quality are a key part of building statistical MT (SMT) systems. They play two 1 METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), TESLA (Liu et al., 2010), AMBER (Chen and Kuhn, 2011) and MTeRater (Parton et al., 2011) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging, paraphrasing tables or word root lists. PORT: Precision-Order-Recall Tunable metric. Among these metrics, BLEU is the most widely used for both evaluation and tuning. Many of the metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et 930 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 930–939, c Jej"
P12-1098,D11-1035,0,0.036546,"tric PORT. In the table, TER scores are presented as 1-TER to ensure that for all metrics, higher scores mean higher quality. All scores are averages over the relevant test sets. There are twenty comparisons in the table. Among these, there is one case (FrenchEnglish assessed with METEOR) where BLEU outperforms PORT, there are seven ties, and there are twelve cases where PORT is better. Table 3 shows that fr-en outputs are very similar for both tuning types, so the fr-en results are perhaps less informative than the others. Overall, PORT tuning has a striking advantage over BLEU tuning. Both (Liu et al., 2011) and (Cer et al., 2011) showed that with MERT, if you want the best possible score for a system’s translations according to metric M, then you should tune with M. This doesn’t appear to be true when PORT and BLEU tuning are compared in Table 4. For the two Chinese-to-English tasks in the table, PORT tuning yields a better BLEU score than BLEU tuning, with significance at p < 0.05. We are currently investigating why PORT tuning gives higher BLEU scores than BLEU tuning for ChineseEnglish and German-English. In internal tests we have found no systematic difference in dev-set BLEUs, so we specula"
P12-1098,P11-1023,0,0.0461895,"Missing"
P12-1098,P03-1021,0,0.0729119,"Missing"
P12-1098,J03-1002,0,0.00549957,"ference. Several ordering measures have been integrated into MT evaluation metrics recently. Birch and Osborne (2011) use either Hamming Distance or Kendall’s τ Distance (Kendall, 1938) in their metric LRscore, thus obtaining two versions of LRscore. Similarly, Isozaki et al. (2011) adopt either Kendall’s τ Distance or Spearman’s ρ (Spearman, 1904) distance in their metrics. Our measure, v, is different from all of these. We use word alignment to compute the two permutations (LRscore also uses word alignment). The word alignment between the source input and reference is computed using GIZA++ (Och and Ney, 2003) beforehand with the default settings, then is refined with the heuristic grow-diag-finaland; the word alignment between the source input and the translation is generated by the decoder with the help of word alignment inside each phrase pair. Let ν1 = 1 − DIST1 ( P1 , P2 ) n( n + 1) / 2 (12) v1 ranges from 0 to 1; a larger value means more similarity between the two permutations. This metric is similar to Spearman’s ρ (Spearman, 1904). However, we have found that ρ punishes long-distance reorderings too heavily. For instance, ν 1 is more tolerant than ρ of the movement of “recently” in this ex"
P12-1098,P09-1034,0,0.0268432,"Missing"
P12-1098,P02-1040,0,0.0982367,"Missing"
P12-1098,W11-2111,0,0.0164433,"tic information; they are fast to compute (except TER). • • More sophisticated metrics such as RTE (Pado et al., 2009), DCU-LFG (He et al., 2010) and MEANT (Lo and Wu, 2011) use higher level syntactic or semantic analysis to score translations. Introduction Automatic evaluation metrics for machine translation (MT) quality are a key part of building statistical MT (SMT) systems. They play two 1 METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), TESLA (Liu et al., 2010), AMBER (Chen and Kuhn, 2011) and MTeRater (Parton et al., 2011) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging, paraphrasing tables or word root lists. PORT: Precision-Order-Recall Tunable metric. Among these metrics, BLEU is the most widely used for both evaluation and tuning. Many of the metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et 930 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 930–939, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Compu"
P12-1098,2006.amta-papers.25,0,0.180093,"Missing"
P12-1098,W09-0441,0,0.0219228,"dington, 2002), WER, PER, TER (Snover et al., 2006), and LRscore (Birch and Osborne, 2011) do not use external linguistic information; they are fast to compute (except TER). • • More sophisticated metrics such as RTE (Pado et al., 2009), DCU-LFG (He et al., 2010) and MEANT (Lo and Wu, 2011) use higher level syntactic or semantic analysis to score translations. Introduction Automatic evaluation metrics for machine translation (MT) quality are a key part of building statistical MT (SMT) systems. They play two 1 METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), TESLA (Liu et al., 2010), AMBER (Chen and Kuhn, 2011) and MTeRater (Parton et al., 2011) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging, paraphrasing tables or word root lists. PORT: Precision-Order-Recall Tunable metric. Among these metrics, BLEU is the most widely used for both evaluation and tuning. Many of the metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et 930 Proceedings of the 50th Annual Meeting of the Associati"
P12-1098,C96-2141,0,0.221564,"rd alignment between the source input and the translation is generated by the decoder with the help of word alignment inside each phrase pair. Let ν1 = 1 − DIST1 ( P1 , P2 ) n( n + 1) / 2 (12) v1 ranges from 0 to 1; a larger value means more similarity between the two permutations. This metric is similar to Spearman’s ρ (Spearman, 1904). However, we have found that ρ punishes long-distance reorderings too heavily. For instance, ν 1 is more tolerant than ρ of the movement of “recently” in this example: Ref: Recently, I visited Paris Hyp: I visited Paris recently Inspired by HMM word alignment (Vogel et al., 1996), our second distance measure is based on jump width. This punishes a sequence of words that moves a long distance with its internal order conserved, only once rather than on every word. In the following, only two groups of words have moved, so the jump width punishment is light: Ref: In the winter of 2010, I visited Paris Hyp: I visited Paris in the winter of 2010 So the second distance measure is 932 (11) i =1 n DIST2 ( P1 , P2 ) = ∑ |( p1i − p1i −1 ) − ( p2i − p2i −1 ) |(13) i =1 where we set p10 = 0 and p20 = 0 . Let DIST2 ( P1 , P2 ) n2 −1 v2 = 1 − (14) As with v1, v2 is also from 0 to 1,"
P12-1098,W07-0734,0,\N,Missing
P12-1098,W10-1703,0,\N,Missing
P13-1126,D11-1033,0,0.368647,"models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a s"
P13-1126,W09-0432,0,0.103531,"st approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically us"
P13-1126,P08-2040,1,0.823159,"2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Associati"
P13-1126,2011.mtsummit-papers.30,1,0.887165,"Missing"
P13-1126,N12-1047,1,0.101994,"ing IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with KN smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 Results For the baseline, we simply concatenate all training data. We have also compared our approach to two widely used TM domain adaptation ap1288 proaches. One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. The other is a linear combination of TMs trained on each subcorpus, with the weights of each model learned with an EM algorithm to maximize the likelihood of joint empirical phrase pair counts for in-domain dev data. For details, refer to (Foster and Kuhn, 200"
P13-1126,W07-0717,1,0.91413,"omain and the (test) domain in which the SMT system will be used, one can often get better performance by adapting the system to the test domain. Domain adaptation is an active topic in the natural language processing (NLP) research community. Its application to SMT systems has recently received considerable attention. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen"
P13-1126,D10-1044,1,0.928725,"Missing"
P13-1126,D08-1089,0,0.0852581,"Missing"
P13-1126,2005.eamt-1.19,0,0.0733146,"different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the"
P13-1126,P90-1034,0,0.285011,"independent of the subcorpora. One can think of several other ways of defining the vector space that might yield even better results than those reported here. Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments. 2 Vector space model adaptation Vector space models (VSMs) have been widely applied in many information retrieval and natural language processing applications. For instance, to compute the sense similarity between terms, many researchers extract features for each term from its context in a corpus, define a VSM and then apply similarity functions (Hindle, 1990; Lund and Burgess, 1996; Lin, 1998; Turney, 2001). In our experiments, we exploited the fact that the training data come from a set of subcorpora. For instance, the Chinese-English training data are made up of 14 subcorpora (see section 3 below). Suppose we have C subcorpora. The domain vector for a phrase-pair (f, e) is defined as V (f, e) =< w1 (f, e), ...wi (f, e), ..., wC (f, e) &gt;, (1) where wi (f, e) is a standard tf · idf weight, i.e. wi (f, e) = tfi (f, e) · idf (f, e) . (2) To avoid a bias towards longer corpora, we normalize the raw joint count ci (f, e) in the corpus si by dividing"
P13-1126,C10-1056,0,0.0231682,"ve learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop o"
P13-1126,P10-1026,0,0.0348231,"Missing"
P13-1126,W07-0733,0,0.55098,"cently received considerable attention. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add th"
P13-1126,P07-2045,0,0.0039338,"Missing"
P13-1126,W04-3250,0,0.184155,"nglish and Chinese-to-English experiment, these values obtained on Arabic dev were used to obtain the results below: λ was set to 8, and α was set to 0.01. (Later, we ran an experiment on Chinese-to-English with λ and α tuned specifically for that language pair, but the performance for the Chinese-English system only improved by a tiny, insignificant amount). Our metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores averaged across both test sets NIST06 and NIST08 for Chinese; NIST08 and NIST09 for Arabic. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p < 0.05 and p < 0.01 levels, respectively. We first compare the performance of different similarity functions: cosine (COS), JensenShannon divergence (JSD) and Bhattacharyya coefficient (BC). The results are shown in Table 3. All three functions obtained improvements. Both COS and BC yield statistically significant improvements over the baseline, with BC performing better than COS by a further statistically significant margin. The Bhattacharyya coefficient"
P13-1126,E12-1055,0,0.385818,"in data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 1 proposed phras"
P13-1126,P07-1004,0,0.0242508,"in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2"
P13-1126,C04-1059,0,0.0646896,"Missing"
P13-1126,P98-2127,0,0.0289323,"think of several other ways of defining the vector space that might yield even better results than those reported here. Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments. 2 Vector space model adaptation Vector space models (VSMs) have been widely applied in many information retrieval and natural language processing applications. For instance, to compute the sense similarity between terms, many researchers extract features for each term from its context in a corpus, define a VSM and then apply similarity functions (Hindle, 1990; Lund and Burgess, 1996; Lin, 1998; Turney, 2001). In our experiments, we exploited the fact that the training data come from a set of subcorpora. For instance, the Chinese-English training data are made up of 14 subcorpora (see section 3 below). Suppose we have C subcorpora. The domain vector for a phrase-pair (f, e) is defined as V (f, e) =< w1 (f, e), ...wi (f, e), ..., wC (f, e) &gt;, (1) where wi (f, e) is a standard tf · idf weight, i.e. wi (f, e) = tfi (f, e) · idf (f, e) . (2) To avoid a bias towards longer corpora, we normalize the raw joint count ci (f, e) in the corpus si by dividing by the maximum raw count of any phr"
P13-1126,D07-1036,0,0.352527,"Missing"
P13-1126,D09-1074,0,0.415681,"n the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might rec"
P13-1126,P10-2041,0,0.137255,"nearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For examp"
P13-1126,P02-1040,0,0.103578,"For details, refer to (Foster and Kuhn, 2007). The value of λ and α (see Eq 4 and Section 2.1) are determined by the performance on the dev set of the Arabic-to-English system. For both Arabic-to-English and Chinese-to-English experiment, these values obtained on Arabic dev were used to obtain the results below: λ was set to 8, and α was set to 0.01. (Later, we ran an experiment on Chinese-to-English with λ and α tuned specifically for that language pair, but the performance for the Chinese-English system only improved by a tiny, insignificant amount). Our metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores averaged across both test sets NIST06 and NIST08 for Chinese; NIST08 and NIST09 for Arabic. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p < 0.05 and p < 0.01 levels, respectively. We first compare the performance of different similarity functions: cosine (COS), JensenShannon divergence (JSD) and Bhattacharyya coefficient (BC). The results are shown in Table 3. All three functions obtained improvements. Both"
P13-1126,2011.mtsummit-papers.2,0,0.0163856,"em trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT"
P13-1126,2008.iwslt-papers.6,0,0.0772445,"ded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computat"
P13-1126,C98-2122,0,\N,Missing
P15-2025,W05-0909,0,0.33108,"both segment and system-level. Furthermore, our results also indicate that the representation based metrics are robust to a variety of training conditions, such as the data volume and domain. Automatic machine translation (MT) evaluation metrics measure the quality of the translations against human references. They allow rapid comparisons between different systems and enable the tuning of parameter values during system training. Many machine translation metrics have been proposed in recent years, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), Meteor (Banerjee and Lavie, 2005) and its extensions, and the MEANT family (Lo and Wu, 2011), amongst others. Precisely evaluating translation, however, is not easy. This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words. One straightforward solution to improve the evaluation quality is to increase the number of various references. Nevertheless, it is expensive to create multiple references. In order to catch synonym matches between the translations and references, synonym 150 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and th"
P15-2025,2006.amta-papers.25,0,0.0886031,"metrics, by a large margin on both segment and system-level. Furthermore, our results also indicate that the representation based metrics are robust to a variety of training conditions, such as the data volume and domain. Automatic machine translation (MT) evaluation metrics measure the quality of the translations against human references. They allow rapid comparisons between different systems and enable the tuning of parameter values during system training. Many machine translation metrics have been proposed in recent years, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), Meteor (Banerjee and Lavie, 2005) and its extensions, and the MEANT family (Lo and Wu, 2011), amongst others. Precisely evaluating translation, however, is not easy. This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words. One straightforward solution to improve the evaluation quality is to increase the number of various references. Nevertheless, it is expensive to create multiple references. In order to catch synonym matches between the translations and references, synonym 150 Proceedings of the 53rd Annual Meeting of the Association f"
P15-2025,N15-1075,1,0.836873,"tation for the whole sentence. Interested readers are referred to (Socher et al., 2011) for detailed discussions of the strategy. A representation, in the context of NLP, is a mathematical object associated with each word, sentence, or document. This object is typically a vector where each element’s value describes, to some degree, the semantic or syntactic properties of the associated word, sentence, or document. Using word or phrase representations as extra features has been proven to be an effective and simple way to improve the predictive performance of an NLP system (Turian et al., 2010; Cherry and Guo, 2015). Our evaluation metrics are based on three widely used representations, as discussed next. 2.1 One-hot Representations Conventionally, a word is represented by a one-hot vector. In a one-hot representation, a vocabulary is first defined, and then each word in the vocabulary is assigned a symbolic ID. In this scenario, for each word, the feature vector has the same length as the size of the vocabulary, and only one dimension that corresponds to the word is on, such as a vector with one element set to 1 and all others set to 0. This feature representation has been traditionally used for many NL"
P15-2025,W14-3348,0,0.0728694,"Missing"
P15-2025,D11-1014,0,0.137286,"e best performance, outperforming the state-of-the-art translation metrics by a large margin. In particular, training the distributed representations only needs a reasonable amount of monolingual, unlabeled data that is not necessary drawn from the test domain. 1 This paper leverages recent developments on distributed representations to address the above mentioned two challenges. A distributed representation maps each word or sentence to a continuous, low dimensional space, where words or sentences having similar syntactic and semantic properties are close to one another (Bengio et al., 2003; Socher et al., 2011; Socher et al., 2013; Mikolov et al., 2013). For example, the words vacation and holiday are close to each other in the vector space, but both are far from the word business in that space. Introduction We propose to evaluate the translations with different word and sentence representations. Specifically, we investigate the use of three widely deployed representations: one-hot representations, distributed word representations learned from a neural network model, and distributed sentence representations computed with recursive autoencoder. In particular, to leverage the different advantages and"
P15-2025,D13-1170,0,0.00465256,"utperforming the state-of-the-art translation metrics by a large margin. In particular, training the distributed representations only needs a reasonable amount of monolingual, unlabeled data that is not necessary drawn from the test domain. 1 This paper leverages recent developments on distributed representations to address the above mentioned two challenges. A distributed representation maps each word or sentence to a continuous, low dimensional space, where words or sentences having similar syntactic and semantic properties are close to one another (Bengio et al., 2003; Socher et al., 2011; Socher et al., 2013; Mikolov et al., 2013). For example, the words vacation and holiday are close to each other in the vector space, but both are far from the word business in that space. Introduction We propose to evaluate the translations with different word and sentence representations. Specifically, we investigate the use of three widely deployed representations: one-hot representations, distributed word representations learned from a neural network model, and distributed sentence representations computed with recursive autoencoder. In particular, to leverage the different advantages and focuses, in terms of"
P15-2025,P11-1023,0,0.145838,"e that the representation based metrics are robust to a variety of training conditions, such as the data volume and domain. Automatic machine translation (MT) evaluation metrics measure the quality of the translations against human references. They allow rapid comparisons between different systems and enable the tuning of parameter values during system training. Many machine translation metrics have been proposed in recent years, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), Meteor (Banerjee and Lavie, 2005) and its extensions, and the MEANT family (Lo and Wu, 2011), amongst others. Precisely evaluating translation, however, is not easy. This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words. One straightforward solution to improve the evaluation quality is to increase the number of various references. Nevertheless, it is expensive to create multiple references. In order to catch synonym matches between the translations and references, synonym 150 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Pr"
P15-2025,P10-1040,0,0.0761478,"s the vector representation for the whole sentence. Interested readers are referred to (Socher et al., 2011) for detailed discussions of the strategy. A representation, in the context of NLP, is a mathematical object associated with each word, sentence, or document. This object is typically a vector where each element’s value describes, to some degree, the semantic or syntactic properties of the associated word, sentence, or document. Using word or phrase representations as extra features has been proven to be an effective and simple way to improve the predictive performance of an NLP system (Turian et al., 2010; Cherry and Guo, 2015). Our evaluation metrics are based on three widely used representations, as discussed next. 2.1 One-hot Representations Conventionally, a word is represented by a one-hot vector. In a one-hot representation, a vocabulary is first defined, and then each word in the vocabulary is assigned a symbolic ID. In this scenario, for each word, the feature vector has the same length as the size of the vocabulary, and only one dimension that corresponds to the word is on, such as a vector with one element set to 1 and all others set to 0. This feature representation has been traditi"
P15-2025,D13-1141,0,0.0313498,"ormulate our five presentations based metrics as follows. For the one-hot representation metric, once we have the representations of the words and n-grams, we sum all the vectors to obtain the representation of the sentence. For efficiency, we only keep the entries which are not both zero in the reference and translation vectors. After we generate the two vectors for both translation and reference, we then compute the score using Equation 1. For the word embedding based metric, we first learn the word vector representation using the code provided by (Mikolov et al., 2013) 1 . Next, following (Zou et al., 2013), we average the word embeddings of all words in the sentence to obtain the representation of the sentence. As discussed in Section 2.4, the three sentencelevel one-hot, word embedding and RAE representations have different strength when they are 1 2 http://www.statmt.org/wmt13/translation-task.html http://www.cs.umd.edu/ snover/tercom/ 4 http://www.cs.cmu.edu/ alavie/METEOR/ 5 Meteor universal package does not include paraphrasing table for other target language except English, so we did not run Out-of-English experiments for this metric. 3 https://code.google.com/p/word2vec/ 152 metric BLEU"
P15-2025,W13-2202,0,0.0300441,"system-level γ score for all representation based metrics on the dev sets. We tuned the weights for combining the three vectors automatically, using the downhill simplex method as described in (Press et al., 2002). The weights are 1 for the RAE vector, about 0.1 for the word embedding vector, and around 0.01 for the one-hot vector, respectively. We tuned other parameters manually. Specifically, we set n equal to 2 for the one-hot n-gram representation, the vector size of the recursive auto-encoder to 10, and the vector size of word embeddings to 80. Following WMT 2013’s metric task (Mach´acˇ ek and Bojar, 2013), to measure the correlation with human judgment, we use Kendall’s rank correlation coefficient τ for the segment level, and Pearson’s correlation coefficient (γ in the below tables and figures) for the system-level respectively. (3) where α is a free parameter, vi (.) is the value of the vector element, Plen is the length penalty, and lr , lt are length of the translation and reference, respectively. In the scenarios of there exist multiple references, we compute the score with each reference, then choose the highest one. Also, we treat the document-level score as the weighted average of sent"
P15-2025,P02-1040,0,\N,Missing
P19-1305,P00-1041,0,0.241717,"sentences paired with the true summaries to build a training corpus for the cross-lingual ASSUM. This alleviates the data scarcity that no cross-lingual ASSUM corpus is available. • Extensive experimental results on two benchmark datasets show that our proposed method is able to perform better than several baselines and related works, and significantly reduce the performance gap between the crosslingual ASSUM and the monolingual ASSUM. 2 2.1 Related Work Monolingual ASSUM There are various methods exploring the effective way to model the monolingual ASSUM process including statistical models (Banko et al., 2000; Cohn and Lapata, 2008) or neural models (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). Neural models become dominant in this task since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). On the basis of the sequence-tosequence neural architecture, there are many further explorations such as using rich linguistic features and large vocabulary set (Nallapati et al., 2016), global training procedures on the sentence level (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018; Wang et al., 2018), topic enhancement in"
P19-1305,P17-1176,0,0.0183466,"018). 2.2 Zero Resource Neural Machine Translation Current state-of-the-art NMT models are effective in modeling the translation process, but they are highly dependent on the large-scale parallel corpus. When applied on zero resource language pairs such as the two languages that do not have direct parallel corpus, the NMT systems perform well below the satisfactory level. To address such problem, three NMT paradigms are explored. The first is the triangular NMT systems that add one additional resource rich language to the zero resource language pair to build a triangular translation scenario (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017), the second is the multilingual translation system that concatenates parallel corpora of different language pairs and builds one NMT model for all (Johnson et al., 2017), the third is the unsupervised NMT systems that do not use any parallel data resources (Artetxe et al., 2018; Lample et al., 2018). Our work is closely related to the first paradigm in which source language, pivot language, and target language form a triangular translation scenario. In our setting, the target language {sentence, summary} pair and the source language sentence form the t"
P19-1305,D18-1399,0,0.0519969,"Missing"
P19-1305,P18-1163,0,0.0181086,"he NMT system involved in all our experiments 4 https://github.com/pytorch/fairseq is Transformer, with the same parameter setup to those of ASSUM systems. It is trained on 1.25M sentence pairs extracted from LDC corpora5 , and is evaluated on NIST sets using multibleu.perl. Chinese-to-English results of caseinsensitive BLEU and English-to-Chinese results of character-based BLEU are reported in Table 1. Since there are four English references for one Chinese sentence in NIST evaluation sets, we report averaged BLEU of four English input sentences in English-to-Chinese translation. Compared to Cheng et al. (2018) on Chineseto-English translation, which targets at robust machine translation and uses the same data to ours, our Transformer significantly outperforms their work, indicating that we build a solid system for machine translation. 4.3 Experimental Results Monolingual ASSUM Performance We build a strong monolingual ASSUM system as shown in Table 2. The comparison is made between our basis architecture Transformer and previous works including state-of-the-art monolingual ASSUM systems. The work of ABS+ (Rush et al., 2015) is the pioneer work of using neural models for monolingual ASSUM. The works"
P19-1305,N16-1012,0,0.0342663,"s-lingual ASSUM. This alleviates the data scarcity that no cross-lingual ASSUM corpus is available. • Extensive experimental results on two benchmark datasets show that our proposed method is able to perform better than several baselines and related works, and significantly reduce the performance gap between the crosslingual ASSUM and the monolingual ASSUM. 2 2.1 Related Work Monolingual ASSUM There are various methods exploring the effective way to model the monolingual ASSUM process including statistical models (Banko et al., 2000; Cohn and Lapata, 2008) or neural models (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). Neural models become dominant in this task since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). On the basis of the sequence-tosequence neural architecture, there are many further explorations such as using rich linguistic features and large vocabulary set (Nallapati et al., 2016), global training procedures on the sentence level (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018; Wang et al., 2018), topic enhancement in the summaries (Wang et al., 2018), additional selective gate networks in the enco"
P19-1305,C08-1018,0,0.0490986,"h the true summaries to build a training corpus for the cross-lingual ASSUM. This alleviates the data scarcity that no cross-lingual ASSUM corpus is available. • Extensive experimental results on two benchmark datasets show that our proposed method is able to perform better than several baselines and related works, and significantly reduce the performance gap between the crosslingual ASSUM and the monolingual ASSUM. 2 2.1 Related Work Monolingual ASSUM There are various methods exploring the effective way to model the monolingual ASSUM process including statistical models (Banko et al., 2000; Cohn and Lapata, 2008) or neural models (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). Neural models become dominant in this task since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). On the basis of the sequence-tosequence neural architecture, there are many further explorations such as using rich linguistic features and large vocabulary set (Nallapati et al., 2016), global training procedures on the sentence level (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018; Wang et al., 2018), topic enhancement in the summaries (Wang et a"
P19-1305,N18-1033,0,0.152927,"ASSUM process including statistical models (Banko et al., 2000; Cohn and Lapata, 2008) or neural models (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016). Neural models become dominant in this task since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). On the basis of the sequence-tosequence neural architecture, there are many further explorations such as using rich linguistic features and large vocabulary set (Nallapati et al., 2016), global training procedures on the sentence level (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018; Wang et al., 2018), topic enhancement in the summaries (Wang et al., 2018), additional selective gate networks in the encoder (Zhou et al., 2017), and facts fusion measures (Cao et al., 2018). 2.2 Zero Resource Neural Machine Translation Current state-of-the-art NMT models are effective in modeling the translation process, but they are highly dependent on the large-scale parallel corpus. When applied on zero resource language pairs such as the two languages that do not have direct parallel corpus, the NMT systems perform well below the satisfactory level. To address such problem, three NMT p"
P19-1305,D15-1229,0,0.715344,"qual contribution. monolingual ASSUM corpus, the cross-lingual ASSUM is seldom explored due to the lack of training corpus. This zero-shot challenge drives the cross-lingual ASSUM to resort to two existing independent techniques, i.e., the monolingual ASSUM and the bilingual translation. The both techniques should be leveraged together to overcome the difficulty of data scarcity in the cross-lingual ASSUM. Regarding the techniques of the monolingual ASSUM, neural methods become dominant in this area since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). The corpus consists of huge number of source-summary pairs, and neural methods model these pairs as as a sequence-to-sequence task by encoding the source sentence into vectorized information and decoding it into the abstractive summary. Regarding the techniques of the bilingual translation, recent years witnessed the method transition from statistical machine translation (SMT) (Koehn et al., 2003) to neural machine translation (NMT). NMT employs the sequence-to-sequence architecture with various implementations such as RNN-based (Sutskever et al., 2014; Bahdanau et al., 2015), CNN-based (Geh"
P19-1305,Q17-1024,0,0.037043,"n applied on zero resource language pairs such as the two languages that do not have direct parallel corpus, the NMT systems perform well below the satisfactory level. To address such problem, three NMT paradigms are explored. The first is the triangular NMT systems that add one additional resource rich language to the zero resource language pair to build a triangular translation scenario (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017), the second is the multilingual translation system that concatenates parallel corpora of different language pairs and builds one NMT model for all (Johnson et al., 2017), the third is the unsupervised NMT systems that do not use any parallel data resources (Artetxe et al., 2018; Lample et al., 2018). Our work is closely related to the first paradigm in which source language, pivot language, and target language form a triangular translation scenario. In our setting, the target language {sentence, summary} pair and the source language sentence form the triangle in which the target language sentence functions as the pivot. We adopt the teacher-student framework that is also applied 3163 teacher in Chen et al. (2017), but we have significant difference to them in"
P19-1305,N03-1017,0,0.062611,"Missing"
P19-1305,J82-2005,0,0.347435,"Missing"
P19-1305,W04-1013,0,0.0537104,"luation Metric Transformer is employed as our basis architecture4 (Vaswani et al., 2017). Six layers are stacked in both the encoder and decoder, and the dimensions of the embedding vectors and all hidden vectors are set 512. We set eight heads in the multi-head attention. The source embedding, the target embedding and the linear sublayer are shared in the teacher networks, while are not shared in the student networks. Byte-pair encoding is employed with a vocabulary of about 32k tokens on English side and Chinese side respectively (Sennrich et al., 2016b). During evaluation, we employ ROUGE (Lin, 2004) as our evaluation metric. On Gigaword, the full-length F-1 based ROUGE scores are reported. On DUC2004, the recall based ROUGE scores are reported to be consistent with previous works. NMT Performance The NMT system involved in all our experiments 4 https://github.com/pytorch/fairseq is Transformer, with the same parameter setup to those of ASSUM systems. It is trained on 1.25M sentence pairs extracted from LDC corpora5 , and is evaluated on NIST sets using multibleu.perl. Chinese-to-English results of caseinsensitive BLEU and English-to-Chinese results of character-based BLEU are reported in"
P19-1305,K16-1028,0,0.133639,"d on the large-scale ∗ Equal contribution. monolingual ASSUM corpus, the cross-lingual ASSUM is seldom explored due to the lack of training corpus. This zero-shot challenge drives the cross-lingual ASSUM to resort to two existing independent techniques, i.e., the monolingual ASSUM and the bilingual translation. The both techniques should be leveraged together to overcome the difficulty of data scarcity in the cross-lingual ASSUM. Regarding the techniques of the monolingual ASSUM, neural methods become dominant in this area since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). The corpus consists of huge number of source-summary pairs, and neural methods model these pairs as as a sequence-to-sequence task by encoding the source sentence into vectorized information and decoding it into the abstractive summary. Regarding the techniques of the bilingual translation, recent years witnessed the method transition from statistical machine translation (SMT) (Koehn et al., 2003) to neural machine translation (NMT). NMT employs the sequence-to-sequence architecture with various implementations such as RNN-based (Sutskever et al., 2014; Bahdanau et al., 201"
P19-1305,D15-1044,0,0.593029,"udies that are based on the large-scale ∗ Equal contribution. monolingual ASSUM corpus, the cross-lingual ASSUM is seldom explored due to the lack of training corpus. This zero-shot challenge drives the cross-lingual ASSUM to resort to two existing independent techniques, i.e., the monolingual ASSUM and the bilingual translation. The both techniques should be leveraged together to overcome the difficulty of data scarcity in the cross-lingual ASSUM. Regarding the techniques of the monolingual ASSUM, neural methods become dominant in this area since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). The corpus consists of huge number of source-summary pairs, and neural methods model these pairs as as a sequence-to-sequence task by encoding the source sentence into vectorized information and decoding it into the abstractive summary. Regarding the techniques of the bilingual translation, recent years witnessed the method transition from statistical machine translation (SMT) (Koehn et al., 2003) to neural machine translation (NMT). NMT employs the sequence-to-sequence architecture with various implementations such as RNN-based (Sutskever et al., 20"
P19-1305,P16-1009,0,0.549035,"stem. We use the teacher-student framework in which the monolingual ASSUM system is taken as the teacher and the cross-lingual ASSUM system is the student. The teacher let the student to simulate both the summary word distribution and attention weights according to those of the teacher networks. In comparison to the pseudo summaries used in the work of Ayana et al. (2018), we generate pseudo sources instead and use true summaries to constitute source-summary pairs. This is motivated by the successful application of backtranslation which generates pseudo-source paired with true-target for NMT (Sennrich et al., 2016a; Lample et al., 2018). The main contributions of this paper include: • We propose teaching both summary word generation distribution and attention weights in the cross-lingual ASSUM networks by using the monolingual ASSUM networks. The distribution teacher is directly from the monolingual ASSUM, while the attention weights teacher is obtained by an attention relay mechanism. • We use a back-translation procedure that generates pseudo source sentences paired with the true summaries to build a training corpus for the cross-lingual ASSUM. This alleviates the data scarcity that no cross-lingual"
P19-1305,P16-1162,0,0.763181,"stem. We use the teacher-student framework in which the monolingual ASSUM system is taken as the teacher and the cross-lingual ASSUM system is the student. The teacher let the student to simulate both the summary word distribution and attention weights according to those of the teacher networks. In comparison to the pseudo summaries used in the work of Ayana et al. (2018), we generate pseudo sources instead and use true summaries to constitute source-summary pairs. This is motivated by the successful application of backtranslation which generates pseudo-source paired with true-target for NMT (Sennrich et al., 2016a; Lample et al., 2018). The main contributions of this paper include: • We propose teaching both summary word generation distribution and attention weights in the cross-lingual ASSUM networks by using the monolingual ASSUM networks. The distribution teacher is directly from the monolingual ASSUM, while the attention weights teacher is obtained by an attention relay mechanism. • We use a back-translation procedure that generates pseudo source sentences paired with the true summaries to build a training corpus for the cross-lingual ASSUM. This alleviates the data scarcity that no cross-lingual"
P19-1305,P11-1155,0,0.51672,"t al., 2014; Bahdanau et al., 2015), CNN-based (Gehring et al., 2017), and Transformer (Vaswani et al., 2017). Early works on the cross-lingual ASSUM leverage the above two techniques through using bilingual features to cooperate with the monolingual ASSUM based on the data condition that largescale monolingual ASSUM corpus is not available while large-scale translation corpora are easy to obtain. They utilize bilingual features such as phrase pairs or predicate-argument parallel structures, which are obtained from SMT systems, to achieve extractive or abstractive cross-lingual summarization (Wan, 2011; Yao et al., 2015; Zhang et al., 2016). 3162 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3162–3172 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Recently, Ayana et al. (2018) propose the first large-scale corpus-based cross-lingual ASSUM system in which the ASSUM corpus is monolingual. They generate summaries using the monolingual ASSUM system, and train the cross-lingual ASSUM based on these pseudo summaries. On the contrary, we propose in this paper to use genuine summaries paired with the gener"
P19-1305,P10-1094,0,0.467142,"opt the teacher-student framework that is also applied 3163 teacher in Chen et al. (2017), but we have significant difference to them in that we generate pseudo source while they generate pseudo target, which results in different teacher-student networks. 2.3 3.1 NMT tgt lang sentence tgt lang summary student Cross-lingual Summarization Early explorations on cross-lingual summarization mainly depend on the traditional monolingual summarization methods, and integrate bilingual parallel informations into the monolingual methods through sentence selection based on translation quality estimation (Wan et al., 2010), sentence ranking based on cross-lingual sentence similarity (Wan, 2011), or abstractive summarization based on phrase pair (Yao et al., 2015) and predicate-argument structure fusing (Zhang et al., 2016). The first cross-lingual ASSUM system based on the large-scale monolingual ASSUM corpus is proposed by Ayana et al. (2018), which is most related to our work. It is motivated by the triangular NMT systems with pseudo target in the teacher-student networks. In contrast, we use pseudo source and apply different teacher-student networks. 3 src lang sentence Our Approach Overall Framework To over"
P19-1305,D15-1012,0,0.31399,"Missing"
P19-1305,P17-1101,0,0.0238251,"lapati et al., 2016). Neural models become dominant in this task since the creation of the large-scale ASSUM corpus (Rush et al., 2015; Nallapati et al., 2016; Hu et al., 2015). On the basis of the sequence-tosequence neural architecture, there are many further explorations such as using rich linguistic features and large vocabulary set (Nallapati et al., 2016), global training procedures on the sentence level (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018; Wang et al., 2018), topic enhancement in the summaries (Wang et al., 2018), additional selective gate networks in the encoder (Zhou et al., 2017), and facts fusion measures (Cao et al., 2018). 2.2 Zero Resource Neural Machine Translation Current state-of-the-art NMT models are effective in modeling the translation process, but they are highly dependent on the large-scale parallel corpus. When applied on zero resource language pairs such as the two languages that do not have direct parallel corpus, the NMT systems perform well below the satisfactory level. To address such problem, three NMT paradigms are explored. The first is the triangular NMT systems that add one additional resource rich language to the zero resource language pair to"
P19-1649,Q18-1036,0,0.0542762,"14), because both the input and † x3 x2 1: iban Attention in Lattice Transformer Encoder Introduction ∗ x1 Attention in Standard Transformer Encoder output consist of sequential tokens. Therefore, in most neural speech translation, such as that of (Bojar et al., 2018), the input to the translation system is usually the 1-best hypothesis from the ASR instead of the word lattice output with its corresponding probability scores. How to consume word lattice rather than sequential input has been substantially researched in several natural language processing (NLP) tasks, such as language modeling (Buckman and Neubig, 2018), Chinese Named Entity Recognition (NER) (Zhang and Yang, 2018), and NMT (Su et al., 2017). Additionally, some pioneering works (Adams et al., 2016; Sperber et al., 2017; Osamura et al., 2018) demonstrated the potential improvements in speech translation by leveraging the additional information and uncertainty of the packed lattice structure produced by ASR acoustic model. Efforts have since continued to push the boundaries of long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) models. More precisely, most previous works are in line with the existing method Tree-LSTMs (Tai et al.,"
P19-1649,D17-1145,0,0.463385,"ial tokens. Therefore, in most neural speech translation, such as that of (Bojar et al., 2018), the input to the translation system is usually the 1-best hypothesis from the ASR instead of the word lattice output with its corresponding probability scores. How to consume word lattice rather than sequential input has been substantially researched in several natural language processing (NLP) tasks, such as language modeling (Buckman and Neubig, 2018), Chinese Named Entity Recognition (NER) (Zhang and Yang, 2018), and NMT (Su et al., 2017). Additionally, some pioneering works (Adams et al., 2016; Sperber et al., 2017; Osamura et al., 2018) demonstrated the potential improvements in speech translation by leveraging the additional information and uncertainty of the packed lattice structure produced by ASR acoustic model. Efforts have since continued to push the boundaries of long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) models. More precisely, most previous works are in line with the existing method Tree-LSTMs (Tai et al., 6475 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6475–6484 c Florence, Italy, July 28 - August 2, 2019. 2019 Associat"
P19-1649,P18-1027,0,0.0219099,"us works are in line with the existing method Tree-LSTMs (Tai et al., 6475 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6475–6484 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2015), adapting to task-specific variant LatticeLSTMs that can successfully handle lattices and robustly establish better performance than the original models. However, the inherently sequential nature still remains in Lattice-LSTMs due to the topological representation of the lattice graph, precluding long-path dependencies (Khandelwal et al., 2018) and parallelization within training examples that are the fundamental constraint of LSTMs. In this work, we introduce a generalization of the standard transformer architecture to accept lattice-structured network topologies. The standard transformer is a transduction model relying entirely on attention modules to compute latent representations, e.g., the self-attention requires to calculate the intra-attention of every two tokens for each sequence example. Latest works such as (Yu et al., 2018; Devlin et al., 2018; Lample et al., 2018; Su et al., 2018) empirically find that transformer can ou"
P19-1649,P18-1007,0,0.0183717,"rinciple possible to use them, for example in attention weights reweighing, to increase the uncertainty of the attention for other alternative tokens. Our lattice attention is controllable and flexible enough for the utilization of each score. The lattice transformer can readily consume the lattice input alone if the scores are unavailable. A common application is found in the Chinese NER task, in which a Chinese sentence could possibly have multiple word segmentation possibilities (Zhang and Yang, 2018). Furthermore, different BPE operations (Sennrich et al., 2016) or probabilistic subwords (Kudo, 2018) can also bring similar uncertainty to subword candidates and form a compact lattice structure. In summary, this paper makes the following main contributions. i) To our best knowledge, we are the first to propose a novel attention mechanism that consumes a word lattice and the probability scores from the ASR system. ii) The proposed approach is naturally applied to both the encoder self-attention and encoder-decoder attention. iii) Another appealing feature is that the lattice transformer can be reduced to standard latticeto-sequence model without probability scores, fitting the text translati"
P19-1649,D18-1549,0,0.0171291,"the lattice graph, precluding long-path dependencies (Khandelwal et al., 2018) and parallelization within training examples that are the fundamental constraint of LSTMs. In this work, we introduce a generalization of the standard transformer architecture to accept lattice-structured network topologies. The standard transformer is a transduction model relying entirely on attention modules to compute latent representations, e.g., the self-attention requires to calculate the intra-attention of every two tokens for each sequence example. Latest works such as (Yu et al., 2018; Devlin et al., 2018; Lample et al., 2018; Su et al., 2018) empirically find that transformer can outperform LSTMs by a large margin, and the success is mainly attributed to selfattention. In our lattice transformer, we propose a lattice relative positional attention mechanism that can incorporate the probability scores of ASR word lattices. The major difference with the selfattention in transformer encoder is illustrated in Figure 1. We first borrow the idea from the relative positional embedding (Shaw et al., 2018) to maximally encode the information of the lattice graph into its corresponding relative positional matrix. This desig"
P19-1649,2013.iwslt-papers.14,0,0.0635657,"major difference with the selfattention in transformer encoder is illustrated in Figure 1. We first borrow the idea from the relative positional embedding (Shaw et al., 2018) to maximally encode the information of the lattice graph into its corresponding relative positional matrix. This design essentially does not allow a token to pay attention to any token that has not appeared in a shared path. Secondly, the attention weights depend not only on the query and key representations in the standard attention module, but also on the marginal / forward / backward probability scores (Rabiner, 1989; Post et al., 2013) derived from the upstream systems (such as ASR). Instead of 1-best hypothesis alone (though it is based on forward scores), the additional probability scores have rich information about the distribution of each path (Sperber et al., 2017). It is in principle possible to use them, for example in attention weights reweighing, to increase the uncertainty of the attention for other alternative tokens. Our lattice attention is controllable and flexible enough for the utilization of each score. The lattice transformer can readily consume the lattice input alone if the scores are unavailable. A comm"
P19-1649,P16-1162,0,0.0564317,"on of each path (Sperber et al., 2017). It is in principle possible to use them, for example in attention weights reweighing, to increase the uncertainty of the attention for other alternative tokens. Our lattice attention is controllable and flexible enough for the utilization of each score. The lattice transformer can readily consume the lattice input alone if the scores are unavailable. A common application is found in the Chinese NER task, in which a Chinese sentence could possibly have multiple word segmentation possibilities (Zhang and Yang, 2018). Furthermore, different BPE operations (Sennrich et al., 2016) or probabilistic subwords (Kudo, 2018) can also bring similar uncertainty to subword candidates and form a compact lattice structure. In summary, this paper makes the following main contributions. i) To our best knowledge, we are the first to propose a novel attention mechanism that consumes a word lattice and the probability scores from the ASR system. ii) The proposed approach is naturally applied to both the encoder self-attention and encoder-decoder attention. iii) Another appealing feature is that the lattice transformer can be reduced to standard latticeto-sequence model without probabi"
P19-1649,P15-1150,0,0.129377,"Missing"
P19-1649,P18-1144,0,0.211472,"e Transformer Encoder Introduction ∗ x1 Attention in Standard Transformer Encoder output consist of sequential tokens. Therefore, in most neural speech translation, such as that of (Bojar et al., 2018), the input to the translation system is usually the 1-best hypothesis from the ASR instead of the word lattice output with its corresponding probability scores. How to consume word lattice rather than sequential input has been substantially researched in several natural language processing (NLP) tasks, such as language modeling (Buckman and Neubig, 2018), Chinese Named Entity Recognition (NER) (Zhang and Yang, 2018), and NMT (Su et al., 2017). Additionally, some pioneering works (Adams et al., 2016; Sperber et al., 2017; Osamura et al., 2018) demonstrated the potential improvements in speech translation by leveraging the additional information and uncertainty of the packed lattice structure produced by ASR acoustic model. Efforts have since continued to push the boundaries of long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) models. More precisely, most previous works are in line with the existing method Tree-LSTMs (Tai et al., 6475 Proceedings of the 57th Annual Meeting of the Association"
P19-1649,N18-2074,0,0.410569,"-attention of every two tokens for each sequence example. Latest works such as (Yu et al., 2018; Devlin et al., 2018; Lample et al., 2018; Su et al., 2018) empirically find that transformer can outperform LSTMs by a large margin, and the success is mainly attributed to selfattention. In our lattice transformer, we propose a lattice relative positional attention mechanism that can incorporate the probability scores of ASR word lattices. The major difference with the selfattention in transformer encoder is illustrated in Figure 1. We first borrow the idea from the relative positional embedding (Shaw et al., 2018) to maximally encode the information of the lattice graph into its corresponding relative positional matrix. This design essentially does not allow a token to pay attention to any token that has not appeared in a shared path. Secondly, the attention weights depend not only on the query and key representations in the standard attention module, but also on the marginal / forward / backward probability scores (Rabiner, 1989; Post et al., 2013) derived from the upstream systems (such as ASR). Instead of 1-best hypothesis alone (though it is based on forward scores), the additional probability scor"
P19-1649,W18-6401,0,\N,Missing
W10-1702,P07-2045,0,0.00952282,"sh. Chinese-to-English tasks are based on training data for the NIST 1 2009 evaluation Chinese-toEnglish track. All the allowed bilingual corpora have been used for estimating the translation model. We trained two language models: the first one is a 5-gram LM which is estimated on the target side of the parallel data. The second is a 5- 1 1,506 1,664 1,357 - Table 1: Statistics of training, dev, and test sets for Chinese-to-English task. We carried out experiments based on translation N-best lists generated by a state-of-the-art phrase-based statistical machine translation system, similar to (Koehn et al., 2007). In detail, the phrase table is derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a log-linear feature combination that includes language models, the distortion components, translation model, phrase and word penalties. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 3.1 |S| |W| |S| |S| |S| |S| Eng 10.1M 270.0M 279.1M 2 http://www.nist.gov/speech/tests/mt 14 http://www.st"
W10-1702,N04-1022,0,0.317546,"ments across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, De11 P"
W10-1702,D08-1076,0,0.0202448,"N-best lists generated by a state-of-the-art phrase-based statistical machine translation system, similar to (Koehn et al., 2007). In detail, the phrase table is derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a log-linear feature combination that includes language models, the distortion components, translation model, phrase and word penalties. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 3.1 |S| |W| |S| |S| |S| |S| Eng 10.1M 270.0M 279.1M 2 http://www.nist.gov/speech/tests/mt 14 http://www.statmt.org/wmt06/ 20 additional features. In this experiment, the former is about 10 times faster than the latter in terms of processing time, as shown in Table 3. testset NIST’06 NIST’08 baseline 35.70 28.60 rescoring 36.01 28.97 three-pass 35.98 28.99 FCD 36.00 29.10 Fwd. 36.13 29.19 Bwd. 36.11 29.20 Bid. 36.20 29.28 In our second experiment, we set the size of N-best list N equal to 10,000 for both Chinese-toEnglish and German-to-English tasks. The results are reported in Table 4. The s"
W10-1702,P03-1021,0,0.0144611,"Missing"
W10-1702,N04-1021,0,0.030639,"performance. Experimental results show consistent improvements across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Theref"
W10-1702,P02-1040,0,0.101853,"Missing"
W10-1702,D08-1065,0,0.239694,"of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, De11 Proceedings of the Joint 5th Workshop on Statist"
W10-1702,P08-1025,0,0.021642,"airs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, De11 Proceedings of the Joint"
W10-1702,2007.mtsummit-papers.15,1,0.906463,"am n-gram one week's expansion new partial hyp. one week's work about one week's work new about week's work hypotheses one weeks' work . one week's work . one week's work . Fast Consensus Hypothesis Regeneration Since the three hypothesis regeneration methods with n-gram expansion, confusion network decoding and re-decoding produce very similar performance (Chen et al., 2008), we consider only n-gram expansion method in this paper. N-gram expansion can (almost) fully exploit the search space of target strings which can be generated by an n-gram language model trained on the N-best hypotheses (Chen et al., 2007). Figure 1: Example of original hypotheses; bi-grams collected from them; backward expanding a partial hypothesis via an overlapped n-1-gram; and new hypotheses generated through backward n-gram expansion. 2.1 2.2 Hypothesis regeneration with bidirectional n-gram expansion Feature-based scoring functions To speed up the search, the partial hypotheses are pruned via beam-search in each expanding step. Therefore, the scoring functions applied with the beam-search algorithm are very important. In (Chen et al., 2008), more than 10 additional global features are computed to rank the partial hypothe"
W10-1702,C08-1014,1,0.891638,"-Taché Boulevard, Gatineau (Québec), Canada J8X 3X7 {Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca Nero et al. (2009) proposed a fast consensus decoding (FCD) algorithm in which the similarity scores are computed based on the feature expectations over the translation N-best list or translation forest. It is equivalent to MBR decoding when using a linear similarity function, such as unigram precision. Re-ranking approaches improve performance on an N-best list whose contents are fixed. A complementary strategy is to augment the contents of an N-best list in order to broaden the search space. Chen et al (2008) have proposed a three-pass SMT process, in which a hypothesis regeneration pass is added between the decoding and rescoring passes. New hypotheses are generated based on the original N-best hypotheses through n-gram expansion, confusion-network decoding or re-decoding. All three hypothesis regeneration methods obtained decent and comparable improvements in conjunction with the same rescoring model. However, since the final translation candidates in this approach are produced from different methods, local feature functions (such as translation models and reordering models) of each hypothesis a"
W10-1702,P09-1064,0,\N,Missing
W10-1702,W07-0724,0,\N,Missing
W10-1702,P07-1019,0,\N,Missing
W10-1717,W08-0304,0,0.0612136,"Missing"
W10-1717,W09-0439,1,0.800854,"alf of GigaFrEn; 7. Dynamic LM composed of 4 LMs, each trained on the French half of a parallel corpus (5-gram LM trained on “domain”, 4-gram LM on GigaFrEn, 5-gram LM on news-commentary and 5-gram LM on UN). The F-E system is a mirror image of the E-F system. 3 Details of lattice MERT (LMERT) Our system’s implementation of LMERT (Macherey et al., 2008) is the most notable recent change in our system. As more and more features are included in the loglinear model, especially if they are correlated, N-best MERT (Och, 2003) shows more and more instability, because of convergence to local optima (Foster and Kuhn, 2009). We had been looking for methods that promise more stability and better convergence. LMERT seemed to fit the bill. It optimizes over the complete lattice of candidate translations after a decoding run. This avoids some of the problems of N-best lists, which lack variety, leading to poor local optima and the need for many decoder runs. Though the algorithm is straightforward and is highly parallelizable, attention must be paid to space and time resource issues during implementation. Lattices output by our decoder were large and needed to be shrunk dramatically for the algorithm to function wel"
W10-1717,W07-0717,1,0.856161,"kward conditional probabilities. The lexicalized distortion probabilities are also obtained by adding IBM2 and HMM counts. They involve 6 features (monotone, swap and discontinuous features for following and preceding phrase) and are conditioned on phrase pairs in a model similar to that of Moses (Koehn et al., 2005); a MAP-based backoff smoothing scheme is used to combat data sparseness when estimating these probabilities. Dynamic mixture LMs are linear mixtures of ngram models trained on parallel sub-corpora with weights set to minimize perplexity of the current source text as described in (Foster and Kuhn, 2007); henceforth, we’ll call them “dynamic LMs”. Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Contrary to the usual implementation of distortion limits, we allow a new phrase to end 127 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127–132, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics more than 7 words past the first non-covered word, as long as the new phrase starts within 7 words from the first non-covered word. Notwithstanding the distortion limit, cont"
W10-1717,P07-1019,0,0.047578,"MM counts. They involve 6 features (monotone, swap and discontinuous features for following and preceding phrase) and are conditioned on phrase pairs in a model similar to that of Moses (Koehn et al., 2005); a MAP-based backoff smoothing scheme is used to combat data sparseness when estimating these probabilities. Dynamic mixture LMs are linear mixtures of ngram models trained on parallel sub-corpora with weights set to minimize perplexity of the current source text as described in (Foster and Kuhn, 2007); henceforth, we’ll call them “dynamic LMs”. Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Contrary to the usual implementation of distortion limits, we allow a new phrase to end 127 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127–132, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics more than 7 words past the first non-covered word, as long as the new phrase starts within 7 words from the first non-covered word. Notwithstanding the distortion limit, contiguous phrases can always be swapped. Out-of-vocabulary (OOV) source words are passed through unchanged to the ta"
W10-1717,2005.iwslt-1.8,0,\N,Missing
W10-1717,N04-1033,0,\N,Missing
W10-1717,D08-1076,0,\N,Missing
W10-1717,P03-1021,0,\N,Missing
W11-2105,W05-0909,0,0.163547,"tic information in AMBER. We evaluate its system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following fact"
W11-2105,W10-1749,0,0.0124594,"=1− ∑d i 2 i (n + 1)n(n − 1) (16) where di indicates the distance between the ranks of the i-th element. For example: T: Bob reading book likes R: Bob likes reading book The rank vector of the reference is [1, 2, 3, 4], while the translation rank vector is [1, 3, 4, 2]. The Spearman’s correlation score between these two 2 2 2 vectors is 1 − 0 + (3 − 2) + (4 − 3) + (2 − 4) =0.90. (4 + 1) ⋅ 4 ⋅ (4 − 1) In order to avoid negative values, we normalized the correlation score, obtaining the penalty NSCP: (17) NSCP = (1 + ρ) / 2 Normalized Kendall’s correlation penalty (NKCP): this is adopted from (Birch and Osborne, 2010) and (Isozaki et al., 2010). In the previous example, where the rank vector of the 73 translation is [1, 3, 4, 2], there are C42 = 6 pairs of integers. There are 4 increasing pairs: (1,3), (1,4), (1,2) and (3,4). Kendall’s correlation is defined by: τ = 2× # increasing pairs −1 # all pairs (18) Therefore, Kendall’s correlation for the translation “Bob reading book likes” is 2 × 4 / 6 − 1 =0.33. Again, to avoid negative values, we normalized the coefficient score, obtaining the penalty NKCP: (19) NKCP = (1 + τ ) / 2 2.3 Term weighting The original BLEU metric weights all n-grams equally; howeve"
W11-2105,W08-0309,0,0.0730212,"ng WMT 2008 all-to-English submissions as the dev set. Test sets include WMT 2009 all-to-English, WMT 2010 all-to-English and 2010 English-to-all submissions. Table 1 summarizes the dev and test set statistics. Default settings Table 2: Weight of each penalty 3.3 Evaluation metrics We used Spearman’s rank correlation coefficient to measure the correlation of AMBER with the human judgments of translation at the system level. The human judgment score we used is based on the “Rank” only, i.e., how often the translations of the system were rated as better than the translations from other systems (Callison-Burch et al., 2008). Thus, AMBER and the other metrics were evaluated on how well their rankings correlated with the human ones. For the sentence level, we use consistency rate, i.e., how consistent the ranking of sentence pairs is with the human judgments. 3.4 Results All test results shown in this section are averaged over all three tests described in 3.1. First, we compare AMBER with two of the most widely used metrics: original IBM BLEU and METEOR v1.0. Table 3 gives the results; it shows both the version of AMBER with basic preprocessing, AMBER(1) (with tokenization and lowercasing) and the default version"
W11-2105,E06-1032,0,0.1159,", we decided to make it a modified version of BLEU whose rankings of translations would (ideally) correlate even more highly with human rankings. Thus, our metric is called AMBER: “A Modified Bleu, Enhanced Ranking” metric. Some of the AMBER variants use an information source with a mild linguistic flavour – morphological knowledge about suffixes, roots and prefixes – but otherwise, the metric is based entirely on surface comparisons. 2 AMBER Like BLEU, AMBER is composed of two parts: a score and a penalty. AMBER = score × penalty (1) To address weaknesses of BLEU described in the literature (Callison-Burch et al., 2006; Lavie and Denkowski, 2009), we use more sophisticated formulae to compute the score and penalty. 2.1 Enhancing the score First, we enrich the score part with geometric average of n-gram precisions (AvgP), F-measure derived from the arithmetic averages of precision and recall (Fmean), and arithmetic average of Fmeasure of precision and recall for each n-gram (AvgF). Let us define n-gram precision and recall as follows: 71 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 71–77, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics"
W11-2105,N10-1080,0,0.0252719,"her level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important when choosing a metric to tune an MT system. 3. BLEU seems to be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). When we developed our own metric, we decided to make it a modified version of BLEU whose rankings of translations would (ideally) correlate even more highly with human rankings. Thus, our metric is called AMBER: “A Modified Bleu, Enhanced Ranking” metric. Some of the AMBER variants use an information source with a mild linguistic flavour – morphological knowledge about suffixes, roots and prefixes – but otherwise, the metric is based entirely on surface comparisons. 2 AMBER Like BLEU, AMBER is composed of two parts: a score and a penalty. AMBER = score × penalty (1) To address weaknesses of"
W11-2105,P08-1007,0,0.0929208,"th human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed"
W11-2105,D08-1064,0,0.0298414,"precision and recall for each n-gram (AvgF) is given by: 1 N p ( n) r ( n) (8) AvgF ( N , α ) = ∑ N n =1 αp (n ) + (1 − α ) r ( n) The score is the weighted average of the three values: AvgP, Fmean, and AvgF. score( N ) = θ1 × AvgP( N ) + θ 2 × Fmean( N , M ,α ) (9) + (1 − θ1 − θ 2 ) × AvgF ( N ,α ) The free parameters N, M, α , θ1 and θ 2 were manually tuned on a dev set. 2.2 Various penalties Instead of the original brevity penalty, we experimented with a product of various penalties: P penalty = ∏ peniwi (10) i =1 where wi is the weight of each penalty peni. Strict brevity penalty (SBP): (Chiang et al., 2008) proposed this penalty. Let ti be the transla72 tion of input sentence i, and let ri be its reference (or if there is more than one, the reference whose length in words |ri |is closest to length |ti |). Set  ∑i |ri | SBP = exp1 −  ∑ min{ |ti |, |ri |}  i   (11) Strict redundancy penalty (SRP): long sentences are preferred by recall. Since we rely on both recall and precision to compute the score, it is necessary to punish the sentences that are too long.  ∑ max{ |ti |, |ri |}   SRP = exp1 − i   | r | ∑i i   (12) Character-based strict brevity penalty (CSBP) and Character-based"
W11-2105,W10-1751,0,0.0211502,"s system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following facts: 1. BLEU is language independent (exce"
W11-2105,W10-1753,0,0.0605815,"Missing"
W11-2105,D10-1092,0,0.035676,"s the reference. To make AMBER more portable across all Indo-European languages, we use short words (those with fewer than 4 characters) to approximate the stop words. SWDP = exp(− |a−b| ) # unigram(r ) (15) where a and b are the number of short words in the translation and reference respectively. Long word difference penalty (LWDP): is defined similarly to SWDP. LWDP = exp(− |c−d | ) # unigram(r ) (15) where c and d are the number of long words (those longer than 3 characters) in the translation and reference respectively. Normalized Spearman’s correlation penalty (NSCP): we adopt this from (Isozaki et al., 2010). This penalty evaluates similarity in word order between the translation and reference. We first determine word correspondences between the translation and reference; then, we rank words by their position in the sentences. Finally, we compute Spearman’s correlation between the ranks of the n words common to the translation and reference. ρ =1− ∑d i 2 i (n + 1)n(n − 1) (16) where di indicates the distance between the ranks of the i-th element. For example: T: Bob reading book likes R: Bob likes reading book The rank vector of the reference is [1, 2, 3, 4], while the translation rank vector is"
W11-2105,W04-1013,0,0.0117415,"weight each n-gram according to its information value. 2.4 Four matching strategies In the original BLEU metric, there is only one matching strategy: n-gram matching. In AMBER, we provide four matching strategies (the best AMBER variant used three of these): 1. N-gram matching: involved in computing precision and recall. 2. Fixed-gap n-gram: the size of the gap between words “word1 [] word2” is fixed; involved in computing precision only. 3. Flexible-gap n-gram: the size of the gap between words “word1 * word2” is flexible; involved in computing precision only. 4. Skip n-gram: as used ROUGE (Lin, 2004); involved in computing precision only. 2.5 Input preprocessing The AMBER score can be computed with different types of preprocessing. When using more than one type, we computed the final score as an average over runs, one run per type (our default AMBER variant used three of the preprocessing types): Final _ AMBER = 1 T ∑ AMBER(t ) T t =1 We provide 8 types of possible text input: 0. Original - true-cased and untokenized. 1. Normalized - tokenized and lower-cased. (All variants 2-7 below also tokenized and lower-cased.) 2. “Stemmed” - each word only keeps its first 4 letters. 3. “Suffixed” -"
W11-2105,W10-1754,0,0.490409,"shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important wh"
W11-2105,P09-1034,0,0.0239911,"development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important when choosing a metric to tune an MT system. 3. BLEU seems to be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from"
W11-2105,P02-1040,0,0.108716,"ation evaluation metric: AMBER, which is based on the metric BLEU but incorporates recall, extra penalties, and some text processing variants. There is very little linguistic information in AMBER. We evaluate its system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several o"
W11-2105,2006.amta-papers.25,0,0.171115,"t incorporates recall, extra penalties, and some text processing variants. There is very little linguistic information in AMBER. We evaluate its system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment"
W11-2105,W09-0441,0,0.0621069,"ce-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Introduction Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems. Several metrics have been proposed in recent years. Metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching. METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables. More sophisticated metrics such as RTE (Pado et al., 2009) and DCULFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations. Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisio"
W11-2105,W07-0734,0,\N,Missing
W12-3104,W08-0309,0,0.0671104,"Missing"
W12-3104,P12-1098,1,0.790951,"0 and p20 = 0 . Let Then we obtain nd +1+1+9 ) Its ρ = 1 - 6(14(16 −1) = −0.2 ; however, its th P1: 1 2 3 4 5 6 (the 2 word “the”, 4 word “of” and 6th word “,” in the reference are not aligned to any word in the hypothesis. Thus, their positions are not in P1, so the positions of the matching words “in winter 2010 I visited Paris” are normalized to 1 2 3 4 5 6) P2: 4 5 6 1 3 2 (the word “’s” was unaligned). 60 v2 = 1 − DIST2 ( P1 , P2 ) n2 −1 (7) As with v1, v2 is also from 0 to 1, and larger values indicate more similar permutations. The ordering measure vs is the harmonic mean of v1 and v2 (Chen et al., 2012): (8) vs = 2 / (1/v1 + 1/v2 ) . In (Chen et al., 2012) we found this to be slightly more effective than the geometric mean. vs in (8) is computed at segment level. We compute document level ordering vD with a weighted arithmetic mean: l vD ∑ v × len ( R) = ∑ len ( R) s =1 s l s =1 s (9) s where l is the number of segments of the document, and len(R) is the length of the reference after text preprocessing. vs is the segment-level ordering penalty. Recall that the penalty part of AMBER is the weighted product of several component penalties. In the original version of AMBER, there were 10 compone"
W12-3104,W11-2105,1,0.899965,"o AMBER. The first one is incorporation of a new ordering penalty; the second one is the use of the downhill simplex algorithm to tune the weights for the components of AMBER. We tested the impact of the two changes, using data from the WMT metrics task. Each of the changes by itself improved the performance of AMBER, and the two together yielded even greater improvement, which in some cases was more than additive. The new version of AMBER clearly outperforms BLEU in terms of correlation with human judgment. 1 2 Introduction AMBER is a machine translation evaluation metric first described in (Chen and Kuhn, 2011). It is designed to have the advantages of BLEU (Papineni et al., 2002), such as nearly complete language independence and rapid computability, while attaining even higher correlation with human judgment. According to the paper just cited: “It can be thought of as a weighted combination of dozens of computationally cheap features based on word surface forms for evaluating MT quality”. Many recently defined machine translation metrics seek to exploit deeper sources of knowledge than are available to BLEU, such as external lexical and syntactic resources. Unlike these and like BLEU, AMBER relies"
W12-3104,D10-1092,0,0.01535,"s. The first is absolute permutation distance: n DIST1 ( P1 , P2 ) = ∑ |p1i − p2i | (4) i =1 Let DIST1 ( P1 , P2 ) n( n + 1) / 2 ν1 = 1 − (5) v1 ranges from 0 to 1; a larger value means more similarity between the two permutations. This metric is similar to Spearman’s ρ (Spearman, 1904). However, we have found that ρ punishes long-distance reordering too heavily. For instance, ν 1 is more tolerant than ρ of the movement of “recently” in this example: Ref: Recently , I visited Paris Hyp: I visited Paris recently P1: 1 2 3 4 P2: 2 3 4 1 3.1 Ordering penalty v We use a simple matching algorithm (Isozaki et al., 2010) to do 1-1 word alignment between the hypothesis and the reference. After word alignment, represent the reference by a list of normalized positions of those of its words that were aligned with words in the hypothesis, and represent the hypothesis by a list of positions for the corresponding words in the reference. For both lists, unaligned words are ignored. E.g., let P1 = reference, P2 = hypothesis: P1: p11 p12 p13 p14 … p1i … p1n P2: p12 p22 p23 p24 … p2i … p2n Suppose we have +1+1+ 3 v1 = 1 - 14(4 +1)/2 = 0.4 . Inspired by HMM word alignment (Vogel et al., 1996), our second distance measure"
W12-3104,P02-1040,0,0.0936783,"e second one is the use of the downhill simplex algorithm to tune the weights for the components of AMBER. We tested the impact of the two changes, using data from the WMT metrics task. Each of the changes by itself improved the performance of AMBER, and the two together yielded even greater improvement, which in some cases was more than additive. The new version of AMBER clearly outperforms BLEU in terms of correlation with human judgment. 1 2 Introduction AMBER is a machine translation evaluation metric first described in (Chen and Kuhn, 2011). It is designed to have the advantages of BLEU (Papineni et al., 2002), such as nearly complete language independence and rapid computability, while attaining even higher correlation with human judgment. According to the paper just cited: “It can be thought of as a weighted combination of dozens of computationally cheap features based on word surface forms for evaluating MT quality”. Many recently defined machine translation metrics seek to exploit deeper sources of knowledge than are available to BLEU, such as external lexical and syntactic resources. Unlike these and like BLEU, AMBER relies entirely on matching surface forms in tokens in the hypothesis and ref"
W12-3104,C96-2141,0,0.429494,"simple matching algorithm (Isozaki et al., 2010) to do 1-1 word alignment between the hypothesis and the reference. After word alignment, represent the reference by a list of normalized positions of those of its words that were aligned with words in the hypothesis, and represent the hypothesis by a list of positions for the corresponding words in the reference. For both lists, unaligned words are ignored. E.g., let P1 = reference, P2 = hypothesis: P1: p11 p12 p13 p14 … p1i … p1n P2: p12 p22 p23 p24 … p2i … p2n Suppose we have +1+1+ 3 v1 = 1 - 14(4 +1)/2 = 0.4 . Inspired by HMM word alignment (Vogel et al., 1996), our second distance measure is based on jump width. This punishes only once a sequence of words that moves a long distance with the internal word order conserved, rather than on every word. In the following, only two groups of words have moved, so the jump width punishment is light: Ref: In the winter of 2010, I visited Paris Hyp: I visited Paris in the winter of 2010 The second distance measure is n DIST2 ( P1 , P2 ) = ∑ |( p1i − p1i −1 ) − ( p2i − p2i −1 ) |(6) Ref: in the winter of 2010 , I visited Paris Hyp: I visited Paris in 2010 ’s winter i =1 0 1 where we set p = 0 and p20 = 0 . Let"
W12-3104,W10-1703,0,\N,Missing
W14-3346,W11-2103,0,0.0229985,"iques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques. Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine translation tuning. 1 Introduction Since its invention, BLEU (Papineni et al., 2002) has been the most widely used metric for both machine translation (MT) evaluation and tuning. Many other metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et al., 2011; Callison-Burch et al., 2012). However, BLEU remains the de facto standard evaluation and tuning metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important when choosing a tuning metric. 3. BLEU seems to be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). One of the main criticisms of BLEU is that it has a poor correlation with human judgments on the sent"
W14-3346,W12-3102,0,0.0550855,"Missing"
W14-3346,2011.mtsummit-papers.30,1,0.719107,"We test on the evaluation sets from NIST 2006 and 2008. For the Arabic-to-English task, we use the evaluation sets from NIST 2006, 2008, and 2009 as our dev set and two test sets, respectively. Table 4 summarizes the training, dev and test sets. Experiments were carried out with an in-house, state-of-the-art phrase-based system. Each corpus was word-aligned using IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram En3 We use K = 100 in our experiments. 365 http://www.nist.gov/itl/iad/mig/openmt12.cfm 0 1 2 3 4 5 6 7 Tune 27.6 27.6 27.5 27.6 27.6 27.6 27.5 27.6 std 0.1 0.0 0.1 0.1 0.1 0.1 0.1 0.1 MT06 35.6 35.7 35.8 35.8 35.7 35.5 35.7 35.6 std 0.1 0.1 0.1 0.1 0.2 0.1 0.1 0.1 MT08 29.0 29.1 29.1 29.1 29.1 28.9 29.0 29.0 std"
W14-3346,N12-1047,1,0.883272,"l BLEU have been proposed (Lin and Och, 2004; Gao and He, 2013). In this paper, we systematically compare 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques on the WMT metrics task. Moreover, we compare the performance of using the 7 smoothing techniques in statistical machine translation tuning on NIST Chinese-to-English and Arabic-to-English tasks. We show that when tuning optimizes the expected sum of these sentence-level metrics (as advocated by Cherry and Foster (2012) and Gao and He (2013) among others), all of these metrics perform similarly in terms of their ability to produce strong BLEU scores on a held-out test set. 2 BLEU and smoothing 2.1 BLEU Suppose we have a translation T and its reference R, BLEU is computed with precision P (N, T, R) and brevity penalty BP(T,R): BLEU(N, T, R) = P (N, T, R) × BP(T, R) (1) where P (N, T, R) is the geometric mean of ngram precisions: P (N, T, R) = N Y ! N1 pn n=1 362 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 362–367, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association fo"
W14-3346,P11-2031,0,0.0127436,"also conducted a set of experiments with a much larger feature set. This system used only GIZA++ for word alignment, increased the distortion limit from 7 to 9, and is trained on a highquality subset of the parallel corpora used earlier. Most importantly, it includes the full set of sparse phrase-pair features used by both Hopkins and May (2011) and Cherry and Foster (2012), which results in nearly 7,000 features. Our evaluation metric is the original IBM BLEU, which performs case-insensitive matching of n-grams up to n = 4. We perform random replications of parameter tuning, as suggested by Clark et al. (2011). Each replication uses a different random seed to determine the order in which SGD visits tuning sentences. We test for significance using the MultEval tool,4 which uses a stratified approximate randomization test to account for multiple replications. We report results averaged across replications as well as standard deviations, which indicate optimizer stability. Results for the small feature set are shown in Tables 5 and 6. All 7 smoothing techniques, as well as the no smoothing baseline, all yield very similar results on both Chinese and Arabic tasks. We did not find any two results to be"
W14-3346,D08-1089,0,0.0170121,"ish task, we use the evaluation sets from NIST 2006, 2008, and 2009 as our dev set and two test sets, respectively. Table 4 summarizes the training, dev and test sets. Experiments were carried out with an in-house, state-of-the-art phrase-based system. Each corpus was word-aligned using IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram En3 We use K = 100 in our experiments. 365 http://www.nist.gov/itl/iad/mig/openmt12.cfm 0 1 2 3 4 5 6 7 Tune 27.6 27.6 27.5 27.6 27.6 27.6 27.5 27.6 std 0.1 0.0 0.1 0.1 0.1 0.1 0.1 0.1 MT06 35.6 35.7 35.8 35.8 35.7 35.5 35.7 35.6 std 0.1 0.1 0.1 0.1 0.2 0.1 0.1 0.1 MT08 29.0 29.1 29.1 29.1 29.1 28.9 29.0 29.0 std 0.2 0.1 0.1 0.1 0.2 0.2 0.2 0.1 0 1 2 3 4 5 6 7 Table 5: Chinese-to-English Results f"
W14-3346,N13-1048,0,0.202148,"g metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). One of the main criticisms of BLEU is that it has a poor correlation with human judgments on the sentence-level. Because it computes a geometric mean of n-gram precisions, if a higher order n-gram precision (eg. n = 4) of a sentence is 0, then the BLEU score of the entire sentence is 0, no matter how many 1-grams or 2-grams are matched. Therefore, several smoothing techniques for sentence-level BLEU have been proposed (Lin and Och, 2004; Gao and He, 2013). In this paper, we systematically compare 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques on the WMT metrics task. Moreover, we compare the performance of using the 7 smoothing techniques in statistical machine translation tuning on NIST Chinese-to-English and Arabic-to-English tasks. We show that when tuning optimizes the expected sum of these sentence-level metrics (as advocated by Cherry and Foster (2012) and Gao and He (2013) among others), a"
W14-3346,D11-1125,0,0.0319222,"1 0.1 MT08 29.0 29.1 29.1 29.1 29.1 28.9 29.0 29.0 std 0.2 0.1 0.1 0.1 0.2 0.2 0.2 0.1 0 1 2 3 4 5 6 7 Table 5: Chinese-to-English Results for the small feature set tuning task. Results are averaged across 5 replications; std is the standard deviation. glish Gigaword LM. We also conducted a set of experiments with a much larger feature set. This system used only GIZA++ for word alignment, increased the distortion limit from 7 to 9, and is trained on a highquality subset of the parallel corpora used earlier. Most importantly, it includes the full set of sparse phrase-pair features used by both Hopkins and May (2011) and Cherry and Foster (2012), which results in nearly 7,000 features. Our evaluation metric is the original IBM BLEU, which performs case-insensitive matching of n-grams up to n = 4. We perform random replications of parameter tuning, as suggested by Clark et al. (2011). Each replication uses a different random seed to determine the order in which SGD visits tuning sentences. We test for significance using the MultEval tool,4 which uses a stratified approximate randomization test to account for multiple replications. We report results averaged across replications as well as standard deviation"
W14-3346,P04-1077,0,0.55547,"o be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). One of the main criticisms of BLEU is that it has a poor correlation with human judgments on the sentence-level. Because it computes a geometric mean of n-gram precisions, if a higher order n-gram precision (eg. n = 4) of a sentence is 0, then the BLEU score of the entire sentence is 0, no matter how many 1-grams or 2-grams are matched. Therefore, several smoothing techniques for sentence-level BLEU have been proposed (Lin and Och, 2004; Gao and He, 2013). In this paper, we systematically compare 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques on the WMT metrics task. Moreover, we compare the performance of using the 7 smoothing techniques in statistical machine translation tuning on NIST Chinese-to-English and Arabic-to-English tasks. We show that when tuning optimizes the expected sum of these sentence-level metrics (as advocated by Cherry and Foster (2012) and Gao and He (201"
W14-3346,W13-2202,0,0.06674,"task as evaluation metrics, then they were compared as metrics for tuning SMT systems to maximize the sum of expected sentence-level BLEU scores. 3.1 Evaluation task We first compare the correlations with human judgment for the 7 smoothing techniques on WMT data; the development set (dev) is the WMT 2008 all-to-English data; the test sets are the WMT 2012 and WMT 2013 all-to-English, and English-to-all submissions. The languages “all” (“xx” in Table 1) include French, Spanish, German, Czech and Russian. Table 1 summarizes the dev/test set statistics. Following WMT 2013’s metric task (Mach´acˇ ek and Bojar, 2013), for the segment level, we use Kendall’s rank correlation coefficient τ to measure the correlation with human judgment: #concordant-pairs − #discordant-pairs #concordant-pairs + #discordant-pairs (12) We extract all pairwise comparisons where one system’s translation of a particular segment was judged to be better than the other system’s translation, i.e., we removed all tied human judgments for a particular segment. If two translations for a particular segment are assigned the same BLEU score, then the #concordant-pairs and #discordant-pairs both get a half count. In this way, we can keep th"
W14-3346,P02-1040,0,0.108856,"ic. However, because BLEU computes a geometric mean of n-gram precisions, it often correlates poorly with human judgment on the sentence-level. Therefore, several smoothing techniques have been proposed. This paper systematically compares 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques. Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine translation tuning. 1 Introduction Since its invention, BLEU (Papineni et al., 2002) has been the most widely used metric for both machine translation (MT) evaluation and tuning. Many other metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et al., 2011; Callison-Burch et al., 2012). However, BLEU remains the de facto standard evaluation and tuning metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important when choosing a tuning metric. 3. BLEU seems to be the best tun"
W14-3346,N10-1080,0,\N,Missing
W15-3044,2006.amta-papers.25,0,0.0917436,"ge of [0,1]. Syntax−based Fscore 1 Density Density Abstract Introduction The aims of automatic Machine Translation (MT) evaluation metrics, which measure the quality of translations against human references, are twofold. Firstly, they enable rapid comparisons between different statistical machine translation (SMT) systems. Secondly, they are necessary to the tuning of parameter values during system trainings. To attain these goals, many machine translation metrics have been introduced in recent years. For example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. word−based Fscore Figure 2: Clusters of translatio"
W15-3044,P15-2025,1,0.72825,"th multi-level and non-multi-level scoring frameworks. V =< w1 · Voh , w2 · Vwd , w3 · VgRAE , w4 · VtRAE > (10) where Voh is the one-hot representation, Vwd denotes the word representations, and VgRAE and VtRAE are representations learned with greedy RAE and tree-based RAE, respectively. The weights w1 ... w4 are tuned on development data. 3.2 Representation based metric Distributed representations for words and sentences have been shown to significantly boost the performance of a NLP system (Turian et al., 2010). A representation-based translation evaluation metric, DREEM, is introduced in (Anonymous, 2015). The metric has shown to be able to achieve state-of-the-art performance, compared to popular metrics such as BLEU and Meteor. Therefore, in this paper, we also adapt this metric for our experiments. In a nutshell, the DREEM metric evaluates translations by employing three different types of word and sentence representations: one-hot representations, distributed word representations learned from a neural network model, and distributed sentence representations computed with a recursive autoencoder (RAE). Two different RAEbased representations are used in this metric: one is based on a greedy u"
W15-3044,W05-0909,0,0.636809,"ly, they enable rapid comparisons between different statistical machine translation (SMT) systems. Secondly, they are necessary to the tuning of parameter values during system trainings. To attain these goals, many machine translation metrics have been introduced in recent years. For example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. word−based Fscore Figure 2: Clusters of translations based on quality. Both X-axis and Y-axis are in the range of [0,1]. of their quality against human references. As a result, current metrics often perform better for a portion of translations but worse against the others. Conside"
W15-3044,P10-1040,0,0.00916093,". Fmean = Score(t, r) = Cosα (t, r) × Plen (6) In our studies, we fine-tune all the parameters for both multi-level and non-multi-level scoring frameworks. V =< w1 · Voh , w2 · Vwd , w3 · VgRAE , w4 · VtRAE > (10) where Voh is the one-hot representation, Vwd denotes the word representations, and VgRAE and VtRAE are representations learned with greedy RAE and tree-based RAE, respectively. The weights w1 ... w4 are tuned on development data. 3.2 Representation based metric Distributed representations for words and sentences have been shown to significantly boost the performance of a NLP system (Turian et al., 2010). A representation-based translation evaluation metric, DREEM, is introduced in (Anonymous, 2015). The metric has shown to be able to achieve state-of-the-art performance, compared to popular metrics such as BLEU and Meteor. Therefore, in this paper, we also adapt this metric for our experiments. In a nutshell, the DREEM metric evaluates translations by employing three different types of word and sentence representations: one-hot representations, distributed word representations learned from a neural network model, and distributed sentence representations computed with a recursive autoencoder"
W15-3044,W14-3348,0,0.0318647,"f sentence-level scores, with the weights being the reference lengths, as follows. PD len(ri )Scorei Scored = i=1 (2) PD i=1 len(ri ) where Scorei is the score of sentence i, and D is the number of sentences in the document. 3 Evaluation metrics We apply our multi-level approach to two metrics. The first one is Meteor (Banerjee and Lavie, 2005), which has been widely used for machine translation evaluations. The second one is DREEM, a new metric based on distributed representations generated by deep neural networks. 3.1 Metric Meteor We use the latest version of Meteor, i.e. Meteor Universal (Denkowski and Lavie, 2014) in this paper. Meteor computes a one-to-one alignment between matching words in a translation and a reference. The space of possible alignments is constructed by exhaustively identifying all possible matches of the following types: exact word matches, word stem matches, synonym word matches, and matches between phrases listed as paraphrases. Alignment is then conducted as a beam search. From the final alignment, the translation’s Meteor score is calculated as follows. First, content DREEM with a similarity score computed with the Cosine function and a length penalty. Let the size of the vecto"
W15-3044,W13-2256,0,0.0142521,"r example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. word−based Fscore Figure 2: Clusters of translations based on quality. Both X-axis and Y-axis are in the range of [0,1]. of their quality against human references. As a result, current metrics often perform better for a portion of translations but worse against the others. Consider, for example, two widely used metrics, namely the sentence-level Meteor and BLUE. Figure 1 depicts the distributions of the two metrics’ evaluation scores, computed on system outputs for two WMT test sets, i.e., the newstest2013.fr-en and newstest2012.en-cs. As shown in F"
W15-3044,W05-0904,0,0.0418492,"introduced in recent years. For example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. word−based Fscore Figure 2: Clusters of translations based on quality. Both X-axis and Y-axis are in the range of [0,1]. of their quality against human references. As a result, current metrics often perform better for a portion of translations but worse against the others. Consider, for example, two widely used metrics, namely the sentence-level Meteor and BLUE. Figure 1 depicts the distributions of the two metrics’ evaluation scores, computed on system outputs for two WMT test sets, i.e., the newstest2013.fr-en and newstes"
W15-3044,D11-1035,0,0.0155037,"systems. Secondly, they are necessary to the tuning of parameter values during system trainings. To attain these goals, many machine translation metrics have been introduced in recent years. For example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. word−based Fscore Figure 2: Clusters of translations based on quality. Both X-axis and Y-axis are in the range of [0,1]. of their quality against human references. As a result, current metrics often perform better for a portion of translations but worse against the others. Consider, for example, two widely used metrics, namely the sentence-level Meteor and BLU"
W15-3044,P11-1023,0,0.0792917,"pineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) metrics. word−based Fscore Figure 2: Clusters of translations based on quality. Both X-axis and Y-axis are in the range of [0,1]. of their quality against human references. As a result, current metrics often perform better for a portion of translations but worse against the others. Consider, for example, two widely used metrics, namely the sentence-level Meteor and BLUE. Figure 1 depicts the distributions of the two metrics’ evaluation scores, computed on system outputs for two WMT test sets, i.e., the newstest2013.fr-en and newstest2012.en-cs. As shown in Figures 1, the variances of the crea"
W15-3044,W14-3336,0,0.0186478,"sk data. Development sets include WMT 2012 all-to-English, and English-to-all submissions. Test sets contain WMT 2013, and WMT 2014 all-to-English, plus 2013, 2014 Englishto-all submissions. The languages “all” include French, Spanish, German, Czech and Russian. For training the word embedding and recursive auto-encoder model, we used WMT 2014 training data 1 . We used the English, French, German and Czech sentences in “Europarl v7” and “News Commentary” for our experiments. To train the representations for Russian, we used the “Yandex 1M corpus”. 4.2 Results Following WMT 2014’s metric task (Machacek and Bojar, 2014), to measure the correlation with 1 363 http://www.statmt.org/wmt14/translation-task.html metric Original BLEU Sentence BLEU Original Meteor Sentence Meteor M ulti − levelw Meteor M ulti − levelwd Meteor DREEM M ulti − levelw DREEM M ulti − levelwd DREEM Into-English seg τ sys γ – 0.821 0.259 0.841 0.279 0.849 0.279 0.863 0.285 0.871 0.294⋆ 0.885⋆ 0.287 0.875 0.293 0.880 0.303⋆ 0.892⋆ metric Original BLEU Sentence BLEU Original Meteor Sentence Meteor M ulti − levelw Meteor DREEM M ulti − levelw DREEM Out-of-English seg τ sys γ – 0.843 0.221 0.846 0.228 0.845 0.228 0.853 0.234 0.861 0.236 0.904"
W15-3044,P02-1040,0,0.0963605,"Distributions of translation quality. Xaxis is in the range of [0,1]. Syntax−based Fscore 1 Density Density Abstract Introduction The aims of automatic Machine Translation (MT) evaluation metrics, which measure the quality of translations against human references, are twofold. Firstly, they enable rapid comparisons between different statistical machine translation (SMT) systems. Secondly, they are necessary to the tuning of parameter values during system trainings. To attain these goals, many machine translation metrics have been introduced in recent years. For example, metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006) rely on word n-gram surface matching. Also, metrics that make use of linguistic resources such as synonym dictionaries, partof-speech tagging, or paraphrasing tables, have been proposed, including Meteor (Banerjee and Lavie, 2005) and its extensions, TER-Plus (Snover et al., 2009), and TESLA (Liu et al., 2011). In addition, attempts to deploy syntactic features or semantic information for evaluation have also been made, giving rise to the STM and DSTM (Liu and Gildea, 2005), DEPREF (Wu et al., 2013) and MEANT family (Lo and Wu, 2011) met"
W17-3205,D11-1033,0,0.559108,"or author’s or publication’s style (Chen et al., 2013). Training data varies significantly across domains, and cross-domain translations are unreliable, so performance can often be improved by adapting the MT system to the test domain. Domain adaptation (DA) techniques for SMT systems have been widely studied. Approaches in40 Proceedings of the First Workshop on Neural Machine Translation, pages 40–46, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weig"
W17-3205,W09-0432,0,0.0213964,"on them, then combine these in a weighted fashion. Specifically, we: 1) train an NMT model on the large combined general-domain corpus; 2) initialize with the previous model, and train several new models on sub-corpora; 3) weight each sub-corpus according to its proximity to the in-domain data (dev set), using target-side language model perSMT adaptation techniques Most SMT domain adaptation (DA) techniques can be classified into one of five categories: selftraining, context-based DA, topic-based DA, data selection, and data weighting. With self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate large in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes wo"
W17-3205,P13-1141,0,0.0625807,"data is used to translate large in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model."
W17-3205,P13-1126,1,0.917929,"Missing"
W17-3205,P17-2061,0,0.160124,"Missing"
W17-3205,P10-2041,0,0.180906,"ional origin, dialect, or author’s or publication’s style (Chen et al., 2013). Training data varies significantly across domains, and cross-domain translations are unreliable, so performance can often be improved by adapting the MT system to the test domain. Domain adaptation (DA) techniques for SMT systems have been widely studied. Approaches in40 Proceedings of the First Workshop on Neural Machine Translation, pages 40–46, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data sel"
W17-3205,P12-2023,0,0.0191037,"synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when computing the cost of each mini-batch during NMT training. We defer a detailed description to section 4, after fir"
W17-3205,P02-1040,0,0.115578,"Missing"
W17-3205,W07-0717,1,0.366807,"on Neural Machine Translation, pages 40–46, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weighting methods. Experiments on Chinese-to-English NIST and English-to-French WMT tasks show that: 1) data selection and corpus weighting methods yield significant improvement over the non-adapted baseline; and 2) the new cost weighting method obtains the biggest improvement. The cost weighting scheme has the additional advantage of being integrated with sequenc"
W17-3205,2008.iwslt-papers.6,0,0.0546254,"NMT sub-models on them, then combine these in a weighted fashion. Specifically, we: 1) train an NMT model on the large combined general-domain corpus; 2) initialize with the previous model, and train several new models on sub-corpora; 3) weight each sub-corpus according to its proximity to the in-domain data (dev set), using target-side language model perSMT adaptation techniques Most SMT domain adaptation (DA) techniques can be classified into one of five categories: selftraining, context-based DA, topic-based DA, data selection, and data weighting. With self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate large in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper"
W17-3205,D10-1044,1,0.472945,"Missing"
W17-3205,W16-2323,0,0.0410984,"being integrated with sequence-to-sequence training. 2 Applying SMT adaptation techniques to NMT 2.2 There are several adaptation scenarios for MT, of which the most common is: 1) the training material is heterogeneous, with some parts that are not too far from the test domain; 2) a bilingual development set drawn from the test domain is available. In this paper, we study adaptation techniques for this scenario. 2.1 Application to NMT In this paper, we apply data selection, corpus weighting, and sentence weighting strategies to NMT. Data selection Some previous work (Luong and Manning, 2015; Sennrich et al., 2016b) has shown that the performance of NMT systems is highly sensitive to data size. Therefore, we follow the solution in (Luong and Manning, 2015): we first train an NMT system on all available training data, then further train on the selected in-domain data. We adopt two data selection methods in this paper. The first one is based on bilingual language model cross-entropy difference (Axelrod et al., 2011). For both the source and target language, two language models are trained on in-domain and out-of-domain data respectively; then, a sentence pair is evaluated with the cross-entropy differenc"
W17-3205,P16-1009,0,0.0550095,"being integrated with sequence-to-sequence training. 2 Applying SMT adaptation techniques to NMT 2.2 There are several adaptation scenarios for MT, of which the most common is: 1) the training material is heterogeneous, with some parts that are not too far from the test domain; 2) a bilingual development set drawn from the test domain is available. In this paper, we study adaptation techniques for this scenario. 2.1 Application to NMT In this paper, we apply data selection, corpus weighting, and sentence weighting strategies to NMT. Data selection Some previous work (Luong and Manning, 2015; Sennrich et al., 2016b) has shown that the performance of NMT systems is highly sensitive to data size. Therefore, we follow the solution in (Luong and Manning, 2015): we first train an NMT system on all available training data, then further train on the selected in-domain data. We adopt two data selection methods in this paper. The first one is based on bilingual language model cross-entropy difference (Axelrod et al., 2011). For both the source and target language, two language models are trained on in-domain and out-of-domain data respectively; then, a sentence pair is evaluated with the cross-entropy differenc"
W17-3205,D11-1084,0,0.0213312,"nrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when com"
W17-3205,2012.iwslt-papers.17,0,0.0170511,"t can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when computing the cost of each mini-batch during NMT training. We defer a detailed description to section 4, after first presenting the NMT"
W17-3205,E12-1055,0,0.0524815,"slation, pages 40–46, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weighting methods. Experiments on Chinese-to-English NIST and English-to-French WMT tasks show that: 1) data selection and corpus weighting methods yield significant improvement over the non-adapted baseline; and 2) the new cost weighting method obtains the biggest improvement. The cost weighting scheme has the additional advantage of being integrated with sequence-to-sequence tra"
W17-3205,P13-2122,0,0.0372216,"formance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when computing the cost of each mini-batch during NMT training. We defer a detailed description to section 4, after first presenting the NMT approach used in our experim"
W17-3205,P07-1066,0,0.0237287,"sing the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when computing the cost of each mini-batch during NMT training. We defer a detailed description"
W17-3205,W10-2602,0,0.0122925,"raining data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and a"
W17-3205,J07-1003,0,0.0120236,"orpora, we first train NMT sub-models on them, then combine these in a weighted fashion. Specifically, we: 1) train an NMT model on the large combined general-domain corpus; 2) initialize with the previous model, and train several new models on sub-corpora; 3) weight each sub-corpus according to its proximity to the in-domain data (dev set), using target-side language model perSMT adaptation techniques Most SMT domain adaptation (DA) techniques can be classified into one of five categories: selftraining, context-based DA, topic-based DA, data selection, and data weighting. With self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate large in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenari"
W17-3205,W04-3250,0,0.0729876,"Missing"
W17-3205,2015.iwslt-evaluation.11,0,0.218972,"e additional advantage of being integrated with sequence-to-sequence training. 2 Applying SMT adaptation techniques to NMT 2.2 There are several adaptation scenarios for MT, of which the most common is: 1) the training material is heterogeneous, with some parts that are not too far from the test domain; 2) a bilingual development set drawn from the test domain is available. In this paper, we study adaptation techniques for this scenario. 2.1 Application to NMT In this paper, we apply data selection, corpus weighting, and sentence weighting strategies to NMT. Data selection Some previous work (Luong and Manning, 2015; Sennrich et al., 2016b) has shown that the performance of NMT systems is highly sensitive to data size. Therefore, we follow the solution in (Luong and Manning, 2015): we first train an NMT system on all available training data, then further train on the selected in-domain data. We adopt two data selection methods in this paper. The first one is based on bilingual language model cross-entropy difference (Axelrod et al., 2011). For both the source and target language, two language models are trained on in-domain and out-of-domain data respectively; then, a sentence pair is evaluated with the"
W17-3205,D09-1074,0,0.0117488,"couver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weighting methods. Experiments on Chinese-to-English NIST and English-to-French WMT tasks show that: 1) data selection and corpus weighting methods yield significant improvement over the non-adapted baseline; and 2) the new cost weighting method obtains the biggest improvement. The cost weighting scheme has the additional advantage of being integrated with sequence-to-sequence training. 2 Applying SMT adaptation te"
W17-3205,P13-2119,0,\N,Missing
W17-3205,K16-1031,1,\N,Missing
W17-4732,D11-1033,0,0.0310836,"omain. Finally, we experimented with a cost weighting domain adaptation technique (Chen et al., 2017). This technique trains a domain classifier concurrently with the NMT system, and uses the classifier probabilities to weight training instances according to their similarity to the development set. Since the majority of the 25 million sentence pairs in the training corpus are general domain, we experimented with different data selection and domain adaptation techniques to further train the NMT system with data that are similar to the development set so as to perform better in the news domain. Axelrod et al. (2011) introduced the bilingual language model cross-entropy difference as a similarity function for identifying sentence pairs from general-domain training corpora that are close to the target domain. We built four language models using the input and output sides of the training corpora and the development set respectively to select 3 million sentence pairs from the training corpora that are close to the news domain. However, the development set, which consists of only 1k sentence pairs, is too tiny to be a suitable corpus for building the in-domain language models that will enable the bilingual LM"
W17-4732,W16-2317,1,0.894593,"Missing"
W17-4732,W17-3205,1,0.837791,"cted by bilingual LM cross-entropy difference (xent), c) further trained with synthetic data, d) further trained with cost weighting, e) further trained with in-domain data selected by semi-supervised convolutional neural network classifier (sscnn), f) greedy model averaging and g) optimized against sentence-level BLEU on the intersection of the subsets of data selected by xent and sscnn using MRT. 3.2 Data selection and domain adaptation lion sentence pairs from the training corpora that are close to the news domain. Finally, we experimented with a cost weighting domain adaptation technique (Chen et al., 2017). This technique trains a domain classifier concurrently with the NMT system, and uses the classifier probabilities to weight training instances according to their similarity to the development set. Since the majority of the 25 million sentence pairs in the training corpus are general domain, we experimented with different data selection and domain adaptation techniques to further train the NMT system with data that are similar to the development set so as to perform better in the news domain. Axelrod et al. (2011) introduced the bilingual language model cross-entropy difference as a similarit"
W17-4732,K16-1031,1,0.92652,"-English (out of twenty participants) in WMT 2017 human evaluation. 1 George Foster∗ Work performed while at NRC. 330 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 330–337 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics the one described in Section 2.1,1 and then employed the machine-translated Russian and perfect English sentence pairs as additional data to train the Russian-English MT system. To select sentences for back-translation, we used a semi-supervised convolutional neural network classifier (Chen and Huang, 2016). We sampled two million sentences from the English monolingual News Crawl 2015 & 2016 corpora according to their classifier scores, which reflect their similarity to the the English half of our development set. formance (third place in both language pairs) in the preliminary automatic evaluation of WMT 2017. In this paper, we discuss the lessons learned in building large-scale state-of-the-art NMT systems. 2 Russian-English news translation We used all the Russian-English parallel corpora available for the constrained news translation task. They include the CommonCrawl corpus, the NewsComment"
W17-4732,N04-1021,0,0.0967458,"lt is rather disappointing by comparison with the exciting improvement reported in Sennrich et al. (2016a), i.e. 3-4 BLEU. Another disappointing result is that model averaging does not work well with the dropout models. We can see model averaging yields around 1 BLEU gain on non-dropout systems. However, the improvement achieved by model averaging drops to 0-0.1 BLEU on dropout systems. In other experiments not shown here, we also saw no improvement from ensembling the checkpoints of our dropout systems. We rescored 1000-best lists output from the phrase-based decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 13 features: 3 NMT models, 2 language models, 5 NNJMs and 3 n-best features. The rescoring model was tuned using n-best MIRA (Cherry and Foster, 2012). The three NMT systems used as rescoring features were: 1) baseline further trained with synthetic data, 2) dropout baseline further trained with synthetic data and with dropout turned off, and 3) the previous model optimized to the development set using minimum risk training. The five NNJM rescoring features include two Russian-word NNJMs and three Russian-lemma ones. Following Devlin et al. (2014), we take"
W17-4732,N12-1047,1,0.741262,"s not work well with the dropout models. We can see model averaging yields around 1 BLEU gain on non-dropout systems. However, the improvement achieved by model averaging drops to 0-0.1 BLEU on dropout systems. In other experiments not shown here, we also saw no improvement from ensembling the checkpoints of our dropout systems. We rescored 1000-best lists output from the phrase-based decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 13 features: 3 NMT models, 2 language models, 5 NNJMs and 3 n-best features. The rescoring model was tuned using n-best MIRA (Cherry and Foster, 2012). The three NMT systems used as rescoring features were: 1) baseline further trained with synthetic data, 2) dropout baseline further trained with synthetic data and with dropout turned off, and 3) the previous model optimized to the development set using minimum risk training. The five NNJM rescoring features include two Russian-word NNJMs and three Russian-lemma ones. Following Devlin et al. (2014), we take advantage of the rescoring framework to have our NNJMs view each candidate translation from 332 Figure 1: Russian-English learning curve on development set in cased BLEU of selected model"
W17-4732,P14-1129,0,0.0771307,"Missing"
W17-4732,E17-3017,0,0.0554434,"Missing"
W17-4732,P16-1009,0,0.450161,"inflections, since they play an important role in disambiguating the meaning of sentences. Chinese does not have clear word boundaries. The number of Chinese word types created by automatic word segmentation software is high, while naive character segmentation would result in a skewed Chinese to English sentence length ratio. These characteristics make it difficult for machine translation systems to learn the correct association between words in Chinese and English. Since this was the first time we deployed NMT models in an evaluation, we first tried to replicate the results of previous work (Sennrich et al., 2016a). Our NMT systems are based on Nematus (Sennrich et al., 2017). We used automatic back-translation (Sennrich et al., 2016b) of a subselected monolingual News corpus as additional training data, and all the training data is segmented into subword units using BPE (Sennrich et al., 2016c). We also experimented with pervasive dropout as implemented in Nematus. For Russian-English, our WMT16 PBMT system scored higher than all the NMT systems we built this year. We therefore experimented with using the NMT systems as features for rescoring the 1000-best output from our WMT16 PBMT system. This stra"
W17-4732,W16-2316,0,0.0118976,"and embedding layers to 0.15. For the hidden layers, we set the dropout probability to 0.3. NMT baseline system Our NMT baseline system is developed using Nematus (Sennrich et al., 2017). The dimension of word embeddings is set to 512 and that of the hidden layers is set to 1024. We train the models with rmsprop (Tieleman and Hinton, 2012), reshuffling the training corpus between epochs. We use minibatches of size 100 and validate the model every 8000 minibatches against BLEU on the WMT 15 news translation test set. We perform early stopping on the baseline system. We use AmuNMT C++ decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4. 2.2 Pervasive dropout 2.4 Minimum risk training Minimum risk training (MRT) (Shen et al., 2016) allows model optimization to arbitrary loss functions, which do not need to be differentiable, thus enabling direct model tuning against automatic MT evaluation metrics. It uses the MT evaluation metric as the loss function and minimizes the expected loss on the training data at sentence-level. We experimented with further model optimization using MRT on the whole training corpus against sentence BLEU at the final stage. 2.5 Synthetic training data Greedy model averaging A"
W17-4732,P16-1162,0,0.103844,"Missing"
W17-4732,P16-1159,0,0.0343062,"ystem is developed using Nematus (Sennrich et al., 2017). The dimension of word embeddings is set to 512 and that of the hidden layers is set to 1024. We train the models with rmsprop (Tieleman and Hinton, 2012), reshuffling the training corpus between epochs. We use minibatches of size 100 and validate the model every 8000 minibatches against BLEU on the WMT 15 news translation test set. We perform early stopping on the baseline system. We use AmuNMT C++ decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4. 2.2 Pervasive dropout 2.4 Minimum risk training Minimum risk training (MRT) (Shen et al., 2016) allows model optimization to arbitrary loss functions, which do not need to be differentiable, thus enabling direct model tuning against automatic MT evaluation metrics. It uses the MT evaluation metric as the loss function and minimizes the expected loss on the training data at sentence-level. We experimented with further model optimization using MRT on the whole training corpus against sentence BLEU at the final stage. 2.5 Synthetic training data Greedy model averaging A common practice for avoiding overfitting to the training data is ensembling the last few models saved as checkpoints. Rec"
W17-4732,W03-1730,0,0.0742719,"We used all the Chinese-English parallel corpora available for the constrained news translation task. They include the UN corpus, the NewsCommentary v12 corpus and the CWMT corpus. In total, 25 million parallel Chinese-English sentences were used to train the baseline system. We used half of the WMT 17 news translation development set as our development set and the other half as internal test set. The English texts in the training/development/test corpora were tokenized and lowercased while the Chinese texts in the training/development/test corpora were segmented using the ICTCLAS segmenter (Zhang et al., 2003). Then the Chinese and English text were combined to train a BPE model with vocabulary size of 90k. Although in figure 1 we see that none of the NMT systems manage to beat our WMT16 PBMT submission, the more interesting result is that there is more than 1.8 BLEU gain on the development set and 1.1 BLEU gain on the test set by rescoring the PBMT 1000-best list using just one of our NMT systems and no other features, as in line (g). The final rescoring with weighted collections of NMT systems, language model features, NNJM features and n-best features shows 1.8 BLEU improvement over the WMT 16 s"
W18-6408,D11-1033,0,0.0223611,"-hot vectors, Fine-tuning with In-domain Data Fine-tuning is a common method for domain adaption in NMT, which has proven effective for boosting the translation quality in a specific domain. Following Luong and Manning (2015), we first train a model on a large out-of-domain corpus and then continue a few epochs only on a small indomain corpus. In our work, we try two different approaches to select the small in-domain corpus, namely, n-grams and binary classification. N-grams: In order to acquire high-quality indomain data, we exploit the algorithm detailed in Duh et al. (2013); Axelrod et al. (2011), which 370 lection process, we makes the latter aware of the test set information, thereby enables it to retrieve better in-domain bi-texts for this specific test set. Subsequently, these synthetic bi-texts can serve as train data as they are in-domain parallel data of good quality, the idea is to imitate the effect of model ensemble, but at the data level. Finally, we replace Adam (Kingma and Ba, 2014) optimizer with SGD and use the learning rate decay, then we continue training the current best model for a few more iterations on the mixture of synthetic bi-texts and top n (n=100K) selected"
W18-6408,K16-1031,1,0.857838,"to convert textual content into numerical representation that bears much more information than the traditional ngram-based models can, such as positional, semantic and syntactical information. In most sentences, there are parts that carry strong domain information and are very useful in determining whether a particular sentence is in-domain or out-domain, while other parts are much more general and thus less useful. To extract such key domain information from a sentence, we can use convolutional neural network (CNN) with a softmax classifier sitting in the top layer. We follow the footstep of Chen and Huang (2016) where the Semi-Supervised CNN (SSCNN) domain adaption method was proposed. We use our own cloud-based word2vec to train word embeddings of 300 dimensions, using all available WMT18 bilingual and monolingual corpora for the constrained translation tasks and all the corpora that we have access to for the unconstrained tasks. Similar to Chen and Huang (2016), we also make full use of conText (Johnson and Zhang, 2015) as the CNN-based text classifier, which features a stack of two independent CNNs. The inputs to the first network, which is a simple convolution layer, are bag-of-words one-hot vect"
W18-6408,P12-1098,1,0.895395,"Missing"
W18-6408,N12-1047,0,0.0629928,"t = C current 19: S best = S current 20: else 21: C current = C best 22: R = {M cand } ∪R 23: end if 24: end while 3.4 Greedy Feature Selection based Reranking We describe the greedy feature selection based reranking (GFSR) we used in WMT 2018 in this section. N-best reranking in machine translation is a common-used technology, which can improve translation quality by picking better translations from n-best list to replace the one with the highest MT model score. GFSR Framework: We adopted the widely used an open-source implementation in moses (Koehn et al., 2007) of K-batched MIRA algorithm (Cherry and Foster, 2012) to rerank the nbest list. Unlike most common reranking architectures, we select the features greedily from a large feature pool, in which there are about 50+ different feature types. Start N # features &gt; threshold End Y Greedy Removing each feature, then tuning weights on dev Removing the one with the lowest dev score, saving weights conﬁgura?on 1 As mentioned at the beginning of the section, the effect of model ensemble is diminished with strong single models, especially with fine-tuned models. In order to boost the performance, we trained independently a large number of models using differe"
W18-6408,P09-1064,0,0.079496,"Missing"
W18-6408,P13-2119,0,0.0180502,"lution layer, are bag-of-words one-hot vectors, Fine-tuning with In-domain Data Fine-tuning is a common method for domain adaption in NMT, which has proven effective for boosting the translation quality in a specific domain. Following Luong and Manning (2015), we first train a model on a large out-of-domain corpus and then continue a few epochs only on a small indomain corpus. In our work, we try two different approaches to select the small in-domain corpus, namely, n-grams and binary classification. N-grams: In order to acquire high-quality indomain data, we exploit the algorithm detailed in Duh et al. (2013); Axelrod et al. (2011), which 370 lection process, we makes the latter aware of the test set information, thereby enables it to retrieve better in-domain bi-texts for this specific test set. Subsequently, these synthetic bi-texts can serve as train data as they are in-domain parallel data of good quality, the idea is to imitate the effect of model ensemble, but at the data level. Finally, we replace Adam (Kingma and Ba, 2014) optimizer with SGD and use the learning rate decay, then we continue training the current best model for a few more iterations on the mixture of synthetic bi-texts and t"
W18-6408,N16-1046,0,0.0890856,"Missing"
W18-6408,2015.iwslt-evaluation.11,0,0.0646752,"and monolingual corpora for the constrained translation tasks and all the corpora that we have access to for the unconstrained tasks. Similar to Chen and Huang (2016), we also make full use of conText (Johnson and Zhang, 2015) as the CNN-based text classifier, which features a stack of two independent CNNs. The inputs to the first network, which is a simple convolution layer, are bag-of-words one-hot vectors, Fine-tuning with In-domain Data Fine-tuning is a common method for domain adaption in NMT, which has proven effective for boosting the translation quality in a specific domain. Following Luong and Manning (2015), we first train a model on a large out-of-domain corpus and then continue a few epochs only on a small indomain corpus. In our work, we try two different approaches to select the small in-domain corpus, namely, n-grams and binary classification. N-grams: In order to acquire high-quality indomain data, we exploit the algorithm detailed in Duh et al. (2013); Axelrod et al. (2011), which 370 lection process, we makes the latter aware of the test set information, thereby enables it to retrieve better in-domain bi-texts for this specific test set. Subsequently, these synthetic bi-texts can serve a"
W18-6408,P02-1040,0,0.100678,"encoder and decoder self-attention layers, both embedding and hidden size have a dimension of 512, 8 heads for the self-attention. We use FFN layer with 2048 cells and Swish (Ramachandran et al., 2017) as activation function. Warmup step is set to 16000 with a learning rate equals to 0.0003. We use label smoothing with a confidence score 0.9 and all the dropout probabilities are set to 0.1. All baseline systems are trained with 4 to 8 GPUs using synchronous-SGD with moving average mechanism where the average is taken in time and in space (Zhang et al., 2015). We use BLEU as evaluation metric (Papineni et al., 2002). For English ↔ Russian and English ↔ Turkish, all reported scores are calculated over tokenized texts except for the 2018 submission which is end2end BLEU. For English → Chinese, all reported scores are end2end BLEU score using the SACREBLEU toolkit2 (Post, 2018). 2 https://github.com/awslabs/sockeye/ tree/master/contrib/sacrebleu 3 http://nlp.nju.edu.cn/cwmt-wmt/ http://www.statmt.org/moses/ 373 System baseline + corpus cleaning + back translation + finetuning + ensemble + reranking WMT18 submission newsdev2017 newstest2017 35.47 35.29 36.02 36.64 39.15 40.04 40.06 40.68 40.57 41.18 40.89 41"
W18-6408,W15-3049,0,0.0454904,"Missing"
W18-6408,W18-6319,0,0.0200682,"equals to 0.0003. We use label smoothing with a confidence score 0.9 and all the dropout probabilities are set to 0.1. All baseline systems are trained with 4 to 8 GPUs using synchronous-SGD with moving average mechanism where the average is taken in time and in space (Zhang et al., 2015). We use BLEU as evaluation metric (Papineni et al., 2002). For English ↔ Russian and English ↔ Turkish, all reported scores are calculated over tokenized texts except for the 2018 submission which is end2end BLEU. For English → Chinese, all reported scores are end2end BLEU score using the SACREBLEU toolkit2 (Post, 2018). 2 https://github.com/awslabs/sockeye/ tree/master/contrib/sacrebleu 3 http://nlp.nju.edu.cn/cwmt-wmt/ http://www.statmt.org/moses/ 373 System baseline + corpus cleaning + back translation + finetuning + ensemble + reranking WMT18 submission newsdev2017 newstest2017 35.47 35.29 36.02 36.64 39.15 40.04 40.06 40.68 40.57 41.18 40.89 41.60 43.37 Table 4: EN → ZH BLEU results on newsdev2017 and newstest2017 System baseline + corpus cleaning + back translation + finetuning + ensemble + reranking WMT18 submission 31.99 34.24 34.96 35.98 36.41 newstest2017 31.62 34.71 36.15 38.94 40.37 41.06 41.77 3"
W18-6408,E17-2025,0,0.0243196,"tion contains 7 models. Our reranker contains more than 70 features, including 14 Chinese language models, 8 Target-to-Source models, 4 Right-to-Left models. We use newsdev2017 as the development set and newstest2017 as the validation set during model training. The results of our system are reported in Table 4.1. 4.2 System baseline + corpus cleaning + w/o UN + back translation + finetuning + ensemble + reranking WMT18 submission erations. As the parallel data amount is small, we use a shared vocabulary for both EN and TR, and we tie all embeddings of source, target and output layer following Press and Wolf (2017). The back translation is particularly effective for EN ↔ TR as the amount of parallel data is very limited. For EN → TR, about 6 million sentences are selected from the newscrawl2016, 2017 and common crawl data, which is scored and sorted by domain similarity with newstest2016 test-set and authentic parallel data. Then, the 6 million sentences are translated into English by a TR ↔ EN model trained by the authentic parallel corpus. The domain relevance and the amount of data are important when using back-translation. The TR → EN follows the same procedure to get synthetic data, except the used"
W18-6408,W16-2323,0,0.111174,"lative position information in self-attention enables its propagation to the higher layers. And in contrast to the absolute position, it’s invariant to the sentence length. We compared the translation results between whether using this feature or not, and found that with the relative position features, the model performs better in reordering. We also implement the relative position representation with fast decoding. Experiments showed that it lead to faster convergence and better performance. 2.2 3 3.1 Large-scale Back Translation Adding synthetic data through the process of back translation (Sennrich et al., 2016) has become the paradigm when building state-of-the-art NMT systems. especially when a large amount of targetside in-domain data is available. For low-resource languages, the use of back-translated monolingual data is crucial as the target side lexicon coverage is often insufficient, it is the case for English ↔ Turkish, with only 0.2M bilingual sentence pairs and Turkish being a very morphologicallyrich language. Considering the abundant volume of the monolingual data provided by the organizers and the costful process of back translation, we need to select among the entire monolingual data th"
W18-6408,N18-2074,0,0.0753424,"m Abstract precisely, more bilingual data means greater ability to interact and absorb target side monolingual knowledge through the process of back translation, as well as its ability to retrieve the pertinent in-domain data during the data selection process. We share a very similar model architecture and training flow for different languages directions. Our models are based on the Google’s Transformer architecture (Vaswani et al., 2017). In order to improve our single system’s performance, we experiment with some latest research findings such as transformer with relative position attention (Shaw et al., 2018), weighted transformer (Ahmed et al., 2017) and neural suffix prediction for Russian (Song et al., 2018) which will be developed in the next section. We will also see that different well-known multi-system based techniques such as model ensembling and model reranking can still improve the performance of a very strong single system, even though we have to push further the limit in term of the number of models to employ as well as the methods to combine them together. The paper is structured as follows: Section 2 will describe the novelties of our model architecture compared to the Google’s stan"
W18-6465,P15-4020,0,0.202615,"Missing"
W18-6465,W17-4717,0,0.0467387,"Missing"
W18-6465,W17-4761,0,0.0586933,"Missing"
W18-6465,W16-2378,0,0.0218062,"r case for simplicity. Since not every single model in the ensemble is always needed for the optimized prediction, it is appropriate to select a subset from all candidate models. We follow the greedy ensemble selection algorithm, Focused Ensemble Selection (FES ) (Partalas et al., 2008), to reduce the size of averaging ensembles but improve its efficiency and predictive performance. 4.1.2 Data for Quality Estimation Model The data for quality estimation contains two parts: (i) real QE data provided by WMT QE organizers; (ii) artificial QE data generated by the roundtrip translation technique (Junczys-Dowmunt and Grundkiewicz, 2016). We first combined the real QE data with the artificial QE data to train a baseline quality estimation model, then fine tuned the model with the real QE data alone. The English-German IT domain artificial QE data can be obtained directly from the additional resources of WMT18 Auto Post-Editing task5 created by Junczys-Dowmunt and Grundkiewicz (2016). We applied the English-German artificial QE data on In the sentence level, FES’s output is averaging HTER scores of selected single models. However, in the word level, the ensemble can be made by majority voting of the binary predictions for sele"
W18-6465,W17-4763,0,0.0838264,"eling respectively. Traditional baseline models in WMT 12-17 have two modules: human-crafted rulebased feature extraction model via QuEst++ (Specia et al., 2015) (sentence-level task) or Marmot1 (word-level task); and an SVM regression with an RBF kernel as well as grid search algorithms for predicting how much effort is needed to fix translations to acceptable results (sentence-level task) or a sequence-labeling model with CRFSuit toolkit to predict which word in the translation output needs to be edited (word-level task). A recently proposed predictor-estimator model with stack propagation (Kim et al., 2017) is a recurrent neural network (RNN) based feature extractor and quality prediction model that ranked first place in WMT17. Another novel method is to train an Automatic Post-Editing (APE) system and adapt it to predict sentence-level quality scores and word-level quality labels (Martins et al., 2017). A promising APE system can serve as a guidance to QE system by explicitly explaining errors in the translation output. Our submitted system for sentence and word level QE tasks in WMT18, named QE Brain has two phases: feature extraction and quality estimation. In the phase of feature extraction,"
W18-6465,W16-2385,0,0.0389784,"nd prevents overfitting. To further enhance our model’s performance, we use a greedy algorithm based ensemble selection method to decrease the individual error among a bunch of single quality estimation models. 2 3 Boosting the QE Model Performance 3.1 QE Brain Baseline Model Human-crafted Features Along with the features produced by the Bilingual Expert model, we extract another 17 QE baseline features for the sentence-level task using QuEst++ and additional resources (source and target corpora, language models, ngram counts and lexical translation tables) provided on the WMT18 QE website2 . Kozlova et al. (2016) verifies the significance of these features using Random Forest (Breiman, 2001). Four of them are the most crucial among all according to their degrees of importance. QE Brain base single model contains a feature extractor and a quality estimator. The feature extractor relies on the Bilingual Expert model to extract features representing latent semantic information of the source and translation pair. These features will be fed into a quality estimator to estimate the translation quality. The Bilingual Expert model uses self-attention mechanism and transformer neural networks to construct a bi"
W18-6465,W17-4764,0,0.0947762,"cting how much effort is needed to fix translations to acceptable results (sentence-level task) or a sequence-labeling model with CRFSuit toolkit to predict which word in the translation output needs to be edited (word-level task). A recently proposed predictor-estimator model with stack propagation (Kim et al., 2017) is a recurrent neural network (RNN) based feature extractor and quality prediction model that ranked first place in WMT17. Another novel method is to train an Automatic Post-Editing (APE) system and adapt it to predict sentence-level quality scores and word-level quality labels (Martins et al., 2017). A promising APE system can serve as a guidance to QE system by explicitly explaining errors in the translation output. Our submitted system for sentence and word level QE tasks in WMT18, named QE Brain has two phases: feature extraction and quality estimation. In the phase of feature extraction, it extracts high-level latent joint semantics and alignment information between the source and the translation output, relying on the “neural Bilingual Expert model” introduced by Fan et al. (2018) as a prior knowledge model, which is trained on a large parallel corpus. The high-level latent semantic"
W18-6465,P02-1040,0,0.102559,"experimental results have shown that our system outperformed the best results in WMT 2017 Quality Estimation tasks and obtained top results in WMT 2018. 1 Introduction Quality Estimation (QE) is a task to estimate the quality of a Machine Translation (MT) system without the presence of any manually annotated reference translations. It can serve in a variety of computer-aided scenarios such as translation results screening before release or translation quality comparison between different MT systems. Currently, the classical and widely-used method to evaluate an MT system is measured by BLEU (Papineni et al., 2002), a statistical language-independent metric that requires human golden references for validation. What if we expect to efficiently get the detailed quality evaluation feedbacks (e.g. sentence or token-wise scoring) from an extremely large number of machine translation outputs? An automatic method with no access to any reference is highly appreciated. 1 ∗ * indicates equal contribution. 809 https://github.com/qe-team/marmot Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 809–815 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Assoc"
W18-6482,W15-3003,0,0.0137963,"re more important in NMT. Firstly, a large language model is built on all available English monolingual corpus provided by WMT18. The training corpus is cleaned using some rules mentioned above. Then the normalized-length language model score can be regarded as the monolingual quality score. But in practice, this method has a shortcoming: it gives lower scores for the good sentences that contain rare words. The training corpus needs to be generalized to overcome this shortness, for example, we can replace the words that occur less than 10 times in LM train corpus with their part of speech tag(Axelrod et al., 2015). Finally, the language model is re-built on the generalized corpus. 2.3 f (sj |S1j−1 ) P ng∈N G(sj ,n) weight(ng, j − 1) norm(sj ) (2) j−1 ) , where S1j−1 represents the set of selected sentences which contains 1st to (j − 1)th sentences, and S is the whole sentences pool to be selected. f (sj |S1j−1 ) is the diversity score of sentence sj under the condition that corpus S1j−1 is selected. N G(sj , n) is all n-grams of size n in sentence sj . |N G(sj , n) |is the size of the N G(sj , n). norm(sj ) is the normalization factor for senP tence sj , and equals N n=1 |N G(sj , n)|. F req(ng, S) is"
W18-6482,W11-2131,0,0.0609525,"Missing"
W18-6482,2016.amta-researchers.8,1,0.915364,"ingual score) and accuracy of the sentence pair. Secondly, the quality of the target and/or source sentences of the parallel corpus should also be evaluated. In this work, the target side sentences are concerned a lot for their importance in NMT. 2 Parallel Sentence Pairs Scoring Methods In this section, three kinds of scoring/filtering methods are detailed. 2.1 Bilingual Quality Evaluation Here, we describe the noisy corpus filtering rules and two kinds of translation quality evaluation methods: (1) Word Alignment Based bilingual scoring and (2) Bitoken CNN Classifier based bilingual scoring(Chen et al., 2016). Rule-based Filtering A series of heuristic rules are applied to filter bad sentence pairs. They are simple but efficient, which are described below. • The length ratio of source sentence to target sentence. Sentence length is calculated as the number of tokens/words. In our system, the ratio is set between 0.4 and 2.5. • The edit distance between the source token sequence and the target token sequence. A 917 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 917–922 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Co"
W18-6482,W12-3131,0,0.0843514,"filtering. For German-English parallel corpus, the source and target sentences’ languages should be English and German. We can detect the sentence’s language by using a language detection tool we developed1 . The sentences pair is filtered if the languages of its source and target sides are not German and English. Language Model Scoring We use the language model to evaluate the quality of sentences. The language model is successfully used to select domain-related corpus (Yasuda et al., 2008; Moore and Lewis, 2010). Besides, the language model can also be used to filter out ungrammatical data (Denkowski et al., 2012; Allauzen et al., 2011), which is suitable for this task. In our corpus filtering system, we focus on the quality of target sentences, i.e. English sentences, as they are more important in NMT. Firstly, a large language model is built on all available English monolingual corpus provided by WMT18. The training corpus is cleaned using some rules mentioned above. Then the normalized-length language model score can be regarded as the monolingual quality score. But in practice, this method has a shortcoming: it gives lower scores for the good sentences that contain rare words. The training corpus"
W18-6482,N13-1073,0,0.0684307,"ey, 2005), we simplify the original algorithm, and the translation score of sentence pairs is given below: score(s, t) = 1 X m s ,t ∈a i j log p(tj |si ) s2t 1 X + n s ,t ∈a i j t2s log p(si |tj ) (1) In Equation (1), s and t represent the source and target sentences respectively, p(w1 |w2 ) indicates the word translation probability, as2t indicates the source words to target words alignment, m and n are the lengths of source and target sentences. In this task, the word alignment model is trained on a clean parallel corpus provided by WMT18 New Translation Task. We use the fast align toolkit (Dyer et al., 2013) to train the model, and get the forward and reverse word translation probability tables. This model is also called alignment scoring model. 2.2 Monolingual Quality Evaluation Rule based Filtering A few rules are applied to filtering the sentence pairs whose source or target side are not good. These rules are: • The length of the sentence which is too short (≤ 2 words) or too long (&gt; 80 words) will be dropped. 918 • The ratio of valid tokens counts to the length of the sentence. Here, valid tokens are the tokens which contain the letters in the corresponding language. For example, a valid toke"
W18-6482,P13-2121,0,0.0672549,"Missing"
W18-6482,W18-2709,0,0.235092,"istics of the corpus are investigated, i.e. 1) the bilingual/translation quality, 2) the monolingual quality and 3) the corpus diversity. Both rulebased and model-based methods are adapted to score the parallel sentence pairs. The final parallel corpus filtering system is reliable, easy to build and adapt to other language pairs. 1 Introduction The parallel corpus is an essential resource for machine translation and multilingual natural language processing. Apart from the quantity and domain, the quality of parallel corpus is also very important in MT system training (Koehn and Knowles, 2017; Khayrallah and Koehn, 2018). The Internet contains a large number of multilingual resources, including parallel and comparable sentences (Resnik and Smith, 2003). Many successful machine translation systems are built using the corpus crawled from the web. But in practice, this kind of parallel corpus may be very noisy. The task of Parallel Corpus Filtering tackles the problem of cleaning noisy parallel corpus. In this task, we can divide the corpus cleaning task into three parts. Firstly, a high-quality parallel sentence pair should have the property that its target sentence precisely translates the source sentence, and"
W18-6482,W17-3204,0,0.0345104,"rpus, the three characteristics of the corpus are investigated, i.e. 1) the bilingual/translation quality, 2) the monolingual quality and 3) the corpus diversity. Both rulebased and model-based methods are adapted to score the parallel sentence pairs. The final parallel corpus filtering system is reliable, easy to build and adapt to other language pairs. 1 Introduction The parallel corpus is an essential resource for machine translation and multilingual natural language processing. Apart from the quantity and domain, the quality of parallel corpus is also very important in MT system training (Koehn and Knowles, 2017; Khayrallah and Koehn, 2018). The Internet contains a large number of multilingual resources, including parallel and comparable sentences (Resnik and Smith, 2003). Many successful machine translation systems are built using the corpus crawled from the web. But in practice, this kind of parallel corpus may be very noisy. The task of Parallel Corpus Filtering tackles the problem of cleaning noisy parallel corpus. In this task, we can divide the corpus cleaning task into three parts. Firstly, a high-quality parallel sentence pair should have the property that its target sentence precisely transl"
W18-6482,P10-2041,0,0.0120392,"stem, after selecting a subset S1j−1 , the next sentence sj ’s diversity score is given by: • Language filtering. For German-English parallel corpus, the source and target sentences’ languages should be English and German. We can detect the sentence’s language by using a language detection tool we developed1 . The sentences pair is filtered if the languages of its source and target sides are not German and English. Language Model Scoring We use the language model to evaluate the quality of sentences. The language model is successfully used to select domain-related corpus (Yasuda et al., 2008; Moore and Lewis, 2010). Besides, the language model can also be used to filter out ungrammatical data (Denkowski et al., 2012; Allauzen et al., 2011), which is suitable for this task. In our corpus filtering system, we focus on the quality of target sentences, i.e. English sentences, as they are more important in NMT. Firstly, a large language model is built on all available English monolingual corpus provided by WMT18. The training corpus is cleaned using some rules mentioned above. Then the normalized-length language model score can be regarded as the monolingual quality score. But in practice, this method has a"
W18-6482,J03-3002,0,0.174467,"Both rulebased and model-based methods are adapted to score the parallel sentence pairs. The final parallel corpus filtering system is reliable, easy to build and adapt to other language pairs. 1 Introduction The parallel corpus is an essential resource for machine translation and multilingual natural language processing. Apart from the quantity and domain, the quality of parallel corpus is also very important in MT system training (Koehn and Knowles, 2017; Khayrallah and Koehn, 2018). The Internet contains a large number of multilingual resources, including parallel and comparable sentences (Resnik and Smith, 2003). Many successful machine translation systems are built using the corpus crawled from the web. But in practice, this kind of parallel corpus may be very noisy. The task of Parallel Corpus Filtering tackles the problem of cleaning noisy parallel corpus. In this task, we can divide the corpus cleaning task into three parts. Firstly, a high-quality parallel sentence pair should have the property that its target sentence precisely translates the source sentence, and vice versa. In this task, we attempt to quantify the translation quality (also called bilingual score) and accuracy of the sentence p"
W18-6482,I08-2088,0,0.0423735,"ram weight. In our system, after selecting a subset S1j−1 , the next sentence sj ’s diversity score is given by: • Language filtering. For German-English parallel corpus, the source and target sentences’ languages should be English and German. We can detect the sentence’s language by using a language detection tool we developed1 . The sentences pair is filtered if the languages of its source and target sides are not German and English. Language Model Scoring We use the language model to evaluate the quality of sentences. The language model is successfully used to select domain-related corpus (Yasuda et al., 2008; Moore and Lewis, 2010). Besides, the language model can also be used to filter out ungrammatical data (Denkowski et al., 2012; Allauzen et al., 2011), which is suitable for this task. In our corpus filtering system, we focus on the quality of target sentences, i.e. English sentences, as they are more important in NMT. Firstly, a large language model is built on all available English monolingual corpus provided by WMT18. The training corpus is cleaned using some rules mentioned above. Then the normalized-length language model score can be regarded as the monolingual quality score. But in prac"
