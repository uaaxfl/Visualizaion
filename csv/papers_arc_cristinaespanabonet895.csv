2021.mtsummit-research.6,The Effect of Domain and Diacritics in {Y}oruba{--}{E}nglish Neural Machine Translation,2021,-1,-1,8,0.784314,5024,david adelani,Proceedings of Machine Translation Summit XVIII: Research Track,0,Massively multilingual machine translation (MT) has shown impressive capabilities and including zero and few-shot translation between low-resource language pairs. However and these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating MT models on low-resource pairs is often due to lack of standardized evaluation datasets. In this paper and we present MENYO-20k and the first multi-domain parallel corpus with a especially curated orthography for Yoruba{--}English with standardized train-test splits for benchmarking. We provide several neural MT benchmarks and compare them to the performance of popular pre-trained (massively multilingual) MT models both for the heterogeneous test set and its subdomains. Since these pre-trained models use huge amounts of data with uncertain quality and we also analyze the effect of diacritics and a major characteristic of Yoruba and in the training data. We investigate how and when this training condition affects the final quality of a translation and its understandability.Our models outperform massively multilingual models such as Google ($+8.7$ BLEU) and Facebook M2M ($+9.1$) when translating to Yoruba and setting a high quality benchmark for future research.
2021.mtsummit-research.7,Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages,2021,-1,-1,4,1,3,dana ruiter,Proceedings of Machine Translation Summit XVIII: Research Track,0,For most language combinations and parallel data is either scarce or simply unavailable. To address this and unsupervised machine translation (UMT) exploits large amounts of monolingual data by using synthetic data generation techniques such as back-translation and noising and while self-supervised NMT (SSNMT) identifies parallel sentences in smaller comparable data and trains on them. To this date and the inclusion of UMT data generation techniques in SSNMT has not been investigated. We show that including UMT techniques into SSNMT significantly outperforms SSNMT (up to +4.3 BLEU and af2en) as well as statistical (+50.8 BLEU) and hybrid UMT (+51.5 BLEU) baselines on related and distantly-related and unrelated language pairs.
2021.mtsummit-at4ssl.5,{AVASAG}: A {G}erman {S}ign {L}anguage Translation System for Public Services (short paper),2021,-1,-1,4,0,5112,fabrizio nunnari,Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL),0,"This paper presents an overview of AVASAG; an ongoing applied-research project developing a text-to-sign-language translation system for public services. We describe the scientific innovation points (geometry-based SL-description, 3D animation and video corpus, simplified annotation scheme, motion capture strategy) and the overall translation pipeline."
2021.motra-1.1,Do not Rely on Relay Translations: Multilingual Parallel Direct {E}uroparl,2021,-1,-1,3,0,5251,kwabena amponsahkaakyire,Proceedings for the First Workshop on Modelling Translation: Translatology in the Digital Age,0,None
2021.emnlp-main.676,Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification,2021,-1,-1,5,0,5252,daria pylypenko,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Traditional hand-crafted linguistically-informed features have often been used for distinguishing between translated and original non-translated texts. By contrast, to date, neural architectures without manual feature engineering have been less explored for this task. In this work, we (i) compare the traditional feature-engineering-based approach to the feature-learning-based one and (ii) analyse the neural architectures in order to investigate how well the hand-crafted features explain the variance in the neural models{'} predictions. We use pre-trained neural word embeddings, as well as several end-to-end neural architectures in both monolingual and multilingual settings and compare them to feature-engineering-based SVM classifiers. We show that (i) neural architectures outperform other approaches by more than 20 accuracy points, with the BERT-based model performing the best in both the monolingual and multilingual settings; (ii) while many individual hand-crafted translationese features correlate with neural model predictions, feature importance analysis shows that the most important features for neural and classical architectures differ; and (iii) our multilingual experiments provide empirical evidence for translationese universals across languages."
2020.lrec-1.335,Massive vs. Curated Embeddings for Low-Resourced Languages: the Case of {Y}or{\\`u}b{\\'a} and {T}wi,2020,-1,-1,4,0,5025,jesujoba alabi,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The success of several architectures to learn semantic representations from unannotated text and the availability of these kind of texts in online multilingual resources such as Wikipedia has facilitated the massive and automatic creation of resources for multiple languages. The evaluation of such resources is usually done for the high-resourced languages, where one has a smorgasbord of tasks and test sets to evaluate on. For low-resourced languages, the evaluation is more difficult and normally ignored, with the hope that the impressive capability of deep learning architectures to learn (multilingual) representations in the high-resourced setting holds in the low-resourced setting too. In this paper we focus on two African languages, Yor{\`u}b{\'a} and Twi, and compare the word embeddings obtained in this way, with word embeddings obtained from curated corpora and a language-dependent processing. We analyse the noise in the publicly available corpora, collect high quality and noisy data for the two languages and quantify the improvements that depend not only on the amount of data but on the quality too. We also use different architectures that learn word representations both from surface forms and characters to further exploit all the available information which showed to be important for these languages. For the evaluation, we manually translate the wordsim-353 word pairs dataset from English into Yor{\`u}b{\'a} and Twi. We extend the analysis to contextual word embeddings and evaluate multilingual BERT on a named entity recognition task. For this, we annotate with named entities the Global Voices corpus for Yor{\`u}b{\'a}. As output of the work, we provide corpora, embeddings and the test suits for both languages."
2020.lrec-1.502,{G}e{B}io{T}oolkit: Automatic Extraction of Gender-Balanced Multilingual Corpus of {W}ikipedia Biographies,2020,-1,-1,3,0,5326,marta costajussa,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We introduce GeBioToolkit, a tool for extracting multilingual parallel corpora at sentence level, with document and gender information from Wikipedia biographies. Despite the gender inequalities present in Wikipedia, the toolkit has been designed to extract corpus balanced in gender. While our toolkit is customizable to any number of languages (and different domains), in this work we present a corpus of 2,000 sentences in English, Spanish and Catalan, which has been post-edited by native speakers to become a high-quality dataset for machine translation evaluation. While GeBioCorpus aims at being one of the first non-synthetic gender-balanced test datasets, GeBioToolkit aims at paving the path to standardize procedures to produce gender-balanced datasets."
2020.iwslt-1.34,How Human is Machine Translationese? Comparing Human and Machine Translations of Text and Speech,2020,-1,-1,3,0,2627,yuri bizzoni,Proceedings of the 17th International Conference on Spoken Language Translation,0,"Translationese is a phenomenon present in human translations, simultaneous interpreting, and even machine translations. Some translationese features tend to appear in simultaneous interpreting with higher frequency than in human text translation, but the reasons for this are unclear. This study analyzes translationese patterns in translation, interpreting, and machine translation outputs in order to explore possible reasons. In our analysis we {--} (i) detail two non-invasive ways of detecting translationese and (ii) compare translationese across human and machine translations from text and speech. We find that machine translation shows traces of translationese, but does not reproduce the patterns found in human translation, offering support to the hypothesis that such patterns are due to the model (human vs machine) rather than to the data (written vs spoken)."
2020.emnlp-main.202,Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation,2020,-1,-1,3,1,3,dana ruiter,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle. In this study, we provide an in-depth analysis of the sampling choices the SSNMT model makes during training. We show how, without it having been told to do so, the model self-selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) performing a denoising curriculum. We observe that the dynamics of the mutual-supervision signals of both system internal representation types are vital for the extraction and translation performance. We show that in terms of the Gunning-Fog Readability index, SSNMT starts extracting and learning from Wikipedia data suitable for high school students and quickly moves towards content suitable for first year undergraduate students."
2020.coling-main.532,Understanding Translationese in Multi-view Embedding Spaces,2020,-1,-1,2,0.952381,9996,koel chowdhury,Proceedings of the 28th International Conference on Computational Linguistics,0,"Recent studies use a combination of lexical and syntactic features to show that footprints of the source language remain visible in translations, to the extent that it is possible to predict the original source language from the translation. In this paper, we focus on embedding-based semantic spaces, exploiting departures from isomorphism between spaces built from original target language and translations into this target language to predict relations between languages in an unsupervised way. We use different views of the data {---} words, parts of speech, semantic tags and synsets {---} to track translationese. Our analysis shows that (i) semantic distances between original target language and translations into this target language can be detected using the notion of isomorphism, (ii) language family ties with characteristics similar to linguistically motivated phylogenetic trees can be inferred from the distances and (iii) with delexicalised embeddings exhibiting source-language interference most significantly, other levels of abstraction display the same tendency, indicating the lexicalised results to be not {``}just{''} due to possible topic differences between original and translated texts. To the best of our knowledge, this is the first time departures from isomorphism between embedding spaces are used to track translationese."
2020.cl-2.1,Multilingual and Interlingual Semantic Representations for Natural Language Processing: A Brief Introduction,2020,2,0,2,0,5326,marta costajussa,Computational Linguistics,0,"We introduce the Computational Linguistics special issue on Multilingual and Interlingual Semantic Representations for Natural Language Processing. We situate the special issue{'}s five articles in the context of our fast-changing field, explaining our motivation for this project. We offer a brief summary of the work in the issue, which includes developments on lexical and sentential semantic representations, from symbolic and neural perspectives."
W19-5315,{U}d{S}-{DFKI} Participation at {WMT} 2019: Low-Resource (en-gu) and Coreference-Aware (en-de) Systems,2019,0,0,1,1,5030,cristina espanabonet,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,This paper describes the UdS-DFKI submission to the WMT2019 news translation task for Gujarati{--}English (low-resourced pair) and German{--}English (document-level evaluation). Our systems rely on the on-line extraction of parallel sentences from comparable corpora for the first scenario and on the inclusion of coreference-related information in the training data in the second one.
P19-1178,Self-Supervised Neural Machine Translation,2019,0,2,2,1,3,dana ruiter,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We present a simple new method where an emergent NMT system is used for simultaneously selecting training data and learning internal NMT representations. This is done in a self-supervised way without parallel data, in such a way that both tasks enhance each other during training. The method is language independent, introduces no additional hyper-parameters, and achieves BLEU scores of 29.21 (en2fr) and 27.36 (fr2en) on newstest2014 using English and French Wikipedia data for training."
D19-6501,Analysing Coreference in Transformer Outputs,2019,0,0,2,0,2628,ekaterina lapshinovakoltunski,Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019),0,"We analyse coreference phenomena in three neural machine translation systems trained with different data settings with or without access to explicit intra- and cross-sentential anaphoric information. We compare system performance on two different genres: news and TED talks. To do this, we manually annotate (the possibly incorrect) coreference chains in the MT outputs and evaluate the coreference chain translations. We define an error typology that aims to go further than pronoun translation adequacy and includes types such as incorrect word selection or missing words. The features of coreference chains in automatic translations are also compared to those of the source texts and human translations. The analysis shows stronger potential translationese effects in machine translated outputs than in human translations."
D19-6502,Context-Aware Neural Machine Translation Decoding,2019,0,0,3,1,16565,eva garcia,Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019),0,"This work presents a decoding architecture that fuses the information from a neural translation model and the context semantics enclosed in a semantic space language model based on word embeddings. The method extends the beam search decoding process and therefore can be applied to any neural machine translation framework. With this, we sidestep two drawbacks of current document-level systems: (i) we do not modify the training process so there is no increment in training time, and (ii) we do not require document-level an-notated data. We analyze the impact of the fusion system approach and its parameters on the final translation quality for English{--}Spanish. We obtain consistent and statistically significant improvements in terms of BLEU and METEOR and we observe how the fused systems are able to handle synonyms to propose more adequate translations as well as help the system to disambiguate among several translation candidates for a word."
W17-2617,Learning Bilingual Projections of Embeddings for Vocabulary Expansion in Machine Translation,2017,32,5,2,0,10154,pranava madhyastha,Proceedings of the 2nd Workshop on Representation Learning for {NLP},0,"We propose a simple log-bilinear softmax-based model to deal with vocabulary expansion in machine translation. Our model uses word embeddings trained on significantly large unlabelled monolingual corpora and learns over a fairly small, word-to-word bilingual dictionary. Given an out-of-vocabulary source word, the model generates a probabilistic list of possible translations in the target language using the trained bilingual embeddings. We integrate these translation options into a standard phrase-based statistical machine translation system and obtain consistent improvements in translation quality on the English{--}Spanish language pair. When tested over an out-of-domain testset, we get a significant improvement of 3.9 BLEU points."
S17-2019,Lump at {S}em{E}val-2017 Task 1: Towards an Interlingua Semantic Similarity,2017,0,3,1,1,5030,cristina espanabonet,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This is the Lump team participation at SemEval 2017 Task 1 on Semantic Textual Similarity. Our supervised model relies on features which are multilingual or interlingual in nature. We include lexical similarities, cross-language explicit semantic analysis, internal representations of multilingual neural networks and interlingual word embeddings. Our representations allow to use large datasets in language pairs with many instances to better classify instances in smaller language pairs avoiding the necessity of translating into a single language. Hence we can deal with all the languages in the task: Arabic, English, Spanish, and Turkish."
W16-2336,The {TALP}{--}{UPC} {S}panish{--}{E}nglish {WMT} Biomedical Task: Bilingual Embeddings and Char-based Neural Language Model Rescoring in a Phrase-based System,2016,12,2,2,0,5326,marta costajussa,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the TALPxe2x80x93UPC system in the Spanishxe2x80x93English WMT 2016 biomedical shared task. Our system is a standard phrase-based system enhanced with vocabulary expansion using bilingual word embeddings and a characterbased neural language model with rescoring. The former focuses on resolving outof- vocabulary words, while the latter enhances the fluency of the system. The two modules progressively improve the final translation as measured by a combination of several lexical metrics."
L16-1469,{T}weet{MT}: A Parallel Microblog Corpus,2016,11,1,3,0,5339,inaki vicente,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We introduce TweetMT, a parallel corpus of tweets in four language pairs that combine five languages (Spanish from/to Basque, Catalan, Galician and Portuguese), all of which have an official status in the Iberian Peninsula. The corpus has been created by combining automatic collection and crowdsourcing approaches, and it is publicly available. It is intended for the development and testing of microtext machine translation systems. In this paper we describe the methodology followed to build the corpus, and present the results of the shared task in which it was tested."
W15-4908,Document-Level Machine Translation with Word Vector Models,2015,30,3,2,1,16565,eva garcia,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"In this paper we apply distributional semantic information to document-level machine translation. We train monolingual and bilingual word vector models on large corpora and we evaluate them first in a cross-lingual lexical substitution task and then on the final translation task. For translation, we incorporate the semantic information in a statistical document-level decoder (Docent), by enforcing translation choices that are semantically similar to the context. As expected, the bilingual word vector models are more appropriate for the purpose of translation. The final document-level translator incorporatingn the semantic model outperforms the basic Docent (without semantics) and alson performs slightly over a standard sentence level SMT system in terms of ULC (the average of a set of standard automatic evaluation metrics for MT). Finally, we also present some manual analysis of the translations of some concrete documents"
W15-3402,A Factory of Comparable Corpora from {W}ikipedia,2015,39,10,2,0,15265,alberto barroncedeno,Proceedings of the Eighth Workshop on Building and Using Comparable Corpora,0,"Multiple approaches to grab comparable data from the Web have been developed up to date. Nevertheless, coming out with a high-quality comparable corpus of a specific topic is not straightforward. We present a model for the automatic extraction of comparable texts in multiple languages and on specific topics from Wikipedia. In order to prove the value of the model, we automatically extract parallel sentences from the comparable collections and use them to train statistical machine translation engines for specific domains. Our experiments on the Englishxe2x80x90 Spanish pair in the domains of Computer Science, Science, and Sports show that our in-domain translator performs significantly better than a generic one when translating in-domain Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts."
2015.eamt-1.9,Document-Level Machine Translation with Word Vector Models,2015,30,3,2,1,16565,eva garcia,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,"In this paper we apply distributional semantic information to document-level machine translation. We train monolingual and bilingual word vector models on large corpora and we evaluate them first in a cross-lingual lexical substitution task and then on the final translation task. For translation, we incorporate the semantic information in a statistical document-level decoder (Docent), by enforcing translation choices that are semantically similar to the context. As expected, the bilingual word vector models are more appropriate for the purpose of translation. The final document-level translator incorporatingn the semantic model outperforms the basic Docent (without semantics) and alson performs slightly over a standard sentence level SMT system in terms of ULC (the average of a set of standard automatic evaluation metrics for MT). Finally, we also present some manual analysis of the translations of some concrete documents"
W14-4015,Word{'}s Vector Representations meet Machine Translation,2014,10,3,3,1,16565,eva garcia,"Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,Distributed vector representations of words are useful in various NLP tasks. We briefly review the CBOW approach and propose a bilingual application of this architecture with the aim to improve consistency and coherence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup.
2013.mtsummit-posters.10,{MT} Techniques in a Retrieval System of Semantically Enriched Patents,2013,11,1,4,0,18034,meritxell gonzalez,Proceedings of Machine Translation Summit XIV: Posters,0,This paper focuses on how automaticn translation techniques integrated in an patent retrieval system increase its capabilities and make possible extended features and functionalities. We describe 1)n a novel methodology for natural languagen to SPARQL translation based on a grammarxe2x80x93n ontology interoperability automation and a query grammar for the patents domain; 2) a devised strategy for statisticalbasedn translation of patents that allows to transfer semantic annotations to the targetn language; 3) a built-in knowledge representation infrastructure that uses multilingual semantic annotations; and 4) an online application that offers a multilingualn search interface over structural knowledgen databases (domain ontologies) and multilingual documents (biomedical patents)n that have been automatically translated.
W12-0103,Full Machine Translation for Factoid Question Answering,2012,0,0,1,1,5030,cristina espanabonet,Proceedings of the Joint Workshop on Exploiting Synergies between Information Retrieval and Machine Translation ({ESIRMT}) and Hybrid Approaches to Machine Translation ({H}y{T}ra),0,"In this paper we present an SMT-based approach to Question Answering (QA). QA is the task of extracting exact answers inn response to natural language questions. Inn our approach, the answer is a translation ofn the question obtained with an SMT system.n We use the n-best translations of a givenn question to find similar sentences in then document collection that contain the realn answer. Although it is not the first time that SMT inspires a QA system, it is the first approach that uses a full Machine Translation system for generating answers. Our approach is validated with the datasets of the TREC QA evaluation."
2012.freeopmt-1.7,Deep evaluation of hybrid architectures: use of different metrics in {MERT} weight optimization,2012,-1,-1,1,1,5030,cristina espanabonet,Proceedings of the Third International Workshop on Free/Open-Source Rule-Based Machine Translation,0,None
2012.eamt-1.15,Context-Aware Machine Translation for Software Localization,2012,5,4,3,0,43860,victor muntesmulero,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"Software localization requires translating short text strings appearing in user interfaces (UI) into several languages. These strings are usually unrelated to the other strings in the UI. Due to the lack of semantic context, many ambiguity problems cannot be solved during translation. However, UI are composed of several visual components to which text strings are associated. Although this association might be very valuable for word disambiguation, it has not been exploited. In this paper, we present the problem of lack of context awareness for UI localization, providing real examples and identifying the main research challenges."
2012.eamt-1.61,A Hybrid System for Patent Translation,2012,20,13,2,1,36791,ramona enache,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"This work presents a HMT system for patent translation. The system exploits the high coverage of SMT and the high precision of an RBMT system based on GF to deal with specific issues of the language. The translator is specifically developed to translate patents and it is evaluated in the English-French language pair. Although the number of issues tackled by the grammar are not extremely numerous yet, both manual and automatic evaluations consistently show their preference for the hybrid system in front of the two individual translators."
2011.mtsummit-wpt.7,Patent translation within the {MOLTO} project,2011,9,16,1,1,5030,cristina espanabonet,Proceedings of the 4th Workshop on Patent Translation,0,"MOLTO is an FP7 European project whose goal is to translate texts between multiple languages in real time with high quality. Patents translation is a case of study where research is focused on simultaneously obtaining a large coverage without loosing quality in the translation. This is achieved by hybridising between a grammar-based multilingual translation system, GF, and a specialised statistical machine translation system. Moreover, both individual systems by themselves already represent a step forward in the translation of patents in the biomedical domain, for which the systems have been trained."
2011.mtsummit-papers.63,Hybrid Machine Translation Guided by a Rule{--}Based System,2011,11,17,1,1,5030,cristina espanabonet,Proceedings of Machine Translation Summit XIII: Papers,0,"This paper presents a machine translation architecture which hybridizes Matxin, a rulebased system, with regular phrase-based Statistical Machine Translation. In short, the hybrid translation process is guided by the rulebased engine and, before transference, a set of partial candidate translations provided by SMT subsystems is used to enrich the treebased representation. The final hybrid translation is created by choosing the most probable combination among the available fragments with a statistical decoder in a monotonic way.n We have applied the hybrid model to a pairn of distant languages, Spanish and Basque, andn according to our evaluation (both automaticn and manual) the hybrid approach significantlyn outperforms the best SMT system on out-of-domain data."
melero-etal-2010-language,Language Technology Challenges of a {`}Small{'} Language ({C}atalan),2010,3,2,4,0,4983,maite melero,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we present a brief snapshot of the state of affairs in computational processing of Catalan and the initiatives that are starting to take place in an effort to bring the field a step forward, by making a better and more efficient use of the already existing resources and tools, by bridging the gap between research and market, and by establishing periodical meeting points for the community. In particular, we present the results of the First Workshop on the Computational Processing of Catalan, which succeeded in putting together a fair representation of the research in the area, and received attention from both the industry and the administration. Aside from facilitating communication among researchers and between developers and users, the Workshop provided the organizers with valuable information about existing resources, tools, developers and providers. This information has allowed us to go a step further by setting up a ÂharvestingÂ procedure which will hopefully build the seed of a portal-catalogue-observatory of language resources and technologies in Catalan."
2010.eamt-1.15,Robust Estimation of Feature Weights in Statistical Machine Translation,2010,18,4,1,1,5030,cristina espanabonet,Proceedings of the 14th Annual conference of the European Association for Machine Translation,0,"Weights of the various components in a standard Statistical Machine Translation model are usually estimated via Minimum Error Rate Training. With this, one finds their optimum value on a development set with the expectation that these optimal weights generalise well to other test sets. However, this is not always the case when domains differ. This work uses a perceptron algorithm to learn more robust weights to be used on out-of-domain corpora without the need for specialised data. For an Arabic-to-English translation system, the generalisation of weights represents an improvement of more than 2 points of BLEU with respect to the MERT baseline using the same information."
