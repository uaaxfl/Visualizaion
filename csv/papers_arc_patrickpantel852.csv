N16-1171,Activity Modeling in Email,2016,24,10,3,0,29454,ashequl qadir,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We introduce a latent activity model for workplace emails, positing that communication at work is purposeful and organized by activities. We pose the problem as probabilistic inference in graphical models that jointly capture the interplay between latent activities and the email contexts they govern, such as the recipients, subject and body. The model parameters are learned using maximum likelihood estimation with an expectation maximization algorithm. We present three variants of the model that incorporate the recipients, co-occurrence of the recipients, and email body and subject. We demonstrate the modelxe2x80x99s effectiveness in an email recipient recommendation task and show that it outperforms a state-of-the-art generative model. Additionally, we show that the activity model can be used to identify email senders who engage in similar activities, resulting in further improvements in recipient recommendation."
D15-1174,Representing Text for Joint Embedding of Text and Knowledge Bases,2015,31,215,3,0,9781,kristina toutanova,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Models that learn to represent textual and knowledge base relations in the same continuous latent space are able to perform joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion (Riedel et al., 2013). In this paper we propose a model that captures the compositional structure of textual relations, and jointly optimizes entity, knowledge base, and textual relation representations. The proposed model significantly improves performance over a model that does not share parameters among textual relations with common sub-structure."
P14-1143,Smart Selection,2014,0,0,1,1,34734,patrick pantel,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
D14-1002,Modeling Interestingness with Deep Neural Networks,2014,123,138,2,0,3502,jianfeng gao,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"An xe2x80x9cInterestingness Modelerxe2x80x9d uses deep neural networks to learn deep semantic models (DSM) of xe2x80x9cinterestingness.xe2x80x9d The DSM, consisting of two branches of deep neural networks or their convolutional versions, identifies and predicts target documents that would interest users reading source documents. The learned model observes, identifies, and detects naturally occurring signals of interestingness in click transitions between source and target documents derived from web browser logs. Interestingness is modeled with deep neural networks that map source-target document pairs to feature vectors in a latent space, trained on document transitions in view of a xe2x80x9ccontextxe2x80x9d and optional xe2x80x9cfocusxe2x80x9d of source and target documents. Network parameters are learned to minimize distances between source documents and their corresponding xe2x80x9cinterestingxe2x80x9d targets in that space. The resulting interestingness model has applicable uses, including, but not limited to, contextual entity searches, automatic text highlighting, prefetching documents of likely interest, automated content recommendation, automated advertisement placement, etc."
C14-1140,Predicting Interesting Things in Text,2014,28,10,3,0,15131,michael gamon,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"While reading a document, a user may encounter concepts, entities, and topics that she is interested in exploring more. We propose models of xe2x80x9cinterestingnessxe2x80x9d, which aim to predict the level of interest a user has in the various text spans in a document. We obtain naturally occurring interest signals by observing user browsing behavior in clicks from one page to another. We cast the problem of predicting interestingness as a discriminative learning problem over this data. We leverage features from two principal sources: textual context features and topic features that assess the semantics of the document transition. We learn our topic features without supervision via probabilistic inference over a graphical model that captures the latent joint topic space of the documents in the transition. We train and test our models on millions of realworld transitions between Wikipedia documents as observed from web browser session logs. On the task of predicting which spans are of most interest to users, we show significant improvement over various baselines and highlight the value of our latent semantic model."
P12-1059,Mining Entity Types from Query Logs via User Intent Modeling,2012,27,45,1,1,34734,patrick pantel,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We predict entity type distributions in Web search queries via probabilistic inference in graphical models that capture how entity-bearing queries are generated. We jointly model the interplay between latent user intents that govern queries and unobserved entity types, leveraging observed signals from query formulations and document clicks. We apply the models to resolve entity types in new queries and to assign prior type distributions over an existing knowledge base. Our models are efficiently trained using maximum likelihood estimation over millions of real-world Web search queries. We show that modeling user intent significantly improves entity type resolution for head queries over the state of the art, on several metrics, without degradation in tail query performance."
N12-1074,Predicting Responses to Microblog Posts,2012,31,73,2,0,6799,yoav artzi,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Microblogging networks serve as vehicles for reaching and influencing users. Predicting whether a message will elicit a user response opens the possibility of maximizing the virality, reach and effectiveness of messages and ad campaigns on these networks. We propose a discriminative model for predicting the likelihood of a response or a retweet on the Twitter network. The approach uses features derived from various sources, such as the language used in the tweet, the user's social network and history. The feature design process leverages aggregate statistics over the entire social network to balance sparsity and informativeness. We use real-world tweets to train models and empirically show that they are capable of generating accurate predictions for a large number of tweets."
C12-1143,Underspecified Query Refinement via Natural Language Question Generation,2012,14,3,2,0,3156,hassan sajjad,Proceedings of {COLING} 2012,0,"Underspecified queries are common in vertical search engines, leading to large result sets that are difficult for users to navigate. In this paper, we show that we can automatically guide users to their target results by engaging them in a dialog consisting of well-formed binary questions mined from unstructured data. We propose a system that extracts candidate attribute-value question terms from unstructured descriptions of records in a database. These terms are then filtered using a Maximum Entropy classifier to identify those that are suitable for question formation given a user query. We then select question terms via a novel ranking function that aims to minimize the number of question turns necessary for a user to find her target result. We evaluate the quality of system-generated questions for grammaticality and refinement effectiveness. Our final system shows best results in effectiveness, percentage of well-formed questions, and percentage of answerable questions over three baseline systems."
W11-0319,Automatically Building Training Examples for Entity Extraction,2011,28,7,2,0.502956,44420,marco pennacchiotti,Proceedings of the Fifteenth Conference on Computational Natural Language Learning,0,"In this paper we present methods for automatically acquiring training examples for the task of entity extraction. Experimental evidence show that: (1) our methods compete with a current heavily supervised state-of-the-art system, within 0.04 absolute mean average precision; and (2) our model significantly outperforms other supervised and unsupervised baselines by between 0.15 and 0.30 in absolute mean average precision."
P11-1009,Jigs and Lures: Associating Web Queries with Structured Entities,2011,33,38,1,1,34734,patrick pantel,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We propose methods for estimating the probability that an entity from an entity database is associated with a web search query. Association is modeled using a query entity click graph, blending general query click logs with vertical query click logs. Smoothing techniques are proposed to address the inherent data sparsity in such graphs, including interpolation using a query synonymy model. A large-scale empirical analysis of the smoothing techniques, over a 2-year click graph collected from a commercial search engine, shows significant reductions in modeling error. The association models are then applied to the task of recommending products to web queries, by annotating queries with products from a large catalog and then mining query-product associations through web search session analysis. Experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall, that our top-performing model affects 9% of general web queries with 94% precision."
C10-1057,{F}act{R}ank: Random Walks on a Web of Facts,2010,28,14,2,0,46525,alpa jain,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Fact collections are mostly built using semi-supervised relation extraction techniques and wisdom of the crowds methods, rendering them inherently noisy. In this paper, we propose to validate the resulting facts by leveraging global constraints inherent in large fact collections, observing that correct facts will tend to match their arguments with other facts more often than with incorrect ones. We model this intuition as a graph-ranking problem over a fact graph and explore novel random walk algorithms. We present an empirical study, over a large set of facts extracted from a 500 million document webcrawl, validating the model and showing that it improves fact quality over state-of-the-art methods."
N09-1033,Semi-Automatic Entity Set Refinement,2009,31,22,2,1,47345,vishnu vyas,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"State of the art set expansion algorithms produce varying quality expansions for different entity types. Even for the highest quality expansions, errors still occur and manual refinements are necessary for most practical uses. In this paper, we propose algorithms to aide this refinement process, greatly reducing the amount of manual labor required. The methods rely on the fact that most expansion errors are systematic, often stemming from the fact that some seed elements are ambiguous. Using our methods, empirical evidence shows that average R-precision over random entity sets improves by 26% to 51% when given from 5 to 10 manually tagged errors. Both proposed refinement models have linear time complexity in set size allowing for practical online use in set expansion systems."
D09-1025,Entity Extraction via Ensemble Semantics,2009,27,75,2,0.564516,44420,marco pennacchiotti,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Combining information extraction systems yields significantly higher quality resources than each system in isolation. In this paper, we generalize such a mixing of sources and features in a framework called Ensemble Semantics. We show very large gains in entity extraction by combining state-of-the-art distributional and pattern-based systems with a large set of features from a webcrawl, query logs, and Wikipedia. Experimental results on a web-scale extraction of actors, athletes and musicians show significantly higher mean average precision scores (29% gain) compared with the current state of the art."
D09-1098,{W}eb-Scale Distributional Similarity and Entity Set Expansion,2009,50,209,1,1,34734,patrick pantel,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task. Parallelization and optimizations are necessary. We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web. The pairwise similarity between 500 million terms is computed in 50 hours using 200 quad-core nodes. We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia."
C08-2033,Explaining Similarity of Terms,2008,15,3,2,1,47345,vishnu vyas,Coling 2008: Companion volume: Posters,0,"Computing the similarity between entities is a core component of many NLP tasks such as measuring the semantic similarity of terms for generating a distributional thesaurus. In this paper, we study the problem of explaining post-hoc why a set of terms are similar. Given a set of terms, our task is to generate a small set of explanations that best characterizes the similarity of those terms. Our contributions include: 1) an information-theoretic objective function for quantifying the utility of an explanation set; 2) a survey of psycholinguistics and philosophy for evidence of different sources of explanations such as descriptive properties and prototypes; 3) computational baseline models for automatically generating various types of explanations; and 4) a qualitative evaluation of our explanation generation engine."
C08-1086,A Joint Information Model for N-Best Ranking,2008,32,3,1,1,34734,patrick pantel,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In this paper, we present a method for modeling joint information when generating n-best lists. We apply the method to a novel task of characterizing the similarity of a group of terms where only a small set of many possible semantic properties may be displayed to a user. We demonstrate that considering the results jointly, by accounting for the information overlap between results, generates better n-best lists than considering them independently. We propose an information theoretic objective function for modeling the joint information in an n-best list and show empirical evidence that humans prefer the result sets produced by our joint model. Our results show with 95% confidence that the n-best lists generated by our joint ranking model are significantly different from a baseline independent model 50.0% xc2xb1 3.1% of the time, out of which they are preferred 76.6% xc2xb1 5.2% of the time."
N07-1017,The Domain Restriction Hypothesis: Relating Term Similarity and Semantic Consistency,2007,15,12,3,0,3532,alfio gliozzo,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"In this paper, we empirically demonstrate what we call the domain restriction hypothesis, claiming that semantically related terms extracted from a corpus tend to be semantically coherent. We apply this hypothesis to define a post-processing module for the output of Espresso, a state of the art relation extraction system, showing that irrelevant and erroneous relations can be filtered out by our module, increasing the precision of the final output. Results are confirmed by both quantitative and qualitative analyses, showing that very high precision can be reached."
N07-1071,{ISP}: Learning Inferential Selectional Preferences,2007,20,87,1,1,34734,patrick pantel,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness."
D07-1017,{LEDIR}: An Unsupervised Algorithm for Learning Directionality of Inference Rules,2007,24,49,2,0,41637,rahul bhagat,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"Semantic inference is a core component of many natural language applications. In response, several researchers have developed algorithms for automatically learning inference rules from textual corpora. However, these rules are often either imprecise or underspecified in directionality. In this paper we propose an algorithm called LEDIR that filters incorrect inference rules and identifies the directionality of correct ones. Based on an extension to Harrisxe2x80x99s distributional hypothesis, we use selectional preferences to gather evidence of inference directionality and plausibility. Experiments show empirical evidence that our approach can classify inference rules significantly better than several baselines."
W06-3909,A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations,2006,-1,-1,2,0.925926,44420,marco pennacchiotti,Proceedings of the Fifth International Workshop on Inference in Computational Semantics ({IC}o{S}-5),0,None
W06-1650,Automatically Assessing Review Helpfulness,2006,24,367,2,0,121,soomin kim,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"User-supplied reviews are widely and increasingly used to enhance e-commerce and other websites. Because reviews can be numerous and varying in quality, it is important to assess how helpful each review is. While review helpfulness is currently assessed manually, in this paper we consider the task of automatically assessing it. Experiments using SVM regression on a variety of features over Amazon.com product reviews show promising results, with rank correlations of up to 0.66. We found that the most useful features include the length of the review, its unigrams, and its product rating."
P06-1015,{E}spresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations,2006,23,490,1,1,34734,patrick pantel,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we present Espresso, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations. The main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the Web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm. We present an empirical comparison of Espresso with various state of the art systems, on different size and genre corpora, on extracting various general and specific relations. Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision."
P06-1100,Ontologizing Semantic Relations,2006,20,63,2,0.925926,44420,marco pennacchiotti,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Many algorithms have been developed to harvest lexical semantic resources, however few have linked the mined knowledge into formal knowledge repositories. In this paper, we propose two algorithms for automatically ontologizing (attaching) semantic relations into WordNet. We present an empirical evaluation on the task of attaching part-of and causation relations, showing an improvement on F-score over a baseline model."
P05-1016,Inducing Ontological Co-occurrence Vectors,2005,22,44,1,1,34734,patrick pantel,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"In this paper, we present an unsupervised methodology for propagating lexical cooccurrence vectors into an ontology such as WordNet. We evaluate the framework on the task of automatically attaching new concepts into the ontology. Experimental results show 73.9% attachment accuracy in the first position and 81.3% accuracy in the top-5 positions. This framework could potentially serve as a foundation for ontologizing lexical-semantic resources and assist the development of other largescale and internally consistent collections of semantic information."
P05-1077,Randomized Algorithms and {NLP}: Using Locality Sensitive Hash Functions for High Speed Noun Clustering,2005,15,187,2,0,47397,deepak ravichandran,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic to practically linear in the number of elements to be computed."
I05-7009,The Omega Ontology,2005,22,60,3,0,41097,andrew philpot,Proceedings of {O}nto{L}ex 2005 - Ontologies and Lexical Resources,0,"We present the Omega ontology, a large terminological ontology obtained by remerging WordNet and Mikrokosmos, adding information from various other sources, and subordinating the result to a newly designed feature-oriented upper model. We explain the organizing principles of the representation used for Omega and discuss the methodology used to merge the constituent conceptual hierarchies. We survey a range of auxiliary knowledge sources (including instances, verb frame annotations, and domainspecific sub-ontologies) incorporated into the basic conceptual structure and applications that have benefited from Omega. Omega is available for browsing at http://omega.isi.edu/."
I05-1069,Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics,2005,6,21,2,0.869565,49341,timothy chklovski,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Recently, researchers have applied text- and web-mining algorithms to mine semantic resources. The result is often a noisy graph of relations between words. We propose a mathematically rigorous refinement framework, which uses path-based analysis, updating the likelihood of a relation between a pair of nodes using evidence provided by multiple indirect paths between the nodes. Evaluation on refining temporal verb relations in a semantic resource called VerbOcean showed a 16.1% error reduction after refinement."
W04-3205,{V}erb{O}cean: Mining the Web for Fine-Grained Semantic Verb Relations,2004,26,334,2,0.869565,49341,timothy chklovski,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy. Analysis of error types shows that on the relation strength we achieved 75% accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/."
N04-1041,Automatically Labeling Semantic Classes,2004,23,214,1,1,34734,patrick pantel,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc. The current state of the art discovers many semantic classes but fails to label their concepts. We propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach.
C04-1111,Towards Terascale Semantic Acquisition,2004,0,10,1,1,34734,patrick pantel,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,None
N03-4011,Automatically Discovering Word Senses,2003,10,10,1,1,34734,patrick pantel,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Demonstrations,0,We will demonstrate the output of a distributional clustering algorithm called Clustering by Committee that automatically discovers word senses from text.
C02-1144,Concept Discovery from Text,2002,15,157,2,0,30549,dekang lin,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Broad-coverage lexical resources such as WordNet are extremely useful. However, they often include many rare senses while missing domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers concepts from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning elements to their most similar cluster. Evaluating cluster quality has always been a difficult task. We present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key). Our experiments show that CBC outperforms several well-known clustering algorithms in cluster quality."
P00-1014,An Unsupervised Approach to Prepositional Phrase Attachment using Contextually Similar Words,2000,12,75,1,1,34734,patrick pantel,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"Prepositional phrase attachment is a common source of ambiguity in natural language processing. We present an unsupervised corpus-based approach to prepositional phrase attachment that achieves similar performance to supervised methods. Unlike previous unsupervised approaches in which training data is obtained by heuristic extraction of unambiguous examples from a corpus, we use an iterative process to extract training data from an automatically parsed corpus. Attachment decisions are made using a linear combination of features and low frequency events are approximated using contextually similar words."
A00-2011,Word-for-Word Glossing with Contextually Similar Words,2000,12,17,1,1,34734,patrick pantel,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Many corpus-based machine translation systems require parallel corpora. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. To gloss a word, we first identify its similar words that occurred in the same context in a large corpus. We then determine the gloss by maximizing the similarity between the set of contextually similar words and the different translations of the word in a bilingual thesaurus."
