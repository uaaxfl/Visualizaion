W16-2107,You Shall Know People by the Company They Keep: Person Name Disambiguation for Social Network Construction,2016,24,3,3,1,8096,mariona ardanuy,"Proceedings of the 10th {SIGHUM} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,None
2015.lilt-12.4,Clustering of Novels Represented as Social Networks,2015,31,4,2,0,37948,mariona adanay,"Linguistic Issues in Language Technology, Volume 12, 2015 - Literature Lifts up Computational Linguistics",0,"Within the field of literary analysis, there are few branches as confusing as that of genre theory. Literary criticism has failed so far to reach a consensus on what makes a genre a genre. In this paper, we examine the degree to which the character structure of a novel is indicative of the genre it belongs to. With the premise that novels are societies in miniature, we build static and dynamic social networks of characters as a strategy to represent the narrative structure of novels in a quantifiable manner. For each of the novels, we compute a vector of literary-motivated features extracted from their network representation. We perform clustering on the vectors and analyze the resulting clusters in terms of genre and authorship."
W14-0905,Structure-based Clustering of Novels,2014,-1,-1,2,1,8096,mariona ardanuy,Proceedings of the 3rd Workshop on Computational Linguistics for Literature ({CLFL}),0,None
J14-3007,Improved Estimation of Entropy for Evaluation of Word Sense Induction,2014,29,2,3,1,9084,linlin li,Computational Linguistics,0,"Information-theoretic measures are among the most standard techniques for evaluation of clustering methods including word sense induction (WSI) systems. Such measures rely on sample-based estimates of the entropy. However, the standard maximum likelihood estimates of the entropy are heavily biased with the bias dependent on, among other things, the number of clusters and the sample size. This makes the measures unreliable and unfair when the number of clusters produced by different systems vary and the sample size is not exceedingly large. This corresponds exactly to the setting of WSI evaluation where a ground-truth cluster sense number arguably does not exist and the standard evaluation scenarios use a small number of instances of each word to compute the score. We describe more accurate entropy estimators and analyze their performance both in simulations and on evaluation of WSI systems."
C14-1059,Lyrics-based Analysis and Classification of Music,2014,20,18,2,0,12067,michael fell,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,None
W13-5602,Invited Keynote: Detecting and Processing Figurative Language in Discourse,2013,9,0,1,1,33924,caroline sporleder,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,"Figurative language poses a serious challenge to NLP systems. The use of idiomatic and metaphoric expressions is not only extremely widespread in natural language; many figurative expressions, in particular idioms, also behave idiosyncratically. These idiosyncrasies are not restricted to a non-compositional meaning but often also extend to syntactic properties, selectional preferences etc. To deal appropriately with such expressions, NLP tools need to detect figurative language and assign the correct analyses to non-literal expressions. While there has been quite a bit of work on determining the general xe2x80x98idiomaticityxe2x80x99 of an expression (type-based approaches), this only solves part of the problem as many expressions, such as break the ice or play with fire, can also have a literal, perfectly compositional meaning (e.g. break the ice on the duck pond). Such expressions have to be disambiguated in context (token-based approaches). Token-based approaches have received increased attention recently. In this talk, I will present an unsupervised method for token-based idiom detection. The method exploits the fact that well-formed texts exhibit lexical cohesion, i.e. words are semantically related to other words in the context. I will show how cohesion can be modelled and how the cohesive structure can be used to distinguish literal and idiomatic usages and even detect newly coined figurative expressions."
W13-0111,Towards Weakly Supervised Resolution of Null Instantiations,2013,-1,-1,3,1,22652,philip gorinski,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,None
P13-1160,"A {B}ayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations",2013,38,51,3,0,20683,angeliki lazaridou,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure."
penas-etal-2012-evaluating,Evaluating Machine Reading Systems through Comprehension Tests,2012,6,5,7,0,37160,anselmo penas,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes a methodology for testing and evaluating the performance of Machine Reading systems through Question Answering and Reading Comprehension Tests. The methodology is being used in QA4MRE (QA for Machine Reading Evaluation), one of the labs of CLEF. The task was to answer a series of multiple choice tests, each based on a single document. This allows complex questions to be asked but makes evaluation simple and completely automatic. The evaluation architecture is completely multilingual: test documents, questions, and their answers are identical in all the supported languages. Background text collections are comparable collections harvested from the web for a set of predefined topics. Each test received an evaluation score between 0 and 1 using c@1. This measure encourages systems to reduce the number of incorrect answers while maintaining the number of correct ones by leaving some questions unanswered. 12 groups participated in the task, submitting 62 runs in 3 different languages (German, English, and Romanian). All runs were monolingual; no team attempted a cross-language task. We report here the conclusions and lessons learned after the first campaign in 2011."
J12-2001,Modality and Negation: An Introduction to the Special Issue,2012,118,84,2,0.0799381,5465,roser morante,Computational Linguistics,0,"Traditionally, most research in NLP has focused on propositional aspects of meaning. To truly understand language, however, extra-propositional aspects are equally important. Modality and negation typically contribute significantly to these extra-propositional meaning aspects. Although modality and negation have often been neglected by mainstream computational linguistics, interest has grown in recent years, as evidenced by several annotation projects dedicated to these phenomena. Researchers have started to work on modeling factuality, belief and certainty, detecting speculative sentences and hedging, identifying contradictions, and determining the scope of expressions of modality and negation. In this article, we will provide an overview of how modality and negation have been modeled in computational linguistics."
R11-1046,In Search of Missing Arguments: A Linguistic Approach,2011,11,14,3,1,3382,josef ruppenhofer,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"Semantic argument structures are often incomplete in that core arguments are not locally instantiated. However, many of these implicit arguments can be linked to referents in the wider context. In this paper we explore a number of linguistically motivated strategies for identifying and resolving such null instantiations (NIs). We show that a more sophisticated model for identifying definite NIs can lead to noticeable performance gains over the state-ofthe-art for NI resolution."
R11-1090,Robust Semantic Analysis for Unseen Data in {F}rame{N}et,2011,15,0,3,1,1316,alexis palmer,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"We present a novel method for FrameNetbased semantic role labeling (SRL), focusing on limitations posed by the limited coverage of available annotated data. Our SRL model is based on Bayesian clustering and has the advantage of being very robust in the face of unseen and incomplete data. Frame labeling and role labeling are modeled in like fashions, allowing cascading classification scenarios. The model is shown to perform especially well on unseen data. In addition, we show that for seen data, predicting semantic types for roles improves role labeling performance."
I11-1021,Enhancing Active Learning for Semantic Role Labeling via Compressed Dependency Trees,2011,41,3,3,0,20471,chenhua chen,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"This paper explores new approaches to active learning (AL) for semantic role labeling (SRL), focusing in particular on combining typical informativity-based sampling strategies with a novel measure of representativeness based on compressed dependency trees (CDTs). In essence, the compressed representation encodes the target predicate and the key dependents of the verb complex in the sentence. We first present our method for producing CDTs from the output of an existing dependency parser. The compressed trees are used as features for training a supervised SRL system. Second, we present a study of AL for SRL. We investigate a number of different sample selection strategies, and the best results are achieved by incorporating CDTs for example selection based on both informativity and representativeness. We show that our approach can reduce by up to 50% the amount of training data needed to attain a given level of performance."
S10-1008,{S}em{E}val-2010 Task 10: Linking Events and Their Participants in Discourse,2010,6,67,2,1,3382,josef ruppenhofer,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"We describe the SemEval-2010 shared task on Linking Events and Their Participants in Discourse. This task is an extension to the classical semantic role labeling task. While semantic role labeling is traditionally viewed as a sentence-internal task, local semantic argument structures clearly interact with each other in a larger context, e.g., by sharing references to specific discourse entities or events. In the shared task we looked at one particular aspect of cross-sentence links between argument structures, namely linking locally uninstantiated roles to their co-referents in the wider discourse context (if such co-referents exist). This task is potentially beneficial for a number of NLP applications, such as information extraction, question answering or text summarization."
P10-1116,Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection,2010,31,53,3,1,9084,linlin li,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrases given a context. We use a topic model to decompose this conditional probability into two conditional probabilities with latent variables. We propose three different instantiations of the model for solving sense disambiguation problems with different degrees of resource availability. The proposed models are tested on three different tasks: coarse-grained word sense disambiguation, fine-grained word sense disambiguation, and detection of literal vs. non-literal usages of potentially idiomatic expressions. In all three cases, we outperform state-of-the-art systems either quantitatively or statistically significantly."
N10-1039,Using {G}aussian Mixture Models to Detect Figurative Language in Context,2010,11,35,2,1,9084,linlin li,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,We present a Gaussian Mixture model for detecting different types of figurative language in context. We show that this model performs well when the parameters are estimated in an unsupervised fashion using EM. Performance can be improved further by estimating the parameters from a small annotated data set.
ruppenhofer-etal-2010-speaker,Speaker Attribution in Cabinet Protocols,2010,10,4,2,1,3382,josef ruppenhofer,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Historical cabinet protocols are a useful resource which enable historians to identify the opinions expressed by politicians on different subjects and at different points of time. While cabinet protocols are often available in digitized form, so far the only method to access their information content is by keyword-based search, which often returns sub-optimal results. We present a method for enriching German cabinet protocols with information about the originators of statements. This requires automatic speaker attribution. Unlike many other approaches, our method can also deal with cases in which the speaker is not explicitly identified in the sentence itself. Such cases are very common in our domain. To avoid costly manual annotation of training data, we design a rule-based system which exploits morpho-syntactic cues. We show that such a system obtains good results, especially with respect to recall which is particularly important for information access."
sporleder-etal-2010-idioms,Idioms in Context: The {IDIX} Corpus,2010,23,7,1,1,33924,caroline sporleder,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Idioms and other figuratively used expressions pose considerable problems to natural language processing applications because they are very frequent and often behave idiosyncratically. Consequently, there has been much research on the automatic detection and extraction of idiomatic expressions. Most studies focus on type-based idiom detection, i.e., distinguishing whether a given expression can (potentially) be used idiomatically. However, many expressions such as ''``break the ice'''' can have both literal and non-literal readings and need to be disambiguated in a given context (token-based detection). So far relatively few approaches have attempted context-based idiom detection. One reason for this may be that few annotated resources are available that disambiguate expressions in context. With the IDIX corpus, we aim to address this. IDIX is available as an add-on to the BNC and disambiguates different usages of a subset of idioms. We believe that this resource will be useful both for linguistic and computational linguistic studies."
wang-sporleder-2010-constructing,Constructing a Textual Semantic Relation Corpus Using a Discourse Treebank,2010,11,4,2,0,3690,rui wang,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we present our work on constructing a textual semantic relation corpus by making use of an existing treebank annotated with discourse relations. We extract adjacent text span pairs and group them into six categories according to the different discourse relations between them. After that, we present the details of our annotation scheme, which includes six textual semantic relations, 'backward entailment', 'forward entailment', 'equality', 'contradiction', 'overlapping', and 'independent'. We also discuss some ambiguous examples to show the difficulty of such annotation task, which cannot be easily done by an automatic mapping between discourse relations and semantic relations. We have two annotators and each of them performs the task twice. The basic statistics on the constructed corpus looks promising: we achieve 81.17{\%} of agreement on the six semantic relation annotation with a .718 kappa score, and it increases to 91.21{\%} if we collapse the last two labels with a .775 kappa score."
C10-2078,Linguistic Cues for Distinguishing Literal and Non-Literal Usages,2010,19,20,2,1,9084,linlin li,Coling 2010: Posters,0,"We investigate the effectiveness of different linguistic cues for distinguishing literal and non-literal usages of potentially idiomatic expressions. We focus specifically on features that generalize across different target expressions. While idioms on the whole are frequent, instances of each particular expression can be relatively infrequent and it will often not be feasible to extract and annotate a sufficient number of examples for each expression one might want to disambiguate. We experimented with a number of different features and found that features encoding lexical cohesion as well as some syntactic features can generalize well across idioms."
C10-2107,Evaluating {F}rame{N}et-style semantic parsing: the role of coverage gaps in {F}rame{N}et,2010,25,18,2,1,1316,alexis palmer,Coling 2010: Posters,0,"Supervised semantic role labeling (SRL) systems are generally claimed to have accuracies in the range of 80% and higher (Erk and Pado, 2006). These numbers, though, are the result of highly-restricted evaluations, i.e., typically evaluating on hand-picked lemmas for which training data is available. In this paper we consider performance of such systems when we evaluate at the document level rather than on the lemma level. While it is well-known that coverage gaps exist in the resources available for training supervised SRL systems, what we have been lacking until now is an understanding of the precise nature of this coverage problem and its impact on the performance of SRL systems. We present a typology of five different types of coverage gaps in FrameNet. We then analyze the impact of the coverage gaps on performance of a supervised semantic role labeling system on full texts, showing an average oracle upper bound of 46.8%."
W09-3738,Semantic Argument Structure in {D}iscours{E}: The {SEASIDE} project (project abstract),2009,8,0,1,1,33924,caroline sporleder,Proceedings of the Eight International Conference on Computational Semantics,0,"The recently started SEASIDE project is funded for five years (20082013) by the German Excellence Initiative as part of Saarland Universityxe2x80x99s Cluster of Excellence on xe2x80x9cMultimodal Computing and Interactionxe2x80x9d. In the project, we aim to bring together two active research areas which both deal with xe2x80x9ccomputing meaningxe2x80x9d but currently stand more or less independently next to each other: discourse processing and computation of semantic argument structure. We expect that both areas will benefit from this: semantic argument information will allow for a more sophisticated representation of discourse meaning, while discourse information can also be beneficial for systems which compute semantic argument structure (i.e. semantic role labellers). Eventually we aim for an incremental model of text meaning which can be computed in a robust, data-driven way by utilising and combining information from several levels of linguistic analysis. The model should be sophisticated enough to aid applications such as text mining, information extraction, question answering, and text summarisation. Discourse processing deals with modelling the meaning of multisentence units. Early approaches (e.g. Hobbs et al., 1993) were heavily knowledge-based and, while these systems worked well on small, well-defined domains, they generally did not scale up very well. More recent research largely abandoned the knowledge-based approach in favour of much shallower systems, either rule-based (Polanyi et al., 2004) or machine-learned (Soricut and Marcu, 2003). These systems rely largely on surface cues. While shallow models can be quite successful, they also have clear limitations. For example, progress on discourse parsing has stagnated in the last years and text summarisation is still a challenge, especially from multiple input documents."
W09-3211,A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of Multiword Expressions,2009,52,7,2,1,9084,linlin li,Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing ({T}ext{G}raphs-4),0,"We present a graph-based model for representing the lexical cohesion of a discourse. In the graph structure, vertices correspond to the content words of a text and edges connecting pairs of words encode how closely the words are related semantically. We show that such a structure can be used to distinguish literal and non-literal usages of multi-word expressions."
W09-3003,Assessing the benefits of partial automatic pre-labeling for frame-semantic annotation,2009,13,10,3,0,5564,ines rehbein,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"In this paper, we present the results of an experiment in which we assess the usefulness of partial semi-automatic annotation for frame labeling. While we found no conclusive evidence that it can speed up human annotation, automatic pre-annotation does increase its overall quality."
W09-2417,{S}em{E}val-2010 Task 10: Linking Events and Their Participants in Discourse,2009,18,19,2,1,3382,josef ruppenhofer,Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions ({SEW}-2009),0,"In this paper, we describe the SemEval-2010 shared task on Linking Events and Their Participants in Discourse. This task is a variant of the classical semantic role labelling task. The novel aspect is that we focus on linking local semantic argument structures across sentence boundaries. Specifically, the task aims at linking locally uninstantiated roles to their co-referents in the wider discourse context (if such co-referents exist). This task is potentially beneficial for a number of NLP applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co-reference resolution and information extraction."
E09-1086,Unsupervised Recognition of Literal and Non-Literal Use of Idiomatic Expressions,2009,27,57,1,1,33924,caroline sporleder,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We propose an unsupervised method for distinguishing literal and non-literal usages of idiomatic expressions. Our method determines how well a literal interpretation is linked to the overall cohesive structure of the discourse. If strong links can be found, the expression is classified as literal, otherwise as idiomatic. We show that this method can help to tell apart literal and non-literal usages, even for idioms which occur in canonical form."
D09-1033,Classifier Combination for Contextual Idiom Detection Without Labelled Data,2009,18,23,2,1,9084,linlin li,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"We propose a novel unsupervised approach for distinguishing literal and non-literal use of idiomatic expressions. Our model combines an unsupervised and a supervised classifier. The former bases its decision on the cohesive structure of the context and labels training data for the latter, which can then take a larger feature space into account. We show that a combination of both classifiers leads to significant improvements over using the unsupervised classifier alone."
C08-1084,Semantic Role Assignment for Event Nominalisations by Leveraging Verbal Data,2008,25,20,3,0,411,sebastian pado,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a novel approach to the task of semantic role labelling for event nominalisations, which make up a considerable fraction of predicates in running text, but are underrepresented in terms of training data and difficult to model. We propose to address this situation by data expansion. We construct a model for nominal role labelling solely from verbal training data. The best quality results from salvaging grammatical features where applicable, and generalising over lexical heads otherwise."
S07-1039,{ILK}: Machine learning of semantic relations with shallow features and almost no data,2007,14,8,3,0,16715,iris hendrickx,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This paper summarizes our approach to the Semeval 2007 shared task on Classification of Semantic Relations between Nominals. Our overall strategy is to develop machine-learning classifiers making use of a few easily computable and effective features, selected independently for each classifier in wrapper experiments. We train two types of classifiers for each of the seven relations: with and without WordNet information."
D07-1087,Bootstrapping Information Extraction from Field Books,2007,11,13,2,0,47609,sander canisius,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We present two machine learning approaches to information extraction from semi-structured documents that can be used if no annotated training data are available, but there does exist a database filled with information derived from the type of documents to be processed. One approach employs standard supervised learning for information extraction by artificially constructing labelled training data from the contents of the database. The second approach combines unsupervised Hidden Markov modelling with language models. Empirical evaluation of both systems suggests that it is possible to bootstrap a field segmenter from a database alone. The combination of Hidden Markov and language modelling was found to perform best at this task."
W06-2206,Spotting the {`}Odd-one-out{'}: Data-Driven Error Detection and Correction in Textual Databases,2006,11,8,1,1,33924,caroline sporleder,Proceedings of the Workshop on Adaptive Text Extraction and Mining ({ATEM} 2006),0,"We present two methods for semiautomatic detection and correction of errors in textual databases. The first method (horizontal correction) aims at correcting inconsistent values within a database record, while the second (vertical correction) focuses on values which were entered in the wrong column. Both methods are data-driven and language-independent. We utilise supervised machine learning, but the training data is obtained automatically from the database; no manual annotation is required. Our experiments show that a significant proportion of errors can be detected by the two methods. Furthermore, both methods were found to lead to a precision that is high enough to make semi-automatic error correction feasible."
sporleder-etal-2006-identifying,Identifying Named Entities in Text Databases from the Natural History Domain,2006,15,10,1,1,33924,caroline sporleder,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper, we investigate whether it is possible to bootstrap a named entity tagger for textual databases by exploiting the database structure to automatically generate domain and database-specific gazetteer lists. We compare three tagging strategies: (i) using the extracted gazetteers in a look-up tagger, (ii) using the gazetteers to automatically extract training data to train a database-specific tagger, and (iii) using a generic named entity tagger. Our results suggest that automatically built gazetteers in combination with a look-up tagger lead to a relatively good performance and that generic taggers do not perform particularly well on this type of data."
H05-1033,Discourse Chunking and its Application to Sentence Compression,2005,22,102,1,1,33924,caroline sporleder,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"In this paper we consider the problem of analysing sentence-level discourse structure. We introduce discourse chunking (i.e., the identification of intra-sentential nucleus and satellite spans) as an alternative to full-scale discourse parsing. Our experiments show that the proposed modelling approach yields results comparable to state-of-the-art while exploiting knowledge-lean features and small amounts of discourse annotations. We also demonstrate how discourse chunking can be successfully applied to a sentence compression task."
W04-3210,Automatic Paragraph Identification: A Study across Languages and Domains,2004,16,13,1,1,33924,caroline sporleder,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,None
C04-1007,Combining Hierarchical Clustering and Machine Learning to Predict High-Level Discourse Structure,2004,18,14,1,1,33924,caroline sporleder,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We propose a novel method to predict the interparagraph discourse structure of text, i.e. to infer which paragraphs are related to each other and form larger segments on a higher level. Our method combines a clustering algorithm with a model of segment relatedness acquired in a machine learning step. The model integrates information from a variety of sources, such as word co-occurrence, lexical chains, cue phrases, punctuation, and tense. Our method outperforms an approach that relies on word co-occurrence alone."
