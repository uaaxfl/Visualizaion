2003.mtsummit-papers.21,P98-1080,1,0.808985,"Missing"
2003.mtsummit-papers.21,A00-1002,0,0.516352,"t on the quality of translation, • • 3 from the user’s point of view there is no difference (except for the small difference in the quality of translation memories) when our method is used compared to the original process of working with the support of solely human-made translation memories, and given a sufficient quality of the MT from the pivot to the target language, our method may substantially increase the speed and reduce the costs of the translation from the source to the target languages. Basic properties of the system The method described above has been applied in the system Česílko (Hajič and Kuboň, 2000) for the translation between Czech as a pivot language and Slovak as a target language. The basic premise of the system was to use as simple method of analysis and transfer as possible. The experience from an existing MT system RUSLAN (Czech-toRussian MT system) aimed at the translation of software manuals for operating systems of mainframes – cf. (Oliva, 1989) made it apparent that a full-fledged syntactic analysis of Czech is both unnecessary and too unreliable and costly. The system Česílko therefore uses the method of direct word-for-word translation (after necessary morphological processi"
2003.mtsummit-papers.21,P99-1065,0,0.0308968,"Missing"
2005.eamt-1.11,E03-1004,1,0.86863,"Missing"
2005.eamt-1.11,P99-1065,0,0.0572303,"Missing"
2005.eamt-1.11,P98-1080,1,0.855716,"Missing"
2005.eamt-1.11,J93-2004,0,0.025287,"on would have been more money and time consuming. The choice of the Penn Treebank as the source corpus was pragmatically motivated: firstly it is a widely recognized linguistic resource, and secondly the translators were native speakers of Czech, capable of high quality translation into their native language. Since Czech is a language with a relatively high degree of word-order freedom, and its sentences contain certain syntactic phenomena, such as discontinuous constituents (non-projective constructions), which cannot be straightforwardly handled using the annotation scheme of Penn Treebank (Marcus et al., 1993; Linguistic Data Consortium, 1999), based on phrasestructure trees, we decided to adopt for the PCEDT the dependency-based annotation scheme of the Prague Dependency Treebank – PDT (Linguistic Data Consortium, 2001; Sgall et al., 1986), which is described in Section 3. In Section 2., we describe the process of translating the Penn Treebank into Czech and reference retranslations. Section 4. presents manual tectogrammatical annotation for both languages. The automatic process of transformation of Penn Treebank annotation of English into both representations, analytical and tectogrammatical, is"
2005.eamt-1.11,J03-1002,0,0.0111049,". For the translation of this set of words we used three different Czech-English manual dictionaries: two of them were available on the Web (WinGED and GNU/FDL) and one was extracted from Czech and English EuroWordNets. We included only translations that occurred in at least two of the three dictionaries or the frequency of which is significant in the English North American News Text Collection (310M words). POS tag and lemma were added to each Czech entry. If possible, we selected the same POS for the English translation, otherwise the most frequent one. By training GIZA++ translation model (Och and Ney, 2003) on the training part of the PCEDT extended by the obtained entry-translation pairs, we created a EAMT 2005 Conference Proceedings Cmejrek et al. probabilistic Czech-English dictionary more sensitive to the domain of financial news specific for the Wall Street Journal. The resulting probabilistic dictionary contains 46,150 entry-translation pairs. Czech-English Dictionary of Word Forms Since Czech is highly inflective, the PCEDT also comprises a translation dictionary of word forms containing pairs of Czech and English word forms agreeing in appropriate morphological categories (such as number"
2005.eamt-1.11,zabokrtsky-etal-2002-machine,0,0.0410851,"Missing"
2016.eamt-2.1,W15-3009,1,0.880814,"Missing"
2016.eamt-2.1,W15-5711,1,0.827881,"Missing"
2016.eamt-2.1,W15-3006,1,0.877207,"Missing"
2016.eamt-2.1,P15-4020,0,0.0200302,"Missing"
2016.eamt-2.1,P13-4014,0,0.0192755,"Missing"
2016.eamt-2.1,W15-2505,0,0.0245204,"Missing"
2016.eamt-2.1,P15-3002,0,0.0279562,"Missing"
2020.lrec-1.407,gavrilidou-etal-2012-meta,1,0.919419,"Missing"
2020.lrec-1.407,2020.lrec-1.420,1,0.860379,"Missing"
2020.lrec-1.407,L18-1213,1,0.894888,"Missing"
2020.lrec-1.407,piperidis-etal-2014-meta,1,0.824391,"ween 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META-NET results (Rehm and Uszkoreit, 2012), funded a"
2020.lrec-1.407,piperidis-2012-meta,1,0.92358,"n 34 European countries. META-NET was, between 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META"
2020.lrec-1.407,L16-1251,1,0.865781,"Missing"
2020.lrec-1.407,2020.lrec-1.413,1,0.785764,"Missing"
2020.lrec-1.413,cassidy-etal-2014-alveo,0,0.0760054,"Missing"
2020.lrec-1.413,W03-0810,0,0.137563,"Missing"
2020.lrec-1.413,gavrilidou-etal-2012-meta,1,0.915791,"Missing"
2020.lrec-1.413,hinrichs-krauwer-2014-clarin,0,0.182909,"Missing"
2020.lrec-1.413,P10-4005,0,0.0528445,"Missing"
2020.lrec-1.413,2020.lrec-1.420,1,0.821941,"Missing"
2020.lrec-1.413,L18-1213,1,0.812955,"0b) and plan to integrate experimental workflow functionality into ELG. SHARE metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent activities such as ELRC-SHARE (Lösch et al., 2018; Piperidis et al., 2018) and META-SHARE (Piperidis, 2012; Piperidis et al., 2014). Table 2 provides an overview of what has been identified in these repositories and what is planned to be ingested into ELG, if their access and licensing conditions allow it. Corpora Lexicons Models Total ELRA ELRC-SHARE META-SHARE ELG 848 396 1580 78 1084 132 1261 109 0 0 18 76 1932 528 2859 263 Total 2902 2586 94 5582 Table 2: Identified LRs in the ELG consortium LR modalities covered are text (corpora, lexicons, etc.), speech/audio, video/audiovisual, images/OCR, sign language, and others. About 220 addition"
2020.lrec-1.413,2020.iwltp-1.12,1,0.789706,"ercial services, written in disparate programming languages (Java/Spring, .NET, Python) with just a few days work in the first iteration, falling to a few hours once developers became more familiar with the infrastructure and required formats. 14 These requests are received and handled by the LT Service Execution Server (Section 3.1). 3370 The composition of individual services offered by ELG directly or other cloud platforms is not addressed by ELG itself. However, we experiment with workflow composition and platform interoperability in other contexts (Rehm et al., 2020a; Rehm et al., 2020b; Moreno-Schneider et al., 2020a; Moreno-Schneider et al., 2020b) and plan to integrate experimental workflow functionality into ELG. SHARE metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent"
2020.lrec-1.413,2020.lrec-1.284,1,0.363908,"Missing"
2020.lrec-1.413,piperidis-etal-2014-meta,1,0.89681,"metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent activities such as ELRC-SHARE (Lösch et al., 2018; Piperidis et al., 2018) and META-SHARE (Piperidis, 2012; Piperidis et al., 2014). Table 2 provides an overview of what has been identified in these repositories and what is planned to be ingested into ELG, if their access and licensing conditions allow it. Corpora Lexicons Models Total ELRA ELRC-SHARE META-SHARE ELG 848 396 1580 78 1084 132 1261 109 0 0 18 76 1932 528 2859 263 Total 2902 2586 94 5582 Table 2: Identified LRs in the ELG consortium LR modalities covered are text (corpora, lexicons, etc.), speech/audio, video/audiovisual, images/OCR, sign language, and others. About 220 additional repositories have been located so far, which will increase the numbers in Table"
2020.lrec-1.413,L18-1205,1,0.945027,"tities of interest to users (Section 3.6), appropriately indexed and described so that they can easily search, find and select the resources that meet their requirements and deploy them, as well as visualise the LT domain activities, stakeholders and resources with specific criteria (e. g., service type, language, etc.). All entities are described in compliance with the ELG-SHARE metadata schema (Labropoulou et al., 2019; Labropoulou et al., 2020).7 The schema builds upon, consolidates and updates previous activities, especially the META-SHARE schema and its profiles (Gavrilidou et al., 2012; Piperidis et al., 2018; Labropoulou et al., 2018), taking into account the ELG user requirements (Melnika et al., 2019a), recent developments in the (meta)data domain (e. g., FAIR8 , data and software citation recommendations9 , Open Science movement, etc.), and the need for establishing a common pool of resources through exchange mechanisms with collaborating projects and initiatives (Rehm et al., 2020c), cf. Section 3.6. The schema caters for the description of the ELG core entities (Figure 2), i. e., Language Technologies (tools/services), including functional services and nonfunctional ones (e. g., downloadable"
2020.lrec-1.413,piperidis-2012-meta,1,0.90149,"y into ELG. SHARE metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent activities such as ELRC-SHARE (Lösch et al., 2018; Piperidis et al., 2018) and META-SHARE (Piperidis, 2012; Piperidis et al., 2014). Table 2 provides an overview of what has been identified in these repositories and what is planned to be ingested into ELG, if their access and licensing conditions allow it. Corpora Lexicons Models Total ELRA ELRC-SHARE META-SHARE ELG 848 396 1580 78 1084 132 1261 109 0 0 18 76 1932 528 2859 263 Total 2902 2586 94 5582 Table 2: Identified LRs in the ELG consortium LR modalities covered are text (corpora, lexicons, etc.), speech/audio, video/audiovisual, images/OCR, sign language, and others. About 220 additional repositories have been located so far, which will incr"
2020.lrec-1.413,L18-1519,1,0.634981,"any are world-class, with technologies that outperform the global players. However, European LT business is also fragmented – by nation states, languages, domains and sectors (Vasiljevs et al., 2019) –, significantly holding back its impact. In addition, many European languages are severely under-resourced and, thus, in danger of digital language exinction (Rehm and Uszkoreit, 2012; Kornai, 2013; Rehm et al., 2014; Rehm et al., 2016a), which is why there is an enormous need for a European LT platform as a unifying umbrella (Rehm and Uszkoreit, 2013; Rehm et al., 2016b; STOA, 2017; Rehm, 2017; Rehm and Hegele, 2018; European Parliament, 2018). The project European Language Grid (ELG; 2019-2021) addresses this fragmentation by establishing the ELG as the primary platform and marketplace for the European LT community, both industry and research.1 The ELG is developed to be a scalable cloud platform, providing, in an easy-to-integrate way, access to hundreds of commercial and non-commercial LTs for all European languages, including running tools and services as well as data sets and 1 https://www.european-language-grid.eu resources. Once fully operational, it will enable the commercial and non-commercial E"
2020.lrec-1.413,L16-1251,1,0.856739,"Missing"
2020.lrec-1.413,P14-5010,0,\N,Missing
2020.lrec-1.497,de-marneffe-etal-2006-generating,1,\N,Missing
2020.lrec-1.497,zeman-2008-reusable,1,\N,Missing
2020.lrec-1.497,de-marneffe-etal-2014-universal,1,\N,Missing
2020.lrec-1.497,W08-1301,1,\N,Missing
2020.lrec-1.497,petrov-etal-2012-universal,0,\N,Missing
2020.lrec-1.497,P13-1051,1,\N,Missing
2020.lrec-1.497,P15-2111,0,\N,Missing
2020.lrec-1.497,L16-1376,1,\N,Missing
2020.lrec-1.497,L16-1262,1,\N,Missing
2020.lrec-1.497,W18-6012,1,\N,Missing
2020.lrec-1.641,berovic-etal-2012-croatian,0,0.0283116,"tated not annotated not annotated not annotated not annotated not annotated manually manually manually not annotated not annotated not annotated not annotated not annotated not annotated not annotated not annotated not annotated Table 1: Overview of various types of annotation and their realization in the datasets (new manual annotation made to PDT-C 1.0 is indicated in bold) Sect. 3) and also for treebanks originating elsewhere: HamleDT (Zeman et al., 2012), Slovene Dependency Treebank (Džeroski et al., 2006), Greek Dependency Treebank (Prokopidis et al., 2005), Croatian Dependency Treebank (Berović et al., 2012), Latin Dependency Treebank (Bamman and Crane, 2006), and Slovak National Corpus (Šimková and Garabík, 2006). 3. From PDT to PDT-C The first version of the Prague Dependency Treebank (PDT in the sequel) was published in 2001 (Hajič et al., 2001). It only contained annotation at the morphological and surface syntactic layers, and a very small “preview” of how the deep syntactic annotation might look like. The full manual annotation at all three annotation layers including the deep syntactic one is present in the second version, PDT 2.0, published in 2006 (Hajič et al., 2006). The later versions"
2020.lrec-1.641,burchardt-etal-2006-salsa,0,0.0989672,"manually in all four datasets.2 1 https://ufal.mff.cuni.cz/pdt-c Consolidation and fully manual annotation of the surfacesyntactic layer is planned for the next, 2.0 version of PDT-C. PDT-C 1.0 is provided as a digital open resource accessible to all users via the LINDAT/CLARIN repository.3 2. Related Work There is a wide range of corpora with rich linguistic annotation, e.g., Penn Treebank (Marcus et al., 1993), its successors PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006); for German, there is Tiger (Brants et al., 2002) and Salsa (Burchardt et al., 2006). The Prague Dependency Treebank is an effort inspired by the PennTreebank (Marcus et al., 1993) (but annotated natively in dependency-style) and it is unique in its attempt to systematically include and link different layers of language including the deep syntactic layer. The PDT project has been successfully developed over the years and PDTannotation scheme has been used for other in-house development of related treebanks of Czech texts (see next 2 3 5208 http://hdl.handle.net/11234/1-3185 Dataset/Type of annotation Audio ASR transcript Transcript Written non-applicable non-applicable non-ap"
2020.lrec-1.641,dzeroski-etal-2006-towards,0,0.0878895,"t annotated manually not annotated manually manually manually not annotated not annotated manually not annotated not annotated not annotated not annotated not annotated not annotated manually manually manually not annotated not annotated not annotated not annotated not annotated not annotated not annotated not annotated not annotated Table 1: Overview of various types of annotation and their realization in the datasets (new manual annotation made to PDT-C 1.0 is indicated in bold) Sect. 3) and also for treebanks originating elsewhere: HamleDT (Zeman et al., 2012), Slovene Dependency Treebank (Džeroski et al., 2006), Greek Dependency Treebank (Prokopidis et al., 2005), Croatian Dependency Treebank (Berović et al., 2012), Latin Dependency Treebank (Bamman and Crane, 2006), and Slovak National Corpus (Šimková and Garabík, 2006). 3. From PDT to PDT-C The first version of the Prague Dependency Treebank (PDT in the sequel) was published in 2001 (Hajič et al., 2001). It only contained annotation at the morphological and surface syntactic layers, and a very small “preview” of how the deep syntactic annotation might look like. The full manual annotation at all three annotation layers including the deep syntactic"
2020.lrec-1.641,hajic-etal-2012-announcing,1,0.773356,"Missing"
2020.lrec-1.641,N06-2015,0,0.0823361,"the highest deep syntactic layer (structure, functions, valency) have been done manually in all four datasets.2 1 https://ufal.mff.cuni.cz/pdt-c Consolidation and fully manual annotation of the surfacesyntactic layer is planned for the next, 2.0 version of PDT-C. PDT-C 1.0 is provided as a digital open resource accessible to all users via the LINDAT/CLARIN repository.3 2. Related Work There is a wide range of corpora with rich linguistic annotation, e.g., Penn Treebank (Marcus et al., 1993), its successors PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006); for German, there is Tiger (Brants et al., 2002) and Salsa (Burchardt et al., 2006). The Prague Dependency Treebank is an effort inspired by the PennTreebank (Marcus et al., 1993) (but annotated natively in dependency-style) and it is unique in its attempt to systematically include and link different layers of language including the deep syntactic layer. The PDT project has been successfully developed over the years and PDTannotation scheme has been used for other in-house development of related treebanks of Czech texts (see next 2 3 5208 http://hdl.handle.net/11234/1-3185 Dataset/Type of an"
2020.lrec-1.641,kingsbury-palmer-2002-treebank,0,0.330088,"layer (lemmatization and tagging); also, basic phenomena of the annotation at the highest deep syntactic layer (structure, functions, valency) have been done manually in all four datasets.2 1 https://ufal.mff.cuni.cz/pdt-c Consolidation and fully manual annotation of the surfacesyntactic layer is planned for the next, 2.0 version of PDT-C. PDT-C 1.0 is provided as a digital open resource accessible to all users via the LINDAT/CLARIN repository.3 2. Related Work There is a wide range of corpora with rich linguistic annotation, e.g., Penn Treebank (Marcus et al., 1993), its successors PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006); for German, there is Tiger (Brants et al., 2002) and Salsa (Burchardt et al., 2006). The Prague Dependency Treebank is an effort inspired by the PennTreebank (Marcus et al., 1993) (but annotated natively in dependency-style) and it is unique in its attempt to systematically include and link different layers of language including the deep syntactic layer. The PDT project has been successfully developed over the years and PDTannotation scheme has been used for other in-house development of related treebanks of Czech texts (see"
2020.lrec-1.641,J93-2004,0,0.0965641,"en fully performed at the lowest morphological layer (lemmatization and tagging); also, basic phenomena of the annotation at the highest deep syntactic layer (structure, functions, valency) have been done manually in all four datasets.2 1 https://ufal.mff.cuni.cz/pdt-c Consolidation and fully manual annotation of the surfacesyntactic layer is planned for the next, 2.0 version of PDT-C. PDT-C 1.0 is provided as a digital open resource accessible to all users via the LINDAT/CLARIN repository.3 2. Related Work There is a wide range of corpora with rich linguistic annotation, e.g., Penn Treebank (Marcus et al., 1993), its successors PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006); for German, there is Tiger (Brants et al., 2002) and Salsa (Burchardt et al., 2006). The Prague Dependency Treebank is an effort inspired by the PennTreebank (Marcus et al., 1993) (but annotated natively in dependency-style) and it is unique in its attempt to systematically include and link different layers of language including the deep syntactic layer. The PDT project has been successfully developed over the years and PDTannotation scheme has been used for other in-hous"
2020.lrec-1.641,meyers-etal-2004-annotating,0,0.0380176,"basic phenomena of the annotation at the highest deep syntactic layer (structure, functions, valency) have been done manually in all four datasets.2 1 https://ufal.mff.cuni.cz/pdt-c Consolidation and fully manual annotation of the surfacesyntactic layer is planned for the next, 2.0 version of PDT-C. PDT-C 1.0 is provided as a digital open resource accessible to all users via the LINDAT/CLARIN repository.3 2. Related Work There is a wide range of corpora with rich linguistic annotation, e.g., Penn Treebank (Marcus et al., 1993), its successors PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006); for German, there is Tiger (Brants et al., 2002) and Salsa (Burchardt et al., 2006). The Prague Dependency Treebank is an effort inspired by the PennTreebank (Marcus et al., 1993) (but annotated natively in dependency-style) and it is unique in its attempt to systematically include and link different layers of language including the deep syntactic layer. The PDT project has been successfully developed over the years and PDTannotation scheme has been used for other in-house development of related treebanks of Czech texts (see next 2 3 5208 http://hdl.handle.n"
2020.lrec-1.641,W13-3727,0,0.019753,"on is captured at the deep syntactic layer only in the core PDT corpus (written text dataset; cf. Table 1). 5.9. Additional Annotation At the deep syntactic layer in the core PDT part (written dataset), valency of nouns, textual nominal coreference, bridging and discourse relations and other semantic properties of the sentence such as genre specification, multiword expressions, quotation are also annotated. More information of these special annotations can be found in general overview (Mikulová et al., 2013), and also in detailed studies (Lopatková et al., 2012; Nedoluzhko and Mírovský, 2011; Nedoluzhko and Mírovský, 2013; Poláková et al., 2012; Bejček and Straňák, 2010; Zikánová et al., 2015). 6. As it has been already mentioned, the latest published versions of the included datasets have been enhanced in PDT-C 1.0 by a new or corrected manual annotation at the morphological layer (new in translated, spoken, and user-generated data, corrected in the original written dataset). Altogether, there are now 3,895,348 tokens with manual morphological annotation in the PDT-C 1.0 (cf. Table 2). 6.1. Annotation Process The annotation is based on a manual disambiguation of an automatic, dictionary-based morphological an"
2020.lrec-1.641,L16-1262,1,0.897679,"Missing"
2020.lrec-1.641,C08-1085,1,0.713034,"tion of the surface syntactic layer is contained only in the dataset of written texts and it consists of 1,503,739 tokens). Table 1 presents an overview of various types of annotation at the three annotation layers (see Sect. 5) in each dataset and the information of the manner in which the annotations was carried out. The newly provided manual morphological annotation made to PDT-C 1.0 is indicated in bold. The markup used in PDT-C 1.0 is the language-independent Prague Markup Language (PML), which is an XML subset (using a specific scheme) customized for multi-layered linguistic annotation (Pajas and Štěpánek, 2008). 4.1. Written Data The dataset of written texts is based on the core Prague Dependency Treebank (Hajič et al., 2018). The data consist of articles from Czech daily newspapers and magazines. The annotation in the PDT dataset is the richest one and it has completely been perfomed manually (cf. Table 1). In PDT-C 1.0, the annotation at the morphological layer has been checked and corrected to reflect the updated morphological annotation guidelines and also to be fully consistent with the morphological dictionary (see Sect. 6). 4.2. Translated Data The dataset of translated texts comes from the P"
2020.lrec-1.641,panevova-sevcikova-2010-annotation,0,0.0323824,"r content words only. Function words (prepositions, auxiliary verbs, etc.) do not have nodes of their own, but their contribution to the meaning of the sentence is not lost – several attributes are attached to the nodes the values of which represent such a contribution (e.g. tense for verbs). Some of the nodes do not correspond to any original token; they are added in case of surface deletions (ellipses). The types of the (semantic) dependency relations are represented by the functor attribute attached to all nodes. So called grammatemes (described in detail in Razímová and Žabokrtský (2006), Panevová and Ševčíková (2010), Ševčíková et al. (2010)) are attached to some nodes; they provide information about the node that cannot be derived from the deep syntactic structure, the functor and other attributes. Grammatemes are counterparts of those morphological categories which bear relevant deep syntactic or semantic information. Grammatemes are annotated at the deep syntactic layer only in the core PDT corpus (written texts; cf. Table 1). 5.5. Valency The core ingredient in the annotation of the deep syntactic layer is valency, the theoretical description of which, as developed in the framework of Functional Gener"
2020.lrec-1.641,P14-5003,1,0.887454,"Missing"
2020.lrec-1.641,W19-8510,0,0.012621,"ective aspect (‘to happen’) and stát-2 with imperfective aspect (‘to stand’). Thus, we have, e.g., lemma jeřáb-1 for crane as a bird (animate masculine) and jeřáb-2 for both a tree and crane as a device for lifting heavy objects (inanimate masculine). We do not distinguish the latter two meanings (tree vs. device), because they do not differ from the inflectional point of view (same declension). There might be a difference in derivation; e.g. the word jeřábník (a man who works with a crane-device) is derived from jeřáb as a device. This has been delegated to derivational data sources, such as Vidra et al. (2019); here, we do not to take such derivational, stylistic and semantic differences into account. Orthographic and stylistic variants of a word (e.g., an archaic variant these, a standard variant teze, and a nonstandard variant téze ‘thesis’) were not tackled uniformly in MorfFlex. Some variants had different paradigms with different lemmas, others were grouped into one paradigm Lemma teze these_,a_∧ (∧ DD**teze) téze_,h_∧ (∧ GC**teze) Description Part of speech Detailed part of speech Gender Number Case Possessor’s gender Possessor’s number Person Tense Degree of comparison Negation Voice Verbal"
2020.lrec-1.641,zeman-etal-2012-hamledt,1,0.748535,"y not annotated manually not annotated manually not annotated manually not annotated manually manually manually not annotated not annotated manually not annotated not annotated not annotated not annotated not annotated not annotated manually manually manually not annotated not annotated not annotated not annotated not annotated not annotated not annotated not annotated not annotated Table 1: Overview of various types of annotation and their realization in the datasets (new manual annotation made to PDT-C 1.0 is indicated in bold) Sect. 3) and also for treebanks originating elsewhere: HamleDT (Zeman et al., 2012), Slovene Dependency Treebank (Džeroski et al., 2006), Greek Dependency Treebank (Prokopidis et al., 2005), Croatian Dependency Treebank (Berović et al., 2012), Latin Dependency Treebank (Bamman and Crane, 2006), and Slovak National Corpus (Šimková and Garabík, 2006). 3. From PDT to PDT-C The first version of the Prague Dependency Treebank (PDT in the sequel) was published in 2001 (Hajič et al., 2001). It only contained annotation at the morphological and surface syntactic layers, and a very small “preview” of how the deep syntactic annotation might look like. The full manual annotation at all"
C16-2009,cinkova-2006-propbank,0,0.0282211,"rm - the second, more abstract sense requires a particular preposition and case (specified by s+7, lit. with, and 7 for instrumental case), while the deep object of the former sense is a simple direct (prepositionless) accusative (specified by 4, as cases are typically numbered in Czech). Figure 1: Czech Valency lexicon (PDT-Vallex) entry (two senses of “kalkukovat”: lit. “compute” and “count on sth/sb”) In addition, not only the Czech and English treebanks are aligned in the PCEDT, but so are the associated valency lexicons for Czech - PDT-Vallex10 (Urešová, 2011b) and English - EngVallex11 (Cinková, 2006), forming a bilingual parallel CzEngVallex lexicon (Urešová et al., 2016) which explicitly aligns verb senses as well as verb arguments between the two languages. What was missing after having explicit alignments annotated was a tool that would allow inspection of the resulting corpus and lexicons, allowing cross-lingual queries with reasonable flexibility to support linguistic studies, NLP tasks, manual check of results of automatic tools, etc. In our previous work (Fuˇcíková et al., 2015), we have developed a tool that can search the lexicon(s) in a cross-lingual manner, allowing to formulat"
C16-2009,hajic-etal-2012-announcing,1,0.897303,"Missing"
C16-2009,L16-1457,0,0.0602697,"Missing"
C16-2009,J93-2004,0,0.0529356,"ns). Our task was to make the Prague Czech-English Dependency Treebank (PCEDT 2.0) (Hajiˇc et al., 2012) efficiently searchable, aiming to be a help for various applications of computational and traditional linguistics as well as for NLP studies. The PCEDT is a bilingual corpus that contains both rich dependency and predicate-argument annotation itself, as well as links to valency lexicons used (not only) for predicate-argument annotation consistency. In this respect, it is similar to the PropBank (Palmer et al., 2005), which annotates predicate-argument structure on top of the Penn Treebank (Marcus et al., 1993), indexing also by pointing to the frame files, from which additional information about the predicates (verbs) can be extracted. However, PropBank is a monolingual resource. Also, because English is not a very morphologically rich language, PropBank’s frame files do not contain much more than a list of arguments and sense distinctions; in contrast, Czech language is quite rich in this respect, and consequently, the Czech valency lexicon entries (Urešová, 2011b) contain additional information on the required form of verb arguments in terms of case, prepositions to be used, etc. Fig. 1 shows a s"
C16-2009,J05-1004,0,0.0361792,"e corpora contain - in some cases - lemmas, which can be then searched for in independent lexicons). Our task was to make the Prague Czech-English Dependency Treebank (PCEDT 2.0) (Hajiˇc et al., 2012) efficiently searchable, aiming to be a help for various applications of computational and traditional linguistics as well as for NLP studies. The PCEDT is a bilingual corpus that contains both rich dependency and predicate-argument annotation itself, as well as links to valency lexicons used (not only) for predicate-argument annotation consistency. In this respect, it is similar to the PropBank (Palmer et al., 2005), which annotates predicate-argument structure on top of the Penn Treebank (Marcus et al., 1993), indexing also by pointing to the frame files, from which additional information about the predicates (verbs) can be extracted. However, PropBank is a monolingual resource. Also, because English is not a very morphologically rich language, PropBank’s frame files do not contain much more than a list of arguments and sense distinctions; in contrast, Czech language is quite rich in this respect, and consequently, the Czech valency lexicon entries (Urešová, 2011b) contain additional information on the"
C16-2009,stepanek-pajas-2010-querying,0,0.0433523,"Missing"
C18-1208,I13-1150,0,0.0215214,"WordNet synsets across languages (Fellbaum and Vossen, 2012). WordNet contains a rich network or relations between its synsets (hyperonymy, hyponymy, ...), however, it does not contain information about syntactic and compositional semantic behavior, which is a drawback especially in case of verbs. Our approach to verbal synonymy builds on previous research which also considered multilingual data (translations) in parallel corpora as an important resource. Translational context is regarded as a rich source of semantic information (de Jong and Appelo, 1987; Dyvik, 1998; Adamska-Sałaciak, 2013; Andrade et al., 2013). Parallel corpora have also been used (Resnik, 1997; Ide, 1999; Ide et al., 2002) for automatic methods for sense induction and disambiguation, for cross-lingual similarity detection and synonym extraction (Wu et al., 2010; Wu and Palmer, 2011). (Wang and Wu, 2012) studies various assumptions about synonyms in translation, for the purpose of trend detection from titles. While these works aim at automatic methods and applications, they share the idea that if two words are semantically similar in a language, their translations in another language would be also similar. Translations of a word fr"
C18-1208,P98-1013,0,0.884748,"es (about 1800 verbs) and evaluate interannotator agreement using several metrics. 1 Introduction Lexical resources, despite the fast progress in building end-to-end systems based on deep learning and artificial neural networks, are an important piece of the puzzle in Computational Linguistics and Natural Language Processing (NLP). They provide information that humans need to understand relations between words as well as the usage of these words in text. In addition, they can help various NLP tasks. This is why lexicons like WordNet (Miller, 1995; Fellbaum, 1998; Pala et al., 2011), FrameNet (Baker et al., 1998; Fillmore et al., 2003), VerbNet (Schuler, 2006), PropBank (Palmer et al., 2005) or EngVallex (Cinková, 2006; Cinková et al., 2014) have been created. They are for English, but there are also similar resources for other languages, often in a multilingual setting: WordNet is available in many languages (Vossen, 2004; Fellbaum and Vossen, 2012), Predicate Matrix (Lopez de Lacalle et al., 2016) extend coverage of several verbal resources and adds more romance languages, FrameNet has been extended to multiple languages (Boas, 2009), or there is the bilingual valency lexicon CzEngVallex for the ca"
C18-1208,cinkova-2006-propbank,0,0.902403,"s, despite the fast progress in building end-to-end systems based on deep learning and artificial neural networks, are an important piece of the puzzle in Computational Linguistics and Natural Language Processing (NLP). They provide information that humans need to understand relations between words as well as the usage of these words in text. In addition, they can help various NLP tasks. This is why lexicons like WordNet (Miller, 1995; Fellbaum, 1998; Pala et al., 2011), FrameNet (Baker et al., 1998; Fillmore et al., 2003), VerbNet (Schuler, 2006), PropBank (Palmer et al., 2005) or EngVallex (Cinková, 2006; Cinková et al., 2014) have been created. They are for English, but there are also similar resources for other languages, often in a multilingual setting: WordNet is available in many languages (Vossen, 2004; Fellbaum and Vossen, 2012), Predicate Matrix (Lopez de Lacalle et al., 2016) extend coverage of several verbal resources and adds more romance languages, FrameNet has been extended to multiple languages (Boas, 2009), or there is the bilingual valency lexicon CzEngVallex for the case of Czech and English (Urešová et al., 2016). One might thus question why it is necessary to develop anothe"
C18-1208,R15-1021,0,0.0306553,"notes that “there is unfortunately no neat way of characterising synonyms”, the notion of synonymy is mostly seen as “sameness or identity of meaning” (Palmer, 1976; Sparck Jones, 1986). Leech (2012) restricts synonymy to equivalence of conceptual meaning. Synonym is mostly defined as “a same-language equivalent” (Adamska-Sałaciak, 2010; Adamska-Sałaciak, 2013) and “does not exceed the limits of a single language” (Gouws, 2013), while for bilingual contexts the term translational equivalent is used. On the other hand, (Martin, 1960; Klégr, 2004; Hahn et al., 2005; Hayashi, 2012; Haiyan, 2015; Dinu et al., 2015) recognize interlingual synonymy and use either the term foreignlanguage equivalent, cross-lingual synonym, synonymous translation equivalent or bilingual synonym. In building CzEngClass, we consider the relationship between the lexical unit of the source language (SL) and of the target language (TL) unit as a specific type of synonymy, an interlingual synonymy. For words from different languages which are interlingual synonyms, we prefer to use the term bilingual synonyms. We understand that the meaning correspondence does not mean absolute equivalence and automatic interchangeability. We agr"
C18-1208,W02-0808,0,0.115332,"network or relations between its synsets (hyperonymy, hyponymy, ...), however, it does not contain information about syntactic and compositional semantic behavior, which is a drawback especially in case of verbs. Our approach to verbal synonymy builds on previous research which also considered multilingual data (translations) in parallel corpora as an important resource. Translational context is regarded as a rich source of semantic information (de Jong and Appelo, 1987; Dyvik, 1998; Adamska-Sałaciak, 2013; Andrade et al., 2013). Parallel corpora have also been used (Resnik, 1997; Ide, 1999; Ide et al., 2002) for automatic methods for sense induction and disambiguation, for cross-lingual similarity detection and synonym extraction (Wu et al., 2010; Wu and Palmer, 2011). (Wang and Wu, 2012) studies various assumptions about synonyms in translation, for the purpose of trend detection from titles. While these works aim at automatic methods and applications, they share the idea that if two words are semantically similar in a language, their translations in another language would be also similar. Translations of a word from another language are often synonyms of one another (Lin et al., 2003; Wu and Zh"
C18-1208,W99-0508,0,0.0503946,"ains a rich network or relations between its synsets (hyperonymy, hyponymy, ...), however, it does not contain information about syntactic and compositional semantic behavior, which is a drawback especially in case of verbs. Our approach to verbal synonymy builds on previous research which also considered multilingual data (translations) in parallel corpora as an important resource. Translational context is regarded as a rich source of semantic information (de Jong and Appelo, 1987; Dyvik, 1998; Adamska-Sałaciak, 2013; Andrade et al., 2013). Parallel corpora have also been used (Resnik, 1997; Ide, 1999; Ide et al., 2002) for automatic methods for sense induction and disambiguation, for cross-lingual similarity detection and synonym extraction (Wu et al., 2010; Wu and Palmer, 2011). (Wang and Wu, 2012) studies various assumptions about synonyms in translation, for the purpose of trend detection from titles. While these works aim at automatic methods and applications, they share the idea that if two words are semantically similar in a language, their translations in another language would be also similar. Translations of a word from another language are often synonyms of one another (Lin et a"
C18-1208,L16-1349,0,0.0309462,"tically similar in a language, their translations in another language would be also similar. Translations of a word from another language are often synonyms of one another (Lin et al., 2003; Wu and Zhou, 2003). A similar idea, i.e., that words sharing translational context are semantically related, can be found in (Plas and Tiedemann, 2006). Interannotator agreement evaluation is regularly used in corpus annotation. However, it is much more scarcely used in lexicon entry creation and annotation, especially in a multilingual setting. For assignment of topics to words in Hindi and English, see (Kanojia et al., 2016). More detailed account on the influence of semantic lexical granularity within the Context Pattern Analysis paradigm on interannotator agreement can be found in (Cinková et al., 2012). 3 Synonymy in bilingual context Synonymy in bilingual context is closely related to translational equivalence, sameness, similarity, meaning, word sense, etc. These terms - and the term synonymy itself - are not always used in an unambiguous way. We thus discuss the terminology first, and then specify how we define verbal synonymy in bilingual context in the work on building the CzEngClass lexicon. 2457 3.1 Ter"
C18-1208,J93-2004,0,0.0605956,"4 Resources used The resources used to create the CzEngClass lexicon are divided into two groups - primary and secondary. Primary resources are those used for word sense disambiguation and valency information when creating the class member candidates. We use the parallel Prague Czech-English Dependency Treebank 2.0 (PCEDT 2.0) (Hajiˇc et al., 2012) with its associated lexicons. This treebank contains over 1.2 million words in almost 50,000 sentences for each language. About 90,000 tokens are verbs on each side. The English part contains the entire Penn Treebank - Wall Street Journal Section (Marcus et al., 1993). The Czech part is a manual translation of all the Penn Treebank-WSJ texts to Czech. PCEDT is annotated using The Prague Dependency Treebank style (Hajiˇc et al., 2006; Hajiˇc et al., 2018) manual linguistic annotation based on the Functional Generative Description framework (Sgall et al., 1986). Our research benefits primarily from the deep syntactico-semantic (tectogrammatical) dependency trees, interlinked across the two languages on sentence and node (content word) levels. Each deep syntax verb occurrence in the PCEDT is linked to the corresponding valency frame (predicate-argument struct"
C18-1208,J05-1004,0,0.876381,"cs. 1 Introduction Lexical resources, despite the fast progress in building end-to-end systems based on deep learning and artificial neural networks, are an important piece of the puzzle in Computational Linguistics and Natural Language Processing (NLP). They provide information that humans need to understand relations between words as well as the usage of these words in text. In addition, they can help various NLP tasks. This is why lexicons like WordNet (Miller, 1995; Fellbaum, 1998; Pala et al., 2011), FrameNet (Baker et al., 1998; Fillmore et al., 2003), VerbNet (Schuler, 2006), PropBank (Palmer et al., 2005) or EngVallex (Cinková, 2006; Cinková et al., 2014) have been created. They are for English, but there are also similar resources for other languages, often in a multilingual setting: WordNet is available in many languages (Vossen, 2004; Fellbaum and Vossen, 2012), Predicate Matrix (Lopez de Lacalle et al., 2016) extend coverage of several verbal resources and adds more romance languages, FrameNet has been extended to multiple languages (Boas, 2009), or there is the bilingual valency lexicon CzEngVallex for the case of Czech and English (Urešová et al., 2016). One might thus question why it is"
C18-1208,P15-2067,0,0.0469945,"Missing"
C18-1208,P06-2111,0,0.0486875,"raction (Wu et al., 2010; Wu and Palmer, 2011). (Wang and Wu, 2012) studies various assumptions about synonyms in translation, for the purpose of trend detection from titles. While these works aim at automatic methods and applications, they share the idea that if two words are semantically similar in a language, their translations in another language would be also similar. Translations of a word from another language are often synonyms of one another (Lin et al., 2003; Wu and Zhou, 2003). A similar idea, i.e., that words sharing translational context are semantically related, can be found in (Plas and Tiedemann, 2006). Interannotator agreement evaluation is regularly used in corpus annotation. However, it is much more scarcely used in lexicon entry creation and annotation, especially in a multilingual setting. For assignment of topics to words in Hindi and English, see (Kanojia et al., 2016). More detailed account on the influence of semantic lexical granularity within the Context Pattern Analysis paradigm on interannotator agreement can be found in (Cinková et al., 2012). 3 Synonymy in bilingual context Synonymy in bilingual context is closely related to translational equivalence, sameness, similarity, me"
C18-1208,L18-1227,1,0.796185,"Missing"
C18-1208,L18-1136,1,0.803048,"Missing"
C18-1208,W11-1003,0,0.0261203,"ior, which is a drawback especially in case of verbs. Our approach to verbal synonymy builds on previous research which also considered multilingual data (translations) in parallel corpora as an important resource. Translational context is regarded as a rich source of semantic information (de Jong and Appelo, 1987; Dyvik, 1998; Adamska-Sałaciak, 2013; Andrade et al., 2013). Parallel corpora have also been used (Resnik, 1997; Ide, 1999; Ide et al., 2002) for automatic methods for sense induction and disambiguation, for cross-lingual similarity detection and synonym extraction (Wu et al., 2010; Wu and Palmer, 2011). (Wang and Wu, 2012) studies various assumptions about synonyms in translation, for the purpose of trend detection from titles. While these works aim at automatic methods and applications, they share the idea that if two words are semantically similar in a language, their translations in another language would be also similar. Translations of a word from another language are often synonyms of one another (Lin et al., 2003; Wu and Zhou, 2003). A similar idea, i.e., that words sharing translational context are semantically related, can be found in (Plas and Tiedemann, 2006). Interannotator agre"
C18-1208,W03-1610,0,0.103331,"l., 2002) for automatic methods for sense induction and disambiguation, for cross-lingual similarity detection and synonym extraction (Wu et al., 2010; Wu and Palmer, 2011). (Wang and Wu, 2012) studies various assumptions about synonyms in translation, for the purpose of trend detection from titles. While these works aim at automatic methods and applications, they share the idea that if two words are semantically similar in a language, their translations in another language would be also similar. Translations of a word from another language are often synonyms of one another (Lin et al., 2003; Wu and Zhou, 2003). A similar idea, i.e., that words sharing translational context are semantically related, can be found in (Plas and Tiedemann, 2006). Interannotator agreement evaluation is regularly used in corpus annotation. However, it is much more scarcely used in lexicon entry creation and annotation, especially in a multilingual setting. For assignment of topics to words in Hindi and English, see (Kanojia et al., 2016). More detailed account on the influence of semantic lexical granularity within the Context Pattern Analysis paradigm on interannotator agreement can be found in (Cinková et al., 2012). 3"
C18-1208,2010.amta-papers.15,0,0.0269016,"al semantic behavior, which is a drawback especially in case of verbs. Our approach to verbal synonymy builds on previous research which also considered multilingual data (translations) in parallel corpora as an important resource. Translational context is regarded as a rich source of semantic information (de Jong and Appelo, 1987; Dyvik, 1998; Adamska-Sałaciak, 2013; Andrade et al., 2013). Parallel corpora have also been used (Resnik, 1997; Ide, 1999; Ide et al., 2002) for automatic methods for sense induction and disambiguation, for cross-lingual similarity detection and synonym extraction (Wu et al., 2010; Wu and Palmer, 2011). (Wang and Wu, 2012) studies various assumptions about synonyms in translation, for the purpose of trend detection from titles. While these works aim at automatic methods and applications, they share the idea that if two words are semantically similar in a language, their translations in another language would be also similar. Translations of a word from another language are often synonyms of one another (Lin et al., 2003; Wu and Zhou, 2003). A similar idea, i.e., that words sharing translational context are semantically related, can be found in (Plas and Tiedemann, 2006"
cmejrek-etal-2004-prague,A00-2018,0,\N,Missing
cmejrek-etal-2004-prague,E03-1004,1,\N,Missing
cmejrek-etal-2004-prague,P98-1080,1,\N,Missing
cmejrek-etal-2004-prague,C98-1077,0,\N,Missing
cmejrek-etal-2004-prague,J03-1002,0,\N,Missing
cmejrek-etal-2004-prague,zabokrtsky-etal-2002-machine,0,\N,Missing
cmejrek-etal-2004-prague,P01-1030,0,\N,Missing
D18-1532,Q13-1034,1,0.80489,"e train the model in a supervised fashion, requiring training data containing word forms, lemmas, and POS tags. In addition, we incorporate the ideas from Inoue et al. (2017) to optionally allow the network to predict the subcategories of each tag to improve accuracy. Our model is related to the work of M¨uller et al. (2015), which use conditional random fields (CRF) to jointly tag and lemmatize words for morphologically rich languages. The idea of jointly predicting several dimensions of categories has been explored prior to this work, for example, joint morphological and syntactic analysis (Bohnet et al., 2013) or joint parsing and semantic role labeling (Gesmundo et al., 2009). Our model consists of three parts: (1) The shared encoder, which creates an internal representation for every word based on its character se1 The code for this project is available at https:// github.com/hyperparticle/LemmaTag Pa D rt o e f G tail Sp e e e N nde d P ech um r O ( C b (1 S 12 a e 1 ( ) Po se ( r (6 ) 75) 9 ) Posses ) s s Pe ses or’ r s s Te son or’ Ge n ( s n C se 5) Nu der om (6 m ( be 5) N p ) eg ar r (3 Vo at iso ) i n o R ice n ( D es (3 3 eg R erv ) ) re e e e (4 Sp serv d ( ) 1 e ec d ) ia ( l ( 1) 10 )"
D18-1532,P17-1136,0,0.165443,"( s n C se 5) Nu der om (6 m ( be 5) N p ) eg ar r (3 Vo at iso ) i n o R ice n ( D es (3 3 eg R erv ) ) re e e e (4 Sp serv d ( ) 1 e ec d ) ia ( l ( 1) 10 ) Abstract VB-S---3P-AA--Figure 1: The tag components of the PDT Czech treebank with the numbers of valid values. Around 1500 different tags are in use in the PDT. 4921 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4921–4928 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics quence and the sentence context. We adopt the encoder architecture of Chakrabarty et al. (2017), utilizing character-level (Heigold et al., 2017) and word-level embeddings (Mikolov et al., 2013b; Santos and Zadrozny, 2014) processed through several layers of bidirectional recurrent neural networks (BRNN/BiRNN) (Schuster and Paliwal, 1997; Chakrabarty et al., 2017). (2) The tagger decoder, which applies a fully-connected layer to the outputs of the shared encoder to predict the POS tags. (3) The lemmatizer decoder, which applies an RNN sequence decoder to the combined outputs of the shared encoder and tagger decoder, producing a sequence of characters that predict each lemma (similar to"
D18-1532,L16-1239,0,0.242647,"int) 96.90 98.37 SoTA results 95.89a+ 97.86b+ Approach German-TIGER tag lem 98.96 98.84 98.97 99.05 98.04c+ 98.24c+ Arabic-PADT∗ tag lem 95.03 96.07 95.21 96.08 91.68d+ 92.60e Eng-EWT tag lem 95.50 97.03 95.37 97.53 93.90e 96.90e Eng-WSJ tag 97.59 N/A 97.78f+ Table 1: Final accuracies on the test sets comparing the LemmaTag architecture as described (joint), LemmaTag neither sharing the encoder nor providing tagger features (sep), and the state-of-the-art results (SoTA). The stateof-the-art results are taken from the following papers: (a) Hajiˇc et al. (2009), (b) Strakov´a et al. (2014), (c) Eger et al. (2016), (d) Inoue et al. (2017), (e) Straka et al. (2016), (f) Ling et al. (2015). The results marked with a plus+ use additional resources apart from the dataset, and datasets marked with a star∗ indicate the availability of subcategorical tags. mutually beneficial in morphologically rich languages. We have shown that incorporating these ideas results in excellent performance, surpassing state-of-the-art in Czech, German, and Arabic POS tagging and lemmatization by a substantial margin, while closely matching state-of-theart English POS tagging accuracy. However, in languages with weak morphology s"
D18-1532,E12-1068,0,0.0570022,"ficult to process in many NLP tasks (Tsarfaty et al., 2010). As opposed to analytical languages like English, morphologically rich languages encode diverse sets of grammatical information within each word using inflections, which convey characteristics such as case, gender, and tense. The addition of several inflectional variants across many words dramatically increases the vocabulary size, which results in data sparsity and outof-vocabulary (OOV) issues. Due to these issues, morphological part-ofspeech (POS) tagging and lemmatization are heavily used in NLP tasks such as machine translation (Fraser et al., 2012) and sentiment analysis (Abdul-Mageed et al., 2014). In morphologically rich languages, the POS tags typically consist of multiple morpho-syntactic subcategories providing additional information (see Figure 1). Closely related to POS tagging is lemmatization, which involves transforming each word to its root or dictionary form. Both tasks require context-sensitive awareness to disambiguate words with the same form but different syntactic or semantic features and behavior. Furthermore, lemmatization of a word form can benefit substantially from the information present in morphological tags, as"
D18-1532,W09-1205,0,0.0349878,"containing word forms, lemmas, and POS tags. In addition, we incorporate the ideas from Inoue et al. (2017) to optionally allow the network to predict the subcategories of each tag to improve accuracy. Our model is related to the work of M¨uller et al. (2015), which use conditional random fields (CRF) to jointly tag and lemmatize words for morphologically rich languages. The idea of jointly predicting several dimensions of categories has been explored prior to this work, for example, joint morphological and syntactic analysis (Bohnet et al., 2013) or joint parsing and semantic role labeling (Gesmundo et al., 2009). Our model consists of three parts: (1) The shared encoder, which creates an internal representation for every word based on its character se1 The code for this project is available at https:// github.com/hyperparticle/LemmaTag Pa D rt o e f G tail Sp e e e N nde d P ech um r O ( C b (1 S 12 a e 1 ( ) Po se ( r (6 ) 75) 9 ) Posses ) s s Pe ses or’ r s s Te son or’ Ge n ( s n C se 5) Nu der om (6 m ( be 5) N p ) eg ar r (3 Vo at iso ) i n o R ice n ( D es (3 3 eg R erv ) ) re e e e (4 Sp serv d ( ) 1 e ec d ) ia ( l ( 1) 10 ) Abstract VB-S---3P-AA--Figure 1: The tag components of the PDT Czech"
D18-1532,E09-1087,1,0.856869,"Missing"
D18-1532,E17-1048,0,0.0679264,"Missing"
D18-1532,K17-1042,0,0.335061,"ame form but different syntactic or semantic features and behavior. Furthermore, lemmatization of a word form can benefit substantially from the information present in morphological tags, as grammatical attributes often disambiguate word forms using context (M¨uller et al., 2015). We address context-sensitive POS tagging and lemmatization using a neural network model that jointly performs both tasks on each input word in a given sentence.1 We train the model in a supervised fashion, requiring training data containing word forms, lemmas, and POS tags. In addition, we incorporate the ideas from Inoue et al. (2017) to optionally allow the network to predict the subcategories of each tag to improve accuracy. Our model is related to the work of M¨uller et al. (2015), which use conditional random fields (CRF) to jointly tag and lemmatize words for morphologically rich languages. The idea of jointly predicting several dimensions of categories has been explored prior to this work, for example, joint morphological and syntactic analysis (Bohnet et al., 2013) or joint parsing and semantic role labeling (Gesmundo et al., 2009). Our model consists of three parts: (1) The shared encoder, which creates an internal"
D18-1532,D15-1166,0,0.0230296,"der, a setup typical of many sequenceto-sequence (seq2seq) tasks such as in neural machine translation (Sutskever et al., 2014). The lemmatizer consists of a recurrent LSTM layer whose initial state is taken from word-level output ow i and whose inputs consist of three parts. The first part is the embedding of the previous output character (initially a beginning-of-word character BOW). The second part is a character-level attention mechanism (Bahdanau et al., 2014) on the outputs of the character-level BRNN eci,1 , . . . , eci,mi . We employ the multiplicative attention mechanism described in Luong et al. (2015), which allows the LSTM cell to compute an attention vector that selectively weights character-level information in eci,j at each time step j based on the input state of the LSTM cell. The third and final part of the RNN input allows the network to receive the information about the embedding of the word, the surrounding context of the sentence, and the output of the tagger. This output is the same for all time steps of a lemma and is a concatenation of the following: the outw put of the encoder ow i , the embedded word ei and f processed tag features Ti . The tag features are obtained by proje"
D18-1532,J93-2004,0,0.0646752,"Missing"
D18-1532,D15-1272,0,0.192749,"Missing"
D18-1532,N18-1202,0,0.061224,"of tags correlating less with word-level morphology, and more with sentencelevel syntax in morphologically poor languages. Lemma prediction could benefit from the syntactic information in the tags, but the tag predictions rely more on syntactic structure (i.e., word order) rather than on root forms of individual words which could be ambiguous. There are some possible performance improvements and additional metrics which we leave for future work. For simplicity, one improvement we intentionally left out is the use of additional data. We can incorporate word2vec (Mikolov et al., 2013a) or ELMo (Peters et al., 2018) word representations, which have shown to reduce outof-domain issues and provide semantic information (Eger et al., 2016). A second improvement is to integrate information from a morphological dictionary to resolve certain ambiguities (Hajiˇc et al., 2009; Inoue et al., 2017). A third improvement can be to replace the seq2seq lemmatizer decoder with a classifier that chooses a corresponding edit tree to modify (reduce) the word form to its lemma (Chakrabarty et al., 2017). A fourth possible improvement would be to experiment with the Transformer model (Vaswani et al., 2017), which utilizes no"
D18-1532,silveira-etal-2014-gold,0,0.0673003,"Missing"
D18-1532,L16-1680,1,0.851379,"roach German-TIGER tag lem 98.96 98.84 98.97 99.05 98.04c+ 98.24c+ Arabic-PADT∗ tag lem 95.03 96.07 95.21 96.08 91.68d+ 92.60e Eng-EWT tag lem 95.50 97.03 95.37 97.53 93.90e 96.90e Eng-WSJ tag 97.59 N/A 97.78f+ Table 1: Final accuracies on the test sets comparing the LemmaTag architecture as described (joint), LemmaTag neither sharing the encoder nor providing tagger features (sep), and the state-of-the-art results (SoTA). The stateof-the-art results are taken from the following papers: (a) Hajiˇc et al. (2009), (b) Strakov´a et al. (2014), (c) Eger et al. (2016), (d) Inoue et al. (2017), (e) Straka et al. (2016), (f) Ling et al. (2015). The results marked with a plus+ use additional resources apart from the dataset, and datasets marked with a star∗ indicate the availability of subcategorical tags. mutually beneficial in morphologically rich languages. We have shown that incorporating these ideas results in excellent performance, surpassing state-of-the-art in Czech, German, and Arabic POS tagging and lemmatization by a substantial margin, while closely matching state-of-theart English POS tagging accuracy. However, in languages with weak morphology such as English (and German to a lesser extent), sha"
D18-1532,P14-5003,1,0.901304,"Missing"
D18-1532,W10-1401,0,0.0426412,"Missing"
D18-1532,L16-1262,1,\N,Missing
D18-1532,N18-1126,0,\N,Missing
de-smedt-etal-2014-clara,Y12-1015,0,\N,Missing
de-smedt-etal-2014-clara,W11-2153,0,\N,Missing
de-smedt-etal-2014-clara,W12-3903,0,\N,Missing
de-smedt-etal-2014-clara,W11-2605,0,\N,Missing
de-smedt-etal-2014-clara,W13-1728,0,\N,Missing
de-smedt-etal-2014-clara,R11-1041,1,\N,Missing
de-smedt-etal-2014-clara,W11-4647,0,\N,Missing
de-smedt-etal-2014-clara,W13-2907,1,\N,Missing
de-smedt-etal-2014-clara,W11-4604,1,\N,Missing
de-smedt-etal-2014-clara,P13-1054,1,\N,Missing
de-smedt-etal-2014-clara,Y12-1014,0,\N,Missing
de-smedt-etal-2014-clara,P11-3013,0,\N,Missing
de-smedt-etal-2014-clara,ramasamy-zabokrtsky-2012-prague,0,\N,Missing
de-smedt-etal-2014-clara,W13-2805,0,\N,Missing
de-smedt-etal-2014-clara,larasati-2012-identic,0,\N,Missing
de-smedt-etal-2014-clara,alonso-etal-2012-voting,1,\N,Missing
de-smedt-etal-2014-clara,W12-3410,0,\N,Missing
de-smedt-etal-2014-clara,drobac-etal-2014-heuristic,1,\N,Missing
de-smedt-etal-2014-clara,P13-2127,1,\N,Missing
de-smedt-etal-2014-clara,W11-4406,0,\N,Missing
de-smedt-etal-2014-clara,dione-2014-pruning,0,\N,Missing
de-smedt-etal-2014-clara,R11-2019,0,\N,Missing
de-smedt-etal-2014-clara,W12-6304,0,\N,Missing
de-smedt-etal-2014-clara,W14-1203,1,\N,Missing
de-smedt-etal-2014-clara,W12-5017,0,\N,Missing
de-smedt-etal-2014-clara,lis-2012-polish,0,\N,Missing
de-smedt-etal-2014-clara,W14-0808,0,\N,Missing
de-smedt-etal-2014-clara,schumann-2012-knowledge,0,\N,Missing
de-smedt-etal-2014-clara,C12-1065,1,\N,Missing
de-smedt-etal-2014-clara,dione-2012-morphological,0,\N,Missing
de-smedt-etal-2014-clara,escartin-2012-design,0,\N,Missing
de-smedt-etal-2014-clara,W12-2019,1,\N,Missing
de-smedt-etal-2014-clara,lenkiewicz-etal-2012-avatech,1,\N,Missing
de-smedt-etal-2014-clara,W11-3302,1,\N,Missing
de-smedt-etal-2014-clara,escartin-2014-chasing,0,\N,Missing
de-smedt-etal-2014-clara,W12-0503,0,\N,Missing
de-smedt-etal-2014-clara,W13-5411,1,\N,Missing
de-smedt-etal-2014-clara,gebre-etal-2012-towards,1,\N,Missing
E09-1087,A00-1031,0,0.316643,"m for the plain data We have found that it is better to feed the training with different chunks of the unsupervised data at each iteration. We have then experimented with 14 This tagger (possibly different from any of the N taggers from Step 1) runs as usual, but it is given a minimal list of (at most N ) tags that come from Step 2 only. 15 ”Accuracy” means accuracy of the semi-supervised method using this tagger for pre-tagging the unsupervised data, not the accuracy of the tagger itself. 16 In fact, we have experimented with other tagger combinations and configurations as well—with the TnT (Brants, 2000), MaxEnt (Ratnaparkhi, 1996) and TreeTagger (Schmid, 1994), with or without the Morˇce tagger in the pack; see below for the winning combination. 17 This patch is available on the paper’s website (see Section 7). 1. run N different taggers independently; 2. join the results on each position in the data from the previous step — each token thus ends up with between 1 and N tags, a union of the tags output by the taggers at that position; 767 97.35 97.30 Accuracy on development data Czech 96.21 96.20 96.21 97.25 English 97.44 97.44 97.44 Every iteration Every 4th iteration Every 8th iteration Eve"
E09-1087,P98-1029,0,0.0357702,"Missing"
E09-1087,P07-1096,0,0.805112,"ore each of them needs a different set of feature templates. We have empirically tested hundreds of feature templates on both languages, taken over from previous works for direct comparison, inspired by them, or based on a combination of previous experience, error analysis and linguistic intuition. In the following sections, we present the best performing set of feature templates as determined on the development data set using only the supervised training setting; our feature templates have thus not been influenced nor extended by the unsupervised data.13 11 The full list of tags, as used by (Shen et al., 2007), also makes the underlying Viterbi algorithm unbearably slow. 12 The English morphology tool is also downloadable as a separate module on the paper’s accompanying website. 13 Another set of experiments has shown that there is not, perhaps surprisingly, a significant gain in doing so. 766 5 The (un)supervised training setup 3. do final disambiguation (by a single tagger14 ). We have extended the averaged perceptron setup in the following way: the training algorithm is fed, in each iteration, by a concatenation of the supervised data (the manually tagged corpus) and the automatically pre-tagged"
E09-1087,W07-1709,1,0.225039,"Missing"
E09-1087,W02-1001,0,0.827057,"mean easy to use for further research on problems requiring POS tagging, especially multilingual ones. 3 And much easier to (re)implement as libraries in prototype systems, which is often difficult if not impossible with other people’s code. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 763–771, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 763 97.5 97.0 96.5 96.0 Accuracy on development data 98.0 which we cannot.) To summarize, we can describe our system as follows: it is based on (Votrubec, 2006)’s implementation of (Collins, 2002), which has been fed at each iteration by a different dataset consisting of the supervised and unsupervised part: precisely, by a concatenation of the manually tagged training data (WSJ portion of the PTB 3 for English, morphologically disambiguated data from PDT 2.0 for Czech) and a chunk of automatically tagged unsupervised data. The “parameters” of the training process (feature templates, the size of the unsupervised chunks added to the trainer at each iteration, number of iterations, the combination of taggers that should be used in the auto-tagging of the unsupervised chunk, etc.) have be"
E09-1087,P08-1076,0,0.0514596,"a renewed interest in improving POS tagging results for English, and an inflow of results (initial or improved) for many other languages. For English, after a relatively big jump achieved by (Collins, 2002), we have seen two significant improvements: (Toutanova et al., 2003) and (Shen et al., 2007) pushed the results by a significant amount each time.1 1 In our final comparison, we have also included the results of (Gim´enez and M`arquez, 2004), because it has surpassed (Collins, 2002) as well and we have used this tagger in the data preparation phase. See more details below. Most recently, (Suzuki and Isozaki, 2008) published their Semi-supervised sequential labelling method, whose results on POS tagging seem to be optically better than (Shen et al., 2007), but no significance tests were given and the tool is not available for download, i.e. for repeating the results and significance testing. Thus, we compare our results only to the tools listed above. 2 We mean easy to use for further research on problems requiring POS tagging, especially multilingual ones. 3 And much easier to (re)implement as libraries in prototype systems, which is often difficult if not impossible with other people’s code. Proceedin"
E09-1087,gimenez-marquez-2004-svmtool,0,0.456944,"Missing"
E09-1087,N03-1033,0,0.598907,"PTB/WSJ (English) ing the training phase. Apart from feeding the perceptron by various mixtures of manually tagged (“supervised”) and auto-tagged (“unsupervised”)4 data, we have also used various feature templates extensively; for example, we use lexicalization (with the added twist of lemmatization, useful especially for Czech, an inflectionally rich language), “manual” tag classification into large classes (again, useful especially for Czech to avoid the huge, still-to-beovercome data sparseness for such a language5 ), and sub-lexical features mainly targeted at OOV words. Inspired i.a. by (Toutanova et al., 2003) and (Hajiˇc and Vidov´a-Hladk´a, 1998), we also use “lookahead” features (however, we still remain in the left-to-right HMM world – in this respect our solution is closer to the older work of (Hajiˇc and Vidov´a-Hladk´a, 1998) than to (Toutanova et al., 2003), who uses bidirectional dependencies to include the right-hand side disambiguated tags, 2 The perceptron algorithm We have used the Morˇce6 tagger (Votrubec, 2006) as a main component in our experiments. It is a reimplementation of the averaged perceptron described in (Collins, 2002), which uses such features that it behaves like an HMM"
E09-1087,P98-1080,1,0.825023,"Missing"
E09-1087,J01-2002,0,0.0172286,"Missing"
E09-1087,J93-2004,0,\N,Missing
E09-1087,W96-0213,0,\N,Missing
E09-1087,C98-1077,0,\N,Missing
E09-1087,C98-1029,0,\N,Missing
H05-1066,P99-1065,0,0.0348557,"Missing"
H05-1066,J93-2004,0,0.0564691,"ta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), and lexical resource augmentation (Snow et al., 2004). The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications. In English, projective trees are sufficient to analyze most sentence types. In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al., 1993) and is by convention exclusively projective. However, there are certain examples in which a nonprojective tree is preferable. Consider the sentence John saw a dog yesterday which was a Yorkshire Terrier. Here the relative clause which was a Yorkshire Terrier and the object it modifies (the dog) are separated by an adverb. There is no way to draw the dependency tree for this sentence in the plane with no crossing edges, as illustrated in Figure 2. In languages with more flexible word order than English, such as German, Dutch and Czech, non-projective dependencies are more frequent. Rich inflec"
H05-1066,W02-1001,0,0.419449,"he highest score, s(x, y). The resulting online update (to be inserted in Figure 4, line 4) would then be: min w(i+1) − w(i) s.t. s(xt , yt ) − s(xt , y 0 ) ≥ L(yt , y 0 ) where y 0 = arg maxy0 s(xt , y 0 ) McDonald et al. (2005) used a similar update with k constraints for the k highest-scoring trees, and showed that small values of k are sufficient to achieve the best accuracy for these methods. However, here we stay with a single best tree because kbest extensions to the Chu-Liu-Edmonds algorithm are too inefficient (Hou, 1996). This model is related to the averaged perceptron algorithm of Collins (2002). In that algorithm, the single highest scoring tree (or structure) is used to update the weight vector. However, MIRA aggressively updates w to maximize the margin between the correct tree and the highest scoring tree, which has been shown to lead to increased accuracy. 3.2 Factored MIRA It is also possible to exploit the structure of the output space and factor the exponential number of margin constraints into a polynomial number of local constraints (Taskar et al., 2003; Taskar et al., 2004). For the directed maximum spanning tree problem, we can factor the output by edges to obtain the fol"
H05-1066,P05-1012,1,0.17838,".ms.mff.cuni.cz Abstract root We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n 3 ) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2 ) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al., 2003; McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies. John hit the ball with the bat Figure 1: An example dependency tree. Dependency representations, which link words to their arguments, have a long history (Hudson, 1984). Figure 1 shows a dependency tree for the sentence John hit the ball with the bat. We restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or a dummy root symbol as shown in the figure. The tree in Figure 1 is projective, meaning that if we put the words in"
H05-1066,P05-1013,0,0.802738,"es, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser. They test this system on Czech and show improved accuracy relative to a projective parser. Our approach differs from those earlier efforts in searching optimally and efficiently the full space of non-projective trees. The main idea of our method is that dependency parsing can be formalized as the search for a maximum spanning tree in a directed graph. This formalization generalizes standard projective parsing models bas"
H05-1066,P04-1054,0,0.385345,"tree for the sentence John hit the ball with the bat. We restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or a dummy root symbol as shown in the figure. The tree in Figure 1 is projective, meaning that if we put the words in their linear order, preceded by the root, the edges can be drawn above the words without crossings, or, equivalently, a word and its descendants form a contiguous substring of the sentence. 1 Introduction Dependency parsing has seen a surge of interest lately for applications such as relation extraction (Culotta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), and lexical resource augmentation (Snow et al., 2004). The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications. In English, projective trees are sufficient to analyze most sentence types. In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al., 1993) a"
H05-1066,P05-1067,0,0.184843,"at. We restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or a dummy root symbol as shown in the figure. The tree in Figure 1 is projective, meaning that if we put the words in their linear order, preceded by the root, the edges can be drawn above the words without crossings, or, equivalently, a word and its descendants form a contiguous substring of the sentence. 1 Introduction Dependency parsing has seen a surge of interest lately for applications such as relation extraction (Culotta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), and lexical resource augmentation (Snow et al., 2004). The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications. In English, projective trees are sufficient to analyze most sentence types. In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al., 1993) and is by convention exclusively projective. H"
H05-1066,C96-1058,0,0.827257,"w vˇetˇsinou a dog nem´a yesterday ani z´ajem which a taky was na a Yorkshire to vˇetˇsinou Terrier nem´a pen´ıze He is mostly not even interested in the new things and in most cases, he has no money for it either. Figure 2: Non-projective dependency trees in English and Czech. grammatical relations, allowing non-projective dependencies that we need to represent and parse efficiently. A non-projective example from the Czech Prague Dependency Treebank (Hajiˇc et al., 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a pa"
H05-1066,C04-1010,0,0.0300593,"Yorkshire to vˇetˇsinou Terrier nem´a pen´ıze He is mostly not even interested in the new things and in most cases, he has no money for it either. Figure 2: Non-projective dependency trees in English and Czech. grammatical relations, allowing non-projective dependencies that we need to represent and parse efficiently. A non-projective example from the Czech Prague Dependency Treebank (Hajiˇc et al., 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dep"
H05-1066,W04-3201,0,0.00698393,"s algorithm are too inefficient (Hou, 1996). This model is related to the averaged perceptron algorithm of Collins (2002). In that algorithm, the single highest scoring tree (or structure) is used to update the weight vector. However, MIRA aggressively updates w to maximize the margin between the correct tree and the highest scoring tree, which has been shown to lead to increased accuracy. 3.2 Factored MIRA It is also possible to exploit the structure of the output space and factor the exponential number of margin constraints into a polynomial number of local constraints (Taskar et al., 2003; Taskar et al., 2004). For the directed maximum spanning tree problem, we can factor the output by edges to obtain the following constraints: min w(i+1) − w(i) s.t. s(l, j) − s(k, j) ≥ 1 ∀(l, j) ∈ yt , (k, j) ∈ / yt This states that the weight of the correct incoming edge to the word xj and the weight of all other incoming edges must be separated by a margin of 1. It is easy to show that when all these constraints are satisfied, the correct spanning tree and all incorrect spanning trees are separated by a score at least as large as the number of incorrect incoming edges. This is because the scores for all the corr"
H05-1066,W04-0307,0,0.0166533,"non-projective example from the Czech Prague Dependency Treebank (Hajiˇc et al., 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser. They test this system on Czech and show improved accuracy relative to a projective parser. Our approach differs from those earlier efforts in searching optimally and efficiently the full space of non-projective trees. The main idea of ou"
H05-1066,W03-3023,0,0.934387,"z´ajem which a taky was na a Yorkshire to vˇetˇsinou Terrier nem´a pen´ıze He is mostly not even interested in the new things and in most cases, he has no money for it either. Figure 2: Non-projective dependency trees in English and Czech. grammatical relations, allowing non-projective dependencies that we need to represent and parse efficiently. A non-projective example from the Czech Prague Dependency Treebank (Hajiˇc et al., 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-"
hajic-etal-2012-announcing,W04-2705,0,\N,Missing
hajic-etal-2012-announcing,C00-2163,0,\N,Missing
hajic-etal-2012-announcing,P07-1031,0,\N,Missing
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
K18-2001,K18-2015,0,0.053009,"Missing"
K18-2001,Q17-1010,0,0.211935,"Missing"
K18-2001,K18-2010,0,0.0386566,"Missing"
K18-2001,K18-2017,0,0.075361,"Missing"
K18-2001,W06-2920,0,0.453112,"Missing"
K18-2001,K18-2025,0,0.0365994,"Missing"
K18-2001,K18-2005,0,0.120251,"Missing"
K18-2001,K18-2013,1,0.806044,"Missing"
K18-2001,K18-2026,0,0.0321915,"Missing"
K18-2001,K18-2012,0,0.0235436,"above are all intrinsic measures: they evaluate the grammatical analysis task per se, with the hope that better scores correspond to output that is more useful for downstream NLP applications. Nevertheless, such correlations are not automatically granted. We thus seek to complement our task with an extrinsic evaluation, where the output of parsing systems is exploited by applications like biological event extraction, opinion analysis and negation scope resolution. This optional track involves English only. It is organized in collaboration with the EPE initiative;7 for details see Fares et al. (2018). Syntactic Word Alignment The higher segmentation level is based on the notion of syntactic word. Some languages contain multi-word tokens (MWT) that are regarded as contractions of multiple syntactic words. For example, the German token zum is a contraction of the preposition zu “to” and the article dem “the”. Syntactic words constitute independent nodes in dependency trees. As shown by the example, it is not required that the MWT is a pure concatenation of the participating words; the simple token alignment thus does not work when MWTs 4 TIRA: The System Submission Platform Similarly to our"
K18-2001,K18-2003,0,0.040574,"Missing"
K18-2001,K18-2006,0,0.0774162,"Missing"
K18-2001,K18-2014,0,0.0664725,"Missing"
K18-2001,K18-2008,0,0.0697052,"Missing"
K18-2001,L16-1262,1,0.910778,"Missing"
K18-2001,W17-0411,1,0.849881,"and in the system output before comparing them. In the end-to-end evaluation of our task, LAS is re-defined as the harmonic mean (F1 ) of precision P and recall R, where P = #correctRelations #systemNodes (1) R= #correctRelations #goldNodes (2) LAS = 2P R P +R (3) Note that attachment of all nodes including punctuation is evaluated. LAS is computed separately for each of the 82 test files and a macro-average of all these scores is used to rank the systems. 3.2 MLAS: Morphology-Aware Labeled Attachment Score MLAS aims at cross-linguistic comparability of the scores. It is an extension of CLAS (Nivre and Fang, 2017), which was tested experimentally in the 2017 task. CLAS focuses on dependencies between content words and disregards attachment of function words; in MLAS, function words are not ignored, but they are treated as features of content words. In addition, part-of-speech tags and morphological features are evaluated, too. 3.3 BLEX: Bilexical Dependency Score BLEX is similar to MLAS in that it focuses on relations between content words. Instead of morphological features, it incorporates lemmatization in the evaluation. It is thus closer to semantic content and evaluates two aspects of UD annota5 ar"
K18-2001,K18-2022,0,0.0296323,"Missing"
K18-2001,K18-2011,1,0.844373,"Missing"
K18-2001,W17-0412,1,0.901947,"Missing"
K18-2001,L16-1680,1,0.90044,"Missing"
K18-2001,K17-3009,1,0.858784,"Missing"
K18-2001,tiedemann-2012-parallel,0,0.0674866,"at follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers provided with large amounts of freely available data. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. We provided dependency-annotated training and test data, and also large quantities of crawled raw texts. Other language resources are available from third-party servers and we only referred to the respective download sites. 2.1 Training Data: UD 2.2 Training and development data came from the Universal Dependencies (UD) 2.2 collection (Nivre et al., 2018). This year, the official UD release immediately followed the test phase of the shared task. The training and development data were available to the"
K18-2001,K18-2016,0,0.0988933,"Missing"
K18-2001,K18-2019,0,0.110064,"Missing"
K18-2001,K18-2007,0,0.0602044,"Missing"
K18-2001,K18-2004,0,0.103154,"Missing"
L16-1251,piperidis-etal-2014-meta,1,0.621298,"itectures 1. Introduction A truly multilingual Europe, which is supported through sophisticated Language Technologies (LT) is still far from being a reality. Since its inception in 2010, it has been one of the key goals of META-NET to foster and stimulate research and technology development towards this scenario. Important milestones along the way were the publication of the META-NET White Papers (Rehm and Uszkoreit, 2012; Rehm et al., 2014) and the Strategic Research Agenda for Multilingual Europe 2020 (SRA) (Rehm and Uszkoreit, 2013) as well as the deployment of META-SHARE (Piperidis, 2012; Piperidis et al., 2014). While all these activities did have a certain amount of impact in various European countries (Rehm et al., 2016b), new challenges and new opportunities have been emerging in the last two years. In this paper we1 provide an overview of the most recent developments around META-NET and the topic of multilingual Europe (Section 2.). We describe two new emerging initiatives that are becoming increasingly important for the community (Section 3.). The main current challenges and opportunities are sketched in Section 4. while Section 5. concludes with several suggested next steps. 2. Recent Developm"
L16-1251,piperidis-2012-meta,0,0.0809183,"ructures and Architectures 1. Introduction A truly multilingual Europe, which is supported through sophisticated Language Technologies (LT) is still far from being a reality. Since its inception in 2010, it has been one of the key goals of META-NET to foster and stimulate research and technology development towards this scenario. Important milestones along the way were the publication of the META-NET White Papers (Rehm and Uszkoreit, 2012; Rehm et al., 2014) and the Strategic Research Agenda for Multilingual Europe 2020 (SRA) (Rehm and Uszkoreit, 2013) as well as the deployment of META-SHARE (Piperidis, 2012; Piperidis et al., 2014). While all these activities did have a certain amount of impact in various European countries (Rehm et al., 2016b), new challenges and new opportunities have been emerging in the last two years. In this paper we1 provide an overview of the most recent developments around META-NET and the topic of multilingual Europe (Section 2.). We describe two new emerging initiatives that are becoming increasingly important for the community (Section 3.). The main current challenges and opportunities are sketched in Section 4. while Section 5. concludes with several suggested next"
L16-1251,W15-4941,1,0.744968,"lishment of a Memorandum of Understanding among the 12 organisations is foreseen with the goal of forming a “Coalition for a Multilingual Europe”. 5 http://rigasummit2015.eu The Riga Summit 2015 was jointly organised by METANET (through the project CRACKER), LT-Innovate (through the project LT_Observatory), Tilde and the European Commission. 7 BDVA, CITIA, CLARIN, EFNIL, ELEN, ELRA, GALA, LTInnovate, META-NET, NPLD, TAUS, W3C. 6 2.5. The Strategic Agenda for the Multilingual Digital Single Market Building upon past activities, in particular the METANET SRA (2013), the two EU-projects CRACKER (Rehm, 2015) and LT_Observatory prepared the Strategic Agenda for the Multilingual Digital Single Market (SRIA, 2015). Here, we can only provide a brief summary: the setup of the large and ambitious strategic programme towards the MDSM consists of three layers. On the top layer we have a set of focused Technology Solutions for Businesses and Public Services. These innovative application scenarios and solutions are supported, enabled, and driven by the middle layer which consists of a small group of Services, Infrastructures and Platforms that provide, through standardised interfaces, data exchange formats"
L16-1262,W13-2308,0,0.0114723,"g diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Span"
L16-1262,W06-2920,0,0.443151,"clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies in the enhanced representation and miscellaneous information. The format is illustrated in Figure 3, with the French sentence from Figure 2. To support work on treebanks in this format,"
L16-1262,W09-2307,1,0.329071,"standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Ja"
L16-1262,P11-1061,1,0.573621,"UNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapt"
L16-1262,W08-1301,1,0.58058,"Missing"
L16-1262,de-marneffe-etal-2006-generating,1,0.213978,"Missing"
L16-1262,de-marneffe-etal-2014-universal,1,0.831309,"Missing"
L16-1262,E14-4028,0,0.0377832,"Missing"
L16-1262,N15-3011,1,0.696846,"Missing"
L16-1262,D07-1013,1,0.230402,"hocolat . le fille adorer le dessert a` le chocolat . DET NOUN VERB DET NOUN ADP DET NOUN PUNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged"
L16-1262,P13-2017,1,0.877648,"Missing"
L16-1262,W15-2127,0,0.0113361,"n English, we also obtain parallel representations between prepositional phrases and subordinate clauses, which are in practice often introduced by a preposition, as in (5). nmod case nsubj (5) a. Sue 1662 det left after the rehearsal advcl nsubj b. Sue Language mark nsubj left after we did The choice to make content words the backbone of the syntactic representations may seem to be at odds with the strong tendency in modern syntactic theory to give priority to functional heads, a tendency that is found in both constituency-based and dependency-based approaches to syntax (Brug´e et al., 2012; Osborne and Maxwell, 2015). We believe, however, that this conflict is more apparent than real. The UD view is that we need to recognize both lexical and functional heads, but in order to maximize parallelism across languages, only lexical heads are inferable from the topology of our tree structures. Functional heads are instead represented as specifying features of content words, using dedicated relation labels, features which can alternatively be specified through morphological processes. In the dependency grammar tradition, this is very close to the view of Tesni`ere (1959), according to whom dependencies hold betwe"
L16-1262,petrov-etal-2012-universal,1,0.717175,"exist to build consistent resources for many languages, and the UD project is a merger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we p"
L16-1262,rosa-etal-2014-hamledt,1,0.832586,"Missing"
L16-1262,L16-1376,1,0.208211,"fferent languages. For instance, while the universal UD scheme has a single relation acl for adnominal clauses, several languages make use of the subtype acl:relcl to distinguish relative clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies i"
L16-1262,E12-2021,1,0.581828,"Missing"
L16-1262,stepanek-pajas-2010-querying,0,0.067769,"Missing"
L16-1262,P13-2103,1,0.625022,"hese resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Spanish and Swedish). The first proposal for incorporating morphology was made by Tsarfaty (2013). The second version of HamleDT (Rosa et al., 2014) provided Stanford/Google annotation for 30 languages by automatically harmonizing treebanks with different native annotations. These efforts were followed by the development of the universal Stanford dependencies (USD), revising Stanford Dependencies for cross-linguistic annotations in light of the Google scheme (de Marneffe et al., 2014). UD is the result of merging all these initiatives into a single coherent framework, based on the universal Stanford dependencies, an extended version of the Google universal tag set, a revised subset of the"
L16-1262,I08-3008,1,0.20904,"the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-"
L16-1262,zeman-etal-2012-hamledt,1,0.729155,"Missing"
L16-1262,zeman-2008-reusable,1,0.897534,"erger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we present version 1 of the universal guidelines and explain the underlying d"
L16-1483,agerri-etal-2014-ixa,0,0.0139978,"us into a pivot language, English, and this into the remaining six languages. The current annotated corpus covers the first 2,000 sentences of the QTLeap corpus, which have been used to train the MT systems in the project. 3. Annotation Tools In this section, we describe the NERC, NED, WSD and coreference tools used to annotate the corpora. We have chosen the tools based on their performance and their ease of use. We also describe the annotation formats. 3.1. Named-entity recognition and classification Basque, English and Spanish ixa-pipe-nerc is a multilingual NERC tagger, part of IXA pipes (Agerri et al., 2014). Every model has been trained with the averaged Perceptron algorithm as described in Collins (2002) and as implemented in Apache OpenNLP. The datasets used for training the models are the following: Egunkaria dataset for Basque, a combination of Ontonotes 4.0, CoNLL 2003 and MUC 7 for English, and CoNLL 2002 for Spanish. Bulgarian The Bulgarian NERC is a rule-based module. It uses a gazetteer with names categorized in four types: Person, Location, Organization, Other. The identification of new names is based on two factors – sure positions in the text and classifying contextual information, s"
L16-1483,E09-1005,1,0.857409,"ese extraction of DBpedia Spotlight on a local server, then takes an input text pre-processed with lemmas, Part of Speech tags and named entities using the LX-Suite (Branco and Silva, 2006) and converts it to the ’spotted’ format understood by Spotlight. This spotted input text is then disambiguated using DBpedia Spotlight, returning among other information links to existing Portuguese DBpedia resource pages for each named entity discovered. 3024 3.3. Word-sense disambiguation Basque, English and Spanish ixa-pipe-wsd-ukb is based on UKB, a collection of programs for performing graphbased WSD (Agirre and Soroa, 2009). It applies the socalled Personalized PageRank on a Lexical Knowledge Base (LKB) to rank the vertices of the LKB and thus perform disambiguation. WordNet 3.0 is the LKB used for this processing. Bulgarian The basic version of Bulgarian WSD is implemented on the assumption of one sense per discourse and bigram statistics. Czech Two different approaches were used for Czech WSD. The first approach based on the work of Duˇsek et al. (2015) focuses on verbal WSD. The second approach followed for the annotation is a straightforward way of achieving compatibility with English WordNet IDs. Since the"
L16-1483,barreto-etal-2006-open,1,0.822658,"Missing"
L16-1483,bojar-etal-2012-joy,1,0.897675,"Missing"
L16-1483,E06-2024,1,0.746535,"It offers the “disambiguate” and “candidates” service endpoints. The former takes the spotted text input and it returns the DBpedia resource page for each entity. The later is similar to disambiguate, but returns a ranked list of candidates. Portuguese The NED module for Portuguese, LX-NED, uses DBpedia Spotlight to find links to resources about entities identified in pre-processed input text. It creates a process to run a Portuguese extraction of DBpedia Spotlight on a local server, then takes an input text pre-processed with lemmas, Part of Speech tags and named entities using the LX-Suite (Branco and Silva, 2006) and converts it to the ’spotted’ format understood by Spotlight. This spotted input text is then disambiguated using DBpedia Spotlight, returning among other information links to existing Portuguese DBpedia resource pages for each named entity discovered. 3024 3.3. Word-sense disambiguation Basque, English and Spanish ixa-pipe-wsd-ukb is based on UKB, a collection of programs for performing graphbased WSD (Agirre and Soroa, 2009). It applies the socalled Personalized PageRank on a Lexical Knowledge Base (LKB) to rank the vertices of the LKB and thus perform disambiguation. WordNet 3.0 is the"
L16-1483,W02-2004,0,0.181331,"Missing"
L16-1483,W02-1001,0,0.104764,"covers the first 2,000 sentences of the QTLeap corpus, which have been used to train the MT systems in the project. 3. Annotation Tools In this section, we describe the NERC, NED, WSD and coreference tools used to annotate the corpora. We have chosen the tools based on their performance and their ease of use. We also describe the annotation formats. 3.1. Named-entity recognition and classification Basque, English and Spanish ixa-pipe-nerc is a multilingual NERC tagger, part of IXA pipes (Agerri et al., 2014). Every model has been trained with the averaged Perceptron algorithm as described in Collins (2002) and as implemented in Apache OpenNLP. The datasets used for training the models are the following: Egunkaria dataset for Basque, a combination of Ontonotes 4.0, CoNLL 2003 and MUC 7 for English, and CoNLL 2002 for Spanish. Bulgarian The Bulgarian NERC is a rule-based module. It uses a gazetteer with names categorized in four types: Person, Location, Organization, Other. The identification of new names is based on two factors – sure positions in the text and classifying contextual information, such as, titles for persons, types of geographical objects or organizations, etc. The disambiguation"
L16-1483,W15-2111,1,0.864612,"Missing"
L16-1483,hajic-etal-2012-announcing,1,0.885975,"Missing"
L16-1483,W03-1901,0,0.0627303,"n to comparing the cores and the morphological information (gender and number) of the two expressions. As such, we found it easier to directly implement equivalent tests in-code instead of having to feed the extracted features to the Weka J48 classifier proper. 3.5. Annotation formats Basque, Bulgarian, Czech, English and Spanish These corpora are annotated in the NAF format. The NAF format (Fokkens et al., 2014) is a linguistic annotation format designed for complex NLP pipelines that combines strengths of the Linguistic Annotation Framework (LAF) and the NLP Interchange Formats described by Ide and Romary (2003). Because of its layered extensible format, it can easily be incorporated in a variety of NLP modules that may require different linguistic information as their input. Portuguese The corpus for Portuguese is divided into 4 text files - the raw corpus, and one file for the output of each of the three tools used to process it (WSD, NED and coreference). For each of the three tools output is provided in a standoff annotation format, consisting of one token per line (ID of each token in a markable pair in the case of the coreference tool), the appropriate output element of the respective tools (wo"
L16-1483,2005.mtsummit-papers.11,0,0.506663,"ces, the less language-specific differences will remain between the representations of the meaning of the source and target texts. As a result, chances of success are expected to increase considerably by MT systems that are based on deeper semantic engineering approaches. Following this assumption, one of the approaches taken by the QTLeap project1 is to enrich MT training resources with lexico-semantic information. In this work, we present a solid effort to build multilingual parallel corpora annotated at multiple semantic levels. Our overall goal is to enrich two parallel corpora, Europarl (Koehn, 2005) and the QTLeap corpus (Agirre et al., 2015b), with token, lemma, part-of-speech (POS), namedentity recognition and classification (NERC), named-entity disambiguation (NED), word-sense disambiguation (WSD) and coreference for six languages covered in the QTLeap project, namely, Basque (EU), Bulgarian (BG), Czech (CS), English (EN), Portuguese (PT) and Spanish (ES). Specifically, this paper presents the first release of such corpora, which includes NERC, NED, WSD and coreferencelevel annotation for these six languages. Additionally, some languages have extra annotations, such as wikification (E"
L16-1483,W11-1902,0,0.0321161,"ce in Czech. English and Spanish ixa-pipe-coref is loosely based on the Stanford Multi Sieve Pass system (Lee et al., 2013). The system consists of a number of rule-based sieves. Each sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favors a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al., 2013; Lee et al., 2011), in which the Stanford system obtained the best results. The results show a pattern which has also been shown in other results reported with other evaluation sets (Raghunathan et al., 2010), namely, the fact that a large part of the performance of the multi-pass sieve system is based on a set of significant sieves. Thus, this module so far focuses on a subset of sieves only, namely, Speaker Match, Exact Match, Precise Constructs, Strict Head Match and Pronoun Match (Lee et al., 2013). Portuguese For the Portuguese coreference tool, a decision tree classifier was experimented with. Given a pai"
L16-1483,J13-4004,0,0.0359801,"owed for the annotation is a straightforward way of achieving compatibility with English WordNet IDs. Since the Czech corpus contains the same sentences as the English corpus, the English WordNet ID annotation from this corpus is projected onto Czech words using GIZA++ word alignment. Portuguese The Portuguese WSD tool, LX-WSD, is also based on UKB. The LKB from which UKB returns word senses within the pipeline has been generated from an extraction of the Portuguese MultiWordNet6 . 3.4. Coreference Basque ixa-pipe-coref-eu is an adaptation of the Stanford Deterministic Coreference Resolution (Lee et al., 2013), which gives state-of-the art performance for English. The original system applies a succession of ten independent deterministic coreference models or sieves. During the adaptation process, firstly, a baseline system has been created which receives as input texts processed by Basque analysis tools and uses specifically adapted static lists to identify language dependent features like gender, animacy or number. Afterwards, improvements over the baseline system have been applied, adapting and replacing some of the original sieves (Soraluze et al., 2015), taking into account that morphosyntactic"
L16-1483,S07-1008,0,0.017093,"Missing"
L16-1483,S07-1006,0,0.117234,"Missing"
L16-1483,W11-1901,0,0.087359,"Missing"
L16-1483,D10-1048,0,0.0133306,"h sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favors a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al., 2013; Lee et al., 2011), in which the Stanford system obtained the best results. The results show a pattern which has also been shown in other results reported with other evaluation sets (Raghunathan et al., 2010), namely, the fact that a large part of the performance of the multi-pass sieve system is based on a set of significant sieves. Thus, this module so far focuses on a subset of sieves only, namely, Speaker Match, Exact Match, Precise Constructs, Strict Head Match and Pronoun Match (Lee et al., 2013). Portuguese For the Portuguese coreference tool, a decision tree classifier was experimented with. Given a pair of expressions, the classifier returns a true or false value that indicates whether those expressions are coreferent. The classifier was trained over the Summit Corpus (Collovini et al., 2"
L16-1483,P14-5003,1,0.902631,"Missing"
L16-1483,tiedemann-2012-parallel,0,0.0371283,"with corresponding document IDs. Then, sentence boundaries were identified and aligned (for further collection and processing information, see Koehn (2005)). The Europarl corpus consists of monolingual data as well as bilingual parallel data with English as pivot language. In our effort, we have annotated the BG, CS, ES and PT parts of the corpus separately while the EN side of the ESEN language pair was used as pivot language to link all six languages. Given that Europarl does not include Basque, we annotated an alternative publicly available Basque-English parallel corpus, the GNOME corpus (Tiedemann, 2012), which includes GNOME localization files. 2.2. QTLeap corpus The QTLeap corpus consists of 4,000 pairs of questions and respective answers in the domain of IT troubleshooting http://hdl.handle.net/11234/1-1477 4 http://www.statmt.org/europarl/ 3023 for both hardware and software, distributed in four 1,000pair batches (Gaudio et al., 2016). This material was collected using a real-life, commercial online support service via chat. The QTLeap corpus is a unique resource in that it is a multilingual data set with parallel utterances in different languages (Basque, Bulgarian, Czech, Dutch, English"
L16-1483,E14-4045,0,0.0404676,"Missing"
L16-1483,M95-1005,0,0.280516,"Missing"
L16-1630,D11-1031,0,0.0192561,"linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where subst"
L16-1630,W13-2322,0,0.201241,"ns as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer t"
L16-1630,W08-2222,0,0.0201912,"and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collec"
L16-1630,Q15-1040,1,0.86691,"banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was perf"
L16-1630,J93-2004,0,0.0550061,"ies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was performed). CCD: Combinatory Categorial Grammar Dependencies Hockenmaier & Steedman (2007) construct CCGbank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “includ"
L16-1630,S14-2082,0,0.0694129,"information. We envision that general availability of a standardized and comprehensive set of semantic dependency graphs and associated tools will stimulate more research in this sub-area of semantic parsing. To date, reported ‘parsing success’ 7 To seek to relate these different approaches to the encoding of lexical valency, one can multiply out the DM frame identifiers with verb lemmata, which yields a count of some 4,600 distinct combinations, i.e. slightly less than the set of observed sense distinctions in PSD. measures in terms of dependency F1 range between the high seventies for PSD (Martins & Almeida, 2014) and high eighties to low nineties for CCD, DM, and PAS (Du et al., 2015; Miyao et al., 2014). Such variation may in principle be owed to differences in the number and complexity of linguistic distinctions made, to homogeneity and consistency of training and test data, and of course to the cumulative effort that has gone into pushing the state of the art on individual target representations. A deeper understanding of these parameters, as well as of contentful vs. superficial linguistic differences across frameworks, will be a prerequisite to judging the relative suitability of different resour"
L16-1630,J07-4004,0,0.0382917,"bank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “include most semantically relevant non-anaphoric local and longrange dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. While these have mainly been used for contrastive parser evaluation (Clark & Curran, 2007; Fowler & Penn, 2010; inter alios), recent parsing work as mentioned above views each set of triples as a directed graph and parses directly into these target representations. Our CCD graphs combine the CCGbank dependency triples with information gleaned from the CCG syntactic derivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannot"
L16-1630,copestake-flickinger-2000-open,1,0.723309,"al trees), and with corresponding ‘companion’ syntactic analyses from a broad variety of frameworks and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic propert"
L16-1630,S14-2056,1,0.955729,"s). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was performed). CCD: Combinatory Categorial Grammar Dependencies Hockenmaier & Steedman (2007) construct CCGbank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Jo"
L16-1630,P15-1149,0,0.063476,"s of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional"
L16-1630,flickinger-etal-2014-towards,1,0.859802,"ing ‘companion’ syntactic analyses from a broad variety of frameworks and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks a"
L16-1630,P10-1035,0,0.0195635,"n of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “include most semantically relevant non-anaphoric local and longrange dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. While these have mainly been used for contrastive parser evaluation (Clark & Curran, 2007; Fowler & Penn, 2010; inter alios), recent parsing work as mentioned above views each set of triples as a directed graph and parses directly into these target representations. Our CCD graphs combine the CCGbank dependency triples with information gleaned from the CCG syntactic derivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBan"
L16-1630,hajic-etal-2012-announcing,1,0.917423,"Missing"
L16-1630,S15-2153,1,0.925074,"Missing"
L16-1630,S14-2008,1,0.928327,"Missing"
L16-1630,oepen-lonning-2006-discriminant,1,0.921801,"rivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBank2 , of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses from the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). Native ERG semantics take the form of underspecified logical forms, which Oepen et al. (2002); Oepen & Lønning (2006); and Ivanova et al. (2012) map onto the DM bi-lexical semantic dependencies in a twostep conversion pipeline.3 For this target representation, top nodes designate the highest-scoping (non-quantificational) predicate in the graph, e.g. the scopal adverb almost in Figure 1 below. PAS: Enju Predicate–Argument Structures The Enju Treebank4 is derived from automatic HPSG-style reannotation of the PTB (Miyao, 2006). Our PAS graphs stem from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; Xue et al., 2005). To"
L16-1630,J07-3004,0,0.0404021,"hon tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Tree"
L16-1630,W12-3602,1,0.888177,"of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBank2 , of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses from the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). Native ERG semantics take the form of underspecified logical forms, which Oepen et al. (2002); Oepen & Lønning (2006); and Ivanova et al. (2012) map onto the DM bi-lexical semantic dependencies in a twostep conversion pipeline.3 For this target representation, top nodes designate the highest-scoping (non-quantificational) predicate in the graph, e.g. the scopal adverb almost in Figure 1 below. PAS: Enju Predicate–Argument Structures The Enju Treebank4 is derived from automatic HPSG-style reannotation of the PTB (Miyao, 2006). Our PAS graphs stem from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; Xue et al., 2005). Top nodes in this representat"
L16-1680,Q16-1031,0,0.0288813,"1.3 87.8 85.0 81.3 87.2 84.7 85.7 86.2 68.4 56.4 74.1 63.2 83.8 80.2 89.5 88.1 83.3 79.7 87.1 84.5 83.7 84.3 81.2 77.0 86.2 83.2 83.5 84.5 64.8 56.3 78.1 71.5 Table 3: Parsing accuracy on all treebanks of Universal Dependencies version 1.2. We characterize each treebank by its size and level of non-projectivity and present our parsing results in terms of unlabeled attachment score UAS and labeled attachment score LAS. We show the results separately for the case when the POS tags are automatically recognized, and when the gold POS tags are used. For comparison, we also show parsing results of (Ammar et al., 2016), both in the monolingual setting and in multilingual setting. Best results in each category (UAS/LAS) are shown in bold font. 4295 We compare our parser results to (Ammar et al., 2016), which present two parsers trained on 7 treebanks from Universal Dependencies 1.2. The first parser is monolingual (it is trained on the training data of the target language only), the second one is trained on all 7 treebanks. Although both these parsers utilize additional raw corpora during training (by employing pretrained word embeddings and Brown clusters embeddings) and use more advanced Stack-LSTM model ("
L16-1680,D14-1082,0,0.0536646,"gging and lemmatization accuracy is presented also in Table 2. We do not compare our tagger to any related work, because we could not find any part-ofspeech tagging and lemmatization results on Universal Dependencies 1.2. However, the used POS tagger algorithm and the used feature set achieve state-of-the-art results on Czech and very good results on English (Spoustov´a et al., 2009). 5. Dependency Parsing Parsito is a transition-based, non-projective dependency parser described in (Straka et al., 2015), capable of parsing both projective and non-projective sentences. The parser, inspired by (Chen and Manning, 2014), uses a neural network classifier for prediction and requires no feature engineering. In (Straka et al., 2015) a new search-based oracle is proposed, which improves parsing accuracy similarly to a dynamic oracle, but is applicable to any transition system, such as the fully non-projective swap system. The parser has excellent parsing speed, compact models, and achieves high accuracy. UDPipe employs the Parsito parser nearly unmodified. Compared to (Straka et al., 2015), only an optional beamsearch decoding along the lines of (Zhang and Nivre, 2011) was added. The beam-search decoding improves"
L16-1680,W14-4012,0,0.00636492,"Missing"
L16-1680,W02-1001,0,0.0400739,"ological Analysis and POS Tagging There are several morphological fields used in the CoNLLU format: • universal part-of-speech tag (Petrov et al., 2012), • list of morphological features (Zeman, 2008), • language-specific part-of-speech tag, • lemma or stem. UDPipe is able to fill all these fields, depending on which of them are available in the training data. To perform the POS tagging and lemmatization, UDPipe uses MorphoDiTa (Strakov´a et al., 2014). The MorphoDiTa POS tagger is based on part of (Spoustov´a et al., 2009) and is implemented as a supervised, rich feature averaged perceptron (Collins, 2002), employing dynamic programming at runtime (Viterbi decoder). 4.1. Morphological Analysis In order to use the averaged perceptron tagger, morphological analyses for every input form must be provided. Performing such analysis for a language with small tag set may 7 http://hdl.handle.net/11858/ 00-097C-0000-0022-6133-9 8 Surprisingly, we obtained best tokenization results by using only the first 500kB of every raw corpus. 4292 be trivial (by considering all possible tags), but if the tag set is richer (which is the case of many languages using morphological features in CoNLL-U) or if lemmatizati"
L16-1680,de-marneffe-etal-2014-universal,0,0.0271181,"Missing"
L16-1680,P15-1033,0,0.0169467,", both in the monolingual setting and in multilingual setting. Best results in each category (UAS/LAS) are shown in bold font. 4295 We compare our parser results to (Ammar et al., 2016), which present two parsers trained on 7 treebanks from Universal Dependencies 1.2. The first parser is monolingual (it is trained on the training data of the target language only), the second one is trained on all 7 treebanks. Although both these parsers utilize additional raw corpora during training (by employing pretrained word embeddings and Brown clusters embeddings) and use more advanced Stack-LSTM model (Dyer et al., 2015), UDPipe parsing results are quite competitive, as presented in Table 3. Note that for some languages there is a noticeable drop in parsing performance when automatically generated POS tags are used instead of gold POS tags. Since we train the tagger and the parser using the same input data, we were concerned whether it is proper to train the parser using generated POS tags that were trained on the same data (because these POS tags are nearly identical to the gold tags, unlike POS tags generated on development and testing portion of the data). However, when the parser is trained using POS tags"
L16-1680,D15-1176,0,0.0486689,"Missing"
L16-1680,L16-1262,1,0.86,"Missing"
L16-1680,petrov-etal-2012-universal,0,0.0373543,"the pipeline is easily trainable with training data in CoNLL-U format (and in some cases also with additional raw corpora) and requires minimal linguistic knowledge on the users’ part. The training code is also released. Keywords: Universal Dependencies; dependency parsing; part-of-speech tagging 1. Introduction The Universal Dependencies project (Nivre et al., 2016) seeks to develop cross-linguistically consistent treebank annotation for many languages. The annotation scheme is based on the universal Stanford dependencies (de Marneffe et al., 2014), the Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic features (Zeman, 2008). The latest version of UD (Univeral Dependencies Treebanks version 1.2, 2015)1 consists of 37 dependency treebanks. Our aim is to create a simple-to-use, one-binary and a single-model tool (per language), to easily process raw text to CoNLL-U-formatted tagged and/or parsed dependency trees (with morphological features if available in UD). In particular, our goals are • state-of-the-art tools for tokenization, morphological analysis, part-of-speech tagging and dependency parsing, • free C++ tools available under the Mozil"
L16-1680,E09-1087,1,0.497318,"Missing"
L16-1680,P14-5003,1,0.77998,"Missing"
L16-1680,zeman-2008-reusable,0,0.0472733,"me cases also with additional raw corpora) and requires minimal linguistic knowledge on the users’ part. The training code is also released. Keywords: Universal Dependencies; dependency parsing; part-of-speech tagging 1. Introduction The Universal Dependencies project (Nivre et al., 2016) seeks to develop cross-linguistically consistent treebank annotation for many languages. The annotation scheme is based on the universal Stanford dependencies (de Marneffe et al., 2014), the Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic features (Zeman, 2008). The latest version of UD (Univeral Dependencies Treebanks version 1.2, 2015)1 consists of 37 dependency treebanks. Our aim is to create a simple-to-use, one-binary and a single-model tool (per language), to easily process raw text to CoNLL-U-formatted tagged and/or parsed dependency trees (with morphological features if available in UD). In particular, our goals are • state-of-the-art tools for tokenization, morphological analysis, part-of-speech tagging and dependency parsing, • free C++ tools available under the Mozilla Public License (MPL) 2.0 license (code) and CC BY-NC-SA 4.0 license (m"
L16-1680,P11-2033,0,0.0173287,"jective sentences. The parser, inspired by (Chen and Manning, 2014), uses a neural network classifier for prediction and requires no feature engineering. In (Straka et al., 2015) a new search-based oracle is proposed, which improves parsing accuracy similarly to a dynamic oracle, but is applicable to any transition system, such as the fully non-projective swap system. The parser has excellent parsing speed, compact models, and achieves high accuracy. UDPipe employs the Parsito parser nearly unmodified. Compared to (Straka et al., 2015), only an optional beamsearch decoding along the lines of (Zhang and Nivre, 2011) was added. The beam-search decoding improves accuracy, but decreases runtime performance. By default, beam search of size 5 is used. 5.1. Parsing Results We trained the parser as described in (Straka et al., 2015), choosing the hyperparameters, transition system and an oracle which maximize the parser performance on the development portion of the data. We report both unlabelled (UAS) and labelled attachment score (LAS) of the parser in Table 3. We consider both the situations when the part-of-speech tags are automatically generated and when the gold tags are used, because while most users wil"
L18-1136,P85-1033,0,0.754181,"customized interface for browsing and searching will follow and is part of future work (Sec. 5.). The editor as well as the associated data will be publicly available under a CC license. 2. Related Work When designing the “CzEngClass” lexicon as well as the editor, we were looking for an existing annotation tool for a similar type of lexicon(s) we could possibly adapt. We concentrated on those that allow working with corpora, since that is also the way we approach building the lexicon. Lexicons have been built using software tools (and corpora) since the 1980s, mainly at publishers, such as (Ahlswede, 1985); such efforts are summarized in (Teubert, 2007). We have considered many other existing tools, either standalone or available as web services and applications. Lexicon Creator is a tool designed to help developers produce lexical data for its use in a variety of linguistic applications. According to (Fontenelle et al., 2008), Lexicon Creator enables to work on existing wordlists derived either directly from corpora or from previously created wordlist data. Lexicon Builder, available as web service (Parai et al., 2010), aims at automated methods to compile custom lexicons from BioPortal ontolo"
L18-1136,choi-etal-2010-propbank-instance,0,0.0965447,"n of different processes on different parts of the screen. A database schema for developing and maintaining Japanese linguistic resources (Asahara et al., 2002) is a stand-off framework combining XML and a relational database. SIL’s 3 latest version (8.3) of FLEx (FieldWorks Language Explorer) is a next specific program designed to assist linguists in collecting, managing and publishing linguistic data.4 FLEx features powerful bulk editing tools and a large number of built-in fields. For the resources we are linking CzEngClass to, there are also several tools. A specific editor - Cornerstone (Choi et al., 2010a) - has been specifically customized to create and edit frameset files for PropBank project.5 One of the biggest advantages of Cornerstone is that it accommodates several languages (it was used for e.g., Arabic, Chinese, English, Hindi, and Korean). A semi-automatic VerbaLex–FrameNet linking tool (Materna, 2009; Materna and Pala, 2010; Materna, 2011; Materna, 2014) has been developed. This tool aims to build a core of Czech FrameNet. All the above mentioned editors and tools are very sophisticated and useful, but rather specialized for the particular lexical resource. Those few exceptions, su"
L18-1136,cinkova-2006-propbank,0,0.3226,"itute of Linguistics, Inc. 4 https://software.sil.org/fieldworks 5 For proper PropBank annotation another editor called Jubilee (Choi et al., 2010b) has been used. their professional translation into Czech6 . The PCEDT annotations capture linkage of the surface and deep syntactic layers; moreover, the deep layer contains verbal word sense labeling by keeping links of each verb occurrence to the appropriate valency frame in the associated valency lexicons, PDT-Vallex and EngVallex. The main lexicon resources we build on are thus the following valency lexicons: EngVallex (Cinková et al., 2014),(Cinková, 2006), PDT-Vallex (Urešová et al., 2014), (Urešová, 2011) and also CzEngVallex (Urešová et al., 2015), (Urešová et al., 2016). These lexicons are based on the Functional Generative Description Valency Theory (FGDVT)7 . In the lexicons, each entry has a headword with one or more valency frame(s). Every valency frame contains labeled arguments, their obligatoriness and the required surface form of valency frame members (arguments). PDT-Vallex contains about 12,000 valency frames for about 7,000 verbs; EngVallex, contains about 7,000 valency frames for almost 4,500 verbs. CzEngVallex links them across"
L18-1136,J90-3002,0,0.698382,"tom lexicons from BioPortal ontologies. CoBaLT (Kenter et al., 2012), is a web-based editor optimized for work with large 1 The final size should cover at least the current Czech and English valency lexicons, i.e. about 15,000 verbs (verb senses); it is very hard to estimate the number of classes created in the end. 850 datasets and to produce historical lexica. Dicet (Gader et al., 2012) is a knowledge-based, tailor-made lexical graph editor and browser that allows lexicographers to browse through the lexical network and directly expand and revise it. DECFC,2 a specialized dictionary editor (Decary and Lapalme, 1990), provides a multi-windowing environment that enables the simultaneous execution of different processes on different parts of the screen. A database schema for developing and maintaining Japanese linguistic resources (Asahara et al., 2002) is a stand-off framework combining XML and a relational database. SIL’s 3 latest version (8.3) of FLEx (FieldWorks Language Explorer) is a next specific program designed to assist linguists in collecting, managing and publishing linguistic data.4 FLEx features powerful bulk editing tools and a large number of built-in fields. For the resources we are linking"
L18-1136,W12-5109,0,0.0319816,"reator enables to work on existing wordlists derived either directly from corpora or from previously created wordlist data. Lexicon Builder, available as web service (Parai et al., 2010), aims at automated methods to compile custom lexicons from BioPortal ontologies. CoBaLT (Kenter et al., 2012), is a web-based editor optimized for work with large 1 The final size should cover at least the current Czech and English valency lexicons, i.e. about 15,000 verbs (verb senses); it is very hard to estimate the number of classes created in the end. 850 datasets and to produce historical lexica. Dicet (Gader et al., 2012) is a knowledge-based, tailor-made lexical graph editor and browser that allows lexicographers to browse through the lexical network and directly expand and revise it. DECFC,2 a specialized dictionary editor (Decary and Lapalme, 1990), provides a multi-windowing environment that enables the simultaneous execution of different processes on different parts of the screen. A database schema for developing and maintaining Japanese linguistic resources (Asahara et al., 2002) is a stand-off framework combining XML and a relational database. SIL’s 3 latest version (8.3) of FLEx (FieldWorks Language Ex"
L18-1136,J93-2004,0,0.0628815,"accessible and richly annotated data, namely, on the Prague Czech-English parallel treebank and on the Prague Dependency Treebank (Hajiˇc et al., 2006) valency lexicons (PDT-Vallex, EngVallex and CzEngVallex), as well as on other well-established lexical databases (e.g., FrameNet, VerbNet, Semlink, PropBank and Czech and English WordNets). The core corpus resource is the Prague Czech-English Dependency Treebank (Hajiˇc et al., 2012), which stores parallel PDT-style annotations (manual annotation of morphology, syntax and semantics) of English texts (Wall Street Journal part of Penn Treebank (Marcus et al., 1993)) and 2 DECFC (Explanatory and Combinatory Dictionary of Contemporary French). 3 Originally known as the Summer Institute of Linguistics, Inc. 4 https://software.sil.org/fieldworks 5 For proper PropBank annotation another editor called Jubilee (Choi et al., 2010b) has been used. their professional translation into Czech6 . The PCEDT annotations capture linkage of the surface and deep syntactic layers; moreover, the deep layer contains verbal word sense labeling by keeping links of each verb occurrence to the appropriate valency frame in the associated valency lexicons, PDT-Vallex and EngVallex"
L18-1136,materna-pala-2010-using,0,0.0253217,"o assist linguists in collecting, managing and publishing linguistic data.4 FLEx features powerful bulk editing tools and a large number of built-in fields. For the resources we are linking CzEngClass to, there are also several tools. A specific editor - Cornerstone (Choi et al., 2010a) - has been specifically customized to create and edit frameset files for PropBank project.5 One of the biggest advantages of Cornerstone is that it accommodates several languages (it was used for e.g., Arabic, Chinese, English, Hindi, and Korean). A semi-automatic VerbaLex–FrameNet linking tool (Materna, 2009; Materna and Pala, 2010; Materna, 2011; Materna, 2014) has been developed. This tool aims to build a core of Czech FrameNet. All the above mentioned editors and tools are very sophisticated and useful, but rather specialized for the particular lexical resource. Those few exceptions, such as the Japanese lexical resource builder, are on the other hand too general and would need a substantial amount of customization, since for the CzEngClass lexicon we need to express more specific requirements. We have thus decided to write a new editor, reusing some parts that have been developed in the past for editing the valency"
L18-1136,J05-1004,0,0.109444,"the required surface form of valency frame members (arguments). PDT-Vallex contains about 12,000 valency frames for about 7,000 verbs; EngVallex, contains about 7,000 valency frames for almost 4,500 verbs. CzEngVallex links them across Czech and English using the automatic PCEDT corpus alignments (after manual pruning of erroneous alignments has been applied).8 Since CzEngClass aims at synonymy based on semantics, we also use the following lexical resources: FrameNet (Fillmore et al., 2003; Fillmore et al., 2003), VerbNet (Schuler, 2006), Semlink (Palmer, 2009; Bonial et al., 2012), PropBank (Palmer et al., 2005), Czech WordNet (Pala et al., 2011), (Pala and Smrž, 2004) and English WordNet (Miller, 1995; Fellbaum, 1998).9 These resources are mainly being referred to by the newly built CzEngClass entries; in addition, FrameNet and VerbNet semantic roles are being consulted when defining any particular synonym class. 4. Lexicon Design The lexicon groups translational verbal equivalents, i.e., verb senses, together both in Czech and English, originally represented as valency frames in the Czech and English valency lexicons, into “Synonym Classes.” We call the synonymous senses in one class “Class Members"
L18-1136,L18-1227,1,0.662221,"Missing"
L18-1206,W06-2920,0,0.0607984,"Missing"
L18-1206,W14-5211,0,0.072335,"Missing"
L18-1206,hinrichs-krauwer-2014-clarin,1,0.875661,"Missing"
L18-1206,L16-1262,1,0.886086,"Missing"
L18-1206,L16-1680,1,0.905458,"Missing"
L18-1227,P98-1013,0,0.226387,"orresponding valency frame in the associated valency lexicons, PDT-Vallex and EngVallex (Sect. 3.), effectively providing also word sense labeling for all verb occurrences in the bilingual corpus. 3. interlinked database of argument structures available for each verb and documenting a cross-lingual comparison of Czech and English valency behavior. VALLEX6 (Lopatková et al., 2016) is closely related to PDT-Vallex because it is built on the same theoretical framework. This lexicon is much more elaborated, however it is not based on the PDT data. Among other resources we use, there are FrameNet (Baker et al., 1998; Fillmore et al., 2003), FrameNet+ (Pavlick et al., 2015), VerbNet (Schuler, 2006), SemLink (Palmer, 2009; Bonial et al., 2012), PropBank (Palmer et al., 2005) and English WordNet7 (Miller, 1995; Fellbaum, 1998) as well as Czech WordNet (Pala et al., 2011), (Pala and Smrž, 2004). These resources have been used for an initial set of semantic roles (taken mostly from FrameNet and VerbNet),8 and their entries will be referred to explicitly from all the corresponding entries in the CzEngClass lexicon, if possible to the exact lexical units/frames/sense groups/synsets. Lexical Resources 4. PDT-Val"
L18-1227,J93-2004,0,0.0614026,"meaning and sense is captured in Section 4.. Section 5. deals with the design of CzEngclass lexicon. In Section 6., we exemplify the main (albeit first) findings. We comment on the CzEngClass’ criteria for grouping synonymous, based on samples that have been annotated and processed while creating the first entries. Section 7. summarizes our approach and points to future work. 2. Corpus Resources For our work we use primarily the parallel Prague CzechEnglish Dependency Treebank 2.0 (PCEDT 2.0) (Hajiˇc et al., 2012). This corpus contains the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) and its manual (human) Czech translations. Each language part is enhanced with a rich manual linguistic an1432 notation in The Prague Dependency Treebank (PDT 2.0) style (Hajiˇc et al., 2006; Hajiˇc et al., 2018), which is in turn based on the Functional Generative Dependency (FGD) framework (Sgall et al., 1986). The PDT annotation uses a “stratificational” (layered) approach containing multiple layers (morphology, surface syntax and deep syntax). The main annotated phenomena are (surface) dependency structure and (deep) syntactico-semantic labeling of predicateargument structure. For the pur"
L18-1227,J05-1004,0,0.413528,"b occurrences in the bilingual corpus. 3. interlinked database of argument structures available for each verb and documenting a cross-lingual comparison of Czech and English valency behavior. VALLEX6 (Lopatková et al., 2016) is closely related to PDT-Vallex because it is built on the same theoretical framework. This lexicon is much more elaborated, however it is not based on the PDT data. Among other resources we use, there are FrameNet (Baker et al., 1998; Fillmore et al., 2003), FrameNet+ (Pavlick et al., 2015), VerbNet (Schuler, 2006), SemLink (Palmer, 2009; Bonial et al., 2012), PropBank (Palmer et al., 2005) and English WordNet7 (Miller, 1995; Fellbaum, 1998) as well as Czech WordNet (Pala et al., 2011), (Pala and Smrž, 2004). These resources have been used for an initial set of semantic roles (taken mostly from FrameNet and VerbNet),8 and their entries will be referred to explicitly from all the corresponding entries in the CzEngClass lexicon, if possible to the exact lexical units/frames/sense groups/synsets. Lexical Resources 4. PDT-Vallex (Urešová et al., 2014), (Urešová, 2011) is a Czech valency lexicon, manually created in a bottom-up way during the annotation of the PDT/PCEDT 2.0. Each ent"
L18-1227,P15-2067,0,0.136087,"Missing"
L18-1227,L18-1136,1,0.662221,"Missing"
L18-1227,C14-1121,0,0.0504396,"Missing"
L18-1247,W14-1307,0,0.0617326,"Missing"
L18-1247,D15-1274,0,0.183275,": Most frequent characters with diacritics from data listed in Table 1, together with their relative frequency. The bold characters are recognized only using the uninames method. bic (Azmi and Almajed, 2015) Croatian, Slovenian, Serbian (Ljubešic et al., 2016), and many other languages were published. The system complexity ranges from simplest models, that for each word apply its most frequent translation as observed in the training data, to models that incorporate language models, part-of-speech tags, morphological and many other features. One of the most similar model to ours is a system by Belinkov and Glass (2015) who used recurrent neural networks for Arabic diacritization. 4. Model Architecture The core of our model (see Figure 1) is a bidirectional recurrent neural network, which for each input character outputs its correct label (e.g. its variant with diacritics). The input and output vocabularies contain a special out-of-alphabet symbol. The input characters are embedded, i.e. each character in the input sentence is represented by a vector of d real numbers. The character embeddings are initialized randomly and updated during training. The embeddings are fed to a bidirectional RNN (Graves and Schm"
L18-1247,buck-etal-2014-n,0,0.0417183,"Missing"
L18-1247,W11-2123,0,0.0251336,"(1 − α) log PN N (y1:k |x) + α log PLM (y1:k ), where x denotes the input sequence, y stands for the decoded symbols contained within current hypothesis, PN N and PLM are neural network and language model probabilities and the hyper-parameter α determines the weight of the language model. To keep both log PN N and log PLM terms within a similar range in the decoding, we compute the log PN N as the mean of output token log probabilities and additionally normalize PLM by the number of words in the sequence. To train the language model as well as to run it, we use the open-source KenLM toolkit (Heaﬁeld, 2011). 5. Experiments To compare performance of our model with current approaches, we perform experiments using two existing datasets. The ﬁrst one, created by Ljubešic et al. (2016), consists of Croatian, Serbian and Slovenian sentences from three sources: Wikipedia texts, general Web texts and texts from Twitter. Since Web data are assumed to be the noisiest, they are used only for training. Wikipedia and Twitter testing sets should then cover both standard and non-standard language. The second evaluation dataset we utilize consists of Czech sentences collected mainly from newspapers, thus it cov"
L18-1247,L16-1573,0,0.0781494,"re neural network and language model probabilities and the hyper-parameter α determines the weight of the language model. To keep both log PN N and log PLM terms within a similar range in the decoding, we compute the log PN N as the mean of output token log probabilities and additionally normalize PLM by the number of words in the sequence. To train the language model as well as to run it, we use the open-source KenLM toolkit (Heaﬁeld, 2011). 5. Experiments To compare performance of our model with current approaches, we perform experiments using two existing datasets. The ﬁrst one, created by Ljubešic et al. (2016), consists of Croatian, Serbian and Slovenian sentences from three sources: Wikipedia texts, general Web texts and texts from Twitter. Since Web data are assumed to be the noisiest, they are used only for training. Wikipedia and Twitter testing sets should then cover both standard and non-standard language. The second evaluation dataset we utilize consists of Czech sentences collected mainly from newspapers, thus it covers mostly standard Czech. 5.1. Training and Decoding Details We used the same model conﬁguration for all experiments. The bidirectional RNN has 2 stacked layers with residual c"
L18-1247,C12-2099,1,0.811024,"racters that do not decompose under NFD, but whose diacritics can be stripped by the proposed method. As shown in Table 3, the proposed uninames method recognizes all characters the uninorms method does, and some additional ones. Therefore, we employ the uninames method to strip diacritics in the paper. 3. Related Work One of the ﬁrst papers to describe systems for automatic diacritics restoration is a seminal work by Yarowsky (1999), who compares several algorithms for restoration of diacritics in French and Spanish. Later, models for diacrization in Vietnamese (Nguyen and Ock, 2010), Czech (Richter et al., 2012), Turkish (Adali and Eryiğit, 2014), Ara1 What constitutes a “diacritic mark” is a bit of a problem. On one hand not all characters with a graphical element added to a letter letter contain diacritics, e.g. ¥ (symbol of Japanese Yen) or Ð/ð (Icelandic “eth”). On the other end of the spectrum we have clear diacritics with Unicode canonical decomposition into a letter and a combining mark. Between these clear borders there are the characters that do not have a unicode decomposition, but their names still indicate they are latin letters with some modiﬁer and often they are used the same as charac"
L18-1247,K17-3001,1,0.849998,"Missing"
L18-1247,W02-2021,0,\N,Missing
L18-1247,U11-1016,0,\N,Missing
L18-1551,W14-3348,0,0.0170761,"tionally, Filippova and Altun (2013) proposed a method for constructing datasets for extractive sentence summarization.1 To our best knowledge, only small summarization datasets exist for Czech: Czech part of the MultiLing dataset (Giannakopoulos et al., 2015; Li et al., 2013; Elhadad et al., 2013) containing 40 Wikipedia articles, and SummEC (Rott ˇ and Cerva, 2013) containing 50 news articles. 2.2. Metrics ROUGE (Lin, 2004) is the most commonly used metric, proposed as an English-specific recall-based metric. It utilizes English stemmer, stop words and synonyms. Recently, the METEOR metric (Denkowski and Lavie, 2014) has been used by See et al. (2017) to evaluate multisentence summarization. 2.3. Summarization Methods Summarization methods are generally either extractive or abstractive. Extractive methods only select suitable parts (sentences, words or phrases) from the document, while abstractive methods can produce an arbitrary text as the summary. The extractive summarization methods are typically unsupervised, for example Luhn (Luhn, 1958), Latent Se1 The dataset has been recently released at https://github. com/google-research-datasets/sentence-compression. 3488 mantic Analysis (Steinberger and Jeˇze"
L18-1551,W13-3102,0,0.0353555,"Missing"
L18-1551,D13-1155,0,0.0333726,"2003 and DUC-2004 competitions (Over et al., 2007), which provided a standard evaluation set consisting of 500 news articles from New York Times and Associated Press Wire, each paired with 4 different human-generated reference summaries. For training, the Gigaword dataset (Graff et al., 2003) has been used frequently, offering 4 million news articles including their headlines. Recently, Nallapati et al. (2016a) modified the CNN/Daily Mail corpus constructed by Hermann et al. (2015) to serve for multi-sentence summarization. The corpus consists of approximately 300 000 documents. Additionally, Filippova and Altun (2013) proposed a method for constructing datasets for extractive sentence summarization.1 To our best knowledge, only small summarization datasets exist for Czech: Czech part of the MultiLing dataset (Giannakopoulos et al., 2015; Li et al., 2013; Elhadad et al., 2013) containing 40 Wikipedia articles, and SummEC (Rott ˇ and Cerva, 2013) containing 50 news articles. 2.2. Metrics ROUGE (Lin, 2004) is the most commonly used metric, proposed as an English-specific recall-based metric. It utilizes English stemmer, stop words and synonyms. Recently, the METEOR metric (Denkowski and Lavie, 2014) has been"
L18-1551,D15-1042,0,0.0198908,"hile abstractive methods can produce an arbitrary text as the summary. The extractive summarization methods are typically unsupervised, for example Luhn (Luhn, 1958), Latent Se1 The dataset has been recently released at https://github. com/google-research-datasets/sentence-compression. 3488 mantic Analysis (Steinberger and Jeˇzek, 2004), LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004), SumBasic (Vanderwende et al., 2007) or KL-Sum (Haghighi and Vanderwende, 2009). However, very good results in extractive summarization were achieved recently with recurrent neural networks (Filippova et al., 2015; Filippova and Alfonseca, 2015; Nallapati et al., 2016b; Nallapati et al., 2017). Abstractive approach relies predominantly on the machine translation paradigm, also boosted by the recent success of neural machine translation (Rush et al., 2015; Nallapati et al., 2016a; G¨ulc¸ehre et al., 2016; See et al., 2017). 3. 3.1. The Dataset Choice of Data Sources When designing the dataset, we considered two main requirements. First, and most importantly, we wanted to produce a dataset that would be sufficiently large for deep learning methods to be applicable to it. However, we possessed limited hum"
L18-1551,W15-4638,0,0.170383,"ence summaries. For training, the Gigaword dataset (Graff et al., 2003) has been used frequently, offering 4 million news articles including their headlines. Recently, Nallapati et al. (2016a) modified the CNN/Daily Mail corpus constructed by Hermann et al. (2015) to serve for multi-sentence summarization. The corpus consists of approximately 300 000 documents. Additionally, Filippova and Altun (2013) proposed a method for constructing datasets for extractive sentence summarization.1 To our best knowledge, only small summarization datasets exist for Czech: Czech part of the MultiLing dataset (Giannakopoulos et al., 2015; Li et al., 2013; Elhadad et al., 2013) containing 40 Wikipedia articles, and SummEC (Rott ˇ and Cerva, 2013) containing 50 news articles. 2.2. Metrics ROUGE (Lin, 2004) is the most commonly used metric, proposed as an English-specific recall-based metric. It utilizes English stemmer, stop words and synonyms. Recently, the METEOR metric (Denkowski and Lavie, 2014) has been used by See et al. (2017) to evaluate multisentence summarization. 2.3. Summarization Methods Summarization methods are generally either extractive or abstractive. Extractive methods only select suitable parts (sentences, w"
L18-1551,P16-1014,0,0.0577471,"Missing"
L18-1551,N09-1041,0,0.042182,"re generally either extractive or abstractive. Extractive methods only select suitable parts (sentences, words or phrases) from the document, while abstractive methods can produce an arbitrary text as the summary. The extractive summarization methods are typically unsupervised, for example Luhn (Luhn, 1958), Latent Se1 The dataset has been recently released at https://github. com/google-research-datasets/sentence-compression. 3488 mantic Analysis (Steinberger and Jeˇzek, 2004), LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004), SumBasic (Vanderwende et al., 2007) or KL-Sum (Haghighi and Vanderwende, 2009). However, very good results in extractive summarization were achieved recently with recurrent neural networks (Filippova et al., 2015; Filippova and Alfonseca, 2015; Nallapati et al., 2016b; Nallapati et al., 2017). Abstractive approach relies predominantly on the machine translation paradigm, also boosted by the recent success of neural machine translation (Rush et al., 2015; Nallapati et al., 2016a; G¨ulc¸ehre et al., 2016; See et al., 2017). 3. 3.1. The Dataset Choice of Data Sources When designing the dataset, we considered two main requirements. First, and most importantly, we wanted to"
L18-1551,W04-1013,0,0.10006,"tion is performed using a language-agnostic variant of ROUGE. Keywords: SumeCzech, summarization dataset, document summarization, ROUGE, Czech 1. Introduction Similarly to many other NLP tasks, performance of automatic document summarization has been improving with the recent rise of neural network methods. While deep neural network models can leverage large datasets, only a few moderately-sized datasets are available for document summarization when compared to, e.g., machine translation. Additionally, document summarization has been explored mostly on English, with the dominant ROUGE metric (Lin, 2004) being English-specific (utilizing English stemmer, stop words and synonyms). In order to provide more data for document summarization in Czech, this paper introduces SumeCzech – a collection of one million Czech news articles, each consisting of a headline, a several sentence abstract and a full text. The documents originate from five Czech Internet news sites. The dataset can be downloaded using our provided scripts. Headline-abstract-text structure of the documents allows the dataset to be used for multiple summarization setups: headline generation either from an abstract or a full text, or"
L18-1551,W04-3252,0,0.074127,"sentence summarization. 2.3. Summarization Methods Summarization methods are generally either extractive or abstractive. Extractive methods only select suitable parts (sentences, words or phrases) from the document, while abstractive methods can produce an arbitrary text as the summary. The extractive summarization methods are typically unsupervised, for example Luhn (Luhn, 1958), Latent Se1 The dataset has been recently released at https://github. com/google-research-datasets/sentence-compression. 3488 mantic Analysis (Steinberger and Jeˇzek, 2004), LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004), SumBasic (Vanderwende et al., 2007) or KL-Sum (Haghighi and Vanderwende, 2009). However, very good results in extractive summarization were achieved recently with recurrent neural networks (Filippova et al., 2015; Filippova and Alfonseca, 2015; Nallapati et al., 2016b; Nallapati et al., 2017). Abstractive approach relies predominantly on the machine translation paradigm, also boosted by the recent success of neural machine translation (Rush et al., 2015; Nallapati et al., 2016a; G¨ulc¸ehre et al., 2016; See et al., 2017). 3. 3.1. The Dataset Choice of Data Sources When designing the dataset,"
L18-1551,K16-1028,0,0.042758,"Missing"
L18-1551,D15-1044,0,0.0696861,"earch-datasets/sentence-compression. 3488 mantic Analysis (Steinberger and Jeˇzek, 2004), LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004), SumBasic (Vanderwende et al., 2007) or KL-Sum (Haghighi and Vanderwende, 2009). However, very good results in extractive summarization were achieved recently with recurrent neural networks (Filippova et al., 2015; Filippova and Alfonseca, 2015; Nallapati et al., 2016b; Nallapati et al., 2017). Abstractive approach relies predominantly on the machine translation paradigm, also boosted by the recent success of neural machine translation (Rush et al., 2015; Nallapati et al., 2016a; G¨ulc¸ehre et al., 2016; See et al., 2017). 3. 3.1. The Dataset Choice of Data Sources When designing the dataset, we considered two main requirements. First, and most importantly, we wanted to produce a dataset that would be sufficiently large for deep learning methods to be applicable to it. However, we possessed limited human and time resources making it impossible to accomplish this task by creating summaries manually. This implied an automatic or a semi-automatic method of collecting the data, facilitating the need for a data source consisting of documents that"
L18-1551,P17-1099,0,0.441174,"d a method for constructing datasets for extractive sentence summarization.1 To our best knowledge, only small summarization datasets exist for Czech: Czech part of the MultiLing dataset (Giannakopoulos et al., 2015; Li et al., 2013; Elhadad et al., 2013) containing 40 Wikipedia articles, and SummEC (Rott ˇ and Cerva, 2013) containing 50 news articles. 2.2. Metrics ROUGE (Lin, 2004) is the most commonly used metric, proposed as an English-specific recall-based metric. It utilizes English stemmer, stop words and synonyms. Recently, the METEOR metric (Denkowski and Lavie, 2014) has been used by See et al. (2017) to evaluate multisentence summarization. 2.3. Summarization Methods Summarization methods are generally either extractive or abstractive. Extractive methods only select suitable parts (sentences, words or phrases) from the document, while abstractive methods can produce an arbitrary text as the summary. The extractive summarization methods are typically unsupervised, for example Luhn (Luhn, 1958), Latent Se1 The dataset has been recently released at https://github. com/google-research-datasets/sentence-compression. 3488 mantic Analysis (Steinberger and Jeˇzek, 2004), LexRank (Erkan and Radev,"
novak-hajic-2006-perspectives,W04-2706,0,\N,Missing
P06-1119,2006.eamt-1.18,1,0.694848,"Missing"
P06-1119,J93-2003,0,0.00721652,"Missing"
P06-1119,cmejrek-etal-2004-prague,1,0.901336,"Missing"
P06-1119,W05-1010,0,0.0143561,"and word order. 5 Related Work Several studies have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation"
P06-1119,C96-1006,0,0.0262257,"Work Several studies have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward pro"
P06-1119,J03-1002,0,0.00292812,"us translation are geared toward providing domain-specific thesauri to specialists in a particular field, e.g., medical terminology (Déjean, et al., 2005) and agricultural terminology (Chun & Wenlin, 2002). Researchers on these projects are faced with either finding human translators who are specialized enough to manage the domain-particular translations—or applying automatic techniques to large-scale parallel corpora where data sparsity poses a problem for lowfrequency terms. Data sparsity is also an issue for more general state-of-the-art bilingual alignment approaches (Brown, et al., 2000; Och & Ney, 2003; Wantanabe & Sumita, 2003). 6 Conclusion The task of translating large ontologies can be recast as a problem of implementing fast and efficient processes for acquiring task-specific lexical resources. We developed a method for prioritizing keyword phrases from an English thesaurus of concepts and elicited Czech translations for a subset of the keyword phrases. From these, we decomposed phrase elements for reuse in an English-Czech probabilistic dictionary. We then applied the dictionary in machine translation of the rest of the thesaurus. Our results show an overall improvement in machine tra"
P06-1119,P02-1040,0,0.0751386,"rily phrase by phrase/word by word translation. Our evaluation scores below will partially reflect the simplicity of our system. Our system is simple by design. Any improvement or degradation to the input of our system has direct influence on the output. Thus, measures of translation accuracy for our system can be directly interpreted as quality measures for the lexical resources used and the process by which they were developed. 4 Evaluation We performed two different types of evaluation to validate our process. First, we compared our system output to human reference translations using Bleu (Papineni, et al., 2002), a widelyaccepted objective metric for evaluation of machine translations. Second, we showed corrected and uncorrected machine translations to Czech speakers and collected subjective judgments of fluency and accuracy. For evaluation purposes, we selected 418 keyword phrases to be used as target translations. These phrases were selected using a stratified sampling technique so that different levels of thesaurus value would be represented. There was no overlap between these keyword phrases and the 3000 prioritized keyword phrases used to build our lexicon. Prior to machine translation we obtain"
P06-1119,P99-1067,0,0.00918922,"have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domai"
P06-1119,P03-2025,0,0.0476662,"Missing"
P06-1119,C96-2098,0,0.0202396,"knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domain-specific thesauri to sp"
P14-5003,P05-1001,0,0.00933016,"ted, or the templates are reused if the set of endings is the same. Larger N leads to longer endings and larger number of classes, and smaller N leads to short endings and less classes.7 Named Entity Recognition For English, many NE datasets and shared tasks exist, e.g. CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003), MUC7 (Chinchor, 1998). These shared tasks and the associated freely available NE annotated corpora allowed wide and successful research in NE recognition in English. For example, the systems which published high scores on the CoNLL-2003 task include (Suzuki and Isozaki, 2008), (Ando and Zhang, 2005) and to our knowledge, the best currently known results on this dataset were published by (Ratinov and Roth, 2009). One should also mention a well-known and widely used Stanford parser (Finkel et al., 2005). In Czech, the referential corpus for NE recognition is called the Czech Named Entity Corpus4 ˇ c´ıkov´a et al., 2007) and we describe its’ prop(Sevˇ erties further in Section 4.2. The development of the Czech NE recognition research is easy to folˇ c´ıkov´a et al., low: started by a pilot project by (Sevˇ 2007), the results were improved by (Kravalov´a ˇ and Zabokrtsk´ y, 2009), (Konkol an"
P14-5003,W09-3538,0,0.110668,"Missing"
P14-5003,A00-1031,0,0.398861,"POS tagger and NE recognizer which would • be well suited and trainable for languages with very rich morphology and thus a large 2 2.1 Related Work POS Tagging In English, the task of POS tagging has been in the center of computational linguists’ attention for decades (Kucera and Francis, 1967), with renewed interest after significant improvements achieved by (Collins, 2002). The recent state-of-the-art for English POS supervised tagging without external data for training is by (Shen et al., 2007) and there are many available taggers, such as well-known Brill tagger (Brill, 1992), TnT tagger (Brants, 2000) and many others. 13 Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 13–18, c Baltimore, Maryland USA, June 23-24, 2014. 2014 Association for Computational Linguistics In Czech, the POS tagging research has been carried out mostly by Czech speaking linguistic community and the current state-of-the-art was reported by (Spoustov´a et al., 2009) in Morˇce research project1 . Based on this project, two taggers were released: Morˇce tagger (released as part of COMPOST2 containing morphological analyzer, tagger and trained models, ava"
P14-5003,J93-2004,0,0.0490254,"procedure. This algorithm can be implemented efficiently – our implementation performs 500k word form lookups per second in the Czech morphological dictionary. b h r l í i r á a p p a n n d a e ě u ů y m c m m m a h a á ě o u y a á m m c u i a h o m c m u i h Figure 1: A simple trie for noun “hrad“ (castle in Czech), and two lemmas sharing templates. Czech language was trained on the training part of the Prague Dependency Treebank 2.5 (Bejˇcek et al., 2012). The English language was trained on the standard training portion (Sections 0-18) of the Wall Street Journal part of the Penn Treebank (Marcus et al., 1993). In both cases, the system was tuned on the development set (Sections 19-21 in PTB/WSJ in English) and tested on the testing section (Sections 22-24 in PTB/WSJ in English). 3.3 POS Tagger Evaluation The POS tagger is an offspring of Morˇce and Featurama research projects based on (Spoustov´a et al., 2009). For each form in the text, the morphological dictionary suggests all possible lemmatag candidates and these lemma-tag pairs are disambiguated by the tagger. The tagger is implemented as supervised, rich feature averaged perceptron (Collins, 2002) and the classification features are adopted"
P14-5003,H92-1022,0,0.0976344,"active users, we lacked a POS tagger and NE recognizer which would • be well suited and trainable for languages with very rich morphology and thus a large 2 2.1 Related Work POS Tagging In English, the task of POS tagging has been in the center of computational linguists’ attention for decades (Kucera and Francis, 1967), with renewed interest after significant improvements achieved by (Collins, 2002). The recent state-of-the-art for English POS supervised tagging without external data for training is by (Shen et al., 2007) and there are many available taggers, such as well-known Brill tagger (Brill, 1992), TnT tagger (Brants, 2000) and many others. 13 Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 13–18, c Baltimore, Maryland USA, June 23-24, 2014. 2014 Association for Computational Linguistics In Czech, the POS tagging research has been carried out mostly by Czech speaking linguistic community and the current state-of-the-art was reported by (Spoustov´a et al., 2009) in Morˇce research project1 . Based on this project, two taggers were released: Morˇce tagger (released as part of COMPOST2 containing morphological analyzer, tag"
P14-5003,W09-1119,0,0.015453,"number of classes, and smaller N leads to short endings and less classes.7 Named Entity Recognition For English, many NE datasets and shared tasks exist, e.g. CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003), MUC7 (Chinchor, 1998). These shared tasks and the associated freely available NE annotated corpora allowed wide and successful research in NE recognition in English. For example, the systems which published high scores on the CoNLL-2003 task include (Suzuki and Isozaki, 2008), (Ando and Zhang, 2005) and to our knowledge, the best currently known results on this dataset were published by (Ratinov and Roth, 2009). One should also mention a well-known and widely used Stanford parser (Finkel et al., 2005). In Czech, the referential corpus for NE recognition is called the Czech Named Entity Corpus4 ˇ c´ıkov´a et al., 2007) and we describe its’ prop(Sevˇ erties further in Section 4.2. The development of the Czech NE recognition research is easy to folˇ c´ıkov´a et al., low: started by a pilot project by (Sevˇ 2007), the results were improved by (Kravalov´a ˇ and Zabokrtsk´ y, 2009), (Konkol and Konop´ık, 2011) and (Konkol and Konop´ık, 2013). The current state-of-the-art results for CNEC are reported by ("
P14-5003,P07-1096,0,0.0111608,"le number of POS taggers available for English and other languages with a large number of active users, we lacked a POS tagger and NE recognizer which would • be well suited and trainable for languages with very rich morphology and thus a large 2 2.1 Related Work POS Tagging In English, the task of POS tagging has been in the center of computational linguists’ attention for decades (Kucera and Francis, 1967), with renewed interest after significant improvements achieved by (Collins, 2002). The recent state-of-the-art for English POS supervised tagging without external data for training is by (Shen et al., 2007) and there are many available taggers, such as well-known Brill tagger (Brill, 1992), TnT tagger (Brants, 2000) and many others. 13 Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 13–18, c Baltimore, Maryland USA, June 23-24, 2014. 2014 Association for Computational Linguistics In Czech, the POS tagging research has been carried out mostly by Czech speaking linguistic community and the current state-of-the-art was reported by (Spoustov´a et al., 2009) in Morˇce research project1 . Based on this project, two taggers were released"
P14-5003,W02-1001,0,0.670356,"ct, these tasks can even be considered very close to being “solved”. However, despite the fact that there is a considerable number of POS taggers available for English and other languages with a large number of active users, we lacked a POS tagger and NE recognizer which would • be well suited and trainable for languages with very rich morphology and thus a large 2 2.1 Related Work POS Tagging In English, the task of POS tagging has been in the center of computational linguists’ attention for decades (Kucera and Francis, 1967), with renewed interest after significant improvements achieved by (Collins, 2002). The recent state-of-the-art for English POS supervised tagging without external data for training is by (Shen et al., 2007) and there are many available taggers, such as well-known Brill tagger (Brill, 1992), TnT tagger (Brants, 2000) and many others. 13 Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 13–18, c Baltimore, Maryland USA, June 23-24, 2014. 2014 Association for Computational Linguistics In Czech, the POS tagging research has been carried out mostly by Czech speaking linguistic community and the current state-of-the"
P14-5003,E09-1087,1,0.804286,"Missing"
P14-5003,P05-1045,0,0.00818669,"tion For English, many NE datasets and shared tasks exist, e.g. CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003), MUC7 (Chinchor, 1998). These shared tasks and the associated freely available NE annotated corpora allowed wide and successful research in NE recognition in English. For example, the systems which published high scores on the CoNLL-2003 task include (Suzuki and Isozaki, 2008), (Ando and Zhang, 2005) and to our knowledge, the best currently known results on this dataset were published by (Ratinov and Roth, 2009). One should also mention a well-known and widely used Stanford parser (Finkel et al., 2005). In Czech, the referential corpus for NE recognition is called the Czech Named Entity Corpus4 ˇ c´ıkov´a et al., 2007) and we describe its’ prop(Sevˇ erties further in Section 4.2. The development of the Czech NE recognition research is easy to folˇ c´ıkov´a et al., low: started by a pilot project by (Sevˇ 2007), the results were improved by (Kravalov´a ˇ and Zabokrtsk´ y, 2009), (Konkol and Konop´ık, 2011) and (Konkol and Konop´ık, 2013). The current state-of-the-art results for CNEC are reported by (Strakov´a et al., 2013). So far, there was no freely available Czech NE recognizer. 3 Morpho"
P14-5003,P08-1076,0,0.0204222,"either new template is created, or the templates are reused if the set of endings is the same. Larger N leads to longer endings and larger number of classes, and smaller N leads to short endings and less classes.7 Named Entity Recognition For English, many NE datasets and shared tasks exist, e.g. CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003), MUC7 (Chinchor, 1998). These shared tasks and the associated freely available NE annotated corpora allowed wide and successful research in NE recognition in English. For example, the systems which published high scores on the CoNLL-2003 task include (Suzuki and Isozaki, 2008), (Ando and Zhang, 2005) and to our knowledge, the best currently known results on this dataset were published by (Ratinov and Roth, 2009). One should also mention a well-known and widely used Stanford parser (Finkel et al., 2005). In Czech, the referential corpus for NE recognition is called the Czech Named Entity Corpus4 ˇ c´ıkov´a et al., 2007) and we describe its’ prop(Sevˇ erties further in Section 4.2. The development of the Czech NE recognition research is easy to folˇ c´ıkov´a et al., low: started by a pilot project by (Sevˇ 2007), the results were improved by (Kravalov´a ˇ and Zabokrt"
P14-5003,W03-0419,0,\N,Missing
P98-1080,P93-1035,0,0.0190366,"real translation etc.) are being used. The improvement of one model influences the error rate of the combined model only indirectly. This is not the case of tagging. Tagging can be seen as a &quot;final application&quot; problem for which we assume to have enough data at hand to train and use just one model, abandoning the source-channel paradigm. We have therefore used the e r r o r rate directly as t h e objective f u n c t i o n which we try to minimize when selecting the model's features. This idea is not new, but as far as we know it has been implemented in rule-based taggers and parsers, such as (Brill, 1993a), (Brill, 1993b), (Brill, 1993c) and (Ribarov, 1996), but not in models based on probability distributions. Let's define the set of contexts of a set of features: X(F) = {Z: 3~ Bf~,-~ 6 F}, (s) where F is some set of features of interest. The features can therefore be grouped together based on the context they operate on. In the current implementation, we actually add features in &quot;batches&quot;. A &quot;batch&quot; of features is defined as a set of features which share the same context Z (see the definition below). Computationaly, adding features in batches is relatively cheap both time- and spacewise. Fo"
P98-1080,1993.iwpt-1.3,0,0.0160614,"real translation etc.) are being used. The improvement of one model influences the error rate of the combined model only indirectly. This is not the case of tagging. Tagging can be seen as a &quot;final application&quot; problem for which we assume to have enough data at hand to train and use just one model, abandoning the source-channel paradigm. We have therefore used the e r r o r rate directly as t h e objective f u n c t i o n which we try to minimize when selecting the model's features. This idea is not new, but as far as we know it has been implemented in rule-based taggers and parsers, such as (Brill, 1993a), (Brill, 1993b), (Brill, 1993c) and (Ribarov, 1996), but not in models based on probability distributions. Let's define the set of contexts of a set of features: X(F) = {Z: 3~ Bf~,-~ 6 F}, (s) where F is some set of features of interest. The features can therefore be grouped together based on the context they operate on. In the current implementation, we actually add features in &quot;batches&quot;. A &quot;batch&quot; of features is defined as a set of features which share the same context Z (see the definition below). Computationaly, adding features in batches is relatively cheap both time- and spacewise. Fo"
P98-1080,A88-1019,0,0.073796,"f that category; 3. the original word form. Please note that it is customary to number the seven grammatical cases in Czech: (instead of naming them): &quot;nominative&quot; gets 1, &quot;genitive&quot; 2, etc. There are four genders, as the Czech masculine gender is divided into masculine animate (M) and inanimate (I). Fig. 1 is a typical example of the ambiguities encountered in a running text: little POS ambiguity, but a lot of gender, number and case ambiguity (columns 3 to 5). 3 The Model Instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (Merialdo, 1992), (Church, 1988), (Hajji, Hladk~, 1997)) used in the past (notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers), we are using here a &quot;direct&quot; approach to modeling, for which we have chosen an exponential probabilistic model. Such model (when predicting an event 5 y E Y in a context x) has the general form PAC,e (YIX) = exp(~-~in----1Aifi (y, x)) Z(x) (3) where fi (Y, x) is the set (of size n) of binary-valued (yes/no) features of the event value being predicted and its context, hi is a &quot;weigth&quot; (in the exponential sense) of the feature fi, and the normalization factor Z(x) is defin"
P98-1080,J94-2001,0,\N,Missing
Q13-1034,C00-2143,1,0.361351,"bank covered by morphological analyzer. Clusters: number of tokens and types in unlabeled corpus. treebank annotation we have to rely on a heuristic mapping between the two. Word clusters are derived from the so-called Huge German Corpus.7 Hungarian For training and test we use the Szeged Dependency Treebank (Farkas et al., 2012). We use a finite-state morphological analyzer constructed from the morphdb.hu lexical resource (Tr´on et al., 2006), and word clusters come from the Hungarian National Corpus (V´aradi, 2002). Russian Parsers are trained and tested on data from the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). The morphological analyzer is a module of the ETAP-3 linguistic processor (Apresian et al., 2003) with a dictionary comprising more than 130,000 lexemes (Iomdin and Sizov, 2008). Word clusters have been produced on the basis of an unlabeled corpus of Russian compiled by the Russian Language Institute of the Russian Academy of Sciences and tokenized by the ETAP-3 analyzer. 4 Joint Morphology and Syntax We start by exploring different ways of integrating morphology and syntax in a data-driven setting, that is, where our only knowledge source is the annotated training"
Q13-1034,boguslavsky-etal-2002-development,1,0.676986,"cal analyzer. Clusters: number of tokens and types in unlabeled corpus. treebank annotation we have to rely on a heuristic mapping between the two. Word clusters are derived from the so-called Huge German Corpus.7 Hungarian For training and test we use the Szeged Dependency Treebank (Farkas et al., 2012). We use a finite-state morphological analyzer constructed from the morphdb.hu lexical resource (Tr´on et al., 2006), and word clusters come from the Hungarian National Corpus (V´aradi, 2002). Russian Parsers are trained and tested on data from the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). The morphological analyzer is a module of the ETAP-3 linguistic processor (Apresian et al., 2003) with a dictionary comprising more than 130,000 lexemes (Iomdin and Sizov, 2008). Word clusters have been produced on the basis of an unlabeled corpus of Russian compiled by the Russian Language Institute of the Russian Academy of Sciences and tokenized by the ETAP-3 analyzer. 4 Joint Morphology and Syntax We start by exploring different ways of integrating morphology and syntax in a data-driven setting, that is, where our only knowledge source is the annotated training corpus. At both learning a"
Q13-1034,E12-1009,1,0.573095,"ther support for this choice, at least for the languages considered in this paper. Note also that the choice is not motivated by efficiency concerns, since increasing the values of kp and km has only a marginal effect on running time, as explained in Section 2.4. Finally, the choice not to consider k-best lemmas is dictated by the fact that our lemmatizer only provides a 1-best analysis. For the first three models, we use the same feature representations as Bohnet and Nivre (2012),9 consisting of their adaptation of the features used by Zhang and Nivre (2011), the graph completion features of Bohnet and Kuhn (2012), and the special features over k-best tags introduced specifically for joint tagging and parsing by Bohnet and Nivre (2012). For the J OINT model, we simply add features over the k-best morphological descriptions analogous to the features over k-best tags.10 Experimental results for these four models can be found in Table 2. From the P IPELINE results, we see that the 1-best accuracy of the preprocessing tagger ranges from 95.0 (Finnish) to 99.2 (Czech) for POS, and from 89.4 (Finnish) to 96.5 (Hungarian) for MOR. The lemmatizer does a good job for four of the languages (93.9–97.9) but has re"
Q13-1034,D12-1133,1,0.0763756,"s, which tend to assume that all morphological disambiguation has been performed before syntactic analysis begins. However, as argued by Lee et al. (2011), in morphologically rich languages there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. Lee et al. (2011) go on to show that a discriminative model for joint morphological disambiguation and dependency parsing gives consistent improvements in morphological and syntactic accuracy, compared to a pipeline model, for Ancient Greek, Czech, Hungarian and Latin. Similarly, Bohnet and Nivre (2012) propose a model for 1 See https://sites.google.com/site/spmrl2013/home/sharedtask. 415 Transactions of the Association for Computational Linguistics, 1 (2013) 415–428. Action Editor: Brian Roark. c Submitted 7/2013; Revised 9/2013; Published 10/2013. 2013 Association for Computational Linguistics. joint part-of-speech tagging and dependency parsing and report improved accuracy for Czech and German (but also for Chinese and English), although in this case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approa"
Q13-1034,C10-1011,1,0.684406,"nguages. For Czech, the best previous UAS on the standard train-test split of the PDT is 87.32, reported by Koo et al. (2010) with a parser using non-projective head automata and dual decomposition, while the best LAS is 78.82 LAS from Nilsson et al. (2006), using a greedy arc-eager transitionbased system with pseudo-projective parsing. Our best results are 1.7 percentage points better for UAS (89.0) and almost 5 percentage points better for LAS (83.7).14 For Finnish, the only previous results are from Haverinen et al. (2013), who achieve 81.01 LAS and 84.97 UAS with the graph-based parser of Bohnet (2010). We get substantial improvements with 83.1 LAS and 86.6 UAS. We also improve slightly over their best POS score, obtained with the HunPos tagger (Hal´acsy et al., 2007) together with the OMorFi analyzer (95.7 vs. 95.4). For German, the best previous results on the same train-test split are from Seeker and Kuhn (2012), using the graphbased parser of Bohnet (2010) in a pipeline architecture. With the same evaluation setup as in this paper, they achieve 91.50 LAS and 93.48 UAS – 13 L EX S OFT averages 0.132 ms per sentence on an Intel i73930K processor with 6 cores, against 0.112 ms for P IPELIN"
Q13-1034,J92-4003,0,0.242366,"striking for German, where the soft lexical constraints are clearly beneficial (especially for the MOR score) despite not being quite compatible with the morphological descriptions in the training set. In terms of statistical signifance, L EX S OFT outperforms the J OINT model with respect to the PMD score for all languages (p < 0.01). It is also significantly better than L EX H ARD for all languages except Finnish (p < 0.01). 6 Word Clusters Finally, we add word cluster features to the best model for each language (L EX H ARD for Finnish, L EX S OFT for the others).11 We use Brown clusters (Brown et al., 1992), with 800 clusters for all languages, and we use the same feature representation as Bohnet and Nivre (2012). The results in Table 2 show small but consistent improvements in almost all metrics for all languages, confirming the benefit of cluster features for morphologically rich languages. It is worth noting that we see the biggest improvement for Finnish, the language with the smallest training set and therefore most likely to 11 The best model was selected according to results on the dev set (cross-validation on the training set for Finnish). suffer from sparse data, where the syntactic acc"
Q13-1034,W06-2920,0,0.719913,"re languages, it has also been observed that typological differences between languages lead to new challenges. In particular, it has been found over and over again that languages exhibiting rich morphological structure, often together with a relatively free word order, usually obtain lower parsing accuracy, especially in comparison to English. One striking demonstration of this tendency can be found in the CoNLL shared tasks on multilingual dependency parsing, organized in 2006 and 2007, where richly inflected languages clustered at the lower end of the scale with respect to parsing accuracy (Buchholz and Marsi, 2006; Nivre et al., 2007). These and similar observations have led to an increased interest in the special challenges posed by parsing morphologically rich languages, as evidenced most clearly by a new series of workshops devoted to this topic (Tsarfaty et al., 2010), as well as a special issue in Computational Linguistics (Tsarfaty et al., 2013) and a shared task on parsing morphologically rich languages.1 One hypothesized explanation for the lower parsing accuracy observed for richly inflected languages is the strict separation of morphological and syntactic analysis assumed in many parsing fram"
Q13-1034,D07-1022,0,0.0694561,"ish), although in this case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approach to morphological and syntactic analysis can also be found in grammar-based dependency parsers, such as the ETAP-3 linguistic processor (Apresian et al., 2003), where morphological disambiguation is mostly carried out together with syntactic analysis. Finally, it is worth noting that joint models of morphology and syntax have been more popular in constituency-based statistical parsing (Cowan and Collins, 2005; Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and Elhadad (2013) show that"
Q13-1034,D11-1114,0,0.0374383,"Missing"
Q13-1034,P04-1015,0,0.261816,"put sentence x with weight vector w. The symbols h.c, h.s and h.f denote, respectively, the configuration, score and feature vector of a hypothesis h; Γc denotes the MS-parse defined by c. to 0.0, make N iterations over the training data and update the weight vector for every sentence x where the transition sequence C0,m corresponding to the gold parse is different from the highest scoring tran4 More precisely, we use the ∗ sition sequence C0,m 0. passive-aggressive update of Crammer et al. (2006). We also use the early update strategy found beneficial for parsing in several previous studies (Collins and Roark, 2004; Zhang and Clark, 2008; Huang and Sagae, 2010). This means that, at learning time, we terminate the beam search as soon as the hypothesis corresponding to the gold parse is pruned from the beam and then update with respect to the partial transition sequences constructed up to that point. Finally, we use the standard technique of averaging over all weight vectors seen in training, as originally proposed by Collins (2002). 4 Note that there may be more than one transition sequence corresponding to the gold parse, in which case we pick the canonical transition sequence that processes all left-de"
Q13-1034,W02-1001,0,0.162454,"inear for natural language data sets, due to the sparsity of non-projective dependencies (Nivre, 2009). The running time is also linear in |D |+ |P × M |, which means that joint prediction only gives a linear increase in running time, often quite marginal because |D |> |P × M |. This assumes that the lemma is predicted deterministically given a tag and a morphological description, an assumption that is enforced in all our experiments. 2.5 Learning In order to learn a weight vector w from a training set of sentences with gold parses, we use a variant of the structured perceptron, introduced by Collins (2002) and first used for transition-based parsing by Zhang and Clark (2008). We initialize all weights 3 While there exist exact dynamic programming algorithms for projective transition systems (Huang and Sagae, 2010; Kuhlmann et al., 2011) and even for restricted non-projective systems (Cohen et al., 2011), parsing is intractable for systems like ours that permit arbitrary non-projective trees. PARSE(x, w) 1 h0 .c ← cs (x) 2 h0 .s ← 0.0 3 h0 .f ← {0.0}dim(w) 4 B EAM ← [h0 ] 5 while ∃h ∈ B EAM : h.c 6∈ Ct 6 T MP ← [ ] 7 foreach h ∈ B EAM 8 foreach t ∈ T : P ERMISSIBLE(h.c, t) 9 h.f ← h.f + f(x, h.c"
Q13-1034,H05-1100,0,0.0303135,"and German (but also for Chinese and English), although in this case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approach to morphological and syntactic analysis can also be found in grammar-based dependency parsers, such as the ETAP-3 linguistic processor (Apresian et al., 2003), where morphological disambiguation is mostly carried out together with syntactic analysis. Finally, it is worth noting that joint models of morphology and syntax have been more popular in constituency-based statistical parsing (Cowan and Collins, 2005; Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus"
Q13-1034,E12-1007,1,0.93185,"22 1105 33 151,971 71,263 200,249,814 538,138 14 454 78 97,905 35,039 195,897,041 639,446 Table 1: Statistics about data sets and resources used in the experiments. Treebank: number of tokens in data sets; number of labels in label sets. Morphology: number of word forms and lemmas in treebank covered by morphological analyzer. Clusters: number of tokens and types in unlabeled corpus. treebank annotation we have to rely on a heuristic mapping between the two. Word clusters are derived from the so-called Huge German Corpus.7 Hungarian For training and test we use the Szeged Dependency Treebank (Farkas et al., 2012). We use a finite-state morphological analyzer constructed from the morphdb.hu lexical resource (Tr´on et al., 2006), and word clusters come from the Hungarian National Corpus (V´aradi, 2002). Russian Parsers are trained and tested on data from the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). The morphological analyzer is a module of the ETAP-3 linguistic processor (Apresian et al., 2003) with a dictionary comprising more than 130,000 lexemes (Iomdin and Sizov, 2008). Word clusters have been produced on the basis of an unlabeled corpus of Russian compiled by the Rus"
Q13-1034,W09-1205,0,0.0234606,"binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search decoders explore a larger part of the search space than greedy parsers, which is likely to be especially important for joint models, where the search space is larger than for plain dependen"
Q13-1034,J13-1007,0,0.0248062,"Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and Elhadad (2013) show that integrating an external wide-coverage lexicon with a treebank-trained PCFG parser improves parsing accuracy for Modern Hebrew, which is in line with earlier studies of part-of-speech tagging for morphologically rich languages (Hajiˇc, 2000). The sparsity of lexical features can also be tackled by the use of distributional word clusters as pioneered by Koo et al. (2008). In this paper, we present a transition-based model that jointly predicts complex morphological representations and dependency relations, generalizing the approach of Bohnet and Nivre (2012) to include the full range"
Q13-1034,P08-1043,0,0.0558272,"case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approach to morphological and syntactic analysis can also be found in grammar-based dependency parsers, such as the ETAP-3 linguistic processor (Apresian et al., 2003), where morphological disambiguation is mostly carried out together with syntactic analysis. Finally, it is worth noting that joint models of morphology and syntax have been more popular in constituency-based statistical parsing (Cowan and Collins, 2005; Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and Elhadad (2013) show that integrating an external wide-"
Q13-1034,P98-1080,1,0.597993,"Missing"
Q13-1034,A00-2013,0,0.0703501,"Missing"
Q13-1034,P07-2053,0,0.0543962,"Missing"
Q13-1034,I11-1136,0,0.0554181,"hang and Clark (2008), we assume that the score is given by a linear model whose feature representations decompose in the same way: s(x, C0,m ) = f (x, C0,m ) · w m X = f (x, ci , ti ) · w (2) i=0 Here, f (x, c, t) is a high-dimensional feature vector, where each component fi (x, c, t) is a nonnegative numerical feature (usually binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has show"
Q13-1034,P10-1110,0,0.18704,"near increase in running time, often quite marginal because |D |> |P × M |. This assumes that the lemma is predicted deterministically given a tag and a morphological description, an assumption that is enforced in all our experiments. 2.5 Learning In order to learn a weight vector w from a training set of sentences with gold parses, we use a variant of the structured perceptron, introduced by Collins (2002) and first used for transition-based parsing by Zhang and Clark (2008). We initialize all weights 3 While there exist exact dynamic programming algorithms for projective transition systems (Huang and Sagae, 2010; Kuhlmann et al., 2011) and even for restricted non-projective systems (Cohen et al., 2011), parsing is intractable for systems like ours that permit arbitrary non-projective trees. PARSE(x, w) 1 h0 .c ← cs (x) 2 h0 .s ← 0.0 3 h0 .f ← {0.0}dim(w) 4 B EAM ← [h0 ] 5 while ∃h ∈ B EAM : h.c 6∈ Ct 6 T MP ← [ ] 7 foreach h ∈ B EAM 8 foreach t ∈ T : P ERMISSIBLE(h.c, t) 9 h.f ← h.f + f(x, h.c, t) 10 h.s ← h.s + f(x, h.c, t) · w 11 h.c ← t(h.c) 12 T MP ← I NSERT(h, T MP) 13 B EAM ← P RUNE(T MP) 14 h∗ ← T OP(B EAM) 15 return Γh∗c Figure 2: Beam search algorithm for finding the best MSparse for input s"
Q13-1034,P08-1068,0,0.0257504,"rphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and Elhadad (2013) show that integrating an external wide-coverage lexicon with a treebank-trained PCFG parser improves parsing accuracy for Modern Hebrew, which is in line with earlier studies of part-of-speech tagging for morphologically rich languages (Hajiˇc, 2000). The sparsity of lexical features can also be tackled by the use of distributional word clusters as pioneered by Koo et al. (2008). In this paper, we present a transition-based model that jointly predicts complex morphological representations and dependency relations, generalizing the approach of Bohnet and Nivre (2012) to include the full range of morphological information. We start by investigating different ways of integrating morphological features into the model, go on to examine the effect of using rule-based morphological analyzers to derive hard or soft constraints on the morphological analysis, and finally add word cluster features to combat lexical sparsity. We evaluate our methods on data from Czech, Finnish,"
Q13-1034,D10-1125,0,0.0274644,"Missing"
Q13-1034,P11-1068,0,0.0182537,"Missing"
Q13-1034,P11-1089,0,0.0278511,"al., 2010), as well as a special issue in Computational Linguistics (Tsarfaty et al., 2013) and a shared task on parsing morphologically rich languages.1 One hypothesized explanation for the lower parsing accuracy observed for richly inflected languages is the strict separation of morphological and syntactic analysis assumed in many parsing frameworks (Tsarfaty et al., 2010; Tsarfaty et al., 2013). This is true in particular for data-driven dependency parsers, which tend to assume that all morphological disambiguation has been performed before syntactic analysis begins. However, as argued by Lee et al. (2011), in morphologically rich languages there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. Lee et al. (2011) go on to show that a discriminative model for joint morphological disambiguation and dependency parsing gives consistent improvements in morphological and syntactic accuracy, compared to a pipeline model, for Ancient Greek, Czech, Hungarian and Latin. Similarly, Bohnet and Nivre (2012) propose a model for 1 See https://sites.google.com/site/spmrl2013/home/sharedtask. 415 Transactions of the Association for Computa"
Q13-1034,P06-1033,1,0.867756,"Missing"
Q13-1034,W09-3811,1,0.599747,"ng time, we terminate the beam search as soon as the hypothesis corresponding to the gold parse is pruned from the beam and then update with respect to the partial transition sequences constructed up to that point. Finally, we use the standard technique of averaging over all weight vectors seen in training, as originally proposed by Collins (2002). 4 Note that there may be more than one transition sequence corresponding to the gold parse, in which case we pick the canonical transition sequence that processes all left-dependents before right-dependents and applies the lazy swapping strategy of Nivre et al. (2009). 419 3 Data Sets and Resources Throughout the paper, we experiment with data from five languages: Czech, Finnish, German, Hungarian, and Russian. For each language, we use a morphologically and syntactically annotated corpus (treebank), divided into a training set, a development set and a test set. In addition, we use a lexicon generated by a rule-based morphological analyzer, and distributional word clusters derived from a large unlabeled corpus. Below we describe the specific resources used for each language. Table 1 provides descriptive statistics about the resources. Czech For training an"
Q13-1034,W03-3017,1,0.552345,"ections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search decoders explore a larger part of the search space than greedy parsers, which is likely to be especially important for joint models, where the search space is larger than for plain dependency parsing without morphology (even more so with the S WAP transition for nonprojectivity). Figure 2 outlines the beam search algorithm used for decoding with our model. Different instantiations of the model will require slightly different impl"
Q13-1034,W04-0308,1,0.586101,"e representations decompose in the same way: s(x, C0,m ) = f (x, C0,m ) · w m X = f (x, ci , ti ) · w (2) i=0 Here, f (x, c, t) is a high-dimensional feature vector, where each component fi (x, c, t) is a nonnegative numerical feature (usually binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark,"
Q13-1034,P09-1040,1,0.939947,"Γ = (A, π, µ, λ, δ) is an MS-parse for x. We take the initial configuration for a sentence x = w1 , . . . , wn to be cs (x) = ([0], [1, . . . , n], (∅, ⊥, ⊥, ⊥, ⊥)), where ⊥ is the function that is undefined for all arguments, and we take the set Ct of terminal configurations to be the set of all configurations of the form c = ([0], [ ], Γ) (for any Γ). The MS-parse defined for x by c = (Σ, B, (A, π, µ, λ, δ)) is Γc = (A, π, µ, λ, δ), and the MS-parse defined for x by a complete transition sequence C0,m is Γtm (cm ) . The set T of transitions is shown in Figure 1. It is based on the system of Nivre (2009), where a dependency tree is built by repeated applications of the L EFT-A RCd and R IGHT-A RCd transitions, which add an arc (with some label d ∈ D) between the two topmost nodes on the stack (with the leftmost or rightmost node as the dependent, respectively). The S HIFT transition is used to move nodes from the buffer to the stack, and the S WAP transition is used to permute nodes in order to allow non-projective dependencies. Bohnet and Nivre (2012) modified this system by replacing the simple S HIFT transition by S HIFTp , which not only moves a node from the buffer to the stack but also"
Q13-1034,W11-4644,0,0.0793275,"Missing"
Q13-1034,W09-3829,0,0.0396525,"Missing"
Q13-1034,schmid-etal-2004-smor,0,0.0195468,"Missing"
Q13-1034,seeker-kuhn-2012-making,0,0.0289485,"Missing"
Q13-1034,C10-2129,1,0.896137,"Missing"
Q13-1034,spoustova-spousta-2012-high,0,0.0221884,"Missing"
Q13-1034,W07-2218,0,0.0373151,") = f (x, C0,m ) · w m X = f (x, ci , ti ) · w (2) i=0 Here, f (x, c, t) is a high-dimensional feature vector, where each component fi (x, c, t) is a nonnegative numerical feature (usually binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search deco"
Q13-1034,tron-etal-2006-morphdb,0,0.119292,"Missing"
Q13-1034,W10-1401,0,0.0166567,"Missing"
Q13-1034,J13-1003,1,0.829911,"Missing"
Q13-1034,P06-3009,0,0.0901789,"Chinese and English), although in this case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approach to morphological and syntactic analysis can also be found in grammar-based dependency parsers, such as the ETAP-3 linguistic processor (Apresian et al., 2003), where morphological disambiguation is mostly carried out together with syntactic analysis. Finally, it is worth noting that joint models of morphology and syntax have been more popular in constituency-based statistical parsing (Cowan and Collins, 2005; Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and E"
Q13-1034,varadi-2002-hungarian,0,0.105288,"Missing"
Q13-1034,W03-3023,0,0.0799723,"tantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search decoders explore a larger part of the search space than greedy parsers, which is likely to be especially important for joint models, where the search space is larger than for plain dependency parsing without morphology (even more so with the S WAP transition for nonprojectivity). Figure 2 outlines the beam search algorithm used for decoding with our model. Different instantiations of the model will require slightly"
Q13-1034,D08-1059,0,0.738873,"so that a node moved from the buffer to the stack is assigned not only a tag p but also a morphological description m and a lemma l. In this way, we get a joint model for the prediction of part-ofspeech tags, morphological features, lemmas, and dependency trees. 2.3 Scoring In transition-based parsing, we score parses in an indirect fashion by scoring transition sequences. In general, we assume that the score function s factors by configuration-transition pairs: s(x, C0,m ) = m X s(x, ci , ti ) (1) i=0 Moreover, when using structured learning, as first proposed for transition-based parsing by Zhang and Clark (2008), we assume that the score is given by a linear model whose feature representations decompose in the same way: s(x, C0,m ) = f (x, C0,m ) · w m X = f (x, ci , ti ) · w (2) i=0 Here, f (x, c, t) is a high-dimensional feature vector, where each component fi (x, c, t) is a nonnegative numerical feature (usually binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011)"
Q13-1034,P11-2033,1,0.457721,"ection 7, we present an empirical analysis that gives further support for this choice, at least for the languages considered in this paper. Note also that the choice is not motivated by efficiency concerns, since increasing the values of kp and km has only a marginal effect on running time, as explained in Section 2.4. Finally, the choice not to consider k-best lemmas is dictated by the fact that our lemmatizer only provides a 1-best analysis. For the first three models, we use the same feature representations as Bohnet and Nivre (2012),9 consisting of their adaptation of the features used by Zhang and Nivre (2011), the graph completion features of Bohnet and Kuhn (2012), and the special features over k-best tags introduced specifically for joint tagging and parsing by Bohnet and Nivre (2012). For the J OINT model, we simply add features over the k-best morphological descriptions analogous to the features over k-best tags.10 Experimental results for these four models can be found in Table 2. From the P IPELINE results, we see that the 1-best accuracy of the preprocessing tagger ranges from 95.0 (Finnish) to 99.2 (Czech) for POS, and from 89.4 (Finnish) to 96.5 (Hungarian) for MOR. The lemmatizer does a"
Q13-1034,C12-2136,1,0.0524599,"out the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search decoders explore a larger part of the search space than greedy parsers, which is likely to be especially important for joint models, where the search space is larger than for plain dependency parsing without morphology (even more so with the S WAP transition for nonprojectivity). Figure 2 outlines the beam search algorithm used for decoding with our model. Different instantiations of the model will require slightly different implementations of the permissibility condition invoked in line 8, which can be used to filter out labels that are improbable or incom"
Q13-1034,C98-1077,0,\N,Missing
Q13-1034,W09-1201,1,\N,Missing
Q13-1034,D07-1096,1,\N,Missing
rehm-etal-2014-strategic,P07-2045,0,\N,Missing
rehm-etal-2014-strategic,piperidis-etal-2014-meta,1,\N,Missing
rehm-etal-2014-strategic,piperidis-2012-meta,1,\N,Missing
S14-2008,D12-1133,0,0.0149346,"ained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given in the LF column. In cases where a team submitted two runs to a track, only the highestranked score is included in the t"
S14-2008,W06-2920,0,0.101128,"em in comparison to other sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results. 1 Background and Motivation Syntactic dependency parsing has seen great advances in the past decade, in part owing to relatively broad consensus on target representations, and in part reflecting the successful execution of a series of shared tasks at the annual Conference for Natural Language Learning (CoNLL; Buchholz & Marsi, 2006; Nivre et al., 2007; inter alios). From this very active research area accurate and efficient syntactic parsers have developed for a wide range of natural languages. However, the predominant data structure in dependency parsing to date are trees, in the formal sense that every node in the dependency graph is reachable from a distinguished root node by exactly one directed path. (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice. Semantically, technique arguably is dependent on the determiner (the quantificational locus), the modifier simil"
S14-2008,de-marneffe-etal-2006-generating,0,0.0702807,"Missing"
S14-2008,oepen-lonning-2006-discriminant,1,0.758648,"antic dependency graphs originate in a manual re-annotation of Sections 00– 21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czec"
S14-2008,W09-1201,1,0.855031,"Missing"
S14-2008,J05-1004,0,0.129381,"ependency graphs for Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in o"
S14-2008,P06-1055,0,0.0124328,"e of the gold-standard syntactic or semantic analyses of the SDP 2014 test data, i.e. were directly or indirectly trained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given i"
S14-2008,hajic-etal-2012-announcing,1,0.772691,"Missing"
S14-2008,W12-3602,1,0.885947,"yses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical ("
S14-2008,C08-1095,0,0.478753,".27 to 75.89 and the corresponding scores across systems are 88.64 for PAS, 84.95 for DM, and 67.52 for PCEDT. While these scores are consistently higher than in the closed track, the differences are small. In fact, for each of the three teams that submitted to both tracks (Alpage, Potsdam, and Priberam) improvements due to the use of additional resources in the open track do not exceed two points LF. 7 dencies), while the others apply post-processing to recover non-tree structures. The second strategy is to use a parsing algorithm that can directly generate graph structures (in the spirit of Sagae & Tsujii, 2008; Titov et al., 2009). In many cases such algorithms generate restricted types of graph structures, but these restrictions appear feasible for our target representations. The last approach is more machine learning–oriented; they apply classifiers or scoring methods (e.g. edge-factored scores), and find the highest-scoring structures by some decoding method. It is difficult to tell which approach is the best; actually, the top three systems in the closed and open tracks selected very different approaches. A possible conclusion is that exploiting existing systems or techniques for dependency par"
S14-2008,P10-5006,0,0.0693315,"ple inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another difference to common interpretations of SRL is that the SDP 2014 task definition does not encompass predicate disambiguation, a design decision in part owed to our goal to focus on parsing-oriented, i.e. structural, analysis, and in part to lacking consensus on sense inventories for all content words. Finally, a third closely related area of much current interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to most work in this tradition, our SDP target representations aim to be task- and domainindependent, though at least part of this generality comes at the expense of ‘completeness’ in the above sense; i.e. there are aspects of sentence meaning that arguably remain implicit. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure"
S14-2008,J93-2004,0,0.0590496,"t of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs). Thus, Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP 2014),1 seeks to stimulate the dependency parsing community to move towards more general graph processing, to thus enable a more direct analysis of Who did What to Whom? For English, there exist several independent annotations of sentence meaning over the venerable Wall Street Journal (WSJ) text of the Penn Treebank (PTB; Marcus et al., 1993). These resources constitute parallel semantic annotations over the same common text, but to date they have not been related to each other and, in fact, have hardly been applied for training and testing of datadriven parsers. In this task, we have used three different such target representations for bi-lexical semantic dependencies, as demonstrated in Figure 1 below for the WSJ sentence: Task 8 at SemEval 2014 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure"
S14-2008,P07-1031,0,0.0115637,"ersely, in PCEDT the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.7 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase–internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PCEDT, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PCEDT annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely ass"
S14-2008,meyers-etal-2004-annotating,0,0.0465249,"Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another"
S14-2008,S14-2056,1,0.893424,"pBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical (a-trees) and tectogrammatical (t-trees). PCEDT bi-lexical dependencies in this task have been extracted from the t-trees. The specifics of the PCE"
S14-2008,C10-1011,0,\N,Missing
S14-2008,S14-2080,0,\N,Missing
S14-2008,S14-2082,0,\N,Missing
S14-2008,J02-3001,0,\N,Missing
S14-2008,W15-0128,1,\N,Missing
S14-2008,D07-1096,0,\N,Missing
S14-2008,cinkova-2006-propbank,0,\N,Missing
S15-2153,D12-1133,0,0.036307,"semantic dependencies distributed for the task. Systems in the open track, on the other hand, could use additional resources, such as a syntactic parser, for example—provided that they make sure to not use any tools or resources that encompass knowledge of the gold-standard syntactic or semantic analyses of the SDP 2015 test data.11 To simplify participation in the open track, the organizers prepared ready-touse ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in the form of Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). Finally, to more directly gauge the the contributions of syntactic structure on the semantic dependency parsing problem, an idealized gold track was introduced in SDP 2015. For this track, gold-standard syntactic companion files were provided in a varity of formats, viz. (a) Stanford Basic dependencies, derived from the PTB, (b) HPSG syntactic dependencies in the form called DM by Ivanova et al. (2012), derived from DeepBank, and (c) HPSG syntactic dependencies derived from the Enju Treebank. 6 Submissions and Results From almost 40 teams who had registered for the task, twelve teams obtaine"
S15-2153,cinkova-2006-propbank,1,0.772792,".6993 .5743 .6719 − .5630 .5675 .5490 − Table 2: Pairwise F1 similarities, including punctuation (upper right diagonals) or not (lower left). Frame or sense distinctions are a new property in SDP 2015 and currently are only available for the English DM and PSD data. Table 1 reveals a stark difference in granularity: DM limits itself to argument structure distinctions that are grammaticized, e.g. causative vs. inchoative contrasts or differences in the arity or coarse semantic typing of argument frames; PSD, on the other hand, draws on the much richer sense inventory of the EngValLex database (Cinková, 2006). Accordingly, the two target representations represent quite different challenges for the predicate disambiguation sub-task of SDP 2015. Finally, in Table 2 we seek to quantify pairwise structural similarity between the three representations in terms of unlabeled dependency F1 (dubbed UF in Section 5 below). We provide four variants of this metric, (a) taking into account the directionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discardin"
S15-2153,de-marneffe-etal-2006-generating,0,0.0580061,"Missing"
S15-2153,S14-2080,0,0.42538,"ably because the additional dependency parser they used was trained on data from the target domain. 7 Overview of Approaches Table 5 shows a summary of the tracks in which each submitted system participated, and Table 6 shows an overview of approaches and additionally used resources. All the teams except In-House submitted results for cross-lingual data (Czech and Chinese). Teams except Lisbon also tackled with predicate disambiguation. Only Turku participated in the Gold track. The submitted teams explored a variety of approaches. Riga and Peking relied on the graph-to-tree transformation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12 Please see the task web page at the address indicated above for full labeled and unlabeled scores. Team In-House Lisbon Minsk Peking Riga Turku Closed X X X X Open X X X Cross-Lingual Predicate Disambiguation Gold X X X X X X X X X X X Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House Lisbon Minsk Peking Riga Turku grammar-based parsing (Miyao et al., 2014) graph parsing with dual decomposition (Martins & Almeida, 2014) tra"
S15-2153,hajic-etal-2012-announcing,1,0.91158,"Missing"
S15-2153,W12-3602,1,0.93906,"ctionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discarding punctuation. While relaxing the comparison to ignore edge directionality also increases similarity scores for this pair, the effect is much more pronounced when comparing either to PSD. This suggests that directionality of semantic dependencies is a major source of diversion between DM and PAS on the one hand, and PSD on the other hand. Linguistic Comparison Among other aspects, Ivanova et al. (2012) categorize a range of syntactic and semantic dependency annotation schemes according to the role that functional elements take. In Figure 1 and the discussion of Table 1 above, we already observed that PAS differs from the other representations in integrating into the graph auxiliaries, the infinitival marker, the case-marking preposition introducing the argument of apply (to), and most punctuation marks;9 while these (and other functional elements, e.g. complementizers) are analyzed as semantically vacuous in DM and PSD, they function as predicates in PAS, though do not always serve as ‘loca"
S15-2153,P10-5006,0,0.0554855,"R-arg ACT-arg PAT-arg RSTR EXT RSTR CONJ.m APPS.m ADDR-arg APPS.m CONJ.m A similar technique is almost impossible to apply to other crops , such as cotton , soybeans _ _ _ ev-w218f2 _ _ _ ev-w119f2 _ _ _ _ _ _ _ _ _ CONJ.m and _ rice . _ _ (c) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PSD). Figure 1: Sample semantic dependency graphs for Example (1). sentence’ semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Finally, a third related area of much interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to much work in this tradition, our SDP target representations aim to be task- and domain-independent. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure 1), showing what are called the DM, PAS, and PSD semantic dependencies, there are contentful differences among these annotations, and there is of course not one obvious (o"
S15-2153,J93-2004,0,0.0679778,"stitute of Informatics, Tokyo Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics • Stanford University, Center for the Study of Language and Information ♠ ◦ sdp-organizers@emmtee.net Abstract more general graph processing, to thus enable a more direct analysis of Who did What to Whom? Extending the very similar predecessor task SDP 2014 (Oepen et al., 2014), we make use of three distinct, parallel semantic annotations over the same common texts, viz. the venerable Wall Street Journal (WSJ) and Brown segments of the Penn Treebank (PTB; Marcus et al., 1993) for English, as well as comparable resources for Chinese and Czech. Figure 1 below shows example target representations, bi-lexical semantic dependency graphs in all cases, for the WSJ sentence: Task 18 at SemEval 2015 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic de"
S15-2153,S14-2082,0,0.0634842,"ormation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12 Please see the task web page at the address indicated above for full labeled and unlabeled scores. Team In-House Lisbon Minsk Peking Riga Turku Closed X X X X Open X X X Cross-Lingual Predicate Disambiguation Gold X X X X X X X X X X X Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House Lisbon Minsk Peking Riga Turku grammar-based parsing (Miyao et al., 2014) graph parsing with dual decomposition (Martins & Almeida, 2014) transition-based dependency graph parsing in the spirit of Titov et al. (2009) (Du et al., 2014) extended with weighted tree approximation, parser ensemble (Du et al., 2014)’s graph-to-tree transformation, Mate, C6.0, parser ensemble sequence labeling for argument detection for each predicate, SVM classifiers for top node recognition and sense prediction ERG & Enju companion — — — companion Table 6: Overview of approaches and additional resources used (if any). graphs are converted into tree structures, and wellestablished parsing methods for tree structures are applied to converted structure"
S15-2153,meyers-etal-2004-annotating,0,0.0555434,"the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘full2 In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics top BV ARG3 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-i p:e-u-i n:x n:x _ n:"
S15-2153,S14-2056,1,0.858363,"ndencies, there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. Advancing in-depth comparison of representations and underlying design decisions, in fact, is among the moand other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—often remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inference-based techniques. 916 tivations for the SDP task series. Please see Oepen et al. (2014) and Miyao et al. (2014) for additional background. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation, dubbed DeepBank, of Sections 00–21 of the WSJ Corpus and of selected parts of the Brown Corpus with syntacticosemantic analyses of the LinGO English Resource Grammar (Flickinger, 2000; Flickinger et al., 2012). For this target representation, top nodes designate the highest-scoping (non-quantifier) predicate in the graph, e.g. the (scopal) adverb almost in Figure 1.3 PAS: Enju Predicate–Argument Structures The Enju Treebank and parser4 are derived f"
S15-2153,S14-2008,1,0.363094,"Missing"
S15-2153,J05-1004,0,0.359612,"s preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘full2 In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics top BV ARG3 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-"
S15-2153,P07-1031,0,0.0187644,"ersely, in PSD the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.10 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase– internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PSD, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PSD annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely assigns"
spoustova-etal-2008-validating,W02-1001,0,\N,Missing
spoustova-etal-2008-validating,J08-4004,0,\N,Missing
uresova-etal-2014-multilingual,E12-1012,0,\N,Missing
uresova-etal-2014-multilingual,W12-3102,0,\N,Missing
uresova-etal-2014-multilingual,P02-1040,0,\N,Missing
uresova-etal-2014-multilingual,P07-2045,0,\N,Missing
uresova-etal-2014-multilingual,W04-3250,0,\N,Missing
uresova-etal-2014-multilingual,P03-1021,0,\N,Missing
uresova-etal-2014-multilingual,2011.iwslt-papers.5,0,\N,Missing
W02-2231,J93-2003,0,0.00277477,"t ""dirt"" from the other language as possible. There has been a number of attempts to use syntactic structure of a sentence to do MT; recently, the most succesful one is statistically based (Yamada and Knight, 2001). We propose·here, however, to go to a ""deeper"" level of analysis. 3.1. The Overall Design Fig. 3 shows the overall scheme of a transfer-based approach to machine translation. This triangle-based scheme 16 is currently considered the common scherne of all machine translation systems, whether they are of commercial nature (such as (Flanagan and McClure, 2002)) or of research nature ((Brown et al., 1993), (Knight, 1999)) and regardless oftheir prevailing rnethodology (with the exception ofvery few interlingua-based systems (Cavalli-Sforza et al., 2000)). As Fig. 3 suggests, we propose three essential analysis steps and three generation steps: • Morphological processing; • Analytical (surface syntax) processing; • Tectogrammatical processing (underlying syntax); and, of course, transfer at the top ofthe processing ""triangle"" 17 • An output frorn one step is the input to the following step; thus we have here four representations of the data along the ""up-leading"" as weil as the ""down-leading"" p"
W02-2231,P99-1065,0,0.0197772,"available. 3. Sentence-break errors are rnanually corrected at the analytical layer as well. 4. . And no equivalent rnarkup either. Proceedings .Öf TAG+6 218 < ＣＱＰＨ･ｭ｡ＺＯｴｧｚＭｾＩ＠ Aux.S eekame(lemma:cekat_:T/tag:VB-P-1 P-AA·) Pred politiku(temma:po!itikaltag:NNFS4-A-) Obj (lemma:od/tag:RR-2-) AuxP b v!ady(lemma:vlada/tag:NNFS2-A-) autonomni(lemma:autonomnf/tag:AAFS4-1A-) ekologickou(lemma:ekologickY/tag:AAFS4-1A-) ｾ＠ ｾ＠ ｾ＠ Figure 2: Analytieal annotation (sentence from Fig. 1): fonn, function (+ dependencies, preserved word order) (such as subordinate clause root markup; for more details, see (Collins et al., 1999)). However, there are still many ""technical"" dependencies Ieft - we are here at the level of the smface syntax, and there is often no Jinguistic reason to create a dependency between e.g. an analytical verb form, or a puncwation and everything eise, etc. Coordination and apposition is handled using such ""technical"" dep·C;mdencies, too: the conjunction is the head and the members are its ""dependent"" nodes. Common modifiers of the coordinated structure are also dependents of the coordinating conjunction, but they are not marked as coordinated structure members. This additional ""coordinated struc"
W02-2231,A97-1014,0,0.100888,"Missing"
W02-2231,P01-1067,0,0.0154515,"rors are likely to creep in. We in principle agree with this, since only careful experiments and variety of evaluations must be run to prove or disprove thiS&apos;: We would like to argue at this point, however, that (even though we have not done such convincing experiments yet), intuitively, there must be an advantage ifthe transfer end points are defined at a locally clean information saddle point with as least ""dirt"" from the other language as possible. There has been a number of attempts to use syntactic structure of a sentence to do MT; recently, the most succesful one is statistically based (Yamada and Knight, 2001). We propose·here, however, to go to a ""deeper"" level of analysis. 3.1. The Overall Design Fig. 3 shows the overall scheme of a transfer-based approach to machine translation. This triangle-based scheme 16 is currently considered the common scherne of all machine translation systems, whether they are of commercial nature (such as (Flanagan and McClure, 2002)) or of research nature ((Brown et al., 1993), (Knight, 1999)) and regardless oftheir prevailing rnethodology (with the exception ofvery few interlingua-based systems (Cavalli-Sforza et al., 2000)). As Fig. 3 suggests, we propose three esse"
W02-2231,cavalli-sforza-etal-2000-challenges,0,\N,Missing
W07-1709,W02-1001,0,0.00551292,"ists of the last three characters of wi . Also, we introduce another component P (t∗i |t∗i+1 , t∗i+2 ) based on a reduced tagset T ∗ that contains positions POS, GENDER, NUMBER and CASE only (chosen on linguistic grounds). 1 The optimum value of the scaling parameter λT can be tuned using held-out data. 68 Morˇce The Morˇce2 tagger assumes some of the HMM properties at runtime, namely those that allow the Viterbi algorithm to be used to find the best tag sequence for a given text. However, the transition weights are not probabilities. They are estimated by an Averaged Perceptron described in (Collins, 2002). Averaged Perceptron works with features which describe the current tag and its context. Features can be derived from any information we already have about the text. Every feature can be true or false in a given context, so we can regard current true features as a description of the current tag context. For every feature, the Averaged Perceptron stores its weight coefficient, which is typically an integer number. The whole task of Averaged Perceptron is to sum all the coefficients of true features in a given context. The result is passed to the Viterbi algorithm as a transition weight for a g"
W07-1709,A00-2013,0,0.0490627,"Missing"
W07-1709,W96-0213,0,0.497076,"Missing"
W07-1709,H05-1060,0,0.0142391,"Missing"
W07-1709,P97-1032,0,0.0125657,"Missing"
W07-1709,A94-1008,0,0.0618507,"Missing"
W07-1709,borin-2000-something,0,\N,Missing
W07-1709,W96-0102,0,\N,Missing
W07-1709,C90-2040,0,\N,Missing
W07-1709,P01-1035,1,\N,Missing
W07-1709,P97-1029,0,\N,Missing
W07-1709,E95-1021,0,\N,Missing
W07-1709,P98-1029,0,\N,Missing
W07-1709,C98-1029,0,\N,Missing
W08-0319,bojar-etal-2008-czeng,1,0.701467,"Missing"
W08-0319,W07-0735,1,0.91216,"ectly to produce a grammatical sentence and preserve the expressed relations between elements in the sentence, e.g. verbs and their modifiers. This year, we have taken two radically different approaches to English-to-Czech MT. Section 2 describes our setup of the phrase-based system Moses (Koehn et al., 2007) and Section 3 focuses on a system with probabilistic tree transfer employed at a deep syntactic layer and the new challenges this approach brings. ∗ The work on this project was supported by the grants FP6ˇ IST-5-034291-STP (EuroMatrix), MSM0021620838, MSMT ˇ LC536, and GA405/06/0589. CR Bojar (2007) describes various experiments with factored translation to Czech aimed at improving target-side morphology. We use essentially the same setup with some cleanup and significantly larger target-side training data: Parallel data from CzEng 0.7 (Bojar et al., 2008), with original sentence-level alignment and tokenization. The parallel corpus was taken as a monolithic text source disregarding differences between CzEng data sources. We use only 1-1 aligned sentences. Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. We bas"
W08-0319,A00-1031,0,0.013451,"and significantly larger target-side training data: Parallel data from CzEng 0.7 (Bojar et al., 2008), with original sentence-level alignment and tokenization. The parallel corpus was taken as a monolithic text source disregarding differences between CzEng data sources. We use only 1-1 aligned sentences. Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. We based the word alignment on Czech and English lemmas (base forms of words) as provided by the combination of taggers and lemmatizers by Hajiˇc (2004) for Czech and Brants (2000) followed by Minnen et al. (2001) for English. We symmetrized the two GIZA++ runs using grow-diag-final heuristic. Truecasing. We attempted to preserve meaningbearing case distinctions. The Czech lemmatizer produces case-sensitive lemmas and thus makes it easy to cast the capitalization of the lemma back on the word form.1 For English we approximate the same effect by a two-step procedure.2 1 We change the capitalization of the form to match the lemma in cases where the lemma is lowercase, capitalized (ucfirst) or all-caps. For mixed-case lemmas, we keep the form intact. 2 We first collect a l"
W08-0319,J92-4003,0,0.0509682,"Missing"
W08-0319,P96-1025,0,0.0155031,"sis, both formally captured as labelled ordered dependency trees: the ANALYTICAL (a-, surface syntax) representation bears a 1-1 correspondence between tokens in the sentence and nodes in the tree; the TECTOGRAMMATICAL (t-, deep syntax) representation contains nodes only for autosemantic words and adds nodes for elements not expressed on the surface but required by the grammar (e.g. dropped pronouns). We use the following tools to automatically annoˇ ska, tate plaintext up to the t-layer: (1) TextSeg (Ceˇ 2006) for tokenization, (2) tagging and lemmatization see above, (3) parsing to a-layer: Collins (1996) followed by head-selection rules for English, McDonald and others (2005) for Czech, (4) parsing to tˇ layer: Zabokrtsk´ y (2008) for English, Klimeˇs (2006) for Czech. 3.2 Probabilistic Tree Transfer The transfer step is based on Synchronous Tree Subˇ stitution Grammars (STSG), see Bojar and Cmejrek (2007) for a detailed explanation. The essence is a log-linear model to search for the most likely synchronous derivation δˆ of the source T1 and target T2 dependency trees: δˆ = exp argmax 3 MT with a Deep Syntactic Transfer Theoretical Background said Figure 1: Sample treelet pair, a-layer. δ s."
W08-0319,P05-1067,0,0.0442,"ses, either. Lack of n-gram LM in the (deterministic) generation procedures from a t-tree. While we support final LM-based rescoring, there is too little variance in n-best lists due to the explosion mentioned above. Too many model parameters given our stack limit. We use identical MERT implementation to optimize λm s but in the large space of hypotheses, MERT does not converge. 3.3.2 Related Research Our approach should not be confused with the ˇ TectoMT submission by Zdenˇek Zabokrtsk´ y with a deterministic transfer: heuristics fully exploiting the similarity of English and Czech t-layers. Ding and Palmer (2005) improve over word-based MT baseline with a formalism very similar to STSG. Though not explicitly stated, they seem not to encode frontiers in the treelets and allow for adjunction (adding siblings), like Quirk et al. (2005), which significantly reduces data sparseness. Riezler and III (2006) report an improvement in MT grammaticality on a very restricted test set: short sentences parsable by an LFG grammar without back-off rules. 4 Conclusion We have presented our best-performing factored phrase-based English-to-Czech translation and a highly experimental complex system with treebased transfe"
W08-0319,2006.amta-papers.8,0,0.016828,"based MT on WMT07 DevTest. Moses Moses, CzEng data only etct, TectoMT annotation WMT07 DevTest 14.9±0.9 13.9±0.9 4.7±0.5 WMT08 NC Test News Test 16.4±0.6 12.3±0.6 15.2±0.6 10.0±0.5 4.9±0.3 3.3±0.3 Table 2: WMT08 shared task BLEU scores. rules for t-layer parsing and generation instead of ˇ Klimeˇs (2006) and (Pt´acˇ ek and Zabokrtsk´ y, 2006). 3.3.1 Discussion Our syntax-based approach does not reach scores of phrase-based MT due to the following reasons: Cumulation of errors at every step of analysis. Data loss due to incompatible parses and node alignment. Unlike e.g. Quirk et al. (2005) or Huang et al. (2006) who parse only one side and project the structure, we parse both languages independently. Natural divergence and random errors in either of the parses and/or the alignment prevent us from extracting many treelet pairs. Combinatorial explosion in target node attributes. Currently, treelet options are fully built in advance. Uncertainty in the many t-node attributes leads to too many insignificant variations while e.g. different lexical choices are pushed off the stack. While vital for final sentence generation (see Table 1), fine-grained t-node attributes should be produced only once all key s"
W08-0319,D07-1091,0,0.0615314,"d right away or a binode model promoting likely combinations of the governor g(e) and the child c(e) of an edge e ∈ T2 : hbinode (δ) = log Y p(c(e) |g(e)) (3) e∈T2 The probabilistic dictionary of aligned treelet pairs is extracted from node-aligned (GIZA++ on linearized trees) parallel automatic treebank as in Moses’ training: all treelet pairs compatible with the node alignment. 3.2.1 Factored Treelet Translation Labels of nodes at the t-layer are not atomic but consist of more than 20 attributes representing various linguistic features.3 We can consider the attributes as individual factors (Koehn and Hoang, 2007). This allows us to condition the translation choice on a subset of source factors only. In order to generate a value for each target-side factor, we use a sequence of mapping steps similar to Koehn and Hoang (2007). For technical reasons, our current implementation allows to generate factored targetside only when translating a single node to a single node, i.e. preserving the tree structure. In our experiments we used 8 source (English) tnode attributes and 14 target (Czech) attributes. 3.3 Recent Experimental Results Table 1 shows BLEU scores for various configurations of our decoder. The ab"
W08-0319,P07-2045,1,0.0205542,"d right away or a binode model promoting likely combinations of the governor g(e) and the child c(e) of an edge e ∈ T2 : hbinode (δ) = log Y p(c(e) |g(e)) (3) e∈T2 The probabilistic dictionary of aligned treelet pairs is extracted from node-aligned (GIZA++ on linearized trees) parallel automatic treebank as in Moses’ training: all treelet pairs compatible with the node alignment. 3.2.1 Factored Treelet Translation Labels of nodes at the t-layer are not atomic but consist of more than 20 attributes representing various linguistic features.3 We can consider the attributes as individual factors (Koehn and Hoang, 2007). This allows us to condition the translation choice on a subset of source factors only. In order to generate a value for each target-side factor, we use a sequence of mapping steps similar to Koehn and Hoang (2007). For technical reasons, our current implementation allows to generate factored targetside only when translating a single node to a single node, i.e. preserving the tree structure. In our experiments we used 8 source (English) tnode attributes and 14 target (Czech) attributes. 3.3 Recent Experimental Results Table 1 shows BLEU scores for various configurations of our decoder. The ab"
W08-0319,H05-1066,1,0.769471,"Missing"
W08-0319,C00-2163,0,0.0100666,"ˇ IST-5-034291-STP (EuroMatrix), MSM0021620838, MSMT ˇ LC536, and GA405/06/0589. CR Bojar (2007) describes various experiments with factored translation to Czech aimed at improving target-side morphology. We use essentially the same setup with some cleanup and significantly larger target-side training data: Parallel data from CzEng 0.7 (Bojar et al., 2008), with original sentence-level alignment and tokenization. The parallel corpus was taken as a monolithic text source disregarding differences between CzEng data sources. We use only 1-1 aligned sentences. Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. We based the word alignment on Czech and English lemmas (base forms of words) as provided by the combination of taggers and lemmatizers by Hajiˇc (2004) for Czech and Brants (2000) followed by Minnen et al. (2001) for English. We symmetrized the two GIZA++ runs using grow-diag-final heuristic. Truecasing. We attempted to preserve meaningbearing case distinctions. The Czech lemmatizer produces case-sensitive lemmas and thus makes it easy to cast the capitalization of the lemma back on the word form.1 For English we approxim"
W08-0319,P03-1021,0,0.0173091,"weighted language models for the target (Czech) side: • 3-grams of word forms based on all CzEng 0.7 data, 15M tokens, • 3-grams of word forms in Project Syndicate section of CzEng (in-domain for WMT07 and WMT08 NC-test set), 1.8M tokens, • 4-grams of word forms based on Czech National Corpus (Kocek et al., 2000), version SYN2006, 365M tokens, • three models of 7-grams of morphological tags from the same sources. Lexicalized reordering using the monotone/swap/discontinuous bidirectional model based on both source and target word forms. MERT. We use the minimum-error rate training procedure by Och (2003) as implemented in the Moses toolkit to set the weights of the various translation and language models, optimizing for BLEU. Final detokenization is a simple rule-based procedure based on Czech typographical conventions. Finally, we capitalize the beginnings of sentences. See BLEU scores in Table 2 below. Pred VP = Sb uvedla , zˇ e Pred Czech has a well-established theory of linguistic analysis called Functional Generative Description (Sgall et al., 1986) supported by a big treebanking enterprise (Hajiˇc and others, 2006) and on-going adaptations for other languages including English (Cinkov´a"
W08-0319,P05-1034,0,0.258943,"BLEU scores for syntax-based MT on WMT07 DevTest. Moses Moses, CzEng data only etct, TectoMT annotation WMT07 DevTest 14.9±0.9 13.9±0.9 4.7±0.5 WMT08 NC Test News Test 16.4±0.6 12.3±0.6 15.2±0.6 10.0±0.5 4.9±0.3 3.3±0.3 Table 2: WMT08 shared task BLEU scores. rules for t-layer parsing and generation instead of ˇ Klimeˇs (2006) and (Pt´acˇ ek and Zabokrtsk´ y, 2006). 3.3.1 Discussion Our syntax-based approach does not reach scores of phrase-based MT due to the following reasons: Cumulation of errors at every step of analysis. Data loss due to incompatible parses and node alignment. Unlike e.g. Quirk et al. (2005) or Huang et al. (2006) who parse only one side and project the structure, we parse both languages independently. Natural divergence and random errors in either of the parses and/or the alignment prevent us from extracting many treelet pairs. Combinatorial explosion in target node attributes. Currently, treelet options are fully built in advance. Uncertainty in the many t-node attributes leads to too many insignificant variations while e.g. different lexical choices are pushed off the stack. While vital for final sentence generation (see Table 1), fine-grained t-node attributes should be produ"
W08-0319,N06-1032,0,0.311055,"Missing"
W09-1201,burchardt-etal-2006-salsa,1,0.483589,"Missing"
W09-1201,D07-1101,0,0.391748,"Missing"
W09-1201,W09-1202,0,0.0278745,"order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the different subtasks of the CoNLL shared task. The idea is to decompose the joint learning problem into four subtasks – syntactic dependency identification, syntactic dependency labeling, semantic dependency identification and semantic dependency labeling. The initial step is to use a pipeline approach to use the input of one subtask as input to the next, in the order specified. The iterative steps then use additional features that are not available in the initial step to improve the accuracy of the overall system. For ex"
W09-1201,W09-1205,0,0.222475,"al token; and (c), the existence of an edge between each pair of tokens. Subsequently, they combine the (possibly conflicting) output of the three classifiers by a ranking approach to determine the most likely structure that meets all well-formedness constraints. (Llu´ıs et al., 2009) present a joint approach based on an extension of Eisner’s parser to accommodate also semantic dependency labels. This architecture is similar to the one presented by the same authors in the past edition, with the extension to a second-order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the differen"
W09-1201,W09-1212,1,0.83271,"Missing"
W09-1201,S07-1008,1,0.697359,"Missing"
W09-1201,H05-1066,1,0.168004,"Missing"
W09-1201,W04-2705,1,0.527791,"Missing"
W09-1201,W09-1219,0,0.0294123,"Missing"
W09-1201,H05-1108,1,0.506508,"y, adding further manual labels where necessary. Then, we used frequency and grammatical realization information to map the remaining roles onto higher-numbered Arg roles. We considerably simplified the annotations provided by SALSA, which use a rather complex annotation scheme. In particular, we removed annotation for multi-word expressions (which may be non-contiguous), annotations involving multiple frames for the same predicate (metaphors, underspecification), and inter-sentence roles. The out-of-domain dataset was taken from a study on the multi-lingual projection of FrameNet annotation (Pado and Lapata, 2005). It is sampled from the EUROPARL corpus and was chosen to maximize the lexical coverage, i.e., it contains of a large number of infrequent predicates. Both syntactic and semantic structure were annotated manually, in the TIGER and SALSA format, respectively. Since it uses a simplified annotation schemes, we did not have to discard any annotation. For both datasets, we converted the syntactic TIGER (Brants et al., 2002) representations into dependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con1"
W09-1201,C08-1085,1,0.175934,"Missing"
W09-1201,J05-1004,0,0.213522,"nnotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation. For the CoNLL-2008 shared task evaluation, this corpus was extended by the task organizers to cover the subset of the Brown corpus used as a secondary testing dataset. From this corpus we only used NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. • Proposition Bank I (PropBank) – The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (Arg0, Arg1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types of adjuncts (ArgM-TMP, -ADV, etc.). • NomBank – NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun"
W09-1201,E09-1087,1,0.646818,"Missing"
W09-1201,W08-2121,1,0.597132,"Missing"
W09-1201,taule-etal-2008-ancora,0,0.543017,"Missing"
W09-1201,cmejrek-etal-2004-prague,1,0.63993,"Missing"
W09-1201,kawahara-etal-2002-construction,1,\N,Missing
W09-1201,J93-2004,0,\N,Missing
W09-1201,D07-1096,1,\N,Missing
W14-2902,W09-1201,1,0.891298,"Missing"
W14-2902,C12-1015,0,0.0260406,"Missing"
W14-2902,W09-1206,0,0.0992918,"Missing"
W14-2902,hajic-etal-2012-announcing,1,0.858905,"Missing"
W14-2902,bojar-etal-2012-joy,1,0.896852,"Missing"
W14-2902,burchardt-etal-2006-salsa,0,0.085696,"Missing"
W14-2902,I05-1081,0,0.447373,"Missing"
W14-2902,W04-3250,0,0.0155595,"Missing"
W14-2902,E12-1085,0,0.0220204,"Missing"
W14-2902,J93-2004,0,0.0462079,"nd Applied Linguistics, Charles University in Prague. It serves two main purposes: 1. to test and validate the FGD linguistic theory, 2. to apply and test machine learning methods for part-of-speech and morphological tagging, dependency parsing, semantic role labeling, coreference resolution, discourse annotation, natural language generation, machine translation and other natural language processing tasks. The language data in the PDT are non-abbreviated articles from Czech newspapers and journals. The PCEDT contains English sentences from the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993, PTB-WSJ) and their Czech translations, all annotated using the same theoretical framework as the PDT. The annotation of the PDT and the PCEDT is very rich in linguistic information. Following the stratificational approach of the FGD, the texts are annotated at different but interlinked layers. There are four such layers, two linear and two structured: 2 Attributes such as tense are annotated automatically, and most advanced information such as topic and focus annotation is not present. 7 4 PropBank frame files (Palmer et al., 2005) and by subsequent manual refinement.3 EngVallex was used for"
W14-2902,cinkova-2006-propbank,0,0.724884,"ed, indexed, and sorted. Each valency frame includes the frame’s “valency” (number of arguments, or frame members) and the following information for each argument: • its label (see Section 2.1), • its (semantic) obligatoriness according to Panevová (1974)’s dialogue test, • its required surface form (or several alternative forms) typically using morphological, lexical and syntactic constraints. Most valency frames are further accompanied by a note or an example which explains their meaning and usage. The version of PDT-Vallex used here contains 9,191 valency frames for 5,510 verbs. EngVallex (Cinková, 2006) is a valency lexicon of English verbs based on the FGD framework, created by an automatic conversion from Tectogrammatical annotation The PDT is a project for FGD-based manual annotation of Czech texts, started in 1996 at the Institute of Formal and Applied Linguistics, Charles University in Prague. It serves two main purposes: 1. to test and validate the FGD linguistic theory, 2. to apply and test machine learning methods for part-of-speech and morphological tagging, dependency parsing, semantic role labeling, coreference resolution, discourse annotation, natural language generation, machine"
W14-2902,J05-1004,0,0.105323,"es from the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993, PTB-WSJ) and their Czech translations, all annotated using the same theoretical framework as the PDT. The annotation of the PDT and the PCEDT is very rich in linguistic information. Following the stratificational approach of the FGD, the texts are annotated at different but interlinked layers. There are four such layers, two linear and two structured: 2 Attributes such as tense are annotated automatically, and most advanced information such as topic and focus annotation is not present. 7 4 PropBank frame files (Palmer et al., 2005) and by subsequent manual refinement.3 EngVallex was used for the tectogrammatical annotation of the English part of the PCEDT. Currently, it contains 7,699 valency frames for 4,337 verbs. 3 Experiments We evaluated the system described in Section 3 on PDT 2.5 for Czech and on the English part of PCEDT 2.0 for English. From PCEDT 2.0, whose division follows the PTB-WSJ, we used Sections 02-21 as training data, Section 24 as development data, and Section 23 as evaluation data. Since the system is intended to be used in a fully automatic annotation scenario, we use automatically parsed sentences"
W14-2902,W12-3132,1,0.919077,"Missing"
W14-2902,S01-1001,0,0.0529322,"Missing"
W14-2902,W12-3146,1,0.834698,"Missing"
W14-2902,J08-2004,0,0.0549514,"Missing"
W14-2902,W08-0325,0,0.0292435,"Missing"
W14-2902,W08-2121,0,\N,Missing
W14-3326,D11-1033,0,0.426651,"ain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the standard task (“general dom"
W14-3326,2011.iwslt-evaluation.18,0,0.0458693,"ction 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et"
W14-3326,bojar-etal-2012-joy,1,0.843764,"Missing"
W14-3326,N13-1073,0,0.0271109,"s are trained on the monolingual data in the target language (constrained or unconstrained, depending on the setting). The general-domain models are trained on the WMT News data. Compared to the approach of Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables"
W14-3326,C04-1114,0,0.358032,", 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provi"
W14-3326,E12-3006,0,0.0294557,"Missing"
W14-3326,2005.eamt-1.19,0,0.105464,"nd Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the stan"
W14-3326,2011.iwslt-papers.5,0,0.0956886,"aining phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the standard task (“general domain” here is used to denote data Statistical"
W14-3326,P10-2041,0,0.508413,") or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general"
W14-3326,W08-0320,0,0.122804,"paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; M"
W14-3326,W07-0733,0,0.0769909,"work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation mo"
W14-3326,P03-1021,0,0.0285359,"Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expected, the unconstrained systems outperform the constrained ones. Linear interpolation outperforms data concatenation quite reliably"
W14-3326,P07-2045,0,0.00503854,"ined or unconstrained, depending on the setting). The general-domain models are trained on the WMT News data. Compared to the approach of Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expecte"
W14-3326,2011.mtsummit-plenaries.5,0,0.0420922,"xts of nonmedical patents in the PatTR collection. Parallel data The parallel data summary is presented in Table 1. The main sources of the medical-domain data for all the language pairs include the EMEA corpus (Tiedemann, 2009), the UMLS metathesaurus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12"
W14-3326,2005.mtsummit-papers.11,0,0.0172802,"and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 4 https://www.hon.ch/ https://sites.google.com/site/ shareclefehealth/ 5 223 10 10 5 5 0 0 −5 −5 −10 −10 −15 −15 15 10 general 5 0 −5 −10 15 constrained 15 unconstrained medical unconstrained constrained 15 −15 Figure 1: Distribution of the domain-specificity s"
W14-3326,C10-2124,0,0.432112,"arch queries and document summaries. Section 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt l"
W14-3326,W02-1405,0,0.131062,"ranslation of search queries and document summaries. Section 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training co"
W14-3326,E12-1055,0,0.0150313,"n con unc unc concat interpol concat interpol cs→en 30.87±4.70 32.46±5.05 34.88±5.04 33.82±5.16 de→en 33.21±5.03 33.74±4.97 31.24±5.59 34.19±5.27 en→cs 23.25±4.85 21.56±4.80 22.61±4.91 23.93±5.16 en→de 17.72±4.75 16.90±4.39 19.13±5.66 15.87±11.31 en→fr 28.64±3.77 29.34±3.73 33.08±3.80 31.19±3.73 fr→en 35.56±4.94 35.28±5.26 36.73±4.88 40.25±5.14 Table 4: BLEU scores of query translations. each section and use linear interpolation to combine them into a single model. For language models, we use the SRILM linear interpolation feature (Stolcke, 2002). We interpolate phrase tables using Tmcombine (Sennrich, 2012). In both cases, the held-out set for minimizing the perplexity is the system development set. The two language models for sentence scoring are trained with a restricted vocabulary extracted from the in-domain training data as words occurring at least twice (singletons and other words are treated as out-of-vocabulary). In our experiments, we apply this technique to select both monolingual data for language models and parallel data for translation models. Selection of parallel data is based on the English side only. The in-domain models are trained on the monolingual data in the target language"
W14-3326,P13-1135,0,0.0121052,"rus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 4 https://www.hon.ch/ https://sites.google.com/site/ shareclefehealth/ 5 223 10 10 5 5 0 0 −5 −5 −10 −10 −15 −15 15 10 general 5 0 −5 −10 15 constrained 15 unconstrained medical unconstrained constrained 15 −15 Figure 1: Distri"
W14-3326,wu-wang-2004-improving-domain,0,0.0358057,"approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et"
W14-3326,W12-3151,1,0.842853,"difference) in the FR–EN parallel data and FR monolingual data is illustrated in Figures 1 and 2, respectively.6 The scores (Y axis) are presented for each sentence in increasing order from left to right (X axis). The rest of the preprocessing procedure was applied to all the datasets mentioned above, both parallel and monolingual. The data were tokenized and normalized by converting or omitting some (mostly punctuation) characters. A set of language-dependent heuristics was applied in an attempt to restore and normalize the opening/closing quotation marks, i.e. convert ""quoted"" to “quoted” (Zeman, 2012). The motivation here is twofold: First, we hope that paired quotation marks could occasionally work as brackets and better denote parallel phrases for Moses; second, if Moses learns to output directed quotation marks, the subsequent detokenization will be easier. For all systems which translate from German, decompounding is employed to reduce source-side data sparsity. We used BananaSplit for this task (M¨uller and Gurevych, 2006). We perform all training and internal evaluation on lowercased data; we trained recasers to postprocess the final submissions. 6 For the medical domain, constrained"
W14-3326,W12-3102,0,\N,Missing
W14-3326,eck-etal-2004-language,0,\N,Missing
W14-5808,Q13-1034,1,0.823033,"Missing"
W14-5808,A00-2018,0,0.0661509,"Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (H"
W14-5808,J02-3001,0,0.013808,"nnotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and TimeBank has fueled much research in the area of temporal analysis. There have been efforts to create"
W14-5808,J93-2004,0,0.0452856,"also present quantitative comparison on 100 parallel sentences, for all the aforementioned categories and some of their subtypes. We will first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Ro"
W14-5808,W04-2705,0,0.050433,"first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al.,"
W14-5808,J05-1004,0,0.0825818,"ome of their subtypes. We will first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue an"
W14-5808,N07-1051,0,0.0137229,"anks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and"
W14-5808,N04-1030,0,0.050937,"opBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and TimeBank has fueled much research in the area of temporal analysis. There have been efforts to create a unified representati"
W14-5808,prasad-etal-2008-penn,0,0.0548286,"resent the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are a"
W14-5808,sindlerova-etal-2014-resources,1,0.82705,"nk leaves out ARG0 e.g. for unaccusative verbs (for example The window.ARG1 broke vs. Okno.ARG0←ACT/window se/itself rozbilo/broke.). Finally, some differences are due to some arguments not being considered arguments at all in the other language, in which case some other AMR label is used instead (for example, We could have spent 400M.ARG3 ... elsewhere vs. ... mohli/could utratit/spend 400M.extent ... jinde/elsewhere). These differences could possibly be consolidated (only) by carefully linking the two lexicons (with AMR guidelines intact). This is in fact being performed in another project (Sindlerova et al., 2014), but it is a daunting manual task, since the underlying theories behind PropBank and PDT-Vallex/EngVallex differ. However, one has to ask if it does make sense to do so, because with enough parallel data available, the mappings can be learned relatively easily: in most cases, no structural differences are involved and there will be a simple one-to-one mapping between the labels (conditioned on the particular verb sense). 4.4 Structural Differences Local differences can be safely ignored, since they will be in most cases resolved during the assumed process of wikification, i.e., linking to an"
W14-5808,Q13-1019,0,0.0125518,"ibution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and TimeBank has fueled much research in the area of temporal analysis. There have been efforts to create a unified representation which would cover at least a whole sentence, or even a continuous text (Hajiˇc et al., 2003; Srikumar and Roth, 2013), and currently the Abstract Meaning Representation represents an attempt to provide a common ground for truly semantic and fully covering annotation representation. An Abstract Meaning Representation is a rooted, directional and labeled graph that represents the meaning of a sentence and it abstracts away from such syntactic notions as word category (verbs and nouns), word order, morphological variation etc. Instead, it focuses on semantic relations between concepts and makes heavy use of predicate-argument structures as defined in PropBank (for English). As a result, the word order in the se"
W14-5808,W04-3212,0,0.0235348,"2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and TimeBank has fueled much research in the area of temporal analysis. There have been efforts to create a unified representation which would cover a"
W14-5808,xue-etal-2014-interlingua,1,0.234271,"ions into Czech, annotated manually by AMRs, with the goal to describe the differences and if possible, to classify them into two main categories: those which are merely convention differences and thus can be unified by changing such conventions in the AMR annotation guidelines, and those which are so deeply rooted in the language structure that the level of abstraction which is inherent in the current AMR scheme does not allow for such unification. 1 Introduction In this paper, we follow on a previous first exploratory investigation of differences in AMR annotation among different languages (Xue et al., 2014), which has classified the similarities and differences into four categories: (a) no difference, (b) local difference only (such as multiword expressions vs. single word terms), (c) reconcilable difference due to AMR conventions, and (d) deep differences which cannot be unified in the AMR guidelines. In this paper, we would like to elaborate especially on the (b) and (c) types, which have been only exemplified in the previous work. In this paper, we would like to not only go deeper, but also present quantitative comparison on 100 parallel sentences, for all the aforementioned categories and so"
W14-5808,J03-4003,0,\N,Missing
W14-5808,W09-1201,1,\N,Missing
W14-5808,W13-2322,0,\N,Missing
W15-1613,ahrenberg-etal-2002-system,0,0.0556281,"able graphical editor and viewer for any tree-like structures. It allows displaying and editing sentential tree structures annotated on multiple linguistic layers. The new CzEngVallex TrEd extension uses the data format of the Treex NLP 7 Frame ID ev-w1f2, which has been created from abandon.02 in the PropBank, as in Noriega abandoned command ... for an exile. 8 These papers describe only a pilot experiment; the current process differs from their suggestions in several substantial respects. 9 There are other environments for manual alignment, such as (Melamed, 1998; Samuelsson and Volk, 2007; Ahrenberg et al., 2002), but they work on plain text or phrases, not dependency trees. 126 Figure 2: Highlighted alignment in the annotation tool TrEd; color-coding: green for verbs, blue for arguments/adjuncts framework (Žabokrtský, 2011; Popel and Žabokrtský, 2010) and pre-existing TrEd extensions for PCEDT, PDTVallex, and EngVallex. The annotation interface includes keyboard macros to change values of individual attributes or to add or delete whole nodes from the structure. Links between English and Czech nodes are added or changed in a drag-and-drop fashion. 4.2.3 Manual Annotation Workflow The environment descr"
W15-1613,1999.tmi-1.21,0,0.137082,"Missing"
W15-1613,sindlerova-bojar-2010-building,1,0.781977,", there are also many verb–non-verb or non-verb–verb pairs, which have been left aside for this first version of CzEngVallex as none of the underlying lexicons include a complete description of other parts-of-speech. 4.2 The Annotation Process During the actual annotation process, we have manually aligned English and Czech verbs and their arguments (and in some clear cases also adjuncts). After carefully checking all occurrences of any given valency frame pair in the PCEDT, we included it in CzEngVallex using the structure described in Sect. 4.1, which is based on (Šindlerová and Bojar, 2009; Bojar and Šindlerová, 2010).8 The process is helped by automatic preprocessing steps. 4.2.1 Preprocessing and Data Preparation The following steps had been taken before the manual annotation proper started: • automatic pre-alignment using GIZA++ word alignment (Och and Ney, 2003) and a projection to deep dependency trees (taken from the original PCEDT); • grouping the occurrences of the same verb sense pairs together to simplify annotation. 4.2.2 Annotation Environment The annotation interface for manual valency frame alignment9 has been built as an extension of the TrEd annotation environment (Pajas and Fabian, 2011)."
W15-1613,cinkova-2006-propbank,0,0.321601,"ACT"" cs_functor=""ACT""/&gt; &lt;slot en_functor=""PAT"" cs_functor=""PAT""/&gt; &lt;slot en_functor=""EFF"" cs_functor=""SUBS""/&gt; &lt;/slots&gt; &lt;/frame_pair&gt; &lt;/en_frame&gt; &lt;/valency_word&gt; &lt;/body&gt; &lt;/frames_pairs&gt; Figure 1: Structure of CzEngVallex (part of abandon pairing) verbs and frames come mostly from the data appearing in the latest versions of the PDT and PCEDT. 3.3 EngVallex – The English Valency Lexicon EngVallex has been created by a (largely manual) adaptation of an already existing similar resource for English, the PropBank (Kingsbury and Palmer, 2002), to the FGD valency format and to PDT labeling standards (Cinková, 2006). During the adaptation process, arguments were re-labeled, obligatoriness was marked for each valency slot and frames with identical meaning were merged (and some split as well). Links to the original PropBank frame file and roleset have been kept wherever possible. EngVallex was used for the annotation of the English part of the PCEDT. It contains 7,148 valency frames for 4,337 verbs. 4 Building CzEngVallex 4.1 Structure of CzEngVallex CzEngVallex builds on all the resources mentioned in Sect. 3. It connects pairs of valency frames in the PCEDT (verb senses) which are translations of each ot"
W15-1613,2004.tmi-1.6,0,0.10371,"Missing"
W15-1613,W04-2206,0,0.0927102,"Missing"
W15-1613,W06-2705,0,0.0282032,"lex builds on all the resources mentioned in Sect. 3. It connects pairs of valency frames in the PCEDT (verb senses) which are translations of each other, aligning their arguments as well. This resource cannot be used independently, since it refers to the valency frame descriptions contained in both PDT-Vallex and EngVallex, and it also relies on the PCEDT. The structure of this new resource, which is technically a single XML file, is shown in Fig. 1.6 Aligned pairs of verb frames are grouped by the English verb frame (&lt;en_frame&gt;), and for each English verb sense, 6 Similar scheme is used in (Hansen-Schirra et al., 2006). their Czech counterparts are listed (&lt;frame_pair&gt;). For each of such pairs, all the aligned valency slots are listed and referred to by the functor assigned to the slot in the respective valency lexicon. In this example, for the pair abandon7 – opustit (lit. leave [alone]) the first two arguments match perfectly (ACT:ACT, PAT:PAT) and the third argument in English (EFF) does not match any argument for this particular Czech counterpart, while for the pair abandon – zˇríci se (lit. get rid of [for sth]), the third English argument maps to a Czech adjunct (SUBS, substitution). It must be noted"
W15-1613,kingsbury-palmer-2002-treebank,0,0.116501,"/slots&gt; &lt;/frame_pair&gt; &lt;frame_pair id=... cs_id=""v-w9887f1""&gt; &lt;slots&gt; &lt;slot en_functor=""ACT"" cs_functor=""ACT""/&gt; &lt;slot en_functor=""PAT"" cs_functor=""PAT""/&gt; &lt;slot en_functor=""EFF"" cs_functor=""SUBS""/&gt; &lt;/slots&gt; &lt;/frame_pair&gt; &lt;/en_frame&gt; &lt;/valency_word&gt; &lt;/body&gt; &lt;/frames_pairs&gt; Figure 1: Structure of CzEngVallex (part of abandon pairing) verbs and frames come mostly from the data appearing in the latest versions of the PDT and PCEDT. 3.3 EngVallex – The English Valency Lexicon EngVallex has been created by a (largely manual) adaptation of an already existing similar resource for English, the PropBank (Kingsbury and Palmer, 2002), to the FGD valency format and to PDT labeling standards (Cinková, 2006). During the adaptation process, arguments were re-labeled, obligatoriness was marked for each valency slot and frames with identical meaning were merged (and some split as well). Links to the original PropBank frame file and roleset have been kept wherever possible. EngVallex was used for the annotation of the English part of the PCEDT. It contains 7,148 valency frames for 4,337 verbs. 4 Building CzEngVallex 4.1 Structure of CzEngVallex CzEngVallex builds on all the resources mentioned in Sect. 3. It connects pairs of va"
W15-1613,J03-1002,0,0.00559637,"annotation process, we have manually aligned English and Czech verbs and their arguments (and in some clear cases also adjuncts). After carefully checking all occurrences of any given valency frame pair in the PCEDT, we included it in CzEngVallex using the structure described in Sect. 4.1, which is based on (Šindlerová and Bojar, 2009; Bojar and Šindlerová, 2010).8 The process is helped by automatic preprocessing steps. 4.2.1 Preprocessing and Data Preparation The following steps had been taken before the manual annotation proper started: • automatic pre-alignment using GIZA++ word alignment (Och and Ney, 2003) and a projection to deep dependency trees (taken from the original PCEDT); • grouping the occurrences of the same verb sense pairs together to simplify annotation. 4.2.2 Annotation Environment The annotation interface for manual valency frame alignment9 has been built as an extension of the TrEd annotation environment (Pajas and Fabian, 2011). TrEd is a fully customizable and programmable graphical editor and viewer for any tree-like structures. It allows displaying and editing sentential tree structures annotated on multiple linguistic layers. The new CzEngVallex TrEd extension uses the data"
W15-1613,sindlerova-etal-2014-resources,1,0.893522,"Missing"
W15-2111,W09-1206,0,0.0853316,"Missing"
W15-2111,W14-2902,1,0.890401,"Missing"
W15-2111,1992.tmi-1.8,0,0.471548,"valency frame in the associated valency lexicon, effectively providing verbal word sense labeling. The parallel Prague Czech-English Dependency Treebank 2.0 (PCEDT 2.0) (Hajiˇc et al., 2012) has been annotated using the same principles as the PDT, providing us with manually disambiguated verb senses on both the Czech and the English side. The texts are disjoint from the PDT; PCEDT contains the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) and its Introduction Using parallel data for Word Sense Disambiguation (WSD) is as old as Statistical Machine Translation (SMT): Brown et al. (1992) analyze texts in both languages before the IBM SMT models are trained and used, including WSD driven purely by translation equivalents.1 A combination of parallel texts and lexicons also proved useful for SMT at the time (Brown et al., 1993). In our previous experiments (Dušek et al., 2014), we have shown that WSD based on a manually created valency lexicon (for verbs) can achieve encouraging results. Combining the above ideas and previous findings with parallel data and a manually created bilingual valency lexicon, we have moved to add bilingual 1 Given the “automatic” nature of the word sen"
W15-2111,S01-1001,0,0.0274924,"Missing"
W15-2111,H93-1039,0,0.448683,"Missing"
W15-2111,I05-1081,0,0.0862138,"Missing"
W15-2111,W09-1201,1,0.803063,"Missing"
W15-2111,J93-2004,0,0.0496536,"pts on the deep layer; for the purpose of our experiments, it is important that the deep layer links each verb node (occurrence) to the corresponding valency frame in the associated valency lexicon, effectively providing verbal word sense labeling. The parallel Prague Czech-English Dependency Treebank 2.0 (PCEDT 2.0) (Hajiˇc et al., 2012) has been annotated using the same principles as the PDT, providing us with manually disambiguated verb senses on both the Czech and the English side. The texts are disjoint from the PDT; PCEDT contains the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) and its Introduction Using parallel data for Word Sense Disambiguation (WSD) is as old as Statistical Machine Translation (SMT): Brown et al. (1992) analyze texts in both languages before the IBM SMT models are trained and used, including WSD driven purely by translation equivalents.1 A combination of parallel texts and lexicons also proved useful for SMT at the time (Brown et al., 1993). In our previous experiments (Dušek et al., 2014), we have shown that WSD based on a manually created valency lexicon (for verbs) can achieve encouraging results. Combining the above ideas and previous findin"
W15-2111,hajic-etal-2012-announcing,1,0.809152,"Missing"
W15-2111,H05-1066,1,0.611891,"Missing"
W15-2111,C14-1003,0,0.0380015,"Missing"
W15-2111,A00-2013,0,0.174257,"Missing"
W15-2111,J03-1002,0,0.00589822,"in the IBM Candide SMT system had been given in the Brown et al. (1992) paper. 2 3 http://hunch.net/~vw http://ufal.mff.cuni.cz/pdt2.0 82 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 82–90, Uppsala, Sweden, August 24–26 2015. radit2 ACT(1) PAT(4;k+3;aby) ADDR(3) help1 ACT() PAT() ADDR() Figure 1: Valency frame examples from PDTVallex and EngVallex (Czech radit = ‘give advice, help’). translation into Czech. Sentences have been manually aligned during the human translation process, and words have been then aligned automatically using GIZA++ (Och and Ney, 2003). We have used valency frame annotation (and other features) of the PCEDT 2.0 in our previous work; however, billingual alignment information has not been used before. 2.2 Figure 2: PCEDT trees aligned using the CzEngVallex mapping Valency lexicons PDT-Vallex4 (Hajiˇc et al., 2003; Urešová, 2011) is a valency lexicon of Czech verbs (and nouns), manually created during the annotation of the PDT/PCEDT 2.0. Each entry in the lexicon contains a headword (lemma), according to which the valency frames (i.e., senses) are grouped. Each valency frame includes the valency frame members and the following"
W15-2111,J05-1004,0,0.0239311,"PAT, ADDR, EFF, ORIG, TWHEN, LOC, CAUS (actor, patient, addressee, effect, origin, time, location, cause),5 • its semantic “obligatoriness” attribute, • subcategorization: its required surface form(s) using morphosyntactic and lexical constraints. Most valency frames are further accompanied by a note or an example which explains their meaning and usage. The version of PDT-Vallex used here contains 11,933 valency frames for 7,121 verbs. EngVallex6 (Cinková, 2006) is a valency lexicon of English verbs based also on the FGD framework, created by an automatic conversion from PropBank frame files (Palmer et al., 2005) and subsequent manual refinement.7 EngVallex was used for the annotation of the English part of the PCEDT 2.0. Currently, it contains 7,148 valency frames for 4,337 verbs. EngVallex does not contain the explicitly formalized subcategorization information. 2.3 CzEngVallex: Valency lexicon mapping CzEngVallex (Urešová et al., 2015a; Urešová et al., 2015b) is a manually annotated Czech-English valency lexicon linking the Czech and English valency lexicons, PDT-Vallex and EngVallex. It contains 19,916 frame (verb sense) pairs. CzEngVallex builds links not only between corresponding frames but als"
W15-2111,W12-4205,1,0.894553,"Missing"
W15-2111,W02-0808,0,0.157321,"Missing"
W15-2111,P12-1073,0,0.0646998,"Missing"
W15-2111,W04-3250,0,0.0177928,"increase its usefulness to the classifier. 3.4 Results The results of the individual settings are given in Tables 1 and 2. The figures include the sense detection F-measure in an unlabeled (just detecting a verb occurrence whose sense must be inferred) and labeled setting (also selecting the correct sense) as well as the accuracy of the sense detection alone (in total and in ambiguous verbs with two or more senses). We can see that just using the Vowpal Wabbit classifier with the same features provides a substantial performance boost. The aligned lemma 12 We used paired bootstrap resampling (Koehn, 2004) with 1,000 resamples to assess statistical significance. 85 PCEDT annotation error. On the whole, the positive effects of using information from parallel data are prevailing. 4 tionaries have been used in POS tagging (Hajiˇc, 2000). More distant is the approach of, e.g., Brown et al. (1992) and Ide et al. (2002), where parallel text is used for learning supervision, but not for feature extraction; Diab and Resnik (2002) use an unsupervised method. We should also mention the idea of using parallel corpora as hidden features, a task first performed by (Brown et al., 1992) for WSD and subsequent"
W15-2111,W07-1709,1,0.845918,"Missing"
W15-2111,P97-1009,0,0.432533,"Missing"
W15-2111,P14-5003,1,0.893555,"Missing"
W15-2111,C04-1192,0,0.0753298,"Missing"
W15-2111,W15-1613,1,0.749109,"Missing"
W15-2111,F14-1005,0,0.0227862,"Missing"
W15-2111,N09-1004,0,\N,Missing
W15-2111,P02-1033,0,\N,Missing
W15-2111,W12-3132,1,\N,Missing
W15-2111,cinkova-2006-propbank,0,\N,Missing
W16-1812,W14-2902,1,0.892056,"Missing"
W16-3810,baranes-sagot-2014-language,0,0.022006,"erivation such as English -ing (Czech: -ˇení, -a/ání, -í/ávání, -(u)tí, ...), but also those which are much less frequent (-ba, -nost, -ota). 2 Related Work Derivations, especially verbal derivations, have been studied extensively. Almost all grammars include a section on derivations, even if they use different theoretical starting points. The most recent work on Czech derivations is (Žabokrtský and Ševˇcíková, 2014; Ševˇcíková and Žabokrtský, 2014; Vidra, 2015; Vidra et al., 2015). These authors also created a resource called DeriNet (cf. Sect. 3.2). The background for their work comes from (Baranes and Sagot, 2014) and (Baayen et al., 1995). DeriNet, while keeping explicit the connection between the verb and its derivative, does not use valency as a criterion for having such a link, and therefore is broader than what we are aiming at in our study; however, we have used it as one of the starting points for the creation of the gold standard data (Sect. 4). Event nouns, which form a major part of our definition of deverbatives, have also been studied extensively. A general approach to events and their identification in text can be found, e.g., in (Palmer et al., 2009) or (Stone et al., 2000). NomBank (Meye"
W16-3810,bojar-etal-2012-joy,0,0.0381231,"Missing"
W16-3810,cinkova-2006-propbank,0,0.027418,"endency Treebank (PCEDT 2.0) (Hajiˇc et al., 2012). The PCEDT is a 1-million-word bilingual corpus that is manually annotated and sentence-aligned and automatically word-aligned. In addition, it contains the predicate-argument annotation itself, where the verbs are sense-disambiguated by linking them to Czech and English valency lexicons. The English side builds on the PropBank corpus (Palmer et al., 2005), which annotates predicate-argument structure over the Penn Treebank (Marcus et al., 1993). The associated valency lexicons for Czech - PDT-Vallex3 (Urešová, 2011) and English - EngVallex4 (Cinková, 2006) are also interlinked, forming a bilingual lexicon CzEngVallex (Urešová et al., 2016), which explicitly pairs verb senses and their arguments between the two languages. The second corpus used was CzEng5 (Bojar et al., 2011; Bojar et al., 2012; Bojar et al., 2016), a 15-million sentence parallel corpus of Czech and English texts. This corpus is automatically parsed and deep-parsed, verbs are automatically annotated by links to the same valency lexicons as in the PCEDT. The corpus is automatically sentence- and word-aligned. The reason for using both a small high-quality annotated and a “noisy”"
W16-3810,hajic-etal-2012-announcing,1,0.904505,"Missing"
W16-3810,kingsbury-palmer-2002-treebank,0,0.149657,"ture and/or valency, in some cases also subcategorization information or semantic preferences are included. Creating such a lexicon is a laborious task. On top of the sheer volume of such a lexicon (to achieve good coverage of the given language), the biggest difficulty is to keep consistency among entries that describe verbs with the same or very similar behavior. The same holds for derivations; in most cases, no attempt is made to link the derivations to the base verbs in the lexicon (with NomBank (Meyers et al., 2004) being an exception, linking nouns to base verbs in the English PropBank (Kingsbury and Palmer, 2002)). Valency information (number and function of the arguments) is shared between the base verb and its deverbatives, undergoing certain transformations in defined cases.1 Moreover, especially in richly inflective languages, the subcategorization information (morphosyntactic surface expression of the arguments) can be derived by more or less deterministic rules from the verb, the deverbative relation and the verb’s arguments’ subcategorization (Koláˇrová, 2006; Koláˇrová, 2005; Koláˇrová, 2014). These rules, for example, transform the case of Actor (Deep subject) from nominative to genitive as t"
W16-3810,J93-2004,0,0.0541607,"i, 1994). 72 3 The Data Available 3.1 Corpora As one source of bilingual text, we have used the Prague Czech-English Dependency Treebank (PCEDT 2.0) (Hajiˇc et al., 2012). The PCEDT is a 1-million-word bilingual corpus that is manually annotated and sentence-aligned and automatically word-aligned. In addition, it contains the predicate-argument annotation itself, where the verbs are sense-disambiguated by linking them to Czech and English valency lexicons. The English side builds on the PropBank corpus (Palmer et al., 2005), which annotates predicate-argument structure over the Penn Treebank (Marcus et al., 1993). The associated valency lexicons for Czech - PDT-Vallex3 (Urešová, 2011) and English - EngVallex4 (Cinková, 2006) are also interlinked, forming a bilingual lexicon CzEngVallex (Urešová et al., 2016), which explicitly pairs verb senses and their arguments between the two languages. The second corpus used was CzEng5 (Bojar et al., 2011; Bojar et al., 2012; Bojar et al., 2016), a 15-million sentence parallel corpus of Czech and English texts. This corpus is automatically parsed and deep-parsed, verbs are automatically annotated by links to the same valency lexicons as in the PCEDT. The corpus is"
W16-3810,W04-2705,0,0.325648,"). There have been created many lexicons that contain verbs and their predicate-argument structure and/or valency, in some cases also subcategorization information or semantic preferences are included. Creating such a lexicon is a laborious task. On top of the sheer volume of such a lexicon (to achieve good coverage of the given language), the biggest difficulty is to keep consistency among entries that describe verbs with the same or very similar behavior. The same holds for derivations; in most cases, no attempt is made to link the derivations to the base verbs in the lexicon (with NomBank (Meyers et al., 2004) being an exception, linking nouns to base verbs in the English PropBank (Kingsbury and Palmer, 2002)). Valency information (number and function of the arguments) is shared between the base verb and its deverbatives, undergoing certain transformations in defined cases.1 Moreover, especially in richly inflective languages, the subcategorization information (morphosyntactic surface expression of the arguments) can be derived by more or less deterministic rules from the verb, the deverbative relation and the verb’s arguments’ subcategorization (Koláˇrová, 2006; Koláˇrová, 2005; Koláˇrová, 2014)."
W16-3810,J05-1004,0,0.0780125,"corpora has been described previously. 2 We have also found similar work for Italian (Graffi, 1994). 72 3 The Data Available 3.1 Corpora As one source of bilingual text, we have used the Prague Czech-English Dependency Treebank (PCEDT 2.0) (Hajiˇc et al., 2012). The PCEDT is a 1-million-word bilingual corpus that is manually annotated and sentence-aligned and automatically word-aligned. In addition, it contains the predicate-argument annotation itself, where the verbs are sense-disambiguated by linking them to Czech and English valency lexicons. The English side builds on the PropBank corpus (Palmer et al., 2005), which annotates predicate-argument structure over the Penn Treebank (Marcus et al., 1993). The associated valency lexicons for Czech - PDT-Vallex3 (Urešová, 2011) and English - EngVallex4 (Cinková, 2006) are also interlinked, forming a bilingual lexicon CzEngVallex (Urešová et al., 2016), which explicitly pairs verb senses and their arguments between the two languages. The second corpus used was CzEng5 (Bojar et al., 2011; Bojar et al., 2012; Bojar et al., 2016), a 15-million sentence parallel corpus of Czech and English texts. This corpus is automatically parsed and deep-parsed, verbs are a"
W16-3810,sevcikova-zabokrtsky-2014-word,0,0.0632846,"Missing"
W16-3810,W00-2028,0,0.0699664,"mes from (Baranes and Sagot, 2014) and (Baayen et al., 1995). DeriNet, while keeping explicit the connection between the verb and its derivative, does not use valency as a criterion for having such a link, and therefore is broader than what we are aiming at in our study; however, we have used it as one of the starting points for the creation of the gold standard data (Sect. 4). Event nouns, which form a major part of our definition of deverbatives, have also been studied extensively. A general approach to events and their identification in text can be found, e.g., in (Palmer et al., 2009) or (Stone et al., 2000). NomBank (Meyers et al., 2004) is a prime resource for nominal predicate-argument structure in English. Closest to what we want to achieve here, is the paper (Meyers, 2008), where the authors also use various resources for helping to construct English NomBank; however, they do not make use of parallel resources. For Czech, while we assume that relations between verbs and their deverbatives regarding valency structure can be described by grammatical rules (Koláˇrová, 2014; Koláˇrová, 2006; Koláˇrová, 2005),2 no attempt to automatically extract deverbatives from lexicons and/or corpora has been"
W16-3810,P14-5003,1,0.885359,"Missing"
W19-7805,P98-1013,0,0.366734,"Missing"
W19-7805,cinkova-2006-propbank,0,0.0284707,"om LDC12 , described in (Hajič et al., 2012), is used. It contains about 55,000 sentences on each language side. The English side is the Wall Street Journal part of the Penn Treebank and the Czech side is its translation. Both sides are annotated for the so-called Tectogrammatical Representation (TR), used for the Prague Dependency Treebank family of projects (Hajič et al., 2006). Most importantly, content verbs are annotated by their corresponding valency lexicon entries as captured in the PDT-Vallex lexicon (Urešová et al., 2014) on the Czech side and in the EngVallex (Cinková et al., 2014; Cinková, 2006) on the English side. As described in (Urešová et al., 2018a) (and in Sect. 2.1 above), the PCEDT corpus has been used as a source information for building the CzEngClass lexicon, raising the question of why the annotation cannot be fully deterministic. However, the classes are in principle independent of the PCEDT data and they underwent manual pruning; it is thus likely we get ambiguous (or even no) annotation by simply following the links from the CzEngClass entries directly to the corpus. In any case, the coverage of the CzEngClass entries will be relatively high, since they have been extr"
W19-7805,W07-1508,0,0.0221056,"s #any,#sb, and #sth. 6 https://framenet.icsi.berkeley.edu Class: decline – odmítnout Roleset (semantic roles) Class Member (sense ID) Authority Proposer Proposal decline (EngVallex-ID-ev-w829f1) ACT ADDR PAT deny (EngVallex-ID-ev-ev-w876f1) ACT #sb PAT deny (EngVallex-ID-ev-ev-w876f2) ACT ADDR PAT odmítnout (PDT-Vallex-ID-v-w2785f1) ACT #sb PAT refuse (EngVallex-ID-ev-w@2598f1) ACT #sb PAT přijmout (PDT-Vallex-ID-v-w5161f3) ACT ORIG PAT Restrictions or requirements negation Table 2: Example class for decline – odmítnout with role mappings (simplified, from 24 verbs) • VerbNet (Schuler, 2006; Duffield et al., 2007; Kipper et al., 2006), a class-based verb lexicon7 with syntactic and semantic information of English verbs, • PropBank (Palmer et al., 2005), linked to the OntoNotes corpus,8 • SemLink (Palmer, 2009)9 which connects the above lexicons, and • WordNet (Miller, 1995; Fellbaum, 1998).10 The external links allow to compare and use these lexical resources with the annotated corpus, but for the proper semantic annotation are not used except as a source of secondary knowledge for the annotator when making annotation decisions.11 2.2 The PCEDT Corpus For this project, the Prague Czech-English Depende"
W19-7805,kipper-etal-2006-extending,0,0.0479087,"https://framenet.icsi.berkeley.edu Class: decline – odmítnout Roleset (semantic roles) Class Member (sense ID) Authority Proposer Proposal decline (EngVallex-ID-ev-w829f1) ACT ADDR PAT deny (EngVallex-ID-ev-ev-w876f1) ACT #sb PAT deny (EngVallex-ID-ev-ev-w876f2) ACT ADDR PAT odmítnout (PDT-Vallex-ID-v-w2785f1) ACT #sb PAT refuse (EngVallex-ID-ev-w@2598f1) ACT #sb PAT přijmout (PDT-Vallex-ID-v-w5161f3) ACT ORIG PAT Restrictions or requirements negation Table 2: Example class for decline – odmítnout with role mappings (simplified, from 24 verbs) • VerbNet (Schuler, 2006; Duffield et al., 2007; Kipper et al., 2006), a class-based verb lexicon7 with syntactic and semantic information of English verbs, • PropBank (Palmer et al., 2005), linked to the OntoNotes corpus,8 • SemLink (Palmer, 2009)9 which connects the above lexicons, and • WordNet (Miller, 1995; Fellbaum, 1998).10 The external links allow to compare and use these lexical resources with the annotated corpus, but for the proper semantic annotation are not used except as a source of secondary knowledge for the annotator when making annotation decisions.11 2.2 The PCEDT Corpus For this project, the Prague Czech-English Dependency Treebank (PCEDT),"
W19-7805,J05-1004,0,0.277799,"y Proposer Proposal decline (EngVallex-ID-ev-w829f1) ACT ADDR PAT deny (EngVallex-ID-ev-ev-w876f1) ACT #sb PAT deny (EngVallex-ID-ev-ev-w876f2) ACT ADDR PAT odmítnout (PDT-Vallex-ID-v-w2785f1) ACT #sb PAT refuse (EngVallex-ID-ev-w@2598f1) ACT #sb PAT přijmout (PDT-Vallex-ID-v-w5161f3) ACT ORIG PAT Restrictions or requirements negation Table 2: Example class for decline – odmítnout with role mappings (simplified, from 24 verbs) • VerbNet (Schuler, 2006; Duffield et al., 2007; Kipper et al., 2006), a class-based verb lexicon7 with syntactic and semantic information of English verbs, • PropBank (Palmer et al., 2005), linked to the OntoNotes corpus,8 • SemLink (Palmer, 2009)9 which connects the above lexicons, and • WordNet (Miller, 1995; Fellbaum, 1998).10 The external links allow to compare and use these lexical resources with the annotated corpus, but for the proper semantic annotation are not used except as a source of secondary knowledge for the annotator when making annotation decisions.11 2.2 The PCEDT Corpus For this project, the Prague Czech-English Dependency Treebank (PCEDT), as available from LDC12 , described in (Hajič et al., 2012), is used. It contains about 55,000 sentences on each languag"
W19-7805,W09-2417,0,0.039082,"lass mapping and its semantic role is filled into this node’s syn_class attributes. Similarly, for optional arguments or free modifications (adjuncts) listed in the CzEngClass entry mappings (for the given verb) which have not been found in the TR of the sentence in the corpus, a new artificial node is inserted. This node gets also the #SitRef “lemma”, the appropriate functor based on the CzEngClass mapping, and the corresponding semantic role; all filled into this new node’s syn_class attributes. This is similar to the approach to implicit semantic roles (“Null Instantiations”) described in (Ruppenhofer et al., 2009); the difference is that in our approach, licensing of such elements is based strictly on the set of roles assigned to the class (not on English - or any other language’s - grammar, given that we aim at multilingual classes). Also, we do not distinguish types of such situational references (indefinite, definite, ...) and defer this to the future process of discourse-based linking of the #SitRef to their referents, again without taking (grammatical) licensing into account; existence of a link will then correspond to definite null instantiations. The #SitRef nodes are not created when two (or mo"
W19-7805,L18-1227,1,0.414424,"e, report, ..., říci, sdělit, uvést, ... and its dependents in the semantic representation (regardless of their syntactic realization) are labeled by semantic roles assigned to that class (Speaker, Addressee, Information). Such a resource can be divided into two components: • a semantically oriented bi- or multilingual verbal synonym lexicon, linked to all the other lexical resources, and • the richly annotated corpus that contains references to entries in this semantic lexicon at every content verb (predicate) in the corpus. The first component is covered by the existing CzEngClass lexicon1 (Urešová et al., 2018c) which, while not complete and covering only Czech and English at this time, already provides enough synonym classes (and promises more coverage in the future) to approach the annotation task (the 2nd point above). In this paper, we start with a short description of the resources used directly or indirectly through the available lexicon and corpus (Sect. 2), then we show how we have proceeded with the annotation process 1 http://hdl.handle.net/11234/1-2977. (Sect. 3) and present the basic statistics of the automatic part of the annotation part of the process as applied to the whole corpus (S"
W19-7805,C18-1208,1,0.37827,"e, report, ..., říci, sdělit, uvést, ... and its dependents in the semantic representation (regardless of their syntactic realization) are labeled by semantic roles assigned to that class (Speaker, Addressee, Information). Such a resource can be divided into two components: • a semantically oriented bi- or multilingual verbal synonym lexicon, linked to all the other lexical resources, and • the richly annotated corpus that contains references to entries in this semantic lexicon at every content verb (predicate) in the corpus. The first component is covered by the existing CzEngClass lexicon1 (Urešová et al., 2018c) which, while not complete and covering only Czech and English at this time, already provides enough synonym classes (and promises more coverage in the future) to approach the annotation task (the 2nd point above). In this paper, we start with a short description of the resources used directly or indirectly through the available lexicon and corpus (Sect. 2), then we show how we have proceeded with the annotation process 1 http://hdl.handle.net/11234/1-2977. (Sect. 3) and present the basic statistics of the automatic part of the annotation part of the process as applied to the whole corpus (S"
xue-etal-2014-interlingua,N07-1051,0,\N,Missing
xue-etal-2014-interlingua,W04-2705,0,\N,Missing
xue-etal-2014-interlingua,A00-2018,0,\N,Missing
xue-etal-2014-interlingua,J93-2004,0,\N,Missing
xue-etal-2014-interlingua,C12-1083,0,\N,Missing
xue-etal-2014-interlingua,W04-3212,1,\N,Missing
xue-etal-2014-interlingua,J03-4003,0,\N,Missing
xue-etal-2014-interlingua,Q13-1034,1,\N,Missing
xue-etal-2014-interlingua,W09-1201,1,\N,Missing
xue-etal-2014-interlingua,J02-3001,0,\N,Missing
xue-etal-2014-interlingua,J05-1004,1,\N,Missing
xue-etal-2014-interlingua,Q13-1019,0,\N,Missing
xue-etal-2014-interlingua,prasad-etal-2008-penn,0,\N,Missing
xue-etal-2014-interlingua,W13-2322,1,\N,Missing
xue-etal-2014-interlingua,N04-1030,0,\N,Missing
zeman-etal-2012-hamledt,zeman-2008-reusable,1,\N,Missing
zeman-etal-2012-hamledt,bosco-etal-2010-comparing,0,\N,Missing
zeman-etal-2012-hamledt,W08-2121,0,\N,Missing
zeman-etal-2012-hamledt,C00-2143,0,\N,Missing
zeman-etal-2012-hamledt,P06-1033,0,\N,Missing
zeman-etal-2012-hamledt,W08-0325,1,\N,Missing
zeman-etal-2012-hamledt,D11-1036,0,\N,Missing
zeman-etal-2012-hamledt,D11-1006,0,\N,Missing
zeman-etal-2012-hamledt,ramasamy-zabokrtsky-2012-prague,1,\N,Missing
zeman-etal-2012-hamledt,R09-1007,0,\N,Missing
zeman-etal-2012-hamledt,dzeroski-etal-2006-towards,0,\N,Missing
zeman-etal-2012-hamledt,taule-etal-2008-ancora,0,\N,Missing
zeman-etal-2012-hamledt,afonso-etal-2002-floresta,0,\N,Missing
